<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>服务网格概述 | 云原生资料库</title>
    <link>https://jimmysong.io/docs/istio-handbook/intro/</link>
      <atom:link href="https://jimmysong.io/docs/istio-handbook/intro/index.xml" rel="self" type="application/rss+xml" />
    <description>服务网格概述</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>zh</language><lastBuildDate>Wed, 18 May 2022 00:00:00 +0800</lastBuildDate>
    <image>
      <url>https://jimmysong.io/docs/media/logo.svg</url>
      <title>服务网格概述</title>
      <link>https://jimmysong.io/docs/istio-handbook/intro/</link>
    </image>
    
    <item>
      <title>什么是服务网格？</title>
      <link>https://jimmysong.io/docs/istio-handbook/intro/what-is-service-mesh/</link>
      <pubDate>Wed, 18 May 2022 00:00:00 +0800</pubDate>
      <guid>https://jimmysong.io/docs/istio-handbook/intro/what-is-service-mesh/</guid>
      <description>&lt;p&gt;Service Mesh 又译作 “服务网格”，作为服务间通信的基础设施层。Buoyant 公司的 CEO Willian Morgan 在他的这篇文章 &lt;a href=&#34;https://buoyant.io/what-is-a-service-mesh/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;WHAT’S A SERVICE MESH? AND WHY DO I NEED ONE?&lt;/a&gt; 中解释了什么是 Service Mesh，为什么云原生应用需要 Service Mesh。&lt;/p&gt;
&lt;p&gt;服务网格是用于处理服务间通信的专用基础设施层。它负责通过包含现代云原生应用程序的复杂服务拓扑来可靠地传递请求。实际上，服务网格通常通过一组轻量级网络代理来实现，这些代理与应用程序代码一起部署，而不需要感知应用程序本身。——  &lt;a href=&#34;https://twitter.com/wm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Willian Morgan&lt;/a&gt; Buoyant CEO&lt;/p&gt;
&lt;p&gt;服务网格（Service Mesh）这个术语通常用于描述构成这些应用程序的微服务网络以及应用之间的交互。随着规模和复杂性的增长，服务网格越来越难以理解和管理。它的需求包括服务发现、负载均衡、故障恢复、指标收集和监控以及通常更加复杂的运维需求，例如 A/B 测试、金丝雀发布、限流、访问控制和端到端认证等。&lt;/p&gt;
&lt;h2 id=&#34;服务网格的特点&#34;&gt;服务网格的特点&lt;/h2&gt;
&lt;p&gt;服务网格有如下几个特点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;应用程序间通讯的中间层&lt;/li&gt;
&lt;li&gt;轻量级网络代理&lt;/li&gt;
&lt;li&gt;应用程序无感知&lt;/li&gt;
&lt;li&gt;解耦应用程序的重试/超时、监控、追踪和服务发现&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;目前两款流行的服务网格开源软件 &lt;a href=&#34;https://linkerd.io&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Linkerd&lt;/a&gt; 和 &lt;a href=&#34;https://Istio.io&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Istio&lt;/a&gt; 都可以直接在 Kubernetes 中集成，其中 Linkerd 是 CNCF 成员项目，并在 2021 年 7 月毕业。Istio 在 2018年7月31日宣布 1.0，并在 2020 年 7 月将&lt;a href=&#34;https://istio.io/latest/blog/2020/open-usage/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;商标捐献&lt;/a&gt;给 &lt;a href=&#34;https://openusage.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Open Usage Commons&lt;/a&gt;。&lt;/p&gt;
&lt;h2 id=&#34;理解服务网格&#34;&gt;理解服务网格&lt;/h2&gt;
&lt;p&gt;如果用一句话来解释什么是服务网格，可以将它比作是应用程序或者说微服务间的 TCP/IP，负责服务之间的网络调用、限流、熔断和监控。对于编写应用程序来说一般无须关心 TCP/IP 这一层（比如通过 HTTP 协议的 RESTful 应用），同样使用服务网格也就无须关系服务之间的那些原来是通过应用程序或者其他框架实现的事情，比如 Spring Cloud、OSS，现在只要交给服务网格就可以了。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://philcalcado.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Phil Calçado&lt;/a&gt; 在他的这篇博客 &lt;a href=&#34;http://philcalcado.com/2017/08/03/pattern_service_mesh.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pattern: Service Mesh&lt;/a&gt; 中详细解释了服务网格的来龙去脉：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;从最原始的主机之间直接使用网线相连&lt;/li&gt;
&lt;li&gt;网络层的出现&lt;/li&gt;
&lt;li&gt;集成到应用程序内部的控制流&lt;/li&gt;
&lt;li&gt;分解到应用程序外部的控制流&lt;/li&gt;
&lt;li&gt;应用程序的中集成服务发现和断路器&lt;/li&gt;
&lt;li&gt;出现了专门用于服务发现和断路器的软件包/库，如 &lt;a href=&#34;https://finagle.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Twitter 的 Finagle&lt;/a&gt; 和 &lt;a href=&#34;https://code.fb.com/networking-traffic/introducing-proxygen-facebook-s-c-http-framework/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Facebook  的 Proxygen&lt;/a&gt;，这时候还是集成在应用程序内部&lt;/li&gt;
&lt;li&gt;出现了专门用于服务发现和断路器的开源软件，如 &lt;a href=&#34;http://netflix.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Netflix OSS&lt;/a&gt;、Airbnb 的 &lt;a href=&#34;https://github.com/airbnb/synapse&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;synapse&lt;/a&gt; 和 &lt;a href=&#34;https://github.com/airbnb/nerve&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;nerve&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;最后作为微服务的中间层服务网格出现&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;服务网格的架构如下图所示：&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-service-mesh-架构图&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;../../images/00704eQkly1fswh7dbs1pj30id0bpmxl.jpg&#34; alt=&#34;Service Mesh 架构图&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Service Mesh 架构图
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;图片来自：&lt;a href=&#34;http://philcalcado.com/2017/08/03/pattern_service_mesh.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pattern: Service Mesh&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;服务网格作为 sidecar 运行，对应用程序来说是透明，所有应用程序间的流量都会通过它，所以对应用程序流量的控制都可以在 serivce mesh 中实现。&lt;/p&gt;
&lt;h2 id=&#34;服务网格如何工作&#34;&gt;服务网格如何工作？&lt;/h2&gt;
&lt;p&gt;下面以 Istio 为例讲解服务网格如何在 Kubernetes 中工作。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Istio 将服务请求路由到目的地址，根据中的参数判断是到生产环境、测试环境还是 staging 环境中的服务（服务可能同时部署在这三个环境中），是路由到本地环境还是公有云环境？所有的这些路由信息可以动态配置，可以是全局配置也可以为某些服务单独配置。&lt;/li&gt;
&lt;li&gt;当 Istio 确认了目的地址后，将流量发送到相应服务发现端点，在 Kubernetes 中是 service，然后 service 会将服务转发给后端的实例。&lt;/li&gt;
&lt;li&gt;Istio 根据它观测到最近请求的延迟时间，选择出所有应用程序的实例中响应最快的实例。&lt;/li&gt;
&lt;li&gt;Istio 将请求发送给该实例，同时记录响应类型和延迟数据。&lt;/li&gt;
&lt;li&gt;如果该实例挂了、不响应了或者进程不工作了，Istio 将把请求发送到其他实例上重试。&lt;/li&gt;
&lt;li&gt;如果该实例持续返回 error，Istio 会将该实例从负载均衡池中移除，稍后再周期性得重试。&lt;/li&gt;
&lt;li&gt;如果请求的截止时间已过，Istio 主动失败该请求，而不是再次尝试添加负载。&lt;/li&gt;
&lt;li&gt;Istio 以 metric 和分布式追踪的形式捕获上述行为的各个方面，这些追踪信息将发送到集中 metric 系统。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;为何使用服务网格&#34;&gt;为何使用服务网格？&lt;/h2&gt;
&lt;p&gt;服务网格并没有给我们带来新功能，它是用于解决其他工具已经解决过的问题，只不过这次是在云原生的 Kubernetes 环境下的实现。&lt;/p&gt;
&lt;p&gt;在传统的 MVC 三层 Web 应用程序架构下，服务之间的通讯并不复杂，在应用程序内部自己管理即可，但是在现今的复杂的大型网站情况下，单体应用被分解为众多的微服务，服务之间的依赖和通讯十分复杂，出现了 Twitter 开发的 &lt;a href=&#34;https://twitter.github.io/finagle/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Finagle&lt;/a&gt;、Netflix 开发的 &lt;a href=&#34;https://github.com/Netflix/Hystrix&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hystrix&lt;/a&gt; 和 Google 的 Stubby 这样的 “胖客户端” 库，这些就是早期的服务网格，但是它们都近适用于特定的环境和特定的开发语言，并不能作为平台级的服务网格支持。&lt;/p&gt;
&lt;p&gt;在云原生架构下，容器的使用给予了异构应用程序的更多可行性，Kubernetes 增强的应用的横向扩容能力，用户可以快速的编排出复杂环境、复杂依赖关系的应用程序，同时开发者又无须过分关心应用程序的监控、扩展性、服务发现和分布式追踪这些繁琐的事情而专注于程序开发，赋予开发者更多的创造性。&lt;/p&gt;
&lt;h2 id=&#34;参考&#34;&gt;参考&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://medium.com/attest-engineering/Istio-a-service-mesh-for-aws-ecs-937f201f847a&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Istio: A service mesh for AWS ECS - medium.com&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://istio.io/latest/news/releases/0.x/announcing-0.1/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;初次了解 Istio - istio.io&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://blog.christianposta.com/microservices/application-network-functions-with-esbs-api-management-and-now-service-mesh/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Application Network Functions With ESBs, API Management, and Now.. Service Mesh? - blog.christianposta.com&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://philcalcado.com/2017/08/03/pattern_service_mesh.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pattern: Service Mesh - philcalcado.com&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://cloudnative.to/envoy/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Envoy 官方文档中文版 - cloudnative.to&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://istio.io&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Istio 官方文档 - istio.io&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>为什么要使用服务网格？</title>
      <link>https://jimmysong.io/docs/istio-handbook/intro/why-service-mesh/</link>
      <pubDate>Wed, 18 May 2022 00:00:00 +0800</pubDate>
      <guid>https://jimmysong.io/docs/istio-handbook/intro/why-service-mesh/</guid>
      <description>&lt;p&gt;要想说明为什么要使用服务网格，那就要从微服务架构说起，可以说服务网格很大程度上是一种新一代的微服务架构，它解决了微服务中网络层操控性、弹性、可视性的问题。&lt;/p&gt;
&lt;h2 id=&#34;微服务架构&#34;&gt;微服务架构&lt;/h2&gt;
&lt;p&gt;开发人员经常将云原生应用程序分解为多个执行特定动作的服务。你可能有一个只处理客户的服务和另一个处理订单或付款的服务。所有这些服务都通过网络相互沟通。如果一个新的付款需要被处理，请求会被发送到付款服务。如果客户数据需要更新，请求会被发送到客户服务，等等。&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-微服务架构&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;../../images/008i3skNly1gt2kidx2v7j30zk0k03za.jpg&#34; alt=&#34;微服务架构&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      微服务架构
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;这种类型的架构被称为微服务架构。这种架构有几个好处。你可以有多个较小的团队从事个别服务。这些团队可以灵活地选择他们的技术栈和语言，并且通常有独立部署和发布服务的自主权。这种机制得以运作得益于在其背后通信的网络。随着服务数量的增加，它们之间的通信和网络通信也在增加。服务和团队的数量使得监控和管理通信逻辑变得相当复杂。由于我们也知道网络是不可靠的，它们会失败，所有这些的结合使得微服务的管理和监控相当复杂。&lt;/p&gt;
&lt;h2 id=&#34;服务网格概述&#34;&gt;服务网格概述&lt;/h2&gt;
&lt;p&gt;服务网格被定义为一个专门的基础设施层，用于管理服务与服务之间的通信，使其可管理、可见、可控制。在某些版本的定义中，你可能还会听到服务网格如何使服务间的通信安全和可靠。如果我必须用一个更直接的句子来描述服务网格，我会说，服务网格是关于服务之间的通信。&lt;/p&gt;
&lt;p&gt;但是，服务网格是如何帮助通信的呢？让我们思考一下通信逻辑和它通常所在的地方。在大多数情况下，开发人员将这种逻辑作为服务的一部分来构建。通信逻辑是处理入站或出站请求的任何代码，重试逻辑，超时，甚至可能是流量路由。因此，无论何时服务 A 调用服务 B，请求都要经过这个通信代码逻辑，这个逻辑决定如何处理这个请求。&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-通信逻辑&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;../../images/008i3skNly1gt2kiyf8ymj30zk0k074v.jpg&#34; alt=&#34;通信逻辑&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      通信逻辑
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;我们提到，如果我们采用微服务的方法，最终可能会有大量的服务。我们如何处理所有这些服务的通信逻辑呢？我们可以创建一个包含这种逻辑的共享库，并在多个地方重用它。假设我们对所有的服务都使用相同的堆栈或编程语言，共享库的方法可能会很有效。如果我们不这样做，我们将不得不重新实现这个库，这会带来巨大的工作量而且效率低下。你也可能使用自己本身不拥有代码库的服务。在这种情况下，我们无法控制通信逻辑或监控。&lt;/p&gt;
&lt;p&gt;第二个问题是配置。除了配置你的应用程序外，我们还必须维护通信逻辑配置。如果我们需要同时调整或更新多个服务，我们将不得不为每个服务单独进行调整。&lt;/p&gt;
&lt;p&gt;服务网格所做的是，它将这种通信逻辑、重试、超时等从单个服务中分离出来，并将其移到一个单独的基础设施层。在服务网格的情况下，基础设施层是一个网络代理的阵列。这些网络代理的集合（每个服务实例旁边都有一个）处理你的服务之间的所有通信逻辑。我们称这些代理为 sideecar，因为它们与每个服务并存。&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-sidecar-代理&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;../../images/008i3skNly1gt2kj8kf84j30zk0k0t9s.jpg&#34; alt=&#34;Sidecar 代理&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Sidecar 代理
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;以前，我们让 Customer 服务直接与 Payment 服务通信，现在我们有一个 Customer 服务旁边的代理与 Payment 服务旁边的代理通信。服务网格控制平面以这样一种方式配置代理，即它们透明地拦截所有入站和出站请求。这些代理的集合（基础设施层）形成了一个网络网格，称为服务网格。&lt;/p&gt;
&lt;p&gt;将通信逻辑从业务和应用逻辑中分离出来，可以使开发人员专注于业务逻辑，而服务网格运维人员则专注于服务网格配置。&lt;/p&gt;
&lt;h2 id=&#34;服务网格的功能&#34;&gt;服务网格的功能&lt;/h2&gt;
&lt;p&gt;服务网格为我们提供了一种一致的方式来连接、保护和观察微服务。网格内的代理捕获了网格内所有通信的请求和指标。每一次失败、每一次成功的调用、重试或超时都可以被捕获、可视化，并发出警报。此外，可以根据请求属性做出决定。例如，我们可以检查入站（或出站）请求并编写规则，将所有具有特定头值的请求路由到不同的服务版本。&lt;/p&gt;
&lt;p&gt;所有这些信息和收集到的指标使得一些场景可以合理地直接实现。开发人员和运营商可以配置和执行以下方案，而不需要对服务进行任何代码修改。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;mTLS 和自动证书轮换&lt;/li&gt;
&lt;li&gt;使用指标识别性能和可靠性问题&lt;/li&gt;
&lt;li&gt;在 Grafana 等工具中实现指标的可视化&lt;/li&gt;
&lt;li&gt;使用 Jaeger 或 Zipkin（需要对代码进行小的修改，以便在服务之间传播跟踪头信息） 对服务进行调试和追踪&lt;/li&gt;
&lt;li&gt;基于权重和请求的流量路由，金丝雀部署，A/B 测试&lt;/li&gt;
&lt;li&gt;流量镜像&lt;/li&gt;
&lt;li&gt;通过超时和重试提高服务的弹性&lt;/li&gt;
&lt;li&gt;通过在服务之间注入故障和延迟来进行混沌测试&lt;/li&gt;
&lt;li&gt;检测和弹出不健康的服务实例的断路器&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>云原生应用网络</title>
      <link>https://jimmysong.io/docs/istio-handbook/intro/cloud-native-application-network/</link>
      <pubDate>Wed, 18 May 2022 00:00:00 +0800</pubDate>
      <guid>https://jimmysong.io/docs/istio-handbook/intro/cloud-native-application-network/</guid>
      <description>&lt;p&gt;如果你听说过服务网格，并尝试过 &lt;a href=&#34;https://istio.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Istio&lt;/a&gt;，你可能有以下问题。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;为什么 Istio 要在 Kubernetes 上运行？&lt;/li&gt;
&lt;li&gt;Kubernetes 和服务网格在云原生应用架构中分别扮演什么角色？&lt;/li&gt;
&lt;li&gt;Istio 扩展了 Kubernetes 的哪些方面？它解决了哪些问题？&lt;/li&gt;
&lt;li&gt;Kubernetes、Envoy 和 Istio 之间是什么关系？&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;本文将带大家了解 Kubernetes 和 Istio 的内部工作原理。此外，我会介绍 Kubernetes 中的负载均衡方法，并解释为什么有了 Kubernetes 后还需要 Istio。&lt;/p&gt;
&lt;p&gt;Kubernetes 本质上是通过声明式配置来实现应用生命周期管理，而服务网格本质上是提供应用间的流量、安全管理和可观察性。如果你已经使用 Kubernetes 搭建了一个稳定的应用平台，那么如何设置服务间调用的负载均衡和流量控制？是否有这样一个通用的工具或者说平台（非 SDK），可以实现？这就需要用到服务网格了。&lt;/p&gt;
&lt;p&gt;Envoy 引入了 xDS 协议，这个协议得到了各种开源软件的支持，比如 Istio、&lt;a href=&#34;https://mosn.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MOSN&lt;/a&gt; 等。Envoy 将 xDS 贡献给服务网格或云原生基础设施。Envoy 本质上是一个现代版的代理，可以通过 API 进行配置，在此基础上衍生出许多不同的使用场景，比如 API Gateway、服务网格中的 sidecar 代理和边缘代理。&lt;/p&gt;
&lt;p&gt;本文包含以下内容。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;kube-proxy 的作用描述。&lt;/li&gt;
&lt;li&gt;Kubernetes 在微服务管理方面的局限性。&lt;/li&gt;
&lt;li&gt;Istio 服务网格的功能介绍。&lt;/li&gt;
&lt;li&gt;Kubernetes、Envoy 和 Istio 服务网格中一些概念的比较。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;kubernetes-vs-service-mesh&#34;&gt;Kubernetes vs Service Mesh&lt;/h2&gt;
&lt;p&gt;下图展示的是 Kubernetes 与 Service Mesh 中的的服务访问关系，本文仅针对 sidecar per-pod 模式，详情请参考&lt;a href=&#34;../../concepts/service-mesh-patterns&#34;&gt;服务网格的实现模式&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-kubernetes-vs-service-mesh&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;../../images/k8s-vs-service-mesh.jpg&#34; alt=&#34;kubernetes vs service mesh&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      kubernetes vs service mesh
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;流量转发&#34;&gt;流量转发&lt;/h3&gt;
&lt;p&gt;Kubernetes 集群中的每个节点都部署了一个 kube-proxy 组件，该组件与 Kubernetes API Server 进行通信，获取集群中的服务信息，然后设置 iptables 规则，将服务请求直接发送到对应的 Endpoint（属于同一组服务的 pod）。&lt;/p&gt;
&lt;h3 id=&#34;服务发现&#34;&gt;服务发现&lt;/h3&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-服务发现&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;../../images/service-discovery.jpg&#34; alt=&#34;服务发现&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      服务发现
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Istio 可以跟踪 Kubernetes 中的服务注册，也可以在控制平面中通过平台适配器与其他服务发现系统对接；然后生成数据平面的配置（使用 CRD，这些配置存储在 etcd 中），数据平面的透明代理。数据平面的透明代理以 sidecar 容器的形式部署在每个应用服务的 pod 中，这些代理都需要请求控制平面同步代理配置。代理之所以 “透明”，是因为应用容器完全不知道代理的存在。过程中的 kube-proxy 组件也需要拦截流量，只不过 kube-proxy 拦截的是进出 Kubernetes 节点的流量，而 sidecar 代理拦截的是进出 pod 的流量。&lt;/p&gt;
&lt;h3 id=&#34;服务网格的劣势&#34;&gt;服务网格的劣势&lt;/h3&gt;
&lt;p&gt;由于 Kubernetes 的每个节点上都运行着很多 pod，所以在每个 pod 中放入原有的 kube-proxy 路由转发功能，会增加响应延迟——由于 sidecar 拦截流量时跳数更多，消耗更多的资源。为了对流量进行精细化管理，将增加一系列新的抽象功能。这将进一步增加用户的学习成本，但随着技术的普及，这种情况会慢慢得到缓解。&lt;/p&gt;
&lt;h3 id=&#34;服务网格的优势&#34;&gt;服务网格的优势&lt;/h3&gt;
&lt;p&gt;kube-proxy 的设置是全局的，无法对每个服务进行细粒度的控制，而 service mesh 通过 sidecar proxy 的方式将 Kubernetes 中的流量控制从服务层中抽离出来–可以实现更大的弹性。&lt;/p&gt;
&lt;h3 id=&#34;kube-proxy-的不足之处&#34;&gt;Kube-proxy 的不足之处&lt;/h3&gt;
&lt;p&gt;首先，如果转发的 pod 不能正常服务，它不会自动尝试其他 pod。每个 pod 都有一个健康检查机制，当一个 pod 出现健康问题时，kubelet 会重启 pod，kube-proxy 会删除相应的转发规则。另外，节点 Port 类型的服务不能添加 TLS 或更复杂的消息路由机制。&lt;/p&gt;
&lt;p&gt;Kube-proxy 实现了一个 Kubernetes 服务的多个 pod 实例之间的流量负载均衡，但如何对这些服务之间的流量进行精细化控制–比如将流量按百分比划分给不同的应用版本（这些应用版本都是同一个服务的一部分，但在不同的部署上），或者做金丝雀发布（灰度发布）和蓝绿发布？&lt;/p&gt;
&lt;p&gt;Kubernetes 社区给出了一个使用 Deployment 做&lt;a href=&#34;https://kubernetes.io/docs/concepts/cluster-administration/manage-deployment/#canary-deployments&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;金丝雀发布&lt;/a&gt;的方法，本质上是通过修改 pod 的标签来给部署的服务分配不同的 pod。&lt;/p&gt;
&lt;h3 id=&#34;kubernetes-ingress-vs-istio-gateway&#34;&gt;Kubernetes Ingress vs Istio Gateway&lt;/h3&gt;
&lt;p&gt;如上所述，kube-proxy 只能在 Kubernetes 集群内路由流量。Kubernetes 集群的 pods 位于 CNI 创建的网络中。一个 ingress—— 一个在 Kubernetes 中创建的资源对象 - 被创建用于集群外部的通信。它由位于 Kubernetes 边缘节点上的入口控制器驱动，负责管理南北向流量。Ingress 必须与各种 Ingress 控制器对接，比如 &lt;a href=&#34;https://github.com/kubernetes/ingress-nginx&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;nginx ingress 控制器&lt;/a&gt;和 &lt;a href=&#34;https://traefik.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;traefik&lt;/a&gt;。Ingress 只适用于 HTTP 流量，使用简单。它只能通过匹配有限的字段来路由流量–如服务、端口、HTTP 路径等。这使得它无法对 TCP 流量进行路由，如 MySQL、Redis 和各种 RPC。这就是为什么你会看到人们在 ingress 资源注释中写 nginx 配置语言的原因。直接路由南北流量的唯一方法是使用服务的 LoadBalancer 或 NodePort，前者需要云厂商支持，后者需要额外的端口管理。&lt;/p&gt;
&lt;p&gt;Istio Gateway 的功能与 Kubernetes Ingress 类似，它负责进出集群的南北流量。Istio Gateway 描述了一个负载平衡器，用于承载进出网状结构边缘的连接。该规范描述了一组开放端口和这些端口所使用的协议，以及用于负载均衡的 SNI 配置等。Gateway 是一个 CRD 扩展，它也重用了 sidecar 代理的功能；详细配置请参见 &lt;a href=&#34;https://istio.io/latest/docs/reference/config/networking/gateway/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Istio 网站&lt;/a&gt;。&lt;/p&gt;
&lt;h2 id=&#34;envoy&#34;&gt;Envoy&lt;/h2&gt;
&lt;p&gt;Envoy 是 Istio 中默认的 sidecar 代理。Istio 基于 Envoy 的 xDS 协议扩展了其控制平面。在讨论 Envoy 的 xDS 协议之前，我们需要先熟悉 Envoy 的基本术语。以下是 Envoy 中的基本术语及其数据结构的列表，更多细节请参考 &lt;a href=&#34;https://envoyproxy.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Envoy 文档&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-envoy-proxy-架构图&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;../../images/envoy-arch.jpg&#34; alt=&#34;Envoy proxy 架构图&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Envoy proxy 架构图
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;基本术语&#34;&gt;基本术语&lt;/h3&gt;
&lt;p&gt;下面是您应该了解的 Envoy 里的基本术语：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Downstream（下游）&lt;/strong&gt;：下游主机连接到 Envoy，发送请求并接收响应，即发送请求的主机。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Upstream（上游）&lt;/strong&gt;：上游主机接收来自 Envoy 的连接和请求，并返回响应，即接受请求的主机。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Listener（监听器）&lt;/strong&gt;：监听器是命名网地址（例如，端口、unix domain socket 等)，下游客户端可以连接这些监听器。Envoy 暴露一个或者多个监听器给下游主机连接。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Cluster（集群）&lt;/strong&gt;：集群是指 Envoy 连接的一组逻辑相同的上游主机。Envoy 通过服务发现来发现集群的成员。可以选择通过主动健康检查来确定集群成员的健康状态。Envoy 通过负载均衡策略决定将请求路由到集群的哪个成员。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在 Envoy 中可以设置多个监听器，每个监听器可以设置一个过滤链（过滤链表），而且过滤链是可扩展的，这样我们可以更方便地操纵流量的行为–比如设置加密、私有 RPC 等。&lt;/p&gt;
&lt;p&gt;xDS 协议是由 Envoy 提出的，是 Istio 中默认的 sidecar 代理，但只要实现了 xDS 协议，理论上也可以作为 Istio 中的 sidecar 代理 —— 比如蚂蚁集团开源的 &lt;a href=&#34;https://mosn.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MOSN&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-envoy-proxy-架构图&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;../../images/istio-arch.jpg&#34; alt=&#34;Istio 架构图&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Envoy proxy 架构图
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Istio 是一个功能非常丰富的服务网格，包括以下功能。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;流量管理。这是 Istio 最基本的功能。&lt;/li&gt;
&lt;li&gt;策略控制。实现访问控制系统、遥测采集、配额管理、计费等功能。&lt;/li&gt;
&lt;li&gt;可观察性。在 sidecar 代理中实现。&lt;/li&gt;
&lt;li&gt;安全认证。由 Citadel 组件进行密钥和证书管理。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;istio-中的流量管理&#34;&gt;Istio 中的流量管理&lt;/h2&gt;
&lt;p&gt;Istio 中定义了以下 CRD 来帮助用户进行流量管理。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;网关。网关描述了一个运行在网络边缘的负载均衡器，用于接收传入或传出的 HTTP/TCP 连接。&lt;/li&gt;
&lt;li&gt;虚拟服务（VirtualService）。VirtualService 实际上是将 Kubernetes 服务连接到 Istio 网关。它还可以执行额外的操作，例如定义一组流量路由规则，以便在主机寻址时应用。&lt;/li&gt;
&lt;li&gt;DestinationRule。DestinationRule 定义的策略决定了流量被路由后的访问策略。简单来说，它定义了流量的路由方式。其中，这些策略可以定义为负载均衡配置、连接池大小和外部检测（用于识别和驱逐负载均衡池中不健康的主机）配置。&lt;/li&gt;
&lt;li&gt;EnvoyFilter。EnvoyFilter 对象描述了代理服务的过滤器，可以自定义 Istio Pilot 生成的代理配置。这种配置一般很少被主用户使用。&lt;/li&gt;
&lt;li&gt;ServiceEntry。默认情况下，Istio 服务 Mesh 中的服务无法发现 Mesh 之外的服务。ServiceEntry 可以在 Istio 内部的服务注册表中添加额外的条目，从而允许 Mesh 中自动发现的服务访问并路由到这些手动添加的服务。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;kubernetes-vs-xds-vs-istio&#34;&gt;Kubernetes vs xDS vs Istio&lt;/h2&gt;
&lt;p&gt;在回顾了 Kubernetes 的 kube-proxy 组件、xDS 和 Istio 对流量管理的抽象后，现在我们仅从流量管理的角度来看看这三个组件 / 协议的比较（注意，三者并不完全等同）。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;strong&gt;Kubernetes&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;xDS&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Istio 服务网格&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Endpoint&lt;/td&gt;
&lt;td&gt;Endpoint&lt;/td&gt;
&lt;td&gt;WorkloadEntry&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Service&lt;/td&gt;
&lt;td&gt;Route&lt;/td&gt;
&lt;td&gt;VirtualService&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;kube-proxy&lt;/td&gt;
&lt;td&gt;Route&lt;/td&gt;
&lt;td&gt;DestinationRule&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;kube-proxy&lt;/td&gt;
&lt;td&gt;Listener&lt;/td&gt;
&lt;td&gt;EnvoyFilter&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Ingress&lt;/td&gt;
&lt;td&gt;Listener&lt;/td&gt;
&lt;td&gt;Gateway&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Service&lt;/td&gt;
&lt;td&gt;Cluster&lt;/td&gt;
&lt;td&gt;ServiceEntry&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;核心观点&#34;&gt;核心观点&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Kubernetes 的本质是应用生命周期管理，具体来说就是部署和管理（伸缩、自动恢复、发布）。&lt;/li&gt;
&lt;li&gt;Kubernetes 为微服务提供了一个可扩展、高弹性的部署和管理平台。&lt;/li&gt;
&lt;li&gt;服务网格是基于透明代理，通过 sidecar 代理拦截服务之间的流量，然后通过控制平面配置管理它们的行为。&lt;/li&gt;
&lt;li&gt;服务网格将流量管理与 Kubernetes 解耦，不需要 kube-proxy 组件来支持服务网格内的流量；通过提供更接近微服务应用层的抽象来管理服务间的流量、安全性和可观察性。&lt;/li&gt;
&lt;li&gt;xDS 是服务网格的协议标准之一。&lt;/li&gt;
&lt;li&gt;服务网格是 Kubernetes 中服务的一个更高层次的抽象。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;
&lt;p&gt;如果说 Kubernetes 管理的对象是一个 pod，那么服务网格管理的对象就是一个服务，所以用 Kubernetes 管理微服务，然后应用服务网格就可以了。如果你连服务都不想管理，那就用 &lt;a href=&#34;https://knative.dev/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Knative&lt;/a&gt; 这样的无服务器平台，不过这是后话。&lt;/p&gt;
&lt;ul class=&#34;cta-group&#34;&gt;
  
  &lt;li&gt;
    &lt;a href=&#34;../../concepts/&#34;  class=&#34;btn btn-primary px-3 py-3&#34;&gt;下一章&lt;/a&gt;
  &lt;/li&gt;
  
  
&lt;/ul&gt;

</description>
    </item>
    
  </channel>
</rss>
