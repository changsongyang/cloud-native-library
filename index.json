[{"authors":null,"categories":null,"content":"Kubernetes Hardening Guidance（查看英文原版 PDF） 是由美国国家安全局（NSA）于 2021 年 8 月发布的，其中文版《Kubernetes 加固指南》（或译作《Kubernetes 强化指南》），译者 Jimmy Song。\n  《Kubernetes加固指南》封面  许可证 您可以使用署名 - 非商业性使用 - 相同方式共享 4.0 (CC BY-NC-SA 4.0) 协议共享。\n交流群 欢迎加入云原生社区可观测性讨论组（微信群）参与讨论交流，加入前请先填写入群申请问卷后联系 Jimmy Song 入群。\n 开始阅读   ","date":1652889600,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1652965920,"objectID":"58218a6a687ffe5baccf7c93936e20aa","permalink":"https://jimmysong.io/docs/kubernetes-hardening-guidance/","publishdate":"2022-05-19T00:00:00+08:00","relpermalink":"/docs/kubernetes-hardening-guidance/","section":"kubernetes-hardening-guidance","summary":"本指南译自美国国家安全局（NSA）于 2021 年 8 月发布的的 Kubernetes Hardening Guidance。","tags":null,"title":"Kubernetes 加固指南","type":"book"},{"authors":null,"categories":null,"content":"在 Envoy 介绍章节中，你将了解 Envoy 的概况，并通过一个例子了解 Envoy 的构建模块。在本章结束时，你将通过运行 Envoy 的示例配置来了解 Envoy。\n 阅读本章   ","date":1652803200,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1652965920,"objectID":"c2825fc1f107df89f308e1698849459d","permalink":"https://jimmysong.io/docs/envoy-handbook/intro/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/envoy-handbook/intro/","section":"envoy-handbook","summary":"在 Envoy 介绍章节中，你将了解 Envoy 的概况，并通过一个例子了解 Envoy 的构建","tags":null,"title":"Envoy 简介","type":"book"},{"authors":null,"categories":null,"content":"本手册梳理了 Envoy 基础知识，适用于初学者，帮你快速掌握 Envoy 代理。\n  《Envoy基础教程》封面  关于本书 Envoy 是一个开源的边缘和服务代理，专为云原生应用而设计。Envoy 与每个应用程序一起运行，通过提供网络相关的功能，如重试、超时、流量路由和镜像、TLS 终止等，以一种平台无关的方式抽象出网络。由于所有的网络流量都流经 Envoy 代理，因此很容易观察到流量和问题区域，调整性能，并准确定位延迟来源。\n本书为 Tetrate 出品的《Envoy 基础教程》的课程及实验内容，其他测试及考核证书的获取，请访问 Tetrate 学院。\n关于作者 宋净超（Jimmy Song），CNCF Ambassador，云原生社区创始人，个人网站 jimmysong.io。\n许可证 您可以使用署名 - 非商业性使用 - 相同方式共享 4.0 (CC BY-NC-SA 4.0) 协议共享。\n交流群 欢迎加入云原生社区微信讨论群，加入前请先填写入群申请问卷后联系 Jimmy Song 入群。\n 开始阅读   ","date":1652803200,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1652965920,"objectID":"10f2f42a96033280702982d0329ba73a","permalink":"https://jimmysong.io/docs/envoy-handbook/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/envoy-handbook/","section":"envoy-handbook","summary":"本手册梳理了 Envoy 基础知识，适用于初学者，帮你快速掌握 Envoy 代理。","tags":null,"title":"Envoy基础教程","type":"book"},{"authors":null,"categories":null,"content":"Istio 是由 Google、IBM、Lyft 等共同开源的 Service Mesh（服务网格）框架，于2017 年开源。\n  《Istio 基础教程》封面  关于本书 本书的主题包括：\n 服务网格概念解析 控制平面和数据平面的原理 Istio 架构详解 基于 Istio 的自定义扩展 迁移到 Istio 服务网格 构建云原生应用网络  书中部分内容来自 Tetrate 出品的 Istio 基础教程，请访问 Tetrate 学院，解锁全部教程及测试，获得 Tetrate 认证的 Istio 认证。\n关于作者 宋净超（Jimmy Song），CNCF Ambassador，云原生社区创始人，个人网站 jimmysong.io。\n许可证 您可以使用署名 - 非商业性使用 - 相同方式共享 4.0 (CC BY-NC-SA 4.0) 协议共享。\n交流群 欢迎加入云原生社区微信讨论群，加入前请先填写入群申请问卷后联系 Jimmy Song 入群。\n 开始阅读   ","date":1652803200,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1652965920,"objectID":"92c1281b311cfb2840fa65f11e1b2b7e","permalink":"https://jimmysong.io/docs/istio-handbook/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/istio-handbook/","section":"istio-handbook","summary":"云原生应用网络构建指南。","tags":null,"title":"Istio 基础教程","type":"book"},{"authors":null,"categories":null,"content":"Google 有许多通用工程实践，几乎涵盖所有语言和项目。此文档为长期积累的最佳实践，是集体经验的结晶。我们尽可能地将其公之于众，您的组织和开源项目也会从中受益。\n  《谷歌工程实践》封面  当前包含以下文档：\nGoogle 代码审查指南，实则两套指南：\n 代码审查者指南 代码开发者指南  译者序 此仓库翻译自 google/eng-practices，目前为止的主要内容为 Google 总结的如何进行 Code Review（代码审查） 指南，根据原 Github 仓库的标题判断以后会追加更多 Google 工程实践的内容。\n许可证 您可以使用署名 - 非商业性使用 - 相同方式共享 4.0 (CC BY-NC-SA 4.0) 协议共享。\n交流群 欢迎加入云原生社区微信讨论群，加入前请先填写入群申请问卷后联系 Jimmy Song 入群。\n 开始阅读   ","date":1652803200,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1652965920,"objectID":"ac520334b2cbeece15f0e5ad76ae0c0e","permalink":"https://jimmysong.io/docs/eng-practices/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/eng-practices/","section":"eng-practices","summary":"谷歌代码审查实践指南","tags":null,"title":"谷歌工程实践","type":"book"},{"authors":null,"categories":null,"content":"本报告译自 O’Reilly 出品的 The Future of Observablity with OpeTelemetry，作者 Ted Young，译者 Jimmy Song。\n  《OpenTelemetry 可观察性的未来》封面  关于本书 本书内容包括：\n OpenTelemetry 如何满足库作者、应用程序拥有者、运维和响应者的需求 应用程序的不同角色如何围绕 OpenTelemetry 来协同和独立工作 关于在组织中采用和管理 OpenTelemetry 的实用建议  关于作者 Ted Young 是 OpenTelemetry 项目的联合创始人之一。在过去的二十年里，他设计并建立了各种大规模的分布式系统，包括可视化 FX 管道和容器调度系统。他目前在 Lightstep 公司担任开发者教育总监，住在俄勒冈州波特兰的一个小农场里。\n许可证 您可以使用署名 - 非商业性使用 - 相同方式共享 4.0 (CC BY-NC-SA 4.0) 协议共享。\n交流群 欢迎加入云原生社区可观测性讨论组（微信群）参与讨论交流，加入前请先填写入群申请问卷后联系 Jimmy Song 入群。\n 开始阅读   ","date":1651536000,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1652965920,"objectID":"97582bf10d4164c190d079eb4828b3ca","permalink":"https://jimmysong.io/docs/opentelemetry-obervability/","publishdate":"2022-05-03T00:00:00Z","relpermalink":"/docs/opentelemetry-obervability/","section":"opentelemetry-obervability","summary":"本报告译自 O'Reilly 出品的 The Future of Observablity with OpeTelemetry，作者 Ted Young，译者 Jimmy Song。","tags":null,"title":"OpenTelemetry 可观测性的未来","type":"book"},{"authors":null,"categories":null,"content":"欢迎来到云原生宝典——你的一站式云原生知识库。\n 开始阅读   ","date":1651536000,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1652965920,"objectID":"b19faf0f20ec6dc5b1b82bcc463e9659","permalink":"https://jimmysong.io/docs/cloud-native/","publishdate":"2022-05-03T00:00:00Z","relpermalink":"/docs/cloud-native/","section":"cloud-native","summary":"一站式云原生知识库","tags":null,"title":"云原生宝典","type":"book"},{"authors":null,"categories":null,"content":"本章将介绍什么是云原生、云原生应用。\n 阅读本章   ","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1652965920,"objectID":"84f73c250ec51ab0040a2edb5cdec91b","permalink":"https://jimmysong.io/docs/cloud-native/intro/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/docs/cloud-native/intro/","section":"cloud-native","summary":"本章介绍什么是云原生。","tags":null,"title":"关于云原生","type":"book"},{"authors":null,"categories":null,"content":"简介 代码审查是除了代码作者之外，其他人检查代码的过程。\nGoogle 通过 Code Review 来维护代码和产品质量。\n此文档是 Google Code Review 流程和政策的规范说明。\n此页面是我们进行 Code Review 流程的概述。本指南还有另外两套文档：\n 如何进行 Code Review：针对代码审查者的详细指南。 代码开发者指南：针对 CL 开发者的的详细指南。  代码审查者应该关注哪些方面？ 代码审查时应该关注以下方面：\n 设计：代码是否经过精心设计并适合您的系统？ 功能：代码的行为是否与作者的意图相同？代码是否可以正常响应用户的行为？ 复杂度：代码能更简单吗？将来其他开发人员能轻松理解并使用此代码吗？ 测试：代码是否具有正确且设计良好的自动化测试？ 命名：开发人员是否为变量、类、方法等选择了明确的名称？ 注释：注释是否清晰有用？ 风格：代码是否遵守了风格指南？ 文档：开发人员是否同时更新了相关文档？  参阅 如何进行 Code Review 获取更多资料。\n选择最合适审查者 一般而言，您希望找到能在合理的时间内回复您的评论的最合适的审查者。\n最合适的审查者应该是能彻底了解和审查您代码的人。他们通常是代码的所有者，可能是 OWNERS 文件中的人，也可能不是。有时 CL 的不同部分可能需要不同的人审查。\n如果您找到了理想的审查者但他们又没空，那您也至少要抄送他们。\n面对面审查 如果您与有资格做代码审查的人一起结对编程了一段代码，那么该代码将被视为已审查。\n您还可以进行面对面的代码审查，审查者提问，CL 的开发人员作答。\n术语 文档中使用了 Google 内部术语，在此为外部读者澄清：\n CL：代表 “变更列表（Change List）”，表示已提交到版本控制或正在进行代码审查的自包含更改。有的组织会将其称为 “变更（change）” 或 “补丁（patch）”。 LGTM：意思是 “我觉得不错（Looks Good to Me）”。这是批准 CL 时代码审查者所说的。  参考  如何进行 Code Review：针对代码审查者的详细指南。 代码开发者指南：针对 CL 开发者的的详细指南。  ","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1652965920,"objectID":"7604ca651c03ef6c0cb7acfe4ef65b5d","permalink":"https://jimmysong.io/docs/eng-practices/review/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/docs/eng-practices/review/","section":"eng-practices","summary":"简介 代码审查是除了代码作者之外，其他人检查代码的过程。 Google 通过","tags":null,"title":"代码审查指南","type":"book"},{"authors":null,"categories":null,"content":"在 HCM 章节中，我们将对 HTTP 连接管理器过滤器进行扩展。我们将学习过滤器的排序，以及 HTTP 路由和匹配如何工作。我们将向你展示如何分割流量、操作 Header 信息、配置超时、实现重试、请求镜像和速率限制。\n在本章结束时，你将对 HCM 过滤器有一个很好的理解，以及如何路由和拆分 HTTP 流量，操纵 Header 等等。\n 阅读本章   ","date":1652803200,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1652965920,"objectID":"1b089774cac533a634aea664bf5f811a","permalink":"https://jimmysong.io/docs/envoy-handbook/hcm/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/envoy-handbook/hcm/","section":"envoy-handbook","summary":"在 HCM 章节中，我们将对 HTTP 连接管理器过滤器进行扩展。我们将学习过","tags":null,"title":"HTTP 连接管理器","type":"book"},{"authors":null,"categories":null,"content":"本章将介绍什么是云原生、云原生应用。\n","date":1651536000,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1652965920,"objectID":"8bcfde77956a598c3ac9b7be1fa96051","permalink":"https://jimmysong.io/docs/cloud-native/kubernetes/","publishdate":"2022-05-03T00:00:00Z","relpermalink":"/docs/cloud-native/kubernetes/","section":"cloud-native","summary":"关于Kubernetes。","tags":null,"title":"Kubernetes","type":"book"},{"authors":null,"categories":null,"content":"本章将介绍服务网格。\n 阅读本章   ","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1652965920,"objectID":"3387e00a683fd66a6132f4c422c9314e","permalink":"https://jimmysong.io/docs/cloud-native/service-mesh/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/docs/cloud-native/service-mesh/","section":"cloud-native","summary":"本章介绍服务网格。","tags":null,"title":"服务网格","type":"book"},{"authors":null,"categories":null,"content":"在本章中，我们将学习集群以及如何管理它们。在 Envoy 的集群配置部分，我们可以配置一些功能，如负载均衡、健康检查、连接池、异常点检测等。\n在本章结束时，你将了解集群和端点是如何工作的，以及如何配置负载均衡策略、异常点检测和断路。\n 阅读本章   ","date":1652803200,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1652965920,"objectID":"296b19cc52b86725568e4f0e38a83925","permalink":"https://jimmysong.io/docs/envoy-handbook/cluster/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/envoy-handbook/cluster/","section":"envoy-handbook","summary":"在本章中，我们将学习集群以及如何管理它们。在 Envoy 的集群配置部分","tags":null,"title":"集群","type":"book"},{"authors":null,"categories":null,"content":"本节页面的内容为开发人员进行代码审查的最佳实践。这些指南可帮助您更快地完成审核并获得更高质量的结果。您不必全部阅读它们，但它们适用于每个 Google 开发人员，并且许阅读全文通常会很有帮助。\n 写好 CL 描述 小型 CL 如何处理审查者的评论  另请参阅代码审查者指南，它为代码审阅者提供了详细的指导。\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1652965920,"objectID":"1bc0acdabd47c346b08c8e5dabfa9dd3","permalink":"https://jimmysong.io/docs/eng-practices/review/developer/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/docs/eng-practices/review/developer/","section":"eng-practices","summary":"本节页面的内容为开发人员进行代码审查的最佳实践。这些指南可帮","tags":null,"title":"开发者指南","type":"book"},{"authors":null,"categories":null,"content":"本节是基于过往经验编写的 Code Review 最佳方式建议。其中分为了很多独立的部分，共同组成完整的文档。虽然您不必阅读文档，但通读一遍会对您自己和团队很有帮助。\n Code Review 标准 Code Review 要点 查看 CL 的步骤 Code Review 速度 如何撰写 Code Review 评论 处理 Code Review 中的抵触  另请参阅代码开发者指南，该指南为正在进行 Code Review 的开发开发者提供详细指导。\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1652965920,"objectID":"58a2fff07384fdf4647c4de2f3ee82c0","permalink":"https://jimmysong.io/docs/eng-practices/review/reviewer/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/docs/eng-practices/review/reviewer/","section":"eng-practices","summary":"本节是基于过往经验编写的 Code Review 最佳方式建议。其中分为了很多独立","tags":null,"title":"审查者指南","type":"book"},{"authors":null,"categories":null,"content":"在本章中，我们将学习如何使用动态配置来配置 Envoy 代理。到现在为止，我们一直在使用静态配置。本章将教我们如何在运行时从文件系统或通过网络使用发现服务为单个资源提供配置。\n在本章结束时，你将了解静态和动态配置之间的区别。\n 阅读本章   ","date":1652803200,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1652965920,"objectID":"fd749a86315e5912d690d0630631ead5","permalink":"https://jimmysong.io/docs/envoy-handbook/dynamic-config/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/envoy-handbook/dynamic-config/","section":"envoy-handbook","summary":"在本章中，我们将学习如何使用动态配置来配置 Envoy 代理。到现在为止","tags":null,"title":"动态配置","type":"book"},{"authors":null,"categories":null,"content":"本章将介绍什么是云原生、云原生应用。\n 阅读本章   ","date":1651536000,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1652965920,"objectID":"d443b3d261cf296c83fcf85f14315f92","permalink":"https://jimmysong.io/docs/cloud-native/community/","publishdate":"2022-05-03T00:00:00Z","relpermalink":"/docs/cloud-native/community/","section":"cloud-native","summary":"社区","tags":null,"title":"社区","type":"book"},{"authors":null,"categories":null,"content":"在监听器子系统章节中，我们将学习 Envoy 代理的监听器子系统。我们将介绍过滤器、过滤器链匹配、以及不同的监听器过滤器。\n在本章节结束时，你将了解监听器子系统的不同部分，并知道过滤器和过滤器链匹配是如何工作的。\n 阅读本章   ","date":1652803200,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1652965920,"objectID":"6e361f02528b59f11bddcac813ae4694","permalink":"https://jimmysong.io/docs/envoy-handbook/listener/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/envoy-handbook/listener/","section":"envoy-handbook","summary":"在监听器子系统章节中，我们将学习 Envoy 代理的监听器子系统。我们将","tags":null,"title":"监听器子系统","type":"book"},{"authors":null,"categories":null,"content":"在日志章节中，我们将学习 Envoy 中不同类型的日志和方法。\n无论是作为流量网关还是作为服务网格中的 Sidecar，Envoy 都处于独特的地位，可以揭示你的网络中正在发生的事情。日志记录是了解系统状态的重要方式，无论是分析、审计还是故障排除。日志记录也有一个数量问题，有可能会泄露秘密。\n在本章结束时，你将了解 Envoy 中存在哪些日志选项，包括如何对日志进行结构化和过滤，以及它们可以被写到哪里。\n 阅读本章   ","date":1652803200,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1652965920,"objectID":"103c5755101a3deaa4f99e94bfc899e4","permalink":"https://jimmysong.io/docs/envoy-handbook/logging/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/envoy-handbook/logging/","section":"envoy-handbook","summary":"在日志章节中，我们将学习 Envoy 中不同类型的日志和方法。 无论是作为","tags":null,"title":"日志","type":"book"},{"authors":null,"categories":null,"content":"在管理接口章节中，我们将学习 Envoy 所暴露的管理接口，以及我们可以用来检索配置和统计的不同端点，以及执行其他管理任务。\n在本章结束时，你将了解如何启用管理接口以及我们可以通过它执行的不同任务。\n 阅读本章   ","date":1652803200,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1652965920,"objectID":"3b682f5af5943ff43a4c68712eed2aad","permalink":"https://jimmysong.io/docs/envoy-handbook/admin-interface/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/envoy-handbook/admin-interface/","section":"envoy-handbook","summary":"在管理接口章节中，我们将学习 Envoy 所暴露的管理接口，以及我们可以","tags":null,"title":"管理接口","type":"book"},{"authors":null,"categories":null,"content":"在本章节中，我们将了解扩展 Envoy 的不同方法。\n我们将更详细地介绍使用 Lua 和 Wasm 过滤器来扩展 Envoy 的功能。\n在本章结束时，你将了解扩展 Envoy 的不同方法以及如何使用 Lua 和 Wasm 过滤器。\n 阅读本章   ","date":1652803200,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1652965920,"objectID":"a9793b4ee69cc483677360c87c7294f5","permalink":"https://jimmysong.io/docs/envoy-handbook/extending-envoy/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/envoy-handbook/extending-envoy/","section":"envoy-handbook","summary":"在本章节中，我们将了解扩展 Envoy 的不同方法。 我们将更详细地介绍使","tags":null,"title":"扩展 Envoy","type":"book"},{"authors":null,"categories":null,"content":"本章将为你介绍什么是服务网格，为什么服务网格对于云原生应用十分重要。\n 阅读本章   ","date":1652803200,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1652965920,"objectID":"79a81b97679d5df5cb1a7d6695364b15","permalink":"https://jimmysong.io/docs/istio-handbook/intro/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/istio-handbook/intro/","section":"istio-handbook","summary":"本章将为你介绍什么是服务网格，为什么服务网格对于云原生应用十","tags":null,"title":"服务网格概述","type":"book"},{"authors":null,"categories":null,"content":"在这一章中，你将了解：\n 服务网格的基本概念 Istio 的架构 stio 中的主要部署模式——Sidecar 模式 透明流量拦截——Istio 的核心设计思想   阅读本章   ","date":1652803200,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1652965920,"objectID":"fc8f2b0ef3525f2a6f9b15ae969b6814","permalink":"https://jimmysong.io/docs/istio-handbook/concepts/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/istio-handbook/concepts/","section":"istio-handbook","summary":"在这一章中，你将了解： 服务网格的基本概念 Istio 的架构 stio 中的主要部","tags":null,"title":"概念原理","type":"book"},{"authors":null,"categories":null,"content":"Istio 中的数据平面默认使用的是 Envoy 代理，你也可以根据 xDS 构建其他数据平面，例如蚂蚁开源的 MOSN。\n 阅读本章   ","date":1652803200,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1652965920,"objectID":"5eb5db074391b0ac2bdccf2d024c3938","permalink":"https://jimmysong.io/docs/istio-handbook/data-plane/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/istio-handbook/data-plane/","section":"istio-handbook","summary":"Istio 中的数据平面默认使用的是 Envoy 代理，你也可以根据 xDS 构建其他数据","tags":null,"title":"数据平面","type":"book"},{"authors":null,"categories":null,"content":"有两种方法可以在单个 Kubernetes 集群上安装 Istio：\n 使用 Istioctl（istioctl） 使用 Istio Operator  在本章中，我们将使用 Istio Operator 在一个 Kubernetes 集群上安装 Istio。\n使用 Istioctl 安装 Istioctl 是一个命令行工具，我们可以用它来安装和定制 Istio 的安装。使用该命令行工具，我们生成一个包含所有 Istio 资源的 YAML 文件，然后将其部署到 Kubernetes 集群上。\n使用 Istio Operator 安装 与 istioctl 相比，Istio Operator 安装的优势在于，我们不需要手动升级 Istio。相反，我们可以部署 Istio Operator，为你管理安装。我们通过更新一个自定义资源来控制 Operator，而操作员则为你应用配置变化。\n生产部署情况如何？ 在决定 Istio 的生产部署模式时，还有一些额外的考虑因素需要牢记。我们可以配置 Istio 在不同的部署模型中运行 —— 可能跨越多个集群和网络，并使用多个控制平面。我们将在高级功能模块中了解其他部署模式、多集群安装以及在虚拟机上运行工作负载。\n平台安装指南 Istio 可以安装在不同的 Kubernetes 平台上。关于特定云供应商的最新安装指南，请参考平台安装文档。\n 阅读本章   ","date":1652803200,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1652965920,"objectID":"99189028192986ccca087b8bc27d78cc","permalink":"https://jimmysong.io/docs/istio-handbook/setup/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/istio-handbook/setup/","section":"istio-handbook","summary":"有两种方法可以在单个 Kubernetes 集群上安装 Istio： 使用 Istioc","tags":null,"title":"安装","type":"book"},{"authors":null,"categories":null,"content":"在本章中，我们将开始使用 Istio 服务网格在服务之间进行流量路由。我们将学习如何设置一个 Ingress 资源以允许流量进入我们的集群，以及一个 Egress 资源以使流量流出集群。\n使用流量路由，我们将学习如何部署新版本的服务，并在已发布的生产版本的服务旁边运行，而不干扰生产流量。随着两个服务版本的部署，我们将逐步发布（金丝雀发布）新版本，并开始将一定比例的传入流量路由到最新版本。\n流量管理是 Isito 中的最基础功能，使用 Istio 的流量管理模型，本质上是将流量与基础设施扩容解耦，让运维人员可以通过 Pilot 指定流量遵循什么规则，而不是指定哪些 pod/VM 应该接收流量——Pilot 和智能 Envoy 代理会帮我们搞定。\n所谓流量管理是指：\n 控制服务之间的路由：通过在 VirtualService 中的规则条件匹配来设置路由，可以在服务间拆分流量。 控制路由上流量的行为：设定好路由之后，就可以在路由上指定超时和重试机制，例如超时时间、重试次数等；做错误注入、设置断路器等。可以由 VirtualService 和 DestinationRule 共同完成。 显式地向网格中注册服务：显示地引入 Service Mesh 内部或外部的服务，纳入服务网格管理。由 ServiceEntry 实现。 控制网格边缘的南北向流量：为了管理进入 Istio service mesh 的南北向入口流量，需要创建 Gateway 对象并与 VirtualService 绑定。   阅读本章   ","date":1652803200,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1652965920,"objectID":"99957e88a4373afd6d712d117108ae0b","permalink":"https://jimmysong.io/docs/istio-handbook/traffic-management/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/istio-handbook/traffic-management/","section":"istio-handbook","summary":"在本章中，我们将开始使用 Istio 服务网格在服务之间进行流量路由。我","tags":null,"title":"流量管理","type":"book"},{"authors":null,"categories":null,"content":"流量管理配置一节描述如何配置 Istio 服务网格中与流量管理相关的一些资源对象。\n 阅读本章   ","date":1652803200,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1652965920,"objectID":"96af5f978b4c26f2c23bb7d10d37d35b","permalink":"https://jimmysong.io/docs/istio-handbook/config-networking/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/istio-handbook/config-networking/","section":"istio-handbook","summary":"流量管理配置一节描述如何配置 Istio 服务网格中与流量管理相关的一些","tags":null,"title":"流量管理配置","type":"book"},{"authors":null,"categories":null,"content":"在本章中，我们将学习一些监控（Prometheus）、追踪（Zipkin）和数据可视化工具（Grafana）。\n什么是可观察性？ 由于采用了 sidecar 部署模式，即 Envoy 代理运行在应用实例旁边并拦截流量，这些代理也收集指标。\nEnvoy 代理收集的指标可以帮助我们获得系统状态的可见性。获得系统的这种可见性是至关重要的，因为我们需要了解正在发生的事情，并授权运维人员对应用程序进行故障排除、维护和优化。\nIstio 生成三种类型的遥测数据，为网格中的服务提供可观察性：\n 指标度量（Metric） 分布式跟踪 访问日志  指标度量 Istio 基于四个黄金信号生成指标：延迟、流量、错误和饱和度。\n延迟表示服务一个请求所需的时间。这个指标应该分成成功请求（如 HTTP 200）和失败请求（如 HTTP 500）的延迟。\n流量是衡量对系统的需求有多大，它是以系统的具体指标来衡量的。例如，每秒的 HTTP 请求，或并发会话，每秒的检索量，等等。\n错误用来衡量请求失败的比率（例如 HTTP 500）。\n饱和度衡量一个服务中最紧张的资源有多满。例如，线程池的利用率。\n这些指标是在不同的层面上收集的，首先是最细的，即 Envoy 代理层面，然后是服务层面和控制面的指标。\n代理级指标 生成指标的一个关键角色是 Envoy，它生成了一套关于所有通过代理的流量的丰富指标。使用 Envoy 生成的指标，我们可以以最低的粒度来监控服务网格，例如 Envoy 代理中的 inidivdual 监听器和集群的指标。\n作为网格运维人员，我们有能力控制在每个2工作负载实例中生成和收集哪些 Envoy 指标。\n下面是几个代理级指标的例子。\nenvoy_cluster_internal_upstream_rq{response_code_class=\u0026#34;2xx\u0026#34;,cluster_name=\u0026#34;xds-grpc\u0026#34;} 7163 envoy_cluster_upstream_rq_completed{cluster_name=\u0026#34;xds-grpc\u0026#34;} 7164 envoy_cluster_ssl_connection_error{cluster_name=\u0026#34;xds-grpc\u0026#34;} 0 envoy_cluster_lb_subsets_removed{cluster_name=\u0026#34;xds-grpc\u0026#34;} 0 envoy_cluster_internal_upstream_rq{response_code=\u0026#34;503\u0026#34;,cluster_name=\u0026#34;xds-grpc\u0026#34;} 1  注意你可以从每个 Envoy 代理实例的 /stats 端点查看代理级指标。\n 服务级指标 服务级别的指标涵盖了我们前面提到的四个黄金信号。这些指标使我们能够监控服务与服务之间的通信。此外，Istio 还提供了一组仪表盘，我们可以根据这些指标来监控服务行为。\n就像代理级别的指标一样，运营商可以自定义收集哪些服务级别的指标。\n默认情况下，Istio 的标准指标集会被导出到 Prometheus。\n下面是几个服务级指标的例子。\n控制平面度量 Istio 也会发射控制平面指标，可以帮助监控 Istio 的控制平面和行为，而不是用户服务。\n输出的控制平面指标的完整列表可以在这里找到。\n控制平面指标包括冲突的入站/出站监听器的数量、没有实例的集群数量、被拒绝或被忽略的配置等指标。\n 阅读本章   ","date":1652803200,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1652965920,"objectID":"d59534ab0ad37757da1a12b6c8d7132e","permalink":"https://jimmysong.io/docs/istio-handbook/observability/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/istio-handbook/observability/","section":"istio-handbook","summary":"在本章中，我们将学习一些监控（Prometheus）、追踪（","tags":null,"title":"可观察性","type":"book"},{"authors":null,"categories":null,"content":"在本章中，我们将学习 Istio 的安全功能。具体来说，就是认证和授权策略、安全命名和身份。\n在 Istio 中，有多个组件参与提供安全功能：\n 用于管理钥匙和证书的证书颁发机构（CA）。 Sidecar 和周边代理：实现客户端和服务器之间的安全通信，它们作为政策执行点（Policy Enforcement Point，简称PEP）工作 Envoy 代理扩展：管理遥测和审计 配置 API 服务器：分发认证、授权策略和安全命名信息   阅读本章   ","date":1652803200,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1652965920,"objectID":"e3d84b03093329eb571bee3fdce533cd","permalink":"https://jimmysong.io/docs/istio-handbook/security/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/istio-handbook/security/","section":"istio-handbook","summary":"在本章中，我们将学习 Istio 的安全功能。具体来说，就是认证和授权策","tags":null,"title":"安全","type":"book"},{"authors":null,"categories":null,"content":"安全一节描述如何配置 Istio mesh 中与安全性相关的配置，其中包括：\n AuthorizationPolicy RequestAuthentication PeerAuthentication JWTRule   阅读本章   ","date":1652803200,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1652965920,"objectID":"e2b3cd936970c5ab86532e950c209d63","permalink":"https://jimmysong.io/docs/istio-handbook/config-security/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/istio-handbook/config-security/","section":"istio-handbook","summary":"安全一节描述如何配置 Istio mesh 中与安全性相关的配置，其中包括： AuthorizationPolicy RequestAuthentication","tags":null,"title":"安全配置","type":"book"},{"authors":null,"categories":null,"content":"在本章中，我们将了解在多个集群上安装 Istio 的不同方法，以及如何将运行在虚拟机上的工作负载纳入服务网格。\n当决定在多集群场景下运行 Istio 时，有多种组合需要考虑。在高层次上，我们需要决定以下几点：\n 单个集群或多个集群 单个网络或多个网络 单个控制平面或多个控制平面 单个网格或多个网格  上述模式的任何组合都是可能的，然而，并非所有的模式都有意义。在本章中，我们将重点讨论涉及多个集群的场景。\n 阅读本章   ","date":1652803200,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1652965920,"objectID":"9b7257e7af4164679c5001bbea6c6bb9","permalink":"https://jimmysong.io/docs/istio-handbook/advanced/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/istio-handbook/advanced/","section":"istio-handbook","summary":"在本章中，我们将了解在多个集群上安装 Istio 的不同方法，以及如何将","tags":null,"title":"高级功能","type":"book"},{"authors":null,"categories":null,"content":"本章介绍了在使用 Istio 时可能遇到的问题的几种排查方法。\n 阅读本章   ","date":1652803200,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1652965920,"objectID":"b2a44b9b8831dd6950a702a195be5eff","permalink":"https://jimmysong.io/docs/istio-handbook/troubleshooting/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/istio-handbook/troubleshooting/","section":"istio-handbook","summary":"本章介绍了在使用 Istio 时可能遇到的问题的几种排查方法。 阅读本章","tags":null,"title":"问题排查","type":"book"},{"authors":null,"categories":null,"content":"Istio 服务网格自 2017 年 5 月开源以来，已围绕其周边诞生了诸多开源项目，这些项目有的是对 Istio 本身的扩展，有的是可以与 Istio 集成，还有的是 Istio 周边的开源工具。\n本章将分门别类为读者介绍 Istio 生态中的开源项目，以帮助读者对 Istio 的开源生态有个更直观的了解，另一方面也可以作为读者个人的选型参考。\nIstio 周边开源项目 下表中列举 Istio 生态中的开源项目，按照开源时间排序。\n   项目名称 开源时间 类别 描述 主导公司 Star 数量 与 Istio 的关系     Envoy 2016年 9 月 网络代理 云原生高性能边缘/中间服务代理 Lyft 18300 默认的数据平面   Istio 2017 年 5 月 服务网格 连接、保护、控制和观察服务。 Google 28400 控制平面   Emissary Gateway 2018 年 2 月 网关 用于微服务的 Kubernetes 原生 API 网关，基于 Envoy 构建 Ambassador 3500 可连接 Istio   APISIX 2019 年 6 月 网关 云原生 API 网关 API7 7400 可作为 Istio 的数据平面运行也可以单独作为网关   MOSN 2019 年 12 月 代理 云原生边缘网关及代理 蚂蚁 3400 可作为 Istio 数据平面   Slime 2021 年 1月 扩展 基于 Istio 的智能服务网格管理器 网易 204 为 Istio 增加一个管理平面   GetMesh 2021 年 2 月 工具 Istio 集成和命令行管理工具 Tetrate 91 实用工具，可用于 Istio 多版本管理   Aeraki 2021 年 3 月 扩展 管理 Istio 的任何七层负载 腾讯 280 扩展多协议支持   Layotto 2021 年 6 月 运行时 云原生应用运行时 蚂蚁 325 可以作为 Istio 的数据平面   Hango Gateway 2021 年 8 月 网关 基于 Envoy 和 Istio 构建的 API 网关 网易 187 可与 Istio 集成    注意\n 开源时间以 GitHub 仓库创建时间为准 Star 数量统计截止时间为 2021年11月11 日  参考  Building Ambassador, an Open Source API Gateway on Kubernetes and Envoy - blog.ambassador.io 蚂蚁开源多运行时项目 Layotto 简介 - cloudnative.to Slime：让 Istio 服务网格变得更加高效与智能 - cloudnative.to 使用 Aeraki 在 Isito 中支持 Dubbo、 Thrift、 Redis，以及任何七层协议 - cloudnative.to Apache APISIX 的全流量 API 网关统筹集群流量 - cloudnative.to GetMesh 官网 - istio.tetratelabs.io MOSN 官网 - mosn.io Istio 官网 - istio.io Envoy 官网 - envoyproxy.io   阅读本章   ","date":1652803200,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1652965920,"objectID":"77ec59b7b9951623f41ab955b66b150f","permalink":"https://jimmysong.io/docs/istio-handbook/ecosystem/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/istio-handbook/ecosystem/","section":"istio-handbook","summary":"Istio 服务网格自 2017 年 5 月开源以来，已围绕其周边诞生了诸多开源项目","tags":null,"title":"Istio 生态","type":"book"},{"authors":null,"categories":null,"content":"在本章中，我们将部署名为 Online Boutique 的微服务演示应用程序，并尝试不同的Istio功能。\nOnline Boutique 是一个云原生微服务演示应用程序。Online Boutique 由一个 10 层的微服务应用组成。该应用是一个基于 Web 的电子商务应用，用户可以浏览商品，将其添加到购物车，并购买商品。\n 阅读本章   ","date":1652803200,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1652965920,"objectID":"17f4b296fe262cfe036e1527bcaf58c3","permalink":"https://jimmysong.io/docs/istio-handbook/practice/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/istio-handbook/practice/","section":"istio-handbook","summary":"在本章中，我们将部署名为 Online Boutique 的微服务演示应用程序，并尝试不同","tags":null,"title":"实战案例","type":"book"},{"authors":null,"categories":null,"content":"自 2017 年以来，笔者累计翻译和撰写了十几本电子书和实体书，本站中列出了所有书目，其中电子书可以直接通过导航跳转阅读。\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1652965920,"objectID":"edca7cbb245146be752b62ec0ec546fa","permalink":"https://jimmysong.io/docs/book/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/docs/book/","section":"book","summary":"自 2017 年以来，笔者累计翻译和撰写了十几本电子书和实体书，本站中","tags":null,"title":"书目列表","type":"book"},{"authors":null,"categories":null,"content":"文件变更历史\n英文版\n   日期 版本 描述     2021 年 8 月 1.0 首次发布    中文版\n   日期 版本 描述     2021 年 8 月 8 日 1.0 首次发布    担保和认可的免责声明\n本文件中的信息和意见是 “按原样” 提供的，没有任何保证或担保。本文件以商品名称、商标、制造商或其他方式提及任何具体的商业产品、程序或服务，并不一定构成或暗示美国政府对其的认可、推荐或青睐，而且本指南不得用于广告或产品代言的目的。\n关于中文版\n中文版为 Jimmy Song 个人翻译，翻译过程中完全遵照原版，未做任何删减。其本人与本书的原作者没有任何组织或利益上的联系，翻译本书仅为交流学习之用。\n商标认可\n Kubernetes 是 Linux 基金会的注册商标。 SELinux 是美国国家安全局的注册商标。 AppArmor 是 SUSE LLC 的注册商标。 Windows 和 Hyper-V 是微软公司的注册商标。 ETCD 是 CoreOS, Inc. 的注册商标。 Syslog-ng 是 One Identity Software International Designated Activity 公司的注册商标。 Prometheus 是 Linux 基金会的注册商标。 Grafana 是 Raintank, Inc.dba Grafana Labs 的注册商标。 Elasticsearch 和 ELK Stack 是 Elasticsearch B.V 的注册商标。  版权确认\n本文件中的信息、例子和数字基于 Kubernetes 作者的 Kubernetes 文档，以知识共享署名 4.0 许可方式发布。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"41195e39476d86cfb31a2cd9ed789d88","permalink":"https://jimmysong.io/docs/kubernetes-hardening-guidance/notices-and-hitory/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/kubernetes-hardening-guidance/notices-and-hitory/","section":"kubernetes-hardening-guidance","summary":"文件变更历史 英文版 日期 版本 描述 2021 年 8 月 1.0 首次发布 中文版 日期 版","tags":null,"title":"通知和历史","type":"book"},{"authors":null,"categories":null,"content":"众所周知，Kubernetes 是 Google 于 2014 年 6 月基于其内部使用的 Borg 系统开源出来的容器编排调度引擎。其实从 2000 年开始，Google 就开始基于容器研发三个容器管理系统，分别是 Borg、Omega 和 Kubernetes。这篇由 Google 工程师 Brendan Burns、Brian Grant、David Oppenheimer、Eric Brewer 和 John Wilkes 几人在 2016 年发表的《Borg, Omega, and Kubernetes》论文里，阐述了 Google 从 Borg 到 Kubernetes 这个旅程中所获得知识和经验教训。\nBorg、Omega 和 Kubernetes Google 从 2000 年初就开始使用容器（Linux 容器）系统，Google 开发出来的第一个统一的容器管理系统在内部称之为 “Borg”，用来管理长时间运行的生产服务和批处理服务。由于 Borg 的规模、功能的广泛性和超高的稳定性，一直到现在 Borg 在 Google 内部依然是主要的容器管理系统。\nGoogle 的第二套容器管理系统叫做 Omega，作为 Borg 的延伸，它的出现是出于提升 Borg 生态系统软件工程的愿望。Omega 应用到了很多在 Borg 内已经被认证的成功的模式，但是是从头开始来搭建以期更为一致的构架。由于越来越多的应用被开发并运行在 Borg 上，Google 开发了一个广泛的工具和服务的生态系统。它被应用到了很多在 Borg 内已经被认证的成功的模式，但是是从头开始来搭建以期更为一致的构架。这些系统提供了配置和更新 job 的机制，能够预测资源需求，动态地对在运行中的程序推送配置文件、服务发现、负载均衡、自动扩容、机器生命周期管理、额度管理等。许多 Omega 的创新（包括多个调度器）都被收录进了 Borg。\nGoogle 的第三套容器管理系统就是我们所熟知的 Kubernetes，它是针对在 Google 外部的对 Linux 容器感兴趣的开发者以及 Google 在公有云底层商业增长的考虑而研发的。和 Borg、Omega 完全是谷歌内部系统相比，Kubernetes 是开源的。像 Omega 一样，Kubernetes 在其核心有一个被分享的持久存储，有组件来检测相关 object 的变化。跟 Omega 不同的是，Omega 把存储直接暴露给信任的控制平面的组件，而在 Kubernete 中，提供了完全由特定领域更高层面的版本控制、认证、语义、策略的 REST API 接口，以服务更多的用户。更重要的是，Kubernetes 是由一群底层开发能力更强的开发者开发的，他们主要的设计目标是用更容易的方法去部署和管理复杂的分布式系统，同时仍能从容器提升的效率中受益。\n2014 年 Kubernetes 正式开源，2015 年被作为初创项目贡献给了云原生计算基金会（CNCF），从此开启了 Kubernetes 及云原生化的大潮。\n参考  Borg, Omega, and Kubernetes: Lessons learned from three container-management systems over a decade - queue.acm.org Borg、Omega 和 Kubernetes：谷歌十几年来从这三个容器管理系统中得到的经验教训 - dockone.io  ","date":1651532400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"91d878fe08db7eaf59677aac500bc0fe","permalink":"https://jimmysong.io/docs/cloud-native/kubernetes/history/","publishdate":"2022-05-03T00:00:00+01:00","relpermalink":"/docs/cloud-native/kubernetes/history/","section":"cloud-native","summary":"众所周知，Kubernetes 是 Google 于 2014 年 6 月基于其内部使用的","tags":null,"title":"Kubernetes 的历史","type":"book"},{"authors":null,"categories":null,"content":"Service Mesh 又译作 “服务网格”，作为服务间通信的基础设施层。Buoyant 公司的 CEO Willian Morgan 在他的这篇文章 WHAT’S A SERVICE MESH? AND WHY DO I NEED ONE? 中解释了什么是 Service Mesh，为什么云原生应用需要 Service Mesh。\n服务网格是用于处理服务间通信的专用基础设施层。它负责通过包含现代云原生应用程序的复杂服务拓扑来可靠地传递请求。实际上，服务网格通常通过一组轻量级网络代理来实现，这些代理与应用程序代码一起部署，而不需要感知应用程序本身。—— Willian Morgan Buoyant CEO\n服务网格（Service Mesh）这个术语通常用于描述构成这些应用程序的微服务网络以及应用之间的交互。随着规模和复杂性的增长，服务网格越来越难以理解和管理。它的需求包括服务发现、负载均衡、故障恢复、指标收集和监控以及通常更加复杂的运维需求，例如 A/B 测试、金丝雀发布、限流、访问控制和端到端认证等。\n服务网格的特点 服务网格有如下几个特点：\n 应用程序间通讯的中间层 轻量级网络代理 应用程序无感知 解耦应用程序的重试 / 超时、监控、追踪和服务发现  目前两款流行的服务网格开源软件 Linkerd 和 Istio 都可以直接在 Kubernetes 中集成，其中 Linkerd 是 CNCF 成员项目，并在 2021 年 7 月毕业。Istio 在 2018 年 7 月 31 日宣布 1.0，并在 2020 年 7 月将 商标捐献给 Open Usage Commons。\n理解服务网格 如果用一句话来解释什么是服务网格，可以将它比作是应用程序或者说微服务间的 TCP/IP，负责服务之间的网络调用、限流、熔断和监控。对于编写应用程序来说一般无须关心 TCP/IP 这一层（比如通过 HTTP 协议的 RESTful 应用），同样使用服务网格也就无须关系服务之间的那些原来是通过应用程序或者其他框架实现的事情，比如 Spring Cloud、OSS，现在只要交给服务网格就可以了。\nPhil Calçado 在他的这篇博客 Pattern: Service Mesh 中详细解释了服务网格的来龙去脉：\n 从最原始的主机之间直接使用网线相连 网络层的出现 集成到应用程序内部的控制流 分解到应用程序外部的控制流 应用程序的中集成服务发现和断路器 出现了专门用于服务发现和断路器的软件包 / 库，如 Twitter 的 Finagle 和 Facebook 的 Proxygen，这时候还是集成在应用程序内部 出现了专门用于服务发现和断路器的开源软件，如 Netflix OSS、Airbnb 的 synapse 和 nerve 最后作为微服务的中间层服务网格出现  服务网格的架构如下图所示：\n   服务网格架构图  图片来自：Pattern: Service Mesh\n服务网格作为 sidecar 运行，对应用程序来说是透明，所有应用程序间的流量都会通过它，所以对应用程序流量的控制都可以在 serivce mesh 中实现。\n服务网格如何工作？ 下面以 Istio 为例讲解服务网格如何在 Kubernetes 中工作。\n Istio 将服务请求路由到目的地址，根据中的参数判断是到生产环境、测试环境还是 staging 环境中的服务（服务可能同时部署在这三个环境中），是路由到本地环境还是公有云环境？所有的这些路由信息可以动态配置，可以是全局配置也可以为某些服务单独配置。 当 Istio 确认了目的地址后，将流量发送到相应服务发现端点，在 Kubernetes 中是 service，然后 service 会将服务转发给后端的实例。 Istio 根据它观测到最近请求的延迟时间，选择出所有应用程序的实例中响应最快的实例。 Istio 将请求发送给该实例，同时记录响应类型和延迟数据。 如果该实例挂了、不响应了或者进程不工作了，Istio 将把请求发送到其他实例上重试。 如果该实例持续返回 error，Istio 会将该实例从负载均衡池中移除，稍后再周期性得重试。 如果请求的截止时间已过，Istio 主动失败该请求，而不是再次尝试添加负载。 Istio 以 metric 和分布式追踪的形式捕获上述行为的各个方面，这些追踪信息将发送到集中 metric 系统。  为何使用服务网格？ 服务网格并没有给我们带来新功能，它是用于解决其他工具已经解决过的问题，只不过这次是在云原生的 Kubernetes 环境下的实现。\n在传统的 MVC 三层 Web 应用程序架构下，服务之间的通讯并不复杂，在应用程序内部自己管理即可，但是在现今的复杂的大型网站情况下，单体应用被分解为众多的微服务，服务之间的依赖和通讯十分复杂，出现了 Twitter 开发的 Finagle、Netflix 开发的 Hystrix 和 Google 的 Stubby 这样的 “胖客户端” 库，这些就是早期的服务网格，但是它们都近适用于特定的环境和特定的开发语言，并不能作为平台级的服务网格支持。\n在云原生架构下，容器的使用给予了异构应用程序的更多可行性，Kubernetes 增强的应用的横向扩容能力，用户可以快速的编排出复杂环境、复杂依赖关系的应用程序，同时开发者又无须过分关心应用程序的监控、扩展性、服务发现和分布式追踪这些繁琐的事情而专注于程序开发，赋予开发者更多的创造性。\n参考  Istio: A service mesh for AWS ECS - medium.com 初次了解 Istio - istio.io Application Network Functions With ESBs, API Management, and Now.. Service Mesh? - blog.christianposta.com Pattern: Service Mesh - philcalcado.com Envoy 官方文档中文版 - cloudnative.to Istio 官方文档 - istio.io  ","date":1651532400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"2910652ff2990412a0ab7789796542e9","permalink":"https://jimmysong.io/docs/cloud-native/service-mesh/what-is-service-mesh/","publishdate":"2022-05-03T00:00:00+01:00","relpermalink":"/docs/cloud-native/service-mesh/what-is-service-mesh/","section":"cloud-native","summary":"Service Mesh 又译作 “服务网格”，作为服务间通信的基础设施层。Buoy","tags":null,"title":"什么是服务网格？","type":"book"},{"authors":null,"categories":null,"content":"云原生（Cloud Native）这个词汇由来已久，以致于何时出现已无据可考。云原生开始大规模出现在受众视线中，与 Pivotal 提出的云原生应用的理念有着莫大的关系。我们现在谈到云原生，更多的指的是一种文化，而不具象为哪些技术体系。\n Pivotal 推出过 Pivotal Cloud Foundry 云原生应用平台和 Spring 开源 Java 开发框架，成为云原生应用架构中先驱者和探路者。Pivotal 是云原生应用平台第一股，2018 年在纽交所上市，2019 年底被 VMWare 以 27 亿美元收购，加入到 VMware 新的产品线 Tanzu。\n Pivotal 最初的定义 早在 2015 年 Pivotal 公司的 Matt Stine 写了一本叫做 迁移到云原生应用架构 的小册子，其中探讨了云原生应用架构的几个主要特征：\n 符合 12 因素应用 面向微服务架构 自服务敏捷架构 基于 API 的协作 抗脆弱性  笔者已于 2017 年翻译了本书，详见 迁移到云原生应用架构。\nCNCF 最初的定义 到了 2015 年 Google 主导成立了云原生计算基金会（CNCF），起初 CNCF 对云原生（Cloud Native）的定义包含以下三个方面：\n 应用容器化 面向微服务架构 应用支持容器的编排调度  重定义 到了 2018 年，随着近几年来云原生生态的不断壮大，所有主流云计算供应商都加入了该基金会，且从 Cloud Native Landscape 中可以看出云原生有意蚕食原先非云原生应用的部分。CNCF 基金会中的会员以及容纳的项目越来越多，该定义已经限制了云原生生态的发展，CNCF 为云原生进行了重新定位。\n以下是 CNCF 对云原生的重新定义（中英对照）：\n Cloud native technologies empower organizations to build and run scalable applications in modern, dynamic environments such as public, private, and hybrid clouds. Containers, service meshes, microservices, immutable infrastructure, and declarative APIs exemplify this approach.\n 云原生技术有利于各组织在公有云、私有云和混合云等新型动态环境中，构建和运行可弹性扩展的应用。云原生的代表技术包括容器、服务网格、微服务、不可变基础设施和声明式 API。\n These techniques enable loosely coupled systems that are resilient, manageable, and observable. Combined with robust automation, they allow engineers to make high-impact changes frequently and predictably with minimal toil.\n 这些技术能够构建容错性好、易于管理和便于观察的松耦合系统。结合可靠的自动化手段，云原生技术使工程师能够轻松地对系统作出频繁和可预测的重大变更。\n The Cloud Native Computing Foundation seeks to drive adoption of this paradigm by fostering and sustaining an ecosystem of open source, vendor-neutral projects. We democratize state-of-the-art patterns to make these innovations accessible for everyone.\n 云原生计算基金会（CNCF）致力于培育和维护一个厂商中立的开源生态系统，来推广云原生技术。我们通过将最前沿的模式民主化，让这些创新为大众所用。\n总结 关于什么是云原生的争论还在进行中，在笔者看来云原生是一种行为方式和设计理念，究其本质，凡是能够提高云上资源利用率和应用交付效率的行为或方式都是云原生的。云计算的发展史就是一部云原生化的历史。Kubernetes 开启了云原生的序幕，服务网格 Istio 的出现，引领了后 Kubernetes 时代的微服务，serverless 的再次兴起，使得云原生从基础设施层不断向应用架构层挺进，我们正处于一个云原生的新时代。\n参考  CNCF Cloud Native Definition v1.0 - github.com 云原生关乎文化，而不是容器 - cloudnative.to  ","date":1651532400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"21c2ae08591f7667538b0f4c3296eceb","permalink":"https://jimmysong.io/docs/cloud-native/intro/what-is-cloud-native/","publishdate":"2022-05-03T00:00:00+01:00","relpermalink":"/docs/cloud-native/intro/what-is-cloud-native/","section":"cloud-native","summary":"云原生（Cloud Native）这个词汇由来已久，以致于何时","tags":null,"title":"什么是云原生？","type":"book"},{"authors":null,"categories":null,"content":"CNCF，全称Cloud Native Computing Foundation（云原生计算基金会），成立于 2015 年7月21日（于美国波特兰OSCON 2015上宣布），其最初的口号是坚持和整合开源技术来让编排容器作为微服务架构的一部分，其作为致力于云原生应用推广和普及的一支重要力量，不论您是云原生应用的开发者、管理者还是研究人员都有必要了解。\nCNCF作为一个厂商中立的基金会，致力于Github上的快速成长的开源技术的推广，如Kubernetes、Prometheus、Envoy等，帮助开发人员更快更好的构建出色的产品。CNCF 维护了一个全景图项目，详见 GitHub。\n关于CNCF的使命与组织方式请参考CNCF章程，概括的讲CNCF的使命包括以下三点：\n 容器化包装。 通过中心编排系统的动态资源管理。 面向微服务。  CNCF这个角色的作用是推广技术，形成社区，开源项目管理与推进生态系统健康发展。\n另外CNCF组织由以下部分组成：\n 会员：白金、金牌、银牌、最终用户、学术和非赢利成员，不同级别的会员在治理委员会中的投票权不同。 理事会：负责事务管理 TOC（技术监督委员会）：技术管理 最终用户社区：推动CNCF技术的采纳并选举最终用户技术咨询委员会 最终用户技术咨询委员会：为最终用户会议或向理事会提供咨询 营销委员会：市场推广  CNCF项目成熟度分级与毕业条件 每个CNCF项目都需要有个成熟度等级，申请成为CNCF项目的时候需要确定项目的成熟度级别。\n成熟度级别（Maturity Level）包括以下三种：\n sandbox（初级） incubating（孵化中） graduated（毕业）  是否可以成为CNCF项目需要通过Technical Oversight Committee (技术监督委员会）简称TOC，投票采取fallback策略，即回退策略，先从最高级别（graduated）开始，如果2/3多数投票通过的话则确认为该级别，如果没通过的话，则进行下一低级别的投票，如果一直到inception级别都没得到2/3多数投票通过的话，则拒绝其进入CNCF项目。\n当前所有的CNCF项目可以访问https://www.cncf.io/projects/ 。\n项目所达到相应成熟度需要满足的条件和投票机制见下图：\n   CNCF项目成熟度级别  TOC（技术监督委员会） TOC（Technical Oversight Committee）作为CNCF中的一个重要组织，它的作用是：\n 定义和维护技术视野 审批新项目加入组织，为项目设定概念架构 接受最终用户的反馈并映射到项目中 调整组件间的访问接口，协调组件之间兼容性  TOC成员通过选举产生，见选举时间表。\n参考 CNCF TOC：https://github.com/cncf/toc\nCNCF Ambassador CNCF Ambassador（CNCF 大使），人员名单详见 https://www.cncf.io/people/ambassadors/，笔者很荣幸作为第二位成为 CNCF Ambassador 的中国人。\n如何成为 CNCF Ambassador 可以通过以下方式成为 CNCF Ambassador：\n 成为 CNCF 会员或对成为某个 CNCF 的项目的贡献者 以 contributor、blogger、演讲者等身份参与 CNCF 社区项目 在社区中演讲或撰写博客 主持云原生社区 meetup  参考  AT\u0026amp;T, Box, Cisco, Cloud Foundry Foundation, CoreOS, Cycle Computing, Docker, eBay, Goldman Sachs, Google, Huawei, IBM, Intel, Joyent, Kismatic, Mesosphere, Red Hat, Switch SUPERNAP, Twitter, Univa, VMware and Weaveworks join new effort to build and maintain cloud native distributed systems - cncf.io  ","date":1651532400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"8819c80b4f7e7ceb991ed10478045cd3","permalink":"https://jimmysong.io/docs/cloud-native/community/cncf/","publishdate":"2022-05-03T00:00:00+01:00","relpermalink":"/docs/cloud-native/community/cncf/","section":"cloud-native","summary":"CNCF，全称Cloud Native Computing Foundation（云原生计算","tags":null,"title":"云原生计算基金会（CNCF）","type":"book"},{"authors":null,"categories":null,"content":"软件开发的模式又一次改变了。开源软件和公有云供应商已经从根本上改变了我们构建和部署软件的方式。有了开源软件，我们的应用不再需要从头开始编码。而通过使用公有云供应商，我们不再需要配置服务器或连接网络设备。所有这些都意味着，你可以在短短几天甚至几小时内从头开始构建和部署一个应用程序。\n但是，仅仅因为部署新的应用程序很容易，并不意味着操作和维护它们也变得更容易。随着应用程序变得更加复杂，更加异质，最重要的是，更加分布式，看清大局，以及准确地指出问题发生的地方，变得更加困难。\n但有些事情并没有改变：作为开发者和运维，我们仍然需要能够从用户的角度理解应用程序的性能，我们仍然需要对用户的事务有一个端到端的看法。我们也仍然需要衡量和说明在处理这些事务的过程中资源是如何被消耗的。也就是说，我们仍然需要可观测性。\n但是，尽管对可观测性的需求没有改变，我们实施可观测性解决方案的方式必须改变。本报告详细介绍了关于可观测性的传统思维方式对现代应用的不足：继续将可观测性作为一系列工具（尤其是作为 “三大支柱”）来实施，几乎不可能以可靠的方式运行现代应用。\nOpenTelemetry 通过提供一种综合的方法来收集有关应用程序行为和性能的数据，包括指标、日志和跟踪，来解决这些挑战。正如本报告所解释的，OpenTelemetry 是一种生成和收集这些数据的新方法，这种方法是为使用开源组件构建的云原生应用而设计的。\n事实上，OpenTelemetry 是专门为这些开源组件设计的。与供应商特定的仪表不同，OpenTelemetry 可以直接嵌入到开放源代码中。这意味着开源库的作者可以利用他们的专业知识来添加高质量的仪表，而不需要在他们的项目中增加任何解决方案或供应商特定的代码。\nOpenTelemetry 对应用程序所有者也有好处。在过去，遥测的产生方式与它的评估、存储和分析方式相联系。这意味着，选择使用哪种可观测性解决方案必须在开发过程的早期完成，并且在之后很难改变。OpenTelemetry（就像它的前身 OpenTracing 和 OpenCensus 一样）将你的应用程序产生遥测的方式与遥测的分析方式相分离。\n因此，当我们采用新技术时，我们往往没有考虑到该技术将如何被使用，特别是它将如何被不同角色的人使用。本报告描述了 OpenTelemetry 如何满足库作者、应用程序拥有者、运维和响应者的需求，以及它如何使这些角色中的每个人都能独立工作 —— 即自动做出关键决定并进行有效协作。\n也许最重要的是，OpenTelemetry 为应用程序所有者和操作者提供了灵活性，使他们能够选择最适合其应用程序和组织需求的可观测性解决方案，包括需要多种工具的情况。它还包括这些需求随时间变化的情况，以及需要新的工具来满足这些需求的情况，因为随着技术的成熟和组织的发展，可能会出现这种情况。\n虽然用户不需要致力于可观测性解决方案，但他们确实需要投资于生成遥测数据的一致方式。本报告展示了 OpenTelemetry 是如何被设计成范围狭窄、可扩展，而且最重要的是稳定的：如果你被要求进行这种投资，这正是你所期望的。\n与以往任何时候相比，可观测性都不能成为事后的想法。没有一个有效的可观测性解决方案的风险，也就是说，无法了解你的应用程序中正在发生的事情的风险是非常高的，而对可观测性采取错误的方法的成本会造成你的组织多年来一直在偿还的债务。无论是长时间停机，还是开发团队被无休止的供应商迁移所困扰，你的组织的成功都取决于像 OpenTelemetry 这样的集成和开放的方法。\n本报告提供了关于在你的组织中采用和管理 OpenTelemetry 的实用建议。它将使你开始走向成功的可观测性实践的道路，并释放出云原生和开源技术的许多真正的好处：你不仅能够快速建立和部署应用程序，而且能够可靠和自信地运行它们。\n—— Daniel “Spoons” Spoonhower Lightstep\n","date":1643990400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"440765fde0de973d8f49a3e80359e2e2","permalink":"https://jimmysong.io/docs/opentelemetry-obervability/foreword/","publishdate":"2022-02-05T00:00:00+08:00","relpermalink":"/docs/opentelemetry-obervability/foreword/","section":"opentelemetry-obervability","summary":"前言","tags":null,"title":"前言","type":"book"},{"authors":null,"categories":null,"content":"代码审查的主要目的是确保逐步改善 Google 代码库的整体健康状况。代码审查的所有工具和流程都是为此而设计的。\n为了实现此目标，必须做出一系列权衡。\n首先，开发人员必须能够对任务进行改进。如果开发者从未向代码库提交过代码，那么代码库的改进也就无从谈起。此外，如果审核人员对代码吹毛求疵，那么开发人员以后也很难再做出改进。\n另外，审查者有责任确保随着时间的推移，CL 的质量不会使代码库的整体健康状况下降。这可能很棘手，因为通常情况下，代码库健康状况会随着时间的而下降，特别是在对团队有严格的时间要求时，团队往往会采取捷径来达成他们的目标。\n此外，审查者应对正在审核的代码负责并拥有所有权。审查者希望确保代码库保持一致、可维护及 Code Review 要点中所提及的所有其他内容。\n因此，我们将以下规则作为 Code Review 中期望的标准：\n一般来说，审核人员应该倾向于批准 CL，只要 CL 确实可以提高系统的整体代码健康状态，即使 CL 并不完美。\n这是所有 Code Review 指南中的高级原则。\n当然，也有一些限制。例如，如果 CL 添加了审查者认为系统中不需要的功能，那么即使代码设计良好，审查者依然可以拒绝批准它。\n此处有一个关键点就是没有“完美”的代码，只有更好的代码。审查者不该要求开发者在批准程序前仔细清理、润色 CL 每个角落。相反，审查者应该在变更的重要性与取得进展之间取得平衡。审查者不应该追求完美，而应是追求持续改进。不要因为一个 CL不是“完美的”，就将可以提高系统的可维护性、可读性和可理解性的 CL 延迟数天或数周才批准。\n审核者应该随时在可以改善的地方留下审核评论，但如果评论不是很重要，请在评论语句前加上“Nit：”之类的内容，让开发者知道这条评论是用来指出可以润色的地方，而他们可以选择是否忽略。\n注意：本文档中没有任何内容证明检查 CL 肯定会使系统的整体代码健康状况恶化。您会做这种事情应该只有在紧急情况时。\n指导 代码审查具有向开发人员传授语言、框架或通用软件设计原则新内容的重要功能。留下评论可以帮助开发人员学习新东西，这总归是很好的。分享知识是随着长年累月改善系统代码健康状况的一部分。请记住，如果您的评论纯粹是教育性的，且对于本文档中描述的标准并不重要，请在其前面添加“Nit：”或以其他方式表明作者不必在此 CL 中解决它。\n原则  基于技术事实和数据否决意见和个人偏好。 关于代码风格问题，风格指南是绝对权威。任何不在风格指南中的纯粹风格点（例如空白等）都是个人偏好的问题。代码风格应该与风格指南中的一致。如果没有以前的风格，请接受作者的风格。 软件设计方面几乎不是纯粹的风格或个人偏好问题。软件设计基于基本原则且应该权衡这些原则，而不仅仅是个人意见。有时候会有多种有效的选择。如果作者可以证明（通过数据或基于可靠的工程原理）该方法同样有效，那么审查者应该接受作者的偏好。否则，就要取决于软件设计的标准原则。 如果没有其他适用规则，则审查者可以要求作者与当前代码库中的内容保持一致，只要不恶化系统的整体代码健康状况即可。  解决冲突 如果在代码审查过程中有任何冲突，第一步应该始终是开发人员和审查者根据本文档中的 CL 开发者指南和审查者指南达成共识。\n当达成共识变得特别困难时，审阅者和开发者可以进行面对面的会议，或者有 VC 参与调停，而不仅仅是试着通过代码审查评论来解决冲突。 （但是，如果您这样做了，请确保在 CL 的评论中记录讨论结果，以供将来的读者使用。）\n如果这样还不能解决问题，那么解决该问题最常用方法是将问题升级。通常是将问题升级为更广泛的团队讨论，有一个 TL 权衡，要求维护人员对代码作出决定，或要求工程经理的帮助。 不要因为 CL 的开发者和审查者不能达成一致，就让 CL 在那里卡壳。\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"99a8b946074829942d1bb98f8d6de638","permalink":"https://jimmysong.io/docs/eng-practices/review/reviewer/standard/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/docs/eng-practices/review/reviewer/standard/","section":"eng-practices","summary":"代码审查的主要目的是确保逐步改善 Google 代码库的整体健康状况。代码","tags":null,"title":"Code Review 标准","type":"book"},{"authors":null,"categories":null,"content":"CL 描述是进行了哪些更改以及为何更改的公开记录。CL 将作为版本控制系统中的永久记录，可能会在长时期内被除审查者之外的数百人阅读。\n开发者将来会根据描述搜索您的 CL。有人可能会仅凭有关联性的微弱印象，但没有更多具体细节的情况下，来查找你的改动。如果所有重要信息都在代码而不是描述中，那么会让他们更加难以找到你的 CL 。\n首行  正在做什么的简短摘要。 完整的句子，使用祈使句。 后面跟一个空行。  CL 描述的第一行应该是关于这个 CL 是做什么的简短摘要，后面跟一个空白行。这是将来大多数的代码搜索者在浏览代码的版本控制历史时，最常被看到的内容，因此第一行应该提供足够的信息，以便他们不必阅读 CL 的整个描述就可以获得这个 CL 实际上是做了什么的信息。\n按照传统，CL 描述的第一行应该是一个完整的句子，就好像是一个命令（一个命令句）。例如，“Delete the FizzBuzz RPC and replace it with the new system.”而不是“Deleting the FizzBuzz RPC and replacing it with the new system.“ 但是，您不必把其余的描述写成祈使句。\nBody 是信息丰富的 其余描述应该是提供信息的。可能包括对正在解决的问题的简要描述，以及为什么这是最好的方法。如果方法有任何缺点，应该提到它们。如果相关，请包括背景信息，例如错误编号，基准测试结果以及设计文档的链接。\n即使是小型 CL 也需要注意细节。在 CL 描述中提供上下文以供参照。\n糟糕的 CL 描述 “Fix bug ”是一个不充分的 CL 描述。什么 bug？你做了什么修复？其他类似的不良描述包括：\n “Fix build.” “Add patch.” “Moving code from A to B.” “Phase 1.” “Add convenience functions.” “kill weird URLs.”  其中一些是真正的 CL 描述。他们的作者可能认为自己提供了有用的信息，却没有达到 CL 描述的目的。\n好的 CL 描述 以下是一些很好的描述示例。\n功能更新  rpc：删除 RPC 服务器消息 freelist 上的大小限制。\n像 FIzzBuzz 这样的服务器有非常大的消息，并且可以从重用中受益。增大 freelist，添加一个 goroutine，缓慢释放 freelist 条目，以便空闲服务器最终释放所有 freelist 条目。\n 前几个词描述了CL实际上做了什么。其余的描述讨论了正在解决的问题，为什么这是一个很好的解决方案，以及有关具体实现的更多信息。\n重构  Construct a Task with a TimeKeeper to use its TimeStr and Now methods.\nAdd a Now method to Task, so the borglet() getter method can be removed (which was only used by OOMCandidate to call borglet’s Now method). This replaces the methods on Borglet that delegate to a TimeKeeper.\nAllowing Tasks to supply Now is a step toward eliminating the dependency on Borglet. Eventually, collaborators that depend on getting Now from the Task should be changed to use a TimeKeeper directly, but this has been an accommodation to refactoring in small steps.\nContinuing the long-range goal of refactoring the Borglet Hierarchy.\n 第一行描述了 CL 的作用以及改变。其余的描述讨论了具体的实现，CL 的背景，解决方案并不理想，以及未来的可能方向。它还解释了为什么正在进行此更改。\n需要上下文的 小 CL  Create a Python3 build rule for status.py.\nThis allows consumers who are already using this as in Python3 to depend on a rule that is next to the original status build rule instead of somewhere in their own tree. It encourages new consumers to use Python3 if they can, instead of Python2, and significantly simplifies some automated build file refactoring tools being worked on currently.\n 第一句话描述实际做了什么。其余的描述解释了为什么正在进行更改并为审查者提供了大量背景信息。\n在提交 CL 前审查描述 CL 在审查期间可能会发生重大变更。在提交 CL 之前检查 CL 描述是必要的，以确保描述仍然反映了 CL 的作用。\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"aa500f7e54b31e45a6dd638393b28812","permalink":"https://jimmysong.io/docs/eng-practices/review/developer/cl-descriptions/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/docs/eng-practices/review/developer/cl-descriptions/","section":"eng-practices","summary":"CL 描述是进行了哪些更改以及为何更改的公开记录。CL 将作为版本","tags":null,"title":"写好 CL 描述","type":"book"},{"authors":null,"categories":null,"content":"HCM 是一个网络级的过滤器，将原始字节转译成 HTTP 级别的消息和事件（例如，收到的 Header，收到的 Body 数据等）。\nHCM 过滤器还处理标准的 HTTP 功能。它支持访问记录、请求 ID 生成和跟踪、Header 操作、路由表管理和统计等功能。\n从协议的角度来看，HCM 原生支持 HTTP/1.1、WebSockets、HTTP/2 和 HTTP/3（仍在 Alpha 阶段）。\nEnvoy 代理被设计成一个 HTTP/2 复用代理，这体现在描述 Envoy 组件的术语中。\nHTTP/2 术语\n在 HTTP/2 中，流是已建立的连接中的字节的双向流动。每个流可以携带一个或多个消息（message）。消息是一个完整的帧（frame）序列，映射到一个 HTTP 请求或响应消息。最后，帧是 HTTP/2 中最小的通信单位。每个帧都包含一个帧头（frame header），它至少可以识别该帧所属的流。帧可以携带有关 HTTP Header、消息有效载荷等信息。\n无论流来自哪个连接（HTTP/1.1、HTTP/2 或 HTTP/3），Envoy 都使用一个叫做 编解码 API（codec API） 的功能，将不同的线程协议翻译成流、请求、响应等协议无关模型。协议无关的模型意味着大多数 Envoy 代码不需要理解每个协议的具体内容。\nHTTP 过滤器 在 HCM 中，Envoy 支持一系列的 HTTP 过滤器。与监听器级别的过滤器不同，这些过滤器对 HTTP 级别的消息进行操作，而不知道底层协议（HTTP/1.1、HTTP/2 等）或复用能力。\n有三种类型的 HTTP 过滤器。\n 解码器（Decoder）：当 HCM 对请求流的部分进行解码时调用。 编码器（Encoder）：当 HCM 对响应流的部分进行编码时调用。 解码器 / 编码器（Decoder/Encoder）：在两个路径上调用，解码和编码  下图解释了 Envoy 如何在请求和响应路径上调用不同的过滤器类型。\n   请求响应路径及 HTTP 过滤器  像网络过滤器一样，单个的 HTTP 过滤器可以停止或继续执行后续的过滤器，并在单个请求流的范围内相互分享状态。\n数据共享 在高层次上，我们可以把过滤器之间的数据共享分成静态和动态。\n静态包含 Envoy 加载配置时的任何不可变的数据集，它被分成三个部分。\n1. 元数据\nEnvoy 的配置，如监听器、路由或集群，都包含一个metadata数据字段，存储键 / 值对。元数据允许我们存储特定过滤器的配置。这些值不能改变，并在所有请求 / 连接中共享。例如，元数据值在集群中使用子集选择器时被使用。\n2. 类型化的元数据\n类型化元数据不需要为每个流或请求将元数据转换为类型化的类对象，而是允许过滤器为特定的键注册一个一次性的转换逻辑。来自 xDS 的元数据在配置加载时被转换为类对象，过滤器可以在运行时请求类型化的版本，而不需要每次都转换。\n3. HTTP 每路过滤器配置\n与适用于所有虚拟主机的全局配置相比，我们还可以指定每个虚拟主机或路由的配置。每个路由的配置被嵌入到路由表中，可以在 typed_per_filter_config 字段下指定。\n另一种分享数据的方式是使用动态状态。动态状态会在每个连接或 HTTP 流中产生，并且它可以被产生它的过滤器改变。名为 StreamInfo 的对象提供了一种从 map 上存储和检索类型对象的方法。\n过滤器顺序 指定 HTTP 过滤器的顺序很重要。考虑一下下面的 HTTP 过滤器链。\nhttp_filters:- filter_1- filter_2- filter_3一般来说，链中的最后一个过滤器通常是路由器过滤器。假设所有的过滤器都是解码器 / 编码器过滤器，HCM 在请求路径上调用它们的顺序是filter_1、filter_2、filter_3。\n在响应路径上，Envoy 只调用编码器过滤器，但顺序相反。由于这三个过滤器都是解码器 / 编码器过滤器，所以在响应路径上的顺序是 filter_3、filter_2、filter_1。\n内置 HTTP 过滤器 Envoy 已经内置了几个 HTTP 过滤器，如 CORS、CSRF、健康检查、JWT 认证等。你可以在这里找到 HTTP 过滤器的完整列表。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"b76cdbd18617a4bb4428f64dabcd72e9","permalink":"https://jimmysong.io/docs/envoy-handbook/hcm/introduction/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/envoy-handbook/hcm/introduction/","section":"envoy-handbook","summary":"HCM 是一个网络级的过滤器，将原始字节转译成 HTTP 级别的消息和事件（","tags":null,"title":"HTTP 连接管理器（HCM）简介","type":"book"},{"authors":null,"categories":null,"content":"前面提到的路由器过滤器（envoy.filters.http.router）就是实现 HTTP 转发的。路由器过滤器几乎被用于所有的 HTTP 代理方案中。路由器过滤器的主要工作是查看路由表，并对请求进行相应的路由（转发和重定向）。\n路由器使用传入请求的信息（例如，host  或 authority 头），并通过虚拟主机和路由规则将其与上游集群相匹配。\n所有配置的 HTTP 过滤器都使用包含路由表的路由配置（route_config）。尽管路由表的主要消费者将是路由器过滤器，但其他过滤器如果想根据请求的目的地做出任何决定，也可以访问它。\n一组虚拟主机构成了路由配置。每个虚拟主机都有一个逻辑名称，一组可以根据请求头被路由到它的域，以及一组指定如何匹配请求并指出下一步要做什么的路由。\nEnvoy 还支持路由级别的优先级路由。每个优先级都有其连接池和断路设置。目前支持的两个优先级是 DEFAULT 和 HIGH。如果我们没有明确提供优先级，则默认为 DEFAULT。\n这里有一个片段，显示了一个路由配置的例子。\nroute_config:name:my_route_config# 用于统计的名称，与路由无关virtual_hosts:- name:bar_vhostdomains:[\u0026#34;bar.io\u0026#34;]routes:- match:prefix:\u0026#34;/\u0026#34;route:priority:HIGHcluster:bar_io- name:foo_vhostdomains:[\u0026#34;foo.io\u0026#34;]routes:- match:prefix:\u0026#34;/\u0026#34;route:cluster:foo_io- match:prefix:\u0026#34;/api\u0026#34;route:cluster:foo_io_api当一个 HTTP 请求进来时，虚拟主机、域名和路由匹配依次发生。\n host或authority头被匹配到每个虚拟主机的domains字段中指定的值。例如，如果主机头被设置为 foo.io，则虚拟主机 foo_vhost 匹配。 接下来会检查匹配的虚拟主机内routs下的条目。如果发现匹配，就不做进一步检查，而是选择一个集群。例如，如果我们匹配了 foo.io 虚拟主机，并且请求前缀是 /api，那么集群 foo_io_api 就被选中。 如果提供，虚拟主机中的每个虚拟集群（virtual_clusters）都会被检查是否匹配。如果有匹配的，就使用一个虚拟集群，而不再进行进一步的虚拟集群检查。   虚拟集群是一种指定针对特定端点的重组词匹配规则的方式，并明确为匹配的请求生成统计信息。\n 虚拟主机的顺序以及每个主机内的路由都很重要。考虑下面的路由配置。\nroute_config:virtual_hosts:- name:hello_vhostdomains:[\u0026#34;hello.io\u0026#34;]routes:- match:prefix:\u0026#34;/api\u0026#34;route:cluster:hello_io_api- match:prefix:\u0026#34;/api/v1\u0026#34;route:cluster:hello_io_api_v1如果我们发送以下请求，哪个路由 / 集群被选中？\ncurl hello.io/api/v1 第一个设置集群 hello_io_api的路由被匹配。这是因为匹配是按照前缀的顺序进行评估的。然而，我们可能错误地期望前缀为 /api/v1 的路由被匹配。为了解决这个问题，我们可以调换路由的顺序，或者使用不同的匹配规则。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"4629630f7fb75d52ba360b1536a1f162","permalink":"https://jimmysong.io/docs/envoy-handbook/hcm/http-routing/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/envoy-handbook/hcm/http-routing/","section":"envoy-handbook","summary":"前面提到的路由器过滤器（envoy.filters.http","tags":null,"title":"HTTP 路由","type":"book"},{"authors":null,"categories":null,"content":"作者\nCybersecurity and Infrastructure Security Agency (CISA)\nNational Security Agency (NSA) Cybersecurity Directorate Endpoint Security\n联系信息\n客户要求 / 一般网络安全问题。\n网络安全需求中心，410-854-4200，Cybersecurity_Requests@nsa.gov。\n媒体咨询 / 新闻台\n媒体关系，443-634-0721，MediaRelations@nsa.gov。\n关于事件响应资源，请联系 CISA：CISAServiceDesk@cisa.dhs.gov。\n中文版\n关于本书中文版的信息请联系 Jimmy Song：jimmysong@jimmysong.io。\n宗旨\n国家安全局和 CISA 制定本文件是为了促进其各自的网络安全，包括其制定和发布网络安全规范和缓解措施的责任。这一信息可以被广泛分享，以触达所有适当的利益相关者。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"0e827effb7fe83fe8cfc65cdb9b42e99","permalink":"https://jimmysong.io/docs/kubernetes-hardening-guidance/publication-information/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/kubernetes-hardening-guidance/publication-information/","section":"kubernetes-hardening-guidance","summary":"作者 Cybersecurity and Infrastructure Security Agency (CISA) National Security Agency (NSA) Cybersecurity Directorate Endpoint Security 联系信息 客户要求 / 一般网络安","tags":null,"title":"出版信息","type":"book"},{"authors":null,"categories":null,"content":"IT 行业正在向微服务架构和云原生解决方案发展。由于使用不同的技术开发了成百上千的微服务，这些系统可能变得复杂，难以调试。\n作为一个应用开发者，你考虑的是业务逻辑——购买产品或生成发票。然而，任何像这样的业务逻辑都会导致不同服务之间的多个服务调用。每个服务可能都有它的超时、重试逻辑和其他可能需要调整或微调的网络特定代码。\n如果在任何时候最初的请求失败了，就很难通过多个服务来追踪，准确地指出失败发生的地方，了解请求为什么失败。是网络不可靠吗？是否需要调整重试或超时？或者是业务逻辑问题或错误？\n服务可能使用不一致的跟踪和记录机制，使这种调试的复杂性增加。这些问题使你很难确定问题发生在哪里，以及如何解决。如果你是一个应用程序开发人员，而调试网络问题不属于你的核心技能，那就更是如此。\n将网络问题从应用程序堆栈中抽离出来，由另一个组件来处理网络部分，让调试网络问题变得更容易。这就是 Envoy 所做的事情。\n在每个服务实例旁边都有一个 Envoy 实例在运行。这种类型的部署也被称为 Sidecar 部署。Envoy 的另一种模式是边缘代理，用于构建 API 网关。\nEnvoy 和应用程序形成一个原子实体，但仍然是独立的进程。应用程序处理业务逻辑，而 Envoy 则处理网络问题。\n在发生故障的情况下，分离关注点可以更容易确定故障是来自应用程序还是网络。\n为了帮助网络调试，Envoy 提供了以下高级功能。\n进程外架构 Envoy 是一个独立的进程，旨在与每个应用程序一起运行 —— 也就是我们前面提到的 Sidecar 部署模式。集中配置的 Envoy 的集合形成了一个透明的服务网格。\n路由和其他网络功能的责任被推给了 Envoy。应用程序向一个虚拟地址（localhost）而不是真实地址（如公共 IP 地址或主机名）发送请求，不知道网络拓扑结构。应用程序不再承担路由的责任，因为该任务被委托给一个外部进程。\n与其让应用程序管理其网络配置，不如在 Envoy 层面上独立于应用程序管理网络配置。在一个组织中，这可以使应用程序开发人员解放出来，专注于应用程序的业务逻辑。\nEnvoy 适用于任何编程语言。你可以用 Go、Java、C++ 或其他任何语言编写你的应用程序，而 Envoy 可以在它们之间架起桥梁。Envoy 的行为是相同的，无论应用程序的编程语言或它们运行的操作系统是什么。\nEnvoy 还可以在整个基础设施中透明地进行部署和升级。这与为每个单独的应用程序部署库升级相比，后者可能是非常痛苦和耗时的。\n进程外架构是有益的，因为它使我们在不同的编程语言 / 应用堆栈中保持一致，我们可以免费获得独立的应用生命周期和所有的 Envoy 网络功能，而不必在每个应用中单独解决这些问题。\nL3/L4 过滤器结构 Envoy 是一个 L3/L4 网络代理，根据 IP 地址和 TCP 或 UDP 端口进行决策。它具有一个可插拔的过滤器链，可以编写你的过滤器来执行不同的 TCP/UDP 任务。\n过滤器链（Filter Chain） 的想法借鉴了 Linux shell，即一个操作的输出被输送到另一个操作中。例如：\nls -l | grep \u0026#34;Envoy*.cc\u0026#34; | wc -l Envoy 可以通过堆叠所需的过滤器来构建逻辑和行为，形成一个过滤器链。许多过滤器已经存在，并支持诸如原始 TCP 代理、UDP 代理、HTTP 代理、TLS 客户端认证等任务。Envoy 也是可扩展的，我们可以编写我们的过滤器。\nL7 过滤器结构 Envoy 支持一个额外的 HTTP L7 过滤器层。我们可以在 HTTP 连接管理子系统中插入 HTTP 过滤器，执行不同的任务，如缓冲、速率限制、路由 / 转发等。\n一流的 HTTP/2 支持 Envoy 同时支持 HTTP/1.1 和 HTTP/2，并且可以作为一个透明的 HTTP/1.1 到 HTTP/2 的双向代理进行操作。这意味着任何 HTTP/1.1 和 HTTP/2 客户端和目标服务器的组合都可以被桥接起来。即使你的传统应用没有通过 HTTP/2 进行通信，如果你把它们部署在 Envoy 代理旁边，它们最终也会通过 HTTP/2 进行通信。\n推荐在所有的服务间配置的 Envoy 使用 HTTP/2，以创建一个持久连接的网格，请求和响应可以在上面复用。\nHTTP 路由 当以 HTTP 模式操作并使用 REST 时，Envoy 支持路由子系统，能够根据路径、权限、内容类型和运行时间值来路由和重定向请求。在将 Envoy 作为构建 API 网关的前台 / 边缘代理时，这一功能非常有用，在构建服务网格（sidecar 部署模式）时，也可以利用这一功能。\ngRPC 准备就绪 Envoy 支持作为 gRPC 请求和响应的路由和负载均衡底层所需的所有 HTTP/2 功能。\n gRPC 是一个开源的远程过程调用（RPC）系统，它使用 HTTP/2 进行传输，并将协议缓冲区作为接口描述语言（IDL），它提供的功能包括认证、双向流和流量控制、阻塞 / 非阻塞绑定，以及取消和超时。\n 服务发现和动态配置 我们可以使用静态配置文件来配置 Envoy，这些文件描述了服务间通信方式。\n对于静态配置 Envoy 不现实的高级场景，Envoy 支持动态配置，在运行时自动重新加载配置。一组名为 xDS 的发现服务可以用来通过网络动态配置 Envoy，并为 Envoy 提供关于主机、集群 HTTP 路由、监听套接字和加密信息。\n健康检查 负载均衡器有一个特点，那就是只将流量路由到健康和可用的上游服务。Envoy 支持健康检查子系统，对上游服务集群进行主动健康检查。然后，Envoy 使用服务发现和健康检查信息的组合来确定健康的负载均衡目标。Envoy 还可以通过异常点检测子系统支持被动健康检查。\n高级负载均衡 Envoy 支持自动重试、断路、全局速率限制（使用外部速率限制服务）、影子请求（或流量镜像）、异常点检测和请求对冲。\n前端 / 边缘代理支持 Envoy 的特点使其非常适合作为边缘代理运行。这些功能包括 TLS 终端、HTTP/1.1、HTTP/2 和 HTTP/3 支持，以及 HTTP L7 路由。\nTLS 终止 应用程序和代理的解耦使网格部署模型中所有服务之间的 TLS 终止（双向 TLS）成为可能。\n一流的可观察性 为了便于观察，Envoy 会生成日志、指标和追踪。Envoy 目前支持 statsd（和兼容的提供者）作为所有子系统的统计。得益于可扩展性，我们也可以在需要时插入不同的统计提供商。\nHTTP/3（Alpha） Envoy 1.19.0 支持 HTTP/3 的上行和下行，并在 HTTP/1.1、HTTP/2 和 HTTP/3 之间进行双向转义。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"6bedec057ecb0868ce5739752a8d71b2","permalink":"https://jimmysong.io/docs/envoy-handbook/intro/what-is-envoy/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/envoy-handbook/intro/what-is-envoy/","section":"envoy-handbook","summary":"IT 行业正在向微服务架构和云原生解决方案发展。由于使用不同的技","tags":null,"title":"什么是 Envoy？","type":"book"},{"authors":null,"categories":null,"content":"将应用程序的功能划分为单独的进程运行在同一个最小调度单元中（例如 Kubernetes 中的 Pod）可以被视为 sidecar 模式。如下图所示，sidecar 模式允许您在应用程序旁边添加更多功能，而无需额外第三方组件配置或修改应用程序代码。\n   Sidecar 模式示意图  就像连接了 Sidecar 的三轮摩托车一样，在软件架构中， Sidecar 连接到父应用并且为其添加扩展或者增强功能。Sidecar 应用与主应用程序松散耦合。它可以屏蔽不同编程语言的差异，统一实现微服务的可观察性、监控、日志记录、配置、断路器等功能。\n使用 Sidecar 模式的优势 使用 sidecar 模式部署服务网格时，无需在节点上运行代理，但是集群中将运行多个相同的 sidecar 副本。在 sidecar 部署方式中，每个应用的容器旁都会部署一个伴生容器，这个容器称之为 sidecar 容器。Sidecar 接管进出应用容器的所有流量。在 Kubernetes 的 Pod 中，在原有的应用容器旁边注入一个 Sidecar 容器，两个容器共享存储、网络等资源，可以广义的将这个包含了 sidecar 容器的 Pod 理解为一台主机，两个容器共享主机资源。\n因其独特的部署结构，使得 sidecar 模式具有以下优势：\n 将与应用业务逻辑无关的功能抽象到共同基础设施，降低了微服务代码的复杂度。 因为不再需要编写相同的第三方组件配置文件和代码，所以能够降低微服务架构中的代码重复度。 Sidecar 可独立升级，降低应用程序代码和底层平台的耦合度。  ","date":1651532400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"e2a02d5866a50abb27ea0bfb8f8a14a5","permalink":"https://jimmysong.io/docs/cloud-native/kubernetes/sidecar-pattern/","publishdate":"2022-05-03T00:00:00+01:00","relpermalink":"/docs/cloud-native/kubernetes/sidecar-pattern/","section":"cloud-native","summary":"将应用程序的功能划分为单独的进程运行在同一个最小调度单元中（","tags":null,"title":"Sidecar 模式","type":"book"},{"authors":null,"categories":null,"content":"云原生一词已经被过度的采用，很多软件都号称是云原生，很多打着云原生旗号的会议也如雨后春笋般涌现。\n云原生本身甚至不能称为是一种架构，它首先是一种基础设施，运行在其上的应用称作云原生应用，只有符合云原生设计哲学的应用架构才叫云原生应用架构。\n云原生的设计理念 云原生系统的设计理念如下:\n 面向分布式设计（Distribution）：容器、微服务、API 驱动的开发； 面向配置设计（Configuration）：一个镜像，多个环境配置； 面向韧性设计（Resistancy）：故障容忍和自愈； 面向弹性设计（Elasticity）：弹性扩展和对环境变化（负载）做出响应； 面向交付设计（Delivery）：自动拉起，缩短交付时间； 面向性能设计（Performance）：响应式，并发和资源高效利用； 面向自动化设计（Automation）：自动化的 DevOps； 面向诊断性设计（Diagnosability）：集群级别的日志、metric 和追踪； 面向安全性设计（Security）：安全端点、API Gateway、端到端加密；  以上的设计理念很多都是继承自分布式应用的设计理念。虽然有如此多的理念但是我们仍然无法辨认什么样的设施才是云原生基础设施，不过可以先用排除法，我将解释什么不是云原生基础设施。\n什么不是云原生基础设施？ 云原生基础设施不等于在公有云上运行的基础设施。光是租用服务器并不会使您的基础设施云原生化。管理 IaaS 的流程与运维物理数据中心没什么两样，将现有架构迁移到云上也未必能获得回报。\n云原生不是指在容器中运行应用程序。Netflix 率先推出云原生基础设施时，几乎所有应用程序部署在虚拟机中，而不是在容器中。改变应用程序的打包方式并不意味着就会增加自治系统的可扩展性和优势。即使应用程序是通过 CI/CD 渠道自动构建和部署的，也不意味着您就可以从增强 API 驱动部署的基础设施中受益。\n这也并不意味着您只能运行容器编排器（例如 Kubernetes 和 Mesos）。容器编排器提供了云原生基础设施所需的许多平台功能，但并未按预期方式使用这些功能，这意味着您的应用程序会在一组服务器上运行，被动态调度。这是一个非常好的起步，但仍有许多工作要做。\n 调度器与编排器\n术语 “调度器” 和 “编排器” 通常可以互换使用。\n在大多数情况下，编排器负责集群中的所有资源利用（例如：存储，网络和 CPU）。该术语典型地用于描述执行许多任务的产品，如健康检查和云自动化。\n 调度器是编排平台的一个子集，仅负责选择运行在每台服务器上的进程和服务。\n云原生不是微服务或基础设施即代码。微服务意味着更快的开发周期和更小的独特功能，但是单体应用程序可以具有相同的功能，使其能够通过软件有效管理，并且还可以从云原生基础设施中受益。\n基础设施即代码以机器可解析语言或领域特定语言（DSL）定义、自动化您的基础设施。将代码应用于基础架构的传统工具包括配置管理工具（例如 Chef 和 Puppet）。这些工具在自动执行任务和提供一致性方面有很大帮助，但是它们在提供必要的抽象来描述超出单个服务器的基础设施方面存在缺陷。\n配置管理工具一次自动化一台服务器，并依靠人员将服务器提供的功能绑定在一起。这将人类定位为基础设施规模的潜在瓶颈。这些工具也不会使构建完整系统所需的云基础设施（例如存储和网络）的额外部分自动化。\n尽管配置管理工具为操作系统的资源（例如软件包管理器）提供了一些抽象，但它们并没有抽象出足够的底层操作系统来轻松管理它。如果一位工程师想要管理系统中的每个软件包和文件，这将是一个非常艰苦的过程，并且对于每个配置变体都是独一无二的。同样，定义不存在或不正确的资源的配置管理仅消耗系统资源并且不能提供任何价值。\n虽然配置管理工具可以帮助自动化部分基础设施，但它们无法更好地管理应用程序。我们将在后面的章节中通过查看部署，管理，测试和操作基础架构的流程，探讨云原生基础设施的不同之处，但首先，我们将了解哪些应用程序是成功的以及应该何时与原生基础设施一起使用。\n云原生应用程序 就像云改变了业务和基础设施之间的关系一样，云原生应用程序也改变了应用程序和基础设施之间的关系。我们需要了解与传统应用程序相比，云本身有什么不同，因此我们需要了解它们与基础设施的新关系。\n为了写好本书，也为了有一个共享词汇表，我们需要定义 “云原生应用程序” 是什么意思。云原生与 12 因素应用程序不同，即使它们可能共享一些类似的特征。如果你想了解更多细节，请阅读 Kevin Hoffman 撰写的 “超越 12 因素应用程序”（O’Reilly，2012）。\n云原生应用程序被设计为在平台上运行，并设计用于弹性，敏捷性，可操作性和可观察性。弹性包含失败而不是试图阻止它们；它利用了在平台上运行的动态特性。敏捷性允许快速部署和快速迭代。可操作性从应用程序内部控制应用程序生命周期，而不是依赖外部进程和监视器。可观察性提供信息来回答有关应用程序状态的问题。\n 云原生定义\n云原生应用程序的定义仍在发展中。还有像 CNCF 这样的组织可以提供其他的定义。\n 云原生应用程序通过各种方法获取这些特征。它通常取决于应用程序的运行位置以及企业流程和文化。以下是实现云原生应用程序所需特性的常用方法：\n 微服务 健康报告 遥测数据 弹性 声明式的，而不是命令式的  微服务 作为单个实体进行管理和部署的应用程序通常称为单体应用。最初开发应用程序时，单体有很多好处。它们更易于理解，并允许您在不影响其他服务的情况下更改主要功能。\n随着应用程序复杂性的增长，单体应用的益处逐渐减少。它们变得更难理解，而且失去了敏捷性，因为工程师很难推断和修改代码。\n对付复杂性的最好方法之一是将明确定义的功能分成更小的服务，并让每个服务独立迭代。这增加了应用程序的灵活性，允许根据需要更轻松地更改部分应用程序。每个微服务可以由单独的团队进行管理，使用适当的语言编写，并根据需要进行独立扩缩容。\n只要每项服务都遵守强有力的合约，应用程序就可以快速改进和改变。当然，转向微服务架构还有许多其他的考虑因素。其中最不重要的是弹性通信，我们在附录 A 中有讨论。\n我们无法考虑转向微服务的所有考虑因素。拥有微服务并不意味着您拥有云原生基础设施。如果您想阅读更多，我们推荐 Sam Newman 的 Building Microservices（O’Reilly，2015）。虽然微服务是实现您的应用程序灵活性的一种方式，但正如我们之前所说的，它们不是云原生应用程序的必需条件。\n健康报告  停止逆向工程应用程序并开始从内部进行监控。 —— Kelsey Hightower，Monitorama PDX 2016：healthz\n 没有人比开发人员更了解应用程序需要什么才能以健康的状态运行。很长一段时间，基础设施管理员都试图从他们负责运行的应用程序中找出 “健康” 该怎么定义。如果不实际了解应用程序的健康状况，他们尝试在应用程序不健康时进行监控并发出警报，这往往是脆弱和不完整的。\n为了提高云原生应用程序的可操作性，应用程序应该暴露健康检查。开发人员可以将其实施为命令或过程信号，以便应用程序在执行自我检查之后响应，或者更常见的是：通过应用程序提供 Web 服务，返回 HTTP 状态码来检查健康状态。\n Google Borg 示例\nGoogle 的 Borg 报告中列出了一个健康报告的例子：\n几乎每个在 Borg 下运行的任务都包含一个内置的 HTTP 服务器，该服务器发布有关任务运行状况和数千个性能指标（如 RPC 延迟）的信息。Borg 会监控运行状况检查 URL 并重新启动不及时响应或返回 HTTP 错误代码的任务。其他数据由监控工具跟踪，用于仪表板和服务级别目标（SLO）违规警报。\n 将健康责任转移到应用程序中使应用程序更容易管理和自动化。应用程序应该知道它是否正常运行以及它依赖于什么（例如，访问数据库）来提供业务价值。这意味着开发人员需要与产品经理合作来定义应用服务的业务功能并相应地编写测试。\n提供健康检查的应用程序示例包括 Zookeeper 的 ruok 命令和 etcd 的 HTTP / 健康端点。\n应用程序不仅仅有健康或不健康的状态。它们将经历一个启动和关闭过程，在这个过程中它们应该通过健康检查，报告它们的状态。如果应用程序可以让平台准确了解它所处的状态，平台将更容易知道如何操作它。\n一个很好的例子就是当平台需要知道应用程序何时可以接收流量。在应用程序启动时，如果它不能正确处理流量，它就应该表现为未准备好。此额外状态将防止应用程序过早终止，因为如果运行状况检查失败，平台可能会认为应用程序不健康，并且会反复停止或重新启动它。\n应用程序健康只是能够自动化应用程序生命周期的一部分。除了知道应用程序是否健康之外，您还需要知道应用程序是否正在进行哪些工作。这些信息来自遥测数据。\n遥测数据 遥测数据是进行决策所需的信息。确实，遥测数据可能与健康报告重叠，但它们有不同的用途。健康报告通知我们应用程序生命周期状态，而遥测数据通知我们应用程序业务目标。\n您测量的指标有时称为服务级指标（SLI）或关键性能指标（KPI）。这些是特定于应用程序的数据，可以确保应用程序的性能处于服务级别目标（SLO）内。如果您需要更多关于这些术语的信息以及它们与您的应用程序、业务需求的关系，我们推荐你阅读来自 Site Reliability Engineering（O’Reilly）的第 4 章。\n遥测和度量标准用于解决以下问题：\n 应用程序每分钟收到多少请求？ 有没有错误？ 什么是应用程序延迟？ 订购需要多长时间？  通常会将数据刮取或推送到时间序列数据库（例如 Prometheus 或 InfluxDB）进行聚合。遥测数据的唯一要求是它将被收集数据的系统格式化。\n至少，可能最好实施度量标准的 RED 方法，该方法收集应用程序的速率，错误和执行时间。\n请求率\n收到了多少个请求\n错误\n应用程序有多少错误\n时间\n多久才能收到回复\n遥测数据应该用于提醒而非健康监测。在动态的、自我修复的环境中，我们更少关注单个应用程序实例的生命周期，更多关注关于整体应用程序 SLO 的内容。健康报告对于自动应用程序管理仍然很重要，但不应该用于页面工程师。\n如果 1 个实例或 50 个应用程序不健康，只要满足应用程序的业务需求，我们可能不会收到警报。度量标准可让您知道您是否符合您的 SLO，应用程序的使用方式以及对于您的应用程序来说什么是 “正常”。警报有助于您将系统恢复到已知的良好状态。\n 如果它移动，我们跟踪它。有时候我们会画出一些尚未移动的图形，以防万一它决定为它运行。\n——Ian Malpass，衡量所有，衡量一切\n 警报也不应该与日志记录混淆。记录用于调试，开发和观察模式。它暴露了应用程序的内部功能。度量有时可以从日志（例如错误率）计算，但需要额外的聚合服务（例如 ElasticSearch）和处理。\n弹性 一旦你有遥测和监测数据，你需要确保你的应用程序对故障有适应能力。弹性是基础设施的责任，但云原生应用程序也需要承担部分工作。\n基础设施被设计为抵制失败。硬件用于需要多个硬盘驱动器，电源以及全天候监控和部件更换以保持应用程序可用。使用云原生应用程序，应用程序有责任接受失败而不是避免失败。\n 在任何平台上，尤其是在云中，最重要的特性是其可靠性。\n——David Rensin，e ARCHITECT Show：来自 Google 的关于云计算的速成课程\n 设计具有弹性的应用程序可能是整本书本身。我们将在云原生应用程序中考虑弹性的两个主要方面：为失败设计和优雅降级。\n为失败设计 唯一永远不会失败的系统是那些让你活着的系统（例如心脏植入物和刹车系统）。如果您的服务永远不会停止运行，您需要花费太多时间设计它们来抵制故障，并且没有足够的时间增加业务价值。您的 SLO 确定服务需要多长时间。您花费在工程设计上超出 SLO 的正常运行时间的任何资源都将被浪费掉。\n您应该为每项服务测量两个值，即平均无故障时 …","date":1651532400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"c620f0b949db9f0203793395c044735f","permalink":"https://jimmysong.io/docs/cloud-native/intro/cloud-native-philosophy/","publishdate":"2022-05-03T00:00:00+01:00","relpermalink":"/docs/cloud-native/intro/cloud-native-philosophy/","section":"cloud-native","summary":"云原生一词已经被过度的采用，很多软件都号称是云原生，很多打着","tags":null,"title":"云原生的设计哲学","type":"book"},{"authors":null,"categories":null,"content":"云原生社区是由 宋净超（Jimmy Song） 于 2020 年 5 月发起的，企业中立的云原生终端用户社区。社区秉持 “共识、共治、共建、共享” 的原则。社区的宗旨是：连接、中立、开源。立足中国，面向世界，企业中立，关注开源，回馈开源。了解更多请访问云原生社区官网：https://cloudnative.to。\n成立背景  Software is eating the world. —— Marc Andreessen\n “软件正在吞噬这个世界” 已被大家多次引用，随着云原生（Cloud Native）的崛起，我们想说的是 “Cloud Native is eating the software”。随着越来越多的企业将服务迁移上云，企业原有的开发模式以及技术架构已无法适应云的应用场景，其正在被重塑，向着云原生的方向演进。\n那么什么是云原生？云原生是一系列架构、研发流程、团队文化的最佳实践组合，以此支撑更快的创新速度、极致的用户体验、稳定可靠的用户服务、高效的研发效率。开源社区与云原生的关系密不可分，正是开源社区尤其是终端用户社区的存在，极大地促进了以容器、服务网格、微服务等为代表的云原生技术的持续演进！\n随着云计算的不断发展，云原生技术在全球范围内变得越来越受关注，同时国内社区同学也展现了对云原生技术热爱。近些年中国已经孕育众多的云原生技术爱好者，也有自发组织的一些相关技术交流和 meetup，同时在云原生领域也涌现了众多优秀的开源项目，在这样的背景下，一个有理想，有组织，有温度的云原生社区应运而生。\n加入社区 加入云原生社区，你将获得：\n 更接近源头的知识资讯 更富有价值的人际网络 更专业个性的咨询解答 更亲近意见领袖的机会 更快速高效的个人成长 更多知识分享曝光机会 更多行业人才挖掘发现  关注云原生社区微信公众号，进入公众号后台，点击 “加入我们”。\n   云原生社区公众号  参考  云原生社区成立 - cloudnative.to  ","date":1651532400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"49ec00026a51b8391bcd5be1769b8259","permalink":"https://jimmysong.io/docs/cloud-native/community/cnc/","publishdate":"2022-05-03T00:00:00+01:00","relpermalink":"/docs/cloud-native/community/cnc/","section":"cloud-native","summary":"云原生社区是由 宋净超（Jimmy Song） 于 2020 年 5 月发起的，","tags":null,"title":"云原生社区（中国）","type":"book"},{"authors":null,"categories":null,"content":"Istio 是一个服务网格的开源实现。Istio 支持以下功能。\n流量管理\n利用配置，我们可以控制服务间的流量。设置断路器、超时或重试都可以通过简单的配置改变来完成。\n可观察性\nIstio 通过跟踪、监控和记录让我们更好地了解你的服务，它让我们能够快速发现和修复问题。\n安全性\nIstio 可以在代理层面上管理认证、授权和通信的加密。我们可以通过快速的配置变更在各个服务中执行政策。\nIstio 组件 Istio 服务网格有两个部分：数据平面和控制平面。\n在构建分布式系统时，将组件分离成控制平面和数据平面是一种常见的模式。数据平面的组件在请求路径上，而控制平面的组件则帮助数据平面完成其工作。\nIstio 中的数据平面由 Envoy 代理组成，控制服务之间的通信。网格的控制平面部分负责管理和配置代理。\n   Istio 架构  Envoy（数据平面） Envoy 是一个用 C++ 开发的高性能代理。Istio 服务网格将 Envoy 代理作为一个 sidecar 容器注入到你的应用容器旁边。然后该代理拦截该服务的所有入站和出站流量。注入的代理一起构成了服务网格的数据平面。\nEnvoy 代理也是唯一与流量进行交互的组件。除了前面提到的功能 —— 负载均衡、断路器、故障注入等。Envoy 还支持基于 WebAssembly（WASM）的可插拔扩展模型。这种可扩展性使我们能够执行自定义策略，并为网格中的流量生成遥测数据。\nIstiod（控制平面） Istiod 是控制平面组件，提供服务发现、配置和证书管理功能。Istiod 采用 YAML 编写的高级规则，并将其转换为 Envoy 的可操作配置。然后，它把这个配置传播给网格中的所有 sidecar。\nIstiod 内部的 Pilot 组件抽象出特定平台的服务发现机制（Kubernetes、Consul 或 VM），并将其转换为 sidecar 可以使用的标准格式。\n使用内置的身份和凭证管理，我们可以实现强大的服务间和终端用户认证。通过授权功能，我们可以控制谁可以访问你的服务。\n控制平面的部分以前被称为 Citadel，作为一个证书授权机构，生成证书，允许数据平面中的代理之间进行安全的 mTLS 通信。\n","date":1651507200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"c5c85a33c0bc35517abbe1c13d23e566","permalink":"https://jimmysong.io/docs/cloud-native/service-mesh/what-is-istio/","publishdate":"2022-05-03T00:00:00+08:00","relpermalink":"/docs/cloud-native/service-mesh/what-is-istio/","section":"cloud-native","summary":"Istio 是一个服务网格的开源实现。Istio 支持以下功能。 流量管理","tags":null,"title":"什么是 Istio?","type":"book"},{"authors":null,"categories":null,"content":"可观测性行业正处在巨变中。\n传统上，我们观察系统时使用的是一套筒仓式的、独立的工具，其中大部分包含结构不良（或完全非结构化）的数据。这些独立的工具也是垂直整合的。工具、协议和数据格式都属于一个特定的后端或服务，不能互换。这意味着更换或采用新的工具需要耗费时间来更换整个工具链，而不仅仅是更换后端。\n这种孤立的技术格局通常被称为可观测性的 “三大支柱”：日志、度量和（几乎没有）追踪（见图 1-1）。\n日志\n记录构成事务的各个事件。\n度量\n记录构成一个事务的事件的集合。\n追踪\n测量操作的延迟和识别事务中的性能瓶颈，或者类似的东西。传统上，许多组织并不使用分布式追踪，许多开发人员也不熟悉它。\n   图 1-1：可观测性的 “三大支柱”。  我们用这种方法工作了很久，以致于我们不常质疑它。但正如我们将看到的，“三大支柱” 并不是一种正确的结构化的可观测性方法。事实上，这个术语只是描述了某些技术碰巧被实现的方式，它掩盖了关于如何实际使用我们的工具的几个基本事实。\n什么是事务和资源？ 在我们深入探讨不同可观测性范式的利弊之前，重要的是要定义我们所观察的是什么。我们最感兴趣的分布式系统是基于互联网的服务。这些系统可以被分解成两个基本组成部分：事务和资源。\n事务（transaction）代表了分布式系统需要执行的所有动作，以便服务能够做一些有用的事情。加载一个网页，购买一个装满物品的购物车，订购一个共享汽车：这些都是事务的例子。重要的是要理解，事务不仅仅是数据库事务。对每个服务的每个请求都是事务的一部分。\n例如，一个事务可能从一个浏览器客户端向一个网络代理发出 HTTP 请求开始。代理首先向认证系统发出请求以验证用户，然后将请求转发给前端应用服务器。应用服务器向各种数据库和后端系统发出若干请求。例如，一个消息系统：Kafka 或 AMQP，被用来排队等待异步处理的额外工作。所有这些工作都必须正确完成，以便向急切等待的用户提供结果。如果任何部分失败或耗时过长，其结果就是糟糕的体验，所以我们需要全面理解事务。\n一路下来，所有这些事务都在消耗资源（resource）。网络服务只能处理这么多的并发请求，然后它们的性能就会下降并开始失效。这些服务可以扩大规模，但它们与可能调用锁的数据库互动，造成瓶颈。数据库读取记录的请求可能会阻碍更新该记录的请求，反之亦然。此外，所有这些资源都是要花钱的，在每个月的月底，你的基础设施供应商会给你发账单。你消耗的越多，你的账单就越长。\n如何解决某个问题或提高服务质量？要么是开发人员修改事务，要么是运维人员修改可用资源。就这样了。这就是它的全部内容。当然，魔鬼就在细节中。\n第四章将详细介绍可观测性工具表示事务和资源的理想方式。但是，让我们回到描述迄今为止我们一直在做的非理想的方式。\n 不是所有事情都是事务\n请注意，除了跨事务之外，还有其他的计算模型。例如，桌面和移动应用程序通常是基于反应器（reactor）模型，用户在很长一段时间内连续互动。照片编辑和视频游戏就是很好的例子，这些应用有很长的用户会话，很难描述为离散的事务。在这些情况下，基于事件的可观测性工具，如真实用户监控（RUM），可以增强分布式追踪，以更好地描述这些长期运行的用户会话。\n也就是说，几乎所有基于互联网的服务都是建立在事务模式上的。由于这些是我们关注的服务，观察事务是我们在本书中描述的内容。附录 B 中对 RUM 进行了更详细的描述。\n 可观测性的三大支柱 考虑到事务和资源，让我们来看看三大支柱模式。将这种关注点的分离标记为 “三大支柱”，听起来是有意的，甚至是明智的。支柱听起来很严肃，就像古代雅典的帕特农神庙。但这种安排实际上是一个意外。计算机的可观测性由多个孤立的、垂直整合的工具链组成，其真正的原因只是一个平庸的故事。\n这里有一个例子。假设有一个人想了解他们的程序正在执行的事务。所以他建立了一个日志工具：一个用于记录包含时间戳的消息的接口，一个用于将这些消息发送到某个地方的协议，以及一个用于存储和检索这些消息的数据系统。这足够简单。\n另一个人想监测在任何特定时刻使用的所有资源；他想捕捉指标。那么，他不会为此使用日志系统，对吗？他想追踪一个数值是如何随时间变化的，并跨越一组有限的维度。很明显，一大堆非结构化的日志信息与这个问题没有什么关系。因此，一个新的、完全独立的系统被创造出来，解决了生成、传输和存储度量的具体问题。\n另一个人想要识别性能瓶颈。同样，日志系统的非结构化性质使它变得无关紧要。识别性能瓶颈，比如一连串可以并行运行的操作，需要我们知道事务中每个操作的持续时间以及这些操作是如何联系在一起的。因此，我们建立了一个完全独立的追踪系统。由于新的追踪系统并不打算取代现有的日志系统，所以追踪系统被大量抽样，以限制在生产中运行第三个可观测系统的成本。\n这种零敲碎打的可观测性方法是人类工程的一个完全自然和可理解的过程。然而，它有其局限性，不幸的是，这些局限性往往与我们在现实世界中使用（和管理）这些系统的方式相悖。\n实际上我们如何观察系统？ 让我们来看看运维人员在调查问题时所经历的实际的、不加修饰的过程。\n调查包括两个步骤：注意到有事情发生，然后确定是什么原因导致了它的发生。\n当我们执行这些任务时，我们使用可观测性工具。但是，重要的是，我们并不是孤立地使用每一个工具。我们把它们全部放在一起使用。而在整个过程中，这些工具的孤立性给操作者带来了巨大的认知负担。\n通常情况下，当有人注意到一个重要的指标**变得歪七扭八时，**调查就开始了。在这种情况下，“毛刺” 是一个重要的术语，因为运维人员在这一点上所掌握的唯一信息是仪表盘上一条小线的形状，以及他们自己对该线的形状是否看起来 “正确” 的内部评估（图 1-2）。\n   图 1-2：传统上，寻找不同数据集之间的关联性是一种可怕的经历。  在确定了线的形状开始看起来 “不对劲” 的那一点后，运维人员将眯起眼睛，试图找到仪表板上同时出现 “毛刺” 的其他线。然而，由于这些指标是完全相互独立的，运维人员必须在他们的大脑中进行比较，而不需要计算机的帮助。\n不用说，盯着图表，希望找到一个有用的关联，需要时间和脑力，更不用说会导致眼睛疲劳。\n就个人而言，我会用一把尺子或一张纸，只看什么东西排成一排。在 “现代” 仪表盘中，标尺现在是作为用户界面的一部分而绘制的线。但这只是一个粗略的解决方案。识别相关性的真正工作仍然必须发生在操作者的头脑中，同样没有计算机的帮助。\n在对问题有了初步的、粗略的猜测后，运维人员通常开始调查他们认为可能与问题有关的事务（日志）和资源（机器、进程、配置文件）（图 1-3）。\n   图 1-3：找到与异常情况相关的日志也是一种可怕的经历。  在这里，计算机也没有真正的帮助。日志存储在一个完全独立的系统中，不能与任何指标仪表板自动关联。配置文件和其他服务的具体信息通常不在任何系统中，运维人员必须通过 SSH 或其他方式访问运行中的机器来查看它们。\n因此，运维人员再次被留下寻找相关性的工作，这次是在指标和相关日志之间。识别这些日志可能很困难；通常必须查阅源代码才能了解可能存在的日志。\n当找到一个（可能是，希望是）相关的日志，下一步通常是确定导致这个日志产生的事件链。这意味着要找到同一事务中的其他日志。\n缺乏关联性给操作者带来了巨大的负担。非结构化和半结构化的日志系统没有自动索引和按事务过滤日志的机制。尽管这是迄今为止运维人员最常见的日志工作流程，但他们不得不执行一系列特别的查询和过滤，将可用的日志筛选成一个子集，希望能代表事务的近似情况。为了成功，他们必须依靠应用程序开发人员来添加各种请求 ID 和记录，以便日后找到并拼接起来。\n在一个小系统中，这种重建事务的过程是乏味的，但却是可能的。但是一旦系统发展到包括许多横向扩展的服务，重建事务所需的时间就开始严重限制了调查的范围。图 1-4 显示了一个涉及许多服务的复杂事务。你将如何收集所有的日志？\n   图 1-4：在传统的日志记录中，找到构成一个特定事务的确切日志需要花费大量的精力。如果系统变得足够大，这几乎成为不可能。  分布式追踪是一个好的答案。它实际上拥有自动重建一个事务所需的所有 ID 和索引工具。不幸的是，追踪系统经常被看作是用于进行延迟分析的利基工具。因此，发送给它们的日志数据相对较少。而且，由于它们专注于延迟分析，追踪系统经常被大量采样，使得它们与这类调查无关。\n不是三根柱子，而是一股绳子 不用说，这是一个无奈之举。上述工作流程确实代表了一种可怕的状态。但是，由于我们已经在这种技术体制下生活了这么久，我们往往没有认识到，与它可以做到的相比，它实际上是多么低效。\n今天，为了了解系统是如何变化的，运维人员必须首先收集大量的数据。然后，他们必须根据仪表盘显示和日志扫描等视觉反馈，用他们的头脑来识别这些数据的相关性。这是一种紧张的脑力劳动。如果一个计算机程序能够自动扫描和关联这些数据，那么这些脑力劳动就是不必要的。如果运维人员能够专注于调查他们的系统是如何变化的，而不需要首先确定什么在变化，那么他们将节省大量宝贵的时间。\n在编写一个能够准确执行这种变化分析的计算机程序之前，所有这些数据点都需要被连接起来。日志需要被连接在一起，以便识别事务。衡量标准需要与日志联系在一起，这样产生的统计数据就可以与它们所测量的事务联系起来。每个数据点都需要与底层系统资源 —— 软件、基础设施和配置细节相关联，以便所有事件都能与整个系统的拓扑结构相关联。\n最终的结果是一个单一的、可遍历的图，包含了描述分布式系统状态所需的所有数据，这种类型的数据结构将给分析工具一个完整的系统视图。与其说是不相连的数据的 “三根支柱”，不如说是相互连接的数据的一股绳子。\nOpenTelemetry 因此诞生。如图 1-5 所示，OpenTelemetry 是一个新的遥测系统，它以一种综合的方式生成追踪、日志和指标。所有这些连接的数据点以相同的协议一起传输，然后可以输入计算机程序，以确定整个数据集的相关性。\n   图 1-5：OpenTelemetry 将所有这些孤立的信息，作为一个单一的、高度结构化的数据流连接起来。  这种统一的数据是什么样子的？在下一章，我们将把这三个支柱放在一边，从头开始建立一个新的模型。\n","date":1643990400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"2abc151a6ddc9d594b442810ef231cfe","permalink":"https://jimmysong.io/docs/opentelemetry-obervability/history/","publishdate":"2022-02-05T00:00:00+08:00","relpermalink":"/docs/opentelemetry-obervability/history/","section":"opentelemetry-obervability","summary":"第 1 章：可观测性的历史","tags":null,"title":"第 1 章：可观测性的历史","type":"book"},{"authors":null,"categories":null,"content":"注意：在考虑这些要点时，请谨记 “Code Review 标准”。\n设计 审查中最重要的是 CL 的整体设计。CL 中各种代码的交互是否有意义？此变更是属于您的代码库（codebase）还是属于库（library）？它是否与您系统的其他部分很好地集成？现在是添加此功能的好时机吗？\n功能 这个 CL 是否符合开发者的意图？开发者的意图对代码的用户是否是好的？ “用户”通常都是最终用户（当他们受到变更影响时）和开发者（将来必须“使用”此代码）。\n大多数情况下，我们希望开发者能够很好地测试 CL，以便在审查时代码能够正常工作。但是，作为审查者，仍然应该考虑边缘情况，寻找并发问题，尝试像用户一样思考，并确保您单纯透过阅读方式审查时，代码没有包含任何 bug。\n当要检查 CL 的行为会对用户有重大影响时，验证 CL 的变化将变得十分重要。例如 UI 变更。当您只是阅读代码时，很难理解某些变更会如何影响用户。如果在 CL 中打 patch 或自行尝试这样的变更太不方便，您可以让开发人员为您提供功能演示。\n另一个在代码审查期间特别需要考虑功能的时机，就是如果 CL 中存在某种并行编程，理论上可能导致死锁或竞争条件。通过运行代码很难检测到这些类型的问题，并且通常需要某人（开发者和审查者）仔细思考它们以确保不会引入问题。 （请注意，这也是在可能出现竞争条件或死锁的情况下，不使用并发模型的一个很好的理由——它会使代码审查或理解代码变得非常复杂。）\n复杂度 CL 是否已经超过它原本所必须的复杂度？针对任何层级的 CL 请务必确认这点——每行程序是否过于复杂？ 功能太复杂了吗？类太复杂了吗？ “太复杂”通常意味着阅读代码的人无法快速理解。也可能意味着“开发者在尝试调用或修改此代码时可能会引入错误。”\n其中一种复杂性就是过度工程（over-engineering），如开发人员使代码过度通用，超过它原本所需的，或者添加系统当前不需要的功能。审查者应特别警惕过度工程。未来的问题应该在它实际到达后解决，且届时才能更清晰的看到其真实样貌及在现实环境里的需求，鼓励开发人员解决他们现在需要解决的问题，而不是开发人员推测可能需要在未来解决的问题。\n测试 将要求单元、集成或端到端测试视为应该做的适当变更。通常，除非 CL 处理紧急情况，否则应在与生产代码相同的 CL 中添加测试。\n确保 CL 中的测试正确，合理且有用。测试并非用来测试自己本身，且我们很少为测试编写测试——人类必须确保测试有效。\n当代码被破坏时，测试是否真的会失败？ 如果代码发生变化时，它们会开始产生误报吗？ 每个测试都会做出简单而有用的断言吗？ 不同测试方法的测试是否适当分开？\n请记住，测试也是必须维护的代码。不要仅仅因为它们不是主二进制文件的一部分而接受测试中的复杂性。\n命名 开发人员是否为所有内容选择了好名字？ 一个好名字应该足够长，可以完全传达项目的内容或作用，但又不会太长，以至于难以阅读。\n注释 开发者是否用可理解的英语撰写了清晰的注释？所有注释都是必要的吗？通常，注释解释为什么某些代码存在时很有用，且不应该用来解释某些代码正在做什么。如果代码无法清楚到去解释自己时，那么代码应该变得更简单。有一些例外（正则表达式和复杂算法通常会从解释他们正在做什么事情的注释中获益很多），但大多数注释都是针对代码本身可能无法包含的信息，例如决策背后的推理。\n查看此 CL 之前的注释也很有帮助。 也许有一个 TODO 现在可以删除，一个注释建议不要进行这种改变，等等。\n请注意，注释与类、模块或函数的文档不同，它们应该代表一段代码的目的，如何使用它，以及使用时它的行为方式。\n风格 Google 提供了所有主要语言的风格指南，甚至包括大多数小众语言。确保 CL 遵循适当的风格指南。\n如果您想改进风格指南中没有的一些样式点，请在评论前加上“Nit：”，让开发人员知道这是您认为可改善代码的小瑕疵，但不是强制性的。不要仅根据个人风格偏好阻止提交 CL。\nCL 的作者不应在主要风格变更中，包括与其他种类的变更。它会使得很难看到 CL 中的变更了什么，使合并和回滚更复杂，并导致其他问题。例如，如果作者想要重新格式化整个文件，让他们只将重新格式化变为一个 CL，其后再发送另一个包含功能变更的 CL。\n文档 如果 CL 变更了用户构建、测试、交互或发布代码的方式，请检查相关文档是否有更新，包括 README、g3doc 页面和任何生成的参考文档。如果 CL 删除或弃用代码，请考虑是否也应删除文档。 如果缺少文档，请询问。\n每一行 查看分配给您审查的每行代码。有时如数据文件、生成的代码或大型数据结构等东西，您可以快速扫过。但不要快速扫过人类编写的类、函数或代码块，并假设其中的内容是 OK 的。显然，某些代码需要比其他代码更仔细的审查——这是您必须做出的判断——但您至少应该确定您理解所有代码正在做什么。\n如果您觉得这些代码太难以阅读了并减慢您审查的速度，您应该在您尝试继续审核前要让开发者知道这件事，并等待他们为程序做出解释、澄清。在 Google，我们聘请了优秀的软件工程师，您就是其中之一。如果您无法理解代码，那么很可能其他开发人员也不会。因此，当您要求开发人员澄清此代码时，您也会帮助未来的开发人员理解这些代码。\n如果您了解代码但觉得没有资格做某些部分的审查，请确保 CL 上有一个合格的审查人，特别是对于安全性、并发性、可访问性、国际化等复杂问题。\n上下文 在广泛的上下文下查看 CL 通常很有帮助。通常，代码审查工具只会显示变更的部分的周围的几行。有时您必须查看整个文件以确保变更确实有意义。例如，您可能只看到添加了四行新代码，但是当您查看整个文件时，您会看到这四行是添加在一个 50 行的方法里，现在确实需要将它们分解为更小的方法。\n在整个系统的上下文中考虑 CL 也很有用。 这个 CL 是否改善了系统的代码健康状况，还是使整个系统更复杂，测试更少等等？不要接受降低系统代码运行状况的 CL。大多数系统通过许多小的变化而变得复杂，因此防止新变更引入即便很小的复杂性也非常重要。\n好的事情 如果您在 CL 中看到一些不错的东西，请告诉开发者，特别是当他们以一种很好的方式解决了您的的一个评论时。代码审查通常只关注错误，但也应该为良好实践提供鼓励。在指导方面，比起告诉他们他们做错了什么，有时更有价值的是告诉开发人员他们做对了什么。\n总结 在进行代码审查时，您应该确保：\n 代码设计精良。 该功能对代码用户是有好处的。 任何 UI 变更都是合理的且看起来是好的。 其中任何并行编程都是安全的。 代码并不比它需要的复杂。 开发人员没有实现他们将来可能需要，但不知道他们现在是否需要的东西。 代码有适当的单元测试。 测试精心设计。 开发人员使用了清晰的名称。 评论清晰有用，且大多用来解释为什么而不是做什么。 代码有适当记录成文件（通常在 g3doc 中）。 代码符合我们的风格指南。  确保查看您被要求查看的每一行代码，查看上下文，确保您提高代码健康状况，并赞扬开发人员所做的好事。\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"68dfb49f9d93687fe9f00cf57b32d625","permalink":"https://jimmysong.io/docs/eng-practices/review/reviewer/looking-for/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/docs/eng-practices/review/reviewer/looking-for/","section":"eng-practices","summary":"注意：在考虑这些要点时，请谨记 “Code Review 标准”。 设计 审查中","tags":null,"title":"Code Review 要点","type":"book"},{"authors":null,"categories":null,"content":"为什么提交小型 CL? 小且简单的 CL 是指：\n 审查更快。审查者更容易抽多次五分钟时间来审查小型 CL，而不是留出 30 分钟来审查一个大型 CL。 审查得更彻底。如果是大的变更，审查者和提交者往往会因为大量细节的讨论翻来覆去而感到沮丧——有时甚至到了重要点被遗漏或丢失的程度。 不太可能引入错误。 由于您进行的变更较少，您和您的审查者可以更轻松有效地推断 CL 的影响，并查看是否已引入错误。 如果被拒绝，减少浪费的工作。 如果您写了一个巨大的 CL，您的评论者说整个 CL 的方向都错误了，你就浪费了很多精力和时间。 更容易合并。 处理大型 CL 需要很长时间，在合并时会出现很多冲突，并且必须经常合并。 更容易设计好。 打磨一个小变更的设计和代码健康状况比完善一个大变更的所有细节要容易得多。 减少对审查的阻碍。 发送整体变更的自包含部分可让您在等待当前 CL 审核时继续编码。 更简单的回滚。 大型 CL 更有可能触及在初始 CL 提交和回滚 CL 之间更新的文件，从而使回滚变得复杂（中间的 CL 也可能需要回滚）。  请注意，审查者可以仅凭 CL 过大而自行决定完全拒绝您的变更。通常他们会感谢您的贡献，但要求您以某种方式将其 CL 改成一系列较小的变更。在您编写完变更后，或者需要花费大量时间来讨论为什么审查者应该接受您的大变更，这可能需要做很多工作。首先编写小型 CL 更容易。\n什么是小型 CL？ 一般来说，CL 的正确大小是自包含的变更。这意味着：\n CL 进行了一项最小的变更，只解决了一件事。通常只是功能的一部分，而不是一个完整的功能。一般来说，因为编写过小的 CL 而犯错也比过大的 CL 犯错要好。与您的审查者讨论以确定可接受的大小。 审查者需要了解的关于 CL 的所有内容（除了未来的开发）都在 CL 的描述、现有的代码库或已经审查过的 CL 中。 对其用户和开发者来说，在签入 CL 后系统能继续良好的工作。 CL 不会过小以致于其含义难以理解。如果您添加新 API，则应在同一 CL 中包含 API 的用法，以便审查者可以更好地了解 API 的使用方式。这也可以防止签入未使用的 API。  关于多大算“太大”没有严格的规则。对于 CL 来说，100 行通常是合理的大小，1000 行通常太大，但这取决于您的审查者的判断。变更中包含的文件数也会影响其“大小”。一个文件中的 200 行变更可能没问题，但是分布在 50 个文件中通常会太大。\n请记住，尽管从开始编写代码开始就您就已经密切参与了代码，但审查者通常不清楚背景信息。对您来说，看起来像是一个可接受的大小的 CL 对您的审查者来说可能是压倒性的。如有疑问，请编写比您认为需要编写的要小的 CL。审查者很少抱怨收到过小的 CL 提交。\n什么时候大 CL 是可以的？ 在某些情况下，大变更也是可以接受的：\n 您通常可以将整个文件的删除视为一行变更，因为审核人员不需要很长时间审核。 有时一个大的 CL 是由您完全信任的自动重构工具生成的，而审查者的工作只是检查并确定想要这样的变更。但这些 CL 可以更大，尽管上面的一些警告（例如合并和测试）仍然适用。  按文件拆分 拆分 CL 的另一种方法是对文件进行分组，这些文件需要不同的审查者，否则就是自包含的变更。\n例如：您发送一个 CL 以修改协议缓冲区，另一个 CL 发送变更使用该原型的代码。您必须在代码 CL 之前提交 proto CL，但它们都可以同时进行审查。如果这样做，您可能希望通知两组审查者审查您编写的其他 CL，以便他们对您的变更具有更充足的上下文。\n另一个例子：你发送一个 CL 用于代码更改，另一个用于使用该代码的配置或实验；如果需要，这也更容易回滚，因为配置/实验文件有时会比代码变更更快地推向生产。\n分离出重构 通常最好在功能变更或错误修复的单独 CL 中进行重构。例如，移动和重命名类应该与修复该类中的错误的 CL 不同。审查者更容易理解每个 CL 在单独时引入的更改。\n但是，修复本地变量名称等小清理可以包含在功能变更或错误修复 CL 中。如果重构大到包含在您当前的 CL 中，会使审查更加困难的话，需要开发者和审查者一起判断是否将其拆开。\n将相关的测试代码保存在同一个 CL 中 避免将测试代码拆分为单独的 CL。验证代码修改的测试应该进入相同的 CL，即使它增加了代码行数。\n但是，独立的测试修改可以首先进入单独的 CL，类似于重构指南。包括：\n 使用新测试验证预先存在的已提交代码。 重构测试代码（例如引入辅助函数）。 引入更大的测试框架代码（例如集成测试）。  不要破坏构建 如果您有几个相互依赖的 CL，您需要找到一种方法来确保在每次提交 CL 后整个系统能够继续运作。否则可能会在您的 CL 提交的几分钟内打破所有开发人员的构建（如果您之后的 CL 提交意外出错，时间可能会甚至更长）。\n如果不能让它足够小 有时你会遇到看起来您的 CL 必须如此庞大，但这通常很少是正确的。习惯于编写小型 CL 的提交者几乎总能找到将功能分解为一系列小变更的方法。\n在编写大型 CL 之前，请考虑在重构 CL 之前是否可以为更清晰的实现铺平道路。与你的同伴聊聊，看看是否有人想过如何在小型 CL 中实现这些功能。\n如果以上的努力都失败了（这应该是非常罕见的），那么请在事先征得审查者的同意后提交大型 CL，以便他们收到有关即将发生的事情的警告。在这种情况下，做好完成审查过程需要很长一段时间的准备，对不引入错误保持警惕，并且在编写测试时要更下功夫。\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"b422b77d1170880fd9dc9dc3bdbbe90c","permalink":"https://jimmysong.io/docs/eng-practices/review/developer/small-cls/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/docs/eng-practices/review/developer/small-cls/","section":"eng-practices","summary":"为什么提交小型 CL? 小且简单的 CL 是指： 审查更快。审查者更容易抽多","tags":null,"title":"小型 CL","type":"book"},{"authors":null,"categories":null,"content":"在这一节中，我们将解释 Envoy 的基本构建模块。\nEnvoy 配置的根被称为引导配置。它包含了一些字段，我们可以在这里提供静态或动态的资源和高级别的 Envoy 配置（例如，Envoy 实例名称、运行时配置、启用管理界面等等）。\n为了开始学习，我们将主要关注静态资源，在课程的后面，我们将介绍如何配置动态资源。\nEnvoy 输出许多统计数据，这取决于启用的组件和它们的配置。我们会在整个课程中提到不同的统计信息，在课程后面的专门模块中，我们会更多地讨论统计信息。\n下图显示了通过这些概念的请求流。\n   Envoy 构建块  这一切都从监听器开始。Envoy 暴露的监听器是命名的网络位置，可以是一个 IP 地址和一个端口，也可以是一个 Unix 域套接字路径。Envoy 通过监听器接收连接和请求。考虑一下下面的 Envoy 配置。\nstatic_resources:listeners:- name:listener_0address:socket_address:address:0.0.0.0port_value:10000filter_chains:[{}]通过上面的 Envoy 配置，我们在 0.0.0.0 地址的 10000 端口上声明了一个名为 listener_0 的监听器。这意味着 Envoy 正在监听 0.0.0.0:10000 的传入请求。\n每个监听器都有不同的部分需要配置。然而，唯一需要的设置是地址。上述配置是有效的，你可以用它来运行 Envoy—— 尽管它没有用，因为所有的连接都会被关闭。\n我们让 filter_chains 字段为空，因为在接收数据包后不需要额外的操作。\n为了进入下一个构件（路由），我们需要创建一个或多个网络过滤器链（filter_chains），至少要有一个过滤器。\n网络过滤器通常对数据包的有效载荷进行操作，查看有效载荷并对其进行解析。例如，Postgres 网络过滤器解析数据包的主体，检查数据库操作的种类或其携带的结果。\nEnvoy 定义了三类过滤器：监听器过滤器、网络过滤器和 HTTP 过滤器。监听器过滤器在收到数据包后立即启动，通常对数据包的头信息进行操作。监听器过滤器包括代理监听器过滤器（提取 PROXY 协议头），或 TLS 检查器监听器过滤器（检查流量是否为 TLS，如果是，则从 TLS 握手中提取数据）。\n每个通过监听器进来的请求可以流经多个过滤器。我们还可以写一个配置，根据传入的请求或连接属性选择不同的过滤器链。\n   过滤器链  一个特殊的、内置的网络过滤器被称为 HTTP 连接管理器过滤器（HTTP Connection Manager Filter）或 HCM。HCM 过滤器能够将原始字节转换为 HTTP 级别的消息。它可以处理访问日志，生成请求 ID，操作头信息，管理路由表，并收集统计数据。我们将在以后的课程中对 HCM 进行更详细的介绍。\n就像我们可以为每个监听器定义多个网络过滤器（其中一个是 HCM）一样，Envoy 也支持在 HCM 过滤器中定义多个 HTTP 级过滤器。我们可以在名为 http_filters 的字段下定义这些 HTTP 过滤器。\n   HCM 过滤器  HTTP 过滤器链中的最后一个过滤器必须是路由器过滤器（envoy.filters.HTTP.router）。路由器过滤器负责执行路由任务。这最终把我们带到了第二个构件 —— 路由。\n我们在 HCM 过滤器的 route_config 字段下定义路由配置。在路由配置中，我们可以通过查看元数据（URI、Header 等）来匹配传入的请求，并在此基础上，定义流量的发送位置。\n路由配置中的顶级元素是虚拟主机。每个虚拟主机都有一个名字，在发布统计数据时使用（不用于路由），还有一组被路由到它的域。\n让我们考虑下面的路由配置和域的集合。\nroute_config:name:my_route_configvirtual_hosts:- name:tetrate_hostsdomains:[\u0026#34;tetrate.io\u0026#34;]routes:...- name:test_hostsdomains:[\u0026#34;test.tetrate.io\u0026#34;,\u0026#34;qa.tetrate.io\u0026#34;]routes:...如果传入请求的目的地是 tetrate.io（即 Host/Authority 标头被设置为其中一个值），则 tetrate_hosts  虚拟主机中定义的路由将得到处理。\n同样，如果 Host/Authority 标头包含 test.tetrate.io 或 qa.tetrate.io，test_hosts 虚拟主机下的路由将被处理。使用这种设计，我们可以用一个监听器（0.0.0.0:10000）来处理多个顶级域。\n如果你在数组中指定多个域，搜索顺序如下：\n 精确的域名（例如：tetrate.io）。 后缀域名通配符（如 *.tetrate.io）。 前缀域名通配符（例如：tetrate.*）。 匹配任何域的特殊通配符（*）。  在 Envoy 匹配域名后，是时候处理所选虚拟主机中的 routes 字段了。这是我们指定如何匹配一个请求，以及接下来如何处理该请求（例如，重定向、转发、重写、发送直接响应等）的地方。\n我们来看看一个例子。\nstatic_resources:listeners:- name:listener_0address:socket_address:address:0.0.0.0port_value:10000filter_chains:- filters:- name:envoy.filters.network.http_connection_managertyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManagerstat_prefix:hello_world_servicehttp_filters:- name:envoy.filters.http.routerroute_config:name:my_first_routevirtual_hosts:- name:direct_response_servicedomains:[\u0026#34;*\u0026#34;]routes:- match:prefix:\u0026#34;/\u0026#34;direct_response:status:200body:inline_string:\u0026#34;yay\u0026#34;配置的顶部部分与我们之前看到的一样。我们已经添加了 HCM 过滤器、统计前缀（hello_world_service）、单个 HTTP 过滤器（路由器）和路由配置。\n在虚拟主机内，我们要匹配任何域名。在 routes 下，我们匹配前缀（/），然后我们可以发送一个响应。\n当涉及到匹配请求时，我们有多种选择。\n   路由匹配 描述 示例     prefix 前缀必须与:path 头的开头相符。 /hello 与 hello.com/hello、hello.com/helloworld 和 hello.com/hello/v1 匹配。   path 路径必须与:path 头完全匹配。 /hello 匹配 hello.com/hello，但不匹配 hello.com/helloworld 或 hello.com/hello/v1   safe_regex 所提供的正则表达式必须与:path 头匹配。 /\\{3} 匹配任何以 / 开头的三位数。例如，与 hello.com/123 匹配，但不能匹配 hello.com/hello 或 hello.com/54321。   connect_matcher 匹配器只匹配 CONNECT 请求。     一旦 Envoy 将请求与路由相匹配，我们就可以对其进行路由、重定向或返回一个直接响应。在这个例子中，我们通过 direct_response 配置字段使用直接响应。\n你可以把上述配置保存到 envoy-direct-response.yaml 中。\n我们将使用一个名为 func-e 的命令行工具。func-e 允许我们选择和使用不同的 Envoy 版本。\n我们可以通过运行以下命令下载 func-e CLI。\ncurl https://func-e.io/install.sh | sudo bash -s -- -b /usr/local/bin 现在我们用我们创建的配置运行 Envoy。\nfunc-e run -c envoy-direct-response.yaml 一旦 Envoy 启动，我们就可以向 localhost:10000 发送一个请求，以获得我们配置的直接响应。\n$ curl localhost:10000 yay 同样，如果我们添加一个不同的主机头（例如 -H \u0026#34;Host: hello.com\u0026#34;）将得到相同的响应，因为 hello.com 主机与虚拟主机中定义的域相匹配。\n在大多数情况下，从配置中直接发送响应是一个很好的功能，但我们会有一组端点或主机，我们将流量路由到这些端点或主机。在 Envoy 中做到这一点的方法是通过定义集群。\n集群（Cluster）是一组接受流量的上游类似主机。这可以是你的服务所监听的主机或 IP 地址的列表。\n例如，假设我们的 hello world 服务是在 127.0.0.0:8000 上监听。然后，我们可以用一个单一的端点创建一个集群，像这样。\nclusters:- name:hello_world_serviceload_assignment:cluster_name:hello_world_serviceendpoints:- lb_endpoints:- endpoint:address:socket_address:address:127.0.0.1port_value:8000集群的定义与监听器的定义在同一级别，使用 clusters 字段。我们在路由配置中引用集群时，以及在导出统计数据时，都会使用集群。该名称在所有集群中必须是唯一的。\n在 load_assignment 字段下，我们可以定义要进行负载均衡的端点列表，以及负载均衡策略设置。\nEnvoy 支持多种负载均衡算法（round-robin、Maglev、least-request、random），这些算法是由静态引导配置、DNS、动态 xDS（CDS 和 EDS 服务）以及主动 / 被动健康检查共同配置的。如果我们没有通过 lb_policy 字段明确地设置负载均衡算法，它默认为 round-robin。\nendpoints 字段定义了一组属于特定地域的端点。使用可选的 locality 字段，我们可以指定上游主机的运行位置，然后在负载均衡过程中使用（即，将请求代理到离调用者更近的端点）。\n添加新的端点指示负载均衡器在一个以上的接收者之间分配流量。通常情况下，负载均衡器对所有端点一视同仁，但集群定义允许在端点内建立一个层次结构。\n例如，端点可以有一个 权重（weight） 属性，这将指示负载均衡器与其他端点相比，向这些端点发送更多 / 更少的流量。\n另一种层次结构类型是基于地域性的（locality），通常用于定义故障转移架构。这种层次结构允许我们定义地理上比较接近的 “首选” 端点，以及在 “首选” 端点变得不健康的情况下应该使用的 “备份” 端点。\n由于我们只有一个端点，所以我们还没有设置 locality。在 lb_endpoints 字段下，可以定义 Envoy 可以路由流量的实际端点。\n我们可以在 Cluster 中配置以下可选功能：\n 主动健康检查（health_checks） 断路器 (circuit_breakers) 异常点检测（outlier_detection） 在处理上游的 HTTP 请求时有额外的协议选项 一组可选的网络过滤器，应用于所有出站连接等  和监 …","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"b5b6eefc1bd51b60edc27e4b851d5743","permalink":"https://jimmysong.io/docs/envoy-handbook/intro/envoy-building-blocks/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/envoy-handbook/intro/envoy-building-blocks/","section":"envoy-handbook","summary":"在这一节中，我们将解释 Envoy 的基本构建模块。 Envoy 配置的根被称为引导","tags":null,"title":"Envoy 的构建模块","type":"book"},{"authors":null,"categories":null,"content":"Kubernetes® 是一个开源系统，可以自动部署、扩展和管理在容器中运行的应用程序，并且通常托管在云环境中。与传统的单体软件平台相比，使用这种类型的虚拟化基础设施可以提供一些灵活性和安全性的好处。然而，安全地管理从微服务到底层基础设施的所有方面，会引入其他的复杂性。本报告中详述的加固指导旨在帮助企业处理相关风险并享受使用这种技术的好处。\nKubernetes 中三个常见的破坏源是供应链风险、恶意威胁者和内部威胁。\n供应链风险往往是具有挑战性的，可以在容器构建周期或基础设施收购中出现。恶意威胁者可以利用 Kubernetes 架构的组件中的漏洞和错误配置，如控制平面、工作节点或容器化应用程序。内部威胁可以是管理员、用户或云服务提供商。对组织的 Kubernetes 基础设施有特殊访问权的内部人员可能会滥用这些特权。\n本指南描述了与设置和保护 Kubernetes 集群有关的安全挑战。包括避免常见错误配置的加固策略，并指导国家安全系统的系统管理员和开发人员如何部署 Kubernetes，并提供了建议的加固措施和缓解措施的配置示例。本指南详细介绍了以下缓解措施：\n 扫描容器和 Pod 的漏洞或错误配置。 以尽可能少的权限运行容器和 Pod。 使用网络隔离来控制漏洞可能造成的损害程度。 使用防火墙来限制不需要的网络连接，并使用加密技术来保护机密。 使用强大的认证和授权来限制用户和管理员的访问，以及限制攻击面。  使用日志审计，以便管理员可以监控活动，并对潜在的恶意活动发出警告。\n定期审查所有 Kubernetes 设置，并使用漏洞扫描，以帮助确保风险得到适当考虑并应用安全补丁。\n有关其他安全加固指导，请参见互联网安全中心 Kubernetes 基准、Docker 和 Kubernetes 安全技术实施指南、网络安全和基础设施安全局（CISA）分析报告以及 Kubernetes 文档。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"c5a39f5eebb33101c46b6bb37c408814","permalink":"https://jimmysong.io/docs/kubernetes-hardening-guidance/executive-summary/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/kubernetes-hardening-guidance/executive-summary/","section":"kubernetes-hardening-guidance","summary":"Kubernetes® 是一个开源系统，可以自动部署、扩展和管","tags":null,"title":"执行摘要","type":"book"},{"authors":null,"categories":null,"content":"本文参考的是 OAM 规范中对云原生应用的定义，并做出了引申。\n云原生应用是一个相互关联但又不独立的组件（service、task、worker）的集合，这些组件与配置结合在一起并在适当的运行时实例化后，共同完成统一的功能目的。\n云原生应用模型 下图是 OAM 定义的云原生应用模型示意图，为了便于理解，图中相同颜色的部分为同一类别的对象定义。\n   云原生应用模型  OAM 的规范中定义了以下对象，它们既是 OAM 规范中的基本术语也是云原生应用的基本组成。\n Workload（工作负载）：应用程序的工作负载类型，由平台提供。 Component组件）：定义了一个 Workload 的实例，并以基础设施中立的术语声明其运维特性。 Trait（特征）：用于将运维特性分配给组件实例。 ApplicationScope（应用作用域）：用于将组件分组成具有共同特性的松散耦合的应用。 ApplicationConfiguration（应用配置）：描述 Component 的部署、Trait 和 ApplicationScope。  关注点分离 下图是不同角色对于该模型的关注点示意图。\n   云原生应用模型中的目标角色  我们可以看到对于一个云原生应用来说，不同的对象是由不同的角色来负责的：\n 基础设施运维：提供不同的 Workload 类型供开发者使用； 应用运维：定义适用于不同 Workload 的运维属性 Trait 和管理 Component 的 ApplicationScope 即作用域； 应用开发者：负责应用组件 Component 的定义； 应用开发者和运维：共同将 Component 与运维属性 Trait 绑定在一起，维护应用程序的生命周期；  基于 OAM 中的对象定义的云原生应用可以充分利用平台能力自由组合，开发者和运维人员的职责可以得到有效分离，组件的复用性得到大幅提高。\n定义标准 CNCF 中的有几个定义标准的「开源项目」，其中有的项目都已经毕业。\n SMI（Service Mesh Interface）：服务网格接口 Cloud Events：Serverless 中的事件标准 TUF：更新框架标准 SPIFFE：身份安全标准  这其中唯独没有应用定义标准，CNCF SIG App delivery 即是要做这个的。当然既然要指定标准，自然要对不同平台和场景的逻辑做出更高级别的抽象（这也意味着你在掌握了底层逻辑的情况下还要学习更多的概念），这样才能屏蔽底层差异。\nOAM 简介 OAM 全称是 Open Application Model，从名称上来看它所定义的就是一种模型，同时也实现了基于 OAM 的我认为这种模型旨在定义了云原生应用的标准。\n 开放（Open）：支持异构的平台、容器运行时、调度系统、云供应商、硬件配置等，总之与底层无关 应用（Application）：云原生应用 模型（Model）：定义标准，以使其与底层平台无关  既然要制定标准，自然要对不同平台和场景的逻辑做出更高级别的抽象（这也意味着你在掌握了底层逻辑的情况下还要学习更多的概念），这样才能屏蔽底层差异。本文将默认底层平台为 Kubernetes。\n 是从管理大量 CRD 中汲取的经验。 业务和研发的沟通成本，比如 YAML 配置中很多字段是开发人员不关心的。  设计原则 OAM 规范的设计遵循了以下原则：\n 关注点分离：根据功能和行为来定义模型，以此划分不同角色的职责， 平台中立：OAM 的实现不绑定到特定平台； 优雅：尽量减少设计复杂性； 复用性：可移植性好，同一个应用程序可以在不同的平台上不加改动地执行； 不作为编程模型：OAM 提供的是应用程序模型，描述了应用程序的组成和组件的拓扑结构，而不关注应用程序的具体实现。  下图是 OAM 规范示意图。\n   OAM 规范示意图  OAM 工作原理 OAM 的工作原理如下图所示（图片引用自孙健波在《OAM: 云原生时代的应用模型与 下一代 DevOps 技术》中的分享）。\n   OAM 的原理  OAM Spec 定义了云原生应用的规范（使用一些 CRD 定义）， KubeVela 可以看做是 OAM 规范的解析器，将应用定义翻译为 Kubernetes 中的资源对象。可以将上图分为三个层次：\n 汇编层：即人工或者使用工具来根据 OAM 规范定义汇编出一个云原生应用的定义，其中包含了该应用的工作负载和运维能力配置。 转义层：汇编好的文件将打包为 YAML 文件，由 KubeVela 或其他 OAM 的实现将其转义为 Kubernetes 或其他云服务（例如 Istio）上可运行的资源对象。 执行层：执行经过转义好的云平台上的资源对象并执行资源配置。  参考  The Open Application Model specification - github.com  ","date":1651532400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"7e9b638a19f6edc05d4848e665905f5a","permalink":"https://jimmysong.io/docs/cloud-native/intro/define-cloud-native-app/","publishdate":"2022-05-03T00:00:00+01:00","relpermalink":"/docs/cloud-native/intro/define-cloud-native-app/","section":"cloud-native","summary":"本文参考的是 OAM 规范中对云原生应用的定义，并做出了引申。 云原生","tags":null,"title":"什么是云原生应用？","type":"book"},{"authors":null,"categories":null,"content":"Kubernetes 一词来自希腊语，意思是 “飞行员” 或 “舵手”。这个名字很贴切，Kubernetes 可以帮助你在波涛汹涌的容器海洋中航行。\nKubernetes 是做什么的？什么是 Docker？什么是容器编排？Kubernetes 是如何工作和扩展的？你可能还有很多其他的问题，本文将一一为你解答。\n这篇文章适合初学者，尤其是那些工作忙碌，没有办法抽出太多时间来了解 Kubernetes 和云原生的开发者们，希望本文可以帮助你进入 Kubernetes 的世界。\n简而言之，Kubernetes 提供了一个平台或工具来帮助你快速协调或扩展容器化应用，特别是在 Docker 容器。让我们深入了解一下这些概念。\n容器和容器化 那么什么是容器呢？\n要讨论容器化首先要谈到虚拟机 (VM)，顾名思义，虚拟机就是可以远程连接的虚拟服务器，比如 AWS 的 EC2 或阿里云的 ECS。\n接下来，假如你要在虚拟机上运行一个网络应用 —— 包括一个 MySQL 数据库、一个 Vue 前端和一些 Java 库，在 Ubuntu 操作系统 (OS) 上运行。你不用熟悉其中的每一个技术 —— 你只要记住，一个应用程序由各种组件、服务和库组成，它们运行在操作系统上。\n现在，将应用程序打包成一个虚拟机镜像，这个镜像中包括了 Ubuntu 操作系统。这使得虚拟机变得非常笨重 —— 通常有几个 G 的大小。\n虚拟机镜像包含了整个操作系统及所有的库，对应用程序来说，这个镜像过于臃肿，其中大部分组件并没有被应用程序直接调用。如果你需要重新创建、备份或扩展这个应用程序，就需要复制整个环境（虚拟机镜像），在新环境中启动应用通常需要几十秒甚至几分钟时间。如果你想单独升级应用中的某个组件，比如说 Vue 应用，就需要重建整个虚拟机镜像。另外，如果你的两个应用依赖同一个底层镜像，升级底层镜像会同时影响这两个应用，而有时候，你只需要升级其中一个应用的依赖而已。这就是所谓的 “依赖陷阱”。\n解决这个问题的办法就是容器。容器是继虚拟机之后更高层次的抽象，在这层抽象中，整个应用程序的每个组件被单独打包成一个个独立的单元，这个单元就是所谓的容器。通过这种方式，可以将代码和应用服务从底层架构中分离出来，实现了完全的可移植性（在任何操作系统或环境上运行应用的能力）。所以在上面的例子中，Ubuntu 操作系统就是一个单元（容器）。MySQL 数据库是另一个容器，Vue 环境和随之而来的库也是一个容器。\n但是，MySQL 数据库是如何自己 “运行” 的？数据库本身肯定也要在操作系统上运行吧？没错！\n更高层次的容器，比如 MySQL 容器，实际上会包含必要的库来与底层的操作系统容器通信和集成。所以你可以把容器看成是整个应用堆栈中的一层，每层都依赖于下层的单元。而这就类似于船舶或港口中集装箱的堆叠方式，每个容器的稳定性都依赖于下面的容器的支持。所以应用容器的核心是一个受控的执行环境。它们允许你从头开始定义整个环境，从操作系统开始，到你要使用的各个版本的库，再到你要添加的代码版本。\n与容器相关的一个重要概念是微服务。将应用程序的各个组件拆分并打包成独立的服务，这样每个组件都可以很容易地被替换、升级、调试。上面的例子中，我们会为 Vue 前端创建一个微服务，为 MySQL 数据库创建另一个微服务，为 Java 中间件部分创建另一个微服务，以此类推。很明显，微服务与容器化是相辅相成的。\n从 Docker 开始 现在你已经对容器有一定了解了吧？Docker 是最常用的容器化工具，也是最流行的容器运行时。\nDocker 开源于 2013 年。用于打包和创建容器，管理基于容器的应用。所有 Linux 发行版、Windows 和 macOS 都支持 Docker。\n还有其他的容器化工具，如 CoreOS rkt、Mesos Containerizer 和 LXC。但是目前，绝大多数的容器化应用都是在 Docker 上运行的。\n再到 Kubernetes 首先，简单介绍一下历史。Kubernetes 是 Google 基于其内部容器调度平台 Borg 的经验开发的。2014 年开源，并作为 CNCF（云原生计算基金会）的核心发起项目。\n那么 Kubernetes 又跟容器是什么关系呢？让我们再回到上面的例子。假设我们的应用爆火，每天的注册用户越来越多。\n现在，我们需要增加后端资源，使浏览我们网站的用户在浏览页面时加载时间不会过长或者超时。最简单的方式就是增加容器的数量，然后使用负载均衡器将传入的负载（以用户请求的形式）分配给容器。\n这样做虽然行之有效，但也只能在用户规模有限的情况下使用。当用户请求达到几十万或几百万时，这种方法也是不可扩展的。你需要管理几十个也许是几百个负载均衡器，这本身就是另一个令人头疼的问题。如果我们想对网站或应用进行任何升级，也会遇到问题，因为负载均衡不会考虑到应用升级的问题。我们需要单独配置每个负载均衡器，然后升级该均衡器所服务的容器。想象一下，当你有 20 个负载均衡器和每周 5 或 6 个小的更新时，你将不得不进行大量的手工劳动。\n我们需要的是一种可以一次性将变更传递给所有受控容器的方法，同时也需要一种可以轻松地调度可用容器的方法，这个过程还必须要是自动化的，这正是 Kubernetes 所做的事情。\n接下来，我们将探讨 Kubernetes 究竟是如何工作的，它的各种组件和服务，以及更多关于如何使用 Kubernetes 来编排、管理和监控容器化环境。为了简单起见，假设我们使用的是 Docker 容器，尽管如前所述，Kubernetes 除了支持 Docker 之外，还支持其他几种容器平台。\nKubernetes 架构和组件 首先，最重要的是你需要认识到 Kubernetes 利用了 “期望状态” 原则。就是说，你定义了组件的期望状态，而 Kubernetes 要将它们始终调整到这个状态。\n例如，你想让你的 Web 服务器始终运行在 4 个容器中，以达到负载均衡的目的，你的数据库复制到 3 个不同的容器中，以达到冗余的目的。这就是你想要的状态。如果这 7 个容器中的任何一个出现故障，Kubernetes 引擎会检测到这一点，并自动创建出一个新的容器，以确保维持所需的状态。\n现在我们来定义一些 Kubernetes 的重要组件。\n当你第一次设置 Kubernetes 时，你会创建一个集群。所有其他组件都是集群的一部分。你也可以创建多个虚拟集群，称为命名空间 (namespace)，它们是同一个物理集群的一部分。这与你可以在同一物理服务器上创建多个虚拟机的方式非常相似。如果你不需要，也没有明确定义的命名空间，那么你的集群将在始终存在的默认命名空间中创建。\nKubernetes 运行在节点 (node) 上，节点是集群中的单个机器。如果你有自己的硬件，节点可能对应于物理机器，但更可能对应于在云中运行的虚拟机。节点是部署你的应用或服务的地方，是 Kubernetes 工作的地方。有 2 种类型的节点 ——master 节点和 worker 节点，所以说 Kubernetes 是主从结构的。\n主节点是一个控制其他所有节点的特殊节点。一方面，它和集群中的任何其他节点一样，这意味着它只是另一台机器或虚拟机。另一方面，它运行着控制集群其他部分的软件。它向集群中的所有其他节点发送消息，将工作分配给它们，工作节点向主节点上的 API Server 汇报。\nMaster 节点本身也包含一个名为 API Server 的组件。这个 API 是节点与控制平面通信的唯一端点。API Server 至关重要，因为这是 worker 节点和 master 节点就 pod、deployment 和所有其他 Kubernetes API 对象的状态进行通信的点。\nWorker 节点是 Kubernetes 中真正干活的节点。当你在应用中部署容器或 pod（稍后定义）时，其实是在将它们部署到 worker 节点上运行。Worker 节点托管和运行一个或多个容器的资源。\nKubernetes 中的逻辑而非物理的工作单位称为 pod。一个 pod 类似于 Docker 中的容器。记得我们在前面讲到，容器可以让你创建独立、隔离的工作单元，可以独立运行。但是要创建复杂的应用程序，比如 Web 服务器，你经常需要结合多个容器，然后在一个 pod 中一起运行和管理。这就是 pod 的设计目的 —— 一个 pod 允许你把多个容器，并指定它们如何组合在一起来创建应用程序。而这也进一步明确了 Docker 和 Kubernetes 之间的关系 —— 一个 Kubernetes pod 通常包含一个或多个 Docker 容器，所有的容器都作为一个单元来管理。\nKubernetes 中的 service 是一组逻辑上的 pod。把一个 service 看成是一个 pod 的逻辑分组，它提供了一个单一的 IP 地址和 DNS 名称，你可以通过它访问服务内的所有 pod。有了服务，就可以非常容易地设置和管理负载均衡，当你需要扩展 Kubernetes pod 时，这对你有很大的帮助，我们很快就会看到。\nReplicationController 或 ReplicaSet 是 Kubernetes 的另一个关键功能。它是负责实际管理 pod 生命周期的组件 —— 当收到指令时或 pod 离线或意外停止时启动 pod，也会在收到指示时杀死 pod，也许是因为用户负载减少。所以换句话说，ReplicationController 有助于实现我们所期望的指定运行的 pod 数量的状态。\n什么是 Kubectl？ kubectl 是一个命令行工具，用于与 Kubernetes 集群和其中的 pod 通信。使用它你可以查看集群的状态，列出集群中的所有 pod，进入 pod 中执行命令等。你还可以使用 YAML 文件定义资源对象，然后使用 kubectl 将其应用到集群中。\nKubernetes 中的自动扩展 请记住，我们使用 Kubernetes 而不是直接使用 Docker 的原因之一，是因为 Kubernetes 能够自动扩展应用实例的数量以满足工作负载的需求。\n自动缩放是通过集群设置来实现的，当服务需求增加时，增加节点数量，当需求减少时，则减少节点数量。但也要记住，节点是 “物理” 结构 —— 我们把 “物理” 放在引号里，因为要记住，很多时候，它们实际上是虚拟机。\n无论如何，节点是物理机器的事实意味着我们的云平台必须允许 Kubernetes 引擎创建新机器。各种云提供商对 Kubernetes 支持基本都满足这一点。\n我们再继续说一些概念，这次是和网络有关的。\n什么是 kubernetes Ingress 和 Egress？ 外部用户或应用程序与 Kubernetes pod 交互，就像 pod 是一个真正的服务器一样。我们需要设置安全规则允许哪些流量可以进入和离开 “服务器”，就像我们为托管应用程序的服务器定义安全规则一样。\n进入 Kubernetes pod 的流量称为 Ingress，而从 pod 到集群外的出站流量称为 egress。我们创建入口策略和出口策略的目的是限制不需要的流量进入和流出服务。而这些策略也是定义 pod 使用的端口来接受传入和传输传出数据 / 流量的地方。\n什么是 Ingress Controller？ 但是在定义入口和出口策略之前，你必须首先启动被称为 Ingress Controller（入口控制器）的组件；这个在集群中默认不启动。有不同类型的入口控制器，Kubernetes 项目默认只支持 Google Cloud 和开箱即用的 Nginx 入口控制器。通常云供应商都会提供自己的入口控制器。\n什么是 Replica 和 ReplicaSet？ 为了保证应用程序的弹性，需要在不同节点上创建多个 pod 的副本。这些被称为 Replica。假设你所需的状态策略是 “ …","date":1651532400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"fbb27bd25b642943d027ad60fa222197","permalink":"https://jimmysong.io/docs/cloud-native/intro/quick-start/","publishdate":"2022-05-03T00:00:00+01:00","relpermalink":"/docs/cloud-native/intro/quick-start/","section":"cloud-native","summary":"Kubernetes 一词来自希腊语，意思是 “飞行员” 或 “舵手”。这个名字很贴切","tags":null,"title":"云原生快速入门","type":"book"},{"authors":null,"categories":null,"content":"你可能参加过各种云原生、服务网格相关的 meetup，在社区里看到很多人在分享和讨论 Istio，但是对于自己是否真的需要 Istio 感到踌躇，甚至因为它的复杂性而对服务网格的前景感到怀疑。那么，在你继阅读 Istio SIG 后续文章之前，请先仔细阅读本文，审视一下自己公司的现状，看看你是否有必要使用服务网格，处于 Istio 应用的哪个阶段。\n本文不是对应用服务网格的指导，而是根据社区里经常遇到的问题而整理。在使用 Istio 之前，请先考虑下以下因素：\n 你的团队里有多少人？ 你的团队是否有使用 Kubernetes、Istio 的经验？ 你有多少微服务？ 这些微服务使用什么语言？ 你的运维、SRE 团队是否可以支持服务网格管理？ 你有采用开源项目的经验吗？ 你的服务都运行在哪些平台上？ 你的应用已经容器化并使用 Kubernetes 管理了吗？ 你的服务有多少是部署在虚拟机、有多少是部署到 Kubernetes 集群上，比例如何？ 你的团队有制定转移到云原生架构的计划吗？ 你想使用 Istio 的什么功能？ Istio 的稳定性是否能够满足你的需求？ 你是否可以忍受 Istio 带来的性能损耗？  Istio 作为目前最流行的 Service Mesh 技术之一，拥有活跃的社区和众多的落地案例。但如果你真的想在你的生产环境大规模落地 Isito，这看似壮观美好的冰山下，却是暗流涌动，潜藏着无数凶险。\n使用 Istio 无法做到完全对应用透明 服务通信和治理相关的功能迁移到 Sidecar 进程中后， 应用中的 SDK 通常需要作出一些对应的改变。\n比如 SDK 需要关闭一些功能，例如重试。一个典型的场景是，SDK 重试 m 次，Sidecar 重试 n 次，这会导致 m * n 的重试风暴，从而引发风险。\n此外，诸如 trace header 的透传，也需要 SDK 进行升级改造。如果你的 SDK 中还有其它特殊逻辑和功能，这些可能都需要小心处理才能和 Isito Sidecar 完美配合。\nIstio 对非 Kubernetes 环境的支持有限 在业务迁移至 Istio 的同时，可能并没有同步迁移至 Kubernetes，而还运行在原有 PAAS 系统之上。 这会带来一系列挑战：\n 原有 PAAS 可能没有容器网络，Istio 的服务发现和流量劫持都可能要根据旧有基础设施进行适配才能正常工作 如果旧有的 PAAS 单个实例不能很好的管理多个容器（类比 Kubernetes 的 Pod 和 Container 概念），大量 Istio Sidecar 的部署和运维将是一个很大的挑战 缺少 Kubernetes webhook 机制，Sidecar 的注入也可能变得不那么透明，而需要耦合在业务的部署逻辑中  只有 HTTP 协议是一等公民 Istio 原生对 HTTP 协议提供了完善的全功能支持，但在真实的业务场景中，私有化协议却非常普遍，而 Istio 却并未提供原生支持。\n这导致使用私有协议的一些服务可能只能被迫使用 TCP 协议来进行基本的请求路由，这会导致很多功能的缺失，这其中包括 Istio 非常强大的基于内容的消息路由，如基于 header、 path 等进行权重路由。\n扩展 Istio 的成本并不低 虽然 Istio 的总体架构是基于高度可扩展而设计，但由于整个 Istio 系统较为复杂，如果你对 Istio 进行过真实的扩展，就会发现成本不低。\n以扩展 Istio 支持某一种私有协议为例，首先你需要在 Istio 的 api 代码库中进行协议扩展，其次你需要修改 Istio 代码库来实现新的协议处理和下发，然后你还需要修改 xds 代码库的协议，最后你还要在 Envoy 中实现相应的 Filter 来完成协议的解析和路由等功能。\n在这个过程中，你还可能面临上述数个复杂代码库的编译等工程挑战（如果你的研发环境不能很好的使用 Docker 或者无法访问部分国外网络的情况下）。\n即使做完了所有的这些工作，你也可能面临这些工作无法合并回社区的情况，社区对私有协议的扩展支持度不高，这会导致你的代码和社区割裂，为后续的升级更新带来隐患。\nIstio 在集群规模较大时的性能问题 Istio 默认的工作模式下，每个 Sidecar 都会收到全集群所有服务的信息。如果你部署过 Istio 官方的 Bookinfo 示例应用，并使用 Envoy 的 config dump 接口进行观察，你会发现，仅仅几个服务，Envoy 所收到的配置信息就有将近 20w 行。\n可以想象，在稍大一些的集群规模，Envoy 的内存开销、Istio 的 CPU 开销、XDS 的下发时效性等问题，一定会变得尤为突出。\nIstio 这么做一是考虑这样可以开箱即用，用户不用进行过多的配置，另外在一些场景，可能也无法梳理出准确的服务之间的调用关系，因此直接给每个 Sidecar 下发了全量的服务配置，即使这个 Sidecar 只会访问其中很小一部分服务。\n当然这个问题也有解法，你可以通过 Sidecar CRD 来显示定义服务调用关系，使 Envoy 只得到他需要的服务信息，从而大幅降低 Envoy 的资源开销，但前提是在你的业务线中能梳理出这些调用关系。\nXDS 分发没有分级发布机制 当你对一个服务的策略配置进行变更的时候，XDS 不具备分级发布的能力，所有访问这个服务的 Envoy 都会立即收到变更后的最新配置。这在一些对变更敏感的严苛生产环境，可能是有很高风险甚至不被允许的。\n如果你的生产环境严格要求任何变更都必须有分级发布流程，那你可能需要考虑自己实现一套这样的机制。\nIstio 组件故障时是否有退路？ 以 Istio 为代表的 Sidecar 架构的特殊性在于，Sidecar 直接承接了业务流量，而不像一些其他的基础设施那样，只是整个系统的旁路组件（比如 Kubernetes）。\n因此在 Isito 落地初期，你必须考虑，如果 Sidecar 进程挂掉，服务怎么办？是否有退路？是否能 fallback 到直连模式？\n在 Istio 落地过程中，是否能无损 fallback，通常决定了核心业务能否接入 Service Mesh。\nIsito 技术架构的成熟度还没有达到预期 虽然 Istio 1.0 版本已经发布了很久，但是如果你关注社区每个版本的迭代，就会发现，Istio 目前架构依然处于不太稳定的状态，尤其是 1.5 版本前后的几个大版本，先后经历了去除 Mixer 组件、合并为单体架构、仅支持高版本 Kubernetes 等等重大变动，这对于已经在生产环境中使用了 Istio 的用户非常不友好，因为升级会面临各种不兼容性问题。\n好在社区也已经意识到这一问题，2021 年社区也成立了专门的小组，重点改善 Istio 的兼容性和用户体验。\nIstio 缺乏成熟的产品生态 Istio 作为一套技术方案，却并不是一套产品方案。\n如果你在生产环境中使用，你可能还需要解决可视化界面、权限和账号系统对接、结合公司已有技术组件和产品生态等问题，仅仅通过命令行来使用，可能并不能满足你的组织对权限、审计、易用性的要求。\n而 Isito 自带的 Kiali 功能还十分简陋，远远没有达到能在生产环境使用的程度，因此你可能需要研发基于 Isito 的上层产品。\nIstio 目前解决的问题域还很有限 Istio 目前主要解决的是分布式系统之间服务调用的问题，但还有一些分布式系统的复杂语义和功能并未纳入到 Istio 的 Sidecar 运行时之中，比如消息发布和订阅、状态管理、资源绑定等等。\n云原生应用将会朝着多 Sidecar 运行时或将更多分布式能力纳入单 Sidecar 运行时的方向继续发展，以使服务本身变得更为轻量，让应用和基础架构彻底解耦。\n如果你的生产环境中，业务系统对接了非常多和复杂的分布式系系统中间件，Istio 目前可能并不能完全解决你的应用的云原生化诉求。\n写在最后 看到这里，你是否感到有些沮丧，而对 Isito 失去信心？\n别担心，上面列举的这些问题，实际上并不影响 Isito 依然是目前最为流行和成功的 Service Mesh 技术选型之一。Istio 频繁的变动，一定程度上也说明它拥有一个活跃的社区，我们应当对一个新的事物报以信心，Isito 的社区也在不断听取来自终端用户的声音，朝着大家期待的方向演进。\n同时，如果你的生产环境中的服务规模并不是很大，服务已经托管于 Kubernetes 之上，也只使用那些 Istio 原生提供的能力，那么 Istio 依然是一个值得尝试的开箱即用方案。\n但如果你的生产环境比较复杂，技术债务较重，专有功能和策略需求较多，亦或者服务规模庞大，那么在开始使用 Istio 之前，你需要仔细权衡上述这些要素，以评估在你的系统之中引入 Istio 可能带来的复杂度和潜在成本。\n参考  在生产环境使用 Istio 前的若干考虑要素 - cloudnative.to  ","date":1651507200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"761cdd41579eb5a0fe1426586988a8eb","permalink":"https://jimmysong.io/docs/cloud-native/service-mesh/do-you-need-a-service-mesh/","publishdate":"2022-05-03T00:00:00+08:00","relpermalink":"/docs/cloud-native/service-mesh/do-you-need-a-service-mesh/","section":"cloud-native","summary":"你可能参加过各种云原生、服务网格相关的 meetup，在社区里","tags":null,"title":"你是否需要 Istio？","type":"book"},{"authors":null,"categories":null,"content":"眯着眼睛看图表并不是寻找相关性的最佳方式。目前在运维人员头脑中进行的大量工作实际上是可以自动化的。这使运维人员可以在识别问题、提出假设和验证根本原因之间迅速行动。\n为了建立更好的工具，我们需要更好的数据。遥测必须具备以下两个要求以支持高质量的自动分析：\n 所有的数据点都必须用适当的索引连接在一个图上。 所有代表常见操作的数据点必须有明确的键和值。  在这一章中，我们将从一个基本的构件开始浏览现代遥测数据模型：属性。\n属性：定义键和值 最基本的数据结构是属性（attribute），定义为一个键和一个值。OpenTelemetry 的每个数据结构都包含一个属性列表。分布式系统的每个组件（HTTP 请求、SQL 客户端、无服务器函数、Kubernetes Pod）在 OpenTelemetry 规范中都被定义为一组特定的属性。这些定义被称为 OpenTelemetry 语义约定。表 2-1 显示了 HTTP 约定的部分列表。\n   属性 类型 描述 示例     http.method string HTTP 请求类型 GET; POST; HEAD   http.target string 在 HTTP 请求行中传递的完整的请求目标或等价物 /path/12314/   http.host string HTTP host header 的值。当标头为空或不存在时，这个属性应该是相同的。 www.example.org   http.scheme string 识别所使用协议的 URI 方案 http; https   http.status_code int HTTP 请求状态码 200    表 2-1：HTTP 规范的部分列表\n有了这样一个标准模式，分析工具就可以对它们所监测的系统进行详细的表述，同时进行细微的分析，而在使用定义不明确或不一致的数据时，是不可能做到的。\n事件：一切的基础 OpenTelemetry 中最基本的对象是事件（event）。事件只是一个时间戳和一组属性。使用一组属性而不是简单的消息 / 报文，可以使分析工具正确地索引事件，并使它们可以被搜索到。\n有些属性对事件来说是独一无二的。时间戳、消息和异常细节都是特定事件的属性的例子。\n然而，大多数属性对单个事件来说并不独特。相反，它们是一组事件所共有的。例如，http.target 属性与作为 HTTP 请求的一部分而记录的每个事件有关。如果在每个事件上反复记录这些属性，效率会很低。相反，我们把这些属性拉出到围绕事件的封装中，在那里它们可以被写入一次。我们把这些封装称为上下文（context）。\n有两种类型的上下文：静态和动态（如图 2-1 所示）。静态上下文定义了一个事件发生的物理位置。在 OpenTelemetry 中，这些静态属性被称为资源。一旦程序启动，这些资源属性的值通常不会改变。\n   图 2-1：虽然事件有一些特定的事件属性，但大多数属性属于事件发生的上下文之一。  动态上下文定义了事件所参与的活动操作。这个操作层面的上下文被称为跨度（span）。每次操作执行时，这些属性的值都会改变。\n不是所有的事件都有两种类型的上下文。只有资源的自由浮动事件，如程序启动时发出的事件，被称为日志（log）。作为分布式事务的一部分而发生的事件被称为跨度事件（span event）。\n资源：观察服务和机器 资源（静态上下文）描述了一个程序正在消费的物理和虚拟信息结构。服务、容器、部署和区域都是资源。图 2-2 显示了一个典型的购物车结账事务中所涉及的资源。\n   图 2-2：一个事务，被视为一组资源。  系统运行中的大多数问题都源于资源争夺，许多并发的事务试图在同一时间利用相同的资源。通过将事件放在它们所使用的资源的上下文中，就有可能自动检测出许多类型的资源争夺。\n像事件一样，资源可以被定义为一组属性。表 2-2 显示了一个服务资源的例子。\n表 2-2：服务资源的例子\n   属性 类型 描述 示例     service.name string 服务的逻辑名称 shopping cart   service.instance.id string 服务实例的 ID 627cc493- f310-47de-96bd-71410b7dec09   service.version string 服务 API 或者实现的版本号 2.0.0    除了识别机器所需的基本信息，配置设置也可以作为资源被记录下来。要访问一台正在运行的机器来了解它是如何配置的，这个负担太让人害怕了。相反，在配置文件中发现的任何重要信息也应该表示为一种资源。\n跨度：观察事务 跨度（动态上下文）描述计算机操作。跨度有一个操作名称，一个开始时间，一个持续时间，以及一组属性。\n标准操作是使用语义约定来描述的，比如上面描述的 HTTP 约定。但也有一些特定的应用属性，如 ProjectID 和 AccountID，可以由应用开发者添加。\n跨度也是我们描述因果关系的方式。为了正确记录整个事务，我们需要知道哪些操作是由其他哪些操作触发的。为了做到这一点，我们需要给跨度增加三个属性：TraceID、SpanID 和 ParentID，如表 2-3 所示。\n表 2-3：跨度的三个额外属性\n   属性 类型 描述 示例     traceid 16 字节数组 识别整个事务 4bf92f3577b34da6a3ce929d0e0e4736   spanid 8 字节数组 识别当前操作 00f067aa0ba902b7   parentid 8 字节数组 识别父操作 53ce929d0e0e4736    这三个属性是 OpenTelemetry 的基础。通过添加这些属性，我们所有的事件现在可以被组织成一个图，代表它们的因果关系。这个图现在可以以各种方式进行索引，我们稍后会讨论这个问题。\n追踪：看似日志，胜过日志 我们现在已经从简单的事件变成了组织成与资源相关的操作图的事件。这种类型的图被称为追踪（trace）。图 2-3 显示了一种常见的可视化追踪方式，重点是识别操作的延迟。\n   图 2-3：当日志被组织成一个图形时，它们就变成了追踪。  从本质上讲，追踪只是用更好的索引来记录日志。当你把适当的上下文添加到适当的结构化的日志中时，可以得到追踪的定义。\n想想你花了多少时间和精力通过搜索和过滤来收集这些日志；那是收集数据的时间，而不是分析数据的时间。而且，你要翻阅的日志越多，执行并发事务数量不断增加的机器堆积，就越难收集到真正相关的那一小部分日志。\n然而，如果你有一个 TraceID，收集这些日志只是一个简单的查询。通过 TraceID 索引，你的存储工具可以自动为你做这项工作；找到一个日志，你就有了该事务中的所有日志，不需要额外的工作。\n既然如此，为什么你还会要那些没有 “追踪\u0026#34;ID 的 “日志”？我们已经习惯了传统的日志管理迫使我们做大量的工作来连接这些点。但这些工作实际上是不必要的；它是我们数据中缺乏结构的副产品。\n分布式追踪不仅仅是一个测量延迟的工具；它是一个定义上下文和因果关系的数据结构。它是把所有东西联系在一起的胶水。正如我们将看到的，这种胶水包括最后一个支柱 —— 指标。\n指标：观察事件的总体情况 现在我们已经确定了什么是事件，让我们来谈谈事件的聚合。在一个活跃的系统中，同样的事件会不断发生，我们以聚合的方式查看它们的属性来寻找模式。属性的值可能出现得太频繁，或者不够频繁，在这种情况下，我们要计算这些值出现的频率。或者该值可能超过某个阈值，在这种情况下，我们想衡量该值是如何随时间变化的。或者我们可能想以直方图的形式来观察数值的分布。\n这些聚合事件被称为度量。就像普通的事件一样，度量有一组属性和一组语义上的便利条件来描述普通概念。表 2-4 显示了一些系统内存的例子属性。\n表 2-4：系统内存的属性\n   属性 值类型 属性值     system.memory.usage int64 used, free, cached, other   system.memory.utilization double used, free, cached, other    与事件相关的指标：统一的系统 传统上，我们认为指标是与日志完全分开的。但实际上它们是紧密相连的。例如，假设一个 API 有一个衡量每分钟错误数量的指标。那是一个统计数字。然而，每一个错误都是由一个特定的事务行为产生的，使用特定的资源。这些细节在我们每次递增该计数器时都会出现，我们想知道这些细节。\n当运维人员被提醒发现错误突然激增时，他们会想到的第一个问题自然是：“是什么导致了这个激增？” 看一下例子的追踪可以回答这个问题。在失败的事务中较早发生的事件（或未能发生的事件）可能是错误的来源。\n在 OpenTelemetry 中，当指标事件在跨度的范围内发生时，这些追踪的样本会自动与指标相关联，作为追踪的范例（trace examplar）。这意味着不需要猜测或寻找日志。OpenTelemetry 明确地将追踪和度量联系在一起。一个建立在 OpenTelemetry 上的分析工具可以让你从仪表盘上直接看到追踪，只需一次点击。如果有一个模式 —— 例如，一个特定的属性值与导致一个特定错误的追踪密切相关 —— 这个模式可以被自动识别。\n自动分析和编织 事件、资源、跨度、指标和追踪：这些都被 OpenTelemetry 连接在一个图中，并且它们都被发送到同一个数据库，作为一个整体进行分析。这就是下一代的可观测性工具。\n现代可观测性将建立在使用结构的数据上，这些结构允许分析工具在所有类型的事件和总量之间进行关联，这些关联将对我们如何实践可观测性产生深远影响。\n向全面观察我们的系统过渡将有许多好处。但我相信，这些新工具提供的主要省时功能将是各种形式的自动关联检测。在寻找根本原因时，注意到相关关系可以产生大量的洞察力。如图 2-4 所示，相关性往往是产生根本原因假设的关键因素，然后可以进一步调查。\n   图 2-4：可能提供关键洞察力、指向根本原因的关联实例。  平均表现是什么样子的？异常值是什么样子的？哪些趋势在一起变化，它们的共同点是什么？相关性可能发生在许多地方：跨度中的属性之间、追踪中的跨度之间、追踪和资源之间、指标内部，以及所有这些地方都有。当所有这些数据被连接到一个图中时，这些相关性就可以被发现了。\n这就是为什么统一的数据编织是如此关键。任何自动匹配的分析的价值完全取决于被分析的数据的结构和质量。机器遍历数据的图表；它们不会进行逻辑的飞跃。准确的统计分析需要一个有意设计的遥测系统来支持它。\n重点：自动分析为您节省时间 为什么我们关心相关性分析的自动化？因为时间和复杂性对我们不利。随着系统规模的扩大，它们最终变得太复杂了，任何操作者都无法完全掌握系统的情况，而且在建立一个假设时，永远没有足够的时间来调查每一个可能的联系。\n问题是，选择调查什么需要直觉，而直觉往往需要对组成分布式系统的每个组件有深刻的了解。随着企业系统的增长和工程人员的相应增加，任何一个工程师对每个系统深入了解的部分自然会缩减到整个系统的一小部分。直觉并不能很好地扩展。\n直觉也极易被误导；问题经常出现在意想不到的地方。根据定义，可以预见的问题几乎不会经常发生。剩下的就是所有未曾预料到的问题了，这些问题已经超出了我们的直觉。\n这就是自动关联检测的作用。有了正确的数据，机器可以更有效地检测出相关的关联。这使得运维人员能够快速行动，反复测试各种假设，直到他们知道足够的信息来制定解决方案。\n","date":1643990400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"3ebe29250964919d7448963fb74df17e","permalink":"https://jimmysong.io/docs/opentelemetry-obervability/the-value-of-structured-data/","publishdate":"2022-02-05T00:00:00+08:00","relpermalink":"/docs/opentelemetry-obervability/the-value-of-structured-data/","section":"opentelemetry-obervability","summary":"第 2 章：结构化数据的价值","tags":null,"title":"第 2 章：结构化数据的价值","type":"book"},{"authors":null,"categories":null,"content":"IT 行业正在向微服务架构和云原生解决方案发展。由于使用不同的技术开发了成百上千的微服务，这些系统可能变得复杂，难以调试。\n作为一个应用开发者，你考虑的是业务逻辑——购买产品或生成发票。然而，任何像这样的业务逻辑都会导致不同服务之间的多个服务调用。每个服务可能都有它的超时、重试逻辑和其他可能需要调整或微调的网络特定代码。\n如果在任何时候最初的请求失败了，就很难通过多个服务来追踪，准确地指出失败发生的地方，了解请求为什么失败。是网络不可靠吗？是否需要调整重试或超时？或者是业务逻辑问题或错误？\n服务可能使用不一致的跟踪和记录机制，使这种调试的复杂性增加。这些问题使你很难确定问题发生在哪里，以及如何解决。如果你是一个应用程序开发人员，而调试网络问题不属于你的核心技能，那就更是如此。\n将网络问题从应用程序堆栈中抽离出来，由另一个组件来处理网络部分，让调试网络问题变得更容易。这就是 Envoy 所做的事情。\n在每个服务实例旁边都有一个 Envoy 实例在运行。这种类型的部署也被称为 Sidecar 部署。Envoy 的另一种模式是边缘代理，用于构建 API 网关。\nEnvoy 和应用程序形成一个原子实体，但仍然是独立的进程。应用程序处理业务逻辑，而 Envoy 则处理网络问题。\n在发生故障的情况下，分离关注点可以更容易确定故障是来自应用程序还是网络。\n为了帮助网络调试，Envoy 提供了以下高级功能。\n进程外架构 Envoy 是一个独立的进程，旨在与每个应用程序一起运行 —— 也就是我们前面提到的 Sidecar 部署模式。集中配置的 Envoy 的集合形成了一个透明的服务网格。\n路由和其他网络功能的责任被推给了 Envoy。应用程序向一个虚拟地址（localhost）而不是真实地址（如公共 IP 地址或主机名）发送请求，不知道网络拓扑结构。应用程序不再承担路由的责任，因为该任务被委托给一个外部进程。\n与其让应用程序管理其网络配置，不如在 Envoy 层面上独立于应用程序管理网络配置。在一个组织中，这可以使应用程序开发人员解放出来，专注于应用程序的业务逻辑。\nEnvoy 适用于任何编程语言。你可以用 Go、Java、C++ 或其他任何语言编写你的应用程序，而 Envoy 可以在它们之间架起桥梁。Envoy 的行为是相同的，无论应用程序的编程语言或它们运行的操作系统是什么。\nEnvoy 还可以在整个基础设施中透明地进行部署和升级。这与为每个单独的应用程序部署库升级相比，后者可能是非常痛苦和耗时的。\n进程外架构是有益的，因为它使我们在不同的编程语言 / 应用堆栈中保持一致，我们可以免费获得独立的应用生命周期和所有的 Envoy 网络功能，而不必在每个应用中单独解决这些问题。\nL3/L4 过滤器结构 Envoy 是一个 L3/L4 网络代理，根据 IP 地址和 TCP 或 UDP 端口进行决策。它具有一个可插拔的过滤器链，可以编写你的过滤器来执行不同的 TCP/UDP 任务。\n过滤器链（Filter Chain） 的想法借鉴了 Linux shell，即一个操作的输出被输送到另一个操作中。例如：\nls -l | grep \u0026#34;Envoy*.cc\u0026#34; | wc -l Envoy 可以通过堆叠所需的过滤器来构建逻辑和行为，形成一个过滤器链。许多过滤器已经存在，并支持诸如原始 TCP 代理、UDP 代理、HTTP 代理、TLS 客户端认证等任务。Envoy 也是可扩展的，我们可以编写我们的过滤器。\nL7 过滤器结构 Envoy 支持一个额外的 HTTP L7 过滤器层。我们可以在 HTTP 连接管理子系统中插入 HTTP 过滤器，执行不同的任务，如缓冲、速率限制、路由 / 转发等。\n一流的 HTTP/2 支持 Envoy 同时支持 HTTP/1.1 和 HTTP/2，并且可以作为一个透明的 HTTP/1.1 到 HTTP/2 的双向代理进行操作。这意味着任何 HTTP/1.1 和 HTTP/2 客户端和目标服务器的组合都可以被桥接起来。即使你的传统应用没有通过 HTTP/2 进行通信，如果你把它们部署在 Envoy 代理旁边，它们最终也会通过 HTTP/2 进行通信。\n推荐在所有的服务间配置的 Envoy 使用 HTTP/2，以创建一个持久连接的网格，请求和响应可以在上面复用。\nHTTP 路由 当以 HTTP 模式操作并使用 REST 时，Envoy 支持路由子系统，能够根据路径、权限、内容类型和运行时间值来路由和重定向请求。在将 Envoy 作为构建 API 网关的前台 / 边缘代理时，这一功能非常有用，在构建服务网格（sidecar 部署模式）时，也可以利用这一功能。\ngRPC 准备就绪 Envoy 支持作为 gRPC 请求和响应的路由和负载均衡底层所需的所有 HTTP/2 功能。\n gRPC 是一个开源的远程过程调用（RPC）系统，它使用 HTTP/2 进行传输，并将协议缓冲区作为接口描述语言（IDL），它提供的功能包括认证、双向流和流量控制、阻塞 / 非阻塞绑定，以及取消和超时。\n 服务发现和动态配置 我们可以使用静态配置文件来配置 Envoy，这些文件描述了服务间通信方式。\n对于静态配置 Envoy 不现实的高级场景，Envoy 支持动态配置，在运行时自动重新加载配置。一组名为 xDS 的发现服务可以用来通过网络动态配置 Envoy，并为 Envoy 提供关于主机、集群 HTTP 路由、监听套接字和加密信息。\n健康检查 负载均衡器有一个特点，那就是只将流量路由到健康和可用的上游服务。Envoy 支持健康检查子系统，对上游服务集群进行主动健康检查。然后，Envoy 使用服务发现和健康检查信息的组合来确定健康的负载均衡目标。Envoy 还可以通过异常点检测子系统支持被动健康检查。\n高级负载均衡 Envoy 支持自动重试、断路、全局速率限制（使用外部速率限制服务）、影子请求（或流量镜像）、异常点检测和请求对冲。\n前端 / 边缘代理支持 Envoy 的特点使其非常适合作为边缘代理运行。这些功能包括 TLS 终端、HTTP/1.1、HTTP/2 和 HTTP/3 支持，以及 HTTP L7 路由。\nTLS 终止 应用程序和代理的解耦使网格部署模型中所有服务之间的 TLS 终止（双向 TLS）成为可能。\n一流的可观察性 为了便于观察，Envoy 会生成日志、指标和追踪。Envoy 目前支持 statsd（和兼容的提供者）作为所有子系统的统计。得益于可扩展性，我们也可以在需要时插入不同的统计提供商。\nHTTP/3（Alpha） Envoy 1.19.0 支持 HTTP/3 的上行和下行，并在 HTTP/1.1、HTTP/2 和 HTTP/3 之间进行双向转义。\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"55b0d5f9d597c5b97998b100344c7df1","permalink":"https://jimmysong.io/docs/cloud-native/service-mesh/what-is-envoy/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/docs/cloud-native/service-mesh/what-is-envoy/","section":"cloud-native","summary":"IT 行业正在向微服务架构和云原生解决方案发展。由于使用不同的技","tags":null,"title":"什么是 Envoy？","type":"book"},{"authors":null,"categories":null,"content":"总结 现在您已经知道了 Code Review 要点，那么管理分布在多个文件中的评论的最有效方法是什么？\n 变更是否有意义？它有很好的描述吗？ 首先看一下变更中最重要的部分。整体设计得好吗？ 以适当的顺序查看 CL 的其余部分。  第一步：全面了解变更 查看 CL 描述和 CL 大致上用来做什么事情。这种变更是否有意义？如果在最初不应该发生这样的变更，请立即回复，说明为什么不应该进行变更。当您拒绝这样的变更时，向开发人员建议应该做什么也是一个好主意。\n例如，您可能会说“看起来你已经完成一些不错的工作，谢谢！但实际上，我们正朝着删除您在这里修改的 FooWidget 系统的方向演进，所以我们不想对它进行任何新的修改。不过，您来重构下新的 BarWidget 类怎么样？“\n请注意，审查者不仅拒绝了当前的 CL 并提供了替代建议，而且他们保持礼貌地这样做。这种礼貌很重要，因为我们希望表明，即使不同意，我们也会相互尊重。\n如果您获得了多个您不想变更的 CL，您应该考虑重整开发团队的开发过程或外部贡献者的发布过程，以便在编写CL之前有更多的沟通。最好在他们完成大量工作之前说“不”，避免已经投入心血的工作现在必须被抛弃或彻底重写。\n第二步：检查 CL 的主要部分 查找作为此 CL “主要”部分的文件。通常，包含大量的逻辑变更的文件就是 CL 的主要部分。先看看这些主要部分。这有助于为 CL 的所有较小部分提供上下文，并且通常可以加速代码审查。如果 CL 太大而无法确定哪些部分是主要部分，请向开发人员询问您应该首先查看的内容，或者要求他们将 CL 拆分为多个 CL。\n如果在该部分发现存在一些主要的设计问题时，即使没有时间立即查看 CL 的其余部分，也应立即留下评论告知此问题。因为事实上，因为该设计问题足够严重的话，继续审查其余部分很可能只是浪费宝贵的时间，因为其他正在审查的程序可能都将无关或消失。\n立即发送这些主要设计评论非常重要，有两个主要原因：\n 通常开发者在发出 CL 后，在等待审查时立即开始基于该 CL 的新工作。如果您正在审查的 CL 中存在重大设计问题，那么他们以后的 CL 也必须要返工。您应该赶在他们在有问题的设计上做了太多无用功之前通知他们。 主要的设计变更比起小的变更来说需要更长的时间才能完成。开发人员基本都有截止日期；为了完成这些截止日期并且在代码库中仍然保有高质量代码，开发人员需要尽快开始 CL 的任何重大工作。  第三步：以适当的顺序查看 CL 的其余部分 一旦您确认整个 CL 没有重大的设计问题，试着找出一个逻辑顺序来查看文件，同时确保您不会错过查看任何文件。 通常在查看主要文件之后，最简单的方法是按照代码审查工具向您提供的顺序浏览每个文件。有时在阅读主代码之前先阅读测试也很有帮助，因为这样您就可以了解该变更应当做些什么。\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"32281be908130af8de14e98ca384a3f9","permalink":"https://jimmysong.io/docs/eng-practices/review/reviewer/navigate/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/docs/eng-practices/review/reviewer/navigate/","section":"eng-practices","summary":"总结 现在您已经知道了 Code Review 要点，那么管理分布在多个文件中的评论","tags":null,"title":"查看 CL 的步骤","type":"book"},{"authors":null,"categories":null,"content":"当您发送 CL 进行审查时，您的审查者可能会对您的 CL 发表一些评论。以下是处理审查者评论的一些有用信息。\n不是针对您 审查的目标是保持代码库和产品的质量。当审查者对您的代码提出批评时，请将其视为在帮助您、代码库和 Google，而不是对您或您的能力的个人攻击。\n有时，审查者会感到沮丧并在评论中表达他们的挫折感。对于审查者来说，这不是一个好习惯，但作为开发人员，您应该为此做好准备。问问自己，“审查者试图与我沟通的建设性意见是什么？”然后像他们实际说的那样操作。\n永远不要愤怒地回应代码审查评论。这严重违反了专业礼仪且将永远存在于代码审查工具中。如果您太生气或恼火而无法好好的回应，那么请离开电脑一段时间，或者做一些别的事情，直到您感到平静，可以礼貌地回答。\n一般来说，如果审查者没有以建设性和礼貌的方式提供反馈，请亲自向他们解释。如果您无法亲自或通过视频通话与他们交谈，请向他们发送私人电子邮件。以友善的方式向他们解释您不喜欢的东西以及您希望他们以怎样不同的方式来做些什么。如果他们也以非建设性的方式回复此私人讨论，或者没有预期的效果，那么请酌情上报给您的经理。\n修复代码 如果审查者说他们不了解您的代码中的某些内容，那么您的第一反应应该是澄清代码本身。 如果无法澄清代码，请添加代码注释，以解释代码存在的原因。 只有在想增加的注释看起来毫无意义时，您才能在代码审查工具中进行回复与解释。\n如果审查者不理解您的某些代码，那么代码的未来读者可能也不会理解。在代码审查工具中回复对未来的代码读者没有帮助，但澄清代码或添加代码注释确可以实实在在得帮助他们。\n自我反思 编写 CL 可能需要做很多工作。在终于发送一个 CL 用于审查后，我们通常会感到满足的，认为它已经完成，并且非常确定不需要进一步的工作。这通常是令人满意的。因此，当审查者回复对可以改进的事情的评论时，很容易本能地认为评论是错误的，审查者正在不必要地阻止您，或者他们应该让您提交 CL。但是，无论您目前多么确定，请花一点时间退一步，考虑审查者是否提供有助于对代码库和对 Google 的有价值的反馈。您首先应该想到的应该是，“审查者是否正确？”\n如果您无法回答这个问题，那么审查者可能需要澄清他们的意见。\n如果您已经考虑过并且仍然认为自己是正确的，请随时回答一下为什么您的方法对代码库、用户和/或 Google 更好。通常，审查者实际上是在提供建议，他们希望您自己思考什么是最好的。您可能实际上对审阅者不知道的用户、代码库或 CL 有所了解。所以提供并告诉他们更多的上下文。通常，您可以根据技术事实在自己和审查者之间达成一些共识。\n解决冲突 解决冲突的第一步应该是尝试与审查者达成共识。 如果您无法达成共识，请参阅“代码审查标准”，该标准提供了在这种情况下遵循的原则。\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"54414c32ea995ca6c230c4577001cada","permalink":"https://jimmysong.io/docs/eng-practices/review/developer/handling-comments/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/docs/eng-practices/review/developer/handling-comments/","section":"eng-practices","summary":"当您发送 CL 进行审查时，您的审查者可能会对您的 CL 发表一些评论。","tags":null,"title":"如何处理审查者的评论","type":"book"},{"authors":null,"categories":null,"content":"勘误 1\nPDF 原文第 4 页，kubelet 端口，默认应为 10250，而不是 10251。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"a95a8d264405f5f828a845f088eb75db","permalink":"https://jimmysong.io/docs/kubernetes-hardening-guidance/corrigendum/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/kubernetes-hardening-guidance/corrigendum/","section":"kubernetes-hardening-guidance","summary":"勘误 1 PDF 原文第 4 页，kubelet 端口，默认应为 10250，","tags":null,"title":"勘误","type":"book"},{"authors":null,"categories":null,"content":"本节将为你介绍 HTTP 连接管理器中的请求匹配。\n路径匹配 我们只谈了一个使用前缀字段匹配前缀的匹配规则。下面的表格解释了其他支持的匹配规则。\n   规则名称 描述     prefix 前缀必须与path标头的开头相匹配。例如，前缀 /api 将匹配路径 /api 和 /api/v1，而不是 /。   path 路径必须与确切的path标头相匹配（没有查询字符串）。例如，路径 /api 将匹配路径 /api，但不匹配 /api/v1 或 /。   safe_regex 路径必须符合指定的正则表达式。例如，正则表达式 ^/products/\\d+$ 将匹配路径 /products/123 或 /products/321，但不是 /products/hello 或 /api/products/123。   connect_matcher 匹配器只匹配 CONNECT 请求（目前在 Alpha 中）。    默认情况下，前缀和路径匹配是大小写敏感的。要使其不区分大小写，我们可以将 case_sensitive 设置为 false。注意，这个设置不适用于 safe_regex 匹配。\nHeader 匹配 另一种匹配请求的方法是指定一组 Header。路由器根据路由配置中所有指定的 Header 检查请求 Header。如果所有指定的头信息都存在于请求中，并且设置了相同的值，则进行匹配。\n多个匹配规则可以应用于Header。\n范围匹配\nrange_match 检查请求 Header 的值是否在指定的以十进制为单位的整数范围内。该值可以包括一个可选的加号或减号，后面是数字。\n为了使用范围匹配，我们指定范围的开始和结束。起始值是包含的，而终止值是不包含的（[start, end)）。\n- match:prefix:\u0026#34;/\u0026#34;headers:- name:minor_versionrange_match:start:1end:11上述范围匹配将匹配 minor_version 头的值，如果它被设置为 1 到 10 之间的任何数字。\n存在匹配\npresent_match 检查传入的请求中是否存在一个特定的头。\n- match:prefix:\u0026#34;/\u0026#34;headers:- name:debugpresent_match:true如果我们设置了debug头，无论头的值是多少，上面的片段都会评估为true。如果我们把 present_match 的值设为 false，我们就可以检查是否有 Header。\n字符串匹配\nstring_match 允许我们通过前缀或后缀，使用正则表达式或检查该值是否包含一个特定的字符串，来准确匹配头的值。\n- match:prefix:\u0026#34;/\u0026#34;headers:# 头部`regex_match`匹配所提供的正则表达式- name:regex_matchstring_match:safe_regex_match:google_re2:{}regex:\u0026#34;^v\\\\d+$\u0026#34;# Header `exact_match`包含值`hello`。- name:exact_matchstring_match:exact:\u0026#34;hello\u0026#34;# 头部`prefix_match`以`api`开头。- name:prefix_matchstring_match:prefix:\u0026#34;api\u0026#34;# 头部`后缀_match`以`_1`结束- name:suffix_matchstring_match:suffix:\u0026#34;_1\u0026#34;# 头部`contains_match`包含值 \u0026#34;debug\u0026#34;- name:contains_matchstring_match:contains:\u0026#34;debug\u0026#34;反转匹配\n如果我们设置了 invert_match，匹配结果就会反转。\n- match:prefix:\u0026#34;/\u0026#34;headers:- name:versionrange_match:start:1end:6invert_match:true上面的片段将检查 version 头中的值是否在 1 和 5 之间；然而，由于我们添加了 invert_match 字段，它反转了结果，检查头中的值是否超出了这个范围。\ninvert_match 可以被其他匹配器使用。例如：\n- match:prefix:\u0026#34;/\u0026#34;headers:- name:envcontains_match:\u0026#34;test\u0026#34;invert_match:true上面的片段将检查 env 头的值是否包含字符串test。如果我们设置了 env 头，并且它不包括字符串test，那么整个匹配的评估结果为真。\n查询参数匹配 使用 query_parameters 字段，我们可以指定路由应该匹配的 URL 查询的参数。过滤器将检查来自path头的查询字符串，并将其与所提供的参数进行比较。\n如果有一个以上的查询参数被指定，它们必须与规则相匹配，才能评估为真。\n请考虑以下例子。\n- match:prefix:\u0026#34;/\u0026#34;query_parameters:- name:envpresent_match:true如果有一个名为 env 的查询参数被设置，上面的片段将评估为真。它没有说任何关于该值的事情。它只是检查它是否存在。例如，使用上述匹配器，下面的请求将被评估为真。\nGET /hello?env=test 我们还可以使用字符串匹配器来检查查询参数的值。下表列出了字符串匹配的不同规则。\n   规则名称 描述     exact 必须与查询参数的精确值相匹配。   prefix 前缀必须符合查询参数值的开头。   suffix 后缀必须符合查询参数值的结尾。   safe_regex 查询参数值必须符合指定的正则表达式。   contains 检查查询参数值是否包含一个特定的字符串。    除了上述规则外，我们还可以使用 ignore_case 字段来指示精确、前缀或后缀匹配是否应该区分大小写。如果设置为 “true”，匹配就不区分大小写。\n下面是另一个使用前缀规则进行不区分大小写的查询参数匹配的例子。\n- match:prefix:\u0026#34;/\u0026#34;query_parameters:- name:envstring_match:prefix:\u0026#34;env_\u0026#34;ignore_case:true如果有一个名为 env 的查询参数，其值以 env_开头，则上述内容将评估为真。例如，env_staging 和 ENV_prod 评估为真。\ngRPC 和 TLS 匹配器 我们可以在路由上配置另外两个匹配器：gRPC 路由匹配器（grpc）和 TLS 上下文匹配器（tls_context）。\ngRPC 匹配器将只在 gRPC 请求上匹配。路由器检查内容类型头的 application/grpc 和其他 application/grpc+ 值，以确定该请求是否是 gRPC 请求。\n例如：\n- match:prefix:\u0026#34;/\u0026#34;grpc:{} 注意 gRPC 匹配器没有任何选项。\n 如果请求是 gRPC 请求，上面的片段将匹配路由。\n同样，如果指定了 TLS 匹配器，它将根据提供的选项来匹配 TLS 上下文。在 tls_context 字段中，我们可以定义两个布尔值——presented 和 validated。presented字段检查证书是否被出示。validated字段检查证书是否被验证。\n例如：\n- match:prefix:\u0026#34;/\u0026#34;tls_context:presented:truevalidated:true如果一个证书既被出示又被验证，上述匹配评估为真。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"dd919aeaca1f50991a338952f0a1a207","permalink":"https://jimmysong.io/docs/envoy-handbook/hcm/request-matching/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/envoy-handbook/hcm/request-matching/","section":"envoy-handbook","summary":"本节将为你介绍 HTTP 连接管理器中的请求匹配。 路径匹配 我们只谈了一","tags":null,"title":"请求匹配","type":"book"},{"authors":null,"categories":null,"content":"我们在前面看到了通过客户端库来治理服务的架构图，那是我们在改造成服务网格架构前使用微服务架构通常的形式，下图是使用服务网格架构的最终形式。\n   服务网格架构示意图  当然在达到这一最终形态之前我们需要将架构一步步演进，下面给出的是参考的演进路线。\nIngress 或边缘代理 如果你使用的是 Kubernetes 做容器编排调度，那么在进化到服务网格架构之前，通常会使用 Ingress Controller，做集群内外流量的反向代理，如使用 Traefik 或 Nginx Ingress Controller。\n   Ingress 或边缘代理架构示意图  这样只要利用 Kubernetes 的原有能力，当你的应用微服务化并容器化需要开放外部访问且只需要 L7 代理的话这种改造十分简单，但问题是无法管理服务间流量。\n路由器网格 Ingress 或者边缘代理可以处理进出集群的流量，为了应对集群内的服务间流量管理，我们可以在集群内加一个 Router 层，即路由器层，让集群内所有服务间的流量都通过该路由器。\n   路由器网格架构示意图  这个架构无需对原有的单体应用和新的微服务应用做什么改造，可以很轻易的迁移进来，但是当服务多了管理起来就很麻烦。\nProxy per Node 这种架构是在每个节点上都部署一个代理，如果使用 Kubernetes 来部署的话就是使用 DaemonSet 对象，Linkerd 第一代就是使用这种方式部署的，一代的 Linkerd 使用 Scala 开发，基于 JVM 比较消耗资源，二代的 Linkerd 使用 Go 开发。\n   Proxy per node 架构示意图  这种架构有个好处是每个节点只需要部署一个代理即可，比起在每个应用中都注入一个 sidecar 的方式更节省资源，而且更适合基于物理机 / 虚拟机的大型单体应用，但是也有一些副作用，比如粒度还是不够细，如果一个节点出问题，该节点上的所有服务就都会无法访问，对于服务来说不是完全透明的。\nSidecar 代理 / Fabric 模型 这个一般不会成为典型部署类型，当企业的服务网格架构演进到这一步时通常只会持续很短时间，然后就会增加控制平面。跟前几个阶段最大的不同就是，应用程序和代理被放在了同一个部署单元里，可以对应用程序的流量做更细粒度的控制。\n   Sidecar代理/Fabric模型示意图  这已经是最接近服务网格架构的一种形态了，唯一缺的就是控制平面了。所有的 sidecar 都支持热加载，配置的变更可以很容易的在流量控制中反应出来，但是如何操作这么多 sidecar 就需要一个统一的控制平面了。\nSidecar 代理 / 控制平面 下面的示意图是目前大多数服务网格的架构图，也可以说是整个服务网格架构演进的最终形态。\n   Sidecar 代理/控制平面架构示意图  这种架构将代理作为整个服务网格中的一部分，使用 Kubernetes 部署的话，可以通过以 sidecar 的形式注入，减轻了部署的负担，可以对每个服务的做细粒度权限与流量控制。但有一点不好就是为每个服务都注入一个代理会占用很多资源，因此要想方设法降低每个代理的资源消耗。\n多集群部署和扩展 以上都是单个服务网格集群的架构，所有的服务都位于同一个集群中，服务网格管理进出集群和集群内部的流量，当我们需要管理多个集群或者是引入外部的服务时就需要网格扩展和多集群配置。\n","date":1651507200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"579d7786227e696ddb06b66890480223","permalink":"https://jimmysong.io/docs/cloud-native/service-mesh/service-mesh-patterns/","publishdate":"2022-05-03T00:00:00+08:00","relpermalink":"/docs/cloud-native/service-mesh/service-mesh-patterns/","section":"cloud-native","summary":"我们在前面看到了通过客户端库来治理服务的架构图，那是我们在改","tags":null,"title":"服务网格的部署模式","type":"book"},{"authors":null,"categories":null,"content":"自动化开始听起来很神奇，但我们要面对现实：计算机分析不能每天告诉你系统有什么问题或为你修复它。它只能为你节省时间。\n至此，我想停止夸赞，我必须承认自动化的一些局限性。我这样做是因为围绕结合人工智能和可观测性会有相当多的炒作。这种炒作导致了惊人的论断，大意是：“人工智能异常检测和根源分析可以完全诊断问题！” 和 “人工智能操作将完全管理你的系统！”\n谨防炒作 为了摆脱此类炒作，我想明确的是，这类梦幻般的营销主张并不是我所宣称的现代可观测性将提供的。事实上，我预测许多与人工智能问题解决有关的主张大多是夸大其词。\n为什么人工智能不能解决我们的问题？一般来说，机器无法识别软件中的 “问题”，因为定义什么是 “问题” 需要一种主观的分析方式。而我们所讨论的那种现实世界的人工智能不能以任何程度的准确性进行主观决策。机器将始终缺乏足够的背景。\n例如，我们说该版本降低了性能，这里面也隐含了一个期望，就是这个版本包含了每个用户都期望的新功能。对于这个版本，性能退步是一个特征，而不是一个错误。\n虽然它们可能看起来像类似的活动，但在确定相关关系和确定根本原因之间存在着巨大的鸿沟。当你有正确的数据结构时，相关性是一种客观的分析 —— 你只需要计算数字。哪些相关关系是相关的，并表明真正问题的来源，总是需要主观的分析，对数据的解释。所有这些相关关系意味着什么？正如杰弗里・李波斯基（Jeffrey Lebowski）所说：“嗯，你知道，这只是你的观点，伙计。”\n神奇的 AIOps 在调查一个系统时，有两种类型的分析起作用：\n 客观的分析，基于事实的、可衡量的、可观测的。 主观分析，基于解释、观点和判断的。  这种二分法 —— 客观与主观，与可计算性理论中一个重要的问题有关，即 停机问题（halting problem）。停机问题的定义是，在给定任意计算机程序及其输入的描述的情况下，是否可以编写一个计算机程序来确定任意程序是否会结束运行或永远继续运行。简而言之，在 1936 年，艾伦・图灵（Alan Turning）证明了解决停机问题的一般算法是不存在的，这个证明的延伸可以应用于计算机软件中许多形式的识别 “问题”。\n阿兰・图灵的意思是，我们没有办法拥有神奇的 AIOps（IT 运维的人工智能）。寻找那些承诺将繁琐的客观分析自动化的工具 —— 计算数字是机器的强项！但要小心那些声称能找到问题根源并自动修复的工具。它们很可能会让你失望。\n这就是为什么我们可以确定相关关系，但不能确定因果关系：想象一下，有一台机器可以确定任意计算机程序中的问题行为是什么，并确定该行为的根本原因。如果我们真的造出了这样一台机器，那就是开香槟的时候了，因为这意味着我们终于解决了停机问题！但是，目前还没有迹象表明，机器可以解决这个问题。然而，没有迹象表明机器学习已经超越了艾伦・图灵的统一计算模型；你可以相信，这不会发生。\n做出正确的决定和修复的工作还是要靠你自己。\n时间是最宝贵的资源 然而，我们不需要神奇的 AIOps 来看到我们工作流程的巨大改善。识别相关性，同时获取相关信息，以便你能有效地浏览这些信息，这是计算机绝对可以做到的事情！这将为你节省时间。大量的时间。这么多的时间，它将从根本上改变你调查系统的方式。\n减少浪费的时间是实践现代可观测性的核心。即使是在简单的系统中，通过分析数字来识别相关性也是很困难的，而在大规模的系统中，这几乎是不可能的。通过将认知负担转移到机器上，运维人员能够有效地管理那些已经超出人类头脑所能容纳的系统。\n但是，我们分析遥测方式的这种转变并不是可观测性世界中即将发生的唯一重大变化。我们需要的大部分遥测数据来自于我们没有编写的软件：我们所依赖的开源库、数据库和管理服务。这些系统在传统上一直在为产生遥测数据而奋斗。我们可以获得哪些数据，以及这些数据来自哪里，也将发生根本性的变化。\n","date":1643990400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"72aa94d210f61545bd73808ac66f7ad5","permalink":"https://jimmysong.io/docs/opentelemetry-obervability/the-limitations-of-automated-analysis/","publishdate":"2022-02-05T00:00:00+08:00","relpermalink":"/docs/opentelemetry-obervability/the-limitations-of-automated-analysis/","section":"opentelemetry-obervability","summary":"第 3 章：自动分析的局限性","tags":null,"title":"第 3 章：自动分析的局限性","type":"book"},{"authors":null,"categories":null,"content":"为什么尽快进行 Code Review？ 在Google，我们优化了开发团队共同开发产品的速度。，而不是优化单个开发者编写代码的速度。个人开发的速度很重要，它并不如整个团队的速度那么重要。\n当代码审查很慢时，会发生以下几件事：\n 整个团队的速度降低了。是的，对审查没有快速响应的个人的确完成了其他工作。但是，对于团队其他人来说重要的新功能与缺陷修復将会被延迟数天、数周甚至数月，只因为每个 CL 正在等待审查和重新审查。 开发者开始抗议代码审查流程。如果审查者每隔几天只响应一次，但每次都要求对 CL 进行重大更改，那么开发者可能会变得沮丧。通常，开发者将表达对审查者过于“严格”的抱怨。如果审查者请求相同实质性更改（确实可以改善代码健康状况），但每次开发者进行更新时都会快速响应，则抱怨会逐渐消失。大多数关于代码审查流程的投诉实际上是通过加快流程来解决的。 代码健康状况可能会受到影响。如果审查速度很慢，则造成开发者提交不尽如人意的 CL 的压力会越来越大。审查太慢还会阻止代码清理、重构以及对现有 CL 的进一步改进。  Code Review 应该有多快？ 如果您没有处于重点任务的中，那么您应该在收到代码审查后尽快开始。\n一个工作日。是应该响应代码审查请求所需的最长时间（即第二天早上的第一件事）。\n遵循这些指导意味着典型的 CL 应该在一天内进行多轮审查（如果需要）。\n速度 vs. 中断 有一种情况下个人速度胜过团队速度。如果您正处于重点任务中，例如编写代码，请不要打断自己进行代码审查。研究表明，开发人员在被打断后需要很长时间才能恢复到顺畅的开发流程中。因此，编写代码时打断自己实际上比让另一位开发人员等待代码审查的代价更加昂贵。\n相反，在回复审查请求之前，请等待工作中断点。可能是当你的当前编码任务完成，午餐后，从会议返回，从厨房回来等等。\n快速响应 当我们谈论代码审查的速度时，我们关注的是响应时间，而不是 CL 需要多长时间才能完成整个审查并提交。理想情况下，整个过程也应该是快速的，快速的个人响应比整个过程快速发生更为重要。\n即使有时需要很久才能完成整个审查流程，但在整个过程中获得审查者的快速响应可以显着减轻开发人员对“慢速”代码审查感到的挫败感。\n如果您太忙而无法对 CL 进行全面审查，您仍然可以发送快速回复，让开发人员知道您什么时候可以开始，或推荐其他能够更快回复的审查人员，或者提供一些大体的初步评论。 （注意：这并不意味着您应该中断编码，即使发送这样的响应，也要在工作中的合理断点处发出响应。）\n重要的是，审查人员要花足够的时间进行审查，确信他们的“LGTM”意味着“此代码符合我们的标准。”但是，理想情况下，个人反应仍然应该很快。\n跨时区审查 在处理时区差异时，尝试在他们还在办公室时回复作者。 如果他们已经下班回家了，那么请确保在第二天回到办公室之前完成审查。\n带评论的 LGTM 为了加快代码审查，在某些情况下，即使他们也在 CL 上留下未解决的评论，审查者也应该给予 LGTM/Approval，这可以是以下任何一种情况：\n 审查者确信开发人员将适当地处理所有审查者的剩余评论。 其余的更改很小，不必由开发者完成。  如果不清楚的话，审查者应该指定他们想要哪些选项。\n当开发者和审查者处于不同的时区时，带评论的 LGTM 尤其值得考虑，否则开发者将等待一整天才能获得 “LGTM，Approval”。\n大型 CL 如果有人向您发送了代码审查太大，您不确定何时有时间查看，那么您应该要求开发者将 CL 拆分为几个较小的 CL 而不是一次审查的一个巨大的 CL。这通常可行，对审查者非常有帮助，即使需要开发人员的额外工作。\n如果 CL 无法分解为较小的 CL，并且您没有时间快速查看整个内容，那么至少要对 CL 的整体设计写一些评论并将其发送回开发人员以进行改进。作为审查者，您的目标之一应该在不牺牲代码健康状况的前提下，始终减少开发者能够快速采取某种进一步的操作的阻力。\n代码审查随时间推移而改进 如果您遵循这些准则，并且您对代码审查非常严格，那么您应该会发现整个代码审核流程会随着时间的推移而变得越来越快。开发者可以了解健康代码所需的内容，并向您发送从一开始就很棒的 CL，且需要的审查时间越来越短。审查者学会快速响应，而不是在审查过程中添加不必要的延迟。但是，从长远来看，不要为了提高想象中的代码审查速度，而在代码审查标准或质量方面妥协，实际上这样做对于长期来说不会有任何帮助。\n紧急情况 还有一些紧急情况，CL 必须非常快速地通过整个审查流程，并且质量准则将放宽。请查看什么是紧急情况？ 中描述的哪些情况属于紧急情况，哪些情况不属于紧急情况。\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"fc0f0fdc9e39a211c52bfc8272fd66cd","permalink":"https://jimmysong.io/docs/eng-practices/review/reviewer/speed/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/docs/eng-practices/review/reviewer/speed/","section":"eng-practices","summary":"为什么尽快进行 Code Review？ 在Google，我们优化了开发","tags":null,"title":"Code Review 速度","type":"book"},{"authors":null,"categories":null,"content":"Kubernetes，经常被缩写为 “K8s”，是一个开源的容器或编排系统，用于自动部署、扩展和管理容器化应用程序。它管理着构成集群的所有元素，从应用中的每个微服务到整个集群。与单体软件平台相比，将容器化应用作为微服务使用可以提供更多的灵活性和安全优势，但也可能引入其他复杂因素。\n   图 1：Kubernetes 集群组件的高层视图  本指南重点关注安全挑战，并尽可能提出适用于国家安全系统和关键基础设施管理员的加固策略。尽管本指南是针对国家安全系统和关键基础设施组织的，但也鼓励联邦和州、地方、部落和领土（SLTT）政府网络的管理员实施所提供的建议。Kubernetes 集群的安全问题可能很复杂，而且经常在利用其错误配置的潜在威胁中被滥用。以下指南提供了具体的安全配置，可以帮助建立更安全的 Kubernetes 集群。\n建议 每个部分的主要建议摘要如下：\n Kubernetes Pod 安全  使用构建的容器，以非 root 用户身份运行应用程序 在可能的情况下，用不可变的文件系统运行容器 扫描容器镜像，以发现可能存在的漏洞或错误配置 使用 Pod 安全政策来执行最低水平的安全，包括:  防止有特权的容器 拒绝经常被利用来突破的容器功能，如 hostPID、hostIPC、hostNetwork、allowedHostPath 等 拒绝以 root 用户身份执行或允许提升为根用户的容器 使用安全服务，如 SELinux®、AppArmor® 和 seccomp，加固应用程序，防止被利用。     网络隔离和加固  使用防火墙和基于角色的访问控制（RBAC）锁定对控制平面节点的访问 进一步限制对 Kubernetes etcd 服务器的访问 配置控制平面组件，使用传输层安全（TLS）证书进行认证、加密通信 设置网络策略来隔离资源。不同命名空间的 Pod 和服务仍然可以相互通信，除非执行额外的隔离，如网络策略 将所有凭证和敏感信息放在 Kubernetes Secret 中，而不是配置文件中。使用强大的加密方法对 Secret 进行加密   认证和授权  禁用匿名登录（默认启用） 使用强大的用户认证 创建 RBAC 策略以限制管理员、用户和服务账户活动   日志审计  启用审计记录（默认为禁用） 在节点、Pod 或容器级故障的情况下，持续保存日志以确保可用性 配置一个 metric logger   升级和应用安全实践  立即应用安全补丁和更新 定期进行漏洞扫描和渗透测试 当组件不再需要时，将其从环境中移除    架构概述 Kubernetes 使用集群架构。一个 Kubernetes 集群是由一些控制平面和一个或多个物理或虚拟机组成的，称为工作节点。工作者节点承载 Pod，其中包含一个或多个容器。容器是包含软件包及其所有依赖关系的可执行镜像。见图 2：Kubernetes 架构。\n   控制平面对集群进行决策。这包括调度容器的运行，检测 / 应对故障，并在部署文件中指定的副本数量没有得到满足时启动新的 Pod。以下逻辑组件都是控制平面的一部分：\n Controller manager（默认端口：10252） - 监视 Kubernetes 集群，以检测和维护 Kubernetes 环境的几个方面，包括将 Pod 加入到服务中，保持一组 Pod 的正确数量，并对节点的丢失做出反应。 Cloud controller manager（默认端口：10258） - 一个用于基于云的部署的可选组件。云控制器与云服务提供商接口，以管理集群的负载均衡器和虚拟网络。 Kubernetes API Server（默认端口：6443 或 8080） - 管理员操作 Kubernetes 的接口。因此，API 服务器通常暴露在控制平面之外。API 服务器被设计成可扩展的，可能存在于多个控制平面节点上。 Etcd（默认端口范围：2379-2380） - 持久化的备份存储，关于集群状态的所有信息都保存在这里。Etcd 不应该被直接操作，而应该通过 API 服务器来管理。 Scheduler（默认端口：10251） - 跟踪工作节点的状态并决定在哪里运行 Pod。Kube-scheduler 只可以由控制平面内的节点访问。  Kubernetes 工作节点是专门为集群运行容器化应用的物理或虚拟机。除了运行容器引擎外，工作节点还承载以下两个服务，允许从控制平面进行协调：\n Kubelet（默认端口：10250） - 在每个工作节点上运行，以协调和验证 Pod 的执行。 Kube-proxy - 一个网络代理，使用主机的数据包过滤能力，确保 Kubernetes 集群中数据包的正确路由。  集群通常使用云服务提供商（CSP）的 Kubernetes 服务或在企业内部托管。在设计 Kubernetes 环境时，组织应了解他们在安全维护集群方面的责任。CSP 管理大部分的 Kubernetes 服务，但组织可能需要处理某些方面，如认证和授权。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"ed1dff92944751022cd7da7ab5ac5119","permalink":"https://jimmysong.io/docs/kubernetes-hardening-guidance/introduction/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/kubernetes-hardening-guidance/introduction/","section":"kubernetes-hardening-guidance","summary":"Kubernetes，经常被缩写为 “K8s\u0026rd","tags":null,"title":"简介","type":"book"},{"authors":null,"categories":null,"content":"Envoy 支持在同一虚拟主机内将流量分割到不同的路由。我们可以在两个或多个上游集群之间分割流量。\n有两种不同的方法。第一种是使用运行时对象中指定的百分比，第二种是使用加权集群。\n使用运行时的百分比进行流量分割 使用运行时对象的百分比很适合于金丝雀发布或渐进式交付的场景。在这种情况下，我们想把流量从一个上游集群逐渐转移到另一个。\n实现这一目标的方法是提供一个 runtime_fraction 配置。让我们用一个例子来解释使用运行时百分比的流量分割是如何进行的。\nroute_config:virtual_hosts:- name:hello_vhostdomains:[\u0026#34;hello.io\u0026#34;]routes:- match:prefix:\u0026#34;/\u0026#34;runtime_fraction:default_value:numerator:90denominator:HUNDREDroute:cluster:hello_v1- match:prefix:\u0026#34;/\u0026#34;route:cluster:hello_v2上述配置声明了两个版本的 hello 服务：hello_v1 和 hello_v2。\n在第一个匹配中，我们通过指定分子（90）和分母（HUNDRED）来配置 runtime_fraction 字段。Envoy 使用分子和分母来计算最终的分数值。在这种情况下，最终值是 90%（90/100 = 0.9 = 90%）。\nEnvoy 在 [0，分母] 范围内生成一个随机数（例如，在我们的案例中是 [0，100]）。如果随机数小于分子值，路由器就会匹配该路由，并将流量发送到我们案例中的集群 hello_v1。\n如果随机数大于分子值，Envoy 继续评估其余的匹配条件。由于我们有第二条路由的精确前缀匹配，所以它是匹配的，Envoy 会将流量发送到集群 hello_v2。一旦我们把分子值设为 0，所有随机数会大于分子值。因此，所有流量都会流向第二条路由。\n我们也可以在运行时键中设置分子值。例如：\nroute_config:virtual_hosts:- name:hello_vhostdomains:[\u0026#34;hello.io\u0026#34;]routes:- match:prefix:\u0026#34;/\u0026#34;runtime_fraction:default_value:numerator:0denominator:HUNDREDruntime_key:routing.hello_ioroute:cluster:hello_v1- match:prefix:\u0026#34;/\u0026#34;route:cluster:hello_v2...layered_runtime:layers:- name:static_layerstatic_layer:routing.hello_io:90在这个例子中，我们指定了一个名为 routing.hello_io 的运行时键。我们可以在配置中的分层运行时字段下设置该键的值——这也可以从文件或通过运行时发现服务（RTDS）动态读取和更新。为了简单起见，我们在配置文件中直接设置。\n当 Envoy 这次进行匹配时，它将看到提供了runtime_key，并将使用该值而不是分子值。有了运行时键，我们就不必在配置中硬编码这个值了，我们可以让 Envoy 从一个单独的文件或 RTDS 中读取它。\n当你有两个集群时，使用运行时百分比的方法效果很好。但是，当你想把流量分到两个以上的集群，或者你正在运行 A/B 测试或多变量测试方案时，它就会变得复杂。\n使用加权集群进行流量分割 当你在两个或多个版本的服务之间分割流量时，加权集群的方法是理想的。在这种方法中，我们为多个上游集群分配了不同的权重。而带运行时百分比的方法使用了许多路由，我们只需要为加权集群提供一条路由。\n我们将在下一个模块中进一步讨论上游集群。为了解释用加权集群进行的流量分割，我们可以把上游集群看成是流量可以被发送到的终端的集合。\n我们在路由内指定多个加权集群（weighted_clusters），而不是设置一个集群（cluster）。\n继续前面的例子，我们可以这样重写配置，以代替使用加权集群。\nroute_config:virtual_hosts:- name:hello_vhostdomains:[\u0026#34;hello.io\u0026#34;]routes:- match:prefix:\u0026#34;/\u0026#34;route:weighted_clusters:clusters:- name:hello_v1weight:90- name:hello_v2weight:10在加权的集群下，我们也可以设置 runtime_key_prefix，它将从运行时密钥配置中读取权重。注意，如果运行时密钥配置不在那里，Envoy 会使用每个集群旁边的权重。\nroute_config:virtual_hosts:- name:hello_vhostdomains:[\u0026#34;hello.io\u0026#34;]routes:- match:prefix:\u0026#34;/\u0026#34;route:weighted_clusters:runtime_key_prefix:routing.hello_ioclusters:- name:hello_v1weight:90- name:hello_v2weight:10...layered_runtime:layers:- name:static_layerstatic_layer:routing.hello_io.hello_v1:90routing.hello_io.hello_v2:10权重代表 Envoy 发送给上游集群的流量的百分比。所有权重的总和必须是 100。然而，使用 total_weight 字段，我们可以控制所有权重之和必须等于的值。例如，下面的片段将 total_weight 设置为 15。\nroute_config:virtual_hosts:- name:hello_vhostdomains:[\u0026#34;hello.io\u0026#34;]routes:- match:prefix:\u0026#34;/\u0026#34;route:weighted_clusters:runtime_key_prefix:routing.hello_iototal_weight:15clusters:- name:hello_v1weight:5- name:hello_v2weight:5- name:hello_v3weight:5为了动态地控制权重，我们可以设置 runtime_key_prefix。路由器使用运行时密钥前缀值来构建与每个集群相关的运行时密钥。如果我们提供了运行时密钥前缀，路由器将检查 runtime_key_prefix + \u0026#34;.\u0026#34; + cluster_name 的值，其中 cluster_name 表示集群数组中的条目（例如 hello_v1、hello_v2）。如果 Envoy 没有找到运行时密钥，它将使用配置中指定的值作为默认值。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"a2618a1eda79d1c40def36d07c63ecdf","permalink":"https://jimmysong.io/docs/envoy-handbook/hcm/traffic-splitting/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/envoy-handbook/hcm/traffic-splitting/","section":"envoy-handbook","summary":"Envoy 支持在同一虚拟主机内将流量分割到不同的路由。我们可以在两个","tags":null,"title":"流量分割","type":"book"},{"authors":null,"categories":null,"content":"到目前为止，我们已经从数据的角度讨论了现代可观测性。但是，现代可观测性还有另一个方面，从长远来看，它可能被证明同样重要：如何对待产生数据的仪表（instrumention）。\n大多数软件系统都是用现成的部件构建的：网络框架、数据库、HTTP 客户端、代理服务器、编程语言。在大多数组织中，很少有这种软件基础设施是在内部编写的。相反，这些组件是在许多组织中共享的。最常见的是，这些共享组件是以开放源码（OSS）库的形式出现的，并具有许可权。\n由于这些开放源码软件库几乎囊括了一般系统中的所有关键功能，因此获得这些库的高质量说明对大多数可观测性系统来说至关重要。\n传统上，仪表是 “单独出售” 的。这意味着，软件库不包括产生追踪、日志或度量的仪表。相反，特定解决方案的仪表是在事后添加的，作为部署可观测系统的一部分。\n 什么是特定解决方案仪表？\n在本章中，术语 “特定解决方案仪表” 是指任何旨在与特定的可观测系统一起工作的仪表，使用的是作为该特定系统的数据存储系统的产物而开发的客户端。在这些系统中，客户端和存储系统常常深深地融合在一起。因此，如果一个应用要从一个观测系统切换到另一个观测系统，通常需要进行全面的重新布设。\n 针对解决方案的仪表是 “三大支柱” 中固有的垂直整合的遗留问题。每个后端都摄取特定类型的专有数据；因此，这些后端的创建者也必须提供产生这些数据的仪表。\n这种工具化的方法给参与软件开发的每个人都带来了麻烦：供应商、用户和开放源码库的作者。\n可观测性被淹没在特定解决方案的仪表中 从可观测性系统的角度来看，仪表化代表了巨大的开销。\n在过去，互联网应用是相当同质化的，可以围绕一个特定的网络框架来建立可观测性系统。Java Spring、Ruby on Rails 或 .NET。但随着时间的推移，软件的多样性已经爆炸性增长。现在为每一个流行的网络框架和数据库客户端维护仪表是一项巨大的负担。\n这导致的重复劳动难以估量。传统上，供应商将他们在仪表上的投资作为销售点和把关的一种形式。但是，日益增长的软件开发速度已经开始使这种做法无法维持了。对于一个合理规模的仪表设备团队来说，覆盖面实在是太大了，无法跟上。\n这种负担对于新的、新颖的观测系统，特别是开放源码软件的观测项目来说尤其严峻。如果一个新的系统在编写了大量的仪表之前无法在生产中部署，而一个开放源码软件项目在广泛部署之前也无法吸引开发者的兴趣，那么科学进步就会陷入僵局。这对于基于追踪的系统来说尤其如此，它需要端到端的仪表来提供最大的价值。\n应用程序被锁定在特定解决方案的仪表中 从应用开发者的角度来看，特定解决方案的仪表代表了一种有害的锁定形式。\n可观测性是一个交叉性的问题。要彻底追踪、记录或度量一个大型的应用程序，意味着成千上万的仪表 API 调用将遍布整个代码库。改变可观测性系统需要把所有这些工具去掉，用新系统提供的不同工具来代替。\n替换仪表是一项重大的前期投资，即使只是为了尝试一个新的系统。更糟的是，大多数系统都太大了以致于在所有服务中同时更换所有仪表是不可行的。大多数系统需要逐步推出新的仪表设备，但这样的上线可能很难设计。\n被特定解决方案的仪表所 “困住” 是非常令人沮丧的。在可观测性供应商开始努力提供自己的仪表的同时，用户也开始拒绝采用这种仪表。由于了解到重新安装仪表的工作量，许多用户强烈希望他们正在考虑的任何新的观测系统能与他们目前使用的仪表一起工作。\n为了支持这一要求，许多可观测性系统试图与其他几个系统提供的仪表一起工作。但这种拼凑的方式降低了每个系统所摄取的数据的质量。从许多来源摄取数据意味着对输入的数据不再有明确的定义，当预期的数据不均衡且定义模糊时，分析工具就很难完成它们的工作。\n针对开源软件的特定解决方案的仪表基本上是不可能的 从一个开放源码库作者的角度来看，特定解决方案的仪表化是一个悲剧。\n来自开放源码软件库的遥测数据对于操作建立在它们之上的应用程序来说至关重要。最了解哪些数据对操作至关重要的人，以及操作者应该如何利用这些数据来补救问题的人，就是实际编写软件的开源库的开发者。\n但是，库的作者却陷入了困境。正如我们将看到的，没有任何一个特定解决方案的仪表化 API，无论写得多么好，都无法作为开放源码库可以接受的选择。\n如何挑选一个日志库？ 假设你正在编写世界上最伟大的开源网络框架。在生产过程中，很多事情都会出错，你自然希望把错误、调试和性能信息传达给你的用户。你使用哪个日志库？\n有很多体面的日志库。事实上，有很多，无论你选择哪个库，你都会有很多用户希望你选择一个不同的库。如果你的 Web 框架选择了一个日志库，而数据库客户端库选择了另一个，怎么办？如果这两个都不是用户想要使用的呢？如果他们选择了同一个库的不兼容的版本呢？\n没有一个完美的方法可以将多个特定解决方案的日志库组合成一个连贯的系统。虽然日志足够简单，不同解决方案的大杂烩可能是可行的，但对于特定解决方案的指标和追踪来说，情况并非如此。\n因此，开放源码软件库通常没有内置的日志、度量或追踪功能。取而代之的是，库提供了 “可观测性钩子”，这需要用户编写和维护一堆适配器，将使用的库连接到他们的可观测性系统上。\n作者们有大量的知识，他们想通信关于他们的系统应该如何运行的知识，但他们没有明确的方法去做。如果你问任何写过大量开源软件的人，他们会告诉你。这种情况是痛苦的！而且是不幸的！一些库的作者确实试图选择一个日志库，但却发现他们无意中为一些用户造成了版本冲突，同时迫使其他用户编写日志适配器来捕捉使用其实际日志库的数据。\n但是对于大多数库来说，可观测性只是一个事后的想法。虽然库的作者经常编写大量的测试套件，但他们很少花时间去考虑运行时的可观测性。考虑到库有大量的测试工具，但可观测性工具为零，这种结果并不令人惊讶。\n正如我们在接下来的几章中所看到的，现代可观测性的设计是为了使在可观测性管道中发挥作用的每个人的代理权最大化。但受益最大的是库的作者；对于特定解决方案的仪表，他们目前根本没有选择。\n分解问题 我们可以通过设计一个可观测性系统来解决上面列出的所有问题，以明确地解决每个人的需求。在本章的其余部分，我们将把现代观测系统的设计分解为基本要求。这些要求将为第五章中描述的 OpenTelemetry 的结构提供动力。\n要求：独立的仪表、遥测和分析 归根结底，计算机系统实际上就是人类系统。像可观测性这样的跨领域问题，几乎与每一个软件组件都有互动。同时，传输和处理遥测数据可能是一个大批量的活动，以至于一个大规模的观测系统会产生自己的操作问题。这意味着，许多不同的人，以不同的身份，需要与观测系统的不同方面交互。为了很好地服务于他们，这个系统必须确保每个参与其中的人都有他们所需要的代理权，以便快速和独立地执行任务。提供代理权是设计一个有效的观测系统的基本要求。\n让我们首先确定与运行中的软件系统有关的每个角色的责任：库的作者、应用程序的所有者、操作者和响应者。\n库的作者了解他们软件的情况\n对于封装了关键功能的软件库，如网络和请求管理，库的作者也必须管理追踪系统的各个方面：注入、提取和上下文传播。\n应用程序拥有者组织软件并管理依赖关系\n应用程序所有者选择构成其应用程序的组件，并确保它们编译成一个连贯的、有功能的系统。应用程序所有者还编写应用程序级别的工具，它必须与库作者提供的指令（和上下文传播）进行正确的交互。\n运维人员管理遥测的生产和传输\n运维管理从应用到响应者的可观测性数据的传输。他们必须能够选择数据的格式以及数据的发送地点。当数据在产生时，他们必须操作传输系统：管理缓冲、处理和传送数据所需的所有资源。\n响应者消费遥测数据并产生有用的见解\n要做到这一点，应对者必须了解数据结构及其内在意义（结构和意义将在第三章中详细描述）。当新的和改进的分析工具出现时，反应者还需要将其添加到他们的工具箱中。\n这些角色代表不同的决策点：\n 库的作者只能通过发布其代码的新版本来进行修改。 应用程序所有者只能通过部署其可执行文件的新版本来进行更改。 运维人员只能通过管理可执行文件的拓扑结构和配置来进行改变。 响应者只能根据他们收到的数据做出改变。  传统的三大支柱方法扰乱了所有这些角色。垂直整合的一个副作用是，几乎所有的数据变化都需要进行代码修改。几乎任何对可观测性系统的非微不足道的改变都需要应用程序所有者进行代码修改。要求其他人进行你所关心的改变，这样会有很大的阻力，并可能导致压力、冲突和不作为。\n显然，一个设计良好的可观测性系统应该侧重于允许每个人尽可能多的代理和直接控制，它应该避免将开发者变成意外的看门人。\n要求：零依赖性 应用程序是由依赖关系（网络框架、数据库客户端），加上依赖关系的依赖关系（OpenTelemetry 或其他仪表库），加上它们的依赖关系的依赖关系的依赖关系（无论这些仪表库依赖什么）组成。这些都被称为反式的依赖关系。\n如果任何两个依赖关系之间有冲突，应用程序就无法运行。例如，两个库可能分别需要一个不同的（不兼容的）底层网络库的版本，如 gRPC。这可能会导致一些不好的情况。例如，一个新版本的库可能包括一个需要的安全补丁，但也包括一个升级的依赖关系，这就产生了依赖关系冲突。\n诸如此类的过渡性依赖冲突给应用程序所有者带来了很大的麻烦，因为这些冲突无法独立解决。相反，应用程序所有者必须联系库的作者，要求他们提供一个解决方案，这最终需要时间（假设库的作者回应了这个请求）。\n为了使现代可观测性发挥作用，库必须能够嵌入仪表，而不必担心当他们的库被用于组成应用程序时而导致问题。因此，可观测性系统必须提供不包含可能无意中引发横向依赖冲突的依赖性的仪表。\n要求：严格的后向兼容和长期支持 当一个仪表化的 API 破坏了向后的兼容性，坏事就会发生。一个精心设计的应用程序最终可能会有成千上万的仪表调用站点。由于 API 的改变而不得不更新数以千计的调用站点是一个相当大的工作量。\n这就产生了一种特别糟糕的依赖性冲突，即一个库中的仪表化不再与另一个库中的仪表化兼容。\n因此，仪表化 API 必须在很长的时间范围内具有严格的向后兼容能力。理想的情况是，仪表化 API 一旦变得稳定，就永远不会破坏向后兼容。新的、实验性的 API 功能的开发方式必须保证它们的存在不会在包含稳定仪表的库之间产生冲突。\n分离关注点是良好设计的基础 在下一章中，我们将深入研究 OpenTelemetry 的架构，看看它是如何满足上面提出的要求的。但在这之前，我想说的是一个重要的问题。\n如果你分析这些需求，你可能会注意到一些奇特的现象：它们中几乎没有任何专门针对可观测性的内容。相反，重点是尽量减少依赖性，保持向后的兼容性，并确保不同的用户可以在没有无谓干扰的情况下发挥作用。\n每一个要求都指出了关注点分离是一个关键的设计特征。但是这些特性并不是 OpenTelemetry 所独有的。任何寻求广泛采用的软件库都会很好地包括它们。在这个意义上，OpenTelemetry 的设计也可以作为设计一般的开源软件的指南。下次当你开始一个新的开放源码软件项目时，请预先考虑这些要求，相应地设计你的库及你的用户会感谢你的。\n","date":1643990400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"1ef167264065609609abcbc50f579f10","permalink":"https://jimmysong.io/docs/opentelemetry-obervability/supporting-open-source-and-native-instrumentation/","publishdate":"2022-02-05T00:00:00+08:00","relpermalink":"/docs/opentelemetry-obervability/supporting-open-source-and-native-instrumentation/","section":"opentelemetry-obervability","summary":"第 4 章：支持开源和原生监测","tags":null,"title":"第 4 章：支持开源和原生监测","type":"book"},{"authors":null,"categories":null,"content":"在这一节中，我们将解释 Envoy 的基本构建模块。\nEnvoy 配置的根被称为引导配置。它包含了一些字段，我们可以在这里提供静态或动态的资源和高级别的 Envoy 配置（例如，Envoy 实例名称、运行时配置、启用管理界面等等）。\n为了开始学习，我们将主要关注静态资源，在课程的后面，我们将介绍如何配置动态资源。\nEnvoy 输出许多统计数据，这取决于启用的组件和它们的配置。我们会在整个课程中提到不同的统计信息，在课程后面的专门模块中，我们会更多地讨论统计信息。\n下图显示了通过这些概念的请求流。\n   Envoy 构建块  这一切都从监听器开始。Envoy 暴露的监听器是命名的网络位置，可以是一个 IP 地址和一个端口，也可以是一个 Unix 域套接字路径。Envoy 通过监听器接收连接和请求。考虑一下下面的 Envoy 配置。\nstatic_resources:listeners:- name:listener_0address:socket_address:address:0.0.0.0port_value:10000filter_chains:[{}]通过上面的 Envoy 配置，我们在 0.0.0.0 地址的 10000 端口上声明了一个名为 listener_0 的监听器。这意味着 Envoy 正在监听 0.0.0.0:10000 的传入请求。\n每个监听器都有不同的部分需要配置。然而，唯一需要的设置是地址。上述配置是有效的，你可以用它来运行 Envoy—— 尽管它没有用，因为所有的连接都会被关闭。\n我们让 filter_chains 字段为空，因为在接收数据包后不需要额外的操作。\n为了进入下一个构件（路由），我们需要创建一个或多个网络过滤器链（filter_chains），至少要有一个过滤器。\n网络过滤器通常对数据包的有效载荷进行操作，查看有效载荷并对其进行解析。例如，Postgres 网络过滤器解析数据包的主体，检查数据库操作的种类或其携带的结果。\nEnvoy 定义了三类过滤器：监听器过滤器、网络过滤器和 HTTP 过滤器。监听器过滤器在收到数据包后立即启动，通常对数据包的头信息进行操作。监听器过滤器包括代理监听器过滤器（提取 PROXY 协议头），或 TLS 检查器监听器过滤器（检查流量是否为 TLS，如果是，则从 TLS 握手中提取数据）。\n每个通过监听器进来的请求可以流经多个过滤器。我们还可以写一个配置，根据传入的请求或连接属性选择不同的过滤器链。\n   过滤器链  一个特殊的、内置的网络过滤器被称为 HTTP 连接管理器过滤器（HTTP Connection Manager Filter）或 HCM。HCM 过滤器能够将原始字节转换为 HTTP 级别的消息。它可以处理访问日志，生成请求 ID，操作头信息，管理路由表，并收集统计数据。我们将在以后的课程中对 HCM 进行更详细的介绍。\n就像我们可以为每个监听器定义多个网络过滤器（其中一个是 HCM）一样，Envoy 也支持在 HCM 过滤器中定义多个 HTTP 级过滤器。我们可以在名为 http_filters 的字段下定义这些 HTTP 过滤器。\n   HCM 过滤器  HTTP 过滤器链中的最后一个过滤器必须是路由器过滤器（envoy.filters.HTTP.router）。路由器过滤器负责执行路由任务。这最终把我们带到了第二个构件 —— 路由。\n我们在 HCM 过滤器的 route_config 字段下定义路由配置。在路由配置中，我们可以通过查看元数据（URI、Header 等）来匹配传入的请求，并在此基础上，定义流量的发送位置。\n路由配置中的顶级元素是虚拟主机。每个虚拟主机都有一个名字，在发布统计数据时使用（不用于路由），还有一组被路由到它的域。\n让我们考虑下面的路由配置和域的集合。\nroute_config:name:my_route_configvirtual_hosts:- name:tetrate_hostsdomains:[\u0026#34;tetrate.io\u0026#34;]routes:...- name:test_hostsdomains:[\u0026#34;test.tetrate.io\u0026#34;,\u0026#34;qa.tetrate.io\u0026#34;]routes:...如果传入请求的目的地是 tetrate.io（即 Host/Authority 标头被设置为其中一个值），则 tetrate_hosts  虚拟主机中定义的路由将得到处理。\n同样，如果 Host/Authority 标头包含 test.tetrate.io 或 qa.tetrate.io，test_hosts 虚拟主机下的路由将被处理。使用这种设计，我们可以用一个监听器（0.0.0.0:10000）来处理多个顶级域。\n如果你在数组中指定多个域，搜索顺序如下：\n 精确的域名（例如：tetrate.io）。 后缀域名通配符（如 *.tetrate.io）。 前缀域名通配符（例如：tetrate.*）。 匹配任何域的特殊通配符（*）。  在 Envoy 匹配域名后，是时候处理所选虚拟主机中的 routes 字段了。这是我们指定如何匹配一个请求，以及接下来如何处理该请求（例如，重定向、转发、重写、发送直接响应等）的地方。\n我们来看看一个例子。\nstatic_resources:listeners:- name:listener_0address:socket_address:address:0.0.0.0port_value:10000filter_chains:- filters:- name:envoy.filters.network.http_connection_managertyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManagerstat_prefix:hello_world_servicehttp_filters:- name:envoy.filters.http.routerroute_config:name:my_first_routevirtual_hosts:- name:direct_response_servicedomains:[\u0026#34;*\u0026#34;]routes:- match:prefix:\u0026#34;/\u0026#34;direct_response:status:200body:inline_string:\u0026#34;yay\u0026#34;配置的顶部部分与我们之前看到的一样。我们已经添加了 HCM 过滤器、统计前缀（hello_world_service）、单个 HTTP 过滤器（路由器）和路由配置。\n在虚拟主机内，我们要匹配任何域名。在 routes 下，我们匹配前缀（/），然后我们可以发送一个响应。\n当涉及到匹配请求时，我们有多种选择。\n   路由匹配 描述 示例     prefix 前缀必须与:path 头的开头相符。 /hello 与 hello.com/hello、hello.com/helloworld 和 hello.com/hello/v1 匹配。   path 路径必须与:path 头完全匹配。 /hello 匹配 hello.com/hello，但不匹配 hello.com/helloworld 或 hello.com/hello/v1   safe_regex 所提供的正则表达式必须与:path 头匹配。 /\\{3} 匹配任何以 / 开头的三位数。例如，与 hello.com/123 匹配，但不能匹配 hello.com/hello 或 hello.com/54321。   connect_matcher 匹配器只匹配 CONNECT 请求。     一旦 Envoy 将请求与路由相匹配，我们就可以对其进行路由、重定向或返回一个直接响应。在这个例子中，我们通过 direct_response 配置字段使用直接响应。\n你可以把上述配置保存到 envoy-direct-response.yaml 中。\n我们将使用一个名为 func-e 的命令行工具。func-e 允许我们选择和使用不同的 Envoy 版本。\n我们可以通过运行以下命令下载 func-e CLI。\ncurl https://func-e.io/install.sh | sudo bash -s -- -b /usr/local/bin 现在我们用我们创建的配置运行 Envoy。\nfunc-e run -c envoy-direct-response.yaml 一旦 Envoy 启动，我们就可以向 localhost:10000 发送一个请求，以获得我们配置的直接响应。\n$ curl localhost:10000 yay 同样，如果我们添加一个不同的主机头（例如 -H \u0026#34;Host: hello.com\u0026#34;）将得到相同的响应，因为 hello.com 主机与虚拟主机中定义的域相匹配。\n在大多数情况下，从配置中直接发送响应是一个很好的功能，但我们会有一组端点或主机，我们将流量路由到这些端点或主机。在 Envoy 中做到这一点的方法是通过定义集群。\n集群（Cluster）是一组接受流量的上游类似主机。这可以是你的服务所监听的主机或 IP 地址的列表。\n例如，假设我们的 hello world 服务是在 127.0.0.0:8000 上监听。然后，我们可以用一个单一的端点创建一个集群，像这样。\nclusters:- name:hello_world_serviceload_assignment:cluster_name:hello_world_serviceendpoints:- lb_endpoints:- endpoint:address:socket_address:address:127.0.0.1port_value:8000集群的定义与监听器的定义在同一级别，使用 clusters 字段。我们在路由配置中引用集群时，以及在导出统计数据时，都会使用集群。该名称在所有集群中必须是唯一的。\n在 load_assignment 字段下，我们可以定义要进行负载均衡的端点列表，以及负载均衡策略设置。\nEnvoy 支持多种负载均衡算法（round-robin、Maglev、least-request、random），这些算法是由静态引导配置、DNS、动态 xDS（CDS 和 EDS 服务）以及主动 / 被动健康检查共同配置的。如果我们没有通过 lb_policy 字段明确地设置负载均衡算法，它默认为 round-robin。\nendpoints 字段定义了一组属于特定地域的端点。使用可选的 locality 字段，我们可以指定上游主机的运行位置，然后在负载均衡过程中使用（即，将请求代理到离调用者更近的端点）。\n添加新的端点指示负载均衡器在一个以上的接收者之间分配流量。通常情况下，负载均衡器对所有端点一视同仁，但集群定义允许在端点内建立一个层次结构。\n例如，端点可以有一个 权重（weight） 属性，这将指示负载均衡器与其他端点相比，向这些端点发送更多 / 更少的流量。\n另一种层次结构类型是基于地域性的（locality），通常用于定义故障转移架构。这种层次结构允许我们定义地理上比较接近的 “首选” 端点，以及在 “首选” 端点变得不健康的情况下应该使用的 “备份” 端点。\n由于我们只有一个端点，所以我们还没有设置 locality。在 lb_endpoints 字段下，可以定义 Envoy 可以路由流量的实际端点。\n我们可以在 Cluster 中配置以下可选功能：\n 主动健康检查（health_checks） 断路器 (circuit_breakers) 异常点检测（outlier_detection） 在处理上游的 HTTP 请求时有额外的协议选项 一组可选的网络过滤器，应用于所有出站连接等  和监 …","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"bfa76d57862601aed97f20a4c2061aa6","permalink":"https://jimmysong.io/docs/cloud-native/service-mesh/envoy-building-blocks/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/docs/cloud-native/service-mesh/envoy-building-blocks/","section":"cloud-native","summary":"在这一节中，我们将解释 Envoy 的基本构建模块。 Envoy 配置的根被称为引导","tags":null,"title":"Envoy 的构建模块","type":"book"},{"authors":null,"categories":null,"content":"总结  保持友善。 解释你的推理。 在给出明确的指示与只指出问题并让开发人员自己决定间做好平衡。 鼓励开发人员简化代码或添加代码注释，而不仅仅是向你解释复杂性。  礼貌 一般而言，对于那些正在被您审查代码的人，除了保持有礼貌且尊重以外，重要的是还要确保您（的评论）是非常清楚且有帮助的。你并不总是必须遵循这种做法，但在说出可能令人不安或有争议的事情时你绝对应该使用它。 例如：\n糟糕的示例：“为什么这里你使用了线程，显然并发并没有带来什么好处？”\n好的示例：“这里的并发模型增加了系统的复杂性，但没有任何实际的性能优势，因为没有性能优势，最好是将这些代码作为单线程处理而不是使用多线程。”\n解释为什么 关于上面的“好”示例，您会注意到的一件事是，它可以帮助开发人员理解您发表评论的原因。 并不总是需要您在审查评论中包含此信息，但有时候提供更多解释，对于表明您的意图，您在遵循的最佳实践，或为您建议如何提高代码健康状况是十分恰当的。\n给予指导 一般来说，修复 CL 是开发人员的责任，而不是审查者。 您无需为开发人员详细设计解决方案或编写代码。\n但这并不意味着审查者应该没有帮助。一般来说，您应该在指出问题和提供直接指导之间取得适当的平衡。指出问题并让开发人员做出决定通常有助于开发人员学习，并使代码审查变得更容易。它还可能产生更好的解决方案，因为开发人员比审查者更接近代码。\n但是，有时直接说明，建议甚至代码会更有帮助。代码审查的主要目标是尽可能获得最佳 CL。第二个目标是提高开发人员的技能，以便他们随着时间的推移需要的审查越来越少。\n接受解释 如果您要求开发人员解释一段您不理解的代码，那通常会导致他们更清楚地重写代码。偶尔，在代码中添加注释也是一种恰当的响应，只要它不仅仅是解释过于复杂的代码。\n**仅在代码审查工具中编写的解释对未来的代码阅读者没有帮助。**这仅在少数情况下是可接受的，例如当您查看一个您不熟悉的领域时，开发人员会用来向您解释普通读者已经知道的内容。\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"278558262e64bbd397064b7e418d57c9","permalink":"https://jimmysong.io/docs/eng-practices/review/reviewer/comments/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/docs/eng-practices/review/reviewer/comments/","section":"eng-practices","summary":"总结 保持友善。 解释你的推理。 在给出明确的指示与只指出问题并让","tags":null,"title":"如何撰写 Code Review 评论","type":"book"},{"authors":null,"categories":null,"content":"HCM 支持在加权集群、路由、虚拟主机和 / 或全局配置层面操纵请求和响应头。\n注意，我们不能直接从配置中修改所有的 Header，使用 Wasm 扩展的情况除外。然后，我们可以修改 :authority  header，例如下面的情况。\n不可变的头是伪头（前缀为:，如:scheme）和host头。此外，诸如 :path 和 :authority 这样的头信息可以通过 prefix_rewrite、regex_rewrite 和 host_rewrite 配置来间接修改。\nEnvoy 按照以下顺序对请求 / 响应应用这些头信息：\n 加权的集群级头信息 路由级 Header 虚拟主机级 Header 全局级 Header  这个顺序意味着 Envoy 可能会用更高层次（路由、虚拟主机或全局）配置的头来覆盖加权集群层次上设置的 Header。\n在每一级，我们可以设置以下字段来添加 / 删除请求 / 响应头。\n response_headers_to_add：要添加到响应中的 Header 信息数组。 response_headers_to_remove：要从响应中移除的 Header 信息数组。 request_headers_to_add：要添加到请求中的 Header 信息数组。 request_headers_to_remove：要从请求中删除的 Header 信息数组。  除了硬编码标头值之外，我们还可以使用变量来为标头添加动态值。变量名称以百分数符号（%）为分隔符。支持的变量名称包括 %DOWNSTREAM_REMOTE_ADDRESS%、%UPSTREAM_REMOTE_ADDRESS%、%START_TIME%、%RESPONSE_FLAGS% 和更多。你可以在这里找到完整的变量列表。\n让我们看一个例子，它显示了如何在不同级别的请求 / 响应中添加 / 删除头信息。\nroute_config:response_headers_to_add:- header:key:\u0026#34;header_1\u0026#34;value:\u0026#34;some_value\u0026#34;# 如果为真（默认），它会将该值附加到现有值上。# 否则它将替换现有的值append:falseresponse_headers_to_remove:\u0026#34;header_we_dont_need\u0026#34;virtual_hosts:- name:hello_vhostrequest_headers_to_add:- header:key:\u0026#34;v_host_header\u0026#34;value:\u0026#34;from_v_host\u0026#34;domains:[\u0026#34;hello.io\u0026#34;]routes:- match:prefix:\u0026#34;/\u0026#34;route:cluster:helloresponse_headers_to_add:- header:key:\u0026#34;route_header\u0026#34;value:\u0026#34;%DOWNSTREAM_REMOTE_ADDRESS%\u0026#34;- match:prefix:\u0026#34;/api\u0026#34;route:cluster:hello_apiresponse_headers_to_add:- header:key:\u0026#34;api_route_header\u0026#34;value:\u0026#34;api-value\u0026#34;- header:key:\u0026#34;header_1\u0026#34;value:\u0026#34;this_will_be_overwritten\u0026#34;标准 Header Envoy 在收到请求（解码）和向上游集群发送请求（编码）时，会操作一组头信息。\n当使用裸露的 Envoy 配置将流量路由到单个集群时，在编码过程中会设置以下头信息。\n\u0026#39;:authority\u0026#39;, \u0026#39;localhost:10000\u0026#39; \u0026#39;:path\u0026#39;, \u0026#39;/\u0026#39; \u0026#39;:method\u0026#39;, \u0026#39;GET\u0026#39; \u0026#39;:scheme\u0026#39;, \u0026#39;http\u0026#39; \u0026#39;user-agent\u0026#39;, \u0026#39;curl/7.64.0\u0026#39; \u0026#39;accept\u0026#39;, \u0026#39;*/*\u0026#39; \u0026#39;x-forwarded-proto\u0026#39;, \u0026#39;http\u0026#39; \u0026#39;x-request-id\u0026#39;, \u0026#39;14f0ac76-128d-4954-ad76-823c3544197e\u0026#39; \u0026#39;x-envoy-expected-rq-timeout-ms\u0026#39;, \u0026#39;15000\u0026#39; 在编码（响应）时，会发送一组不同的头信息。\n\u0026#39;:status\u0026#39;, \u0026#39;200\u0026#39; \u0026#39;x-powered-by\u0026#39;, \u0026#39;Express\u0026#39; \u0026#39;content-type\u0026#39;, \u0026#39;text/html; charset=utf-8\u0026#39; \u0026#39;content-length\u0026#39;, \u0026#39;563\u0026#39; \u0026#39;etag\u0026#39;, \u0026#39;W/\u0026#34;233-b+4UpNDbOtHFiEpLMsDEDK7iTeI\u0026#34;\u0026#39; \u0026#39;date\u0026#39;, \u0026#39;Fri, 16 Jul 2021 21:59:52 GMT\u0026#39; \u0026#39;x-envoy-upstream-service-time\u0026#39;, \u0026#39;2\u0026#39; \u0026#39;server\u0026#39;, \u0026#39;envoy\u0026#39; 下表解释了 Envoy 在解码或编码过程中设置的不同头信息。\n   Header 描述     :scheme 设置并提供给过滤器，并转发到上游。(对于 HTTP/1，:scheme 头是由绝对 URL 或 x-forwaded-proto 头值设置的) 。   user-agent 通常由客户端设置，但在启用 add_user_agent 时可以修改（仅当 Header 尚未设置时）。该值由 --service-cluster命令行选项决定。   x-forwarded-proto 标准头，用于识别客户端用于连接到代理的协议。该值为 http 或 https。   x-request-id Envoy 用来唯一地识别一个请求，也用于访问记录和跟踪。   x-envoy-expected-rq-timeout-ms 指定路由器期望请求完成的时间，单位是毫秒。这是从 x-envoy-upstream-rq-timeout-ms 头值中读取的（假设设置了 respect_expected_rq_timeout）或从路由超时设置中读取（默认为 15 秒）。   x-envoy-upstream-service-time 端点处理请求所花费的时间，以毫秒为单位，以及 Envoy 和上游主机之间的网络延迟。   server 设置为 server_name 字段中指定的值（默认为 envoy）。    根据不同的场景，Envoy 会设置或消费一系列其他头信息。当我们在课程的其余部分讨论这些场景和功能时，我们会引出不同的头信息。\nHeader 清理 Header 清理是一个出于安全原因添加、删除或修改请求 Header 的过程。有一些头信息，Envoy 有可能会进行清理。\n   Header 描述     x-envoy-decorator-operation 覆盖由追踪机制产生的任何本地定义的跨度名称。   x-envoy-downstream-service-cluster 包含调用者的服务集群（对于外部请求则删除）。由 -service-cluster 命令行选项决定，要求 user_agent 设置为 true。   x-envoy-downstream-service-node 和前面的头一样，数值由 --service--node选项决定。   x-envoy-expected-rq-timeout-ms 指定路由器期望请求完成的时间，单位是毫秒。这是从 x-envoy-upstream-rq-timeout-ms 头值中读取的（假设设置了 respect_expected_rq_timeout）或从路由超时设置中读取（默认为 15 秒）。   x-envoy-external-address 受信任的客户端地址（关于如何确定，详见下面的 XFF）。   x-envoy-force-trace 强制收集的追踪。   x-envoy-internal 如果请求是内部的，则设置为 “true”（关于如何确定的细节，见下面的 XFF）。   x-envoy-ip-tags 如果外部地址在 IP 标签中被定义，由 HTTP IP 标签过滤器设置。   x-envoy-max-retries 如果配置了重试策略，重试的最大次数。   x-envoy-retry-grpc-on 对特定 gRPC 状态代码的失败请求进行重试。   x-envoy-retry-on 指定重试策略。   x-envoy-upstream-alt-stat-name Emist 上游响应代码 / 时间统计到一个双统计树。   x-envoy-upstream-rq-per-try-timeout-ms 设置路由请求的每次尝试超时。   x-envoy-upstream-rq-timeout-alt-response 如果存在，在请求超时的情况下设置一个 204 响应代码（而不是 504）。   x-envoy-upstream-rq-timeout-ms 覆盖路由配置超时。   x-forwarded-client-certif 表示一个请求流经的所有客户端 / 代理中的部分证书信息。   x-forwarded-for 表示 IP 地址请求通过了。更多细节见下面的 XFF。   x-forwarded-proto 设置来源协议（http 或 https）。   x-request-id Envoy 用来唯一地识别一个请求。也用于访问日志和追踪。    是否对某个特定的头进行清理，取决于请求来自哪里。Envoy 通过查看 x-forwarded-for 头（XFF）和 internal_address_config 设置来确定请求是外部还是内部。\nXFF XFF 或 x-forwaded-for 头表示请求在从客户端到服务器的途中所经过的 IP 地址。下游和上游服务之间的代理在代理请求之前将最近的客户的 IP 地址附加到 XFF 列表中。\nEnvoy 不会自动将 IP 地址附加到 XFF 中。只有当 use_remote_address（默认为 false）被设置为 true，并且 skip_xff_append 被设置为 false 时，Envoy 才会追加该地址。\n当 use_remote_address 被设置为 true 时，HCM 在确定来源是内部还是外部以及修改头信息时，会使用客户端连接的真实远程地址。这个值控制 Envoy 如何确定可信的客户端地址。\n可信的客户端地址\n可信的客户端地址是已知的第一个准确的源 IP 地址。向 Envoy 代理发出请求的下游节点的源 IP 地址被认为是正确的。\n请注意，完整的 XFF 有时不能被信任，因为恶意的代理可以伪造它。然而，如果一个受信任的代理将最后一个地址放在 XFF 中，那么它就可以被信任。例如，如果我们看一下请求路径 IP1 -\u0026gt; IP2 -\u0026gt; IP3 -\u0026gt; Envoy，IP3 是 Envoy 会认为信任的节点。\nEnvoy 支持通过 original_ip_detection_extensions 字段设置的扩展，以帮助确定原始 IP 地址。目前，有两个扩展：custom_header 和 xff。\n通过自定义头的扩展，我们可以提供一个包含原始下游远程地址的头名称。此外，我们还可以告诉 HCM 将检测到的地址视为可信地址。\n通过 xff 扩展，我们可以指定从 x-forwarded-for 头的右侧开始的额外代理跳数来信任。如果我们将这个值设置为 1 还使用上面的例子，受信任的地址将是 IP2 和 IP3。\nEnvoy 使用可信的客户端地址来确定请求是内部还是外部。如果我们把 use_remote_address 设置为 true，那么如果请求不包含 XFF，并且直接下游节点与 Envoy 的连接有一个内部源地址，那么就认为是内部请求。Envoy 使用 RFC1918 或 RFC4193 来确定内部源地址。\n如果我们把 use_remote_address 设置为 false（默认值），只有当 XFF 包含上述两个 RFC 定义的单一 …","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"0fe93e77f4e191d98c24ec9af4bb72de","permalink":"https://jimmysong.io/docs/envoy-handbook/hcm/header-manipulation/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/envoy-handbook/hcm/header-manipulation/","section":"envoy-handbook","summary":"HCM 支持在加权集群、路由、虚拟主机和 / 或全局配置层面操纵请求和","tags":null,"title":"Header 操作","type":"book"},{"authors":null,"categories":null,"content":"Kubernetes 可以成为数据和 / 或计算能力盗窃的重要目标。虽然数据盗窃是传统上的主要动机，但寻求计算能力（通常用于加密货币挖掘）的网络行为者也被吸引到 Kubernetes 来利用其底层基础设施。除了资源盗窃，网络行为者还可能针对 Kubernetes 造成拒绝服务。下面的威胁代表了 Kubernetes 集群最可能的破坏源。\n 供应链风险 - 对供应链的攻击载体是多种多样的，并且在减轻风险方面具有挑战性。供应链风险是指对手可能颠覆构成系统的任何元素的风险，包括帮助提供最终产品的产品组件、服务或人员。这可能包括用于创建和管理 Kubernetes 集群的第三方软件和供应商。供应链的潜在威胁会在多个层面上影响 Kubernetes，包括：  容器 / 应用层面 - 在 Kubernetes 中运行的应用及其第三方依赖的安全性，它们依赖于开发者的可信度和开发基础设施的防御能力。来自第三方的恶意容器或应用程序可以为网络行为者在集群中提供一个立足点。 基础设施 - 托管 Kubernetes 的底层系统有其自身的软件和硬件依赖性。系统作为工作节点或控制平面一部分的，任何潜在威胁都可能为网络行为者在集群中提供一个立足点。   恶意威胁行为者 - 恶意行为者经常利用漏洞从远程位置获得访问权。Kubernetes 架构暴露了几个 API，网络行为者有可能利用这些 API 进行远程利用。  控制平面 - Kubernetes 控制平面有各种组件，通过通信来跟踪和管理集群。网络行为者经常利用缺乏适当访问控制的暴露的控制平面组件。 工作节点 - 除了运行容器引擎外，工作者节点还承载着 kubelet 和 kube-proxy 服务，这些都有可能被网络行为者利用。此外，工作节点存在于被锁定的控制平面之外，可能更容易被网络行为者利用。 容器化的应用程序 - 在集群内运行的应用程序是常见的目标。应用程序经常可以在集群之外访问，使它们可以被远程网络行为者接触到。然后，网络行为者可以从已经被破坏的应用出发，或者利用暴露的应用程序的内部可访问资源在集群中提升权限。   内部威胁 - 威胁者可以利用漏洞或使用个人在组织内工作时获得的特权。来自组织内部的个人被赋予特殊的知识和特权，可以用来威胁 Kubernetes 集群。  管理员 - Kubernetes 管理员对运行中的容器有控制权，包括在容器化环境中执行任意命令的能力。Kubernetes 强制的 RBAC 授权可以通过限制对敏感能力的访问来帮助降低风险。然而，由于 Kubernetes 缺乏双人制的完整性控制，即必须有至少一个管理账户才能够获得集群的控制权。管理员通常有对系统或管理程序的物理访问权，这也可能被用来破坏 Kubernetes 环境。 用户 - 容器化应用程序的用户可能有知识和凭证来访问 Kubernetes 集群中的容器化服务。这种程度的访问可以提供足够的手段来利用应用程序本身或其他集群组件。 云服务或基础设施供应商 - 对管理 Kubernetes 节点的物理系统或管理程序的访问可被用来破坏 Kubernetes 环境。云服务提供商通常有多层技术和管理控制，以保护系统免受特权管理员的影响。    ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"6b93b0af0915e17422e6179cefbbc504","permalink":"https://jimmysong.io/docs/kubernetes-hardening-guidance/threat-model/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/kubernetes-hardening-guidance/threat-model/","section":"kubernetes-hardening-guidance","summary":"Kubernetes 可以成为数据和 / 或计算能力盗窃的重要目标。虽然数据盗窃是传","tags":null,"title":"威胁建模","type":"book"},{"authors":null,"categories":null,"content":"第 2 章描述了实现自动分析所需的数据模型，第 4 章描述了支持原生 开源仪表的额外要求，并赋予各角色（应用程序所有者、运维和响应者）自主权。这就是我们对现代可观测性的概念模型。\n在本报告的其余部分，我们描述了这个新模型的一个事实实现，即 OpenTelemetry。这一章描述了构成 OpenTelemetry 遥测管道的所有组件。后面的章节将描述稳定性保证、建议的设置以及 OpenTelemetry 现实中的部署策略。有关该项目的更多细节可以在附录中找到。\n信号 OpenTelemetry 规范被组织成不同类型的遥测，我们称之为信号（signal）。主要的信号是追踪。日志和度量是其他例子。信号是 OpenTelemetry 中最基本的设计单位。\n每一个额外的信号首先是独立开发的，然后与追踪和其他相关信号整合。这种分离允许开发新的、实验性的信号，而不影响已经变得稳定的信号的兼容性保证。\nOpenTelemetry 是一个跨领域的关注点（cross-cutting concern），它在事务通过每个库和服务时追踪其执行。为了达到这个目的，所有的信号都建立在低级别的上下文传播系统之上，该系统为信号提供了一个地方来存储它们需要与当前正在执行的代码相关联的任何事务级数据。因为上下文传播系统与追踪系统是完全分开的，其他跨领域的问题也可以利用它。图 5-1 说明了这个分层结构。\n   图 5-1：所有 OpenTelemetry 信号都建立在一个共享的上下文传播系统之上。其他的，非可观测性的交叉关注也可以使用上下文传播机制来通过分布式系统传输他们的数据。  上下文（Context） 上下文对象是一个与执行上下文相关联的键值存储，例如线程或循环程序。如何实现这一点取决于语言，但 OpenTelemetry 在每种语言中都提供一个上下文对象。\n信号在上下文对象中存储它们的数据。因为 OpenTelemetry 的 API 调用总是可以访问整个上下文对象，所以信号有可能成为集成的，并在上下文共享数据，而不需要改变 API。例如，如果追踪和度量信号都被启用，记录一个度量可以自动创建一个追踪范例。日志也是如此：如果有的话，日志会自动绑定到当前的追踪。\n传播器（Propagator） 为了使分布式追踪发挥作用，追踪上下文必须被参与事务的每个服务所共享。传播器通过序列化和反序列化上下文对象来实现这一点，允许信号在网络工作请求中追踪其事务。\n追踪（Tracing） OpenTelemetry 追踪系统是基于 OpenTracing 和 OpenCensus。这两个系统，以及流行的 Zipkin 和 Jaeger 项目，都是基于谷歌开发的 Dapper 追踪系统。OpenTelemetry 试图与所有这些基于 Dapper 的系统兼容。\nOpenTelemetry 追踪包括一个叫做 链接（link） 的概念，它允许单独的追踪被组合成一个更大的图。这被用来连接事务和后台处理，以及观察大型异步系统，如 Kafka 和 AMQP。\n指标（Metric） 度量指标（metric）是一个很大的话题，包含各种各样的方法和实现。OpenTelemetry 度量信号被设计成与 Prometheus 和 StatsD 完全兼容。\n指标包括追踪样本，自动将指标与产生它们的追踪样本联系起来。手工将指标和追踪联系起来往往是一项繁琐且容易出错的任务，自动执行这项任务将为运维人员节省大量的时间。\n日志（Log） OpenTelemetry 结合了高度结构化的日志 API 和高速日志处理系统。现有的日志 API 可以连接到 OpenTelemetry，避免了对应用程序的重新测量。\n每当它出现的时候，日志就会自动附加到当前的追踪中。这使得事务日志很容易找到，并允许自动分析，以找到同一追踪中的日志之间的准确关联。\nBaggage OpenTelemetry Baggage 是一个简单但通用的键值系统。一旦数据被添加为 Baggage（包袱）它就可以被所有下游服务访问。这允许有用的信息，如账户和项目 ID，在事务的后期变得可用，而不需要从数据库中重新获取它们。例如，一个使用项目 ID 作为索引的前端服务可以将其作为 Baggage 添加，允许后端服务也通过项目 ID 对其跨度和指标进行索引。\n你可以将 Baggage 看做是一种分布式文本的形式。直接放入上下文对象的项目只能在当前服务中访问。与追踪上下文一样，作为 Baggage 添加的项目被作为 header 注入网络请求，允许下游服务提取它们。\n与上下文对象一样，Baggage 本身不是一个可观测性工具。它更像是一个通用的数据存储和传输系统。除了可观测性之外，其他跨领域的工具，例如，功能标记、A/B 测试和认证，可以使用 Baggage 来存储他们需要追踪当前事务的任何状态。\n然而，Baggage 是有代价的。因为每增加一个项目都必须被编码为一个头，每增加一个项目都会增加事务中每一个后续网络请求的大小。这就是为什么我们称它为 Baggage。我建议，Baggage 要少用，作为交叉关注的一部分。Baggage 不应该被用作明确定义的服务 API 的 “方便” 替代品，以明确地向下游应用程序发送参数。\nOpenTelemetry 客户端架构 应用程序通过安装一系列的软件库来检测 OpenTelemetry：API、SDK（软件开发工具包）、SDK 插件和库检测。这套库被称为 OpenTelemetry 客户端。图 5-2 显示了这些组件之间的关系。\n   图 5-2：OpenTelemetry 客户端架构。为了帮助管理依赖性，OpenTelemetry 将实现与仪表使用的 API 分开。  在许多语言中，OpenTelemetry 提供安装程序，这有助于自动安装和设置 OpenTelemetry 客户端。然而，可用的自动化程度取决于语言。在 Java 中，OpenTelemetry 提供了一个 Java 代理，它通过动态地注入所有必要的组件来实现安装的完全自动化。在 Go 中，OpenTelemetry 包必须通过编写代码来安装和初始化，就像任何其他 Go 包一样。Python、Ruby 和 NodeJS 介于两者之间，提供不同程度的自动化。\n在学习 OpenTelemetry 时，了解在你使用的语言中如何设置是很重要的。特别是，一定要学习如何安装仪表，因为不同的语言有很大的不同。\n请查看客户端文档，了解更多的入门细节。\n客户端架构：仪表 API OpenTelemetry API 是指用于编写仪表的一组组件。该 API 被设计成可以直接嵌入到开放源码软件库以及应用程序中。这是 OpenTelemetry 的唯一部分，共享库和应用逻辑应该直接依赖它。\n提供者（Provider） API 与任何实现完全分开。当一个应用程序启动时，可以通过为每个信号注册一个提供者来加载一个实现。提供者成为所有 API 调用的接收者。\n当没有加载提供者时，API 默认为无操作提供者。这使得 OpenTelemetry 仪表化可以安全地包含在共享库中。如果应用程序不使用 OpenTelemetry，API 调用就会变成 no-ops，不会产生任何开销。\n对于生产使用，我们建议使用官方的 OpenTelemetry 提供商，我们称之为 OpenTelemetry SDK。\n 为什么有多个实现方案？\nAPI 和实现的分离有很多好处。但是，如果用户被迫总是安装官方的 OpenTelemetry SDK，这又有什么意义？是否有必要安装另一个实现？SDK 已经具有很强的扩展性。\n我们相信是有的。虽然我们希望 OpenTelemetry 仪表是通用的，但建立一个对所有用例都理想的单一实现是不可能的。尽管我们相信 OpenTelemetry SDK 很好，但也应该有一个选择，那就是使用另一种实现。实现的灵活性是提供通用仪表 API 的一个关键特征。\n首先，这种分离保证了 OpenTelemetry 不会产生无法克服的依赖冲突。我们总是可以选择加载一个包括不同依赖链的实现。\n另一个原因是性能。OpenTelemetry SDK 是一个可扩展的、通用的框架。虽然 SDK 的设计是为了尽可能地提高性能，但扩展性和性能总是要权衡一下的。例如，通过外来函数接口创建与 OpenTelemetry C++ SDK 的绑定，有可能成为 Ruby、Python 和 Node.js 等动态脚本语言的一个非常有效的选择。\n 还有一些流媒体架构显示了有希望的性能提升。在许多这样的优化解决方案中，编写插件和生命周期钩子的能力将受到严重限制；支持这些类型的功能所需的数据结构在这些优化解决方案中是不存在的。归根结底，没有 “完美的实现”；只有权衡。\nAPI/SDK 的分离是一个关键的设计选择，该项目大量使用了这一点。例如，除了 SDK 之外，每一种语言都有一个 no-op 的实现，它是默认安装的。还有一个 Fake/Mock 实现，我们用它来测试。而且，还有可能实现更多创造性的实现。例如，为分布式系统建立开发者工具，如一个实时调试器，它可以跨越网络边界工作。\n客户端架构：SDK OpenTelemetry 项目为 OpenTelemetry API 提供了一个官方实现，我们称之为 OpenTelemetry SDK。该 SDK 通过提供一个插件框架来实现 OpenTelemetry API。下面将介绍追踪 SDK；类似的架构也适用于度量和日志。\n基本数据结构是一个无锁的 SpanData 对象。当用户开始一个跨度时，SpanData 对象被创建，当用户添加属性和事件时，它被自动建立起来。一旦一个跨度结束，SpanData 对象将不再被更新，可以安全地传递给后台线程。\nSDK 的插件架构被组织成一个流水线。对于追踪来说，该管道由一连串的 SpanProcessors 组成。每个处理器对 SpanData 对象进行两次同步访问：一次是在跨度开始时，另一次是在跨度结束后。采样器、日志附加器和数据清洗器是 SpanProcessors 的例子。链中的最后一个处理器通常是一个 BatchSpanProcessor，它管理着一个已完成的跨度的缓冲区。输出器可以连接到 BatchSpanProcessor，通过网络将成批的跨度传递到遥测管道中的下一个服务，通常将它们发送到收集器或直接发送到追踪后端。一旦跨度被导出，管道就完成了，SpanData 对象也被释放。\n采样器（Sampler） OpenTelemetry 提供了几种常见的采样算法，包括前期采样和基于优先级的采样。采样可以帮助控制成本，但它是有代价的：你将会错过数据。在启用任何种类的采样算法之前，重要的是要检查你计划使用的分析工具支持哪些类型的采用。意外的采样可能会破坏某些形式的分析。一些工具需要他们自己的采样插件。例如，AWS X-Ray 使用它自己的采样算法，它可以作为 AWS 特定的采样插件使用。\n导出器（Exporter） OpenTelemetry 为 OTLP（OpenTelemetry Protocol）、Jaeger、Zipkin、Prometheus 和 StatsD 提供导出器。由第三方维护的其他导出器可以在每种语言的 OpenTelemetry-Contrib 资源库中找到。使用 OpenTelemetry 注册表来了解目前有哪些插件可用。\n客户端架构：库仪表化 为了正常工作，OpenTelemetry 需要端到端的工具。这不是可有可无的：如果关键的库不包括仪表，上下文传播将被破坏。\n一般来说，必须检测的库包括 HTTP 客户端、HTTP 服务器、应用框架、消息传递 / 队列系统和数据库客户端。这些库经常在上下文传播中起作用。\n HTTP 客户端必须创建一个客户端 span 来记录请求。客户端还必须使用一个传播器，将当前的上下文作为一组 HTTP 头信息注入到请求中。 HTTP 服务器（应用框架） …","date":1643990400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"0329e26e4b51c83ff6de3f3f8d4fc9ae","permalink":"https://jimmysong.io/docs/opentelemetry-obervability/architectural-overview/","publishdate":"2022-02-05T00:00:00+08:00","relpermalink":"/docs/opentelemetry-obervability/architectural-overview/","section":"opentelemetry-obervability","summary":"第 5 章：OpenTelemetry 架构概述","tags":null,"title":"第 5 章：OpenTelemetry 架构概述","type":"book"},{"authors":null,"categories":null,"content":"HCM 是一个网络级的过滤器，将原始字节转译成 HTTP 级别的消息和事件（例如，收到的 Header，收到的 Body 数据等）。\nHCM 过滤器还处理标准的 HTTP 功能。它支持访问记录、请求 ID 生成和跟踪、Header 操作、路由表管理和统计等功能。\n从协议的角度来看，HCM 原生支持 HTTP/1.1、WebSockets、HTTP/2 和 HTTP/3（仍在 Alpha 阶段）。\nEnvoy 代理被设计成一个 HTTP/2 复用代理，这体现在描述 Envoy 组件的术语中。\nHTTP/2 术语\n在 HTTP/2 中，流是已建立的连接中的字节的双向流动。每个流可以携带一个或多个消息（message）。消息是一个完整的帧（frame）序列，映射到一个 HTTP 请求或响应消息。最后，帧是 HTTP/2 中最小的通信单位。每个帧都包含一个帧头（frame header），它至少可以识别该帧所属的流。帧可以携带有关 HTTP Header、消息有效载荷等信息。\n无论流来自哪个连接（HTTP/1.1、HTTP/2 或 HTTP/3），Envoy 都使用一个叫做 编解码 API（codec API） 的功能，将不同的线程协议翻译成流、请求、响应等协议无关模型。协议无关的模型意味着大多数 Envoy 代码不需要理解每个协议的具体内容。\nHTTP 过滤器 在 HCM 中，Envoy 支持一系列的 HTTP 过滤器。与监听器级别的过滤器不同，这些过滤器对 HTTP 级别的消息进行操作，而不知道底层协议（HTTP/1.1、HTTP/2 等）或复用能力。\n有三种类型的 HTTP 过滤器。\n 解码器（Decoder）：当 HCM 对请求流的部分进行解码时调用。 编码器（Encoder）：当 HCM 对响应流的部分进行编码时调用。 解码器 / 编码器（Decoder/Encoder）：在两个路径上调用，解码和编码  下图解释了 Envoy 如何在请求和响应路径上调用不同的过滤器类型。\n   请求响应路径及 HTTP 过滤器  像网络过滤器一样，单个的 HTTP 过滤器可以停止或继续执行后续的过滤器，并在单个请求流的范围内相互分享状态。\n数据共享 在高层次上，我们可以把过滤器之间的数据共享分成静态和动态。\n静态包含 Envoy 加载配置时的任何不可变的数据集，它被分成三个部分。\n1. 元数据\nEnvoy 的配置，如监听器、路由或集群，都包含一个metadata数据字段，存储键 / 值对。元数据允许我们存储特定过滤器的配置。这些值不能改变，并在所有请求 / 连接中共享。例如，元数据值在集群中使用子集选择器时被使用。\n2. 类型化的元数据\n类型化元数据不需要为每个流或请求将元数据转换为类型化的类对象，而是允许过滤器为特定的键注册一个一次性的转换逻辑。来自 xDS 的元数据在配置加载时被转换为类对象，过滤器可以在运行时请求类型化的版本，而不需要每次都转换。\n3. HTTP 每路过滤器配置\n与适用于所有虚拟主机的全局配置相比，我们还可以指定每个虚拟主机或路由的配置。每个路由的配置被嵌入到路由表中，可以在 typed_per_filter_config 字段下指定。\n另一种分享数据的方式是使用动态状态。动态状态会在每个连接或 HTTP 流中产生，并且它可以被产生它的过滤器改变。名为 StreamInfo 的对象提供了一种从 map 上存储和检索类型对象的方法。\n过滤器顺序 指定 HTTP 过滤器的顺序很重要。考虑一下下面的 HTTP 过滤器链。\nhttp_filters:- filter_1- filter_2- filter_3一般来说，链中的最后一个过滤器通常是路由器过滤器。假设所有的过滤器都是解码器 / 编码器过滤器，HCM 在请求路径上调用它们的顺序是filter_1、filter_2、filter_3。\n在响应路径上，Envoy 只调用编码器过滤器，但顺序相反。由于这三个过滤器都是解码器 / 编码器过滤器，所以在响应路径上的顺序是 filter_3、filter_2、filter_1。\n内置 HTTP 过滤器 Envoy 已经内置了几个 HTTP 过滤器，如 CORS、CSRF、健康检查、JWT 认证等。你可以在这里找到 HTTP 过滤器的完整列表。\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"d2d810acd877d87bd0fe44a9214acf4d","permalink":"https://jimmysong.io/docs/cloud-native/service-mesh/http-conneciton-manager/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/docs/cloud-native/service-mesh/http-conneciton-manager/","section":"cloud-native","summary":"HCM 是一个网络级的过滤器，将原始字节转译成 HTTP 级别的消息和事件（","tags":null,"title":"HTTP 连接管理器介绍","type":"book"},{"authors":null,"categories":null,"content":"有时开发人员会拖延（Pushback）代码审查。他们要么不同意您的建议，要么抱怨您太严格。\n谁是对的？ 当开发人员不同意您的建议时，请先花点时间考虑一下是否正确。通常，他们比你更接近代码，所以他们可能真的对它的某些方面有更好的洞察力。他们的论点有意义吗？从代码健康的角度来看它是否有意义？如果是这样，让他们知道他们是对的，把问题解决。\n但是，开发人员并不总是对的。在这种情况下，审查人应进一步解释为什么认为他们的建议是正确的。好的解释在描述对开发人员回复的理解的同时，还会解释为什么请求更改。\n特别是，当审查人员认为他们的建议会改善代码健康状况时，他们应该继续提倡更改，如果他们认为最终的代码质量改进能够证明所需的额外工作是合理的。提高代码健康状况往往只需很小的几步。\n有时需要几轮解释一个建议才能才能让对方真正理解你的用意。只要确保始终保持礼貌，让开发人员知道你有听到他们在说什么，只是你不同意该论点而已。\n沮丧的开发者 审查者有时认为，如果审查者人坚持改进，开发人员会感到不安。有时候开发人员会感到很沮丧，但这样的感觉通常只会持续很短的时间，后来他们会非常感谢您在提高代码质量方面给他们的帮助。通常情况下，如果您在评论中表现得很有礼貌，开发人员实际上根本不会感到沮丧，这些担忧都仅存在于审核者心中而已。开发者感到沮丧通常更多地与评论的写作方式有关，而不是审查者对代码质量的坚持。\n稍后清理 开发人员拖延的一个常见原因是开发人员（可以理解）希望完成任务。他们不想通过另一轮审查来完成该 CL。所以他们说会在以后的 CL 中清理一些东西，所以您现在应该 LGTM 这个 CL。一些开发人员非常擅长这一点，并会立即编写一个修复问题的后续 CL。但是，经验表明，在开发人员编写原始 CL 后，经过越长的时间这种清理发生的可能性就越小。实际上，通常除非开发人员在当前 CL 之后立即进行清理，否则它就永远不会发生。这不是因为开发人员不负责任，而是因为他们有很多工作要做，清理工作在其他工作中被丢失或遗忘。因此，在代码进入代码库并“完成”之前，通常最好坚持让开发人员现在清理他们的 CL。让人们“稍后清理东西”是代码库质量退化的常见原因。\n如果 CL 引入了新的复杂性，除非是紧急情况，否则必须在提交之前将其清除。如果 CL 暴露了相关的问题并且现在无法解决，那么开发人员应该将 bug 记录下来并分配给自己，避免后续被遗忘。又或者他们可以选择在程序中留下 TODO 的注释并连结到刚记录下的 bug。\n关于严格性的抱怨 如果您以前有相当宽松的代码审查，并转而进行严格的审查，一些开发人员会抱怨得非常大声。通常提高代码审查的速度会让这些抱怨逐渐消失。\n有时，这些投诉可能需要数月才会消失，但最终开发人员往往会看到严格的代码审查的价值，因为他们会看到代码审查帮助生成的优秀代码。而且一旦发生某些事情时，最响亮的抗议者甚至可能会成为你最坚定的支持者，因为他们会看到审核变严格后所带来的价值。\n解决冲突 如果上述所有操作仍无法解决您与开发人员之间的冲突，请参阅 “Code Review 标准”以获取有助于解决冲突的指导和原则。\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"9d9412b0a136bd6c025787338e4b4687","permalink":"https://jimmysong.io/docs/eng-practices/review/reviewer/pushback/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/docs/eng-practices/review/reviewer/pushback/","section":"eng-practices","summary":"有时开发人员会拖延（Pushback）代码审查。他们要么不同","tags":null,"title":"处理 Code Review 中的拖延","type":"book"},{"authors":null,"categories":null,"content":"Pod 是 Kubernetes 中最小的可部署单元，由一个或多个容器组成。Pod 通常是网络行为者在利用容器时的初始执行环境。出于这个原因，Pod 应该被加固，以使利用更加困难，并限制成功入侵的影响。\n   图3：有 sidecar 代理作为日志容器的 Pod 组件  “非 root” 容器和 “无 root” 容器引擎 默认情况下，许多容器服务以有特权的 root 用户身份运行，应用程序在容器内以 root 用户身份执行，尽管不需要有特权的执行。\n通过使用非 root 容器或无 root 容器引擎来防止 root 执行，可以限制容器受损的影响。这两种方法都会对运行时环境产生重大影响，因此应该对应用程序进行全面测试，以确保兼容性。\n非 root 容器：容器引擎允许容器以非 root 用户和非 root 组成员身份运行应用程序。通常情况下，这种非默认设置是在构建容器镜像的时候配置的。附录 A：非 root 应用的 Dockerfile 示例 显示了一个 Dockerfile 示例，它以非 root 用户身份运行一个应用。\n非 root 用户。另外，Kubernetes 可以在 SecurityContext:runAsUser 指定一个非零用户的情况下，将容器加载到 Pod。虽然 runAsUser 指令在部署时有效地强制非 root 执行，但 NSA 和 CISA 鼓励开发者构建的容器应用程序，以非 root 用户身份执行。在构建时集成非 root 用户执行，可以更好地保证应用程序在没有 root 权限的情况下正常运行。\n无 root 的容器引擎：一些容器引擎可以在无特权的上下文中运行，而不是使用以 root 身份运行的守护程序。在这种情况下，从容器化应用程序的角度来看，执行似乎是使用 root 用户，但执行被重新映射到主机上的引擎用户上下文。虽然无 root 容器引擎增加了一个有效的安全层，但许多引擎目前是作为实验性发布的，不应该在生产环境中使用。管理员应该了解这一新兴技术，并在供应商发布与 Kubernetes 兼容的稳定版本时寻求采用无 root 容器引擎。\n不可变的容器文件系统 默认情况下，容器在自己的上下文中被允许不受限制地执行。在容器中获得执行权限的网络行为者可以在容器中创建文件、下载脚本和修改应用程序。Kubernetes 可以锁定一个容器的文件系统，从而防止许多暴露后的活动。\n然而，这些限制也会影响合法的容器应用程序，并可能导致崩溃或异常行为。为了防止损害合法的应用程序，Kubernetes 管理员可以为应用程序需要写访问的特定目录挂载二级读 / 写文件系统。附录 B：只读文件系统的部署模板示例 显示了一个具有可写目录的不可变容器的例子。\n构建安全的容器镜像 容器镜像通常是通过从头开始构建容器或在从存储库中提取的现有镜像基础上创建的。除了使用可信的存储库来构建容器外，镜像扫描是确保部署的容器安全的关键。在整个容器构建工作流程中，应该对镜像进行扫描，以识别过时的库、已知的漏洞或错误配置，如不安全的端口或权限。\n   图4：容器的构建工作流程，用 webhook 和准入控制器进行优化  实现镜像扫描的一种方法是使用准入控制器。准入控制器是 Kubernetes 的原生功能，可以在对象的持久化之前，但在请求被验证和授权之后，拦截和处理对 Kubernetes API 的请求。可以实现一个自定义或专有的 webhook，以便在集群中部署任何镜像之前执行扫描。如果镜像符合 webhook 配置中定义的组织的安全策略，这个准入控制器可以阻止部署。\nPod 安全策略  Pod 的创建应遵守最小授权原则。\n Pod 安全策略（PSP）1是一个集群范围内的策略，它规定了 Pod 在集群内执行的安全要求 / 默认值。虽然安全机制通常是在 Pod/Deployment 配置中指定的，但 PSP 建立了一个所有 Pod 必须遵守的最低安全门槛。一些 PSP 字段提供默认值，当 Pod 的配置省略某个字段时使用。其他 PSP 字段被用来拒绝创建不符合要求的 Pod。PSP 是通过 Kubernetes 准入控制器执行的，所以 PSP 只能在 Pod 创建期间执行要求。PSP 并不影响已经在集群中运行的 Pod。\nPSP 很有用，可以在集群中强制执行安全措施。PSP 对于由具有分层角色的管理员管理的集群特别有效。在这些情况下，顶级管理员可以施加默认值，对低层级的管理员强制执行要求。NSA 和 CISA 鼓励企业根据自己的需要调整 附录 C：Pod 安全策略示例 中的 Kubernetes 加固 PSP 模板。下表描述了一些广泛适用的 PSP 组件。\n表 1: Pod 安全策略组件\n   字段名称 使用方法 建议     privileged 控制 Pod 是否可以运行有特权的容器。 设置为 false。   hostPID、hostIPC 控制容器是否可以共享主机进程命名空间。 设置为 false。   hostNetwork 控制容器是否可以使用主机网络。 设置为 false。   allowedHostPaths 将容器限制在主机文件系统的特定路径上。 使用一个 “假的” 路径名称（比如 /foo 标记为只读）。省略这个字段的结果是不对容器进行准入限制。   readOnlyRootFilesystem 需要使用一个只读的根文件系统。 可能时设置为 true。   runAsUser, runAsGroup, supplementalGroups, fsGroup 控制容器应用程序是否能以 root 权限或 root 组成员身份运行。 - 设置 runAsUser 为 MustRunAsNonRoot。- 将 runAsGroup 设置为非零（参见附录 C 中的例子：Pod 安全策略示例）。     将 supplementalGroups 设置为非零（见附录 C 的例子）。将 fsGroup 设置为非零（参见附录 C 中的例子：Pod 安全策略示例）。   allowPrivilegeEscalation 限制升级到 root 权限。 设置为 false。为了有效地执行 runAsUser: MustRunAsNonRoot 设置，需要采取这一措施。   seLinux 设置容器的 SELinux 上下文。 如果环境支持 SELinux，可以考虑添加 SELinux 标签以进一步加固容器。   AppArmor 注解 设置容器所使用的 AppArmor 配置文件。 在可能的情况下，通过采用 AppArmor 来限制开发，以加固容器化的应用程序。   seccomp 注解 设置用于沙盒容器的 seccomp 配置文件。 在可能的情况下，使用 seccomp 审计配置文件来识别运行中的应用程序所需的系统调用；然后启用 seccomp 配置文件来阻止所有其他系统调用。    注意：由于以下原因，PSP 不会自动适用于整个集群：\n 首先，在应用 PSP 之前，必须为 Kubernetes 准入控制器启用 PodSecurityPolicy 插件，这是 kube-apiserver 的一部分。 第二，策略必须通过 RBAC 授权。管理员应从其集群组织内的每个角色中验证已实施的 PSP 的正确功能。  在有多个 PSP 的环境中，管理员应该谨慎行事，因为 Pod 的创建会遵守最小限制性授权策略。以下命令描述了给定命名空间的所有 Pod 安全策略，这可以帮助识别有问题的重叠策略。\nkubectl get psp -n \u0026lt;namespace\u0026gt; 保护 Pod 服务账户令牌 默认情况下，Kubernetes 在创建 Pod 时自动提供一个服务账户（Service Account），并在运行时在 Pod 中挂载该账户的秘密令牌（token）。许多容器化的应用程序不需要直接访问服务账户，因为 Kubernetes 的协调工作是在后台透明进行的。如果一个应用程序被破坏了。Pod 中的账户令牌可以被网络行为者收集并用于进一步破坏集群。当应用程序不需要直接访问服务账户时，Kubernetes 管理员应确保 Pod 规范禁用正在加载的秘密令牌。这可以通过 Pod 的 YAML 规范中的 automountServiceAccountToken: false 指令来完成。\n加固容器引擎 一些平台和容器引擎提供了额外的选项来加固容器化环境。一个强有力的例子是使用管理程序来提供容器隔离。管理程序依靠硬件来执行虚拟化边界，而不是操作系统。管理程序隔离比传统的容器隔离更安全。在 Windows® 操作系统上运行的容器引擎可以被配置为使用内置的 Windows 管理程序 Hyper-V®，以增强安全性。\n此外，一些注重安全的容器引擎将每个容器部署在一个轻量级的管理程序中，以实现深度防御。由管理程序支持的容器可以减少容器的突破。\n  译者注：Pod Security Policy 已在 1.21 版本中宣布弃用，作为替代，1.22 引入了内置的 Pod Security Admission 控制器以及新的 Pod Security Standards 标准。来源 ↩︎\n   ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"b8d62954b039b8bcb7e7631228ca435d","permalink":"https://jimmysong.io/docs/kubernetes-hardening-guidance/kubernetes-pod-security/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/kubernetes-hardening-guidance/kubernetes-pod-security/","section":"kubernetes-hardening-guidance","summary":"Pod 是 Kubernetes 中最小的可部署单元，由一个或多个容器组成。Pod 通常是","tags":null,"title":"Kubernetes Pod 安全","type":"book"},{"authors":null,"categories":null,"content":"HCM 支持修改和定制由 Envoy 返回的响应。请注意，这对上游返回的响应不起作用。\n本地回复是由 Envoy 生成的响应。本地回复的工作原理是定义一组映射器（mapper），允许过滤和改变响应。例如，如果没有定义任何路由或上游集群，Envoy 会发送一个本地 HTTP 404。\n每个映射器必须定义一个过滤器，将请求属性与指定值进行比较（例如，比较状态代码是否等于 403）。我们可以选择从多个过滤器来匹配状态代码、持续时间、Header、响应标志等。\n除了过滤器字段，映射器还有新的状态代码（status_code）、正文（body 和 body_format_override）和 Header（headers_to_add）字段。例如，我们可以有一个匹配请求状态代码 403 的过滤器，然后将状态代码改为 500，更新正文，或添加 Header。\n下面是一个将 HTTP 503 响应改写为 HTTP 401 的例子。注意，这指的是 Envoy 返回的状态代码。例如，如果上游不存在，Envoy 将返回一个 503。\n...- name:envoy.filters.network.http_connection_managertyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManagerlocal_reply_config:mappers:- filter:status_code_filter:comparison:op:EQvalue:default_value:503runtime_key:some_keyheaders_to_add:- header:key:\u0026#34;service\u0026#34;value:\u0026#34;unavailable\u0026#34;append:falsestatus_code:401body:inline_string:\u0026#34;Not allowed\u0026#34; 注意 runtime_key 字段是必须的。如果 Envoy 找不到运行时密钥，它就会返回到 default_value。\n ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"4403512ad89d569960782b15810d1619","permalink":"https://jimmysong.io/docs/envoy-handbook/hcm/reply-modification/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/envoy-handbook/hcm/reply-modification/","section":"envoy-handbook","summary":"HCM 支持修改和定制由 Envoy 返回的响应。请注意，这对上游返回的响应不","tags":null,"title":"修改响应","type":"book"},{"authors":null,"categories":null,"content":"OpenTelemetry 被设计成允许长期稳定性和不确定性并存的局面。在 OpenTelemetry 中，稳定性保证是在每个信号的基础上提供的。与其看版本号，不如检查你想使用的信号的稳定性等级。\n信号生命周期 图 6-1 显示了新信号是如何被添加到 OpenTelemetry 的。实验性信号仍在开发中。它们可能在任何时候改变并破坏兼容性。实验性信号的开发是以规范提案的形式开始的，它是与一组原型一起开发的。一旦实验性信号准备好在生产中使用，信号的特性就会被冻结，新信号的测试版就会以多种语言创建。测试版可能不是完整的功能，它们可能会有一些突破性的变化，但它们被认为是为早期采用者的产品反馈做好准备。一旦一个信号被认为可以被宣布为稳定版本，就会发布一个候选版本。如果候选版本能够在一段时间内保持稳定，没有问题，那么该信号的规范和测试版都被宣布为稳定。\n一旦一个信号变得稳定，它就属于 OpenTelemetry 的长期支持保障范围。OpenTelemetry 非常重视向后兼容和无缝升级。详情见以下章节。\n如果 OpenTelemetry 信号的某个组件需要退役，该组件将被标记为废弃的。被废弃的组件不再获得新的功能，但它们仍然被 OpenTelemetry 的长期支持保证所覆盖。如果可能的话，该组件将永远不会被删除，并将继续发挥作用。如果一个组件必须被删除，将提前宣布删除日期。\n实验性的功能总是与稳定性的功能保持在不同的包中，稳定的功能永远不能引用实验性的功能。这确保了新的开发不会影响现有特性的稳定性。只要库只依赖于稳定的特性，它们就不会经历破坏性的 API 变化。\n   图 6-1：OpenTelemetry 中的每个主要功能都被赋予了一个稳定性等级，并遵循相同的生命周期。  API 的稳定性 OpenTelemetry API 预计将被数以千计的库所依赖，有数以百万计的调用站点。因此，API 的稳定部分决不能破坏向后的兼容性。应用程序的所有者和库的开发者不应该为了升级到一个新版本的 API 而重新测量他们的应用程序。\n如果一个 OpenTelemetry API 被废弃（这不太可能），被废弃的 API 仍将保持稳定并发挥功能。\n 原生工具是稳定的工具\n携带原生 OpenTelemetry 仪表的 OSS 库应该只使用稳定的 API，因为实验性功能的改变可能会造成依赖性冲突。\n也就是说，我们鼓励进行测试。如果一个库愿意为实验性的 OpenTelemetry 功能提供支持，这是一个给项目提供反馈和参与新功能设计的好方法。然而，我们建议将与实验性 OpenTelemetry 功能的集成作为可选的插件提供，终端用户必须单独安装才能启用。\n一旦功能变得稳定，就没有必要把它们作为一个单独的插件。事实上，最好是将 OpenTelemetry 原生集成，因为用户可能会忘记安装插件。这样一来，如果应用程序所有者安装了 OpenTelemetry SDK，他们就会自动开始接收来自每个库的数据。如果没有安装 SDK，API 的调用就没有意义了。原生仪表是我们希望 OpenTelemetry 能够简化应用程序所有者的观察能力的一种方式 —— 它已经存在于每一个库中，只要它需要，就可以随时使用。\n SDK 和收集器的稳定性 SDK 的稳定性集中在两个方面：插件接口和资源使用。SDK 可能偶尔会废止一个插件接口。为了确保应用程序的所有者能够干净利落地进行升级，必须在废弃的接口被删除之前添加一个替代接口，而使用废弃接口的流行插件必须被迁移到新的接口上。通过以这种方式安全地迁移插件生态系统，可以避免应用程序所有者陷入这样的境地：他们想要升级，但却被一个无法使用的插件所阻挡。在废弃和移除一个插件接口之间必须有至少 6 个月的时间，而且废弃的接口只有在维护它们会造成性能问题时才会被移除。否则，我们将无限期地保留这些被废弃的接口。\n说到性能，OpenTelemetry SDK 的稳定部分必须避免性能倒退，以确保 SDK 的较新版本在升级时不会引起资源争夺。很明显，启用新版本中增加的功能可能需要额外的资源。但是，简单地升级 SDK 不应该导致性能退步。\n与 SDK 一样，收集器试图避免性能退步，并为收集器插件生态系统提供一个渐进的升级路径。\n升级 OpenTelemetry 客户端 在运行 OpenTelemetry 时，我们希望用户能保持最新的 SDK 版本。有两个事件可能会迫使用户升级 SDK：一个库将其仪表升级到新版本的 API，或者 OpenTelemetry 发布一个重要的安全补丁。\n上面列出的稳定性保证确保了这种升级路径始终是可行的。只要一个应用程序只依赖于稳定的信号，升级应该只涉及依赖性的提升。教程不需要重写，插件也不会突然变得不受支持。\nOpenTelemetry 致力于向后兼容的一个很好的例子是它对其前身 OpenTracing 的支持。OpenTelemetry 追踪信号与 OpenTracing API 完全兼容，OpenTelemetry 和 OpenTracing API 调用可以混合到同一个应用程序中。OpenTracing 用户可以升级到 OpenTelemetry 而不需要重写现有的仪表。\n","date":1643990400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"a8280488efa53998e991e73ce4e5bf02","permalink":"https://jimmysong.io/docs/opentelemetry-obervability/stability-and-long-term-support/","publishdate":"2022-02-05T00:00:00+08:00","relpermalink":"/docs/opentelemetry-obervability/stability-and-long-term-support/","section":"opentelemetry-obervability","summary":"第 6 章：稳定和长期支持","tags":null,"title":"第 6 章：稳定和长期支持","type":"book"},{"authors":null,"categories":null,"content":"有时候紧急 CL 必须尽快通过 code review 过程。\n什么是紧急情况？ 紧急 CL 是这样的小更新：允许主要发布继续而不是回滚，修复显著影响用户生产的错误，处理紧迫的法律问题，关闭主要安全漏洞等。\n在紧急情况下，我们确实关心 Code Review 的整体速度，而不仅仅是响应的速度。仅在这种情况下，审查人员应该更关心审查的速度和代码的正确性（是否解决了紧急情况？）。此外（显然）这类状况的审查应该优先于所有其他 code reivew。\n但是，在紧急情况解决后，您应该再次查看紧急 CL 并进行更彻底的审查。\n什么不是紧急情况？ 需要说明的是，以下情况并非紧急情况：\n 想要在本周而不是下周推出（除非有一些实际硬性截止日期，例如合作伙伴协议）。 开发人员已经在很长一段时间内完成了一项功能想要获得 CL。 审查者都在另一个时区，目前是夜间或他们已离开现场。 现在是星期五，在开发者在过周末之前获得这个 CL 会很棒。 今天因为软（非硬）截止日期，经理表示必须完成此审核并签入 CL。 回滚导致测试失败或构建破坏的 CL。  等等。\n什么是 Hard Deadline？ 硬性截止日期（Hard Deadline）是指如果你错过它会发生灾难性的事情。例如：\n 对于合同义务，必须在特定日期之前提交 CL。 如果在某个日期之前没有发布，您的产品将在市场上完全失败。 一些硬件制造商每年只发送一次新硬件。如果您错过了向他们提交代码的截止日期，那么这可能是灾难性的，具体取决于您尝试发布的代码类型。  延迟发布一周并不是灾难性的。错过重要会议可能是灾难性的，但往往不是。\n大多数截止日期都是软截止日期，而非最后期限。软截止日期表示希望在特定时间内完成某项功能。它们很重要，但你不应该以牺牲代码健康为前提来达到。\n如果您的发布周期很长（几周），那么在下一个周期之前就可能会牺牲代码审查质量来获取功能。然而，如果重复这种模式，往往会给项目建立压倒性技术债务。如果开发人员在周期结束时经常提交 CL，只需要进行表面评审就必须“进入”，那么团队应该修改其流程，以便在周期的早期发生大的功能变更，并有足够的时间进行良好的审查。\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"e58bdd5c7c6cadaf5039d55bc431c112","permalink":"https://jimmysong.io/docs/eng-practices/review/emergencies/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/docs/eng-practices/review/emergencies/","section":"eng-practices","summary":"有时候紧急 CL 必须尽快通过 code review 过程。 什么是紧急情况？ 紧急 CL 是这","tags":null,"title":"紧急情况","type":"book"},{"authors":null,"categories":null,"content":"唯一的请求 ID 对于通过多个服务追踪请求、可视化请求流和精确定位延迟来源至关重要。\n我们可以通过 request_id_extension 字段配置请求 ID 的生成方式。如果我们不提供任何配置，Envoy 会使用默认的扩展，称为 UuidRequestIdConfig。\n默认扩展会生成一个唯一的标识符（UUID4）并填充到 x-request-id HTTP 头中。Envoy 使用 UUID 的第 14 个位点来确定跟踪的情况。\n如果第 14 个比特位（nibble）被设置为 9，则应该进行追踪采样。如果设置为 a，应该是由于服务器端的覆盖（a）而强制追踪，如果设置为 b，应该是由客户端的请求 ID 加入而强制追踪。\n之所以选择第 14 个位点，是因为它在设计上被固定为 4。因此，4 表示一个默认的 UUID 没有跟踪状态，例如 7b674932-635d-4ceb-b907-12674f8c7267（说明：第 14 比特位实际为第 13 个数字）。\n我们在 UuidRequestIdconfig 中的两个配置选项是 pack_trace_reason 和 use_request_id_for_trace_sampling。\n.....route_config:name:local_routerequest_id_extension:typed_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.request_id.uuid.v3.UuidRequestIdConfigpack_trace_reason:falseuse_request_id_for_trace_sampling:falsehttp_filters:- name:envoy.filters.http.router...pack_trace_reaseon 是一个布尔值，控制实现是否改变 UUID 以包含上述的跟踪采样决定。默认值是 true。use_request_id_for_trace_sampling 设置是否使用 x-request-id 进行采样。默认值也是 true。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"20a8e239982dddaf0078cce4cbf28362","permalink":"https://jimmysong.io/docs/envoy-handbook/hcm/request-id-generation/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/envoy-handbook/hcm/request-id-generation/","section":"envoy-handbook","summary":"唯一的请求 ID 对于通过多个服务追踪请求、可视化请求流和精确定位","tags":null,"title":"生成请求 ID","type":"book"},{"authors":null,"categories":null,"content":"集群网络是 Kubernetes 的一个核心概念。容器、Pod、服务和外部服务之间的通信必须被考虑在内。默认情况下，很少有网络策略来隔离资源，防止集群被破坏时的横向移动或升级。资源隔离和加密是限制网络行为者在集群内转移和升级的有效方法。\n关键点\n 使用网络策略和防火墙来隔离资源。 确保控制平面的安全。 对流量和敏感数据（例如 Secret）进行静态加密。  命名空间 Kubernetes 命名空间是在同一集群内的多个个人、团队或应用程序之间划分集群资源的一种方式。默认情况下，命名空间不会被自动隔离。然而，命名空间确实为一个范围分配了一个标签，这可以用来通过 RBAC 和网络策略指定授权规则。除了网络隔离之外，策略可以限制存储和计算资源，以便在命名空间层面上对 Pod 进行更好的控制。\n默认有三个命名空间，它们不能被删除：\n kube-system（用于 Kubernetes 组件） kube-public（用于公共资源） default（针对用户资源）  用户 Pod 不应该放在 kube-system 或 kube-public 中，因为这些都是为集群服务保留的。可以用 YAML 文件，如 附录 D：命名空间示例 ，可以用来创建新的命名空间。不同命名空间中的 Pod 和服务仍然可以相互通信，除非有额外的隔离措施，如网络策略。\n网络策略 网络策略控制 Pod、命名空间和外部 IP 地址之间的流量。默认情况下，没有网络策略应用于 Pod 或命名空间，导致 Pod 网络内的入口和出口流量不受限制。通过适用于 Pod 或 Pod 命名空间的网络策略，Pod 将被隔离。一旦一个 Pod 在网络策略中被选中，它就会拒绝任何适用的策略对象所不允许的任何连接。\n要创建网络策略，需要一个支持 NetworkPolicy API 的网络插件。使用 podSelector 和 / 或 namespaceSelector 选项来选择 Pod。附录 E 中展示了一个网络策略的例子。网络策略的格式可能有所不同，这取决于集群使用的容器网络接口（CNI）插件。管理员应该使用选择所有 Pod 的默认策略来拒绝所有入口和出口流量，并确保任何未选择的 Pod 被隔离。然后，额外的策略可以放松这些允许连接的限制。\n外部 IP 地址可以使用 ipBlock 在入口和出口策略中使用，但不同的 CNI 插件、云提供商或服务实现可能会影响 NetworkPolicy 处理的顺序和集群内地址的重写。\n资源政策 除了网络策略，LimitRange 和 ResourceQuota 是两个可以限制命名空间或节点的资源使用的策略。LimitRange 策略限制了特定命名空间内每个 Pod 或容器的单个资源，例如，通过强制执行最大计算和存储资源。每个命名空间只能创建一个 LimitRange 约束，如 附录 F 的 LimitRange 示例中所示。Kubernetes 1.10 和更新版本默认支持 LimitRange。\n与 LimitRange 策略不同的是，ResourceQuotas 是对整个命名空间的资源使用总量的限制，例如对 CPU 和内存使用总量的限制。如果用户试图创建一个违反 LimitRange 或 ResourceQuota 策略的 Pod，则 Pod 创建失败。附录 G 中显示了一个 ResourceQuota 策略的示例。\n控制平面加固 控制平面是 Kubernetes 的核心，使用户能够查看容器，安排新的 Pod，读取 Secret，在集群中执行命令。由于这些敏感的功能，控制平面应受到高度保护。除了 TLS 加密、RBAC 和强大的认证方法等安全配置外，网络隔离可以帮助防止未经授权的用户访问控制平面。Kubernetes API 服务器运行在 6443 和 8080 端口上，这些端口应该受到防火墙的保护，只接受预期的流量。8080 端口，默认情况下，可以在没有 TLS 加密的情况下从本地机器访问，请求绕过认证和授权模块。不安全的端口可以使用 API 服务器标志 --insecure-port=0 来禁用。Kubernetes API 服务器不应该暴露在互联网或不信任的网络中。网络策略可以应用于 kube-system 命名空间，以限制互联网对 kube-system 的访问。如果对所有命名空间实施默认的拒绝策略，kube-system 命名空间仍然必须能够与其他控制平面和工作节点进行通信。\n下表列出了控制平面的端口和服务。\n表 2：控制平面端口\n   端口 方向 端口范围 目的     TCP Inbound 6443 or 8080 if not disabled Kubernetes API server   TCP Inbound 2379-2380 etcd server client API   TCP Inbound 10250 kubelet API   TCP Inbound 10251 kube-scheduler   TCP Inbound 10252 kube-controller-manager   TCP Inbound 10258 cloud-controller-manager（可选）    Etcd  etcd 后端数据库是一个关键的控制平面组件，也是集群中最重要的安全部分。\n etcd 后端数据库存储状态信息和集群 Secret。它是一个关键的控制平面组件，获得对 etcd 的写入权限可以使网络行为者获得对整个集群的 root 权限。Etcd 只能通过 API 服务器访问，集群的认证方法和 RBAC 策略可以限制用户。etcd 数据存储可以在一个单独的控制平面节点上运行，允许防火墙限制对 API 服务器的访问。管理员应该设置 TLS 证书以强制执行 etcd 服务器和 API 服务器之间的 HTTPS 通信。etcd 服务器应被配置为只信任分配给 API 服务器的证书。\nKubeconfig 文件 kubeconfig 文件包含关于集群、用户、命名空间和认证机制的敏感信息。Kubectl 使用存储在工作节点的 $HOME/.kube 目录下的配置文件，并控制平面本地机器。网络行为者可以利用对该配置目录的访问，获得并修改配置或凭证，从而进一步破坏集群。配置文件应该被保护起来，以防止非故意的改变，未经认证的非 root 用户应该被阻止访问这些文件。\n工作节点划分 工作节点可以是一个虚拟机或物理机，这取决于集群的实现。由于节点运行微服务并承载集群的网络应用，它们往往是被攻击的目标。如果一个节点被破坏，管理员应主动限制攻击面，将工作节点与其他不需要与工作节点或 Kubernetes 服务通信的网段分开。防火墙可用于将内部网段与面向外部的工作节点或整个 Kubernetes 服务分开，这取决于网络的情况。机密数据库或不需要互联网访问的内部服务，这可能需要与工作节点的可能攻击面分离。\n下表列出了工作节点的端口和服务。\n表 3：工作节点端口\n   端口 方向 端口范围 目的     TCP Inbound 10250 kubelet API   TCP Inbound 30000-32767 NodePort Services    加密 管理员应配置 Kubernetes 集群中的所有流量 —— 包括组件、节点和控制计划之间的流量（使用 TLS 1.2 或 1.3 加密）。\n加密可以在安装过程中设置，也可以在安装后使用 TLS 引导（详见 Kubernetes 文档）来创建并向节点分发证书。对于所有的方法，必须在节点之间分发证书，以便安全地进行通信。\nSecret  默认情况下，Secret 被存储为未加密的 base64 编码的字符串，并且可以被任何有 API 权限的人检索。\n Kubernetes Secret 维护敏感信息，如密码、OAuth 令牌和 SSH 密钥。与在 YAML 文件、容器镜像或环境变量中存储密码或令牌相比，将敏感信息存储在 Secret 中提供了更大的访问控制。默认情况下，Kubernetes 将 Secret 存储为未加密的 base64 编码字符串，任何有 API 权限的人都可以检索到。可以通过对 secret 资源应用 RBAC 策略来限制访问。\n可以通过在 API 服务器上配置静态数据加密或使用外部密钥管理服务（KMS）来对秘密进行加密，该服务可以通过云提供商提供。要启用使用 API 服务器的 Secret 数据静态加密，管理员应修改 kube-apiserver 清单文件，以执行使用 --encryption-provider-config 参数执行。附录 H 中显示了一个 encryption-provider-config 的例子：加密实例。使用 KMS 提供者可以防止原始加密密钥被存储在本地磁盘上。要用 KMS 提供者加密 Secret，encryption-provider-config 文件中应指定 KMS 提供者，如附录 I 的 KMS 配置示例所示。\n在应用了 encryption-provider-config 文件后，管理员应该运行以下命令来读取和加密所有的 Secret。\nkubectl get secrets --all-namespaces -o json | kubectl replace -f - 保护敏感的云基础设施 Kubernetes 通常被部署在云环境中的虚拟机上。因此，管理员应该仔细考虑 Kubernetes 工作节点所运行的虚拟机的攻击面。在许多情况下，在这些虚拟机上运行的 Pod 可以在不可路由的地址上访问敏感的云元数据服务。这些元数据服务为网络行为者提供了关于云基础设施的信息，甚至可能是云资源的短期凭证。网络行为者滥用这些元数据服务进行特权升级。Kubernetes 管理员应通过使用网络策略或通过云配置策略防止 Pod 访问云元数据服务。由于这些服务根据云供应商的不同而不同，管理员应遵循供应商的指导来加固这些访问载体。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"b0b9135a069c9755963eeaaee6b0cbe0","permalink":"https://jimmysong.io/docs/kubernetes-hardening-guidance/network-separation-and-hardening/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/kubernetes-hardening-guidance/network-separation-and-hardening/","section":"kubernetes-hardening-guidance","summary":"集群网络是 Kubernetes 的一个核心概念。容器、Pod、服务和外部服务之间","tags":null,"title":"网络隔离和加固","type":"book"},{"authors":null,"categories":null,"content":"现在我们了解了组成 OpenTelemetry 的各个构件，我们应该如何将它们组合成一个强大的生产管道？\n答案取决于你的出发点是什么。OpenTelemetry 是模块化的，设计成可以在各种不同的规模下工作。你只需要使用相关的部分。这就是说，我们已经创建了一个建议的路线图供你遵循。\n安装 OpenTelemetry 客户端 可以单独使用 OpenTelemetry 客户端而不部署收集器。这种基本设置通常是绿地部署的充分起点，无论是测试还是初始生产。OpenTelemetry SDK 可以被配置为直接向大多数可观测性服务传输遥测数据。\n挑选一个导出器 默认情况下，OpenTelemetry 使用 OTLP 导出数据。该 SDK 提供了几种常见格式的导出器。Zipkin、Prometheus、StatsD 等。如果你使用的可观测性后端没有原生支持 OTLP，那么这些其他格式中的一种很可能会被支持。安装正确的导出器并将数据直接发送到你的后端系统。\n安装库仪表 除了 SDK，OpenTelemetry 仪表必须安装在所有 HTTP 客户端、Web 框架、数据库和应用程序的消息队列中。如果这些库中有一个缺少仪表，上下文传播就会中断，导致不完整的追踪和混乱的数据。\n在某些语言中，如 Java，仪表可以自动安装，这就更容易了。请确保了解 OpenTelemetry 如何在你使用的编程语言中管理仪表，并仔细检查仪表是否正确安装在你的应用程序中。\n选择传播器 仔细检查你的系统需要哪些传播器也很重要。默认情况下，OpenTelemetry 使用 W3C 的追踪上下文和 Baggage 传播器。然而，如果你的应用程序需要与使用不同的追踪传播器的服务进行通信，如 Zipkin 的 B3 或 AWS 的 X-Amzn，那么改变 OTEL_PROPAGATORS 配置以包括这个额外的传播器。\n如果 OpenTelemetry 最终要取代这些其他的追踪系统，我建议同时运行 trace-context 和额外的追踪传播器。这将使你在部署中逐步取代旧系统时，能够无缝地过渡到 W3C 标准。\n部署本地收集器 虽然有些系统有可能只使用客户端，但通过在你的应用程序所运行的机器上添加一个本地收集器，可以改善你的操作体验。\n运行一个本地收集器有许多好处，如图 7-1 所示。收集器可以生成机器指标（CPU、RAM 等），这是遥测的一个重要部分。收集器还可以完成任何需要的数据处理任务，如从追踪和日志数据中清除 PII。\n   图 7-1：一个本地收集器（C）从本地应用程序（A）接收遥测数据，同时收集主机指标（H）。收集器将合并的遥测数据输出到管道的下一个阶段。  运行收集器后就可以将大多数遥测配置从你的应用程序中移出。遥测配置通常是特定的部署，而不是特定的应用。SDK 可以简单地设置为使用默认配置，总是将 OTLP 数据导出到预定义的本地端口。通过管理本地收集器，运维可以在不需要与应用程序开发人员协调或重新启动应用程序的情况下进行配置更改。在通过复杂的 CI/CD（持续集成 / 持续交付）管道移动应用程序时，这尤其有帮助，因为在不同的暂存和负载测试环境中，遥测需要不同的处理方式。\n快速发送遥测数据到本地收集器，可以作为一个缓冲器来处理负载，并确保在应用程序崩溃时，缓冲的遥测数据不会丢失。\n部署收集器处理器池 如果你的本地收集器开始执行大量的缓冲和数据处理，它就会从你的应用程序中窃取资源。这可以通过部署一个只运行收集器的机器池来解决，这些机器位于负载均衡器后面，如图 7-2 所示。现在可以根据数据吞吐量来管理收集器池的大小。\n   图 7-2：应用程序（A）可以通过使用路由器或负载均衡器向收集器（C）池发送遥测信息。  本地收集器现在可以关闭其处理器以释放资源。它们继续收集机器级遥测数据，作为来自本地应用程序的 OTLP 的转发机制。\n添加额外的处理池 有时，单个收集器池是不够的。一些任务可能需要以不同的速度扩展。将收集器池分割成一个更专门的池的管道，可能允许更有效和可管理的扩展策略，因为每个专门的收集器池的工作负载变得更可预测。\n一旦你达到了这个规模，就没有什么部署的问题了。大规模系统的专门需求往往是独特的，这些需求将驱动你的可观测性管道的拓扑结构。利用收集器提供的灵活性，根据你的需求来定制每一件事情。我建议对每个收集器配置的资源消耗进行基准测试，并使用这些信息来创建弹性的、自动扩展的收集器池。\n用收集器管理现有的遥测数据 上面描述的路线图适用于上线 OpenTelemetry。但你应该如何处理现有的遥测？大多数运行中的系统已经有了某种形式的指标、日志和（可能有）追踪。而大型的、长期运行的系统往往最终会有多个遥测解决方案的补丁。不同的组件可能是在不同的时代建立的，有些组件可能是从外部继承的，比如收购。从可观测性的角度来看，这可能会导致混乱的局面。\n即使在这样复杂的遗留情况下，仍然有可能过渡到 OpenTelemetry，而不需要停机或一次重写所有的服务。秘诀是首先部署一个收集器，作为一个透明的代理。\n在收集器中，为接收你的系统目前产生的每一种类型的遥测设置接收器，并与以完全相同的格式发送遥测的导出器相连。一个 StatsD 接收器连接到一个 StatsD 导出器，一个 Zipkin 接收器连接到一个 Zipkin 导出器，以此类推。这种透明的代理可以逐步推出，而不会造成干扰。一旦所有的远程测量都由这些收集器来调解，就可以引入额外的处理。甚至在你把你的仪表切换到 OpenTelemetry 之前，你可能会发现这些收集器是管理和组织你当前拼凑的遥测系统的一个有用的方法。图 7-3 显示了一个收集器处理来自各种来源的数据。\n   图 7-3：收集器可以帮助管理复杂的、拼凑的可观测性系统，这些系统以各种格式向各种存储系统发送数据。  为了开始将服务切换到 OpenTelemetry，可以在收集器上添加一个 OTLP 接收器，与现有的导出器相连。随着服务转向使用 OpenTelemetry 客户端，它们将 OTLP 发送到收集器，收集器将把 OTLP 翻译成这些系统以前产生的相同数据。这使得 OpenTelemetry 可以由不同的应用团队逐步上线，而不会出现中断。\n转移供应商 一旦所有的遥测流量都通过收集器发送，切换到一个新的可观测性后端就变得很容易了：只需在收集器中添加一个导出器，将数据发送到你想尝试的新系统，并将遥测数据同时发送到旧系统和新系统。通过向两个系统发送数据，你创造了一个重叠的覆盖范围。如果你喜欢新系统，你可以在一段时间后让旧系统退役，以避免在可视性方面产生差距。图 7-4 说明了这个过程。\n   图 7-4：使用收集器在可观测性后端之间迁移而不中断服务。  也可以使用 OpenTelemetry 在多个供应商之间进行测验。你可以同时向多个系统发送遥测信息，并直接比较系统，看哪一个最适合你的需要。\n","date":1643990400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"16f19f36772970ce7b130f2c1ed4484e","permalink":"https://jimmysong.io/docs/opentelemetry-obervability/suggested-setups-and-telemetry-pipelines/","publishdate":"2022-02-05T00:00:00+08:00","relpermalink":"/docs/opentelemetry-obervability/suggested-setups-and-telemetry-pipelines/","section":"opentelemetry-obervability","summary":"第 7 章：建议的设置和遥测管道","tags":null,"title":"第 7 章：建议的设置和遥测管道","type":"book"},{"authors":null,"categories":null,"content":"Envoy 支持许多可配置的超时，这取决于你使用代理的场景。\n我们将在 HCM 部分看一下不同的可配置超时。请注意，其他过滤器和组件也有各自的超时时间，我们在此不做介绍。\n在配置的较高层次上设置的一些超时 —— 例如在 HCM 层次，可以覆盖较低层次上的配置，例如 HTTP 路由层次。\n最著名的超时可能是请求超时。请求超时（request_timeout）指定了 Envoy 等待接收整个请求的时间（例如 120s）。当请求被启动时，该计时器被激活。当最后一个请求字节被发送到上游时，或者当响应被启动时，定时器将被停用。默认情况下，如果没有提供或设置为 0，则超时被禁用。\n类似的超时称为 idle_timeout，表示如果没有活动流，下游或上游连接何时被终止。默认的空闲超时被设置为 1 小时。空闲超时可以在 HCM 配置的 common_http_protocol_options 中设置，如下所示。\n...filters:- name:envoy.filters.network.http_connection_managertyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManagerstat_prefix:ingress_httpcommon_http_protocol_options:# 设置空闲超时为 10 分钟idle_timeout:600s...为了配置上游连接的空闲超时，我们可以使用相同的字段 common_http_protocol_options，但在集群部分。\n还有一个与 Header 有关的超时，叫做 request_headers_timeout。这个超时规定了 Envoy 等待接收请求头信息的时间（例如 5s）。该计时器在收到头信息的第一个字节时被激活。当收到头信息的最后一个字节时，该时间就会被停用。默认情况下，如果没有提供或设置为 0，则超时被禁用。\n其他一些超时也可以设置，比如 stream_idle_timeout、drain_timeout 和 delayed_close_timeout。\n接下来就是路由超时。如前所述，路由层面的超时可以覆盖 HCM 的超时和一些额外的超时。\n路由 timeout 是指 Envoy 等待上游做出完整响应的时间。一旦收到整个下游请求，该计时器就开始计时。超时的默认值是 15 秒；但是，它与永不结束的响应（即流媒体）不兼容。在这种情况下，需要禁用超时，而应该使用 stream_idle_timeout。\n我们可以使用 idle_timeout 字段来覆盖 HCM 层面上的 stream_idle_timeout。\n我们还可以提到 per_try_timeout 设置。这个超时是与重试有关的，它为每次尝试指定一个超时。通常情况下，个别尝试应该使用比 timeout 域设置的值更短的超时。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"8af362e015ada559d62f1bc27496d636","permalink":"https://jimmysong.io/docs/envoy-handbook/hcm/timeout/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/envoy-handbook/hcm/timeout/","section":"envoy-handbook","summary":"Envoy 支持许多可配置的超时，这取决于你使用代理的场景。 我们将在 HCM","tags":null,"title":"超时","type":"book"},{"authors":null,"categories":null,"content":"认证和授权是限制访问集群资源的主要机制。如果集群配置错误，网络行为者可以扫描知名的 Kubernetes 端口，访问集群的数据库或进行 API 调用，而不需要经过认证。用户认证不是 Kubernetes 的一个内置功能。然而，有几种方法可以让管理员在集群中添加认证。\n认证  管理员必须向集群添加一个认证方法，以实现认证和授权机制。\n Kubernetes 集群有两种类型的用户：服务账户和普通用户账户。服务账户代表 Pod 处理 API 请求。认证通常由 Kubernetes 通过 ServiceAccount Admission Controller 使用承载令牌自动管理。不记名令牌被安装到 Pod 中约定俗成的位置，如果令牌不安全，可能会在集群外使用。正因为如此，对 Pod Secret 的访问应该限制在那些需要使用 Kubernetes RBAC 查看的人身上。对于普通用户和管理员账户，没有自动的用户认证方法。管理员必须在集群中添加一个认证方法，以实现认证和授权机制。\nKubernetes 假设由一个独立于集群的服务来管理用户认证。Kubernetes 文档中列出了几种实现用户认证的方法，包括客户端证书、承载令牌、认证插件和其他认证协议。至少应该实现一种用户认证方法。当实施多种认证方法时，第一个成功认证请求的模块会缩短评估的时间。管理员不应使用静态密码文件等弱方法。薄弱的认证方法可能允许网络行为者冒充合法用户进行认证。\n匿名请求是被其他配置的认证方法拒绝的请求，并且不与任何个人用户或 Pod 相联系。在一个设置了令牌认证并启用了匿名请求的服务器中，没有令牌的请求将作为匿名请求执行。在 Kubernetes 1.6 和更新的版本中，匿名请求是默认启用的。当启用 RBAC 时，匿名请求需要  system:anonymous 用户或 system:unauthenticated 组的明确授权。匿名请求应该通过向 API 服务器传递 --anonymous-auth=false 选项来禁用。启用匿名请求可能会允许网络行为者在没有认证的情况下访问集群资源。\n基于角色的访问控制 RBAC 是根据组织内个人的角色来控制集群资源访问的一种方法。在 Kubernetes 1.6 和更新的版本中，RBAC 是默认启用的。要使用 kubectl 检查集群中是否启用了 RBAC，执行 kubectl api-version。如果启用，应该列出rbac.authorization.k8s.io/v1 的 API 版本。云 Kubernetes 服务可能有不同的方式来检查集群是否启用了 RBAC。如果没有启用 RBAC，在下面的命令中用 --authorization-mode 标志启动 API 服务器。\nkube-apiserver --authorization-mode=RBAC 留下授权模式标志，如 AlwaysAllow，允许所有的授权请求，有效地禁用所有的授权，限制了执行最小权限的访问能力。\n可以设置两种类型的权限：Roles 和 ClusterRoles。Roles 为特定命名空间设置权限，而 ClusterRoles 则为所有集群资源设置权限，而不考虑命名空间。Roles 和 ClusterRoles 只能用于添加权限。没有拒绝规则。如果一个集群被配置为使用 RBAC，并且匿名访问被禁用，Kubernetes API 服务器将拒绝没有明确允许的权限。附录 J 中显示了一个 RBAC 角色的例子：pod-reader RBAC 角色。\n一个 Role 或 ClusterRole 定义了一个权限，但并没有将该权限与一个用户绑定。RoleBindings 和 ClusterRoleBindings 用于将一个 Roles 或 ClusterRoles 与一个用户、组或服务账户联系起来。角色绑定将角色或集群角色的权限授予定义的命名空间中的用户、组或服务账户。ClusterRoles 是独立于命名空间而创建的，然后可以使用 RoleBinding 来限制命名空间的范围授予个人。ClusterRoleBindings 授予用户、群组或服务账户跨所有集群资源的 ClusterRoles。RBAC RoleBinding 和 ClusterRoleBinding 的例子在附录 K：RBAC RoleBinding 和 ClusterRoleBinding 示例中。\n要创建或更新 Roles 和 ClusterRoles，用户必须在同一范围内拥有新角色所包含的权限，或者拥有对 rbac.authorization.k8s.io API 组中的 Roles 或 ClusterRoles 资源执行升级动词的明确权限。创建绑定后，Roles 或 ClusterRoles 是不可改变的。要改变一个角色，必须删除该绑定。\n分配给用户、组和服务账户的权限应该遵循最小权限原则，只给资源以必要的权限。用户或用户组可以被限制在所需资源所在的特定命名空间。默认情况下，为每个命名空间创建一个服务账户，以便 Pod 访问 Kubernetes API。可以使用 RBAC 策略来指定每个命名空间的服务账户的允许操作。对 Kubernetes API 的访问是通过创建 RBAC 角色或 ClusterRoles 来限制的，该角色具有适当的 API 请求动词和所需的资源，该行动可以应用于此。有一些工具可以通过打印用户、组和服务账户及其相关分配的 Roles 和 ClusterRoles 来帮助审计 RBAC 策略。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"6ac45830cf25146d3deec701992dbcc0","permalink":"https://jimmysong.io/docs/kubernetes-hardening-guidance/authentication-and-authorization/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/kubernetes-hardening-guidance/authentication-and-authorization/","section":"kubernetes-hardening-guidance","summary":"认证和授权是限制访问集群资源的主要机制。如果集群配置错误，网","tags":null,"title":"认证和授权","type":"book"},{"authors":null,"categories":null,"content":"上线一个新的遥测系统可能是一项复杂的工作。它需要整个工程组织的支持，不能一蹴而就。不要低估这可能会产生的问题！\n在大型组织中，通常有许多服务团队负责系统的不同部分。通常情况下，每个团队都需要付出一定的努力来使他们所管理的服务得到充分的工具化。而这些团队都有自己积压的工作，他们当然希望能够优先处理这些工作。\n不幸的是，可观测性计划在开始提供价值和证明其价值之前就会耗尽人的耐心。但通过仔细的计划和协调，这种情况是可以避免的。\n主要目标 在推广 OpenTelemetry 时，重要的是要记住，任何基于分布式追踪的可观测性系统都需要对参与事务的每个服务进行检测，以提供最大价值。如果只有部分服务被检测到，那么追踪就会被分割成小的、不相连的部分。\n这种散乱的仪表的结果是不可取的。这种情况——不一致的仪表和断裂的追踪是你想要避免的主要事情。如果追踪是断开的，运维人员仍然需要在他们的头脑中把所有的东西拼凑起来，以获得他们系统的情况。更糟糕的是，自动分析工具可以使用的数据非常有限。与人类运维人员不同，他们可以运用直觉，跳出框框来思考问题，而分析工具却只能使用他们得到的数据。由于数据有限，他们提供有用的见解的机会也将是有限的。\n为了避免这个陷阱，集中精力对一个工作流程进行检测，在进入下一个工作流程之前对其进行完整的追踪。大多数工作流程并不涉及大系统的每一个部分，所以这种方法将最大限度地减少分析开始和价值实现之前所需的工作量。\n选择一个高价值的目标 谈到实现价值，在开始进行仪表测量工作时，重要的是要有一个有吸引力的目标！这一点很重要。\n最有可能的是，有一个特别的问题促使人们去部署 OpenTelemetry。如果是这样的话，就把重点放在解决该问题所需的最小的推广上。否则，想一想那些众所周知的问题，解决它们就值得公布了。\n目前有哪些痛苦的、长期的问题在困扰着运维？减少系统延迟在哪里可以直接转化为商业价值？当人们问 “为什么这么慢？” 时，他们说的是系统的哪一部分？在选择第一个工作流程时，请以这些信息为指导。\n识别一个有吸引力的目标有两个好处。首先，它可以更有利于说服众人，因为有一个具体的理由来做这项工作。这使得它更容易说服有关团队优先考虑增加指导，并以协调的方式进行。\n第二，速战速决给你的新的可观测性系统一个闪亮的机会。第一次通过分布式追踪的视角来分析一个软件系统时，几乎总是会产生有用的见解。证明可观测性的价值可以引起很多人的兴趣，并有助于降低采用时任何挥之不去的障碍。\n如果直接进入生产是困难的，“生产支持” 系统也是一个好的开始。可以对 CI/CD 系统进行检测，以帮助了解构建和部署的性能。在这里，整个组织都会感受到性能的大幅提升，并可以为将 OpenTelemetry 转移到生产中提供良好的理由。\n集中遥测管理 展开和管理遥测系统从集中化中获益良多。在一些组织中，会有一个平台或信息结构团队可以接触到每一项服务。像这样的团队是集中管理遥测的一个好地方，这可以提供巨大的帮助。遥测管道最好被认为是它自己的系统；允许一个团队操作整个遥测管道，往往比要求许多团队各自拥有系统的一部分要好。\n在软件层面，将 OpenTelemetry 设置与已经广泛部署的代码管理工具——例如共享的启动脚本和应用框架整合起来，减少了每个团队需要管理的代码量。这有助于确保服务与最新的版本和配置保持同步，并使采用更加容易。\n另一个关键工具是一个集中的知识库。OpenTelemetry 有文档，但它是通用的。创建特定于在你的组织内部署、管理和使用 OpenTelemetry 的文档。大多数工程师都是第一次接触 OpenTelemetry 和分布式追踪，这对他们的帮助怎么强调都不为过。\n先广度后深度 还有一个关于合理分配精力的说明。当对一个工作流程进行端对端检测时，通常只需安装 OpenTelemetry 附带的仪表，加上你的组织所使用的任何内部或自创框架的仪表即可。没有必要深入检测应用程序代码，至少在开始时没有必要。OpenTelemetry 附带的标准跨度和指标足以让你识别大多数问题；必要时可以有选择地增加更深层次的检测。在添加这种细节之前，请确保你已经建立并运行了端到端的追踪。\n这就是说，有一个快速的方法可以为你的追踪增加很多细节，那就是把任何现有的日志转换成追踪事件。这可以通过创建一个简单的日志附加器来实现，它可以抓取当前的跨度，并将日志作为一个事件附加到它上面。现在，当你查找追踪时，你所有的应用日志都可以得到，这比在传统的日志工具中寻找它们要容易得多。OpenTelemetry 确实为一些常见的日志系统提供了日志附加程序，但它们也很容易编写。\n与管理层合作 如果你是一个工程师在读这篇文章，我有一个补充说明。推广一个新的遥测系统可能需要组织很多人。幸运的是，有些人已经在做这种组织工作了 —— 经理们！这就是我们的工作。\n但是，如果你想启动其中的一个项目，说服工程或项目经理来帮助你是很好的第一步。他们将对如何完成项目有宝贵的见解，并能在你可能不参加的会议上推销该项目。有时候，组织人比组织代码更难，所以不要害怕寻求帮助！\n加入社区 最后，在个人和组织层面上，考虑加入 OpenTelemetry 社区！维护者和项目负责人都很友好，非常平易近人。社区是一个很好的获取援助和专业知识的资源的地方；我们总是很乐意帮助新的用户得到指导。还有一种汗水文化：如果有你想看到的 OpenTelemetry 功能，加入一个工作组并提供帮助是使它们得到优先考虑的一个好办法。\n至少，一定要给我们反馈。我们从用户那里听到的越多，我们就越能专注于最重要的问题。我们的目标是建立一个推动下一代可观测性的标准。没有你，我们做不到，我们的大门永远是敞开的。\n谢谢你的阅读 我希望你喜欢这份关于 OpenTelemetry 的可观测性的未来的报告。如果你有任何问题、评论、反馈或基于你所读的内容的灵感，请随时在 Twitter 上与我联系，我是 @tedsuo。\n要想获得 OpenTelemetry 的帮助，请加入云原生计算基金会（CNCF）Slack 上的 #OpenTelemetry 频道。我希望能在那里见到你！\n","date":1643990400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"2b8b19ee91b20be28a40d23fb48e1aca","permalink":"https://jimmysong.io/docs/opentelemetry-obervability/roll-out/","publishdate":"2022-02-05T00:00:00+08:00","relpermalink":"/docs/opentelemetry-obervability/roll-out/","section":"opentelemetry-obervability","summary":"第 8 章：如何在组织中推广 OpenTelemetry","tags":null,"title":"第 8 章：如何在组织中推广 OpenTelemetry","type":"book"},{"authors":null,"categories":null,"content":"Aeraki 是腾讯云在 2021 年 3 月开源的一个服务网格领域的项目。Aeraki 提供了一个端到端的云原生服务网格协议扩展解决方案，以一种非侵入的方式为 Istio 提供了强大的第三方协议扩展能力，支持在 Istio 中对 Dubbo、Thrift、Redis，以及对私有协议进行流量管理。Aeraki 的架构如下图所示：\n   Aeraki架构图  来源：https://istio.io/latest/blog/2021/aeraki/\n从 Aeraki 架构图中可以看到，Aeraki 协议扩展解决方案包含了两个组件：\n Aeraki：Aeraki 作为一个 Istio 增强组件运行在控制面，通过自定义 CRD 向运维提供了用户友好的流量规则配置。Aeraki 将这些流量规则配置翻译为 Envoy 配置，通过 Istio 下发到数据面的 sidecar 代理上。Aeraki 还作为一个 RDS 服务器为数据面的 MetaProtocol Proxy 提供动态路由。Aeraki 提供的 RDS 和 Envoy 的 RDS 有所不同，Envoy RDS 主要为 HTTP 协议提供动态路由，而 Aeraki RDS 旨在为所有基于 MetaProtocol 框架开发的七层协议提供动态路由能力。 MetaProtocol Proxy：基于 Envoy 实现的一个通用七层协议代理。依托 Envoy 成熟的基础库，MetaProtocol Proxy 是在 Envoy 代码基础上的扩展。它为七层协议统一实现了服务发现、负载均衡、RDS 动态路由、流量镜像、故障注入、本地 / 全局限流等基础能力，大大降低了在 Envoy 上开发第三方协议的难度，只需要实现编解码的接口，就可以基于 MetaProtocol 快速开发一个第三方协议插件。  如果没有使用 MetaProtocol Proxy，要让 Envoy 识别一个七层协议，则需要编写一个完整的 TCP filter，这个 filter 需要实现路由、限流、遥测等能力，需要投入大量的人力。对于大部分的七层协议来说，需要的流量管理能力是类似的，因此没有必要在每个七层协议的 filter 实现中重复这部分工作。Aeraki 项目采用了一个 MetaProtocol Proxy 来统一实现这些能力，如下图所示：\n   MetaProtocol Proxy 架构图  基于 MetaProtocol Proxy，只需要实现编解码接口部分的代码就可以编写一个新的七层协议 Envoy Filter。除此之外，无需添加一行代码，Aeraki 就可以在控制面提供该七层协议的配置下发和 RDS 动态路由配置。\n   采用 MetaProtocol 编写 Envoy Filter 的对比  Aeraki + MetaProtocol 套件降低了在 Istio 中管理第三方协议的难度，将 Istio 扩展成为一个支持所有协议的全栈服务网格。目前 Aeraki 项目已经基于 MetaProtocol 实现了 Dubbo 和 Thrift 协议。相对 Envoy 自带的 Dubbo 和 Thrift Filter，基于 MetaProtocol 的 Dubbo 和 Thrift 实现功能更为强大，提供了 RDS 动态路由，可以在不中断存量链接的情况下对流量进行高级的路由管理，并且提供了非常灵活的 Metadata 路由机制，理论上可以采用协议数据包中携带的任意字段进行路由。QQ 音乐和央视频 APP 等业务也正在基于 Aeraki 和 MetaProtocol 进行开发，以将一些私有协议纳入到服务网格中进行管理。\n除此之外，Aeraki 中还提供了 xDS 配置下发优化的 lazyXDS 插件、Consul、etcd、Zookeeper 等各种第三方服务注册表对接适配，Istio 运维实战电子书等工具，旨在解决 Istio 在落地中遇到的各种实际问题，加速服务网格的成熟和产品化。\n参考  Aeraki GitHub - github.com  ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"e8117700864f4613fa52e1c8eb6fc9ed","permalink":"https://jimmysong.io/docs/istio-handbook/ecosystem/aeraki/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/istio-handbook/ecosystem/aeraki/","section":"istio-handbook","summary":"Aeraki 是腾讯云在 2021 年 3 月开源的一个服务网格领域的项目。Aerak","tags":null,"title":"Aeraki","type":"book"},{"authors":null,"categories":null,"content":"AuthorizationPolicy（授权策略）实现了对网格中工作负载的访问控制。\n授权策略支持访问控制的 CUSTOM、DENY 和 ALLOW 操作。当 CUSTOM、DENY 和 ALLOW 动作同时用于一个工作负载时，首先评估 CUSTOM 动作，然后是 DENY 动作，最后是 ALLOW 动作。评估是按以下顺序进行：\n 如果有任何 CUSTOM 策略与请求相匹配，如果评估结果为拒绝，则拒绝该请求。 如果有任何 DENY 策略与请求相匹配，则拒绝该请求。 如果没有适合该工作负载的 ALLOW 策略，允许该请求。 如果有任何 ALLOW 策略与该请求相匹配，允许该请求。 拒绝该请求。  Istio 授权策略还支持 AUDIT 动作，以决定是否记录请求。AUDIT 策略不影响请求是否被允许或拒绝到工作负载。请求将完全基于 CUSTOM、DENY 和 ALLOW 动作被允许或拒绝。\n如果工作负载上有一个与请求相匹配的 AUDIT 策略，则请求将被内部标记为应该被审计。必须配置并启用一个单独的插件，以实际履行审计决策并完成审计行为。如果没有启用这样的支持插件，该请求将不会被审计。目前，唯一支持的插件是 Stackdriver 插件。\n示例 下面是一个 Istio 授权策略的例子。\n它将 action 设置为 ALLOW 来创建一个允许策略。默认动作是 ALLOW，但在策略中明确规定是很有用的。\n它允许请求来自：\n 服务账户 cluster.local/ns/default/sa/sleep 或 命名空间 test  来访问以下工作负载：\n 在前缀为 /info 的路径上使用 GET 方法，或者 在路径 /data 上使用 POST 方法  且当请求具有由 https://accounts.google.com 发布的有效 JWT 令牌时。\n任何其他请求将被拒绝。\napiVersion:security.istio.io/v1beta1kind:AuthorizationPolicymetadata:name:httpbinnamespace:foospec:action:ALLOWrules:- from:- source:principals:[\u0026#34;cluster.local/ns/default/sa/sleep\u0026#34;]- source:namespaces:[\u0026#34;test\u0026#34;]to:- operation:methods:[\u0026#34;GET\u0026#34;]paths:[\u0026#34;/info*\u0026#34;]- operation:methods:[\u0026#34;POST\u0026#34;]paths:[\u0026#34;/data\u0026#34;]when:- key:request.auth.claims[iss]values:[\u0026#34;https://accounts.google.com\u0026#34;]下面是另一个例子，它将 action 设置为 DENY 以创建一个拒绝策略。它拒绝来自 dev 命名空间对 foo 命名空间中所有工作负载的 POST 请求。\napiVersion:security.istio.io/v1beta1kind:AuthorizationPolicymetadata:name:httpbinnamespace:foospec:action:DENYrules:- from:- source:namespaces:[\u0026#34;dev\u0026#34;]to:- operation:methods:[\u0026#34;POST\u0026#34;]下面的授权策略将 action 设置为 AUDIT。它将审核任何对前缀为 /user/profile 的路径的 GET 请求。\napiVersion:security.istio.io/v1beta1kind:AuthorizationPolicymetadata:namespace:ns1name:anynamespec:selector:matchLabels:app:myapiaction:AUDITrules:- to:- operation:methods:[\u0026#34;GET\u0026#34;]paths:[\u0026#34;/user/profile/*\u0026#34;]授权策略的范围（目标）由 metadata/namespace 和一个可选的 selector 决定。\n metadata/namespace 告诉策略适用于哪个命名空间。如果设置为根命名空间，该策略适用于网格中的所有命名空间。 工作负载 selector 可以用来进一步限制策略的适用范围。  例如，以下授权策略适用于 bar 命名空间中包含标签 app: httpbin 的工作负载。\napiVersion:security.istio.io/v1beta1kind:AuthorizationPolicymetadata:name:policynamespace:barspec:selector:matchLabels:app:httpbin以下授权策略适用于命名空间 foo 中的所有工作负载。\napiVersion:security.istio.io/v1beta1kind:AuthorizationPolicymetadata:name:policynamespace:foospec:{}以下授权策略适用于网格中所有命名空间中包含标签 version: v1 的工作负载（假设根命名空间被配置为 istio-config）。\napiVersion:security.istio.io/v1beta1kind:AuthorizationPolicymetadata:name:policynamespace:istio-configspec:selector:matchLabels:version:v1 关于 AuthorizationPolicy 配置的详细用法请参考 Istio 官方文档。 关于认证策略条件的详细配置请参考 Istio 官方文档。  参考  Authorization Policy - istio.io Authorization Policy Conditions - istio.io  ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"7291da1aaffd2e47faa1e9526c52611d","permalink":"https://jimmysong.io/docs/istio-handbook/config-security/authorization-policy/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/istio-handbook/config-security/authorization-policy/","section":"istio-handbook","summary":"AuthorizationPolicy（授权策略）实现了对网","tags":null,"title":"AuthorizationPolicy","type":"book"},{"authors":null,"categories":null,"content":"为了排除 Istio 的问题，对 Envoy 的工作原理有一个基本的了解是很有帮助的。Envoy 配置是一个 JSON 文件，分为多个部分。我们需要了解 Envoy 的基本概念是监听器、路由、集群和端点。\n这些概念映射到 Istio 和 Kubernetes 资源，如下图所示。\n   Envoy 概念与 Istio 和 Kubernetes 的映射  监听器是命名的网络位置，通常是一个 IP 和端口。Envoy 对这些位置进行监听，这是它接收连接和请求的地方。\n每个 sidecar 都有多个监听器生成。每个 sidecar 都有一个监听器，它被绑定到 0.0.0.0:15006。这是 IP Tables 将所有入站流量发送到 Pod 的地址。第二个监听器被绑定到 0.0.0.0:15001，这是所有从 Pod 中出站的流量地址。\n当一个请求被重定向（使用 IP Tables 配置）到 15001 端口时，监听器会把它交给与请求的原始目的地最匹配的虚拟监听器。如果它找不到目的地，它就根据配置的 OutboundTrafficPolicy 来发送流量。默认情况下，请求被发送到 PassthroughCluster，该集群连接到应用程序选择的目的地，Envoy 没有进行任何负载均衡。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"576d7ea8d8d5209b418f983142350ea7","permalink":"https://jimmysong.io/docs/istio-handbook/troubleshooting/envoy-basic/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/istio-handbook/troubleshooting/envoy-basic/","section":"istio-handbook","summary":"为了排除 Istio 的问题，对 Envoy 的工作原理有一个基本的了解是很有帮助的","tags":null,"title":"Envoy 基础问题","type":"book"},{"authors":null,"categories":null,"content":"在学习一门新的技术之前，有必要先了解它的基本语境和术语，这样在阅读后续的章节时，可以保证我们的思想一致。在这一节我将为你介绍 Envoy 中常用的术语。\nEnvoy 中常用术语有：\n 主机（Host）：能够进行网络通信的实体（在手机或服务器等上的应用程序）。在 Envoy 中主机是指逻辑网络应用程序。只要每台主机都可以独立寻址，一块物理硬件上就运行多个主机。 下游（Downstream）：下游主机连接到 Envoy，主动向 Envoy 发送请求并期待获得响应。 上游（Upstream）：上游主机收到来自 Envoy 的连接请求同时相应该请求。 集群（Cluster）: 集群是 Envoy 连接到的一组逻辑上相似的上游主机。Envoy 通过服务发现发现集群中的成员。Envoy 可以通过主动运行状况检查来确定集群成员的健康状况。Envoy 如何将请求路由到集群成员由负载均衡策略确定。 网格（Mesh）：网格是一组互相协调以提供一致网络拓扑的主机。Envoy 网格是指一组 Envoy 代理，它们构成了由多种不同服务和应用程序平台组成的分布式系统的消息传递基础。 运行时配置：与 Envoy 一起部署的带外实时配置系统。可以在无需重启 Envoy 或 更改 Envoy 主配置的情况下，通过更改设置来影响操作。 监听器（Listener）: 监听器（listener）是可以由下游客户端连接的命名网络位置（例如，端口、unix 域套接字等）。Envoy 公开一个或多个下游主机连接的侦听器。一般是每台主机运行一个 Envoy，使用单进程运行，但是每个进程中可以启动任意数量的 Listener（监听器），目前只监听 TCP，每个监听器都独立配置一定数量的（L3/L4）网络过滤器。Listenter 也可以通过 Listener Discovery Service（LDS）动态获取。 监听器过滤器（Listener filter）：Listener 使用 listener filter（监听器过滤器）来操作链接的元数据。它的作用是在不更改 Envoy 的核心功能的情况下添加更多的集成功能。Listener filter 的 API 相对简单，因为这些过滤器最终是在新接受的套接字上运行。在链中可以互相衔接以支持更复杂的场景，例如调用速率限制。Envoy 已经包含了多个监听器过滤器。 HTTP 路由表（HTTP Route Table）：HTTP 的路由规则，例如请求的域名，Path 符合什么规则，转发给哪个 Cluster。 健康检查（Health checking）：健康检查会与 SDS 服务发现配合使用。但是，即使使用其他服务发现方式，也有相应需要进行主动健康检查的情况。  ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"86e3d15e79e619ca91d9f09c46523393","permalink":"https://jimmysong.io/docs/istio-handbook/data-plane/envoy-terminology/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/istio-handbook/data-plane/envoy-terminology/","section":"istio-handbook","summary":"在学习一门新的技术之前，有必要先了解它的基本语境和术语，这样","tags":null,"title":"Envoy 中的基本术语","type":"book"},{"authors":null,"categories":null,"content":"Gateway 描述了一个在网格边缘运行的负载均衡器，接收传入或传出的 HTTP/TCP 连接。该规范描述了一组应该暴露的端口、要使用的协议类型、负载均衡器的 SNI 配置等。\n示例 例如，下面的 Gateway 配置设置了一个代理，作为负载均衡器，暴露了 80 和 9080 端口（http）、443（https）、9443（https）和 2379 端口（TCP）的入口（ingress）。网关将被应用于运行在标签为 app: my-gateway-controller 的 pod 上的代理。虽然 Istio 将配置代理来监听这些端口，但用户有责任确保这些端口的外部流量被允许进入网格。\napiVersion:networking.istio.io/v1alpha3kind:Gatewaymetadata:name:my-gatewaynamespace:some-config-namespacespec:selector:app:my-gateway-controllerservers:- port:number:80name:httpprotocol:HTTPhosts:- uk.bookinfo.com- eu.bookinfo.comtls:httpsRedirect:true# 对 HTTP 请求发送 301 重定向- port:number:443name:https-443protocol:HTTPShosts:- uk.bookinfo.com- eu.bookinfo.comtls:mode:SIMPLE# 在此端口上启用 HTTPSserverCertificate:/etc/certs/servercert.pemprivateKey:/etc/certs/privatekey.pem- port:number:9443name:https-9443protocol:HTTPShosts:- \u0026#34;bookinfo-namespace/*.bookinfo.com\u0026#34;tls:mode:SIMPLE# 在此端口上启用 HTTPScredentialName:bookinfo-secret# 从 Kubernetes secret 中获取证书- port:number:9080name:http-wildcardprotocol:HTTPhosts:- \u0026#34;*\u0026#34;- port:number:2379# 通过此端口暴露内部服务name:mongoprotocol:MONGOhosts:- \u0026#34;*\u0026#34;上面的网关规范描述了负载均衡器的 L4 到 L6 属性。然后，VirtualService 可以被绑定到 Gateway 上，以控制到达特定主机或网关端口的流量的转发。\n例如，下面的 VirtualService 将 https://uk.bookinfo.com/reviews、https://eu.bookinfo.com/reviews、 http://uk.bookinfo.com:9080/reviews、http://eu.bookinfo.com:9080/reviews 的流量分成两个版本（prod 和 qa）的内部 reviews 服务，这两个版本分别运行在 prod 和 qa 命名空间中，端口为 9080。此外，包含 cookie user: dev-123 的请求将被发送到 qa 版本的特殊端口 7777。以上规则也适用于网格内部对 reviews.prod.svc.cluster.local 服务的请求。prod 版本获得 80% 的流量，qa 版本获得 20% 的流量。\napiVersion:networking.istio.io/v1alpha3kind:VirtualServicemetadata:name:bookinfo-rulenamespace:bookinfo-namespacespec:hosts:- reviews.prod.svc.cluster.local- uk.bookinfo.com- eu.bookinfo.comgateways:- some-config-namespace/my-gateway- mesh# 应用到网格中所有的 sidecarhttp:- match:- headers:cookie:exact:\u0026#34;user=dev-123\u0026#34;route:- destination:port:number:7777host:reviews.qa.svc.cluster.local- match:- uri:prefix:/reviews/route:- destination:port:number:9080# 如果它是 reviews 的唯一端口，则可以省略。host:reviews.prod.svc.cluster.localweight:80- destination:host:reviews.qa.svc.cluster.localweight:20下面的 VirtualService 将到达（外部）27017 端口的流量转发到 5555 端口的内部 Mongo 服务器。这个规则在网格内部不适用，因为网关列表中省略了保留名称 mesh。\napiVersion:networking.istio.io/v1alpha3kind:VirtualServicemetadata:name:bookinfo-mongonamespace:bookinfo-namespacespec:hosts:- mongosvr.prod.svc.cluster.local# 内部 Mongo 服务的名字gateways:- some-config-namespace/my-gateway# 如果 Gateway 与 VirtualService 处于同一命名空间，可以省略命名空间。tcp:- match:- port:27017route:- destination:host:mongo.prod.svc.cluster.localport:number:5555可以在 hosts 字段中命名空间/主机名语法来限制可以绑定到网关服务器的虚拟服务集。例如，下面的 Gateway 允许 ns1 命名空间中的任何 VirtualService 与之绑定，而只限制 ns2 命名空间中的 foo.bar.com 主机的 VirtualService 与之绑定。\napiVersion:networking.istio.io/v1alpha3kind:Gatewaymetadata:name:my-gatewaynamespace:some-config-namespacespec:selector:app:my-gateway-controllerservers:- port:number:80name:httpprotocol:HTTPhosts:- \u0026#34;ns1/*\u0026#34;- \u0026#34;ns2/foo.bar.com\u0026#34;配置项 下图是 Gateway 资源的配置拓扑图。\n  Gateway 资源配置拓扑图  Gateway 资源的顶级配置项目如下：\n selector：一个或多个标签，表明应在其上应用该网关配置的一组特定的 pod/VM。默认情况下，工作负载是根据标签选择器在所有命名空间中搜索的。这意味着命名空间 “foo” 中的网关资源可以根据标签选择命名空间 “bar” 中的 pod。这种行为可以通过 istiod 中的 PILOT_SCOPE_GATEWAY_TO_NAMESPACE 环境变量控制。如果这个变量被设置为 “true”，标签搜索的范围将被限制在资源所在的配置命名空间。换句话说，Gateway 资源必须驻留在与 Gateway 工作负载实例相同的命名空间中。如果选择器为nil，Gateway 将被应用于所有工作负载。 servers：描述了特定负载均衡器端口上的代理的属性。  关于 Gateway 配置的详细用法请参考 Istio 官方文档。\n参考  Gateway - istio.io  ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"b5bd0b758ec9b2a7e5422956fff568f0","permalink":"https://jimmysong.io/docs/istio-handbook/config-networking/gateway/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/istio-handbook/config-networking/gateway/","section":"istio-handbook","summary":"Gateway 描述了一个在网格边缘运行的负载均衡器，接收传入或传出的 HTTP/TCP 连","tags":null,"title":"Gateway","type":"book"},{"authors":null,"categories":null,"content":"Istio 1.11 引入了 Telemetry API，并在 1.13 中进行了完善。使用该 API，你可以一站式的灵活地配置指标、访问日志和追踪。\n使用 API 下面将向你介绍如何使用 Telemetry API。\n范围、继承和重写 Telemetry API 资源从 Istio 配置中继承的顺序：\n 根配置命名空间（例如：istio-system） 本地命名空间（命名空间范围内的资源，没有工作负载 selector） 工作负载（具有工作负载 selector 的命名空间范围的资源）  根配置命名空间（通常是 istio-system）中的 Telemetry API 资源提供网格范围内的默认行为。根配置命名空间中的任何特定工作负载选择器将被忽略 / 拒绝。在根配置命名空间中定义多个网格范围的 Telemetry API 资源是无效的。\n要想在某个特定的命名空间内覆盖网格范围的配置，可以通过在该命名空间中应用新的 Telemetry 资源（无需工作负载选择器）来实现。命名空间配置中指定的任何字段将完全覆盖父配置（根配置命名空间）中的字段。\n要想覆盖特定工作负载的遥测配置，可以在其命名空间中应用带有工作负载选择器的新的 Telemetry 资源来实现。\n工作负载选择 命名空间内的单个工作负载是通过 selector 选择的，该选择器允许基于标签选择工作负载。\n让两个不同的 Telemetry 资源使用 selector 选择同一个工作负载是无效的。同样，在一个命名空间中有两个不同的 Telemetry 资源而没有指定 selector 也是无效的。\n提供者选择 Telemetry API 使用提供者（Provider）的概念来表示要使用的协议或集成类型。提供者可以在 MeshConfig 中进行配置。\n下面是一个 MeshConfig 中配置提供者的例子。\ndata:mesh:|-extensionProviders: # The following content defines two example tracing providers. - name: \u0026#34;localtrace\u0026#34; zipkin: service: \u0026#34;zipkin.istio-system.svc.cluster.local\u0026#34; port: 9411 maxTagLength: 56 - name: \u0026#34;cloudtrace\u0026#34; stackdriver: maxTagLength: 256为方便起见，Istio 在开箱时就配置了一些默认设置的提供者。\n   提供者名称 功能     prometheus 指标   stackdriver 指标、追踪、日志记录   envoy 日志记录    此外，还可以设置一个默认的提供者，当 Telemetry 资源没有指定提供者时使用。\n示例 Telemetry API 资源继承自网格的根配置命名空间，通常是 istio-system。要配置整个网格的行为，在根配置命名空间中添加一个新的（或编辑现有的）Telemetry 资源。\n下面是一个使用上一节中提供者配置的例子。\napiVersion:telemetry.istio.io/v1alpha1kind:Telemetrymetadata:name:mesh-defaultnamespace:istio-systemspec:tracing:- providers:- name:localtracecustomTags:foo:literal:value:barrandomSamplingPercentage:100这个配置覆盖了 MeshConfig 的默认提供者，将 Mesh 默认设置为 localtrace 提供者。它还设置了 Mesh 范围内的采样百分比为 100，并配置了一个标签，将其添加到所有追踪跨度中，名称为 foo，值为 bar。\n配置命名空间范围内的跟踪行为 要为单个命名空间定制行为，请为所需的命名空间添加 Telemetry 资源。在命名空间资源中指定的任何字段将完全覆盖从配置层次中继承的字段配置。\n例如：\napiVersion:telemetry.istio.io/v1alpha1kind:Telemetrymetadata:name:namespace-overridenamespace:myappspec:tracing:- customTags:userId:header:name:userIddefaultValue:unknown当部署到一个具有先前 Mesh 范围范例配置的 Mesh 中时，这将导致 myapp 命名空间中的追踪行为，将追踪跨度发送到 localtrace 提供者，并以 100% 的速率随机选择请求进行追踪，但为每个跨度设置自定义标签，名称为 userId，其值取自 userId 请求头。重要的是，来自父配置的 foo: bar 标签将不会在 myapp 命名空间中使用。自定义标签的行为完全覆盖了 mesh-default.istio-system 资源中配置的行为。\n注意：Telemetry 资源中的任何配置都会完全覆盖配置层次中其父资源的配置。这包括提供者的选择。\n配置针对工作负载的行为 要为个别工作负载定制行为，请将遥测资源添加到所需的命名空间并使用 selector。工作负载特定资源中指定的任何字段将完全覆盖配置层次中的继承字段配置。\n例如：\napiVersion:telemetry.istio.io/v1alpha1kind:Telemetrymetadata:name:workload-overridenamespace:myappspec:selector:matchLabels:service.istio.io/canonical-name:frontendtracing:- disableSpanReporting:true在这个例子中，对于 myapp 命名空间中的 frontend 工作负载，追踪功能将被禁用。Istio 仍将转发追踪头信息，但不会向配置的追踪提供者报告任何跨度。\n注意：让两个带有工作负载选择器的 Telemetry 资源选择相同的工作负载是无效的。此时，行为是未定义的。\n参考  Telemetry API - istio.io Telemetry configuration - istio.io  ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"3f50c549d740ac530741cba62ceac7de","permalink":"https://jimmysong.io/docs/istio-handbook/observability/telemetry-api/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/istio-handbook/observability/telemetry-api/","section":"istio-handbook","summary":"Istio 1.11 引入了 Telemetry API，并在 1.13 中进行了完善。使用该 API，你可以","tags":null,"title":"Telemetry API","type":"book"},{"authors":null,"categories":null,"content":"要安装 Istio，我们需要一个运行中的 Kubernetes 集群实例。所有的云供应商都有一个管理的 Kubernetes 集群提供，我们可以用它来安装 Istio 服务网格。\n我们也可以在你的电脑上使用以下平台之一在本地运行一个 Kubernetes集群。\n要安装 Istio，我们需要一个运行中的 Kubernetes 集群实例。所有的云供应商都有一个管理的 Kubernetes 集群提供，我们可以用它来安装 Istio 服务网格。\n我们也可以在你的电脑上使用以下平台之一在本地运行一个 Kubernetes 集群。\n Minikube Docker Desktop kind MicroK8s  当使用本地 Kubernetes 集群时，确保你的计算机满足 Istio 安装的最低要求（如 16384MB 内存和 4 个 CPU）。另外，确保 Kubernetes 集群的版本是 v1.19.0 或更高。\nMinikube 在这次培训中，我们将使用 Minikube 的 Hypervisor。Hypervisor 的选择将取决于你的操作系统。要安装 Minikube 和 Hypervisor，你可以按照安装说明进行。\n安装了 Minikube 后，我们可以创建并启动 Kubernetes 集群。下面的命令使用 VirtualBox 管理程序启动一个 Minikube 集群。\nminikube start --memory=16384 --cpus=4 --driver=virtualbox\n请确保用你所使用的 Hypervisor 的名字替换 -driver=virtualbox。关于可用的选项，见下表。\n   标志名称 更多信息     hyperkit HyperKit   hyperv Hyper-V   kvm2 KVM   docker Docker   podman Podman   parallels Parallels   virtualbox VirtualBox   vmware VMware Fusion    为了检查集群是否正在运行，我们可以使用 Kubernetes CLI，运行 kubectl get nodes 命令。\n$ kubectl get nodes NAME STATUS ROLES AGE VERSION minikube Ready master 151m v1.19.0  注意：如果你使用 Brew 软件包管理器安装了 Minikube，你也安装了 Kubernetes CLI。\n Kubernetes CLI 如果你需要安装 Kubernetes CLI，请遵循这些说明。\n我们可以运行 kubectl version 来检查 CLI 是否已经安装。你应该看到与此类似的输出。\n$ kubectl version Client Version: version.Info{Major:\u0026#34;1\u0026#34;, Minor:\u0026#34;19\u0026#34;, GitVersion:\u0026#34;v1.19.2\u0026#34;, GitCommit:\u0026#34;f5743093fd1c663cb0cbc89748f730662345d44d\u0026#34;, GitTreeState:\u0026#34;clean\u0026#34;, BuildDate:\u0026#34;2020-09-16T21:51:49Z\u0026#34;, GoVersion:\u0026#34;go1.15.2\u0026#34;, Compiler:\u0026#34;gc\u0026#34;, Platform:\u0026#34;darwin/amd64\u0026#34;} Server Version: version.Info{Major:\u0026#34;1\u0026#34;, Minor:\u0026#34;19\u0026#34;, GitVersion:\u0026#34;v1.19.0\u0026#34;, GitCommit:\u0026#34;e19964183377d0ec2052d1f1fa930c4d7575bd50\u0026#34;, GitTreeState:\u0026#34;clean\u0026#34;, BuildDate:\u0026#34;2020-08-26T14:23:04Z\u0026#34;, GoVersion:\u0026#34;go1.15\u0026#34;, Compiler:\u0026#34;gc\u0026#34;, Platform:\u0026#34;linux/amd64\u0026#34;} 下载 Istio 在本课程中，我们将使用 Istio 1.10.3。安装 Istio 的第一步是下载 Istio CLI（istioctl）、安装清单、示例和工具。\n安装最新版本的最简单方法是使用 downloadIstio 脚本。打开一个终端窗口，打开你要下载 Istio 的文件夹，然后运行下载脚本。\n$ curl -L https://istio.io/downloadIstio | ISTIO_VERSION=1.10.3 sh - Istio 发行版被下载并解压到名为 istio-1.9.0 的文件夹中。为了访问 istioctl，我们应该把它添加到 path 中。\n$ cd istio-1.10.3 $ export PATH=$PWD/bin:$PATH 要检查 istioctl 是否在 path 里，运行 istioctl version。你应该看到这样的输出。\n$ istioctl version no running Istio pods in \u0026#34;istio-system\u0026#34; 1.10.3 安装 Istio Istio 支持多个配置文件（profile）。配置文件之间的区别在于所安装的组件。\n$ istioctl profile list Istio configuration profiles: default demo empty minimal preview remote 推荐用于生产部署的配置文件是 default 配置文件。我们将安装 demo 配置文件，因为它包含所有的核心组件，启用了跟踪和日志记录，并且是为了学习不同的 Istio 功能。\n我们也可以从 minimal 的组件开始，以后单独安装其他功能，如 ingress 和 egress 网关。\n因为我们将使用 Istio 操作员进行安装，所以我们必须先部署 Operator。\n要部署 Istio Operator，请运行：\n$ istioctl operator init Using operator Deployment image: docker.io/istio/operator:1.9.0 ✔ Istio operator installed ✔ Installation complete init 命令创建了 istio-operator 命名空间，并部署了 CRD、Operator Deployment 以及 operator 工作所需的其他资源。安装完成后，Operator 就可以使用了。\n要安装 Istio，我们必须创建 IstioOperator 资源，并指定我们要使用的配置配置文件。\n创建一个名为 demo-profile.yaml 的文件，内容如下：\napiVersion:v1kind:Namespacemetadata:name:istio-system---apiVersion:install.istio.io/v1alpha1kind:IstioOperatormetadata:namespace:istio-systemname:demo-istio-installspec:profile:demo我们还在文件中添加了命名空间资源，以创建 istio-system 命名空间。\n我们需要做的最后一件事是创建资源：\n$ kubectl apply -f demo-profile.yaml namespace/istio-system created istiooperator.install.istio.io/demo-istio-install created 一旦 Operator 检测到 IstioOperator 资源，它将开始安装 Istio。整个过程可能需要5分钟左右。\n为了检查安装的状态，我们可以看看 istio-system 命名空间中的Pod的状态。\n$ kubectl get po -n istio-system NAME READY STATUS RESTARTS AGE istio-egressgateway-6db9994577-sn95p 1/1 Running 0 79s istio-ingressgateway-58649bfdf4-cs4fk 1/1 Running 0 79s istiod-dd4b7db5-nxrjv 1/1 Running 0 111s 当所有的 Pod 都在运行时，Operator 已经完成了 Istio 的安装。\n启用 sidecar 注入 正如我们在上一节中所了解的，服务网格需要与每个应用程序一起运行的 sidecar 代理。\n要将 sidecar 代理注入到现有的 Kubernetes 部署中，我们可以使用 istioctl 命令中的 kube-inject 动作。\n然而，我们也可以在任何 Kubernetes 命名空间上启用自动 sidecar 注入。如果我们用 istio-injection=enabled 标记命名空间，Istio 会自动为我们在该命名空间中创建的任何 Kubernetes Pod 注入 sidecar。\n让我们通过添加标签来启用默认命名空间的自动 sidecar 注入。\n$ kubectl label namespace default istio-injection=enabled namespace/default labeled 要检查命名空间是否被标记，请运行下面的命令。default 命名空间应该是唯一一个启用了该值的命名空间。\n$ kubectl get namespace -L istio-injection NAME STATUS AGE ISTIO-INJECTION default Active 32m enabled istio-operator Active 27m disabled istio-system Active 15m kube-node-lease Active 32m kube-public Active 32m kube-system Active 32m 现在我们可以尝试在 default 命名空间创建一个 Deployment，并观察注入的代理。我们将创建一个名为 my-nginx 的 Deployment，使用 nginx 镜像的单一容器。\n$ kubectl create deploy my-nginx --image=nginx deployment.apps/my-nginx created 如果我们看一下 Pod，你会发现 Pod 里有两个容器。\n$ kubectl get po NAME READY STATUS RESTARTS AGE my-nginx-6b74b79f57-hmvj8 2/2 Running 0 62s 同样地，描述 Pod 显示 Kubernetes 同时创建了一个 nginx 容器和一个 istio-proxy 容器：\n$ kubectl describe po my-nginx-6b74b79f57-hmvj8 Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 118s default-scheduler Successfully assigned default/my-nginx-6b74b79f57-hmvj8 to minikube Normal Pulling 117s kubelet Pulling image \u0026#34;docker.io/istio/proxyv2:1.9.0\u0026#34; Normal Pulled 116s kubelet Successfully pulled image \u0026#34;docker.io/istio/proxyv2:1.9.0\u0026#34; in 1.102544635s Normal …","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"205763e0a8fa575b5c858e3bfb4f492a","permalink":"https://jimmysong.io/docs/istio-handbook/setup/istio-installation/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/istio-handbook/setup/istio-installation/","section":"istio-handbook","summary":"要安装 Istio，我们需要一个运行中的 Kubernetes 集群实例。所有的云供","tags":null,"title":"安装 Istio","type":"book"},{"authors":null,"categories":null,"content":"我们将使用谷歌云平台来托管 Kubernetes 集群。打开并登录到你的 GCP 控制台账户，并按照以下步骤创建一个 Kubernetes 集群。\n 从导航中，选择 Kubernetes Engine。 点击创建集群。 将集群命名为 boutique-demo。 选择区域选项，选择最接近你的位置的区域。 点击 default-pool，在节点数中输入 5。 点击 Nodes（节点）。 点击机器配置机器类型下拉，选择e2-medium (2 vCPU, 4 GB memory)。 点击 “Create\u0026#34;来创建集群。  集群的创建将需要几分钟的时间。一旦安装完成，集群将显示在列表中，如下图所示。\n   部署在 GCP 中的 Kubernetes 集群   你也可以将 Online Boutique 应用程序部署到托管在其他云平台上的 Kubernetes 集群，如 Azure 或 AWS。\n 访问集群 我们有两种方式来访问集群。我们可以从浏览器中使用 Cloud Shell。要做到这一点，点击集群旁边的 Connect，然后点击 Run in Cloud Shell 按钮。点击该按钮可以打开 Cloud Shell，并配置 Kubernetes CLI 来访问该集群。\n第二个选择是在你的电脑上安装 gcloud CLI，然后从你的电脑上运行相同的命令。\n安装 Istio 我们将使用 GetMesh CLI 在集群中安装 Istio 1.10.3。\n1. 下载 GetMesh CLI\ncurl -sL https://istio.tetratelabs.io/getmesh/install.sh | bash 2. 安装 Istio\ngetmesh istioctl install --set profile=demo 安装完成后，给默认命名空间设置上 istio-injection=enabled 标签：\nkubectl label namespace default istio-injection=enabled ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"624a10645646f9a947d7b22b59127154","permalink":"https://jimmysong.io/docs/istio-handbook/practice/create-cluster/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/istio-handbook/practice/create-cluster/","section":"istio-handbook","summary":"我们将使用谷歌云平台来托管 Kubernetes 集群。打开并登录到你的 GCP 控制台账","tags":null,"title":"创建集群","type":"book"},{"authors":null,"categories":null,"content":"Envoy 的强大功能之一是支持动态配置。到现在为止，我们一直在使用静态配置。我们使用 static_resources 字段将监听器、集群、路由和其他资源指定为静态资源。\n当使用动态配置时，我们不需要重新启动 Envoy 进程就可以生效。相反，Envoy 通过从磁盘或网络上的文件读取配置，动态地重新加载配置。动态配置使用所谓的发现服务 API，指向配置的特定部分。这些 API 也被统称为 xDS。当使用 xDS 时，Envoy 调用外部基于 gRPC/REST 的配置供应商，这些供应商实现了发现服务 API 来检索配置。\n外部基于 gRPC/REST 的配置提供者也被称为控制平面。当使用磁盘上的文件时，我们不需要控制平面。Envoy 提供了控制平面的 Golang 实现，但是 Java 和其他控制平面的实现也可以使用。\nEnvoy 内部有多个发现服务 API。所有这些在下表中都有描述。\n   发现服务名称 描述     监听器发现服务（LDS） 使用 LDS，Envoy 可以在运行时发现监听器，包括所有的过滤器栈、HTTP 过滤器和对 RDS 的引用。   扩展配置发现服务（ECDS） 使用 ECDS，Envoy 可以独立于监听器获取扩展配置（例如，HTTP 过滤器配置）。   路由发现服务（RDS） 使用 RDS，Envoy 可以在运行时发现 HTTP 连接管理器过滤器的整个路由配置。与 EDS 和 CDS 相结合，我们可以实现复杂的路由拓扑结构。   虚拟主机发现服务（VHDS） 使用 VHDS 允许 Envoy 从路由配置中单独请求虚拟主机。当路由配置中有大量的虚拟主机时，就可以使用这个功能。   宽泛路由发现服务（SRDS） 使用 SRDS，我们可以把路由表分解成多个部分。当我们有大的路由表时，就可以使用这个 API。   集群发现服务（CDS） 使用 CDS，Envoy 可以发现上游集群。Envoy 将通过排空和重新连接所有现有的连接池来优雅地添加、更新或删除集群。Envoy 在初始化时不必知道所有的集群，因为我们可以在以后使用 CDS 配置它们。   端点发现服务（EDS） 使用 EDS，Envoy 可以发现上游集群的成员。   秘密发现服务（SDS） 使用 SDS，Envoy 可以为其监听器发现秘密（证书和私钥，TLS 会话密钥），并为对等的证书验证逻辑进行配置。   运行时发现服务（RTDS） 使用 RTDS，Envoy 可以动态地发现运行时层。    聚合发现服务（ADS）\n表中的发现服务是独立的，有不同的 gRPC/REST 服务名称。使用聚合发现服务（ADS），我们可以使用一个单一的 gRPC 服务，在一个 gRPC 流中支持所有的资源类型（监听器、路由、集群…）。ADS 还能确保不同资源的更新顺序正确。请注意，ADS 只支持 gRPC。如果没有 ADS，我们就需要协调其他 gRPC 流来实现正确的更新顺序。\n增量 gRPC xDS\n每次我们发送资源更新时，我们必须包括所有的资源。例如，每次 RDS 更新必须包含每条路由。如果我们不包括一个路由，Envoy 会认为该路由已被删除。这样做更新会导致很高的带宽和计算成本，特别是当有大量的资源在网络上被发送时。Envoy 支持 xDS 的 delta 变体，我们可以只包括我们想添加 / 删除 / 更新的资源，以改善这种情况。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"331d68452dca2dd5853482b77845ac12","permalink":"https://jimmysong.io/docs/envoy-handbook/dynamic-config/dynamic-configuration/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/envoy-handbook/dynamic-config/dynamic-configuration/","section":"envoy-handbook","summary":"Envoy 的强大功能之一是支持动态配置。到现在为止，我们一直在使用静","tags":null,"title":"动态配置简介","type":"book"},{"authors":null,"categories":null,"content":"多集群部署（两个或更多的集群）为我们提供了更大程度的隔离和可用性，但我们付出的代价是增加了复杂性。如果场景要求高可用性（HA），我们将不得不在多个区域和地区部署集群。\n我们需要做出的下一个决定是，决定我们是否要在一个网络内运行集群，或者是否要使用多个网络。\n下图显示了一个多集群方案（集群 A、B 和 C），跨两个网络部署。\n   多集群服务网格  网络部署模式 当涉及到多个网络时，集群内部运行的工作负载必须使用 Istio 网关才能到达其他集群的工作负载。使用多个网络可以实现更好的容错和网络地址的扩展。\n   多网络服务网格  控制平面部署模型 Istio 服务网格使用控制平面来配置网格内工作负载之间的所有通信。工作负载所连接的控制平面取决于其配置。\n在最简单的情况下，我们有一个服务网格，在一个集群中只有一个控制平面。这就是我们在本课程中一直使用的配置。\n共享控制平面模型涉及多个集群，控制平面只在一个集群中运行。该集群被称为主集群，而部署中的其他集群被称为远程集群。这些集群没有自己的控制平面，相反，它们从主集群共享控制平面。\n   共享的控制平面  另一种部署模式是，我们把所有的集群都视为由外部控制平面控制的远程集群。这使我们在控制平面和数据平面之间有了完全的分离。一个典型的外部控制平面的例子是当一个云供应商在管理它。\n为了实现高可用性，我们应该在多个集群、区域或地区部署多个控制平面实例，如下图所示。\n   多个控制平面  这种模式提供了更好的可用性和配置隔离。如果其中一个控制平面变得不可用，那么停电就只限于这一个控制平面。为了改善这一点，你可以实施故障转移，并配置工作负载实例，在发生故障时连接到另一个控制平面。\n为了达到最高的可用性，我们可以在每个集群内部署一个控制平面。\n网格部署模型 到目前为止，我们所看的所有图表和场景都是使用单一的网格。在单网格模型中，所有的服务都在一个网格中，不管它们跨越多少集群和网络。\n将多个网格联合起来的部署模型被称为多网格部署。在这种模式下，服务可以跨网格边界进行通信。该模型为我们提供了一个更清晰的组织边界，更强的隔离性，并允许我们重复使用服务名称和命名空间。\n当联合两个网格时，每个网格可以暴露一组服务和身份，所有参与的网格都可以识别这些身份。为了实现跨网格的服务通信，我们必须在两个网格之间实现信任。信任可以通过向网格导入信任包和为这些身份配置本地策略来建立。\n租户模式 租户是一组共享工作负载的共同访问权和权限的用户。租户之间的隔离是通过网络配置和策略完成的。Istio 支持命名空间和集群租户。请注意，我们在这里谈论的租户是软多租户，而不是硬租户。当多个租户共享同一个 Istio 控制平面时，没有保证对诸如噪音邻居问题的保护。\n在一个网格中，Istio 使用命名空间作为租户的单位。如果使用 Kubernetes，我们可以为每个命名空间的工作负载部署授予权限。默认情况下，来自不同命名空间的服务可以通过完全限定名相互通信。\n在安全模块中，我们已经学会了如何使用授权策略来提高隔离度，并限制只对适当的调用者进行访问。\n在多集群部署模型中，每个集群中共享相同名称的命名空间被认为是同一个命名空间。集群 A 中 default 命名空间的 Customers 服务与集群 B 中 default 命名空间中的 Customers 服务指的是同一个服务。当流量被发送到 Customers 服务时，负载均衡在两个服务的合并端点上进行，如下图所示。\n   多集群共享命名空间  为了在 Istio 中配置集群租约，我们需要将每个集群配置为一个独立的服务网格。网格可以由不同的团队控制和操作，我们可以将网格连接到一起，形成一个多网格部署。如果我们使用与之前相同的例子，在集群 A 的 default 命名空间中运行的服务 Customers 与集群 B 的 default 命名空间中的服务 Customers 所指的不是同一个服务。\n租户的另一个重要功能是隔离不同租户的配置。目前，Istio 并没有解决这个问题，不过，它通过在命名空间级别上的范围配置来尝试解决这个问题。\n最佳多集群部署 最佳的多集群部署拓扑结构是每个集群都有自己的控制平面。对于正常的服务网格部署规模，建议你使用多网格部署，并有一个单独的系统在外部协调网格。一般建议总是在集群间使用入口网关，即使它们在同一个网络中。直接的 pod 到 pod 的连接需要在多个集群之间填充终端数据，这可能会使事情变得缓慢和复杂。一个更简单的解决方案是让流量通过跨集群的入口来流动。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"878bd4d93cf2b01daa1cc316e6e4d967","permalink":"https://jimmysong.io/docs/istio-handbook/advanced/multicluster-deployment/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/istio-handbook/advanced/multicluster-deployment/","section":"istio-handbook","summary":"多集群部署（两个或更多的集群）为我们提供了更大程度的隔离和可","tags":null,"title":"多集群部署","type":"book"},{"authors":null,"categories":null,"content":"本节将为你讲解 Envoy 中的访问日志配置。\n什么是访问日志？ 每当你打开浏览器访问谷歌或其他网站时，另一边的服务器就会收集你的访问信息。具体来说，它在收集和储存你从服务器上请求的网页数据。在大多数情况下，这些数据包括来源（即主机信息）、请求网页的日期和时间、请求属性（方法、路径、Header、正文等）、服务器返回的状态、请求的大小等等。所有这些数据通常被存储在称为访问日志（access log） 的文本文件中。\n通常，来自网络服务器或代理的访问日志条目遵循标准化的通用日志格式。不同的代理和服务器可以使用自己的默认访问日志格式。Envoy 有其默认的日志格式。我们可以自定义默认格式，并配置它，使其以与其他服务器（如 Apache 或 NGINX）有相同的格式写出日志。有了相同的访问日志格式，我们就可以把不同的服务器放在一起使用，用一个工具把数据记录和分析结合起来。\n本模块将解释访问日志在 Envoy 中是如何工作的，以及如何配置和定制。\n捕获和读取访问日志 我们可以配置捕获任何向 Envoy 代理发出的访问请求，并将其写入所谓的访问日志。让我们看看几个访问日志条目的例子。\n[2021-11-01T20:37:45.204Z] \u0026#34;GET / HTTP/1.1\u0026#34; 200 - 0 3 0 - \u0026#34;-\u0026#34; \u0026#34;curl/7.64.0\u0026#34; \u0026#34;9c08a41b-805f-42c0-bb17-40ec50a3377a\u0026#34; \u0026#34;localhost:10000\u0026#34; \u0026#34;-\u0026#34; [2021-11-01T21:08:18.274Z] \u0026#34;POST /hello HTTP/1.1\u0026#34; 200 - 0 3 0 - \u0026#34;-\u0026#34; \u0026#34;curl/7.64.0\u0026#34; \u0026#34;6a593d31-c9ac-453a-80e9-ab805d07ae20\u0026#34; \u0026#34;localhost:10000\u0026#34; \u0026#34;-\u0026#34; [2021-11-01T21:09:42.717Z] \u0026#34;GET /test HTTP/1.1\u0026#34; 404 NR 0 0 0 - \u0026#34;-\u0026#34; \u0026#34;curl/7.64.0\u0026#34; \u0026#34;1acc3559-50eb-463c-ae21-686fe34abbe8\u0026#34; \u0026#34;localhost:10000\u0026#34; \u0026#34;-\u0026#34; 输出包含三个不同的日志条目，并遵循相同的默认日志格式。默认的日志格式看起来像这样。\n[%START_TIME%] \u0026#34;%REQ(:METHOD)% %REQ(X-ENVOY-ORIGINAL-PATH?:PATH)% %PROTOCOL%\u0026#34; %RESPONSE_CODE% %RESPONSE_FLAGS% %BYTES_RECEIVED% %BYTES_SENT% %DURATION% %RESP(X-ENVOY-UPSTREAM-SERVICE-TIME)% \u0026#34;%REQ(X-FORWARDED-FOR)%\u0026#34; \u0026#34;%REQ(USER-AGENT)%\u0026#34; \u0026#34;%REQ(X-REQUEST-ID)%\u0026#34; \u0026#34;%REQ(:AUTHORITY)%\u0026#34; \u0026#34;%UPSTREAM_HOST%\u0026#34; 诸如 %RESPONSE_FLAGS%、%REQ(:METHOD)% 等值被称为命令操作符（command operator）。\n命令操作符 命令操作符提取相关数据并插入到 TCP 和 HTTP 的日志条目中。如果这些值没有设置或不可用（例如，TCP 中的 RESPONSE_CODE），日志将包含字符 -（或 JSON 日志的 \u0026#34;-\u0026#34;）。\n每个命令操作符都以字符 % 开始和结束，例如，%START_TIME%。如果命令操作符接受任何参数，那么我们可以在括号内提供这些参数。例如，如果我们想使用 START_TIME 命令操作符只记录日、月、年，那么我们可以通过在括号中指定这些值来进行配置：%START_TIME(%d-%m-%Y)%。\n让我们来看看不同的命令操作符。我们试图根据它们的共同属性将它们归入单独的表格。\n   命令操作符 描述 示例     START_TIME 请求开始时间，包括毫秒。 %START_TIME(%Y/%m/%dT%H:%M:%S%z ^%)%   PROTOCOL 协议（HTTP/1.1、HTTP/2 或 HTTP/3） ％PROTOCOL％   RESPONSE_CODE HTTP 响应代码。如果下游客户端断开连接，响应代码将被设置为 0。 %RESPONSE_CODE%   RESPONSE_CODE_DETAIL 关于 HTTP 响应的额外信息（例如，谁设置的以及为什么设置）。 RESPONSE_CODE_DETAIL%。   CONNECTION_TERMINATION_DETAILS 提供关于 Envoy 因 L4 原因终止连接的额外信息。 %CONNECTION_TERMINATION_DETAILS%   ROUTE_NAME 路由的名称。 %ROUTE_NAME%   CONNECTION_ID 下游连接的一个标识符。它可以用来交叉引用多个日志汇聚中的 TCP 访问日志，或交叉引用同一连接的基于计时器的报告。该标识符在一个执行过程中很可能是唯一的，但在多个实例或重新启动之间可能会重复。 %CONNECTION_ID%   GRPC_STATUS gRPC 状态代码，包括文本信息和一个数字。 %GRPC_STATUS%   HOSTNAME 系统主机名。 %HOSTNAME%   LOCAL_REPLY_BODY 被 Envoy 拒绝的请求的正文。 %LOCAL_REPLY_BODY%   FILTER_CHAIN_NAME 下游连接的网络过滤器链名称。 %FILTER_CHAIN_NAME%    大小 该组包含所有代表大小的命令操作符——从请求和响应头字节到接收和发送的字节。\n   命令操作符 描述 示例     REQUEST_HEADER_BYTES 请求头的未压缩字节。 %REQUEST_HEADER_BYTES%   RESPONSE_HEADERS_BYTES 响应 Header 的未压缩字节数。 %RESPONSE_HEADERS_BYTES％   RESPONSE_TRAILERS_BYTES 响应 trailer 的未压缩字节。 %RESPONSE_TRAILERS_BYTES%   BYTES_SENT 为 HTTP 发送的正文字节和为 TCP 发送的连接上的下游字节。 %BYTES_SENT%   BYTES_RECEIVED 收到的正文字节数。 %BYTES_RECEIVED%   UPSTREAM_WIRE_BYTES_SENT 由 HTTP 流向上游发送的总字节数。 %UPSTREAM_WIRE_BYTES_SENT％   UPSTREAM_WIRE_BYTES_RECEIVED 从上游 HTTP 流收到的字节总数。 %upstream_wire_bytes_received％   UPSTREAM_HEADER_BYTES_SENT 由 HTTP 流向上游发送的头字节的数量。 %UPSTREAM_HEADER_BYTES_SENT％   UPSTREAM_HEADER_BYTES_RECEIVED HTTP 流从上游收到的头字节的数量。 %UPSTREAM_HEADER_BYTES_RECEIVED%   DOWNSTREAM_WIRE_BYTES_SENT HTTP 流向下游发送的总字节数。 %downstream_wire_bytes_sent%   DOWNSTREAM_WIRE_BYTES_RECEIVED HTTP 流从下游收到的字节总数。 %DOWNSTREAM_WIRE_BYTES_RECEIVED%   DOWNSTREAM_HEADER_BYTES_SENT 由 HTTP 流向下游发送的头字节的数量。 %DOWNSTREAM_HEADER_BYTES_SENT%   DOWNSTREAM_HEADER_BYTES_RECEIVED HTTP 流从下游收到的头字节的数量。 %downstream_header_bytes_received%    时长    命令操作符 描述 示例     DURATION 从开始时间到最后一个字节输出，请求的总持续时间（以毫秒为单位）。 %DURATION%   REQUEST_DURATION 从开始时间到收到下游请求的最后一个字节，请求的总持续时间（以毫秒计）。 %REQUEST_DURATION%   RESPONSE_DURATION 从开始时间到从上游主机读取的第一个字节，请求的总持续时间（以毫秒计）。 RESPONSE_DURATION   RESPONSE_TX_DURATION 从上游主机读取的第一个字节到下游发送的最后一个字节，请求的总时间（以毫秒为单位）。 %RESPONSE_TX_DURATION%    响应标志 RESPONSE_FLAGS 命令操作符包含关于响应或连接的额外细节。下面的列表显示了 HTTP 和 TCP 连接的响应标志的值和它们的含义。\nHTTP 和 TCP\n UH：除了 503 响应代码外，在一个上游集群中没有健康的上游主机。 UF：除了 503 响应代码外，还有上游连接失败。 UO：上游溢出（断路），此外还有 503 响应代码。 NR：除了 404 响应代码外，没有为给定的请求配置路由，或者没有匹配的下游连接的过滤器链。 URX：请求被拒绝是因为达到了上游重试限制（HTTP）或最大连接尝试（TCP）。 NC：未找到上游集群。 DT：当一个请求或连接超过 max_connection_duration 或 max_downstream_connection_duration。  仅限 HTTP\n DC: 下游连接终止。 LH：除了 503 响应代码，本地服务的健康检查请求失败。 UT：除 504 响应代码外的上行请求超时。 LR：除了 503 响应代码外，连接本地重置。 UR：除 503 响应代码外的上游远程复位。 UC：除 503 响应代码外的上游连接终止。 DI：请求处理被延迟了一段通过故障注入指定的时间。 FI：该请求被中止，并有一个通过故障注入指定的响应代码。 RL：除了 429 响应代码外，该请求还被 HTTP 速率限制过滤器在本地进行了速率限制。 UAEX：该请求被外部授权服务拒绝。 RLSE：请求被拒绝，因为速率限制服务中存在错误。 IH：该请求被拒绝，因为除了 400 响应代码外，它还为一个严格检查的头设置了一个无效的值。 SI：除 408 响应代码外，流空闲超时。 DPE：下游请求有一个 HTTP 协议错误。 UPE：上游响应有一个 HTTP 协议错误。 UMSDR：上游请求达到最大流时长。 OM：过载管理器终止了该请求。  上游信息    命令操作符 描述 示例     UPSTREAM_HOST 上游主机 URL 或 TCP 连接的 tcp://ip:端口。 %UPSTREAM_HOST%   UPSTREAM_CLUSTER 上游主机所属的上游集群。如果运行时特性 envoy.reloadable_features.use_observable_cluster_name 被启用，那么如果提供了 alt_stat_name 就会被使用。 %UPSTREAM_CLUSTER%   UPSTREAM_LOCAL_ADDRESS 上游连接的本地地址。如果是一个 IP 地址，那么它包括地址和端口。 %UPSTREAM_LOCAL_ADDRESS％   UPSTREAM_TRANSPORT_FAILURE_REASON 如果由于传输套接字导致连接失败，则提供来自传输套接字的失败原因。 %UPSTREAM_TRANSPORT_FAILURE_REASON%    下游信息    命令描述符 描述 示例     DOWNSTREAM_REMOTE_ADDRESS 下游连接的远程地址。如果是一个 IP 地址，那么它包括地址和端口。 …","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"e676d92a1f373f8d3aeb324e626372c4","permalink":"https://jimmysong.io/docs/envoy-handbook/logging/access-logging/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/envoy-handbook/logging/access-logging/","section":"envoy-handbook","summary":"本节将为你讲解 Envoy 中的访问日志配置。 什么是访问日志？ 每当你打开","tags":null,"title":"访问日志","type":"book"},{"authors":null,"categories":null,"content":"集群可以在配置文件中静态配置，也可以通过集群发现服务（CDS）API 动态配置。每个集群都是一个端点的集合，Envoy 需要解析这些端点来发送流量。\n解析端点的过程被称为服务发现。\n什么是端点？ 集群是一个识别特定主机的端点的集合。每个端点都有以下属性。\n地址 (address)\n该地址代表上游主机地址。地址的形式取决于集群的类型。对于 STATIC 或 EDS 集群类型，地址应该是一个 IP，而对于 LOGICAL 或 STRICT DNS 集群类型，地址应该是一个通过 DNS 解析的主机名。\n主机名 (hostname)\n一个与端点相关的主机名。注意，主机名不用于路由或解析地址。它与端点相关联，可用于任何需要主机名的功能，如自动主机重写。\n健康检查配置（health_check_config）\n可选的健康检查配置用于健康检查器联系健康检查主机。该配置包含主机名和可以联系到主机以执行健康检查的端口。注意，这个配置只适用于启用了主动健康检查的上游集群。\n服务发现类型 有五种支持的服务发现类型，我们将更详细地介绍。\n静态 (STATIC) 静态服务发现类型是最简单的。在配置中，我们为集群中的每个主机指定一个已解析的网络名称。例如：\nclusters:- name:my_cluster_nametype:STATICload_assignment:cluster_name:my_service_nameendpoints:- lb_endpoints:- endpoint:address:socket_address:address:127.0.0.1port_value:8080注意，如果我们不提供类型，默认为 STATIC。\n严格的 DNS（STRICT_DNS） 通过严格的 DNS，Envoy 不断地、异步地解析集群中定义的 DNS 端点。如果 DNS 查询返回多个 IP 地址，Envoy 假定它们是集群的一部分，并在它们之间进行负载均衡。同样，如果 DNS 查询返回 0 个主机，Envoy 就认为集群没有任何主机。\n关于健康检查的说明——如果多个 DNS 名称解析到同一个 IP 地址，则不共享健康检查。这可能会给上游主机造成不必要的负担，因为 Envoy 会对同一个 IP 地址进行多次健康检查（跨越不同的 DNS 名称）。\n当 respect_dns_ttl  字段被启用时，我们可以使用 dns_refresh_rate  控制 DNS 名称的连续解析。如果不指定，DNS 刷新率默认为 5000ms。另一个设置（dns_failure_refresh_rate）控制故障时的刷新频率。如果没有提供，Envoy 使用 dns_refresh_rate。\n下面是一个 STRICT_DNS 服务发现类型的例子。\nclusters:- name:my_cluster_nametype:STRICT_DNSload_assignment:cluster_name:my_service_nameendpoints:- lb_endpoints:- endpoint:address:socket_address:address:my-serviceport_value:8080逻辑 DNS（LOGICAL_DNS） 逻辑 DNS 服务发现与严格 DNS 类似，它使用异步解析机制。然而，它只使用需要启动新连接时返回的第一个 IP 地址。\n因此，一个逻辑连接池可能包含与各种不同上游主机的物理连接。这些连接永远不会耗尽，即使在 DNS 解析返回零主机的情况下。\n 什么是连接池？\n集群中的每个端点将有一个或多个连接池。例如，根据所支持的上游协议，每个协议可能有一个连接池分配。Envoy 中的每个工作线程也为每个集群维护其连接池。例如，如果 Envoy 有两个线程和一个同时支持 HTTP/1 和 HTTP/2 的集群，将至少有四个连接池。连接池的方式是基于底层线程协议的。对于 HTTP/1.1，连接池根据需要获取端点的连接（最多到断路限制）。当请求变得可用时，它们就被绑定到连接上。 当使用 HTTP/2 时，连接池在一个连接上复用多个请求，最多到 max_concurrent_streams 和 max_requests_per_connections 指定的限制。HTTP/2 连接池建立尽可能多的连接，以满足请求。\n 逻辑 DNS 的一个典型用例是用于大规模网络服务。通常使用轮询 DNS，它们在每次查询时返回多个 IP 地址的不同结果。如果我们使用严格的 DNS 解析，Envoy 会认为集群端点在每次内部解析时都会改变，并会耗尽连接池。使用逻辑 DNS，连接将保持存活，直到它们被循环。\n与严格的 DNS 一样，逻辑 DNS 也使用 respect_dns_ttl 和 dns_refresh_rate  字段来配置 DNS 刷新率。\nclusters:- name:my_cluster_nametype:LOGICAL_DNSload_assignment:cluster_name:my_service_nameendpoints:- lb_endpoints:- endpoint:address:socket_address:address:my-serviceport_value:8080端点发现服务（EDS） Envoy 可以使用端点发现服务来获取集群的端点。通常情况下，这是首选的服务发现机制。Envoy 获得每个上游主机的显式知识（即不需要通过 DNS 解析的负载均衡器进行路由）。每个端点都可以携带额外的属性，可以告知 Envoy 负载均衡的权重和金丝雀状态区，等等。\nclusters:- name:my_cluster_nametype:EDSeds_cluster_config:eds_config:...我们会在动态配置和 xDS 一章中更详细地解释动态配置。\n原始目的地（ORIGINAL_DST） 当与 Envoy 的连接通过 iptables REDIRECT 或 TPROXY 目标或与代理协议的连接时，我们使用原来的目标集群类型。\n在这种情况下，请求被转发到重定向元数据（例如，使用 x-envoy-original-dst-host 头）地址的上游主机，而无需任何配置或上游主机发现。\n当上游主机的连接闲置时间超过 cleanup_interval 字段中指定的时间（默认为 5000 毫秒）时，这些连接会被汇集起来并被刷新。\nclusters:- name:original_dst_clustertype:ORIGINAL_DSTlb_policy:ORIGINAL_DST_LBORIGINAL_DST 集群类型可以使用的唯一负载均衡策略是 ORIGINAL_DST_LB 策略。\n除了上述服务发现机制外，Envoy 还支持自定义集群发现机制。我们可以使用 cluster_type 字段配置自定义的发现机制。\nEnvoy 支持两种类型的健康检查，主动和被动。我们可以同时使用这两种类型的健康检查。在主动健康检查中，Envoy 定期向端点发送请求以检查其状态。使用被动健康检查，Envoy 监测端点如何响应连接。它使 Envoy 甚至在主动健康检查将其标记为不健康之前就能检测到一个不健康的端点。Envoy 的被动健康检查是通过异常点检测实现的。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"dd7f3dd2e7a823c8680347ed23cd1139","permalink":"https://jimmysong.io/docs/envoy-handbook/cluster/service-discovery/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/envoy-handbook/cluster/service-discovery/","section":"envoy-handbook","summary":"集群可以在配置文件中静态配置，也可以通过集群发现服务（CDS","tags":null,"title":"服务发现","type":"book"},{"authors":null,"categories":null,"content":"这一节中，我将为你介绍服务网格的架构，首先我们先来了解下服务网格常用的四种部署模式，然后是其最基本的组成部分——控制平面和数据平面。\n服务网格的部署模式 这四种模式分别是Sidecar 代理、节点共享代理、Service Account/节点共享代理、带有微代理的共享远程代理。\n   服务网格的部署模式  这几种架构模式的主要区别是代理所在的位置不同，模式一在每个应用程序旁运行一个代理，模式二在每个节点上运行一个代理，模式三在每个节点中，再根据 ServiceAccount 的数量，每个 ServiceAccount 运行一个代理，模式四在每个应用旁运行一个微代理，这个代理仅处理 mTLS ，而七层代理位于远程，这几种方式在安全性、隔离及运维开销上的对比如这个表格所示。\n   模式 内存开销 安全性 故障域 运维     Sidecar 代理 因为为每个 pod 都注入一个代理，所以开销最大。 由于 sidecar 必须与工作负载一起部署，工作负载有可能绕过 sidecar。 Pod 级别隔离，如果有代理出现故障，只影响到 Pod 中的工作负载。 可以单独升级某个工作负载的 sidecar 而不影响其他工作负载。   节点共享代理 每个节点上只有一个代理，为该节点上的所有工作负载所共享，开销小。 对加密内容和私钥的管理存在安全隐患。 节点级别隔离，如果共享代理升级时出现版本冲突、配置冲突或扩展不兼容等问题，则可能会影响该节点上的所有工作负载。 不需要考虑注入 Sidecar 的问题。   Service Account/节点共享代理 服务账户/身份下的所有工作负载都使用共享代理，开销小。 工作负载和代理之间的连接的认证及安全性无法保障。 节点和服务账号之间级别隔离，故障同“节点共享代理”。 同“节点共享代理”。   带有微代理的共享远程代理 因为为每个 pod 都注入一个微代理，开销比较大。 微代理专门处理 mTLS，不负责 L7 路由，可以保障安全性。 当需要应用7层策略时，工作负载实例的流量会被重定向到L7代理上，若不需要，则可以直接绕过。该L7代理可以采用共享节点代理、每个服务账户代理，或者远程代理的方式运行。 同“Sidecar 代理”。    Istio 是目前最流行的服务网格的开源实现，它的架构目前来说有两种，一种是 Sidecar 模式，这也是 Istio 最传统的部署架构，另一种是 Proxyless 模式。这两种模式，我们将在下一节中介绍。\n下面我们将介绍服务网格的两个主要组成部分——控制平面和数据平面。\n控制平面 控制平面的特点：\n 不直接解析数据包 与控制平面中的代理通信，下发策略和配置 负责网络行为的可视化 通常提供 API 或者命令行工具可用于配置版本化管理，便于持续集成和部署  数据平面 数据平面的特点：\n 通常是按照无状态目标设计的，但实际上为了提高流量转发性能，需要缓存一些数据，因此无状态也是有争议的 直接处理入站和出站数据包，转发、路由、健康检查、负载均衡、认证、鉴权、产生监控数据等 对应用来说透明，即可以做到无感知部署  ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"998587326c9704fc8bfea7ba7744d304","permalink":"https://jimmysong.io/docs/istio-handbook/concepts/service-mesh-architectures/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/istio-handbook/concepts/service-mesh-architectures/","section":"istio-handbook","summary":"这一节中，我将为你介绍服务网格的架构，首先我们先来了解下服务","tags":null,"title":"服务网格架构","type":"book"},{"authors":null,"categories":null,"content":"正如在介绍章节中提到的，监听器子系统处理下游或传入的请求处理。监听器子系统负责传入的请求和对客户端的响应路径。除了定义 Envoy 对传入请求进行 “监听” 的地址和端口外，我们还可以选择对每个监听器进行监听过滤器的配置。\n不要把监听器过滤器和我们前面讨论的网络过滤器链和 L3/L4 过滤器混淆起来。Envoy 在处理网络级过滤器之前先处理监听器过滤器，如下图所示。\n   监听器过滤器  请注意，在没有任何监听器过滤器的情况下操作 Envoy 也是常见的。\nEnvoy 会在网络级过滤器之前处理监听器过滤器。我们可以在监听器过滤器中操作连接元数据，通常是为了影响后来的过滤器或集群如何处理连接。\n监听器过滤器对新接受的套接字进行操作，并可以停止或随后继续执行进一步的过滤器。监听器过滤器的顺序很重要，因为 Envoy 在监听器接受套接字后，在创建连接前，会按顺序处理这些过滤器。\n我们可以使用监听器过滤器的结果来进行过滤器匹配，并选择一个合适的网络过滤器链。例如，我们可以使用 HTTP 检查器监听器过滤器来确定 HTTP 协议（HTTP/1.1 或 HTTP/2）。基于这个结果，我们就可以选择并运行不同的网络过滤器链。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"a5e49bb1d507d6e6f493d1c5644f061f","permalink":"https://jimmysong.io/docs/envoy-handbook/listener/listener-filters/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/envoy-handbook/listener/listener-filters/","section":"envoy-handbook","summary":"正如在介绍章节中提到的，监听器子系统处理下游或传入的请求处理","tags":null,"title":"监听器过滤器","type":"book"},{"authors":null,"categories":null,"content":"扩展 Envoy 的一种方式是实现不同的过滤器，处理或增强请求。这些过滤器可以生成统计数据，翻译协议，修改请求，等等。\nHTTP 过滤器就是一个例子，比如外部的 authz 过滤器和其他内置于 Envoy 二进制的过滤器。\n此外，我们还可以编写我们的过滤器，让 Envoy 动态地加载和运行。我们可以通过正确的顺序声明来决定我们要在过滤器链中的哪个位置运行过滤器。\n我们有几个选择来扩展 Envoy。默认情况下，Envoy 过滤器是用 C++ 编写的。但是，我们可以用 Lua 脚本编写，或者使用 WebAssembly（WASM）来开发其他编程语言的 Envoy 过滤器。\n请注意，与 C++ 过滤器相比，Lua 和 Wasm 过滤器的 API 是有限的。\n1. 原生 C++ API\n第一个选择是编写原生 C++ 过滤器，然后将其与 Envoy 打包。这就需要我们重新编译 Envoy，并维护我们的版本。如果我们试图解决复杂或高性能的用例，采取这种方式是有意义的。\n2. Lua 过滤器\n第二个选择是使用 Lua 脚本。在 Envoy 中有一个 HTTP 过滤器，允许我们定义一个 Lua 脚本，无论是内联还是外部文件，并在请求和响应流程中执行。\n3. Wasm 过滤器\n最后一个选项是基于 Wasm 的过滤器。我们用这个选项把过滤器写成一个单独的 Wasm 模块，Envoy 在运行时动态加载它。\n在接下来的模块中，我们将学习更多关于 Lua 和 Wasm 过滤器的知识。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"9c69fdb2c5cf2cb72e40af04713a332d","permalink":"https://jimmysong.io/docs/envoy-handbook/extending-envoy/overview/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/envoy-handbook/extending-envoy/overview/","section":"envoy-handbook","summary":"扩展 Envoy 的一种方式是实现不同的过滤器，处理或增强请求。这些过滤","tags":null,"title":"可扩展性概述","type":"book"},{"authors":null,"categories":null,"content":"下面将带您了解 Istio 流量管理相关的基础概念与配置示例。\n VirtualService：在 Istio 服务网格中定义路由规则，控制流量路由到服务上的各种行为。 DestinationRule：是 VirtualService 路由生效后，配置应用与请求的策略集。 ServiceEntry：通常用于在 Istio 服务网格之外启用的服务请求。 Gateway：为 HTTP/TCP 流量配置负载均衡器，最常见的是在网格边缘的操作，以启用应用程序的入口流量。 EnvoyFilter：描述了针对代理服务的过滤器，用来定制由 Istio Pilot 生成的代理配置。一定要谨慎使用此功能。错误的配置内容一旦完成传播，可能会令整个服务网格陷入瘫痪状态。这一配置是用于对 Istio 网络系统内部实现进行变更的。  注：本文中的示例引用自 Istio 官方 Bookinfo 示例，见：Istio 代码库，且对于配置的讲解都以在 Kubernetes 中部署的服务为准。\nVirtualService VirtualService 故名思义，就是虚拟服务，在 Istio 1.0 以前叫做 RouteRule。VirtualService 中定义了一系列针对指定服务的流量路由规则。每个路由规则都是针对特定协议的匹配规则。如果流量符合这些特征，就会根据规则发送到服务注册表中的目标服务（或者目标服务的子集或版本）。\n注意：VirtualService 中的规则是按照在 YAML 文件中的顺序执行的，这就是为什么在存在多条规则时，需要慎重考虑优先级的原因。\n配置说明\n下面是 VirtualService 的配置说明。\n   字段 类型 描述     hosts string[] 必要字段：流量的目标主机。可以是带有通配符前缀的 DNS 名称，也可以是 IP 地址。根据所在平台情况，还可能使用短名称来代替 FQDN。这种场景下，短名称到 FQDN 的具体转换过程是要靠下层平台完成的。一个主机名只能在一个 VirtualService 中定义。同一个 VirtualService 中可以用于控制多个 HTTP 和 TCP 端口的流量属性。Kubernetes 用户注意：当使用服务的短名称时（例如使用 reviews，而不是 reviews.default.svc.cluster.local），Istio 会根据规则所在的命名空间来处理这一名称，而非服务所在的命名空间。假设 “default” 命名空间的一条规则中包含了一个 reviews 的 host 引用，就会被视为 reviews.default.svc.cluster.local，而不会考虑 reviews 服务所在的命名空间。为了避免可能的错误配置，建议使用 FQDN 来进行服务引用。 hosts 字段对 HTTP 和 TCP 服务都是有效的。网格中的服务也就是在服务注册表中注册的服务，必须使用他们的注册名进行引用；只有 Gateway 定义的服务才可以使用 IP 地址。   gateways string[] Gateway 名称列表，Sidecar 会据此使用路由。VirtualService 对象可以用于网格中的 Sidecar，也可以用于一个或多个 Gateway。这里公开的选择条件可以在协议相关的路由过滤条件中进行覆盖。保留字 mesh 用来指代网格中的所有 Sidecar。当这一字段被省略时，就会使用缺省值（mesh），也就是针对网格中的所有 Sidecar 生效。如果提供了 gateways 字段，这一规则就只会应用到声明的 Gateway 之中。要让规则同时对 Gateway 和网格内服务生效，需要显式的将 mesh 加入 gateways 列表。   http HTTPRoute[] HTTP 流量规则的有序列表。这个列表对名称前缀为 http-、http2-、grpc- 的服务端口，或者协议为 HTTP、HTTP2、GRPC 以及终结的 TLS，另外还有使用 HTTP、HTTP2 以及 GRPC 协议的 ServiceEntry 都是有效的。进入流量会使用匹配到的第一条规则。   tls TLSRoute[] 一个有序列表，对应的是透传 TLS 和 HTTPS 流量。路由过程通常利用 ClientHello 消息中的 SNI 来完成。TLS 路由通常应用在 https-、tls- 前缀的平台服务端口，或者经 Gateway 透传的 HTTPS、TLS 协议端口，以及使用 HTTPS 或者 TLS 协议的 ServiceEntry 端口上。注意：没有关联 VirtualService 的 https- 或者 tls- 端口流量会被视为透传 TCP 流量。   tcp TCPRoute[] 一个针对透传 TCP 流量的有序路由列表。TCP 路由对所有 HTTP 和 TLS 之外的端口生效。进入流量会使用匹配到的第一条规则。    示例\n下面的例子中配置了一个名为 reviews 的 VirtualService，该配置的作用是将所有发送给 reviews 服务的流量发送到 v1 版本的子集。\napiVersion:networking.istio.io/v1alpha3kind:VirtualServicemetadata:name:reviewsspec:hosts:- reviewshttp:- route:- destination:host:reviewssubset:v1 该配置中流量的目标主机是 reviews，如果该服务和规则部署在 Kubernetes 的 default namespace 下的话，对应于 Kubernetes 中的服务的 DNS 名称就是 reviews.default.svc.cluster.local。 我们在 hosts 配置了服务的名字只是表示该配置是针对 reviews.default.svc.cluster.local 的服务的路由规则，但是具体将对该服务的访问的流量路由到哪些服务的哪些实例上，就是要通过 destination 的配置了。 我们看到上面的 VirtualService 的 HTTP 路由中还定义了一个 destination。destination 用于定义在网络中可寻址的服务，请求或连接在经过路由规则的处理之后，就会被发送给 destination。destination.host 应该明确指向服务注册表中的一个服务。Istio 的服务注册表除包含平台服务注册表中的所有服务（例如 Kubernetes 服务、Consul 服务）之外，还包含了 ServiceEntry 资源所定义的服务。VirtualService 中只定义流量发送给哪个服务的路由规则，但是并不知道要发送的服务的地址是什么，这就需要 DestinationRule 来定义了。 subset 配置流量目的地的子集，下文会讲到。VirtualService 中其实可以除了 hosts 字段外其他什么都不配置，路由规则可以在 DestinationRule 中单独配置来覆盖此处的默认规则。  Subset subset 不属于 Istio 创建的 CRD，但是它是一条重要的配置信息，有必要单独说明下。subset 是服务端点的集合，可以用于 A/B 测试或者分版本路由等场景。另外在 subset 中可以覆盖服务级别的即 VirtualService 中的定义的流量策略。\n以下是subset 的配置信息。对于 Kubernetes 中的服务，一个 subset 相当于使用 label 的匹配条件选出来的 service。\n   字段 类型 描述     name string 必要字段。服务名和 subset 名称可以用于路由规则中的流量拆分。   labels map\u0026lt;string, string\u0026gt; 必要字段。使用标签对服务注册表中的服务端点进行筛选。   trafficPolicy TrafficPolicy 应用到这一 subset 的流量策略。缺省情况下 subset 会继承 DestinationRule 级别的策略，这一字段的定义则会覆盖缺省的继承策略。    DestinationRule DestinationRule 所定义的策略，决定了经过路由处理之后的流量的访问策略。这些策略中可以定义负载均衡配置、连接池大小以及外部检测（用于在负载均衡池中对不健康主机进行识别和驱逐）配置。\n配置说明\n下面是 DestinationRule 的配置说明。\n   字段 类型 描述     name string 必要字段。服务名和 subset 名称可以用于路由规则中的流量拆分。   labels map\u0026lt;string, string\u0026gt; 必要字段。使用标签对服务注册表中的服务端点进行筛选。   trafficPolicy TrafficPolicy 应用到这一子集的流量策略。缺省情况下子集会继承 DestinationRule 级别的策略，这一字段的定义则会覆盖缺省的继承策略。    示例\n下面是一条对 productpage 服务的流量目的地策略的配置。\napiVersion:networking.istio.io/v1alpha3kind:DestinationRulemetadata:name:reviewsspec:host:reviewssubsets:- name:v1labels:version:v1该路由策略将所有对 reviews 服务的流量路由到 v1 的 subset。\nServiceEntry Istio 服务网格内部会维护一个与平台无关的使用通用模型表示的服务注册表，当你的服务网格需要访问外部服务的时候，就需要使用 ServiceEntry 来添加服务注册。\nEnvoyFilter EnvoyFilter 描述了针对代理服务的过滤器，用来定制由 Istio Pilot 生成的代理配置。一定要谨慎使用此功能。错误的配置内容一旦完成传播，可能会令整个服务网格陷入瘫痪状态。这一配置是用于对 Istio 网络系统内部实现进行变更的，属于高级配置，用于扩展 Envoy 中的过滤器的。\nGateway Gateway 为 HTTP/TCP 流量配置了一个负载均衡，多数情况下在网格边缘进行操作，用于启用一个服务的入口（ingress）流量，相当于前端代理。与 Kubernetes 的 Ingress 不同，Istio Gateway 只配置四层到六层的功能（例如开放端口或者 TLS 配置），而 Kubernetes 的 Ingress 是七层的。将 VirtualService 绑定到 Gateway 上，用户就可以使用标准的 Istio 规则来控制进入的 HTTP 和 TCP 流量。\nGateway 设置了一个集群外部流量访问集群中的某些服务的入口，而这些流量究竟如何路由到那些服务上则需要通过配置 VirtualServcie 来绑定。下面仍然以 productpage 这个服务来说明。\napiVersion:networking.istio.io/v1alpha3kind:Gatewaymetadata:name:bookinfo-gatewayspec:selector:istio:ingressgateway# 使用默认的控制器servers:- port:number:80name:httpprotocol:HTTPhosts:- \u0026#34;*\u0026#34;---apiVersion:networking.istio.io/v1alpha3kind:VirtualServicemetadata:name:bookinfospec:hosts:- \u0026#34;*\u0026#34;gateways:- bookinfo-gatewayhttp:- match:- uri:exact:/productpage- uri:exact:/login- …","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"974b5da33836557ad3ae8b68eff70895","permalink":"https://jimmysong.io/docs/istio-handbook/traffic-management/basic/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/istio-handbook/traffic-management/basic/","section":"istio-handbook","summary":"下面将带您了解 Istio 流量管理相关的基础概念与配置示例。 Virtu","tags":null,"title":"流量管理基础概念","type":"book"},{"authors":null,"categories":null,"content":"在整个课程中，我们已经多次提到了管理接口。Envoy 暴露了一个管理接口，允许我们修改 Envoy，并获得一个视图和查询指标和配置。\n管理接口由一个具有多个端点的 REST API 和一个简单的用户界面组成，如下图所示。\n   Envoy 管理接口  管理接口必须使用 admin 字段明确启用。例如：\nadmin:address:socket_address:address:127.0.0.1port_value:9901在启用管理接口时要小心。任何有权限进入管理接口的人都可以进行破坏性的操作，比如关闭服务器（/quitquit 端点）。我们还可能让他们访问私人信息（指标、集群名称、证书信息等）。目前（Envoy 1.20 版本），管理端点是不安全的，也没有办法配置认证或 TLS。有一个工作项目正在进行中，它将限制只有受信任的 IP 和客户端证书才能访问，以确保传输安全。\n在这项工作完成之前，应该只允许通过安全网络访问管理接口，而且只允许从连接到该安全网络的主机访问。我们可以选择只允许通过 localhost 访问管理接口，如上面的配置中所示。另外，如果你决定允许从远程主机访问，那么请确保你也设置了防火墙规则。\n在接下来的课程中，我们将更详细地了解管理接口的不同功能。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"2793f9bc675d039aaa32b9212af3507d","permalink":"https://jimmysong.io/docs/envoy-handbook/admin-interface/enabling-admin-interface/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/envoy-handbook/admin-interface/enabling-admin-interface/","section":"envoy-handbook","summary":"在整个课程中，我们已经多次提到了管理接口。Envoy 暴露了一","tags":null,"title":"启用管理接口","type":"book"},{"authors":null,"categories":null,"content":"为了解释什么是认证或 authn，我们将从访问控制试图回答的问题开始：一个主体能否对一个对象执行操作？\n如果我们把上述问题翻译成 Istio 和 Kubernetes 的世界，它将是 “服务 X 能否对服务 Y 进行操作？”\n这个问题的三个关键部分是：主体、动作和对象。\n主体和对象都是 Kubernetes 中的服务。动作，假设我们谈论的是 HTTP，是一个 GET 请求，一个 POST，一个 PUT 等等。\n认证是关于主体（或者在我们的例子中是服务的身份）的。认证是验证某种凭证的行为，并确保该凭证是有效和可信的。一旦进行了认证，我们就有了一个经过认证的主体。下次你旅行时，你向海关官员出示你的护照或身份证，他们会对其进行认证，确保你的凭证（护照或身份证）是有效和可信的。\n在 Kubernetes 中，每个工作负载都被分配了一个独特的身份，它用来与其他每个工作负载进行通信 - 该身份以服务账户的形式提供给工作负载。服务账户是运行时中存在的身份 Pods。\nIstio 使用来自服务账户的 X.509 证书，它根据名为 SPIFFE（每个人的安全生产身份框架）的规范创建一个新的身份。\n证书中的身份被编码在证书的 Subject alternate name 字段中，它看起来像这样。\nspiffe://cluster.local/ns/\u0026lt;pod namespace\u0026gt;/sa/\u0026lt;pod service account\u0026gt; 当两个服务开始通信时，它们需要交换带有身份信息的凭证，以相互验证自己。客户端根据安全命名信息检查服务器的身份，看它是否是服务的授权运行者。\n服务器根据授权策略确定客户可以访问哪些信息。此外，服务器可以审计谁在什么时间访问了什么，并决定是否批准或拒绝客户对服务器的调用。\n安全命名信息包含从服务身份到服务名称的映射。服务器身份是在证书中编码的，而服务名称是由发现服务或 DNS 使用的名称。从一个身份 A 到一个服务名称 B 的单一映射意味着 “A 被允许和授权运行服务 B”。安全命名信息由 Pilot 生成，然后分发给所有 sidecar 代理。\n证书创建和轮换 对于网格中的每个工作负载，Istio 提供一个 X.509 证书。一个名为 pilot-agent 的代理在每个 Envoy 代理旁边运行，并与控制平面（istiod）一起工作，自动进行密钥和证书的轮转。\n   证书和秘钥管理  在运行时创建身份时，有三个部分在起作用：\n Citadel（控制平面的一部分） Istio 代理 Envoy 的秘密发现服务（SDS）  Istio Agent 与 Envoy sidecar 一起工作，通过安全地传递配置和秘密，帮助它们连接到服务网格。即使 Istio 代理在每个 pod 中运行，我们也认为它是控制平面的一部分。\n秘密发现服务（SDS）简化了证书管理。如果没有 SDS，证书必须作为秘密（Secret）创建，然后装入代理容器的文件系统中。当证书过期时，需要更新秘密，并重新部署代理，因为 Envoy 不会从磁盘动态重新加载证书。当使用 SDS 时，SDS 服务器将证书推送给 Envoy 实例。每当证书过期时，SDS 会推送更新的证书，Envoy 可以立即使用它们。不需要重新部署代理服务器，也不需要中断流量。在 Istio 中，Istio Agent 作为 SDS 服务器，实现了秘密发现服务接口。\n每次我们创建一个新的服务账户时，Citadel 都会为它创建一个 SPIFFE 身份。每当我们安排一个工作负载时，Pilot 会用包括工作负载的服务账户在内的初始化信息来配置其 sidecar。\n当工作负载旁边的 Envoy 代理启动时，它会联系 Istio 代理并告诉它工作负载的服务账户。代理验证该实例，生成 CSR（证书签名请求），将 CSR 以及工作负载的服务账户证明（在 Kubernetes 中，是 pod 的服务账户 JWT）发送给 Citadel。Citadel 将执行认证和授权，并以签名的 X.509 证书作为回应。Istio 代理从 Citadel 获取响应，将密钥和证书缓存在内存中，并通过 SDS 通过 Unix 域套接字将其提供给 Envoy。将密钥存储在内存中比存储在磁盘上更安全；在使用 SDS 时，Istio 绝不会将任何密钥写入磁盘作为其操作的一部分。Istio 代理还定期刷新凭证，在当前凭证过期前从 Citadel 检索任何新的 SVID（SPIFFE 可验证身份文件）。\n   身份签发流程   SVID 是一个工作负载可以用来向资源或调用者证明其身份的文件。它必须由一个权威机构签署，并包含一个 SPIFFE ID，它代表了提出该文件的服务的身份，例如，spiffe://clusterlocal/ns/my-namespace/sa/my-sa。\n 这种解决方案是可扩展的，因为流程中的每个组件只负责一部分工作。例如，Envoy 负责过期证书，Istio 代理负责生成私钥和 CSR，Citadel 负责授权和签署证书。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"73effe013bfa17ea49adbee2b2fe6de8","permalink":"https://jimmysong.io/docs/istio-handbook/security/authn/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/istio-handbook/security/authn/","section":"istio-handbook","summary":"为了解释什么是认证或 authn，我们将从访问控制试图回答的问","tags":null,"title":"认证","type":"book"},{"authors":null,"categories":null,"content":"日志记录了集群中的活动。审计日志是必要的，这不仅是为了确保服务按预期运行和配置，也是为了确保系统的安全。系统性的审计要求对安全设置进行一致和彻底的检查，以帮助识别潜在威胁。Kubernetes 能够捕获集群操作的审计日志，并监控基本的 CPU 和内存使用信息；然而，它并没有提供深入的监控或警报服务。\n关键点\n 在创建时建立 Pod 基线，以便能够识别异常活动。 在主机层面、应用层面和云端（如果适用）进行日志记录。 整合现有的网络安全工具，进行综合扫描、监控、警报和分析。 设置本地日志存储，以防止在通信失败的情况下丢失。  日志 在 Kubernetes 中运行应用程序的系统管理员应该为其环境建立一个有效的日志、监控和警报系统。仅仅记录 Kubernetes 事件还不足以了解系统上发生的行动的全貌。还应在主机级、应用级和云上（如果适用）进行日志记录。而且，这些日志可以与任何外部认证和系统日志相关联，以提供整个环境所采取的行动的完整视图，供安全审计员和事件响应者使用。\n在 Kubernetes 环境中，管理员应监控 / 记录以下内容：\n API 请求历史 性能指标 部署情况 资源消耗 操作系统调用 协议、权限变化 网络流量  当一个 Pod 被创建或更新时，管理员应该捕获网络通信、响应时间、请求、资源消耗和任何其他相关指标的详细日志以建立一个基线。正如上一节所详述的，匿名账户应被禁用，但日志策略仍应记录匿名账户采取的行动，以确定异常活动。\n应定期审计 RBAC 策略配置，并在组织的系统管理员发生变化时进行审计。这样做可以确保访问控制的调整符合基于角色的访问控制部分中概述的 RBAC 策略加固指导。\n审计应包括将当前日志与正常活动的基线测量进行比较，以确定任何日志指标和事件的重大变化。系统管理员应调查重大变动——例如，应用程序使用的变化或恶意程序的安装，如密码器，以确定根本原因。应该对内部和外部流量日志进行审计，以确保对连接的所有预期的安全限制已被正确配置，并按预期运行。管理员还可以在系统发展过程中使用这些审计，以确定何时不再需要外部访问并可以限制。\n日志可以导向外部日志服务，以确保集群外的安全专业人员的可用使用它们，尽可能接近实时地识别异常情况，并在发生损害时保护日志不被删除。如果使用这种方法，日志应该在传输过程中用 TLS 1.2 或 1.3 进行加密，以确保网络行为者无法在传输过程中访问日志并获得关于环境的宝贵信息。在利用外部日志服务器时，要采取的另一项预防措施是在 Kubernetes 内配置日志转发器，只对外部存储进行追加访问。这有助于保护外部存储的日志不被删除或被集群内日志覆盖。\nKubernetes 原生审计日志配置  Kubernetes 的审计功能默认是禁用的，所以如果没有写审计策略，就不会有任何记录。\n kube-apiserver 驻留在 Kubernetes 控制平面上，作为前端，处理集群的内部和外部请求。每个请求，无论是由用户、应用程序还是控制平面产生的，在其执行的每个阶段都会产生一个审计事件。当审计事件注册时，kube-apiserver 检查审计策略文件和适用规则。如果存在这样的规则，服务器会在第一个匹配的规则所定义的级别上记录该事件。Kubernetes 的内置审计功能默认是不启用的，所以如果没有写审计策略，就不会有任何记录。\n集群管理员必须写一个审计策略 YAML 文件，以建立规则，并指定所需的审计级别，以记录每种类型的审计事件。然后，这个审计策略文件被传递给 kube-apiserver，并加上适当的标志。一个规则要被认为是有效的，必须指定四个审计级别中的一个：none、Meatadataa、Request 或 RequestResponse。附录 L：审计策略展示了一个审计策略文件的内容，该文件记录了 RequestResponse 级别的所有事件。附录 M 向 kube-apiserver 提交审计策略文件的标志示例显示了 kube-apiserver 配置文件的位置，并提供了审计策略文件可以被传递给 kube-apiserver 的标志示例。附录 M 还提供了如何挂载卷和在必要时配置主机路径的指导。\nkube-apiserver 包括可配置的日志和 webhook 后端，用于审计日志。日志后端将指定的审计事件写入日志文件，webhook 后端可以被配置为将文件发送到外部 HTTP API。附录 M 中的例子中设置的 --audit-log-path 和 --audit-log-maxage 标志是可以用来配置日志后端的两个例子，它将审计事件写到一个文件中。log-path 标志是启用日志的最小配置，也是日志后端唯一需要的配置。这些日志文件的默认格式是 JSON，尽管必要时也可以改变。日志后端的其他配置选项可以在 Kubernetes 文档中找到。\n为了将审计日志推送给组织的 SIEM 平台，可以通过提交给 kube-apiserver 的 YAML 文件手动配置 webhook 后端。webhook 配置文件以及如何将该文件传递给 kube-apiserver 可以在附录 N：webhook 配置的示例中查看。关于如何在 kube-apiserver 中为 webhook 后端设置的配置选项的详尽列表，可以在 Kubernetes 文档中找到。\n工作节点和容器的日志记录 在 Kubernetes 架构中，有很多方法可以配置日志功能。在日志管理的内置方法中，每个节点上的 kubelet 负责管理日志。它根据其对单个文件长度、存储时间和存储容量的策略，在本地存储和轮转日志文件。这些日志是由 kubelet 控制的，可以从命令行访问。下面的命令打印了一个 Pod 中的容器的日志。\nkubectl logs [-f] [-p] POD [-c CONTAINER] 如果要对日志进行流式处理，可以使用 -f 标志；如果存在并需要来自容器先前实例的日志，可以使用 -p 标志；如果 Pod 中有多个容器，可以使用 -c 标志来指定一个容器。如果发生错误导致容器、Pod 或节点死亡，Kubernetes 中的本地日志解决方案并没有提供一种方法来保存存储在失败对象中的日志。NSA 和 CISA 建议配置一个远程日志解决方案，以便在一个节点失败时保存日志。\n远程记录的选项包括：\n   远程日志选项 使用的理由 配置实施     在每个节点上运行一个日志代理，将日志推送到后端 赋予节点暴露日志或将日志推送到后端的能力，在发生故障的情况下将其保存在节点之外。 配置一个 Pod 中的独立容器作为日志代理运行，让它访问节点的应用日志文件，并配置它将日志转发到组织的 SIEM。   在每个 Pod 中使用一个 sidecar 容器，将日志推送到一个输出流中 用于将日志推送到独立的输出流。当应用程序容器写入不同格式的多个日志文件时，这可能是一个有用的选项。 为每种日志类型配置 sidecar 容器，并用于将这些日志文件重定向到它们各自的输出流，在那里它们可以被 kubelet 处理。然后，节点级的日志代理可以将这些日志转发给 SIEM 或其他后端。   在每个 Pod 中使用一个日志代理 sidecar，将日志推送到后端 当需要比节点级日志代理所能提供的更多灵活性时。 为每个 Pod 配置，将日志直接推送到后端。这是连接第三方日志代理和后端的常用方法。   从应用程序中直接向后端推送日志 捕获应用程序的日志。Kubernetes 没有内置的机制直接来暴露或推送日志到后端。 各组织将需要在其应用程序中建立这一功能，或附加一个有信誉的第三方工具来实现这一功能。    Sidecar 容器与其他容器一起在 Pod 中运行，可以被配置为将日志流向日志文件或日志后端。Sidecar 容器也可以被配置为作为另一个标准功能容器的流量代理，它被打包和部署。\n为了确保这些日志代理在工作节点之间的连续性，通常将它们作为 DaemonSet 运行。为这种方法配置 DaemonSet，可以确保每个节点上都有一份日志代理的副本，而且对日志代理所做的任何改变在集群中都是一致的。\nSeccomp: 审计模式 除了上述的节点和容器日志外，记录系统调用也是非常有益的。在 Kubernetes 中审计容器系统调用的一种方法是使用安全计算模式（seccomp）工具。这个工具默认是禁用的，但可以用来限制容器的系统调用能力，从而降低内核的攻击面。Seccomp 还可以通过使用审计配置文件记录正在进行的调用。\n自定义 seccomp 配置文件用于定义哪些系统调用是允许的，以及未指定调用的默认动作。为了在 Pod 中启用自定义 seccomp 配置文件，Kubernetes 管理员可以将他们的 seccomp 配置文件 JSON 文件写入到 /var/lib/kubelet/seccomp/ 目录，并将 seccompProfile 添加到 Pod 的 securityContext。自定义的 seccompProfile 还应该包括两个字段。Type: Localhost 和 localhostProfile: myseccomppolicy.json。记录所有的系统调用可以帮助管理员了解标准操作需要哪些系统调用，使他们能够进一步限制 seccomp 配置文件而不失去系统功能。\nSYSLOG Kubernetes 默认将 kubelet 日志和容器运行时日志写入 journald，如果该服务可用的话。如果组织希望对默认情况下不使用的系统使用 syslog 工具，或者从整个集群收集日志并将其转发到 syslog 服务器或其他日志存储和聚合平台，他们可以手动配置该功能。Syslog 协议定义了一个日志信息格式化标准。Syslog 消息包括一个头——由时间戳、主机名、应用程序名称和进程 ID（PID）组成，以及一个以明文书写的消息。Syslog 服务，如 syslog-ng® 和 rsyslog，能够以统一的格式收集和汇总整个系统的日志。许多 Linux 操作系统默认使用 rsyslog 或 journald——一个事件日志守护程序，它优化了日志存储并通过 journalctl 输出 syslog 格式的日志。在运行某些 Linux 发行版的节点上，syslog 工具默认在操作系统层面记录事件。运行这些 Linux 发行版的容器，默认也会使用 syslog 收集日志。由 syslog 工具收集的日志存储在每个适用的节点或容器的本地文件系统中，除非配置了一个日志聚合平台来收集它们。\nSIEM 平台 安全信息和事件管理（SIEM）软件从整个组织的网络中收集日志。SIEM 软件将防火墙日志、应用程序日志等汇集在一起；将它们解析出来，提供一个集中的平台，分析人员可以从这个平台上监控系统安全。SIEM 工具在功能上有差异。一般来说，这些平台提供日志收集、威胁检测和警报功能。有些包括机器学习功能，可以更好地预测系统行为并帮助减少错误警报。在其环境中使用这些平台的组织可以将它们与 Kubernetes 集成，以更好地监测和保护集群。用于管理 Kubernetes 环境中的日志的开源平台是作为 SIEM 平台的替代品存在的。\n容器化环境在节点、Pod、容器和服务之间有许多相互依赖的关系。在这些环境中，Pod 和容器不断地在不同的节点上被关闭和重启。这给传统的 SIEM 带来了额外的挑战，它们通常使用 IP 地址来关联日志。即使是下一代的 SIEM 平台也不一定适合复杂的 Kubernetes 环境。然而，随着 Kubernetes 成为最广泛使用的容器编排平台，许多开发 SIEM 工具的组织已经开发了专门用于 Kubernetes 环境的产品变化，为这些容器化环境提供全面的监控解决方案。管理员应该了解他们平台的能力，并确保他们的日志充分捕捉到环境，以支持未来的事件响应。\n警报 Kubernetes 本身并不支持警报功能；然而， …","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"fcbd55f8feac9da9ecaf4c5e32283c76","permalink":"https://jimmysong.io/docs/kubernetes-hardening-guidance/logging/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/kubernetes-hardening-guidance/logging/","section":"kubernetes-hardening-guidance","summary":"日志记录了集群中的活动。审计日志是必要的，这不仅是为了确保服","tags":null,"title":"日志审计","type":"book"},{"authors":null,"categories":null,"content":"我们可以在虚拟主机和路由层面定义重试策略。在虚拟主机级别设置的重试策略将适用于该虚拟主机的所有路由。如果在路由级别上定义了重试策略，它将优先于虚拟主机策略，并被单独处理——即路由级别的重试策略不会继承虚拟主机级别的重试策略的值。即使 Envoy 将重试策略独立处理，配置也是一样的。\n除了在配置中设置重试策略外，我们还可以通过请求头（即 x-envoy-retry-on 头）进行配置。\n在 Envoy 配置中，我们可以配置以下内容。\n 最大重试次数：Envoy 将重试请求，重试次数最多为配置的最大值。指数退避算法是用于确定重试间隔的默认算法。另一种确定重试间隔的方法是通过 Header（例如 x-envoy-upstream-rq-per-try-timeout-ms）。所有重试也包含在整个请求超时中，即 request_timeout 配置设置。默认情况下，Envoy 将重试次数设置为一次。 重试条件：我们可以根据不同的条件重试请求。例如，我们只能重试 5xx 响应代码，网关失败，4xx 响应代码，等等。 重试预算：重试预算规定了与活动请求数有关的并发请求的限制。这可以帮助防止过大的重试流量。 主机选择重试插件：重试期间的主机选择通常遵循与原始请求相同的过程。使用重试插件，我们可以改变这种行为，指定一个主机或优先级谓词，拒绝一个特定的主机，并导致重新尝试选择主机。  让我们看看几个关于如何定义重试策略的配置例子。我们使用 httpbin 并匹配返回 500 响应代码的 /status/500 路径。\nroute_config:name:5xx_routevirtual_hosts:- name:httpbindomains:[\u0026#34;*\u0026#34;]routes:- match:path:/status/500route:cluster:httpbinretry_policy:retry_on:\u0026#34;5xx\u0026#34;num_retries:5在 retry_policy 字段中，我们将重试条件（retry_on）设置为 500 ，这意味着我们只想在上游返回 HTTP 500 的情况下重试（将会如此）。Envoy 将重试该请求五次。这可以通过 num_retries 字段进行配置。\n如果我们运行 Envoy 并发送一个请求，该请求将失败（HTTP 500），并将创建以下日志条目：\n[2021-07-26T18:43:29.515Z] \u0026#34;GET /status/500 HTTP/1.1\u0026#34; 500 URX 0 0 269 269 \u0026#34;-\u0026#34; \u0026#34;curl/7.64.0\u0026#34; \u0026#34;1ae9ffe2-21f2-43f7-ab80-79be4a95d6d4\u0026#34; \u0026#34;localhost:10000\u0026#34; \u0026#34;127.0.0.1:5000\u0026#34; 注意到 500URX 部分告诉我们，上游响应为 500，URX 响应标志意味着 Envoy 拒绝了该请求，因为达到了上游重试限制。\n重试条件可以设置为一个或多个值，用逗号分隔，如下表所示。\n   重试条件（retry_on） 描述     5xx 在 5xx 响应代码或上游不响应时重试（包括 connect-failure 和 refused-stream）。   gatewayerror 对 502、 503 或响应 504 代码进行重试。   reset 如果上游根本没有回应，则重试。   connect-failure 如果由于与上游服务器的连接失败（例如，连接超时）而导致请求失败，则重试。   envoy-ratelimited 如果存在 x-envoy-ratelimited 头，则重试。   retriable-4xx 如果上游响应的是可收回的 4xx 响应代码（目前只有 HTTP 409），则重试。   refused-stream 如果上游以 REFUSED_STREAM 错误代码重置流，则重试。   retriable-status-codes 如果上游响应的任何响应代码与 x-envoy-retriable-status-codes 头中定义的代码相匹配（例如，以逗号分隔的整数列表，例如 \u0026#34;502,409\u0026#34;），则重试。   retriable-header 如果上游响应包括任何在 x-envoy-retriable-header-names 头中匹配的头信息，则重试。    除了控制 Envoy 重试请求的响应外，我们还可以配置重试时的主机选择逻辑。我们可以指定 Envoy 在选择重试的主机时使用的 retry_host_predicate。\n我们可以跟踪之前尝试过的主机（envoy.retry_host_predicates.previous_host），如果它们已经被尝试过，就拒绝它们。或者，我们可以使用 envoy.retry_host_predicates.canary_hosts  拒绝任何标记为 canary 的主机（例如，任何标记为 canary: true 的主机）。\n例如，这里是如何配置 previous_hosts 插件，以拒绝任何以前尝试过的主机，并重试最多 5 次的主机选择。\nroute_config:name:5xx_routevirtual_hosts:- name:httpbindomains:[\u0026#34;*\u0026#34;]routes:- match:path:/status/500route:cluster:httpbinretry_policy:retry_host_predicate:- name:envoy.retry_host_predicates.previous_hostshost_selection_retry_max_attempts:5在集群中定义了多个端点，我们会看到每次重试都会发送到不同的主机上。\n请求对冲 请求对冲背后的想法是同时向不同的主机发送多个请求，并使用首先响应的上游的结果。请注意，我们通常为幂等的请求配置这个功能，在这种情况下，多次进行相同的调用具有相同的效果。\n我们可以通过指定一个对冲策略来配置请求的对冲。目前，Envoy 只在响应请求超时的情况下进行对冲。因此，当一个初始请求超时时，会发出一个重试请求，而不取消原来超时的请求。Envoy 将根据重试策略向下游返回第一个良好的响应。\n可以通过设置 hedge_on_per_try_timeout 字段为 true 来配置对冲。就像重试策略一样，它可以在虚拟主机或路由级别上启用。\nroute_config:name:5xx_routevirtual_hosts:- name:httpbindomains:[\u0026#34;*\u0026#34;]hedge_policy:hedge_on_per_try_timeout:trueroutes:- match:... ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"1292efde041b8a65e56bc743d1de5b32","permalink":"https://jimmysong.io/docs/envoy-handbook/hcm/retries/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/envoy-handbook/hcm/retries/","section":"envoy-handbook","summary":"我们可以在虚拟主机和路由层面定义重试策略。在虚拟主机级别设置","tags":null,"title":"重试","type":"book"},{"authors":null,"categories":null,"content":"OpenTelemetry 是一个大型项目。OpenTelemetry 项目的工作被划分为特殊兴趣小组（SIG）。虽然所有的项目决策最终都是通过 GitHub issue 和 pull request 做出的，但 SIG 成员经常通过 CNCF 的官方 Slack 保持联系，而且大多数 SIG 每周都会在 Zoom 上会面一次。\n任何人都可以加入一个 SIG。要想了解更多关于当前 SIG、项目成员和项目章程的细节，请查看 GitHub 上的 OpenTelemetry 社区档案库。\n规范 OpenTelemetry 是一个规范驱动的项目。OpenTelemetry 技术委员会负责维护该规范，并通过管理规范的 backlog 来指导项目的发展。\n小的改动可以以 GitHub issue 的方式提出，随后的 pull request 直接提交给规范。但是，对规范的重大修改是通过名为 OpenTelemetry Enhancement Proposals（OTEPs）的征求意见程序进行的。\n任何人都可以提交 OTEP。OTEP 由技术委员会指定的具体审批人进行审查，这些审批人根据其专业领域进行分组。OTEP 至少需要四个方面的批准才能被接受。在进行任何批准之前，通常需要详细的设计，以及至少两种语言的原型。我们希望 OTEP 的作者能够认真对待其他社区成员的要求和关注。我们的目标是确保 OpenTelemetry 适合尽可能多的受众的需求。\n一旦被接受，将根据 OTEP 起草规范变更。由于大多数问题已经在 OTEP 过程中得到了解决，因此规范变更只需要两次批准。\n项目治理 管理 OpenTelemetry 项目如何运作的规则和组织结构由 OpenTelemetry 治理委员会定义和维护，其成员经选举产生，任期两年。\n治理成员应以个人身份参与，而不是公司代表。但是，为同一雇主工作的委员会成员的数量有一个上限。如果因为委员会成员换了工作而超过了这个上限，委员会成员必须辞职，直到雇主代表的人数降到这个上限以下。\n发行版 OpenTelemetry 有一个基于插件的架构，因为有些观察能力系统需要一套插件和配置才能正常运行。\n发行版（distros）被定义为广泛使用的 OpenTelemetry 插件的集合，加上一组脚本或辅助功能，可能使 OpenTelemetry 与特定的后端连接更简单，或在特定环境中运行 OpenTelemetry。\n需要澄清的是，如果一个可观测性系统声称它与 OpenTelemetry 兼容，那么它应该总是可以使用 OpenTelemetry 而不需要使用某个特定的发行版。如果一个项目没有经过规范过程就扩展了 OpenTelemetry 的核心功能，或者包括任何导致它与上游 OpenTelemetry 仓库不兼容的变化，那么这个项目就是一个分叉，而不是一个发行版。\n注册表 为了便于发现目前有哪些语言、插件和说明，OpenTelemetry 提供了一个注册表。任何人都可以向 OpenTelemetry 注册表提交插件；在 OpenTelemetry 的 GitHub 组织内托管插件不是必须的。\n","date":1643990400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"21db822b2cec39856e2381ca897da87d","permalink":"https://jimmysong.io/docs/opentelemetry-obervability/organization/","publishdate":"2022-02-05T00:00:00+08:00","relpermalink":"/docs/opentelemetry-obervability/organization/","section":"opentelemetry-obervability","summary":"附录 A：OpenTelemetry 项目组织","tags":null,"title":"附录 A：OpenTelemetry 项目组织","type":"book"},{"authors":null,"categories":null,"content":"使用路由级别的请求镜像策略（request_mirroring_policies），我们可以配置 Envoy 将流量从一个集群镜像到另一个集群。\n流量镜像或请求镜像是指当传入的请求以一个集群为目标时，将其复制并发送给第二个集群。镜像的请求是 “发射并遗忘” 的，这意味着 Envoy 在发送主集群的响应之前不会等待影子集群的响应。\n请求镜像模式不会影响发送到主集群的流量，而且因为 Envoy 会收集影子集群的所有统计数据，所以这是一种有用的测试技术。\n除了 “发送并遗忘” 之外，还要确保你所镜像的请求是空闲的。否则，镜像请求会扰乱你的服务与之对话的后端。\n影子请求中的 authority/host 头信息将被添加 -shadow字符串。\n为了配置镜像策略，我们在要镜像流量的路由上使用 request_mirror_policies 字段。我们可以指定一个或多个镜像策略，以及我们想要镜像的流量的部分。\nroute_config:name:my_routevirtual_hosts:- name:httpbindomains:[\u0026#34;*\u0026#34;]routes:- match:prefix:/route:cluster:httpbinrequest_mirror_policies:cluster:mirror_httpbinruntime_fraction:default_value:numerator:100...上述配置将 100% 地接收发送到集群 httpbin 的传入请求，并将其镜像到mirror_httpbin。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"5b402661c10936d910a40644987df59a","permalink":"https://jimmysong.io/docs/envoy-handbook/hcm/request-mirroring/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/envoy-handbook/hcm/request-mirroring/","section":"envoy-handbook","summary":"使用路由级别的请求镜像策略（request_mirrorin","tags":null,"title":"请求镜像","type":"book"},{"authors":null,"categories":null,"content":"遵循本文件中概述的加固指南是确保在 Kubernetes 协调容器上运行的应用程序安全的一个步骤。然而，安全是一个持续的过程，跟上补丁、更新和升级是至关重要的。具体的软件组件因个人配置的不同而不同，但整个系统的每一块都应尽可能保持安全。这包括更新：Kubernetes、管理程序、虚拟化软件、插件、环境运行的操作系统、服务器上运行的应用程序，以及 Kubernetes 环境中托管的任何其他软件。\n互联网安全中心（CIS）发布了保护软件安全的基准。管理员应遵守 Kubernetes 和任何其他相关系统组件的 CIS 基准。管理员应定期检查，以确保其系统的安全性符合当前安全专家对最佳实践的共识。应定期对各种系统组件进行漏洞扫描和渗透测试，主动寻找不安全的配置和零日漏洞。任何发现都应在潜在的网络行为者发现和利用它们之前及时补救。\n随着更新的部署，管理员也应该跟上从环境中删除任何不再需要的旧组件。使用托管的 Kubernetes 服务可以帮助自动升级和修补 Kubernetes、操作系统和网络协议。然而，管理员仍然必须为他们的容器化应用程序打补丁和升级。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"5d33bbade4d5d8b49347e24dda4cfa79","permalink":"https://jimmysong.io/docs/kubernetes-hardening-guidance/upgrading-and-application-security-practices/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/kubernetes-hardening-guidance/upgrading-and-application-security-practices/","section":"kubernetes-hardening-guidance","summary":"遵循本文件中概述的加固指南是确保在 Kubernetes 协调容器上运行的应用程序","tags":null,"title":"升级和应用安全实践","type":"book"},{"authors":null,"categories":null,"content":"路线图很快就会过时。有关最新的路线图，请参见 OpenTelemetry 状态页面。也就是说，以下是截至目前 OpenTelemetry 的状态。\n核心组件 目前，OpenTelemetry 追踪信号已被宣布为稳定的，并且在许多语言中都有稳定的实现。\n度量信号刚刚被宣布稳定，测试版的实施将在 2022 年第一季度广泛使用。\n日志信号预计将在 2022 年第一季度宣布稳定，测试版实现预计将在 2022 年第二季度广泛使用。2021 年，Stanza 项目被捐赠给 OpenTelemetry，为 OpenTelemetry 收集器增加了高效的日志处理能力。\n用于 HTTP/RPC、数据库和消息系统的语义公约预计将在 2022 年第一季度宣布稳定。\n未来 完成上述路线图就完成了对 OpenTelemetry 核心功能的稳定性要求。这是一个巨大的里程碑，因为它打开了进一步采用 OpenTelemetry 的大门，包括与数据库、管理服务和 OSS 库的原生集成。这些集成工作大部分已经以原型和测试版支持的形式在进行。随着 OpenTelemetry 的稳定性在 2022 年上半年完成，我预计在 2022 年下半年将出现支持 OpenTelemetry 的宣言，使 2022 年成为 “OpenTelemetry 之年”。\n但我们并没有就此止步。下一步是什么？\neBPF Extended Berkeley Packet Filter（eBPF）是一种在 Windows、Linux 和其他类似 Unix 的操作系统上提供底层网络访问的机制。利用 eBPF 将给 OpenTelemetry 提供一种极其有效的网络监控形式，不需要任何开发人员的工具。\n2021 年，Flowmill 项目被捐赠给了 OpenTelemetry。Flowmill 是一个基于 eBPF 的可观测性解决方案，专门设计用于观测分布式系统。Flowmill 的开发者与 Pixie 项目的开发者一起，正在努力为 OpenTelemetry Collector 增加 eBPF 支持。Pixie 是专门为 Kubernetes 设计的基于 eBPF 的观测工具，是 CNCF 旗下 OpenTelemetry 的一个姊妹项目。我们还在一起研究如何将底层（第 2 层）eBPF 数据与高层（第 7 层）分布式追踪数据进一步关联起来，这在业界尚属首次。\nRUM 真实用户监控（RUM）是一种可观测性工具，用于描述用户在长期运行的用户会话中如何与移动、网络和桌面客户端进行交互。RUM 与分布式追踪不同，因为图形用户界面（GUI）往往是基于反应器的系统，与数据库和网络服务器等基于事务的系统有根本的架构差异。长话短说：你不能把一个用户会话建模为一个追踪，然后就收工了。\nClient Instrumentation SIG 目前正在开发一个新的 RUM 设计，它将扩展并与 OpenTelemetry 现有的分布式追踪、指标和日志信号完全整合。\nOpenTelemetry 控制平面 目前，OpenTelemetry 收集器和 SDK 是作为独立的单元来管理的，必须重新启动才能改变它们的配置。目前，Agent Management SIG 正在开发一个控制平面，它可以报告这些组件的当前状态，并允许实时改变配置。它将允许运维人员或自动服务动态地控制整个遥测管道的处理。\n动态配置将允许对采样、日志水平和其他形式的资源管理进行细粒度、反应灵敏的控制，从而减少成本。最终，这个控制平面可以实现更先进的基于尾部的采样形式，即需要在整个部署中协调的采样技术。\n列式编码的 OTLP 目前的 OpenTelemetry 协议是对 OpenTelemetry 数据模型的一种有效而直接的编码。跨度、度量和日志被编码为跨度、度量和日志。这可以被认为是一个基于行的模型，每个跨度、度量和日志都被编码为元素列表中的一个离散元素。基于列的模型则将每个元素编码为一个单一的表格，其中所有元素共享相同的列，用于重叠的概念，如时间戳和属性。\n这种列式表示法有望优化数据批次的创建、大小和处理。这种方法的主要好处是：\n 更好的数据压缩率（一组相似的数据）。 更快的数据处理（更好的数据定位意味着更好地使用 CPU 缓存线）。 更快的序列化和反序列化（更少的对象需要处理）。 更快的批量创建（更少的内存分配）。 更好的 I/O 效率（传输的数据更少）。  这种方法的好处与批次的大小成比例地增加。使用现有的 “面向行” 的表示方法很适合小批量的情况。因此，列式编码将扩展当前的协议。目前提出的实施方案是基于 Apache Arrow，这是一种成熟的列式内存格式。\n这种优化的重点是允许高容量的数据源，如 CDN 和大型多租户系统，像常规服务一样参与可观测性。列式编码也将减少跨网络边界的遥测出口的成本。\n","date":1643990400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"603d251048616273c024bf6ba74c37ec","permalink":"https://jimmysong.io/docs/opentelemetry-obervability/roadmap/","publishdate":"2022-02-05T00:00:00+08:00","relpermalink":"/docs/opentelemetry-obervability/roadmap/","section":"opentelemetry-obervability","summary":"附录 B：OpenTelemetry 项目路线图","tags":null,"title":"附录 B：OpenTelemetry 项目路线图","type":"book"},{"authors":null,"categories":null,"content":"下面的例子是一个 Dockerfile，它以非 root 用户和非 group 成员身份运行一个应用程序。\nFROMubuntu:latest# 升级和安装 make 工具RUN apt update \u0026amp;\u0026amp; apt install -y make# 从一个名为 code 的文件夹中复制源代码，并使用 make 工具构建应用程序。COPY ./codeRUN make /code# 创建一个新的用户（user1）和新的组（group1）；然后切换到该用户的上下文中。RUN useradd user1 \u0026amp;\u0026amp; groupadd group1USERuser1:group1# 设置容器的默认入口CMD /code/app","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"5f6c69d5c0dbe72fb20fea7ac4ffca37","permalink":"https://jimmysong.io/docs/kubernetes-hardening-guidance/appendix/a/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/kubernetes-hardening-guidance/appendix/a/","section":"kubernetes-hardening-guidance","summary":"下面的例子是一个 Dockerfile，它以非 root 用户和非 group 成员","tags":null,"title":"附录 A：非 root 应用的 Dockerfile 示例","type":"book"},{"authors":null,"categories":null,"content":"速率限制是一种限制传入请求的策略。它规定了一个主机或客户端在一个特定的时间范围内发送多少次请求。一旦达到限制，例如每秒 100 个请求，我们就说发送请求的客户端受到速率限制。任何速率受限的请求都会被拒绝，并且永远不会到达上游服务。稍后，我们还将讨论可以使用的断路器，以及速率限制如何限制上游的负载并防止级联故障。\nEnvoy 支持全局（分布式）和局部（非分布式）的速率限制。\n全局和局部速率限制的区别在于，我们要用全局速率限制来控制对一组在多个 Envoy 实例之间共享的上游的访问。例如，我们想对一个叫做多个 Envoy 代理的数据库的访问进行速率限制。另一方面，局部速率限制适用于每一个 Envoy 实例。\n   全局和局部速率限制  局部和全局速率限制可以一起使用，Envoy 分两个阶段应用它们。首先，应用局部速率限制，然后是全局速率限制。\n我们将在接下来的章节中深入研究全局和局部速率限制，并解释这两种情况如何工作。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"a16fc821756404b66812c604848a3c32","permalink":"https://jimmysong.io/docs/envoy-handbook/hcm/rate-limiting-intro/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/envoy-handbook/hcm/rate-limiting-intro/","section":"envoy-handbook","summary":"速率限制是一种限制传入请求的策略。它规定了一个主机或客户端在","tags":null,"title":"速率限制","type":"book"},{"authors":null,"categories":null,"content":"下面是一个使用只读根文件系统的 Kubernetes 部署模板的例子。\napiVersion:apps/v1kind:Deploymentmetadata:labels:app:webname:webspec:selector:matchLabels:app:webtemplate:metadata:labels:app:webname:webspec:containers:- command:[\u0026#34;sleep\u0026#34;]args:[\u0026#34;999\u0026#34;]image:ubuntu:latestname:websecurityContext:readOnlyRootFilesystem:true#使容器的文件系统成为只读volumeMounts:- mountPath:/writeable/location/here#创建一个可写卷name:volNamevolumes:- emptyDir:{}name:volName","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"d80d3219952633092d6ce3257dae19d0","permalink":"https://jimmysong.io/docs/kubernetes-hardening-guidance/appendix/b/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/kubernetes-hardening-guidance/appendix/b/","section":"kubernetes-hardening-guidance","summary":"下面是一个使用只读根文件系统的 Kubernetes 部署模板的例子。 apiVersion:apps/v1kind:Deploymentmetadata:labels:app:webname:webspec:selector:matchLabels:app:webtemplate:metadata:labels:app:webname:webspec:containers:- comma","tags":null,"title":"附录 B：只读文件系统的部署模板示例","type":"book"},{"authors":null,"categories":null,"content":"局部速率限制过滤器对过滤器链处理的传入连接应用一个令牌桶速率限制。\n令牌桶算法的基础是令牌在桶中的类比。桶里的令牌以一个固定的速度被重新填满。每次收到一个请求或连接时，我们都会检查桶里是否还有令牌。如果有，就从桶中取出一个令牌，然后处理该请求。如果没有剩余的令牌，该请求就会被放弃（即速率限制）。\n   令牌桶算法  局部速率限制可以在监听器层面或虚拟主机或路由层面进行全局配置，就像全局速率限制一样。我们还可以在同一配置中结合全局和局部速率限制。\ntoken_bucket 指定了过滤器处理的请求所使用的配置。它包括桶可以容纳的最大令牌数量（max_tokens），每次填充的令牌数量（tokens_per_refill）以及填充间隔（fill_interval）。\n下面是一个最多可以容纳 5000 个令牌的桶的配置实例。每隔 30 秒，向桶中添加 100 个令牌。桶中的令牌容量永远不会超过 5000。\ntoken_bucket:max_tokens:5000tokens_per_fill:100fill_interval:30s为了控制令牌桶是在所有 worker 之间共享（即每个 Envoy 进程）还是按连接使用，我们可以设置 local_rate_limit_per_downstream_connection 字段。默认值是 false，这意味着速率限制被应用于每个 Envoy 进程。\n控制是否启用或强制执行某一部分请求的速率限制的两个设置被称为 filter_enabled 和 filter_enforced。这两个值在默认情况下都设置为 0%。\n速率限制可以被启用，但不一定对一部分请求强制执行。例如，我们可以对 50% 的请求启用速率限制。然后，在这 50% 的请求中，我们可以强制执行速率限制。\n   启用速率限制  以下配置对所有传入的请求启用并执行速率限制。\ntoken_bucket:max_tokens:5000tokens_per_fill:100fill_interval:30sfilter_enabled:default_value:numerator:100denominator:HUNDREDfilter_enforced:default_value:numerator:100denominator:HUNDRED我们还可以为限制速率的请求添加请求和响应头信息。我们可以在 request_headers_to_add_when_not_enforced 字段中提供一个头信息列表，Envoy 将为每个转发到上游的限速请求添加一个请求头信息。请注意，这只会在过滤器启用但未强制执行时发生。\n对于响应头信息，我们可以使用 response_headers_to_add  字段。我们可以提供一个 Header 的列表，这些 Header 将被添加到已被限制速率的请求的响应中。这只有在过滤器被启用或完全强制执行时才会发生。\n如果我们在前面的例子的基础上，这里有一个例子，说明如何在所有速率限制的请求中添加特定的响应头。\ntoken_bucket:max_tokens:5000tokens_per_fill:100fill_interval:30sfilter_enabled:default_value:numerator:100denominator:HUNDREDfilter_enforced:default_value:numerator:100denominator:HUNDREDresponse_headers_to_add:- append:falseheader:key:x-local-rate-limitvalue:\u0026#39;true\u0026#39;我们可以配置局部速率限制器，使所有虚拟主机和路由共享相同的令牌桶。为了在全局范围内启用局部速率限制过滤器（不要与全局速率限制过滤器混淆），我们可以在 http_filters 列表中为其提供配置。\n例如：\n...http_filters:- name:envoy.filters.http.local_ratelimittyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions. filters.http.local_ratelimit.v3.LocalRateLimitstat_prefix:http_local_rate_limitertoken_bucket:max_tokens:10000...- name:envoy.filters.http.router...如果我们想启用每个路由的局部速率限制，我们仍然需要将过滤器添加到 http_filters 列表中，而不需要任何配置。然后，在路由配置中，我们可以使用 typed_per_filter_config 并指定局部速率限制的过滤器配置。\n例如：\n...route_config:name:my_routevirtual_hosts:- name:my_servicedomains:[\u0026#34;*\u0026#34;]routes:- match:prefix:/route:cluster:some_clustertyped_per_filter_config:envoy.filters.http.local_ratelimit:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.http.local_ratelimit.v3.LocalRateLimittoken_bucket:max_tokens:10000tokens_per_fill:1000fill_interval:1sfilter_enabled:default_value:numerator:100denominator:HUNDREDfilter_enforced:default_value:numerator:100denominator:HUNDREDhttp_filters:- name:envoy.filters.http.local_ratelimittyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.http.local_ratelimit.v3.LocalRateLimitstat_prefix:http_local_rate_limiter- name:envoy.filters.http.router上述配置在 http_filter 列表中加载了局部速率限制过滤器。我们在路由配置中使用 typed_per_filter_config 来配置它，并以 envoy.filters.http.local_ratelimit 的名字来引用这个过滤器。\n使用描述符进行局部速率限制 就像我们在做全局速率限制时使用描述符一样，我们也可以把它们用于局部每条路由的速率限制。我们需要配置两个部分：路由上的操作和局部速率限制过滤器配置中的描述符列表。\n我们可以用为全局速率限制定义动作的方式为局部速率限制定义动作。\n...route_config:name:my_routevirtual_hosts:- name:my_servicedomains:[\u0026#34;*\u0026#34;]routes:- match:prefix:/route:cluster:some_clusterrate_limits:- actions:- header_value_match:descriptor_value:post_requestheaders:- name:\u0026#34;:method\u0026#34;exact_match:POST- header_value_match:descriptor_value:get_requestheaders:- name:\u0026#34;:method\u0026#34;exact_match:GET...第二部分是编写配置以匹配生成的描述符，并提供令牌桶信息。这在 `descriptors 字段下的速率限制过滤器配置中得到完成。\n例如：\ntyped_per_filter_config:envoy.filters.http.local_ratelimit:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.http.local_ratelimit.v3.LocalRateLimitstat_prefix:some_stat_prefixtoken_bucket:max_tokens:1000tokens_per_fill:1000fill_interval:60sfilter_enabled:...filter_enforced:...descriptors:- entries:- key:header_matchvalue:post_requesttoken_bucket:max_tokens:20tokens_per_fill:5fill_interval:30s- entries:- key:header_matchvalue:get_requesttoken_bucket:max_tokens:50tokens_per_fill:5fill_interval:20s...对于所有的 POST 请求（即 （\u0026#34;header_match\u0026#34;: \u0026#34;post_request\u0026#34;）），桶被设置为 20 个令牌，它每 30 秒重新填充 5 个令牌。对于所有的 GET 请求，该桶最多可以容纳 50 个令牌，每 20 秒重新填充 5 个令牌。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"f23ee906424b310ce5cf30c9f242cdec","permalink":"https://jimmysong.io/docs/envoy-handbook/hcm/local-rate-limiting/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/envoy-handbook/hcm/local-rate-limiting/","section":"envoy-handbook","summary":"局部速率限制过滤器对过滤器链处理的传入连接应用一个令牌桶速率","tags":null,"title":"局部速率限制","type":"book"},{"authors":null,"categories":null,"content":"当许多主机向少数上游服务器发送请求，且平均延迟较低时，全局或分布式速率限制很有用。\n   许多主机上少许上游服务器发送请求  由于服务器不能快速处理这些请求，请求就会变得滞后。在这种情况下，众多的下游主机可以压倒少数的上游主机。全局速率限制器有助于防止级联故障。\n   速率限制  Envoy 与任何实现定义的 RPC/IDL 协议的外部速率限制服务集成。该服务的参考实现使用 Go、gRPC 和 Redis 作为其后端，可参考这里。\nEnvoy 调用外部速率限制服务（例如，在 Redis 中存储统计信息并跟踪请求），以得到该请求是否应该被速率限制的响应。\n使用外部速率限制服务，我们可以将限制应用于一组服务，或者，如果我们谈论的是服务网格，则应用于网格中的所有服务。我们可以控制进入网格的请求数量，作为一个整体。\n为了控制单个服务层面的请求率，我们可以使用局部速率限制器。局部速率限制器允许我们对每个服务有单独的速率限制。局部和全局速率限制通常是一起使用的。\n配置全局速率限制 在配置全局速率限制时，我们必须设置两个部分——客户端（Envoy）和服务器端（速率限制服务）。\n我们可以在 Envoy 侧将速率限制服务配置为网络级速率限制过滤器或 HTTP 级速率限制过滤器。\n当在网络层面上使用速率限制过滤器时，Envoy 对我们配置了过滤器的监听器的每个新连接调用速率限制服务。同样，使用 HTTP 级别的速率限制过滤器，Envoy 对安装了过滤器的监听器上的每个新请求调用速率限制服务，并且路由表指定应调用全局速率限制服务。所有到目标上游集群的请求以及从发起集群到目标集群的请求都可以被限制速率。\n在配置速率限制服务之前，我们需要解释动作、描述符（键 / 值对）和描述符列表的概念。\n   速率限制概念  在路由或虚拟主机层面的 Envoy 配置中，我们定义了一组动作。每个动作都包含一个速率限制动作的列表。让我们考虑下面的例子，在虚拟主机级别上定义速率限制。\nrate_limits:- actions:- header_value_match:descriptor_value:get_requestheaders:- name::methodprefix_match:GET- header_value_match:descriptor_value:pathheaders:- name::pathprefix_match:/api- actions:- header_value_match:descriptor_value:post_requestheaders:- name::methodprefix_match:POST- actions:- header_value_match:descriptor_value:get_requestheaders:- name::methodprefix_match:GET上面的片段定义了三个独立的动作，其中包含速率限制动作。Envoy 将尝试将请求与速率限制动作相匹配，并生成描述符发送到速率限制服务。如果 Envoy 不能将任何一个速率限制动作与请求相匹配，则不会创建描述符 ——即所有速率限制动作都必须相匹配。\n例如，如果我们收到一个到 /api 的 GET 请求，第一个动作与两个速率限制动作相匹配；因此会创建一个如下描述符。\n(\u0026#34;header_match\u0026#34;: \u0026#34;get_request\u0026#34;), (\u0026#34;header_match\u0026#34;: \u0026#34;path\u0026#34;) 第二个动作是不会匹配的。然而，最后一个也会匹配。因此，Envoy 将向速率限制服务发送以下描述符。\n(\u0026#34;header_match\u0026#34;: \u0026#34;get_request\u0026#34;), (\u0026#34;header_match\u0026#34;: \u0026#34;path\u0026#34;) (\u0026#34;header_match\u0026#34;: \u0026#34;get_request\u0026#34;) 让我们来看看另一个满足以下要求的客户端配置的例子。\n 对 /users 的 POST 请求被限制在每分钟 10 个请求。 对 /users 的请求被限制在每分钟 20 个请求。 带有 dev: true 头的 /api 请求被限制在每秒 10 个请求的速率。 向 /api 发出的带有 dev: false 头的请求被限制在每秒 5 个请求。 对 /api 的任何其他请求都没有速率限制。  请注意，这次我们是在路由层面上定义速率限制。\nroutes:- match:prefix:\u0026#34;/users\u0026#34;route:cluster:some_clusterrate_limits:- actions:- generic_key:descriptor_value:users- header_value_match:descriptor_value:post_requestheaders:- name:\u0026#34;:method\u0026#34;exact_match:POST- actions:- generic_key:descriptor_value:users- match:prefix:\u0026#34;/api\u0026#34;route:cluster:some_clusterrate_limits:- actions:- generic_key:descriptor_value:api- request_headers:header_name:devdescriptor_key:dev_request...http_filters:- name:envoy.filters.http.ratelimittyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.http.ratelimit.v3.RateLimitdomain:some_domainenable_x_ratelimit_headers:DRAFT_VERSION_03rate_limit_service:transport_api_version:V3grpc_service:envoy_grpc:cluster_name:rate-limit-cluster上述配置包含每条路由的 rate_limits 配置和 envoy. filters.http.ratelimit 过滤器配置。过滤器的配置指向速率限制服务的上游集群。我们还设置了域名（domain）和 enabled_x_ratelimit_headers  字段，指定我们要使用 x-ratelimit  头。我们可以按任意的域名来隔离一组速率限制配置。\n如果我们看一下路由中的速率限制配置，注意到我们是如何拆分动作以匹配我们想要设置的不同速率限制的。例如，我们有一个带有 api 通用密钥和请求头的动作。然而，在同一个配置中，我们也有一个只设置了通用密钥的动作。这使得我们可以根据这些动作配置不同的速率限制。\n让我们把动作翻译成描述符。\nGET /users --\u0026gt; (\u0026#34;generic_key\u0026#34;: \u0026#34;users\u0026#34;) POST /users --\u0026gt; (\u0026#34;generic_key\u0026#34;: \u0026#34;users\u0026#34;), (\u0026#34;header_match\u0026#34;: \u0026#34;post_request\u0026#34;) GET /api dev: some_header_value --\u0026gt; (\u0026#34;generic_key\u0026#34;: \u0026#34;api\u0026#34;), (\u0026#34;dev_request\u0026#34;: \u0026#34;some_header_value\u0026#34;) header_match 和 request_headers 之间的区别是，对于后者，我们可以根据特定的头信息值来创建速率限制（例如，dev: true 或 dev: something，因为头信息的值成为描述符的一部分）。\n在速率限制服务方面，我们需要开发一个配置，根据 Envoy 发送的描述符来指定速率限制。\n例如，如果我们向 /users 发送一个 GET 请求，Envoy 会向速率限制服务发送以下描述符。(\u0026#34;generic_key\u0026#34;: \u0026#34;users\u0026#34;)。然而，如果我们发送一个 POST 请求，描述符列表看起来像这样。\n(\u0026#34;generic_key\u0026#34;: \u0026#34;users\u0026#34;), (\u0026#34;header_match\u0026#34;: \u0026#34;post_request\u0026#34;) 速率限制服务配置是分层次的，允许匹配嵌套描述符。让我们看看上述描述符的速率限制服务配置会是什么样子。\ndomain:some_domaindescriptors:- key:generic_keyvalue:usersrate_limit:unit:MINUTErequests_per_unit:20descriptors:- key:header_matchvalue:post_requestrate_limit:unit:MINUTErequests_per_unit:10- key:generic_keyvalue:apidescriptors:- key:dev_requestvalue:truerate_limit:unit:SECONDrequests_per_unit:10- key:dev_requestvalue:falserate_limit:unit:SECONDrequests_per_unit:5我们之前在 Envoy 方面的配置中提到了 domain 值。现在我们可以看看如何使用域名。我们可以在整个代理机群中使用相同的描述符名称，但要用域名来分隔它们。\n让我们看看速率限制服务上的匹配对不同请求是如何工作的。\n   收到的请求 生成的描述符 速率限制 解释     GET /users (\u0026#34;generic_key\u0026#34;: \u0026#34;users\u0026#34;) 20 req/min 键 users 与配置中的第一层相匹配。由于配置中的第二层（header_match）没有包括在描述符中，所以使用了 user` 键的速率限制。    POST /users (\u0026#34;generic_key\u0026#34;: \u0026#34;users\u0026#34;), (\u0026#34;header_match\u0026#34;: \u0026#34;post_request\u0026#34;) 10 req/min 发送的描述符和 header_match 一样匹配用户，所以使用 header_match描述符下的速率限制。   GET /api (\u0026#34;generic_key\u0026#34;: \u0026#34;api\u0026#34;) 无速率限制 我们只有 api描述符的第一级匹配。然而，并没有配置速率限制。为了执行速率限制，我们需要第二级描述符，这些描述符只有在传入的请求中存在 Headerdev 时才会被设置。   GET /apidev: true (\u0026#34;generic_key\u0026#34;: \u0026#34;api\u0026#34;), (\u0026#34;dev_request\u0026#34;: \u0026#34;true\u0026#34;) 10 req/second 列表中的第二个描述符与配置中的第二层相匹配（即我们匹配 api，然后也匹配 dev_request：true）。    GET /apidev: false (\u0026#34;generic_key\u0026#34;: \u0026#34;api\u0026#34;), (\u0026#34;dev_request\u0026#34;: \u0026#34;false\u0026#34;) 5req/second 列表中的第二个描述符与配置中的第二层相匹配（即我们匹配 api，然后也匹配 dev_request：true）。   GET /apidev: hello (\u0026#34;generic_key\u0026#34;: \u0026#34;api\u0026#34;), (\u0026#34;dev_request\u0026#34;: \u0026#34;hello\u0026#34;) 无速率限制 列表中的第二个描述符与配置中的任何二级描述符都不匹配。    除了我们在上面的例子中使用的动作外，下表显示了我们可以用来创建描述符的其他动作。\n   动作名称 描述     source_cluster 源集群的速率限制   destination_cluster 目的地集群的速率限制   request_headers 对请求头的速率限制   remote_address 远程地址的速率限制   generic_key 对一个通用键的速率限制   header_value_match 对请求头的存在进行速率限制   metadata 元数据的速率限制    下图总结了这些操作与描述符和速率限制服务上的实际速率限制配 …","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"bd2a5389138ef6bae5a2972a61509134","permalink":"https://jimmysong.io/docs/envoy-handbook/hcm/global-rate-limiting/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/envoy-handbook/hcm/global-rate-limiting/","section":"envoy-handbook","summary":"当许多主机向少数上游服务器发送请求，且平均延迟较低时，全局或","tags":null,"title":"全局速率限制","type":"book"},{"authors":null,"categories":null,"content":"在这个实验中，我们将学习如何配置使用不同的方式来匹配请求。我们将使用 direct_response，我们还不会涉及任何 Envoy 集群。\n路径匹配 以下是我们想放入配置中的规则：\n 所有请求都需要来自 hello.io 域名（即在向代理发出请求时，我们将使用 Host: hello.io 标头） 所有向路径 /api 发出的请求将返回字符串 hello - path 所有向根路径（即 /）发出的请求将返回字符串 hello - prefix 所有以 /hello 开头并在后面加上数字的请求（如 /hello/1，/hello/523）都应返回 hello - regex字符串  让我们看一下配置：\nstatic_resources:listeners:- name:listener_0address:socket_address:address:0.0.0.0port_value:10000filter_chains:- filters:- name:envoy.filters.network.http_connection_managertyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManagerstat_prefix:hello_servicehttp_filters:- name:envoy.filters.http.routerroute_config:name:routevirtual_hosts:- name:hello_vhostdomains:[\u0026#34;hello.io\u0026#34;]routes:- match:path:\u0026#34;/api\u0026#34;direct_response:status:200body:inline_string:\u0026#34;hello - path\u0026#34;- match:safe_regex:google_re2:{}regex:^/hello/\\d+$direct_response:status:200body:inline_string:\u0026#34;hello - regex\u0026#34;- match:prefix:\u0026#34;/\u0026#34;direct_response:status:200body:inline_string:\u0026#34;hello - prefix\u0026#34;由于我们只有一个域名，我们将使用一个单一的虚拟主机。域名数组将包含一个单一的域名：hello.io。这是 Envoy 要做的第一层匹配。\n然后，虚拟主机将有多个路径匹配。首先，我们将使用路径匹配，因为我们想准确匹配 /api 路径。其次，我们使用 ^/hello/\\d+$ 正则表达式来定义正则表达式匹配。最后，我们定义前缀匹配。注意，定义这些匹配的顺序很重要。如果我们把前缀匹配放在最前面，那么其余的匹配就不会被评估，因为前缀匹配永远是真的。\n将上述 YAML 保存为 2-lab-1-request-matching-1.yaml，然后运行 func-e run -c 2-lab-1-request-matching-1.yaml 来启动 Envoy 代理。\n从另一个单独的终端，我们可以进行一些测试调用。\n$ curl -H \u0026#34;Host: hello.io\u0026#34; localhost:10000 hello - prefix $ curl -H \u0026#34;Host: hello.io\u0026#34; localhost:10000/api hello - path $ curl -H \u0026#34;Host: hello.io\u0026#34; localhost:10000/hello/123 hello - regex 标头匹配 匹配传入请求的头信息可以与路径匹配相结合，以实现复杂的场景。在这个例子中，我们将使用前缀匹配和不同头信息匹配的组合。\n让我们设想一些规则：\n 所有带头信息 debug: 1 的 POST 请求发送到 /1，返回 422 状态码 所有发送至 /2 的头为 path 且与正则表达式 ^/hello/\\d+$ 相匹配的请求都会返回一个 200 状态码和消息 regex 所有将头名称 priority 设置为 1 到 5 之间的请求，发送到 /3 会返回一个 200 状态码和消息 priority 所有发送到 /4 的请求，如果存在 test 头，都会返回一个 500 状态码  以下是翻译成 Envoy 配置的上述规则：\nstatic_resources:listeners:- name:listener_0address:socket_address:address:0.0.0.0port_value:10000filter_chains:- filters:- name:envoy.filters.network.http_connection_managertyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManagerstat_prefix:hello_servicehttp_filters:- name:envoy.filters.http.routerroute_config:name:routevirtual_hosts:- name:vhostdomains:[\u0026#34;*\u0026#34;]routes:- match:path:\u0026#34;/1\u0026#34;headers:- name:\u0026#34;:method\u0026#34;string_match:exact:POST- name:\u0026#34;debug\u0026#34;string_match:exact:\u0026#34;1\u0026#34;direct_response:status:422- match:path:\u0026#34;/2\u0026#34;headers:- name:\u0026#34;path\u0026#34;safe_regex_match:google_re2:{}regex:^/hello/\\d+$direct_response:status:200body:inline_string:\u0026#34;regex\u0026#34;- match:path:\u0026#34;/3\u0026#34;headers:- name:\u0026#34;priority\u0026#34;range_match:start:1end:6direct_response:status:200body:inline_string:\u0026#34;priority\u0026#34;- match:path:\u0026#34;/4\u0026#34;headers:- name:\u0026#34;test\u0026#34;present_match:truedirect_response:status:500将上述 YAML 保存为 2-lab-1-request-matching-2.yaml 并运行\nfunc-e run -c 2-lab-1-request-matching-2.yaml 让我们试着发送几个请求，测试一下规则：\n$ curl -v -X POST -H \u0026#34;debug: 1\u0026#34; localhost:10000/1 ... \u0026gt; User-Agent: curl/7.64.0 \u0026gt; Accept: */* \u0026gt; debug: 1 \u0026gt; \u0026lt; HTTP/1.1 422 Unprocessable Entity $ curl -H \u0026#34;path: /hello/123\u0026#34; localhost:10000/2 regex $ curl -H \u0026#34;priority: 3\u0026#34; localhost:10000/3 priority $ curl -v -H \u0026#34;test: tst\u0026#34; localhost:10000/4 ... \u0026gt; User-Agent: curl/7.64.0 \u0026gt; Accept: */* \u0026gt; test: tst \u0026gt; \u0026lt; HTTP/1.1 500 Internal Server Error 查询参数匹配 与我们做路径和 Header 匹配的方式相同，我们也可以匹配特定的查询参数和它们的值。查询参数匹配支持与其他两项相同的匹配规则：匹配精确值、前缀和后缀，使用正则表达式，以及检查查询参数是否包含特定值。\n让我们考虑配置中的以下情景：\n 所有发送到路径 /1 并带有查询参数 test 请求都返回 422 状态码 所有发送到路径 /2 的查询参数为 env 的请求，其值以 env_开头（忽略大小写），返回 200 状态代码 所有发送到路径 /3 的查询参数 debug 设置为 true 的请求，都返回 500 状态码  上述规则转化为以下 Envoy 配置：\nstatic_resources:listeners:- name:listener_0address:socket_address:address:0.0.0.0port_value:10000filter_chains:- filters:- name:envoy.filters.network.http_connection_managertyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManagerstat_prefix:hello_servicehttp_filters:- name:envoy.filters.http.routerroute_config:name:routevirtual_hosts:- name:vhostdomains:[\u0026#34;*\u0026#34;]routes:- match:path:\u0026#34;/1\u0026#34;query_parameters:- name:testpresent_match:truedirect_response:status:422- match:path:\u0026#34;/2\u0026#34;query_parameters:- name:envstring_match:prefix:env_ignore_case:truedirect_response:status:200- match:path:\u0026#34;/3\u0026#34;query_parameters:- name:debugstring_match:exact:\u0026#34;true\u0026#34;direct_response:status:500将上述 YAML 保存为 2-lab-1-request-matching-3.yaml 并运行\nfunc-e run -c 2-lab-1-request-matching-3.yaml 让我们试着发送几个请求，测试一下规则。\n$ curl -v localhost:10000/1?test ... \u0026gt; GET /1?test HTTP/1.1 \u0026gt; Host: localhost:10000 \u0026gt; User-Agent: curl/7.64.0 \u0026gt; Accept: */* \u0026gt; \u0026lt; HTTP/1.1 422 Unprocessable Entity $ curl -v localhost:10000/2?env=eNv_prod ... \u0026gt; GET /2?env=eNv_prod HTTP/1.1 \u0026gt; Host: localhost:10000 \u0026gt; User-Agent: curl/7.64.0 \u0026gt; Accept: */* \u0026gt; \u0026lt; HTTP/1.1 200 OK $ curl -v localhost:10000/3?debug=true ... \u0026gt; GET /3?debug=true HTTP/1.1 \u0026gt; Host: localhost:10000 \u0026gt; User-Agent: curl/7.64.0 \u0026gt; Accept: */* \u0026gt; \u0026lt; HTTP/1.1 500 Internal Server Error ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"5d1e5a1f767a39d7d1a1c9dca1d1f682","permalink":"https://jimmysong.io/docs/envoy-handbook/hcm/lab1/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/envoy-handbook/hcm/lab1/","section":"envoy-handbook","summary":"在这个实验中，我们将学习如何配置使用不同的方式来匹配请求。我","tags":null,"title":"实验1：请求匹配","type":"book"},{"authors":null,"categories":null,"content":"下面是一个 Kubernetes Pod 安全策略的例子，它为集群中运行的容器执行了强大的安全要求。这个例子是基于官方的 Kubernetes 文档。我们鼓励管理员对该策略进行修改，以满足他们组织的要求。\napiVersion:policy/v1beta1kind:PodSecurityPolicymetadata:name:restrictedannotations:seccomp.security.alpha.kubernetes.io/allowedProfileNames:\u0026#39;docker/default,runtime/default\u0026#39;apparmor.security.beta.kubernetes.io/allowedProfileNames:\u0026#39;runtime/default\u0026#39;seccomp.security.alpha.kubernetes.io/defaultProfileName:\u0026#39;runtime/default\u0026#39;apparmor.security.beta.kubernetes.io/defaultProfileName:\u0026#39;runtime/default\u0026#39;spec:privileged:false# 需要防止升级到 rootallowPrivilegeEscalation:falserequiredDropCapabilities:- ALLvolumes:- \u0026#39;configMap\u0026#39;- \u0026#39;emptyDir\u0026#39;- \u0026#39;projected\u0026#39;- \u0026#39;secret\u0026#39;- \u0026#39;downwardAPI\u0026#39;- \u0026#39;persistentVolumeClaim\u0026#39;# 假设管理员设置的 persistentVolumes 是安全的hostNetwork:falsehostIPC:falsehostPID:falserunAsUser:rule:\u0026#39;MustRunAsNonRoot\u0026#39;# 要求容器在没有 root 的情况下运行 seLinuxrule:\u0026#39;RunAsAny\u0026#39;# 假设节点使用的是 AppArmor 而不是 SELinuxsupplementalGroups:rule:\u0026#39;MustRunAs\u0026#39;ranges:# 禁止添加到 root 组- min:1max:65535runAsGroup:rule:\u0026#39;MustRunAs\u0026#39;ranges:# 禁止添加到 root 组- min:1max:65535fsGroup:rule:\u0026#39;MustRunAs\u0026#39;ranges:# 禁止添加到 root 组- min:1max:65535readOnlyRootFilesystem:true","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"cb13fd3dd123f131f1380488e55a5deb","permalink":"https://jimmysong.io/docs/kubernetes-hardening-guidance/appendix/c/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/kubernetes-hardening-guidance/appendix/c/","section":"kubernetes-hardening-guidance","summary":"下面是一个 Kubernetes Pod 安全策略的例子，它为集群中运行的容器执行了强大","tags":null,"title":"附录 C：Pod 安全策略示例","type":"book"},{"authors":null,"categories":null,"content":"在这个实验中，我们将学习如何使用运行时分数和加权集群来配置 Envoy 的流量分割。\n使用运行时分数 当我们只有两个上游集群时，运行时分数是一种很好的流量分割方法。运行时分数的工作原理是提供一个运行时分数（例如分子和分母），代表我们想要路由到一个特定集群的流量的分数。然后，我们使用相同的条件（即，在我们的例子中相同的前缀）提供第二个匹配，但不同的上游集群。\n让我们创建一个 Envoy 配置，对 70% 的流量返回状态为 201 的直接响应。其余的流量返回状态为 202。\nstatic_resources:listeners:- name:listener_0address:socket_address:address:0.0.0.0port_value:10000filter_chains:- filters:- name:envoy.filters.network.http_connection_managertyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManagerstat_prefix:hello_servicehttp_filters:- name:envoy.filters.http.routerroute_config:virtual_hosts:- name:hello_vhostdomains:[\u0026#34;*\u0026#34;]routes:- match:prefix:\u0026#34;/\u0026#34;runtime_fraction:default_value:numerator:70denominator:HUNDREDruntime_key:routing.hello_iodirect_response:status:201body:inline_string:\u0026#34;v1\u0026#34;- match:prefix:\u0026#34;/\u0026#34;direct_response:status:202body:inline_string:\u0026#34;v2\u0026#34;将上述 Envoy 配置保存为 2-lab-2-traffic-splitting-1.yaml，并以该配置运行 Envoy。\nfunc-e run -c 2-lab-2-traffic-splitting-1.yaml 在 Envoy 运行时，我们可以使用 hey 工具向代理发送 200 个请求。\n$ hey http://localhost:10000 ... Status code distribution: [201] 142 responses [202] 58 responses 看一下状态代码的分布，我们会注意到，我们收到 HTTP 201 响应大概占 71%，其余的响应是 HTTP 202 响应。\n使用加权集群 当我们有两个以上的上游集群，我们想把流量分给它们，我们可以使用加权集群的方法。在这里，我们单独给每个上游集群分配权重。我们在以前的方法中使用了多个匹配，而在加权集群中，我们将使用一个路由和多个加权集群。\n对于这种方法，我们必须定义实际的上游集群。我们将运行 httpbin 镜像的三个实例。让我们在 3030、4040 和 5050 端口上运行三个不同的实例；我们将在 Envoy 配置中把它们称为 instance_1、instance_2 和 instance_3。\ndocker run -d -p 3030:80 kennethreitz/httpbin docker run -d -p 4040:80 kennethreitz/httpbin docker run -d -p 5050:80 kennethreitz/httpbin 一个上游集群可以通过以下片段来定义。\nclusters:- name:instance_1connect_timeout:5sload_assignment:cluster_name:instance_1endpoints:- lb_endpoints:- endpoint:address:socket_address:address:127.0.0.1port_value:3030让我们创建 Envoy 配置，将 50% 的流量分给 instance_1，30% 分给 instance_2，20% 分给 instance_3。\n我们还将启用管理接口来检索指标，这些指标将显示向不同集群发出的请求数量。\nstatic_resources:listeners:- name:listener_0address:socket_address:address:0.0.0.0port_value:10000filter_chains:- filters:- name:envoy.filters.network.http_connection_managertyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManagerstat_prefix:listener_httphttp_filters:- name:envoy.filters.http.routerroute_config:name:routevirtual_hosts:- name:vhdomains:[\u0026#34;*\u0026#34;]routes:- match:prefix:\u0026#34;/\u0026#34;route:weighted_clusters:clusters:- name:instance_1weight:50- name:instance_2weight:30- name:instance_3weight:20clusters:- name:instance_1connect_timeout:5sload_assignment:cluster_name:instance_1endpoints:- lb_endpoints:- endpoint:address:socket_address:address:127.0.0.1port_value:3030- name:instance_2connect_timeout:5sload_assignment:cluster_name:instance_1endpoints:- lb_endpoints:- endpoint:address:socket_address:address:127.0.0.1port_value:4040- name:instance_3connect_timeout:5sload_assignment:cluster_name:instance_1endpoints:- lb_endpoints:- endpoint:address:socket_address:address:127.0.0.1port_value:5050admin:address:socket_address:address:127.0.0.1port_value:9901将上述 YAML 保存为 2-lab-2-traffic-splitting-2.yaml 并运行代理程序：func-e run -c 2-lab-2-traffic-splitting-2.yaml。\n一旦代理运行，我们将使用 hey 来发送 200 个请求。\nhey http://localhost:10000 来自 hey 的响应不会帮助我们确定分割，因为每个上游集群的响应都是 HTTP 200。\n要想看到流量的分割，请在 http://localhost:9901/stats/prometheus 上打开统计表。在指标列表中，寻找 envoy_cluster_external_upstream_rq 指标，该指标计算外部上游请求的数量。我们应该看到与此类似的分割。\n# TYPE envoy_cluster_external_upstream_rq counter envoy_cluster_external_upstream_rq{envoy_response_code=\u0026#34;200\u0026#34;,envoy_cluster_name=\u0026#34;instance_1\u0026#34;} 99 envoy_cluster_external_upstream_rq{envoy_response_code=\u0026#34;200\u0026#34;,envoy_cluster_name=\u0026#34;instance_2\u0026#34;} 63 envoy_cluster_external_upstream_rq{envoy_response_code=\u0026#34;200\u0026#34;,envoy_cluster_name=\u0026#34;instance_3\u0026#34;} 38 如果我们计算一下百分比，我们会发现它们与我们在配置中设置的百分比相对应。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"71a7a7b44d49f7a6adccd38aca581f85","permalink":"https://jimmysong.io/docs/envoy-handbook/hcm/lab2/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/envoy-handbook/hcm/lab2/","section":"envoy-handbook","summary":"在这个实验中，我们将学习如何使用运行时分数和加权集群来配置 Envoy","tags":null,"title":"实验2：流量分割","type":"book"},{"authors":null,"categories":null,"content":"下面的例子是为每个团队或用户组，可以使用 kubectl 命令或 YAML 文件创建一个 Kubernetes 命名空间。应避免使用任何带有 kube 前缀的名称，因为它可能与 Kubernetes 系统保留的命名空间相冲突。\nKubectl 命令来创建一个命名空间。\nkubectl create namespace \u0026lt;insert-namespace-name-here\u0026gt; 要使用 YAML 文件创建命名空间，创建一个名为 my-namespace.yaml 的新文件，内容如下：\napiVersion:v1kind:Namespacemetadata:name:\u0026lt;insert-namespace-name-here\u0026gt;应用命名空间，使用：\nkubectl create –f ./my-namespace.yaml 要在现有的命名空间创建新的 Pod，请切换到所需的命名空间：\nkubectl config use-context \u0026lt;insert-namespace-here\u0026gt; 应用新的 Deployment，使用：\nkubectl apply -f deployment.yaml 另外，也可以用以下方法将命名空间添加到 kubectl 命令中：\nkubectl apply -f deployment.yaml --namespace=\u0026lt;insert-namespace-here\u0026gt; 或在 YAML 声明中的元数据下指定 namespace：\u0026lt;insert-namespace-here\u0026gt;。\n一旦创建，资源不能在命名空间之间移动。必须删除该资源，然后在新的命名空间中创建。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"1f49f13ed35a9a54054e8644205ad23e","permalink":"https://jimmysong.io/docs/kubernetes-hardening-guidance/appendix/d/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/kubernetes-hardening-guidance/appendix/d/","section":"kubernetes-hardening-guidance","summary":"下面的例子是为每个团队或用户组，可以使用 kubectl 命令或 YAML 文件创建一","tags":null,"title":"附录 D：命名空间示例","type":"book"},{"authors":null,"categories":null,"content":"在这个实验中，我们将学习如何在不同的配置级别上操作请求和响应头。\n我们将使用一个单一的例子，它的作用如下：\n 对所有的请求添加一个响应头 lab: 3 在虚拟主机上添加一个请求头 vh: one 为 /json 路由匹配添加一个名为 json 响应头。响应头有来自请求头 hello 的值  我们将只有一个名为 single_cluster 的上游集群，监听端口为 3030。让我们运行监听该端口的 httpbin 容器：\ndocker run -d -p 3030:80 kennethreitz/httpbin 让我们创建遵循上述规则的 Envoy 配置：\nstatic_resources:listeners:- name:listener_0address:socket_address:address:0.0.0.0port_value:10000filter_chains:- filters:- name:envoy.filters.network.http_connection_managertyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManagerstat_prefix:hello_servicehttp_filters:- name:envoy.filters.http.routerroute_config:response_headers_to_add:- header:key:\u0026#34;lab\u0026#34;value:\u0026#34;3\u0026#34;virtual_hosts:- name:vh_1request_headers_to_add:- header:key:vhvalue:\u0026#34;one\u0026#34;domains:[\u0026#34;*\u0026#34;]routes:- match:prefix:\u0026#34;/json\u0026#34;route:cluster:single_clusterresponse_headers_to_add:- header:key:\u0026#34;json\u0026#34;value:\u0026#34;%REQ(hello)%\u0026#34;- match:prefix:\u0026#34;/\u0026#34;route:cluster:single_clusterclusters:- name:single_clusterconnect_timeout:5sload_assignment:cluster_name:single_clusterendpoints:- lb_endpoints:- endpoint:address:socket_address:address:127.0.0.1port_value:3030将上述 YAML 保存为 2-lab-3-header-manipulation-1.yaml 并运行 Envoy 代理：\nfunc-e run -c 2-lab-3-header-manipulation-1.yaml 让我们先对 /headers 做一个简单的请求，这将匹配 / 前缀：\n$ curl -v localhost:10000/headers ... \u0026lt; x-envoy-upstream-service-time: 2 \u0026lt; lab: 3 \u0026lt; { \u0026#34;headers\u0026#34;: { \u0026#34;Accept\u0026#34;: \u0026#34;*/*\u0026#34;, \u0026#34;Host\u0026#34;: \u0026#34;localhost:10000\u0026#34;, \u0026#34;User-Agent\u0026#34;: \u0026#34;curl/7.64.0\u0026#34;, \u0026#34;Vh\u0026#34;: \u0026#34;one\u0026#34;, \u0026#34;X-Envoy-Expected-Rq-Timeout-Ms\u0026#34;: \u0026#34;15000\u0026#34; } } 我们会注意到响应 Header lab: 3 被设置了。这来自路由配置，并将被添加到我们的所有请求中。\n在响应中，我们可以看到 Vh: one 头（注意这个大写字母来自 httpbin 代码）被添加到请求中。httpbin 的响应显示了所收到的头信息（即请求头信息）。\n让我们试着向 /json 路径发出请求。发送到该路径上的 httpbin 的请求将返回一个 JSON 样本。此外，这一次我们将包括一个名为 hello: world 的请求头。\n$ curl -v -H \u0026#34;hello: world\u0026#34; localhost:10000/json \u0026gt; GET /json HTTP/1.1 \u0026gt; Host: localhost:10000 \u0026gt; User-Agent: curl/7.64.0 \u0026gt; Accept: */* \u0026gt; hello: world \u0026gt; \u0026lt; HTTP/1.1 200 OK \u0026lt; server: envoy \u0026lt; json: world \u0026lt; lab: 3 ... 注意这次我们设置的请求头（hello: world），在响应路径上，我们看到 json: world 头，它的值来自我们设置的请求头。同样地，lab: 3 响应头被设置。请求头 Vh: one 也同时被设置，但这次我们看不到它，因为它没有被输出。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"01ab6cb1e9f882cfe69548eb58de4a99","permalink":"https://jimmysong.io/docs/envoy-handbook/hcm/lab3/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/envoy-handbook/hcm/lab3/","section":"envoy-handbook","summary":"在这个实验中，我们将学习如何在不同的配置级别上操作请求和响应","tags":null,"title":"实验3：Header操作","type":"book"},{"authors":null,"categories":null,"content":"网络策略根据使用的网络插件而不同。下面是一个网络策略的例子，参考 Kubernetes 文档将 nginx 服务的访问限制在带有标签访问的 Pod 上。\napiVersion:networking.k8s.io/v1kind:NetworkPolicymetadata:name:example-access-nginxnamespace:prod#这可以是任何一个命名空间，或者在不使用命名空间的情况下省略。spec:podSelector:matchLabels:app:nginxingress:- from:- podSelector:matchLabels:access:\u0026#34;true\u0026#34;新的 NetworkPolicy 可以通过以下方式应用：\nkubectl apply -f policy.yaml 一个默认的拒绝所有入口的策略：\napiVersion:networking.k8s.io/v1kind:NetworkPolicymetadata:name:deny-all-ingressspec:podSelector:{}policyType:- Ingress一个默认的拒绝所有出口的策略：\napiVersion:networking.k8s.io/v1kind:NetworkPolicymetadata:name:deny-all-egressspec:podSelector:{}policyType:- Egress","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"4046f068c4fc57190e32fbd2a973829a","permalink":"https://jimmysong.io/docs/kubernetes-hardening-guidance/appendix/e/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/kubernetes-hardening-guidance/appendix/e/","section":"kubernetes-hardening-guidance","summary":"网络策略根据使用的网络插件而不同。下面是一个网络策略的例子，","tags":null,"title":"附录 E：网络策略示例","type":"book"},{"authors":null,"categories":null,"content":"在这个实验中，我们将学习如何配置不同的重试策略。我们将使用 httpbin Docker 镜像，因为我们可以向不同的路径（例如 /status/[statuscode]）发送请求，而 httpbin 会以该状态码进行响应。\n确保你有 httpbin 容器在 3030 端口监听。\ndocker run -d -p 3030:80 kennethreitz/httpbin 让我们创建 Envoy 配置，定义一个关于 5xx 响应的简单重试策略。我们还将启用管理接口，这样我们就可以在指标中看到重试的报告。\nstatic_resources:listeners:- name:listener_0address:socket_address:address:0.0.0.0port_value:10000filter_chains:- filters:- name:envoy.filters.network.http_connection_managertyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManagerstat_prefix:hello_servicehttp_filters:- name:envoy.filters.http.routerroute_config:virtual_hosts:- name:httpbindomains:[\u0026#34;*\u0026#34;]routes:- match:prefix:\u0026#34;/\u0026#34;route:cluster:httpbinretry_policy:retry_on:\u0026#34;5xx\u0026#34;num_retries:5clusters:- name:httpbinconnect_timeout:5sload_assignment:cluster_name:single_clusterendpoints:- lb_endpoints:- endpoint:address:socket_address:address:127.0.0.1port_value:3030admin:address:socket_address:address:127.0.0.1port_value:9901将上述 YAML 保存为 2-lab-4-retries-1.yaml，然后运行 Envoy 代理：\nfunc-e run -c 2-lab-4-retries-1.yaml 让我们向 /status/500 路径发送一个单一请求。\n$ curl -v localhost:10000/status/500 ... \u0026lt; HTTP/1.1 500 Internal Server Error \u0026lt; server: envoy ... \u0026lt; content-length: 0 \u0026lt; x-envoy-upstream-service-time: 276 正如预期的那样，我们收到了一个 500 响应。另外，注意到 x-envoy-upstream-service-time（上游主机处理该请求所花费的时间，以毫秒为单位）比我们发送 /status/200 请求时要大得多。\n$ curl localhost:10000/status/200 ... \u0026lt; HTTP/1.1 200 OK \u0026lt; server: envoy ... \u0026lt; content-length: 0 \u0026lt; x-envoy-upstream-service-time: 2 这是因为 Envoy 执行了重试，但最后还是失败了。同样，如果我们在管理界面（http://localhost:9901/stats/prometheus）上打开统计页面，我们会发现代表重试次数的指标（envoy_cluster_retry_upstream_rq）的数值为 5。\n# TYPE envoy_cluster_retry_upstream_rq counter envoy_cluster_retry_upstream_rq{envoy_response_code=\u0026#34;500\u0026#34;,envoy_cluster_name=\u0026#34;httpbin\u0026#34;} 5 ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"3d9f571e6366b7cd755e712db57ad708","permalink":"https://jimmysong.io/docs/envoy-handbook/hcm/lab4/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/envoy-handbook/hcm/lab4/","section":"envoy-handbook","summary":"在这个实验中，我们将学习如何配置不同的重试策略。我们将使用 httpbin","tags":null,"title":"实验4:重试","type":"book"},{"authors":null,"categories":null,"content":"在 Kubernetes 1.10 和更新版本中，LimitRange 支持被默认启用。下面的 YAML 文件为每个容器指定了一个 LimitRange，其中有一个默认的请求和限制，以及最小和最大的请求。\napiVersion:v1kind:LimitRangemetadata:name:cpu-min-max-demo-lrspec:limits- default:cpu:1defaultRequest:cpu:0.5max:cpu:2min:cpu 0.5type:ContainerLimitRange 可以应用于命名空间，使用：\nkubectl apply -f \u0026lt;example-LimitRange\u0026gt;.yaml --namespace=\u0026lt;Enter-Namespace\u0026gt; 在应用了这个 LimitRange 配置的例子后，如果没有指定，命名空间中创建的所有容器都会被分配到默认的 CPU 请求和限制。命名空间中的所有容器的 CPU 请求必须大于或等于最小值，小于或等于最大 CPU 值，否则容器将不会被实例化。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"672aeaed1d8c5ac68a293a5b8023d7fb","permalink":"https://jimmysong.io/docs/kubernetes-hardening-guidance/appendix/f/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/kubernetes-hardening-guidance/appendix/f/","section":"kubernetes-hardening-guidance","summary":"在 Kubernetes 1.10 和更新版本中，LimitRange 支持被默认启用。下面","tags":null,"title":"附录 F：LimitRange 示例","type":"book"},{"authors":null,"categories":null,"content":"在这个实验中，我们将学习如何配置一个局部速率限制器。我们将使用运行在 3030 端口的 httpbin 容器。\ndocker run -d -p 3030:80 kennethreitz/httpbin 让我们创建一个有五个令牌的速率限制器。每隔 30 秒，速率限制器就会向桶里补充 5 个令牌。\nstatic_resources:listeners:- name:listener_0address:socket_address:address:0.0.0.0port_value:10000filter_chains:- filters:- name:envoy.filters.network.http_connection_managertyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManagerstat_prefix:ingress_httproute_config:name:local_routevirtual_hosts:- name:instance_1domains:[\u0026#34;*\u0026#34;]routes:- match:prefix:/statusroute:cluster:instance_1- match:prefix:/headersroute:cluster:instance_1typed_per_filter_config:envoy.filters.http.local_ratelimit:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.http.local_ratelimit.v3.LocalRateLimitstat_prefix:headers_routetoken_bucket:max_tokens:5tokens_per_fill:5fill_interval:30sfilter_enabled:default_value:numerator:100denominator:HUNDREDfilter_enforced:default_value:numerator:100denominator:HUNDREDresponse_headers_to_add:- append:falseheader:key:x-rate-limitedvalue:OH_NOhttp_filters:- name:envoy.filters.http.local_ratelimittyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.http.local_ratelimit.v3.LocalRateLimitstat_prefix:httpbin_rate_limiter- name:envoy.filters.http.routerclusters:- name:instance_1connect_timeout:0.25stype:STATIClb_policy:ROUND_ROBINload_assignment:cluster_name:instance_1endpoints:- lb_endpoints:- endpoint:address:socket_address:address:127.0.0.1port_value:3030admin:address:socket_address:address:127.0.0.1port_value:9901将上述 YAML 保存为 2-lab-5-local-rate-limiter-1.yaml，用 func-e run -c 2-lab-5-local-rate-limiter-1.yaml 运行 Envoy。\n上述配置为路由 /headers 启用了一个局部速率限制器。此外，一旦达到速率限制，我们将在响应中添加一个头信息（x-rate-limited）。\n如果我们在 30 秒内向 http://localhost:10000/headers 发出超过 5 个请求，我们会得到 HTTP 429 的响应。\n$ curl -v localhost:10000/headers ... \u0026gt; GET /headers HTTP/1.1 \u0026gt; Host: localhost:10000 \u0026gt; User-Agent: curl/7.64.0 \u0026gt; Accept: */* \u0026gt; \u0026lt; HTTP/1.1 429 Too Many Requests \u0026lt; x-rate-limited: OH_NO ... local_rate_limited 另外，注意到 Envoy 设置的 x-rate-limited 头。\n一旦我们被限制了速率，我们将不得不等待 30 秒，让速率限制器再次用令牌把桶填满。我们也可以尝试向 /status/200 发出请求，你会发现我们不会在这个路径上受到速率限制。\n如果我们打开统计页面（localhost:9901/stats/prometheus），我们会发现限速指标是使用我们配置的 headers_route_rate_limiter 统计前缀记录的。\n# TYPE envoy_headers_route_http_local_rate_limit_enabled counter envoy_headers_route_http_local_rate_limit_enabled{} 13 # TYPE envoy_headers_route_http_local_rate_limit_enforced counter envoy_headers_route_http_local_rate_limit_enforced{} 8 # TYPE envoy_headers_route_http_local_rate_limit_ok counter envoy_headers_route_http_local_rate_limit_ok{} 5 # TYPE envoy_headers_route_http_local_rate_limit_rate_limited counter envoy_headers_route_http_local_rate_limit_rate_limited{} 8 ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"468cdcdd427fd80cbe7001d56de6467d","permalink":"https://jimmysong.io/docs/envoy-handbook/hcm/lab5/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/envoy-handbook/hcm/lab5/","section":"envoy-handbook","summary":"在这个实验中，我们将学习如何配置一个局部速率限制器。我们将使","tags":null,"title":"实验5：局部速率限制","type":"book"},{"authors":null,"categories":null,"content":"通过将 YAML 文件应用于命名空间或在 Pod 的配置文件中指定要求来创建 ResourceQuota 对象，以限制命名空间内的总体资源使用。下面的例子是基于 Kubernetes 官方文档的一个命名空间的配置文件示例：\napiVersion:v1kind:ResourceQuotametadata:name:example-cpu-mem-resourcequotaspec:hard:requests.cpu:\u0026#34;1\u0026#34;requests.memory:1Gilimits.cpu:\u0026#34;2\u0026#34;limits.memory:2Gi可以这样应用这个 ResourceQuota：\nkubectl apply -f example-cpu-mem-resourcequota.yaml -- namespace=\u0026lt;insert-namespace-here\u0026gt; 这个 ResourceQuota 对所选择的命名空间施加了以下限制：\n 每个容器都必须有一个内存请求、内存限制、CPU 请求和 CPU 限制。 所有容器的总内存请求不应超过 1 GiB 所有容器的总内存限制不应超过 2 GiB 所有容器的 CPU 请求总量不应超过 1 个 CPU 所有容器的总 CPU 限制不应超过 2 个 CPU  ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"705c8ab8c0898acfa9093795645c5cfa","permalink":"https://jimmysong.io/docs/kubernetes-hardening-guidance/appendix/g/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/kubernetes-hardening-guidance/appendix/g/","section":"kubernetes-hardening-guidance","summary":"通过将 YAML 文件应用于命名空间或在 Pod 的配置文件中指定要求来创建 ResourceQuota","tags":null,"title":"附录 G：ResourceQuota 示例","type":"book"},{"authors":null,"categories":null,"content":"在这个实验中，我们将学习如何配置一个全局速率限制器。我们将使用速率限制器服务和一个 Redis 实例来跟踪令牌。我们将使用 Docker Compose 来运行 Redis 和速率限制器服务容器。\n让我们首先创建速率限制器服务的配置。\ndomain:my_domaindescriptors:- key:generic_keyvalue:instance_1descriptors:- key:header_matchvalue:get_requestrate_limit:unit:MINUTErequests_per_unit:5我们指定一个通用键（instance_1）和一个名为 header_match 的描述符，速率限制为 5 个请求 / 分钟。\n将上述文件保存到 /config/rl-config.yaml 文件中。\n现在我们可以运行 Docker Compose 文件，它将启动 Redis 和速率限制器服务。\nversion:\u0026#34;3\u0026#34;services:redis:image:redis:alpineexpose:- 6379ports:- 6379:6379networks:- ratelimit-network# Rate limit service configurationratelimit:image:envoyproxy/ratelimit:bd46f11bcommand:/bin/ratelimitports:- 10001:8081- 6070:6070depends_on:- redisnetworks:- ratelimit-networkvolumes:- $PWD/config:/data/config/configenvironment:- USE_STATSD=false- LOG_LEVEL=debug- REDIS_SOCKET_TYPE=tcp- REDIS_URL=redis:6379- RUNTIME_ROOT=/data- RUNTIME_SUBDIRECTORY=confignetworks:ratelimit-network:将上述文件保存为 rl-docker-compose.yaml，并使用下面的命令启动所有容器：\n$ docker-compose -f rl-docker-compose.yaml up 为了确保速率限制器服务正确读取配置，我们可以检查容器的输出或使用速率限制器服务的调试端口。\n$ curl localhost:6070/rlconfig my_domain.generic_key_instance_1.header_match_get_request: unit=MINUTE requests_per_unit=5 随着速率限制器和 Redis 的启动和运行，我们可以启动 httpbin 容器。\ndocker run -d -p 3030:80 kennethreitz/httpbin 接下来，我们将创建 Envoy 配置，定义速率限制动作。我们将设置描述符 instance_1 和 get_request，只要有 GET 请求被发送到 httpbin。\n在 http_filters 下，我们通过指定域名（my_domain）和指向 Envoy 可以用来到达速率限制服务的集群来配置 ratelimit 过滤器。\nstatic_resources:listeners:- name:listener_0address:socket_address:address:0.0.0.0port_value:10000filter_chains:- filters:- name:envoy.filters.network.http_connection_managertyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManagerstat_prefix:ingress_httproute_config:name:local_routevirtual_hosts:- name:namespace.local_servicedomains:[\u0026#34;*\u0026#34;]routes:- match:prefix:/route:cluster:instance_1rate_limits:- actions:- generic_key:descriptor_value:instance_1- header_value_match:descriptor_value:get_requestheaders:- name:\u0026#34;:method\u0026#34;exact_match:GEThttp_filters:- name:envoy.filters.http.ratelimittyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.http.ratelimit.v3.RateLimitdomain:my_domainenable_x_ratelimit_headers:DRAFT_VERSION_03rate_limit_service:transport_api_version:V3grpc_service:envoy_grpc:cluster_name:rate-limit- name:envoy.filters.http.routerclusters:- name:instance_1connect_timeout:0.25stype:STATIClb_policy:ROUND_ROBINload_assignment:cluster_name:instance_1endpoints:- lb_endpoints:- endpoint:address:socket_address:address:127.0.0.1port_value:3030- name:rate-limitconnect_timeout:1stype:STATIClb_policy:ROUND_ROBINprotocol_selection:USE_CONFIGURED_PROTOCOLhttp2_protocol_options:{}load_assignment:cluster_name:rate-limitendpoints:- lb_endpoints:- endpoint:address:socket_address:address:127.0.0.1port_value:10001admin:address:socket_address:address:127.0.0.1port_value:9901将上述 YAML 保存为 2-lab-6-global-rate-limiter-1.yaml，并使用 func-e run -c 2-lab-6-global-rate-limiter-1.yaml 运行代理。\n我们现在可以发送五个以上的请求，我们会得到速率限制。\n$ curl -v localhost:10000 ... \u0026lt; HTTP/1.1 429 Too Many Requests \u0026lt; x-envoy-ratelimited: true \u0026lt; x-ratelimit-limit: 5, 5;w=60 \u0026lt; x-ratelimit-remaining: 0 \u0026lt; x-ratelimit-reset: 25 ... 我们收到了 429 响应，以及表明我们受到速率限制的响应头；在受到速率限制之前我们可以发出多少个请求（x-ratelimit-remaining）以及速率限制何时重置（x-ratelimit-reset）。\n 下一章   ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"fe41f01b0d846ed907240ce823f9ca3e","permalink":"https://jimmysong.io/docs/envoy-handbook/hcm/lab6/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/envoy-handbook/hcm/lab6/","section":"envoy-handbook","summary":"在这个实验中，我们将学习如何配置一个全局速率限制器。我们将使","tags":null,"title":"实验6：全局速率限制","type":"book"},{"authors":null,"categories":null,"content":"要对秘密数据进行静态加密，下面的加密配置文件提供了一个例子，以指定所需的加密类型和加密密钥。将加密密钥存储在加密文件中只能稍微提高安全性。Secret 将被加密，但密钥将在 EncryptionConfiguration 文件中被访问。这个例子是基于 Kubernetes 的官方文档。\napiVersion:apiserver.config.k8s.io/v1kind:EncryptionConfigurationresources:- resources:- secretsproviders:- aescbc:keys:- name:key1secret:\u0026lt;base 64 encoded secret\u0026gt;- identity:{}要使用该加密文件进行静态加密，请在重启 API 服务器时设置 --encryption-provider-config 标志，并注明配置文件的位置。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"6ec4cf7b0153d0ffba5a2cb32f0e3f90","permalink":"https://jimmysong.io/docs/kubernetes-hardening-guidance/appendix/h/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/kubernetes-hardening-guidance/appendix/h/","section":"kubernetes-hardening-guidance","summary":"要对秘密数据进行静态加密，下面的加密配置文件提供了一个例子，","tags":null,"title":"附录 H：加密示例","type":"book"},{"authors":null,"categories":null,"content":"Envoy sidecar 代理配置中包含以下四个部分：\n bootstrap：Envoy proxy 启动时候加载的静态配置。 listeners：监听器配置，使用 LDS 下发。 clusters：集群配置，静态配置中包括 xds-grpc 和 zipkin 地址，动态配置使用 CDS 下发。 routes：路由配置，静态配置中包括了本地监听的服务的集群信息，其中引用了 cluster，动态配置使用 RDS 下发。  每个部分中都包含静态配置与动态配置，其中 bootstrap 配置又是在集群启动的时候通过 sidecar 启动参数注入的，配置文件在 /etc/istio/proxy/envoy-rev0.json。\nEnvoy 的配置 dump 出来后的结构如下图所示。\n   Envoy 配置  由于 bootstrap 中的配置是来自 Envoy 启动时加载的静态文件，主要配置了节点信息、tracing、admin 和统计信息收集等信息，这不是本文的重点，大家可以自行研究。\n   bootstrap 配置  上图是 bootstrap 的配置信息。\nBootstrap 是 Envoy 中配置的根本来源，Bootstrap 消息中有一个关键的概念，就是静态和动态资源的之间的区别。例如 Listener或 Cluster 这些资源既可以从静态资源配置中获得也可以从动态资源中配置的 LDS 或 CDS 之类的 xDS 服务获取。\nListener Listener 顾名思义，就是监听器，监听 IP 地址和端口，然后根据策略转发。\nListener 的特点\n 每个 Envoy 进程中可以有多个 Listener，Envoy 与 Listener 之间是一对多的关系。 每个 Listener 中可以配置一条 filter 链表（filter_chains），Envoy 会根据 filter 顺序执行过滤。 Listener 可以监听下游的端口，也可以接收来自其他 listener 的数据，形成链式处理。 filter 是可扩展的。 可以静态配置，也可以使用 LDS 动态配置。 目前只能监听 TCP，UDP 还未支持。  Listener 的数据结构\nListener 的数据结构如下，除了 name、address 和 filter_chains 为必须配置之外，其他都为可选的。\n{ \u0026#34;name\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;address\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;filter_chains\u0026#34;: [], \u0026#34;use_original_dst\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;per_connection_buffer_limit_bytes\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;metadata\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;drain_type\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;listener_filters\u0026#34;: [], \u0026#34;transparent\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;freebind\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;socket_options\u0026#34;: [], \u0026#34;tcp_fast_open_queue_length\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;bugfix_reverse_write_filter_order\u0026#34;: \u0026#34;{...}\u0026#34; } 下面是关于上述数据结构中的常用配置解析。\n  name：该 listener 的 UUID，唯一限定名，默认60个字符，例如 10.254.74.159_15011，可以使用命令参数指定长度限制。\n  address：监听的逻辑/物理地址和端口号，例如\n\u0026#34;address\u0026#34;: { \u0026#34;socket_address\u0026#34;: { \u0026#34;address\u0026#34;: \u0026#34;10.254.74.159\u0026#34;, \u0026#34;port_value\u0026#34;: 15011 } }   filter_chains：这是一个列表，Envoy 中内置了一些通用的 filter，每种 filter 都有特定的数据结构，Envoy 会根据该配置顺序执行 filter。\n  use_original_dst：这是一个布尔值，如果使用 iptables 重定向连接，则代理接收的端口可能与原始目的地址的端口不一样。当此标志设置为 true 时，Listener 将重定向的连接切换到与原始目的地址关联的 Listener。如果没有与原始目的地址关联的 Listener，则连接由接收它的 Listener 处理。默认为 false。注意：该参数将被废弃，请使用原始目的地址的 Listener filter 替代。该参数的主要用途是，Envoy 通过监听 15001 端口将应用的流量截取后再由其他 Listener 处理而不是直接转发出去。\n  Cluster Cluster 是指 Envoy 连接的一组逻辑相同的上游主机。Envoy 通过服务发现来发现 cluster 的成员。可以选择通过主动健康检查来确定集群成员的健康状态。Envoy 通过负载均衡策略决定将请求路由到 cluster 的哪个成员。\nCluster 的特点\n 一组逻辑上相同的主机构成一个 cluster。 可以在 cluster 中定义各种负载均衡策略。 新加入的 cluster 需要一个热身的过程才可以给路由引用，该过程是原子的，即在 cluster 热身之前对于 Envoy 及 Service Mesh 的其余部分来说是不可见的。 可以通过多种方式来配置 cluster，例如静态类型、严格限定 DNS、逻辑 DNS、EDS 等。  Cluster 的数据结构\nCluster 的数据结构如下，除了 name 字段，其他都是可选的。\n{ \u0026#34;name\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;alt_stat_name\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;eds_cluster_config\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;connect_timeout\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;per_connection_buffer_limit_bytes\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;lb_policy\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;hosts\u0026#34;: [], \u0026#34;load_assignment\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;health_checks\u0026#34;: [], \u0026#34;max_requests_per_connection\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;circuit_breakers\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;tls_context\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;common_http_protocol_options\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;http_protocol_options\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;http2_protocol_options\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;extension_protocol_options\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;dns_refresh_rate\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;dns_lookup_family\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;dns_resolvers\u0026#34;: [], \u0026#34;outlier_detection\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;cleanup_interval\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;upstream_bind_config\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;lb_subset_config\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;ring_hash_lb_config\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;original_dst_lb_config\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;least_request_lb_config\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;common_lb_config\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;transport_socket\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;metadata\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;protocol_selection\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;upstream_connection_options\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;close_connections_on_host_health_failure\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;drain_connections_on_host_removal\u0026#34;: \u0026#34;...\u0026#34; } 下面是关于上述数据结构中的常用配置解析。\n name：如果你留意到作为 Sidecar 启动的 Envoy 的参数的会注意到 --max-obj-name-len 189，该选项用来用来指定 cluster 的名字，例如 inbound|9080||ratings.default.svc.cluster.local。该名字字符串由 | 分隔成四个部分，分别是 inbound 或 outbound 代表入向流量或出向流量、端口号、subcluster 名称、FQDN，其中 subcluster 名称将对应于 Istio DestinationRule 中配置的 subnet，如果是按照多版本按比例路由的话，该值可以是版本号。 type：即服务发现类型，支持的参数有 STATIC（缺省值）、STRICT_DNS、LOGICAL_DNS、EDS、ORIGINAL_DST。 hosts：这是个列表，配置负载均衡的 IP 地址和端口，只有使用了 STATIC、STRICT_DNS、LOGICAL_DNS 服务发现类型时才需要配置。 eds_cluster_config：如果使用 EDS 做服务发现，则需要配置该项目，其中包括的配置有 service_name 和 ads。  Route 我们在这里所说的路由指的是 HTTP 路由，这也使得 Envoy 可以用来处理网格边缘的流量。HTTP 路由转发是通过路由过滤器实现的。该过滤器的主要职能就是执行路由表中的指令。除了可以做重定向和转发，路由过滤器还需要处理重试、统计之类的任务。\nHTTP 路由的特点\n 前缀和精确路径匹配规则。 可跨越多个上游集群进行基于权重/百分比的路由。 基于优先级的路由。 基于哈希策略的路由。  Route 的数据结构\n{ \u0026#34;name\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;virtual_hosts\u0026#34;: [], \u0026#34;internal_only_headers\u0026#34;: [], \u0026#34;response_headers_to_add\u0026#34;: [], \u0026#34;response_headers_to_remove\u0026#34;: [], \u0026#34;request_headers_to_add\u0026#34;: [], \u0026#34;request_headers_to_remove\u0026#34;: [], \u0026#34;validate_clusters\u0026#34;: \u0026#34;{...}\u0026#34; } 下面是关于上述数据结构中的常用配置解析。\n name：该名字跟 envoy.http_connection_manager filter 中的 http_filters.rds.route_config_name 一致，在 Istio Service Mesh 中为 Envoy 下发的配置中的 Route 是以监听的端口号作为名字，而同一个名字下面的 virtual_hosts 可以有多个值（数组形式）。 virtual_hosts：因为 VirtualHosts 是 Envoy 中引入的一个重要概念，我们在下文将详细说明 virtual_hosts 的数据结构。 validate_clusters：这是一个布尔值，用来设置开启使用 cluster manager 来检测路由表引用的 cluster 是否有效。如果是路由表是通过 route_config 静态配置的则该值默认设置为 true，如果是使用 RDS 动态配置的话，则该值默认设置为 false。  route.VirtualHost VirtualHost 即上文中 Route 配置中的 virtual_hosts，VirtualHost 是路由配置中的顶级元素。每个虚拟主机都有一个逻辑名称以及一组根据传入请求的 host header 路由到它的域。这允许单个 Listener 为多个顶级域路径树提供服务。基于域选择了虚拟主机后 Envoy 就会处理路由以查看要路由到哪个上游集群或是否执行重定向。\nVirtualHost 的数 …","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"3e43ede9cec7b399968d8d614bbf4747","permalink":"https://jimmysong.io/docs/istio-handbook/data-plane/envoy-proxy-config-deep-dive/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/istio-handbook/data-plane/envoy-proxy-config-deep-dive/","section":"istio-handbook","summary":"Envoy sidecar 代理配置中包含以下四个部分： bootstrap：Envo","tags":null,"title":"Envoy 代理配置详解","type":"book"},{"authors":null,"categories":null,"content":"让我们以 Web 前端和 customer 服务为例，看看 Envoy 如何确定将请求从 Web 前端发送到 customer 服务（customers.default.svc.cluster.local）的位置。使用 istioctl proxy-config 命令，我们可以列出 web 前端 pod 的所有监听器。\n$ istioctl proxy-config listeners web-frontend-64455cd4c6-p6ft2 ADDRESS PORT MATCH DESTINATION 10.124.0.10 53 ALL Cluster: outbound|53||kube-dns.kube-system.svc.cluster.local 0.0.0.0 80 ALL PassthroughCluster 10.124.0.1 443 ALL Cluster: outbound|443||kubernetes.default.svc.cluster.local 10.124.3.113 443 ALL Cluster: outbound|443||istiod.istio-system.svc.cluster.local 10.124.7.154 443 ALL Cluster: outbound|443||metrics-server.kube-system.svc.cluster.local 10.124.7.237 443 ALL Cluster: outbound|443||istio-egressgateway.istio-system.svc.cluster.local 10.124.8.250 443 ALL Cluster: outbound|443||istio-ingressgateway.istio-system.svc.cluster.local 10.124.3.113 853 ALL Cluster: outbound|853||istiod.istio-system.svc.cluster.local 0.0.0.0 8383 ALL PassthroughCluster 0.0.0.0 15001 ALL PassthroughCluster 0.0.0.0 15006 ALL Inline Route: /* 0.0.0.0 15010 ALL PassthroughCluster 10.124.3.113 15012 ALL Cluster: outbound|15012||istiod.istio-system.svc.cluster.local 0.0.0.0 15014 ALL PassthroughCluster 0.0.0.0 15021 ALL Non-HTTP/Non-TCP 10.124.8.250 15021 ALL Cluster: outbound|15021||istio-ingressgateway.istio-system.svc.cluster.local 0.0.0.0 15090 ALL Non-HTTP/Non-TCP 10.124.7.237 15443 ALL Cluster: outbound|15443||istio-egressgateway.istio-system.svc.cluster.local 10.124.8.250 15443 ALL Cluster: outbound|15443||istio-ingressgateway.istio-system.svc.cluster.local 10.124.8.250 31400 ALL Cluster: outbound|31400||istio-ingressgateway.istio-system.svc.cluster.local 从 Web 前端到客户的请求是一个向外的 HTTP 请求，端口为 80。这意味着它被移交给了0.0.0.0:80的虚拟监听器。我们可以使用 Istio CLI 按地址和端口来过滤监听器。你可以添加-o json来获得监听器的 JSON 表示：\n$ istioctl proxy-config listeners web-frontend-58d497b6f8-lwqkg --address 0.0.0.0 --port 80 -o json ... \u0026#34;rds\u0026#34;: { \u0026#34;configSource\u0026#34;: {\u0026#34;ads\u0026#34;: {}, \u0026#34;resourceApiVersion\u0026#34;: \u0026#34;V3\u0026#34; }, \u0026#34;routeConfigName\u0026#34;: \u0026#34;80\u0026#34; }, ... Listener 使用 RDS（路由发现服务）来寻找路由配置（在我们的例子中是 80）。路由附属于监听器，包含将虚拟主机映射到集群的规则。这允许我们创建流量路由规则，因为 Envoy 可以查看头文件或路径（请求元数据）并对流量进行路由。\n一个路由（route）选择一个集群（cluster）。一个集群是一组接受流量的类似的上游主机 —— 它是一个端点的集合。例如，Web 前端服务的所有实例的集合就是一个集群。我们可以在一个集群内配置弹性功能，如断路器、离群检测和 TLS 配置。\n使用 routes 命令，我们可以通过名称过滤所有的路由来获得路由的详细信息。\n$ istioctl proxy-config routes web-frontend-58d497b6f8-lwqkg --name 80 -o json [ { \u0026#34;name\u0026#34;: \u0026#34;80\u0026#34;, \u0026#34;virtualHosts\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;customers.default.svc.cluster.local:80\u0026#34;, \u0026#34;domains\u0026#34;: [ \u0026#34;customers.default.svc.cluster.local\u0026#34;, \u0026#34;customers.default.svc.cluster.local:80\u0026#34;, \u0026#34;customers\u0026#34;, \u0026#34;customers:80\u0026#34;, \u0026#34;customers.default.svc.cluster\u0026#34;, \u0026#34;customers.default.svc.cluster:80\u0026#34;, \u0026#34;customers.default.svc\u0026#34;, \u0026#34;customers.default.svc:80\u0026#34;, \u0026#34;customers.default\u0026#34;, \u0026#34;customers.default:80\u0026#34;, \u0026#34;10.124.4.23\u0026#34;, \u0026#34;10.124.4.23:80\u0026#34; ], ], \u0026#34;routes\u0026#34;: [ { \u0026#34;match\u0026#34;: {\u0026#34;prefix\u0026#34;: \u0026#34;/\u0026#34;}, \u0026#34;route\u0026#34;: { \u0026#34;cluster\u0026#34;: \u0026#34;outbound|80|v1|customers.default.svc.cluster.local\u0026#34;, \u0026#34;timeout\u0026#34;: \u0026#34;0s\u0026#34;, \u0026#34;retryPolicy\u0026#34;: { \u0026#34;retryOn\u0026#34;: \u0026#34;connect-failure,refused-stream,unavailable,cancelled,retriable-status-codes\u0026#34;, \u0026#34;numRetries\u0026#34;: 2, \u0026#34;retryHostPredicate\u0026#34;: [ {\u0026#34;name\u0026#34;: \u0026#34;envoy.retry_host_predicates.previous_hosts\u0026#34;} ], \u0026#34;hostSelectionRetryMaxAttempts\u0026#34;: \u0026#34;5\u0026#34;, \u0026#34;retriableStatusCodes\u0026#34;: [503] }, \u0026#34;maxGrpcTimeout\u0026#34;: \u0026#34;0s\u0026#34; }, ... 路由80配置为每个服务都有一个虚拟主机。然而，由于我们的请求被发送到customers.default.svc.cluster.local，Envoy 会选择与其中一个域匹配的虚拟主机（customers.default.svc.cluster.local:80）。\n一旦域被匹配，Envoy 就会查看路由，并选择第一个匹配请求的路由。由于我们没有定义任何特殊的路由规则，它匹配第一个（也是唯一的）定义的路由，并指示 Envoy 将请求发送到名为 outbound|80|v1|customers.default.svc.cluster.local 的集群。\n 注意集群名称中的 v1 是因为我们部署了一个 DestinationRule 来创建 v1 子集。如果一个服务没有子集，这部分就留空：outbound|80||customers.default.svc.cluster.local。\n 现在我们有了集群的名称，我们可以查询更多的细节。为了得到一个清楚显示 FQDN、端口、子集和其他信息的输出，你可以省略 -o json 标志。\n$ istioctl proxy-config cluster web-frontend-58d497b6f8-lwqkg --fqdn customers.default.svc.cluster.local SERVICE FQDN PORT SUBSET DIRECTION TYPE DESTINATION RULE customers.default.svc.cluster.local 80 - outbound EDS customers.default customers.default.svc.cluster.local 80 v1 outbound EDS customers.default 最后，使用集群的名称，我们可以查询请求最终将到达的实际端点：\n$ istioctl proxy-config endpoints web-frontend-58d497b6f8-lwqkg --cluster \u0026#34;outbound|80|v1|customers.default.svc.cluster.local\u0026#34; ENDPOINT STATUS OUTLIER CHECK CLUSTER 10.120.0.4:3000 HEALTHY OK outbound|80|v1|customers.default.svc.cluster.local 端点地址等于客户应用程序正在运行的 pod IP。如果我们扩展 customer 的部署，额外的端点会出现在输出中，像这样：\n$ istioctl proxy-config endpoints web-frontend-58d497b6f8-lwqkg --cluster \u0026#34;outbound|80|v1|customers.default.svc.cluster.local\u0026#34; ENDPOINT STATUS OUTLIER CHECK CLUSTER 10.120.0.4:3000 HEALTHY OK outbound|80|v1|customers.default.svc.cluster.local 10.120.3.2:3000 HEALTHY OK outbound|80|v1|customers.default.svc.cluster.local 我们也可以用下图来形象地说明上述流程。\n   Envoy 详情  ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"026696dc7264bbc7e9f645dc2ab572e5","permalink":"https://jimmysong.io/docs/istio-handbook/troubleshooting/envoy-sample/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/istio-handbook/troubleshooting/envoy-sample/","section":"istio-handbook","summary":"让我们以 Web 前端和 customer 服务为例，看看 Envoy 如何确定将请求从 Web 前端发送","tags":null,"title":"Envoy 示例","type":"book"},{"authors":null,"categories":null,"content":"作为 Istio 安装的一部分，我们安装了 Istio 的入口和出口网关。这两个网关都运行一个 Envoy 代理实例，它们在网格的边缘作为负载均衡器运行。入口网关接收入站连接，而出口网关接收从集群出去的连接。\n使用入口网关，我们可以对进入集群的流量应用路由规则。我们可以有一个指向入口网关的单一外部 IP 地址，并根据主机头将流量路由到集群内的不同服务。\n   入口和出口网关  我们可以使用 Gateway 资源来配置网关。网关资源描述了负载均衡器的暴露端口、协议、SNI（服务器名称指示）配置等。\n下面是一个网关资源的例子：\napiVersion:networking.istio.io/v1alpha3kind:Gatewaymetadata:name:my-gatewaynamespace:defaultspec:selector:istio:ingressgatewayservers:- port:number:80name:httpprotocol:HTTPhosts:- dev.example.com- test.example.com上述网关资源设置了一个代理，作为一个负载均衡器，为入口暴露 80 端口。网关配置被应用于 Istio 入口网关代理，我们将其部署到 istio-system 命名空间，并设置了标签 istio: ingressgateway。通过网关资源，我们只能配置负载均衡器。hosts 字段作为一个过滤器，只有以 dev.example.com 和 test.example.com 为目的地的流量会被允许通过。为了控制和转发流量到 Kubernetes 内部运行的实际服务，我们必须将 VirtualService 资源绑定到它。\n   Gateway 和 VirtualService  例如，我们作为 Istio 安装 demo 的一部分而部署的 Ingress 网关创建了一个具有 LoadBalancer 类型的 Kubernetes 服务，并为其分配了一个外部 IP：\n$ kubectl get svc -n istio-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE istio-egressgateway ClusterIP 10.0.146.214 \u0026lt;none\u0026gt; 80/TCP,443/TCP,15443/TCP 7m56s istio-ingressgateway LoadBalancer 10.0.98.7 XX.XXX.XXX.XXX 15021:31395/TCP,80:32542/TCP,443:31347/TCP,31400:32663/TCP,15443:31525/TCP 7m56s istiod ClusterIP 10.0.66.251 \u0026lt;none\u0026gt; 15010/TCP,15012/TCP,443/TCP,15014/TCP,853/TCP 8m6s  LoadBalancer Kubernetes 服务类型的工作方式取决于我们运行 Kubernetes 集群的方式和地点。对于云管理的集群（GCP、AWS、Azure等），在你的云账户中配置了一个负载均衡器资源，Kubernetes LoadBalancer 服务将获得一个分配给它的外部 IP 地址。假设我们正在使用 Minikube 或 Docker Desktop。在这种情况下，外部 IP 地址将被设置为 localhost（Docker Desktop），或者，如果我们使用 Minikube，它将保持待定，我们将不得不使用 minikube tunnel 命令来获得一个IP地址。\n 除了入口网关，我们还可以部署一个出口网关来控制和过滤离开网格的流量。\n就像我们配置入口网关一样，我们可以使用相同的网关资源来配置出口网关。这使我们能够集中管理所有流出的流量、日志和授权。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"fe00d869bdc5d1b09768f644c4e00f9c","permalink":"https://jimmysong.io/docs/istio-handbook/traffic-management/gateway/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/istio-handbook/traffic-management/gateway/","section":"istio-handbook","summary":"作为 Istio 安装的一部分，我们安装了 Istio 的入口和出口网关。这两个网关","tags":null,"title":"Gateway","type":"book"},{"authors":null,"categories":null,"content":"Istio 是最受欢迎和发展最快的开源项目之一。它的发布时间表对企业的生命周期和变更管理实践来说可能非常激进。GetMesh 通过针对不同的 Kubernetes 分发版测试所有 Istio 版本以确保功能的完整性来解决这一问题。GetMesh 的 Istio 版本在安全补丁和其他错误更新方面得到积极的支持，并拥有比上游 Istio 提供的更长的支持期。\n一些服务网格客户需要支持更高的安全要求。GetMesh 通过提供两种 Istio 发行版来解决合规性问题。\n tetrate 发行版，跟踪上游 Istio 并可能应用额外的补丁。 tetratefips 发行版，是符合 FIPS 标准的 tetrate 版本。  如何开始使用？ 第一步是下载 GetMesh CLI。你可以在 macOS 和 Linux 平台上安装 GetMesh。我们可以使用以下命令来下载最新版本的 GetMesh 和认证的 Istio。\n如何开始？ 第一步是下载 GetMesh CLI。你可以在 macOS 和 Linux 平台上安装 GetMesh。我们可以使用以下命令下载最新版本的 GetMesh 并认证 Istio。\ncurl -sL https://istio.tetratelabs.io/getmesh/install.sh | bash 我们可以运行 version 命令以确保 GetMesh 被成功安装。例如：\n$ getmesh version getmesh version: 1.1.1 active istioctl: 1.8.3-tetrate-v0 no running Istio pods in \u0026#34;istio-system\u0026#34; 版本命令输出 GetMesh 的版本、活跃的 Istio CLI 的版本以及 Kubernetes 集群上安装的 Istio 的版本。\n使用 GetMesh 安装 Istio GetMesh 通过 Kubernetes 配置文件与活跃的 Kubernetes 集群进行通信。\n要在当前活动的 Kubernetes 集群上安装 Istio 的演示配置文件，我们可以像这样使用 getmesh istioctl 命令：\ngetmesh istioctl install --set profile=demo 该命令将检查集群，以确保它准备好安装 Istio，一旦你确认，安装程序将继续使用选定的配置文件安装 Istio。\n如果我们现在检查版本，你会注意到输出显示控制平面和数据平面的版本。\n验证配置 config-validate 命令允许你对当前配置和任何尚未应用的 YAML 清单进行验证。\n该命令使用外部资源调用一系列验证，如上游 Istio 验证、Kiali 库和 GetMesh 自定义配置检查。\n下面是一个命令输出的例子，如果没有标记为 Istio 注入的命名空间。\n$ getmesh config-validate Running the config validator. This may take some time... 2021-08-02T19:20:33.873244Z info klog Throttling request took 1.196458809s, request: GET:https://35.185.226.9/api/v1/namespaces/istio-system/configmaps/istio[] NAMESPACE NAME RESOURCE TYPE ERROR CODE SEVERITY MESSAGE default default Namespace IST0102 Info The namespace is not enabled for Istio injection. Run \u0026#39;kubectl label namespace default istio-injection=enabled\u0026#39; to enable it, or \u0026#39;kubectl label namespace default istio-injection=disabled\u0026#39; to explicitly mark it as not needing injection. The error codes of the found issues are prefixed by \u0026#39;IST\u0026#39; or \u0026#39;KIA\u0026#39;. For the detailed explanation, please refer to - https://istio.io/latest/docs/reference/config/analysis/ for \u0026#39;IST\u0026#39; error codes - https://kiali.io/documentation/latest/validations/ for \u0026#39;KIA\u0026#39; error codes 同样，你也可以传入一个 YAML 文件来验证它，然后再将它部署到集群。例如：\n$ getmesh config-validate my-resources.yaml 管理多个 Istio CLI 我们可以使用 show 命令来列出当前下载的 Istio 版本：\ngetmesh show 输出如下所示：\n1.8.2-tetrate-v0 1.8.3-tetrate-v0 (Active) 如果我们想使用的版本在电脑上没有，我们可以使用 getmesh list 命令来列出所有可信的 Istio 版本：\n$ getmesh list ISTIO VERSION FLAVOR FLAVOR VERSION K8S VERSIONS *1.9.5 tetrate 0 1.17,1.18,1.19,1.20 1.9.5 istio 0 1.17,1.18,1.19,1.20 1.9.4 tetrate 0 1.17,1.18,1.19,1.20 1.9.4 istio 0 1.17,1.18,1.19,1.20 1.9.0 tetrate 0 1.17,1.18,1.19,1.20 1.9.0 tetratefips 1 1.17,1.18,1.19,1.20 1.9.0 istio 0 1.17,1.18,1.19,1.20 1.8.6 tetrate 0 1.16,1.17,1.18,1.19 1.8.6 istio 0 1.16,1.17,1.18,1.19 1.8.5 tetrate 0 1.16,1.17,1.18,1.19 1.8.5 istio 0 1.16,1.17,1.18,1.19 1.8.3 tetrate 0 1.16,1.17,1.18,1.19 1.8.3 tetratefips 1 1.16,1.17,1.18,1.19 1.8.3 istio 0 1.16,1.17,1.18,1.19 1.7.8 tetrate 0 1.16,1.17,1.18 1.7.8 istio 0 1.16,1.17,1.18 要获取一个特定的版本（比方说1.8.2 tetratefips），我们可以使用 fetch 命令：\ngetmesh fetch --version 1.9.0 --flavor tetratefips --flavor-version 1 当上述命令完成后，GetMesh 将获取的 Istio CLI 版本设置为 Istio CLI 的活动版本。例如，运行 show 命令现在显示 tetratefips 1.9.0 版本是活跃的：\n$ getmesh show 1.9.0-tetratefips-v1 (Active) 1.9.5-tetrate-v0 同样，如果我们运行 getmesh istioctl version ，我们会发现正在使用的 Istio CLI 的版本：\n$ getmesh istioctl version client version: 1.9.0-tetratefips-v1 control plane version: 1.9.5-tetrate-v0 data plane version: 1.9.5-tetrate-v0 (2 proxies) 要切换到不同版本的 Istio CLI，我们可以运行 getmesh switch 命令：\ngetmesh switch --version 1.9.5 --flavor tetrate --flavor-version 0 CA 集成 我们没有使用自签的根证书，而是从 GCP CAS（证书授权服务）获得一个中间的 Istio 证书授权（CA）来签署工作负载证书。\n假设你已经配置了你的 CAS 实例，你可以用 CA 的参数创建一个 YAML 配置。下面是 YAML 配置的一个例子：\nproviderName:\u0026#34;gcp\u0026#34;providerConfig:gcp:# 这将持有你在 GCP 上创建的证书授权的完整 CA 名称casCAName:\u0026#34;projects/tetrate-io-istio/locations/us-west1/certificateAuthorities/tetrate-example-io\u0026#34;certificateParameters:secretOptions:istioCANamespace:\u0026#34;istio-system\u0026#34;# `cacerts` secrets 所在的命名空间overrideExistingCACertsSecret:true# 重写已存在的 `cacerts` secret，使用新的替换caOptions:validityDays:365# CA 到期前的有效天数keyLength:2048# 创建的 key 的比特数certSigningRequestParams:# x509.CertificateRequest；大部分字段省略subject:commonname:\u0026#34;tetrate.example.io\u0026#34;country:- \u0026#34;US\u0026#34;locality:- \u0026#34;Sunnyvale\u0026#34;organization:- \u0026#34;Istio\u0026#34;organizationunit:- \u0026#34;engineering\u0026#34;emailaddresses:- \u0026#34;youremail@example.io\u0026#34;配置到位后，你可以使用 gen-ca 命令来创建 cacert。\ngetmesh gen-ca --config-file gcp-cas-config.yaml 该命令在 istio-system 中创建 cacerts Kubernetes Secret。为了让 istiod 接受新的 cert，你必须重新启动 istiod。\n如果你创建一个 sample 工作负载，并检查所使用的证书，你会发现 CA 是发布工作负载的那个。\nIstio CA certs 集成可用于 GCP CA 服务和 AWS Private CA 服务。\n发现选择器 发现选择器是 Istio 1.10 中引入的新功能之一。发现选择器允许我们控制 Istio 控制平面观察和发送配置更新的命名空间。\n默认情况下，Istio 控制平面会观察和处理集群中所有 Kubernetes 资源的更新。服务网格中的所有 Envoy代理的配置方式是，它们可以到达服务网格中的每个工作负载，并接受与工作负载相关的所有端口的流量。\n例如，我们在不同的命名空间部署了两个工作负载——foo 和 bar。尽管我们知道 foo 永远不会与 bar 通信，反之亦然，但一个服务的端点将被包含在另一个服务的已发现端点列表中。\n   Foo bar  如果我们运行 istioctl proxy-config 命令，列出 foo 命名空间的 foo 工作负载可以看到的所有端点，你会注意到一个名为 bar 的服务条目：\n$ istioctl proxy-config endpoints deploy/foo.foo ENDPOINT STATUS OUTLIER …","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"4a0a0713a64f92fb4edb014ff64e162f","permalink":"https://jimmysong.io/docs/istio-handbook/setup/getmesh/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/istio-handbook/setup/getmesh/","section":"istio-handbook","summary":"Istio 是最受欢迎和发展最快的开源项目之一。它的发布时间表对企业的","tags":null,"title":"GetMesh","type":"book"},{"authors":null,"categories":null,"content":"过滤器链匹配允许我们指定为监听器选择特定过滤器链的标准。\n我们可以在配置中定义多个过滤器链，然后根据目标端口、服务器名称、协议和其他属性来选择和执行它们。例如，我们可以检查哪个主机名正在连接，然后选择不同的过滤器链。如果主机名 hello.com 连接，我们可以选择一个过滤器链来呈现该特定主机名的证书。\n在 Envoy 开始过滤器匹配之前，它需要有一些由监听器过滤器从接收的数据包中提取的数据。之后，Envoy 要选择一个特定的过滤器链，必须满足所有的匹配条件。例如，如果我们对主机名和端口进行匹配，这两个值都需要匹配，Envoy 才能选择该过滤器链。\n匹配顺序如下：\n 目的地端口（当使用 use_original_dst 时） 目的地 IP 地址 服务器名称（TLS 协议的 SNI） 传输协议 应用协议（TLS 协议的 ALPN） 直接连接的源 IP 地址（这只在我们使用覆盖源地址的过滤器时与源 IP 地址不同，例如，代理协议监听器过滤器） 来源类型（例如，任何、本地或外部网络） 源 IP 地址 来源端口  具体标准，如服务器名称 / SNI 或 IP 地址，也允许使用范围或通配符。如果在多个过滤器链中使用通配符标准，最具体的值将被匹配。\n例如，对于 www.hello.com 从最具体到最不具体的匹配顺序是这样的。\n www.hello.com *.hello.com *.com 任何没有服务器名称标准的过滤器链  下面是一个例子，说明我们如何使用不同的属性配置过滤器链匹配。\nfilter_chains:- filter_chain_match:server_names:- \u0026#34;*.hello.com\u0026#34;filters:...- filter_chain_match:source_prefix_ranges:- address_prefix:192.0.0.1prefix_len:32filters:...- filter_chain_match:transport_protocol:tlsfilters:...让我们假设一个 TLS 请求从 IP 地址进来，并且 192.0.0.1 SNI 设置为 v1.hello.com。记住这个顺序，第一个满足所有条件的过滤器链匹配是服务器名称匹配（v1.hello.com）。因此，Envoy 会执行该匹配下的过滤器。\n但是，如果请求是从 IP 192.0.0.1 进来的，那就不是 TLS，而且 SNI 也不符合 *.hello.com 的要求。Envoy 将执行第二个过滤器链——与特定 IP 地址相匹配的那个。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"c9972e80d89af667062f5e4b8f53187c","permalink":"https://jimmysong.io/docs/envoy-handbook/listener/filter-chain-matching/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/envoy-handbook/listener/filter-chain-matching/","section":"envoy-handbook","summary":"过滤器链匹配允许我们指定为监听器选择特定过滤器链的标准。 我们","tags":null,"title":"Header 操作","type":"book"},{"authors":null,"categories":null,"content":"Envoy 具有一个内置的 HTTP Lua 过滤器，允许在请求和响应流中运行 Lua 脚本。Lua 是一种可嵌入的脚本语言，主要在嵌入式系统和游戏中流行。Envoy 使用 LuaJIT（Lua 的即时编译器）作为运行时。LuaJIT 支持的最高 Lua 脚本版本是 5.1，其中一些功能来自 5.2。\n在运行时，Envoy 为每个工作线程创建一个 Lua 环境。正因为如此，没有真正意义上的全局数据。任何在加载时创建和填充的全局数据都可以从每个独立的工作线程中看到。\nLua 脚本是以同步风格的 coroutines 运行的，即使它们可能执行复杂的异步任务。这使得它更容易编写。Envoy 通过一组 API 执行所有的网络 / 异步处理。当一个异步任务被调用时，Envoy 会暂停脚本的执行，等到异步操作完成就会恢复。\n我们不应该从脚本中执行任何阻塞性操作，因为这会影响 Envoy 的性能。我们应该只使用 Envoy 的 API 来进行所有的 IO 操作。\n我们可以使用 Lua 脚本修改和 / 或检查请求和响应头、正文和 Trailer。我们还可以对上游主机进行出站异步 HTTP 调用，或者执行直接响应，跳过任何进一步的过滤器迭代。例如，在 Lua 脚本中，我们可以进行上游的 HTTP 调用并直接响应，而不继续执行其他过滤器。\n如何配置 Lua 过滤器 Lua 脚本可以使用 inline_code 字段进行内联定义，或者使用过滤器上的 source_codes 字段引用本地文件。\nname:envoy.filters.http.luatyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.http.lua.v3.Luainline_code:|-- Called on the request path. function envoy_on_request(request_handle) -- Do something. end -- Called on the response path. function envoy_on_response(response_handle) -- Do something. endsource_codes:myscript.lua:filename:/scripts/myscript.luaEnvoy 将上述脚本视为全局脚本，对每一个 HTTP 请求都会执行它。在每个脚本中可以定义两个全局函数。\nfunction envoy_on_request(request_handle) end 和\nfunction envoy_on_response(response_handle) end envoy_on_request 函数在请求路径上被调用，而 envoy_on_response 脚本则在响应路径上被调用。每个函数都接收一个句柄，该句柄有不同的定义方法。脚本可以包含响应或请求函数，也可以包含两者。\n我们也有一个选项，可以在虚拟主机、路由或加权集群级别上按路由禁用或改写脚本。\n使用 typed_per_filter_config 字段来禁用或引用主机、路由或加权集群层面上的现有 Lua 脚本。例如，下面是如何使用 typed_per_filter_config 来引用一个现有的脚本（例如：some-script.lua）。\ntyped_per_filter_config:envoy.filters.http.lua。\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions. filters.http.lua.v3.LuaPerRoutename:some-script.lua同样地，我们可以这样定义 source_code 和 inline_string 字段，而不是指定 name 字段。\ntyped_per_filter_config:envoy.filters.http.lua:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.http.lua.v3.LuaPerRoutesource_code:inline_string:|function envoy_on_response(response_handle) -- Do something on response. end流处理 API 我们在前面提到，request_handle 和 response_handle 流句柄会被传递给全局 request 和 response 函数。\n在流句柄上可用的方法包括 headers、body、metadata、各种日志方法（如 logTrace、logInfo、logDebug…）、httpCall、connection等等。你可以在 Lua 过滤器源码中找到完整的方法列表。\n除了流对象外，API 还支持以下对象：\n Header 对象（由 headers() 方法返回） 缓冲区对象（由 body() 方法返回）。 动态元数据对象（由 metadata() 方法返回） Stream 信息对象（由 streamInfo() 方法返回） 连接对象（通过 connection() 方法返回） SSL 连接信息对象（由连接对象的 ssl() 方法返回）  你会在那里看到如何使用 Lua 实验中的一些对象和方法。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"e76fa816c872b5f2c303c616bcbdf37d","permalink":"https://jimmysong.io/docs/envoy-handbook/extending-envoy/lua-filter/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/envoy-handbook/extending-envoy/lua-filter/","section":"envoy-handbook","summary":"Envoy 具有一个内置的 HTTP Lua 过滤器，允许在请求和响应流中运行 Lua 脚本。","tags":null,"title":"Lua 过滤器","type":"book"},{"authors":null,"categories":null,"content":"Prometheus 是一个开源的监控系统和时间序列数据库。Istio 使用 Prometheus 来记录指标，跟踪 Istio 和网格中的应用程序的健康状况。\n要安装 Prometheus，我们可以使用 Istio 安装包中 /samples/addons 文件夹中的示例安装。\n$ kubectl apply -f istio-1.9.0/samples/addons/prometheus.yaml serviceaccount/prometheus created configmap/prometheus created clusterrole.rbac.authorization.k8s.io/prometheus created clusterrolebinding.rbac.authorization.k8s.io/prometheus created service/prometheus created deployment.apps/prometheus created 要打开 Prometheus 仪表板，我们可以使用 Istio CLI 中的 dashboard 命令：\n$ istioctl dashboard prometheus http://localhost:9090 现在我们可以在浏览器中打开 http://localhost:9090，进入 Prometheus 仪表板，如下图所示。\n   Prometheus dashboard  部署示例应用 为了看到一些请求和流量，我们将部署一个 Nginx 实例：\n$ kubectl create deploy my-nginx --image=nginx deployment.apps/my-nginx created 为了能够产生一些流量并访问 Nginx Pod，我们需要以某种方式让它被访问。\n最简单的方法是将 Nginx 部署作为 Kubernetes LoadBalancer 服务公开：\nkubectl expose deployment my-nginx --type=LoadBalancer --name=my-nginx --port 80  注意：在课程的后面，我们将学习如何使用 Istio 资源并通过 Istio 的入口网关暴露服务。\n 现在我们可以运行 kubectl get services，获得 my-nginx 服务的外部 IP 地址：\n$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.48.0.1 \u0026lt;none\u0026gt; 443/TCP 73m my-nginx LoadBalancer 10.48.0.94 [IP HERE] 80:31191/TCP 4m6s 让我们把这个 IP 地址存储为一个环境变量，这样我们就可以在整个实验中使用它。\nexport NGINX_IP=$(kubectl get service my-nginx -o jsonpath=\u0026#39;{.status.loadBalancer.ingress[0].ip}\u0026#39;) 现在你可以对上述 IP 运行 curl，你应该得到默认的 Nginx 页面。\n$ curl $NGINX_IP \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt; \u0026lt;style\u0026gt; body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Welcome to nginx!\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;If you see this page, the nginx web server is successfully installed and working. Further configuration is required.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;For online documentation and support please refer to \u0026lt;a href=\u0026#34;http://nginx.org/\u0026#34;\u0026gt;nginx.org\u0026lt;/a\u0026gt;.\u0026lt;br/\u0026gt; Commercial support is available at \u0026lt;a href=\u0026#34;http://nginx.com/\u0026#34;\u0026gt;nginx.com\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;em\u0026gt;Thank you for using nginx.\u0026lt;/em\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 让我们向我们在开始时创建的 $NGINX_IP 环境变量提出几个请求。然后，从 Prometheus UI 中，你可以搜索 Istio 的一个指标（例如 istio_requests_total），以了解哪些数据点正在被收集。\n下面是一个来自 Prometheus 用户界面的示例元素：\nistio_requests_total{app=\u0026#34;my-nginx\u0026#34;, connection_security_policy=\u0026#34;none\u0026#34;, destination_app=\u0026#34;my-nginx\u0026#34;, destination_canonical_revision=\u0026#34;latest\u0026#34;, destination_canonical_service=\u0026#34;my-nginx\u0026#34;, destination_cluster=\u0026#34;Kubernetes\u0026#34;, destination_principal=\u0026#34;unknown\u0026#34;, destination_service=\u0026#34;my-nginx.default.svc.cluster.local\u0026#34;, destination_service_name=\u0026#34;my-nginx\u0026#34;, destination_service_namespace=\u0026#34;default\u0026#34;, destination_version=\u0026#34;unknown\u0026#34;, destination_workload=\u0026#34;my-nginx\u0026#34;, destination_workload_namespace=\u0026#34;default\u0026#34;, instance=\u0026#34;10.92.4.4:15020\u0026#34;, istio_io_rev=\u0026#34;default\u0026#34;, job=\u0026#34;kubernetes-pods\u0026#34;, kubernetes_namespace=\u0026#34;default\u0026#34;, kubernetes_pod_name=\u0026#34;my-nginx-6b74b79f57-r59sf\u0026#34;, pod_template_hash=\u0026#34;6b74b79f57\u0026#34;, reporter=\u0026#34;destination\u0026#34;, request_protocol=\u0026#34;http\u0026#34;, response_code=\u0026#34;200\u0026#34;, response_flags=\u0026#34;-\u0026#34;, security_istio_io_tlsMode=\u0026#34;istio\u0026#34;, service_istio_io_canonical_name=\u0026#34;my-nginx\u0026#34;, service_istio_io_canonical_revision=\u0026#34;latest\u0026#34;, source_app=\u0026#34;unknown\u0026#34;, source_canonical_revision=\u0026#34;latest\u0026#34;, source_canonical_service=\u0026#34;unknown\u0026#34;, source_cluster=\u0026#34;unknown\u0026#34;, source_principal=\u0026#34;unknown\u0026#34;, source_version=\u0026#34;unknown\u0026#34;, source_workload=\u0026#34;unknown\u0026#34;, source_workload_namespace=\u0026#34;unknown\u0026#34;}\t12 ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"c9495ff4fabf6f0fba08dbff741cefb0","permalink":"https://jimmysong.io/docs/istio-handbook/observability/prometheus/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/istio-handbook/observability/prometheus/","section":"istio-handbook","summary":"Prometheus 是一个开源的监控系统和时间序列数据库。Istio 使用 Prometheus 来记","tags":null,"title":"Prometheus","type":"book"},{"authors":null,"categories":null,"content":"RequestAuthentication（请求认证）定义了工作负载支持哪些请求认证方法。如果请求包含无效的认证信息，它将根据配置的认证规则拒绝该请求。不包含任何认证凭证的请求将被接受，但不会有任何认证的身份。\n示例 为了限制只对经过认证的请求进行访问，应该伴随着一个授权规则。\n例如，要求对具有标签 app:httpbin 的工作负载的所有请求使用 JWT 认证。\napiVersion:security.istio.io/v1beta1kind:RequestAuthenticationmetadata:name:httpbinnamespace:foospec:selector:matchLabels:app:httpbinjwtRules:- issuer:\u0026#34;issuer-foo\u0026#34;jwksUri:https://example.com/.well-known/jwks.json---apiVersion:security.istio.io/v1beta1kind:AuthorizationPolicymetadata:name:httpbinnamespace:foospec:selector:matchLabels:app:httpbinrules:- from:- source:requestPrincipals:[\u0026#34;*\u0026#34;]下一个例子展示了如何为不同的 host 设置不同的 JWT 要求。RequestAuthentication 声明它可以接受由 issuer-foo 或 issuer-bar 签发的 JWT（公钥集是由 OpenID Connect 规范隐性设置的）。\napiVersion:security.istio.io/v1beta1kind:RequestAuthenticationmetadata:name:httpbinnamespace:foospec:selector:matchLabels:app:httpbinjwtRules:- issuer:\u0026#34;issuer-foo\u0026#34;- issuer:\u0026#34;issuer-bar\u0026#34;---apiVersion:security.istio.io/v1beta1kind:AuthorizationPolicymetadata:name:httpbinnamespace:foospec:selector:matchLabels:app:httpbinrules:- from:- source:requestPrincipals:[\u0026#34;issuer-foo/*\u0026#34;]to:- operation:hosts:[\u0026#34;example.com\u0026#34;]- from:- source:requestPrincipals:[\u0026#34;issuer-bar/*\u0026#34;]to:- operation:hosts:[\u0026#34;another-host.com\u0026#34;]你可以对授权策略进行微调，为每个路径设置不同的要求。例如，除了 /healthz，所有路径都需要 JWT，可以使用相同的 RequestAuthentication，但授权策略可以是：\napiVersion:security.istio.io/v1beta1kind:AuthorizationPolicymetadata:name:httpbinnamespace:foospec:selector:matchLabels:app:httpbinrules:- from:- source:requestPrincipals:[\u0026#34;*\u0026#34;]- to:- operation:paths:[\u0026#34;/healthz\u0026#34;]关于 RequestAuthentication 配置的详细用法请参考 Istio 官方文档。\n参考  RequestAuthentication - istio.io  ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"2489eadc16d5f1d5dd48b63f9dd2b217","permalink":"https://jimmysong.io/docs/istio-handbook/config-security/request-authentication/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/istio-handbook/config-security/request-authentication/","section":"istio-handbook","summary":"RequestAuthentication（请求认证）定义了","tags":null,"title":"RequestAuthentication","type":"book"},{"authors":null,"categories":null,"content":"Slime 是由网易数帆微服务团队开源的一款基于 Istio 的智能网格管理器。Slime 基于 Kubernetes Operator 实现，可作为 Istio 的 CRD 管理器，无须对 Istio 做任何定制化改造，就可以定义动态的服务治理策略，从而达到自动便捷使用 Istio 和 Envoy 高阶功能的目的。\nSlime 试图解决的问题 Slime 项目的诞生主要为了解决以下问题：\n 网格内所有服务配置全量下到所有 Sidecar Proxy，导致其消耗大量资源使得应用性能变差的问题 如何在 Istio 中实现高阶扩展的问题：比如扩展 HTTP 插件；根据服务的资源使用率做到自适应限流  Slime 解决以上问题的答案是构建 Istio 的控制平面，具体做法是：\n 构建可拔插控制器 数据平面监控 CRD 转换  通过以上方式 Slime 可以实现配置懒加载和插件管理器。\nSlime 架构 Slime 内部分为三大模块，其架构图如下所示。\n   Slime 内部架构图  Slime 内部三大组件为：\n slime-boot：在 Kubernetes 上部署 Slime 模块的 operator。 slime-controller：Slime 的核心组件，监听 Slime CRD 并将其转换为Istio CRD。 slime-metric：用于获取服务 metrics 信息的组件，slime-controller 会根据其获取的信息动态调整服务治理规则。  目前 Slime 内置了三个控制器子模块：\n 配置懒加载（按需加载）：用户无须手动配置 SidecarScope，Istio 可以按需加载服务配置和服务发现信息； HTTP 插件管理：使用新的 CRD——pluginmanager/envoyplugin 包装了可读性，摒弃了可维护性较差的 envoyfilter，使得插件扩展更为便捷； 自适应限流：结合监控信息自动调整限流策略；   什么是 SidecarScope？\nSidecarScope 是在 Istio 1.1 版本中引入的，它并不是一个直接面向用户的配置项，而是 Sidecar 资源的包装器，具体来说就是 Sidecar 资源中的 egress 选项。通过该配置可以减少 Istio 向 Sidecar 下发的数据量，例如只向某个命名空间中的某些服务下发某些 hosts 的访问配置，从而提高应用提高性能。\n 使用 Slime 作为 Istio 的控制平面 为了解决这些问题，Slime 在 Istio 之上构建了更高层次的抽象，相当于为 Istio 构建了一层管理平面，其工作流程图如下所示。\n   Slime 工作流程图  具体步骤如下：\n Slime Operator 根据管理员的配置在 Kubernetes 中完成 Slime 组件的初始化； 开发者创建符合 Slime CRD 规范的配置并应用到 Kubernetes 集群中； Slime 查询 Prometheus 中保存的相关服务的监控数据，结合 Slime CRD 中自适应部分的配置，将 Slime CRD 转换为 Istio CRD，同时将其推送到 Global Proxy 中； Istio 监听 Istio CRD 的创建； Istio 将 Sidecar Proxy 的配置信息推送到数据平面相应的 Sidecar Proxy 中；  以上只是一个对 Slime 工作流程的一个笼统的介绍，更多详细信息请参考 Slime GitHub。\n配置懒加载 为了解决数据平面中 Sidecar Proxy 资源消耗过大及网络延迟问题，Slime 使用了配置懒加载（按需加载 Sidecar 配置）的方案。该方案的核心思想是向每个 Sidecar Proxy 中只下发其所 Pod 中服务所需的配置，而不是将网格中的所有服务信息全量下发。所以 Slime 需要获取每个服务的调用关系这样才能得到其所需的 Sidecar Proxy 配置。\nSlime 实现 Sidecar Proxy 配置懒加载的方法是：\n 让数据平面中的所有服务的首次调用都通过一个 Global Proxy，该 Proxy 可以记录所有服务的调用和依赖信息，根据该依赖信息更新 Istio 中 Sidecar 资源的配置； 当某个服务的调用链被 VirtualService 中的路由信息重新定义时， Global Proxy 原有记录就失效了，需要一个新的数据结构来维护该服务的调用关系。Slime 创建了名为 ServiceFence 的 CRD 来维护服务调用关系以解决服务信息缺失问题。  使用 Global Proxy 初始化服务调用拓扑 Slime 在数据平面中部署 Global Proxy（也叫做 Global Sidecar，但其与应用的 Pod 不是一对一的关系，笔者更倾向于称其为 Global Proxy），该代理同样使用 Envoy 构建，在每个需要启动配置懒加载的命名空间中部署一个或在整个网格中只部署一个，所有缺失服务发现信息的调用（你也可以手动配置服务调用关系），都会被兜底路由劫持到 Global Proxy，经过其首次转发后，Slime 便可感知到被调用方的信息，然后根据其对应服务的 VirtualService，找到服务名和真实后端的映射关系，将两者的都加入 SidecarScope，以后该服务的调用就不再需要经过 Global Proxy 了。\n使用 ServiceFence 维护服务调用拓扑 在使用 Global Proxy 初始化服务调用拓扑后，一旦服务调用链有变动的话怎么办？对此 Slime 创建了 ServiceFence 的 CRD。使用 ServiceFence 可以维护服务名和后端服务的映射关系。Slime 根据其对应服务的 VirtualService，找到 Kubernetes 服务名和真实后端（host）的映射关系，将两者的都加入 Sidecar 的配置中。ServiceFence 管理生成的 SidecarScope 的生命周期，自动清理长时间不用的调用关系，从而避免上述问题。\n如何开启配置懒加载 配置懒加载功能对于终端用户是透明的，只需要 Kubernetes Service 上打上 istio.dependency.servicefence/status:\u0026#34;true\u0026#34; 的标签，表明该服务需要开启配置懒加载，剩下的事情交给 Slime Operator 来完成即可。\nHTTP 插件管理 Istio 中的插件扩展只能通过 EnvoyFilter 来实现，因为它是 xDS 层面的配置，管理和维护这样的配置需要耗费大量的精力，也极容易出错。因此，Slime 在 EnvoyFilter 的基础上做了一层面向插件的抽象。\nSlime 共有两个 CRD 用于 HTTP 插件管理，分别是：\n PluginManager：配置为哪些负载开启哪些插件，插件的配置顺序即为执行顺序； EnvoyPlugin：EnvoyPlugin 不关心每个插件的具体配置，具体配置会被放在 EnvoyFilter 资源的 patch.typed_config 结构中透传），EnvoyPlugin 的核心思想是将插件配置在需要的维度中做聚合，从而限定插件的生鲜范围。这样做一方面更加贴合插件使用者的习惯，另一方面也降低了上层配置的冗余，  自适应限流 Envoy 内置的限流组件功能单一，只能以实例维度配置限流值，无法做到根据应用负载的自适应限流。Slime 通过与 Prometheus metric server 对接，实时的获取监控情况，来动态配置限流值。\nSlime 自适应限流的流程图如下所示。\n   Slime 的自适应限流流程图  Slime 的自适应限流的流程分为两部分，一部分为 SmartLimiter 到 EnvoyFilter 的转换，另一部分为获取监控数据。目前 Slime 支持从 Kubernetes Metric Server 获取服务的CPU、内存、副本数等数据。Slime 还对外提供了一套监控数据接口（Metric Discovery Server），通过 MDS，可以将自定义的监控指标同步给限流组件。\nSlime 创建的 CRD SmartLimiter 用于配置自适应限流。其的配置是接近自然语义，例如希望在 CPU 超过 80% 时触发服务 A 的访问限制，限额为 30QPS，对应的SmartLimiter 定义如下：\napiVersion:microservice.netease.com/v1alpha1kind:SmartLimitermetadata:name:anamespace:defaultspec:descriptors:- action:fill_interval:seconds:1quota:\u0026#34;30/{pod}\u0026#34;# 30为该服务的额度，将其均分给每个 pod，加入有 3 个 pod，则每个 pod 的限流为 10condition:\u0026#34;{cpu}\u0026gt;0.8\u0026#34;# 根据监控项{cpu}的值自动填充该模板更多 Slime 开源于 2021 年初，本文发稿时该项目仍处于初级阶段，本文大量参考了杨笛航在云原生社区中的分享 Slime：让 Istio 服务网格变得更加高效与智能 及 Slime 的 GitHub。感兴趣的读者可以关注下这个项目的 GitHub，进一步了解它。\n参考  Slime：让 Istio 服务网格变得更加高效与智能 - cloudnative.to Slime GitHub 文档 - github.com Sidecar - istio.io  ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"e4aec3275c747b431d3b261e6d4ea25e","permalink":"https://jimmysong.io/docs/istio-handbook/ecosystem/slime/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/istio-handbook/ecosystem/slime/","section":"istio-handbook","summary":"Slime 是由网易数帆微服务团队开源的一款基于 Istio 的智能网格管理器。S","tags":null,"title":"Slime","type":"book"},{"authors":null,"categories":null,"content":"VirtualService 主要配置流量路由。以下是在流量路由背景下定义的几个有用的术语。\n Service 是与服务注册表（service registry）中的唯一名称绑定的应用行为单元。服务由多个网络端点（endpoint）组成，这些端点由运行在 pod、容器、虚拟机等的工作负载实例实现。 服务版本，又称子集（subset）：在持续部署方案中，对于一个给定的服务，可能有不同的实例子集，运行应用程序二进制的不同变体。这些变体不一定是不同的 API 版本。它们可能是同一服务的迭代变化，部署在不同的环境（prod、staging、dev 等）。发生这种情况的常见场景包括 A/B 测试、金丝雀发布等。一个特定版本的选择可以根据各种标准（header、URL 等）和 / 或分配给每个版本的权重来决定。每个服务都有一个由其所有实例组成的默认版本。 源（source）：下游客户端调用服务。 Host：客户端在尝试连接到服务时使用的地址。 访问模型（access model）：应用程序只针对目标服务（host），而不了解各个服务版本（子集）。版本的实际选择是由代理/sidecar 决定的，使应用程序代码脱离依赖服务。 VirtualService 定义了一套当目标服务（host）被寻址时应用的流量路由规则。每个路由规则定义了特定协议流量的匹配标准。如果流量被匹配，那么它将被发送到注册表中定义的指定目标服务（或它的子集/版本）。  流量的来源也可以在路由规则中进行匹配。这允许为特定的客户环境定制路由。\n示例 以下是 Kubernetes 上的例子，默认情况下，所有的 HTTP 流量都会被路由到标签为 version: v1 的 reviews 服务的 pod 上。此外，路径以 /wpcatalog/ 或 /consumercatalog/ 开头的 HTTP 请求将被重写为 /newcatalog，并被发送到标签为 version: v2 的 pod 上。\napiVersion:networking.istio.io/v1alpha3kind:VirtualServicemetadata:name:reviews-routespec:hosts:- reviews.prod.svc.cluster.localhttp:- name:\u0026#34;reviews-v2-routes\u0026#34;match:- uri:prefix:\u0026#34;/wpcatalog\u0026#34;- uri:prefix:\u0026#34;/consumercatalog\u0026#34;rewrite:uri:\u0026#34;/newcatalog\u0026#34;route:- destination:host:reviews.prod.svc.cluster.localsubset:v2- name:\u0026#34;reviews-v1-route\u0026#34;route:- destination:host:reviews.prod.svc.cluster.localsubset:v1途径目的地的子集/版本是通过对命名的服务子集的引用来识别的，这个子集必须在相应的 DestinationRule 中声明。\napiVersion:networking.istio.io/v1alpha3kind:DestinationRulemetadata:name:reviews-destinationspec:host:reviews.prod.svc.cluster.localsubsets:- name:v1labels:version:v1- name:v2labels:version:v2配置项 下图是 VirtualService 资源的配置拓扑图。\n  VirtualService 资源配置拓扑图  VirtualService 资源的顶级配置项如下：\n  hosts：字符串数组格式。配置流量被发送到的目标主机。可以是一个带有通配符前缀的 DNS 名称或 IP 地址。根据平台的不同，也可以使用短名称来代替 FQDN（即名称中没有点）。在这种情况下，主机的 FQDN 将根据底层平台衍生出来。单个 VirtualService 可以用来描述相应主机的所有流量属性，包括多个 HTTP 和 TCP 端口的属性。另外，可以使用一个以上的 VirtualService 来定义主机的流量属性，但要注意一些问题。详情请参考《流量管理最佳实践》。\nKubernetes 用户的注意事项。当使用短名称时（例如 reviews 而不是 reviews.default.svc.cluster.local），Istio 将根据规则的命名空间而不是服务来解释短名称。在 default 命名空间中包含主机 reviews 的规则将被解释为 reviews.default.svc.cluster.local，而不考虑与 reviews 服务相关的实际命名空间。为了避免潜在的错误配置，建议总是使用完全限定名而不是短名称。\nhosts 字段同时适用于 HTTP 和 TCP 服务。网格内的服务，即那些在服务注册表中发现的服务，必须始终使用它们的字母数字名称来引用。只允许通过网关定义的服务使用 IP 地址来引用。\n注意：对于委托的 VirtualService，必须是空的。\n  gateways：字符串数组格式。配置应用路由的网关和 sidecar 的名称。其他命名空间中的 Gateway 可以通过 \u0026lt;Gateway命名空间\u0026gt;/\u0026lt;Gateway名称\u0026gt; 来引用；指定一个没有命名空间修饰词的 Gateway 默认与 VirtualService 位于同一命名空间。单个 VirtualService 用于网格内的 sidecar，也用于一个或多个网关。这个字段施加的选择条件可以使用协议特定路由的匹配条件中的源字段来覆盖。mesh 这个保留词被用来暗示网格中的所有 sidecar。当这个字段被省略时，将使用默认网关（mesh），这将把规则应用于网格中的所有 sidecar。如果提供了一个网关名称的列表，规则将只应用于网关。要将规则同时应用于网关和 sidecar，请指定 mesh 作为 Gateway 名称。\n  http：HTTP 流量的路由规则的有序列表。HTTP 路由将应用于名为 http-/http2-/grpc-* 的平台服务端口、具有 HTTP/HTTP2/GRPC/TLS 终止的 HTTPS 协议的网关端口以及使用 HTTP/HTTP2/GRPC 协议的 ServiceEntry 端口。请注意该配置是有顺序的，请求第一个匹配的规则将被应用。\n  tls：一个有序的路由规则列表，用于非终止的 TLS 和 HTTPS 流量。路由通常是使用 ClientHello 消息提出的 SNI 值来执行的。TLS 路由将被应用于平台服务端口 https-、tls-，使用 HTTPS/TLS 协议的未终止网关端口（即具有 passthough TLS 模式）和使用 HTTPS/TLS 协议的 ServiceEntry 端口。匹配传入请求的第一条规则被使用。注意：没有相关 VirtualService 的 https- 或 tls- 端口流量将被视为不透明的 TCP 流量。\n  tcp：一个不透明的 TCP 流量的路由规则的有序列表。TCP 路由将被应用于任何不是 HTTP 或 TLS 端口。匹配传入请求的第一个规则被使用。\n  exportTo：VirtualService 被导出的命名空间的列表。导出的 VirtualService 可以被定义在其它命名空间中的 sidecar 和 Gateway 使用。该功能为服务所有者和网格管理员提供了一种机制，以控制 VirtualService 在命名空间边界的可见性。\n如果没有指定命名空间，那么默认情况下，VirtualService 会被输出到所有命名空间。\n值 . 是保留的，它定义了导出到 VirtualService 声明的同一命名空间。同样，值 * 也是保留的，它定义了导出到所有命名空间。\n  完整示例 下面是一个相对完整的 VirtualService 配置的示例，其中包含了所有基本配置，对应的解释请见注释。\napiVersion:networking.istio.io/v1alpha3kind:VirtualServicemetadata:name:reviews-routenamespace:bookinfospec:hosts:- reviews.prod.svc.cluster.local#目标主机gateways:- my-gateway#应用路由的网关和 sidecar 的名称http:#HTTP 路由规则- name:\u0026#34;reviews-v2-routes\u0026#34;match:- uri:prefix:\u0026#34;/wpcatalog\u0026#34;rewrite:uri:\u0026#34;/newcatalog\u0026#34;route:- destination:host:reviews.prod.svc.cluster.localsubset:v1#该子集是在 DestinationRule 中设置的tls:#关于 TLS 的设置mode:MUTUAL#一共有四种模式，DISABLE：关闭 TLS 连接；SIMPLE：发起一个与上游端点的 TLS 连接；MUTUAL：手动配置证书，通过出示客户端证书进行认证，使用双向的 TLS 确保与上游的连接；ISTIO_MUTUAL：该模式使用 Istio 自动生成的证书进行 mTLS 认证。clientCertificate:/etc/certs/myclientcert.pemprivateKey:/etc/certs/client_private_key.pemcaCertificates:/etc/certs/rootcacerts.pemexportTo:#指定 VirtualService 的可见性- \u0026#34;*\u0026#34;#*表示对所有命名空间可见，此为默认值；\u0026#34;.\u0026#34;表示仅对当前命名空间可见关于 VirtualService 配置的详细用法请参考 Istio 官方文档。\n参考  Virtual Service - istio.io  ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"4cdabe2dbbd5ec21dd0678af3df15ff4","permalink":"https://jimmysong.io/docs/istio-handbook/config-networking/virtual-service/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/istio-handbook/config-networking/virtual-service/","section":"istio-handbook","summary":"VirtualService 主要配置流量路由。以下是在流量路由背景下定义的几个有用的术","tags":null,"title":"VirtualService","type":"book"},{"authors":null,"categories":null,"content":"在集群和 Istio 准备好后，我们可以克隆在 Online Boutique 应用库。\n1. 克隆仓库\ngit clone https://github.com/GoogleCloudPlatform/microservices-demo.git 2. 前往 microservices-demo 目录\ncd microservices-demo 3. 创建 Kubernetes 资源\nkubectl apply -f release/kubernetes-manifests.yaml 4. 检查所有 Pod 都在运行\n$ kubectl get pods NAME READY STATUS RESTARTS AGE adservice-5c9c7c997f-n627f 2/2 Running 0 2m15s cartservice-6d99678dd6-767fb 2/2 Running 2 2m16s checkoutservice-779cb9bfdf-l2rs9 2/2 Running 0 2m18s currencyservice-5db6c7d559-9drtc 2/2 Running 0 2m16s emailservice-5c47dc87bf-dk7qv 2/2 Running 0 2m18s frontend-5fcb8cdcdc-8c9dk 2/2 Running 0 2m17s loadgenerator-79bff5bd57-q9qkd 2/2 Running 4 2m16s paymentservice-6564cb7fb9-f6dwr 2/2 Running 0 2m17s productcatalogservice-5db9444549-hkzv7 2/2 Running 0 2m17s recommendationservice-ff6878cf5-jsghw 2/2 Running 0 2m18s redis-cart-57bd646894-zb7ch 2/2 Running 0 2m15s shippingservice-f47755f97-dk7k9 2/2 Running 0 2m15s 5. 创建 Istio 资源\nkubectl apply -f ./istio-manifests 部署了一切后，我们就可以得到入口网关的 IP 地址并打开前端服务：\nINGRESS_HOST=\u0026#34;$(kubectl -n istio-system get service istio-ingressgateway \\  -o jsonpath=\u0026#39;{.status.loadBalancer.ingress[0].ip}\u0026#39;)\u0026#34; echo \u0026#34;$INGRESS_HOST\u0026#34; 在浏览器中打开 INGRESS_HOST，你会看到前端服务，如下图所示。\n   前端服务  我们需要做的最后一件事是删除 frontend-external 服务。frontend-external 服务是一个 LoadBalancer 服务，它暴露了前端。由于我们正在使用 Istio 的入口网关，我们不再需要这个 LoadBalancer 服务了。\n要删除服务，运行：\nkubectl delete svc frontend-external Online Boutique 应用清单还包括一个负载发生器，它正在生成对所有服务的请求——这是为了让我们能够模拟网站的流量。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"1f174b95332deded449f609aa57ee521","permalink":"https://jimmysong.io/docs/istio-handbook/practice/online-boutique/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/istio-handbook/practice/online-boutique/","section":"istio-handbook","summary":"在集群和 Istio 准备好后，我们可以克隆在 Online Boutique 应用库。 1. 克隆仓库 git clone","tags":null,"title":"部署 Online Boutique 应用","type":"book"},{"authors":null,"categories":null,"content":"Istio 提供两种类型的认证：对等认证和请求认证。\n对等认证 对等认证用于服务间的认证，以验证建立连接的客户端。\n当两个服务试图进行通信时，相互 TLS 要求它们都向对方提供证书，因此双方都知道它们在与谁交谈。如果我们想在服务之间启用严格的相互 TLS，我们可以使用 PeerAuthentication 资源，将 mTLS 模式设置为 STRICT。\n使用 PeerAuthentication 资源，我们可以打开整个网状结构的相互 TLS（mTLS），而不需要做任何代码修改。\n然而，Istio 也支持一种优雅的模式，我们可以选择在一个工作负载或命名空间的时间内进入相互 TLS。这种模式被称为许可模式。\n当你安装 Istio 时，允许模式是默认启用的。启用允许模式后，如果客户端试图通过相互 TLS 连接到我，我将提供相互 TLS。如果客户端不使用相互 TLS，我也可以用纯文本响应。我是允许客户端做 mTLS 或不做的。使用这种模式，你可以在你的网状网络中逐渐推广相互 TLS。\n简而言之，PeerAuthentication 谈论的是工作负载或服务的通信方式，它并没有说到最终用户。那么，我们怎样才能认证用户呢？\n请求认证 请求认证（RequestAuthentication 资源）验证了附加在请求上的凭证，它被用于终端用户认证。\n请求级认证是通过 JSON Web Tokens（JWT） 验证完成的。Istio 支持任何 OpenID Connect 提供商，如 Auth0、Firebase 或 Google Auth、Keycloak、ORY Hydra。因此，就像我们使用 SPIFFE 身份来验证服务一样，我们可以使用 JWT 令牌来验证用户。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"a412d190f7edfb68570e4a8dabf0faa8","permalink":"https://jimmysong.io/docs/istio-handbook/security/peer-and-request-authn/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/istio-handbook/security/peer-and-request-authn/","section":"istio-handbook","summary":"Istio 提供两种类型的认证：对等认证和请求认证。 对等认证 对等认证用","tags":null,"title":"对等认证和请求认证","type":"book"},{"authors":null,"categories":null,"content":"我们在前面看到了通过客户端库来治理服务的架构图，那是我们在改造成服务网格架构前使用微服务架构通常的形式，下图是使用服务网格架构的最终形式。\n   服务网格架构示意图  当然在达到这一最终形态之前我们需要将架构一步步演进，下面给出的是参考的演进路线。\nIngress 或边缘代理 如果你使用的是 Kubernetes 做容器编排调度，那么在进化到服务网格架构之前，通常会使用 Ingress Controller，做集群内外流量的反向代理，如使用 Traefik 或 Nginx Ingress Controller。\n   Ingress 或边缘代理架构示意图  这样只要利用 Kubernetes 的原有能力，当你的应用微服务化并容器化需要开放外部访问且只需要 L7 代理的话这种改造十分简单，但问题是无法管理服务间流量。\n路由器网格 Ingress 或者边缘代理可以处理进出集群的流量，为了应对集群内的服务间流量管理，我们可以在集群内加一个 Router 层，即路由器层，让集群内所有服务间的流量都通过该路由器。\n   路由器网格架构示意图  这个架构无需对原有的单体应用和新的微服务应用做什么改造，可以很轻易的迁移进来，但是当服务多了管理起来就很麻烦。\nProxy per Node 这种架构是在每个节点上都部署一个代理，如果使用 Kubernetes 来部署的话就是使用 DaemonSet 对象，Linkerd 第一代就是使用这种方式部署的，一代的 Linkerd 使用 Scala 开发，基于 JVM 比较消耗资源，二代的 Linkerd 使用 Go 开发。\n   Proxy per node 架构示意图  这种架构有个好处是每个节点只需要部署一个代理即可，比起在每个应用中都注入一个sidecar的方式更节省资源，而且更适合基于物理机/虚拟机的大型单体应用，但是也有一些副作用，比如粒度还是不够细，如果一个节点出问题，该节点上的所有服务就都会无法访问，对于服务来说不是完全透明的。\nSidecar代理/Fabric模型 这个一般不会成为典型部署类型，当企业的服务网格架构演进到这一步时通常只会持续很短时间，然后就会增加控制平面。跟前几个阶段最大的不同就是，应用程序和代理被放在了同一个部署单元里，可以对应用程序的流量做更细粒度的控制。\n   Sidecar代理/Fabric模型示意图  这已经是最接近服务网格架构的一种形态了，唯一缺的就是控制平面了。所有的sidecar都支持热加载，配置的变更可以很容易的在流量控制中反应出来，但是如何操作这么多sidecar就需要一个统一的控制平面了。\nSidecar代理/控制平面 下面的示意图是目前大多数服务网格的架构图，也可以说是整个服务网格架构演进的最终形态。\n   Sidecar 代理/控制平面架构示意图  这种架构将代理作为整个服务网格中的一部分，使用 Kubernetes 部署的话，可以通过以 sidecar 的形式注入，减轻了部署的负担，可以对每个服务的做细粒度权限与流量控制。但有一点不好就是为每个服务都注入一个代理会占用很多资源，因此要想方设法降低每个代理的资源消耗。\n多集群部署和扩展 以上都是单个服务网格集群的架构，所有的服务都位于同一个集群中，服务网格管理进出集群和集群内部的流量，当我们需要管理多个集群或者是引入外部的服务时就需要网格扩展和多集群配置。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"30bdfc05dce3472ac3b0a2d235e161b8","permalink":"https://jimmysong.io/docs/istio-handbook/concepts/service-mesh-patterns/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/istio-handbook/concepts/service-mesh-patterns/","section":"istio-handbook","summary":"我们在前面看到了通过客户端库来治理服务的架构图，那是我们在改","tags":null,"title":"服务网格的实现模式","type":"book"},{"authors":null,"categories":null,"content":"要用密钥管理服务（KMS）提供商插件来加密 Secret，可以使用以下加密配置 YAML 文件的例子来为提供商设置属性。这个例子是基于 Kubernetes 的官方文档。\napiVersion:apiserver.config.k8s.io/v1kind:EncryptionConfigurationresources:- resources:- secretsproviders:- kms:name:myKMSPluginendpoint:unix://tmp/socketfile.sockcachesize:100timeout:3s- identity:{}要配置 API 服务器使用 KMS 提供商，请将 --encryption-provider-config 标志与配置文件的位置一起设置，并重新启动 API 服务器。\n要从本地加密提供者切换到 KMS，请将 EncryptionConfiguration 文件中的 KMS 提供者部分添加到当前加密方法之上，如下所示。\napiVersion:apiserver.config.k8s.io/v1kind:EncryptionConfigurationresources:- resources:- secretsproviders:- kms:name:myKMSPluginendpoint:unix://tmp/socketfile.sockcachesize:100timeout:3s- aescbc:keys:- name:key1secret:\u0026lt;base64 encoded secret\u0026gt;重新启动 API 服务器并运行下面的命令来重新加密所有与 KMS 供应商的 Secret。\nkubectl get secrets --all-namespaces -o json | kubectl replace -f - ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"b6d9c1c13d269ee8cfa0a2c22ec0869c","permalink":"https://jimmysong.io/docs/kubernetes-hardening-guidance/appendix/i/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/kubernetes-hardening-guidance/appendix/i/","section":"kubernetes-hardening-guidance","summary":"要用密钥管理服务（KMS）提供商插件来加密 Secret，可以","tags":null,"title":"附录 I：KMS 配置实例","type":"book"},{"authors":null,"categories":null,"content":"动态提供配置另一种方式是通过指向文件系统上的文件。为了使动态配置发挥作用，我们需要在 node 字段下提供信息。如果我们可能有多个 Envoy 代理指向相同的配置文件，那么 node 字段是用来识别一个特定的 Envoy 实例。\n   动态配置  为了指向动态资源，我们可以使用 dynamic_resources 字段来告诉 Envoy 在哪里可以找到特定资源的动态配置。例如：\nnode:cluster:my-clusterid:some-iddynamic_resources:lds_config:path:/etc/envoy/lds.yamlcds_config:path:/etc/envoy/cds.yaml上面的片段是一个有效的 Envoy 配置。如果我们把 LDS 和 CDS 作为静态资源来提供，它们的单独配置将非常相似。唯一不同的是，我们必须指定资源类型和版本信息。下面是 CDS 配置的一个片段。\nversion_info:\u0026#34;0\u0026#34;resources:- \u0026#34;@type\u0026#34;: type.googleapis.com/envoy.config.cluster.v3.Clustername:instance_1connect_timeout:5sload_assignment:cluster_name:instance_1endpoints:- lb_endpoints:- endpoint:address:socket_address:address:127.0.0.1port_value:3030如果我们想使用 EDS 为集群提供端点，我们可以这样写上面的配置。\nversion_info:\u0026#34;0\u0026#34;resources:- \u0026#34;@type\u0026#34;: type.googleapis.com/envoy.config.cluster.v3.Clustername:instance_1type:EDSeds_cluster_config:eds_config:path:/etc/envoy/eds.yaml另外，注意我们已经把集群的类型设置为 EDS。EDS 的配置会是这样的。\nversion_info:\u0026#34;0\u0026#34;resources:- \u0026#34;@type\u0026#34;: type.googleapis.com/envoy.config.endpoint.v3.ClusterLoadAssignmentcluster_name:instance_1endpoints:- lb_endpoints:- endpoint:address:socket_address:address:127.0.0.1port_value:3030当任何一个文件被更新时，Envoy 会自动重新加载配置。如果配置无效，Envoy 会输出错误，但会保持现有（工作）配置的运行。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"e54422a6e569b4864dab33ce8ed02584","permalink":"https://jimmysong.io/docs/envoy-handbook/dynamic-config/filesystem/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/envoy-handbook/dynamic-config/filesystem/","section":"envoy-handbook","summary":"动态提供配置另一种方式是通过指向文件系统上的文件。为了使动态","tags":null,"title":"来自文件系统的动态配置","type":"book"},{"authors":null,"categories":null,"content":"我们可以在 HTTP 或 TCP 过滤器级别和监听器级别上配置访问记录器。我们还可以配置多个具有不同日志格式和日志沉积的访问日志。日志沉积（log sink） 是一个抽象的术语，指的是日志写入的位置，例如，写入控制台（stdout、stderr）、文件或网络服务。\n要配置多个访问日志的情况是，我们想在控制台（标准输出）中看到高级信息，并将完整的请求细节写入磁盘上的文件。用于配置访问记录器的字段被称为 access_log。\n让我们看看在 HTTP 连接管理器（HCM）层面上启用访问日志到标准输出（StdoutAccessLog）的例子。\n- filters:- name:envoy.filters.network.http_connection_managertyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManagerstat_prefix:ingress_httpaccess_log:- name:envoy.access_loggers.stdouttyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.access_loggers.stream.v3.StdoutAccessLogEnvoy 的目标是拥有可移植和可扩展的配置：类型化的配置。这样做的一个副作用是配置的名字很冗长。例如，为了启用访问日志，我们找到 HTTP 配置类型的名称，然后找到对应于控制台的类型（StdoutAccessLog）。\nStdoutAccessLog 配置将日志条目写到标准输出（控制台）。其他支持的访问日志沉积有以下几种：\n 文件 (FileAccessLog) gRPC（HttpGrpcAccessLogConfig 和 TcpGrpcAccessLogConfig） 标准错误（StderrAccessLog） Wasm (WasmAccessLog) Open Telemetry  文件访问日志允许我们将日志条目写到配置中指定的文件中。例如：\n- filters:- name:envoy.filters.network.http_connection_managertyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManagerstat_prefix:ingress_httpaccess_log:- name:envoy.access_loggers.filetyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.access_loggers.file.v3.FileAccessLogpath:./envoy-access-logs.log注意名称（envoy.access_loggers.file）和类型（file.v3.FileAccessLog）的变化。此外，我们还提供了我们希望 Envoy 存储访问日志的路径。\ngRPC 访问日志沉积将日志发送到 HTTP 或 TCP gRPC 日志服务。为了使用 gRPC 日志沉积，我们必须建立一个 gRPC 服务器，其端点要实现 MetricsService，特别是 StreamMetrics 函数。然后，Envoy 可以连接到 gRPC 服务器并将日志发送给它。\n在此之前，我们提到了默认的访问日志格式，它是由不同的命令操作符组成的。\n[%start_time%] \u0026#34;%req(:method)%req(x-envoy-original-path?:path)%%protocol%\u0026#34; %response_code% %response_flags% %bytes_received% %bytes_sent% %duration% %。 %resp(x-envoy-upstream-service-time)% \u0026#34;%req(x-forwarded-for)%\u0026#34; \u0026#34;%req(user-agent)%\u0026#34; \u0026#34;%req(x-request-id)%\u0026#34; \u0026#34;%req(:authority)%\u0026#34; \u0026#34;%upstream_host%\u0026#34; 日志条目的格式是可配置的，可以使用 log_format 字段进行修改。使用 log_format，我们可以配置日志条目包括哪些值，并指定我们是否需要纯文本或 JSON 格式的日志。\n例如，我们只想记录开始时间、响应代码和用户代理。我们会这样配置它。\n- filters:- name:envoy.filters.network.http_connection_managertyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManagerstat_prefix:ingress_httpaccess_log:- name:envoy.access_loggers.stdouttyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.access_loggers.stream.v3.StdoutAccessLoglog_format:text_format_source:inline_string:\u0026#34;%START_TIME% %RESPONSE_CODE% %REQ(USER-AGENT)%\u0026#34;一个使用上述格式的日志条目样本看起来是这样的：\n2021-11-01T21:32:27.170Z 404 curl/7.64.0 同样，如果我们希望日志是 JSON 等结构化格式，我们也可以不提供文本格式，而是设置 JSON 格式字符串。\n为了使用 JSON 格式，我们必须提供一个格式字典，而不是像纯文本格式那样提供一个单一的字符串。\n下面是一个使用相同的日志格式的例子，但用 JSON 写日志条目来代替。\n- filter:- name:envoy.filters.network.http_connection_managertyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions. filters.network.http_connection_manager.v3.HttpConnectionManagerstat_prefix:ingress_httpaccess_log:- name:envoy.access_loggers.stdouttyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.access_loggers.stream.v3.StdoutAccessLoglog_format:json_format:start_time:\u0026#34;%START_TIME%\u0026#34;response_code:\u0026#34;%response_code%\u0026#34;user_agent:\u0026#34;%req(user-agent)%\u0026#34;上述片段将产生以下日志条目。\n{\u0026#34;user_agent\u0026#34;:\u0026#34;curl/7.64.0\u0026#34;,\u0026#34;response_code\u0026#34;:404,\u0026#34;start_time\u0026#34;:\u0026#34;2021-11-01T21:37:59.979Z\u0026#34;} 某些命令操作符，如 FILTER_STATE 或 DYNAMIC_METADATA，可能产生嵌套的 JSON 日志条目。\n日志格式也可以使用通过 formatters 字段指定的 formatter 插件。当前版本中有两个已知的格式化插件：元数据（envoy.formatter.metadata）和无查询请求（envoy.formatter.req_without_query）扩展。\n元数据格式化扩展实现了 METADATA 命令操作符，允许我们输出不同类型的元数据（DYNAMIC、CLUSTER 或 ROUTE）。\n同样，req_without_query 格式化允许我们使用 REQ_WITHOUT_QUERY 命令操作符，其工作方式与 REQ 命令操作符相同，但会删除查询字符串。该命令操作符用于避免将任何敏感信息记录到访问日志中。\n下面是一个如何提供格式化器以及如何在 inline_string 中使用它的例子。\n- filters:- name:envoy.filters.network.http_connection_managertyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManagerstat_prefix:ingress_httpaccess_log:- name:envoy.access_loggers.stdouttyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.access_loggers.stream.v3.StdoutAccessLoglog_format:text_format_source:inline_string:\u0026#34;[%START_TIME%] %REQ(:METHOD)% %REQ_WITHOUT_QUERY(X-ENVOY-ORIGINAL-PATH?:PATH)% %PROTOCOL%\u0026#34;formatters:- name:envoy.formatter.req_without_querytyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.formatter.req_without_query.v3.ReqWithoutQuery上述配置中的这个请求 curl localhost:10000/?hello=1234 会产生一个不包括查询参数（hello=1234）的日志条目。\n[2021-11-01t21:48:55.941z] get / http/1.1 ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"aa23c8340b7b9ba50a1e20fc0d1a0734","permalink":"https://jimmysong.io/docs/envoy-handbook/logging/confguring-logging/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/envoy-handbook/logging/confguring-logging/","section":"envoy-handbook","summary":"我们可以在 HTTP 或 TCP 过滤器级别和监听器级别上配置访问记录器。我们","tags":null,"title":"配置访问记录器","type":"book"},{"authors":null,"categories":null,"content":"/config_dump 端点是一种快速的方法，可以将当前加载的 Envoy 配置显示为 JSON 序列化的 proto 消息。\nEnvoy 输出以下组件的配置，并按照下面的顺序排列。\n 自举（bootstrap） 集群（clusters） 端点（endpoints） 监听器（listeners） 范围路由（scoped routes） 路由（routes） 秘密（secrets）  包括 EDS 配置 为了输出端点发现服务（EDS）的配置，我们可以在查询中加入 ?include_eds 参数。\n筛选输出 同样，我们可以通过提供我们想要包括的资源和一个掩码来过滤输出，以返回一个字段的子集。\n例如，为了只输出静态集群配置，我们可以在资源查询参数中使用 static_clusters 字段，从 ClustersConfigDump proto 在 resource 查询参数中使用。\n$ curl localhost:9901/config_dump?resource=static_clusters{\u0026#34;configs\u0026#34;: [{\u0026#34;@type\u0026#34;: \u0026#34;type.googleapis.com/envoy.admin.v3.ClustersConfigDump.StaticCluster\u0026#34;,\u0026#34;cluster\u0026#34;: {\u0026#34;@type\u0026#34;: \u0026#34;type.googleapis.com/envoy.config.cluster.v3.Cluster\u0026#34;,\u0026#34;name\u0026#34;: \u0026#34;instance_1\u0026#34;,},...{\u0026#34;@type\u0026#34;: \u0026#34;type.googleapis.com/envoy.admin.v3.ClustersConfigDump.StaticCluster\u0026#34;,\u0026#34;cluster\u0026#34;: {\u0026#34;@type\u0026#34;: \u0026#34;type.googleapis.com/envoy.config.cluster.v3.Cluster\u0026#34;,\u0026#34;name\u0026#34;: \u0026#34;instance_2\u0026#34;,...使用mask参数 为了进一步缩小输出范围，我们可以在 mask 参数中指定该字段。例如，只显示每个集群的 connect_timeout 值。\n$ curl localhost:9901/config_dump?resource=static_clusters\u0026amp;mask=cluster.connect_timeout{\u0026#34;configs\u0026#34;: [{\u0026#34;@type\u0026#34;: \u0026#34;type.googleapis.com/envoy.admin.v3.ClustersConfigDump.StaticCluster\u0026#34;,\u0026#34;cluster\u0026#34;: {\u0026#34;@type\u0026#34;: \u0026#34;type.googleapis.com/envoy.config.cluster.v3.Cluster\u0026#34;,\u0026#34;connect_timeout\u0026#34;: \u0026#34;5s\u0026#34;}},{\u0026#34;@type\u0026#34;: \u0026#34;type.googleapis.com/envoy.admin.v3.ClustersConfigDump.StaticCluster\u0026#34;,\u0026#34;cluster\u0026#34;: {\u0026#34;@type\u0026#34;: \u0026#34;type.googleapis.com/envoy.config.cluster.v3.Cluster\u0026#34;,\u0026#34;connect_timeout\u0026#34;: \u0026#34;5s\u0026#34;}},{\u0026#34;@type\u0026#34;: \u0026#34;type.googleapis.com/envoy.admin.v3.ClustersConfigDump.StaticCluster\u0026#34;,\u0026#34;cluster\u0026#34;: {\u0026#34;@type\u0026#34;: \u0026#34;type.googleapis.com/envoy.config.cluster.v3.Cluster\u0026#34;,\u0026#34;connect_timeout\u0026#34;: \u0026#34;1s\u0026#34;}}]}使用正则表达式 另一个过滤选项是指定一个正则表达式来匹配加载的配置的名称。例如，要输出所有名称字段与正则表达式 .*listener.* 相匹配的监听器，我们可以这样写。\n$ curl localhost:9901/config_dump?resource=static_clusters\u0026amp;name_regex=.*listener.* { \u0026#34;configs\u0026#34;: [ { \u0026#34;@type\u0026#34;: \u0026#34;type.googleapis.com/envoy.admin.v3.ListenersConfigDump.StaticListener\u0026#34;, \u0026#34;listener\u0026#34;: { \u0026#34;@type\u0026#34;: \u0026#34;type.googleapis.com/envoy.config.listener.v3.Listener\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;listener_0\u0026#34;, \u0026#34;address\u0026#34;: { \u0026#34;socket_address\u0026#34;: { \u0026#34;address\u0026#34;: \u0026#34;0.0.0.0\u0026#34;, \u0026#34;port_value\u0026#34;: 10000 } }, \u0026#34;filter_chains\u0026#34;: [ {} ] }, \u0026#34;last_updated\u0026#34;: \u0026#34;2021-11-15T20:06:51.208Z\u0026#34; } ] } 同样，/init_dump 端点列出了各种 Envoy 组件的未就绪目标的当前信息。和配置转储一样，我们可以使用 mask 查询参数来过滤特定字段。\n证书 /certs 输出所有加载的 TLS 证书。数据包括证书文件名、序列号、主题候补名称和到期前天数。结果是 JSON 格式的，遵循 admin.v3.Certificates proto。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"7d42322a4629f007648311537acd14fa","permalink":"https://jimmysong.io/docs/envoy-handbook/admin-interface/configuration-dump/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/envoy-handbook/admin-interface/configuration-dump/","section":"envoy-handbook","summary":"/config_dump 端点是一种快速的方法，可以将当前加载的 Envoy 配置显示为 JSON 序列化","tags":null,"title":"配置转储","type":"book"},{"authors":null,"categories":null,"content":"Service Mesh 又译作 “服务网格”，作为服务间通信的基础设施层。Buoyant 公司的 CEO Willian Morgan 在他的这篇文章 WHAT’S A SERVICE MESH? AND WHY DO I NEED ONE? 中解释了什么是 Service Mesh，为什么云原生应用需要 Service Mesh。\n服务网格是用于处理服务间通信的专用基础设施层。它负责通过包含现代云原生应用程序的复杂服务拓扑来可靠地传递请求。实际上，服务网格通常通过一组轻量级网络代理来实现，这些代理与应用程序代码一起部署，而不需要感知应用程序本身。—— Willian Morgan Buoyant CEO\n服务网格（Service Mesh）这个术语通常用于描述构成这些应用程序的微服务网络以及应用之间的交互。随着规模和复杂性的增长，服务网格越来越难以理解和管理。它的需求包括服务发现、负载均衡、故障恢复、指标收集和监控以及通常更加复杂的运维需求，例如 A/B 测试、金丝雀发布、限流、访问控制和端到端认证等。\n服务网格的特点 服务网格有如下几个特点：\n 应用程序间通讯的中间层 轻量级网络代理 应用程序无感知 解耦应用程序的重试/超时、监控、追踪和服务发现  目前两款流行的服务网格开源软件 Linkerd 和 Istio 都可以直接在 Kubernetes 中集成，其中 Linkerd 是 CNCF 成员项目，并在 2021 年 7 月毕业。Istio 在 2018年7月31日宣布 1.0，并在 2020 年 7 月将商标捐献给 Open Usage Commons。\n理解服务网格 如果用一句话来解释什么是服务网格，可以将它比作是应用程序或者说微服务间的 TCP/IP，负责服务之间的网络调用、限流、熔断和监控。对于编写应用程序来说一般无须关心 TCP/IP 这一层（比如通过 HTTP 协议的 RESTful 应用），同样使用服务网格也就无须关系服务之间的那些原来是通过应用程序或者其他框架实现的事情，比如 Spring Cloud、OSS，现在只要交给服务网格就可以了。\nPhil Calçado 在他的这篇博客 Pattern: Service Mesh 中详细解释了服务网格的来龙去脉：\n 从最原始的主机之间直接使用网线相连 网络层的出现 集成到应用程序内部的控制流 分解到应用程序外部的控制流 应用程序的中集成服务发现和断路器 出现了专门用于服务发现和断路器的软件包/库，如 Twitter 的 Finagle 和 Facebook 的 Proxygen，这时候还是集成在应用程序内部 出现了专门用于服务发现和断路器的开源软件，如 Netflix OSS、Airbnb 的 synapse 和 nerve 最后作为微服务的中间层服务网格出现  服务网格的架构如下图所示：\n   Service Mesh 架构图  图片来自：Pattern: Service Mesh\n服务网格作为 sidecar 运行，对应用程序来说是透明，所有应用程序间的流量都会通过它，所以对应用程序流量的控制都可以在 serivce mesh 中实现。\n服务网格如何工作？ 下面以 Istio 为例讲解服务网格如何在 Kubernetes 中工作。\n Istio 将服务请求路由到目的地址，根据中的参数判断是到生产环境、测试环境还是 staging 环境中的服务（服务可能同时部署在这三个环境中），是路由到本地环境还是公有云环境？所有的这些路由信息可以动态配置，可以是全局配置也可以为某些服务单独配置。 当 Istio 确认了目的地址后，将流量发送到相应服务发现端点，在 Kubernetes 中是 service，然后 service 会将服务转发给后端的实例。 Istio 根据它观测到最近请求的延迟时间，选择出所有应用程序的实例中响应最快的实例。 Istio 将请求发送给该实例，同时记录响应类型和延迟数据。 如果该实例挂了、不响应了或者进程不工作了，Istio 将把请求发送到其他实例上重试。 如果该实例持续返回 error，Istio 会将该实例从负载均衡池中移除，稍后再周期性得重试。 如果请求的截止时间已过，Istio 主动失败该请求，而不是再次尝试添加负载。 Istio 以 metric 和分布式追踪的形式捕获上述行为的各个方面，这些追踪信息将发送到集中 metric 系统。  为何使用服务网格？ 服务网格并没有给我们带来新功能，它是用于解决其他工具已经解决过的问题，只不过这次是在云原生的 Kubernetes 环境下的实现。\n在传统的 MVC 三层 Web 应用程序架构下，服务之间的通讯并不复杂，在应用程序内部自己管理即可，但是在现今的复杂的大型网站情况下，单体应用被分解为众多的微服务，服务之间的依赖和通讯十分复杂，出现了 Twitter 开发的 Finagle、Netflix 开发的 Hystrix 和 Google 的 Stubby 这样的 “胖客户端” 库，这些就是早期的服务网格，但是它们都近适用于特定的环境和特定的开发语言，并不能作为平台级的服务网格支持。\n在云原生架构下，容器的使用给予了异构应用程序的更多可行性，Kubernetes 增强的应用的横向扩容能力，用户可以快速的编排出复杂环境、复杂依赖关系的应用程序，同时开发者又无须过分关心应用程序的监控、扩展性、服务发现和分布式追踪这些繁琐的事情而专注于程序开发，赋予开发者更多的创造性。\n参考  Istio: A service mesh for AWS ECS - medium.com 初次了解 Istio - istio.io Application Network Functions With ESBs, API Management, and Now.. Service Mesh? - blog.christianposta.com Pattern: Service Mesh - philcalcado.com Envoy 官方文档中文版 - cloudnative.to Istio 官方文档 - istio.io  ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"a7028f00ff8cebd3881a160a4fe8a049","permalink":"https://jimmysong.io/docs/istio-handbook/intro/what-is-service-mesh/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/istio-handbook/intro/what-is-service-mesh/","section":"istio-handbook","summary":"Service Mesh 又译作 “服务网格”，作为服务间通信的基础设施层。Buoy","tags":null,"title":"什么是服务网格？","type":"book"},{"authors":null,"categories":null,"content":"如果我们有在虚拟机上运行的工作负载，我们可以将它们连接到 Istio 服务网格，使其成为网格的一部分。\n带有虚拟机的 Istio 服务网格有两种架构：单网络架构和多网络架构。\n单网络架构 在这种情况下，有一个单一的网络。Kubernetes 集群和在虚拟机上运行的工作负载都在同一个网络中，它们可以直接相互通信。\n   单网络架构  单网络架构\n控制面的流量（配置更新、证书签署）是通过 Gateway 发送的。\n虚拟机被配置了网关地址，所以它们在启动时可以连接到控制平面。\n多网络架构 多网络架构横跨多个网络。Kubernetes 集群在一个网络内，而虚拟机则在另一个网络内。这使得 Kubernetes 集群中的 Pod 和虚拟机上的工作负载无法直接相互通信。\n   多网络架构  多网络架构\n所有的流量，控制面和 pod 到工作服的流量都流经网关，网关作为两个网络之间的桥梁。\nIstio 中如何表示虚拟机工作负载？ 在 Istio 服务网格中，有两种方式来表示虚拟机工作负载。\n工作负载组（WorkloadGroup 资源）类似于 Kubernetes 中的部署（Deployment），它代表了共享共同属性的虚拟机工作负载的逻辑组。\n描述虚拟机工作负载的第二种方法是使用工作负载条目（WorkloadEntry 资源）。工作负载条目类似于 Pod，它代表了一个虚拟机工作负载的单一实例。\n请注意，创建上述资源将不会提供或运行任何虚拟机工作负载实例。这些资源只是用来参考或指向虚拟机工作负载的。Istio 使用它们来了解如何适当地配置网格，将哪些服务添加到内部服务注册表中，等等。\n为了将虚拟机添加到网格中，我们需要创建一个工作负载组，作为模板。然后，当我们配置并将虚拟机添加到网格中时，控制平面会自动创建一个相应的 WorkloadEntry。\n我们已经提到，WorkloadEntry 的作用类似于 Pod。在添加虚拟机时，会创建 WorkloadEntry 资源，而当虚拟机的工作负载从网格中移除时，该资源会被自动删除。\n除了 WorkloadEntry 资源外，我们还需要创建一个 Kubernetes 服务。创建一个 Kubernetes 服务给了我们一个稳定的主机名和 IP 地址，以便使用选择器字段访问虚拟机工作负载和 pod。这也使我们能够通过 DestinationRule 和 VirtualService 资源使用 Istio 的路由功能。\n 下一章   ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"9886638b7634e816bd7e470cd4e5ff32","permalink":"https://jimmysong.io/docs/istio-handbook/advanced/vm/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/istio-handbook/advanced/vm/","section":"istio-handbook","summary":"如果我们有在虚拟机上运行的工作负载，我们可以将它们连接到 Istio 服","tags":null,"title":"虚拟机负载","type":"book"},{"authors":null,"categories":null,"content":"Envoy 支持端点上不同的主动健康检查方法。HTTP、TCP、gRPC 和 Redis 健康检查。健康检查方法可以为每个集群单独配置。我们可以通过集群配置中的 health_checks 字段来配置健康检查。\n无论选择哪种健康检查方法，都需要定义几个常见的配置设置。\n超时（timeout）表示分配给等待健康检查响应的时间。如果在这个字段中指定的时间值内没有达到响应，健康检查尝试将被视为失败。间隔（internal）指定健康检查之间的时间节奏。例如，5 秒的间隔将每 5 秒触发一次健康检查。\n其他两个必要的设置可用于确定一个特定的端点何时被认为是健康或不健康的。healthy_threshold 指定在一个端点被标记为健康之前所需的 “健康” 检查（例如，HTTP 200 响应）的数量。unhealthy_threshold 的作用与此相同，但是对于 “不健康” 的健康检查，它指定了在一个端点被标记为不健康之前所需的不健康检查的数量。\n1. HTTP 健康检查\nEnvoy 向端点发送一个 HTTP 请求。如果端点回应的是 HTTP 200，Envoy 认为它是健康的。200 响应是默认的响应，被认为是健康响应。使用 expected_statuses 字段，我们可以通过提供一个被认为是健康的 HTTP 状态的范围来进行自定义。\n如果端点以 HTTP 503 响应，unhealthy_threshold 被忽略，并且端点立即被认为是不健康的。\nclusters:- name:my_cluster_namehealth_checks:- timeout:1sinterval:0.25sunhealthy_threshold:5healthy_threshold:2http_health_check:path:\u0026#34;/health\u0026#34;expected_statuses:- start:200end:299...例如，上面的片段定义了一个 HTTP 健康检查，Envoy 将向集群中的端点发送一个 /health 路径的 HTTP 请求。Envoy 每隔 0.25s（internal）发送一次请求，在超时前等待 1s（timeout）。要被认为是健康的，端点必须以 200 和 299 之间的状态（expected_statuses）响应两次（healthy_threshold）。端点需要以任何其他状态代码响应五次（unhealthy_threshold）才能被认为是不健康的。此外，如果端点以 HTTP 503 响应，它将立即被视为不健康（unhealthy_threshold 设置被忽略）。\n2. TCP 健康检查\n我们指定一个 Hex 编码的有效载荷（例如：68656C6C6F），并将其发送给终端。如果我们设置了一个空的有效载荷，Envoy 将进行仅连接的健康检查，它只尝试连接到端点，如果连接成功就认为是成功的。\n除了被发送的有效载荷外，我们还需要指定响应。Envoy 将对响应进行模糊匹配，如果响应与请求匹配，则认为该端点是健康的。\nclusters:- name:my_cluster_namehealth_checks:- timeout:1sinterval:0.25sunhealthy_threshold:1healthy_threshold:1tcp_health_check:send:text:\u0026#34;68656C6C6F\u0026#34;receive:- text:\u0026#34;68656C6C6F\u0026#34;...3. gRPC 健康检查\n本健康检查遵循 grpc.health.v1.Health 健康检查协议。查看 GRPC 健康检查协议文档以了解更多关于其工作方式的信息。\n我们可以设置的两个可选的配置值是 service_name 和 authority。服务名称是设置在 grpc.health.v1.Health 的 HealthCheckRequest 的 service 字段中的值。授权是 :authority 头的值。如果它是空的，Envoy 会使用集群的名称。\nclusters:- name:my_cluster_namehealth_checks:- timeout:1sinterval:0.25sunhealthy_threshold:1healthy_threshold:1grpc_health_check:{}...4. Redis 健康检查\nRedis 健康检查向端点发送一个 Redis PING 命令，并期待一个 PONG 响应。如果上游的 Redis 端点回应的不是 PONG，就会立即导致健康检查失败。我们也可以指定一个 key，Envoy 会执行 EXIST \u0026lt;key\u0026gt; 命令，而不是 PING 命令。如果 Redis 的返回值是 0（即密钥不存在），那么该端点就是健康的。任何其他响应都被视为失败。\nclusters:- name:my_cluster_namehealth_checks:- timeout:1sinterval:0.25sunhealthy_threshold:1healthy_threshold:1redis_health_check:key:\u0026#34;maintenance\u0026#34;...上面的例子检查键 “维护”（如 EXIST maintainance），如果键不存在，健康检查就通过。\nHTTP 健康检查过滤器 HTTP 健康检查过滤器可以用来限制产生的健康检查流量。过滤器可以在不同的操作模式下运行，控制流量是否被传递给本地服务（即不传递或传递）。\n1. 非穿透模式\n当以非穿透模式运行时，健康检查请求永远不会被发送到本地服务。Envoy 会以 HTTP 200 或 HTTP 503 进行响应，这取决于服务器当前的耗尽状态。\n非穿透模式的一个变种是，如果上游集群中至少有指定比例的端点可用，则返回 HTTP 200。端点的百分比可以用 cluster_min_healthy_percentages 字段来配置。\n...pass_through_mode:falsecluster_min_healthy_percentages:value:15...2. 穿透模式\n在穿透模式下，Envoy 将每个健康检查请求传递给本地服务。该服务可以用 HTTP 200 或 HTTP 503 来响应。\n穿透模式的另一个设置是使用缓存。Envoy 将健康检查请求传递给服务，并将结果缓存一段时间（cache_time）。任何后续的健康检查请求将使用缓存起来的值。一旦缓存失效，下一个健康检查请求会再次传递给服务。\n...pass_through_mode:truecache_time:5m...上面的片段启用了穿透模式，缓存在 5 分钟内到期。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"62c39e59a4266a944fe046b7d886c069","permalink":"https://jimmysong.io/docs/envoy-handbook/cluster/active-health-checking/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/envoy-handbook/cluster/active-health-checking/","section":"envoy-handbook","summary":"Envoy 支持端点上不同的主动健康检查方法。HTTP、TCP、gRP","tags":null,"title":"主动健康检查","type":"book"},{"authors":null,"categories":null,"content":"要创建一个 pod-reader 角色，创建一个 YAML 文件，内容如下：\napiVersion:rbac.authorization.k8s.io/v1kind:Rolemetadata:namespace:your-namespace-namename:pod-readerrules:- apiGroups:[\u0026#34;\u0026#34;]# \u0026#34;\u0026#34; 表示核心 API 组resources:[\u0026#34;pods\u0026#34;]verbs:[\u0026#34;get\u0026#34;,\u0026#34;watch\u0026#34;,\u0026#34;list\u0026#34;]应用角色：\nkubectl apply --f role.yaml 要创建一个全局性的 pod-reader ClusterRole：\napiVersion:rbac.authorization.k8s.io/v1kind:ClusterRolemetadata:default# \u0026#34;namespace\u0026#34; 被省略了，因为 ClusterRoles 没有被绑定到一个命名空间上name:global-pod-readerrules:- apiGroups:[\u0026#34;\u0026#34;]# \u0026#34;\u0026#34; 表示核心 API 组resources:[\u0026#34;pods\u0026#34;]verbs:[\u0026#34;get\u0026#34;,\u0026#34;watch\u0026#34;,\u0026#34;list\u0026#34;]应用角色：\nkubectl apply --f clusterrole.yaml ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"eecee7c855abb6e4669480431b8a6ee2","permalink":"https://jimmysong.io/docs/kubernetes-hardening-guidance/appendix/j/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/kubernetes-hardening-guidance/appendix/j/","section":"kubernetes-hardening-guidance","summary":"要创建一个 pod-reader 角色，创建一个 YAML 文件，内容如下： apiVersion:rbac.authorization.k8s.io/v1kind:Rolemetadata:namespace:your-namespace-namename:pod-readerrules:- apiGroups:[\"\"]# \"\" 表示核心 API","tags":null,"title":"附录 J：pod-reader RBAC 角色","type":"book"},{"authors":null,"categories":null,"content":"要创建一个 RoleBinding，需创建一个 YAML 文件，内容如下：\napiVersion:rbac.authorization.k8s.io/v1# 这个角色绑定允许 \u0026#34;jane\u0026#34; 读取 \u0026#34;your-namespace-name\u0026#34; 的 Pod 命名空间# 你需要在该命名空间中已经有一个名为 \u0026#34;pod-reader\u0026#34;的角色。kind:RoleBindingmetadata:name:read-podsnamespace:your-namespace-namesubjects:# 你可以指定一个以上的 \u0026#34;subject\u0026#34;- kind:Username:jane# \u0026#34;name\u0026#34; 是大小写敏感的apiGroup:rbac.authorization.k8s.ioroleRef:# \u0026#34;roleRef\u0026#34; 指定绑定到一个 Role/ClusterRolekind:Role# 必须是 Role 或 ClusterRolename:pod-reader# 这必须与你想绑定的 Role 或 ClusterRole 的名字相匹配apiGroup:rbac.authorization.k8s.io应用 RoleBinding：\nkubectl apply --f rolebinding.yaml 要创建一个ClusterRoleBinding，请创建一个 YAML 文件，内容如下：\napiVersion:rbac.authorization.k8s.io/v1# 这个集群角色绑定允许 \u0026#34;manager\u0026#34; 组中的任何人在任何命名空间中读取 Pod 信息。kind:ClusterRoleBindingmetadata:name:global-pod-readersubjects:# 你可以指定一个以上的 \u0026#34;subject\u0026#34;- kind:Groupname:manager# Name 是大小写敏感的apiGroup:rbac.authorization.k8s.ioroleRef:# \u0026#34;roleRef\u0026#34; 指定绑定到一个 Role/ClusterRolekind:ClusterRole# 必须是 Role 或 ClusterRolename:global-pod-reader# 这必须与你想绑定的 Role 或 ClusterRole 的名字相匹配apiGroup:rbac.authorization.k8s.io应用 RoleBinding：\nkubectl apply --f clusterrolebinding.yaml ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"67e8de9e8b00eb09e8a6db1fd06666a2","permalink":"https://jimmysong.io/docs/kubernetes-hardening-guidance/appendix/k/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/kubernetes-hardening-guidance/appendix/k/","section":"kubernetes-hardening-guidance","summary":"要创建一个 RoleBinding，需创建一个 YAML 文件，内容如下","tags":null,"title":"附录K：RBAC RoleBinding 和 ClusterRoleBinding 示例","type":"book"},{"authors":null,"categories":null,"content":"下面是一个审计策略，它以最高级别记录所有审计事件：\napiVersion:audit.k8s.io/v1kind:Policyrules:- level:RequestResponse# 这个审计策略记录了 RequestResponse 级别的所有审计事件这种审计策略在最高级别上记录所有事件。如果一个组织有可用的资源来存储、解析和检查大量的日志，那么在最高级别上记录所有事件是一个很好的方法，可以确保当事件发生时，所有必要的背景信息都出现在日志中。如果资源消耗和可用性是一个问题，那么可以建立更多的日志规则来降低非关键组件和常规非特权操作的日志级别，只要满足系统的审计要求。如何建立这些规则的例子可以在 Kubernetes 官方文档中找到。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"3ed7fe5e2385b416449f27ff26dd515e","permalink":"https://jimmysong.io/docs/kubernetes-hardening-guidance/appendix/l/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/kubernetes-hardening-guidance/appendix/l/","section":"kubernetes-hardening-guidance","summary":"下面是一个审计策略，它以最高级别记录所有审计事件： apiVersion:audit.k8s.io/v1kind:Policyrules:- level:RequestResponse# 这个审","tags":null,"title":"附录 L：审计策略","type":"book"},{"authors":null,"categories":null,"content":"在控制平面，用文本编辑器打开 kube-apiserver.yaml 文件。编辑 kube-apiserver 配置需要管理员权限。\nsudo vi /etc/kubernetes/manifests/kube-apiserver.yaml 在 kube-apiserver.yaml 文件中添加以下文字：\n--audit-policy-file=/etc/kubernetes/policy/audit-policy.yaml --audit-log-path=/var/log/audit.log --audit-log-maxage=1825 audit-policy-file 标志应该设置为审计策略的路径，而 audit-log-path 标志应该设置为所需的审计日志写入的安全位置。还有一些其他的标志，比如这里显示的 audit-log-maxage 标志，它规定了日志应该被保存的最大天数，还有一些标志用于指定要保留的最大审计日志文件的数量，最大的日志文件大小（兆字节）等等。启用日志记录的唯一必要标志是 audit-policy-file 和 audit-log-path 标志。其他标志可以用来配置日志，以符合组织的政策。\n如果用户的 kube-apiserver 是作为 Pod 运行的，那么就有必要挂载卷，并配置策略和日志文件位置的 hostPath 以保留审计记录。这可以通过在 Kubernetes 文档中指出的 kube-apiserver.yaml 文件中添加以下部分来完成：\nvolumeMounts:- mountPath:/etc/kubernetes/audit-policy.yamlname:auditreadOnly:true- mountPath:/var/log/audit.logname:audit-logreadOnly:falsevolumes:- hostPath:path:/etc/kubernetes/audit-policy.yamltype:Filename:audit- hostPath:path:/var/log/audit.logtype:FileOrCreatename:audit-log","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"d2f2b69d731384bd9b72c7001706ec0a","permalink":"https://jimmysong.io/docs/kubernetes-hardening-guidance/appendix/m/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/kubernetes-hardening-guidance/appendix/m/","section":"kubernetes-hardening-guidance","summary":"在控制平面，用文本编辑器打开 kube-apiserver.yaml 文件。编辑 kube-apiserver 配置需要管理员权限","tags":null,"title":"附录 M：向 kube-apiserver 提交审计策略文件的标志示例","type":"book"},{"authors":null,"categories":null,"content":"YAML 文件示例：\napiVersion:v1kind:Configpreferences:{}clusters:- name:example-clustercluster:server:http://127.0.0.1:8080#web endpoint address for the log files to be sent toname:audit-webhook-serviceusers:- name:example-usersuser:username:example-userpassword:example-passwordcontexts:- name:example-contextcontext:cluster:example-clusteruser:example-usercurrent-context:example-context#source: https://dev.bitolog.com/implement-audits-webhook/由 webhook 发送的审计事件是以 HTTP POST 请求的形式发送的，请求体中包含 JSON 审计事件。指定的地址应该指向一个能够接受和解析这些审计事件的端点，无论是第三方服务还是内部配置的端点。\n向 kube-apiserver 提交 webhook 配置文件的标志示例：\n在控制面编辑 kube-apiserver.yaml 文件\nsudo vi /etc/kubernetes/manifests/kube-apiserver.yaml 在 kube-apiserver.yaml 文件中添加以下文字\n--audit-webhook-config-file=/etc/kubernetes/policies/webhook-policy.yaml --audit-webhook-initial-backoff=5 --audit-webhook-mode=batch --audit-webhook-batch-buffer-size=5 audit-webhook-initial-backoff 标志决定了在一个初始失败的请求后要等待多长时间才能重试。可用的 webhook 模式有 batch、block 和 blocking-stric 的。当使用批处理模式时，有可能配置最大等待时间、缓冲区大小等。Kubernetes 官方文档包含了其他配置选项的更多细节审计和 kube-apiserver。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"73c346d2bfa3e04d2c95bdd26d4ad0b5","permalink":"https://jimmysong.io/docs/kubernetes-hardening-guidance/appendix/n/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/kubernetes-hardening-guidance/appendix/n/","section":"kubernetes-hardening-guidance","summary":"YAML 文件示例： apiVersion:v1kind:Configpreferences:{}clusters:- name:example-clustercluster:server:http://127.0.0.1:8080#web endpoint address for the log files to be sent toname:audit-webhook-serviceusers:- name:example-usersuser:username:example-userpassword:example-passwordcontexts:- name:example-contextcontext:cluster:example-clusteruser:example-usercurrent-context:example-context#source: https://de","tags":null,"title":"附录 N：webhook 配置","type":"book"},{"authors":null,"categories":null,"content":"DestinationRule 定义了在路由发生后适用于服务流量的策略。这些规则指定了负载均衡的配置、来自 sidecar 的连接池大小，以及用于检测和驱逐负载均衡池中不健康主机的异常检测设置。\n示例 例如，ratings 服务的一个简单的负载均衡策略看起来如下。\napiVersion:networking.istio.io/v1alpha3kind:DestinationRulemetadata:name:bookinfo-ratingsspec:host:ratings.prod.svc.cluster.localtrafficPolicy:loadBalancer:simple:LEAST_CONN可以通过定义一个 subset 并覆盖在服务级来配置特定版本的策略。下面的规则对前往由带有标签（version:v3）的端点（如 pod）组成的名为 testversion 的子集的所有流量使用轮询负载均衡策略。\napiVersion:networking.istio.io/v1alpha3kind:DestinationRulemetadata:name:bookinfo-ratingsspec:host:ratings.prod.svc.cluster.localtrafficPolicy:loadBalancer:simple:LEAST_CONNsubsets:- name:testversionlabels:version:v3trafficPolicy:loadBalancer:simple:ROUND_ROBIN注意：只有当路由规则明确地将流量发送到这个子集，为子集指定的策略才会生效。\n流量策略也可以针对特定的端口进行定制。下面的规则对所有到 80 号端口的流量使用最少连接的负载均衡策略，而对 9080 号端口的流量使用轮流负载均衡设置。\napiVersion:networking.istio.io/v1alpha3kind:DestinationRulemetadata:name:bookinfo-ratings-portspec:host:ratings.prod.svc.cluster.localtrafficPolicy:# 应用到所有端口portLevelSettings:- port:number:80loadBalancer:simple:LEAST_CONN- port:number:9080loadBalancer:simple:ROUND_ROBIN配置项 下图是 DestinationRule 资源的配置拓扑图。\n  DestinationRule 资源配置拓扑图  DestinationRule 资源的顶级配置项如下：\n  host：字符串类型。来自服务注册表的服务名称。服务名称从平台的服务注册表（例如，Kubernetes 服务、Consul 服务等）和 ServiceEntry 声明的主机中查找。为服务注册表中不存在的服务定义的规则将被忽略。\nKubernetes 用户的注意事项。当使用短名称时（例如 reviews 而不是 reviews.default.svc.cluster.local），Istio 将根据规则的命名空间而不是服务来解释短名称。在 default 命名空间中包含主机 reviews 的规则将被解释为 reviews.default.svc.cluster.local，而不考虑与 reviews 服务相关的实际命名空间。为了避免潜在的错误配置，建议总是使用完全限定名而不是短名称。\n注意，主机字段适用于 HTTP 和 TCP 服务。\n  trafficPolicy：要应用的流量策略（负载均衡策略、连接池大小、异常值检测）。\n  subsets：一个或多个命名的集合，代表一个服务的单独版本。流量策略可以在子集级别被覆盖。\n  exportTo：DestinationRule 被导出的命名空间的列表。DestinationRule 的解析是在命名空间级别中进行的。导出的 DestinationRule 被包含在其他命名空间的服务的解析层次中。该功能为服务所有者和网格管理员提供了一种机制，以控制 DestinationRule 在命名空间边界的可见性。\n如果没有指定命名空间，那么默认情况下，DestinationRule 会被输出到所有命名空间。\n值 . 是保留的，它定义了导出到 DestinationRule 声明的同一命名空间。同样，值 * 也是保留的，它定义了导出到所有命名空间。\n  下面是一些重要的配置项的使用说明。\n示例配置 下面是一个相对完整的 DestinationRule 配置的示例，其中包含了所有基本配置，对应的解释请见注释。\napiVersion:networking.istio.io/v1alpha3kind:DestinationRulemetadata:name:bookinfo-ratingsnamespace:bookinfospec:host:ratings.prod.svc.cluster.local#来自服务注册表的服务名称trafficPolicy:#要应用的流量策略，默认是应用与所有端口loadBalancer:#配置负载均衡算法simple:ROUND_ROBINportLevelSettings:#对指定端口应用流量策略，可覆盖全局配置- port:number:80loadBalancer:simple:LEAST_CONNconnectionPool:#配置上游服务的连接量，可以配置 TCP 和 HTTPtcp:#同时适用于 TCP 和 HTTP 上游连接maxConnections:100#与上游服务的最大连接数connectTimeout:30ms#TCP 连接的超时时间，默认为 10stcpKeepalive:#如果有该配置，则在套接字上设置 SO_KEEPALIVE，以启用 TCP Keepalive。time:7200s #最后一次探测和第一次探测之间间隔的时间。默认是使用操作系统级别的配置（除非被覆盖，Linux默认为7200s，即2小时。）interval:75s#探测发送间隔，默认是 75sprobs:9# 最大探测次数，使用操作系统级别的默认值，Linux 系统的默认值是 9，如果超过该次数没有得到回复，则意味着连接断开了http:#针对 HTTP 连接池的配置http2MaxRequests:1000#最大请求数maxRequestsPerConnection:10#每个连接中的最大请求数，默认是 0，意味着没有限制maxRetries:5#在给定时间内，集群中的所有主机未完成的最大重试次数outlierDetection:#异常值检测，实际为一个断路器实现，跟踪上游服务中每个独立主机的状态。同时适用于 HTTP 和 TCP 服务。对于 HTTP 服务，持续返回 5xx 错误的 API 调用的主机将在预先定义的时间内从连接池中弹出。对于 TCP 服务，在测量连续错误指标时，对特定主机的连接超时或连接失败算作一个错误。consecutive5xxErrors:7#当达到 7 次 5xx 错误时，该主机将从上游连接中弹出。当通过不透明的 TCP 连接被访问时上游主机时，连接超时、错误/失败和请求失败事件都有资格成为5xx错误。该功能默认为5，但可以通过设置该值为0来禁用。interval:5m#异常值检测的时间间隔，默认是为 10sbaseEjectionTime:15m#主机将保持弹出的时间，等于最小弹出持续时间和主机被弹出次数的乘积。这种技术允许系统自动增加不健康的上游服务器的弹出时间。默认为 30s。tls:#关于 TLS 的设置mode:MUTUAL#一共有四种模式，DISABLE：关闭 TLS 连接；SIMPLE：发起一个与上游端点的 TLS 连接；MUTUAL：手动配置证书，通过出示客户端证书进行认证，使用双向的 TLS 确保与上游的连接；ISTIO_MUTUAL：该模式使用 Istio 自动生成的证书进行 mTLS 认证。clientCertificate:/etc/certs/myclientcert.pemprivateKey:/etc/certs/client_private_key.pemcaCertificates:/etc/certs/rootcacerts.pemsubsets:#服务版本- name:v1labels:version:v1#所有具有该标签的 Pod 被划分为该子集exportTo:#指定DestinationRule 的可见性- \u0026#34;*\u0026#34;#*表示对所有命名空间可见，此为默认值；\u0026#34;.\u0026#34;表示仅对当前命名空间可见关于 DestinationRule 配置的详细用法请参考 Istio 官方文档。\n参考  Destination Rule - istio.io  ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"cee24bb6bd867c31ae74f9a3716a757f","permalink":"https://jimmysong.io/docs/istio-handbook/config-networking/destination-rule/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/istio-handbook/config-networking/destination-rule/","section":"istio-handbook","summary":"DestinationRule 定义了在路由发生后适用于服务流量的策略。这些规则指定了负载","tags":null,"title":"DestinationRule","type":"book"},{"authors":null,"categories":null,"content":"Grafana 是一个用于分析和监控的开放平台。Grafana 可以连接到各种数据源，并使用图形、表格、热图等将数据可视化。通过强大的查询语言，你可以定制现有的仪表盘并创建更高级的可视化。\n通过 Grafana，我们可以监控 Istio 安装和服务网格中运行的应用程序的健康状况。\n我们可以使用 grafana.yaml 来部署带有预配置仪表盘的 Grafana 示例安装。该 YAML 文件在 Istio 安装包的 /samples/addons 下。\n确保在部署 Grafana 之前部署 Promeheus 插件，因为 Grafana 使用 Prometheus 作为其数据源。\n运行下面的命令来部署 Grafana 和预配置的仪表盘：\n$ kubectl apply -f istio-1.9.0/samples/addons/grafana.yaml serviceaccount/grafana created configmap/grafana created service/grafana created deployment.apps/grafana created configmap/istio-grafana-dashboards created configmap/istio-services-grafana-dashboards created  我们不打算在生产中运行这个 Grafana，因为它没有经过性能或安全方面的优化。\n Kubernetes 将 Grafana 部署在 istio-system 命名空间。要访问 Grafana，我们可以使用 istioctl dashboard 命令。\n$ istioctl dashboard grafana http://localhost:3000 我们可以在浏览器中打开 http://localhost:3000，进入 Grafana。然后，点击首页和 Istio 文件夹，查看已安装的仪表板，如下图所示。\n   Grafana 仪表板  Istio Grafana 安装时预配置了以下仪表板：\n1. Istio 控制平面仪表板\n从 Istio 控制平面仪表板，我们可以监控 Istio 控制平面的健康和性能。\n   Istio 控制平面仪表板  这个仪表板将向我们显示控制平面的资源使用情况（内存、CPU、磁盘、Go routines），以及关于 Pilot 、Envoy 和 Webhook 的信息。\n2. Istio 网格仪表板\n网格仪表盘为我们提供了在网格中运行的所有服务的概览。仪表板包括全局请求量、成功率以及 4xx 和 5xx 响应的数量。\n   Istio 网格仪表板  3. Istio 性能仪表板\n性能仪表盘向我们展示了 Istio 主要组件在稳定负载下的资源利用率。\n   Istio 性能仪表板  4. Istio 服务仪表板\n服务仪表板允许我们在网格中查看关于我们服务的细节。\n我们可以获得关于请求量、成功率、持续时间的信息，以及显示按来源和响应代码、持续时间和大小的传入请求的详细图表。\n   Istio 服务仪表板  5. Istio Wasm 扩展仪表板\nIstio Wasm 扩展仪表板显示与 WebAssembly 模块有关的指标。从这个仪表板，我们可以监控活动的和创建的 Wasm 虚拟机，关于获取删除 Wasm 模块和代理资源使用的数据。\n   Istio Wasm 扩展仪表板  6. Istio 工作负载仪表板\n这个仪表板为我们提供了一个工作负载的详细指标分类。\n   Istio 工作负载仪表板  ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"68511e58832f3b917de513ac54bc1b5e","permalink":"https://jimmysong.io/docs/istio-handbook/observability/grafana/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/istio-handbook/observability/grafana/","section":"istio-handbook","summary":"Grafana 是一个用于分析和监控的开放平台。Grafana 可以连接到各","tags":null,"title":"Grafana","type":"book"},{"authors":null,"categories":null,"content":"HTTP 检查器监听器过滤器（envoy.filters.listener.http_inspector）允许我们检测应用协议是否是 HTTP。如果协议不是 HTTP，监听器过滤器将通过该数据包。\n如果应用协议被确定为 HTTP，它也会检测相应的 HTTP 协议（如 HTTP/1.x 或 HTTP/2）。\n我们可以使用过滤器链匹配中的 application_protocols 字段来检查 HTTP 检查过滤器的结果。\n让我们考虑下面的片段。\n...listener_filters:- name:envoy.filters.listener.http_inspectortyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.listener.http_inspector.v3.HttpInspectorfilter_chains:- filter_chain_match:application_protocols:[\u0026#34;h2\u0026#34;]filters:- name:my_http2_filter... - filter_chain_match:application_protocols:[\u0026#34;http/1.1\u0026#34;]filters:- name:my_http1_filter...我们在 listener_filters 字段下添加了 http_inspector 过滤器来检查连接并确定应用协议。如果 HTTP 协议是 HTTP/2（h2c），Envoy 会匹配第一个网络过滤器链（以 my_http2_filter 开始）。\n另外，如果下游的 HTTP 协议是 HTTP/1.1（http/1.1），Envoy 会匹配第二个过滤器链，并从名为 my_http1_filter 的过滤器开始运行过滤器链。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"f39919523831eabea6144595357e15ee","permalink":"https://jimmysong.io/docs/envoy-handbook/listener/http-inspector-listener-filter/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/envoy-handbook/listener/http-inspector-listener-filter/","section":"envoy-handbook","summary":"HTTP 检查器监听器过滤器（envoy.filters.liste","tags":null,"title":"HTTP 检查器监听器过滤器","type":"book"},{"authors":null,"categories":null,"content":"Istio 是一个服务网格的开源实现。从宏观上来看 Istio 支持以下功能。\n1. 流量管理\n利用配置，我们可以控制服务间的流量。设置断路器、超时或重试都可以通过简单的配置改变来完成。\n2. 可观察性\nIstio 通过跟踪、监控和记录让我们更好地了解你的服务，它让我们能够快速发现和修复问题。\n3. 安全性\nIstio 可以在代理层面上管理认证、授权和通信的加密。我们可以通过快速的配置变更在各个服务中执行政策。\nIstio 组件 Istio 服务网格有两个部分：数据平面和控制平面。\n在构建分布式系统时，将组件分离成控制平面和数据平面是一种常见的模式。数据平面的组件在请求路径上，而控制平面的组件则帮助数据平面完成其工作。\nIstio 中的数据平面由 Envoy 代理组成，控制服务之间的通信。网格的控制平面部分负责管理和配置代理。\n   Istio 架构  Envoy（数据平面） Envoy 是一个用 C++ 开发的高性能代理。Istio 服务网格将 Envoy 代理作为一个 sidecar 容器注入到你的应用容器旁边。然后该代理拦截该服务的所有入站和出站流量。注入的代理一起构成了服务网格的数据平面。\nEnvoy 代理也是唯一与流量进行交互的组件。除了前面提到的功能 —— 负载均衡、断路器、故障注入等。Envoy 还支持基于 WebAssembly（WASM）的可插拔扩展模型。这种可扩展性使我们能够执行自定义策略，并为网格中的流量生成遥测数据。\nIstiod（控制平面） Istiod 是控制平面组件，提供服务发现、配置和证书管理功能。Istiod 采用 YAML 编写的高级规则，并将其转换为 Envoy 的可操作配置。然后，它把这个配置传播给网格中的所有 sidecar。\nIstiod 内部的 Pilot 组件抽象出特定平台的服务发现机制（Kubernetes、Consul 或 VM），并将其转换为 sidecar 可以使用的标准格式。\n使用内置的身份和凭证管理，我们可以实现强大的服务间和终端用户认证。通过授权功能，我们可以控制谁可以访问你的服务。\n控制平面的部分以前被称为 Citadel，作为一个证书授权机构，生成证书，允许数据平面中的代理之间进行安全的 mTLS 通信。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"a1dd4248dd574823c7a38e95647a2ce2","permalink":"https://jimmysong.io/docs/istio-handbook/concepts/istio-intro/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/istio-handbook/concepts/istio-intro/","section":"istio-handbook","summary":"Istio 是一个服务网格的开源实现。从宏观上来看 Istio 支持以下功能。 1. 流","tags":null,"title":"Istio 简介","type":"book"},{"authors":null,"categories":null,"content":"Istio 中有个 issue #9066 要求将 Istio 中默认使用的 Service Graph 替换成 Kiali。Kiali 最初是由 Red Hat 开源的，用于解决 Service Mesh 中可观察性即微服务的可视性问题。目前已获得 Istio 社区的官方支持。\n关于 Kiali 单体应用使用微服务架构拆分成了许多微服务的组合。服务的数量显著增加，就对需要了解服务之间的通信模式，例如容错（通过超时、重试、断路等）以及分布式跟踪，以便能够看到服务调用的去向。服务网格可以在平台级别上提供这些服务，并使应用程序编写者从以上繁重的通信模式中解放出来。路由决策在网格级别完成。Kiali 与Istio 合作，可视化服务网格拓扑、断路器和请求率等功能。Kiali还包括 Jaeger Tracing，可以提供开箱即用的分布式跟踪功能。\nKiali 提供的功能 Kiali 提供以下功能：\n 服务拓扑图 分布式跟踪 指标度量收集和图标 配置校验 健康检查和显示 服务发现  下图展示了 kiali 中显示的 Bookinfo 示例的服务拓扑图。\n   Kiali 页面  编译安装与试用 Kilia pod 中运行的进程是 /opt/kiali/kiali -config /kiali-configuration/config.yaml -v 4。\n/kiali-configuration/config.yaml 是使用 ConfigMap 挂载进去的，用于配置 Kiali 的 Web 根路径和外部服务地址。\nserver:port:20001web_root:/external_services:jaeger:url:\u0026#34;http://172.17.8.101:31888\u0026#34;grafana:url:\u0026#34;http://grafana.istio-system:3000\u0026#34;Kiali 中的基本概念 在了解 Kiali 如何提供 Service Mesh 中微服务可观察性之前，我们需要先了解下 Kiali 如何划分监控类别的。\n Application：使用运行的工作负载，必须使用 Istio 的将 Label 标记为 app 才算。注意，如果一个应用有多个版本，只要 app 标签的值相同就是属于同一个应用。 Deployment：即 Kubernetes 中的 Deployment。 Label：这个值对于 Istio 很重要，因为 Istio 要用它来标记 metrics。每个 Application 要求包括 app 和 version 两个 label。 Namespace：通常用于区分项目和用户。 Service：即 Kubernetes 中的 Service，不过要求必须有 app label。 Workload：Kubernetes 中的所有常用资源类型如 Deployment、StatefulSet、Job 等都可以检测到，不论这些负载是否加入到 Istio Service Mesh 中。  Application、Workload 与 Service 的关系如下图所示。\n   Kiali 页面  Kilia 的详细 API 使用说明请查看 Swagger API 文档，在 Kiali 的根目录下运行下面的命令可以查看 API 文档。\nmake swagger-serve Swagger UI 如下图。\n   Kiali API文档  架构 Kiali 部署完成后只启动了一个 Pod，前后端都集成在这一个 Pod 中。Kiali 也有一些依赖的组件，例如如果要在 Kiali 的页面中获取到监控 metric 需要使用在 istio-system 中部署 Prometheus。下图是 Kiali 的架构，来自 Kiali 官网。\n   Kiali 架构图  Kiali 使用传统的前后端分离架构：\n 后端使用 Go 编写：https://github.com/kiali/kiali，为前端提供 API，所有消息使用 JSON 编码，使用 ConfigMap 和 Secret 来存储配置。直接与 Kubernetes 和 Istio 通信来获取数据。 前端使用 Typescript 编写：https://github.com/kiali/kiali-ui，无状态，除了一些证书保存在浏览器中。于查询后端 API，可以跳转访问 Jaeger 分布式追踪和 Grafana 监控页面。  Jaeger 和 Grafana 都是可选组件，使用的都是外部服务，不是由 Kiali 部署的，需要在 kiali-configmap.yaml 中配置 URL。注意该 URL 必须是从你本地浏览器中可以直接访问到的地址。\n注意：如果服务之间没有任何请求就不会在 Prometheus 中保存数据也就无法显示服务拓扑图，所以大家在部署完 Bookinfo 服务之后向 productpage 服务发送一些请求用于生成服务拓扑图。\n服务拓扑图 Kiali 中的服务拓扑图比起 Istio 原来默认部署的 ServiceGraph 的效果更炫也更加直观，具有更多选项。\n   Kiali 中的服务拓扑图  例如使用 CURL 模拟请求。\n$ curl -H \u0026#34;Authorization: Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1c2VybmFtZSI6ImFkbWluIiwiZXhwIjoxNTM5NjczOTYyfQ.6gNz4W6yA9Bih4RkTbcSvqdaiRqsyj8c8o6ictM9iDs\u0026#34; http://172.17.8.101:32439/api/namespaces/all/graph?duration=60s\u0026amp;graphType=versionedApp\u0026amp;injectServiceNodes=false\u0026amp;appenders=dead_node,sidecars_check,istio 会得到如下的返回的 JSON 返回值，为了节省篇幅其中省略了部分结果：\n{ \u0026#34;timestamp\u0026#34;: 1539296648, \u0026#34;graphType\u0026#34;: \u0026#34;versionedApp\u0026#34;, \u0026#34;elements\u0026#34;: { \u0026#34;nodes\u0026#34;: [ { \u0026#34;data\u0026#34;: { \u0026#34;id\u0026#34;: \u0026#34;6519157be154675342fb76c41edc731c\u0026#34;, \u0026#34;nodeType\u0026#34;: \u0026#34;app\u0026#34;, \u0026#34;namespace\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;app\u0026#34;: \u0026#34;reviews\u0026#34;, \u0026#34;isGroup\u0026#34;: \u0026#34;version\u0026#34; } }, ... { \u0026#34;data\u0026#34;: { \u0026#34;id\u0026#34;: \u0026#34;6249668dd0a91adb9e62994d36563365\u0026#34;, \u0026#34;nodeType\u0026#34;: \u0026#34;app\u0026#34;, \u0026#34;namespace\u0026#34;: \u0026#34;istio-system\u0026#34;, \u0026#34;workload\u0026#34;: \u0026#34;istio-ingressgateway\u0026#34;, \u0026#34;app\u0026#34;: \u0026#34;istio-ingressgateway\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;unknown\u0026#34;, \u0026#34;rateOut\u0026#34;: \u0026#34;0.691\u0026#34;, \u0026#34;isOutside\u0026#34;: true, \u0026#34;isRoot\u0026#34;: true } } ], \u0026#34;edges\u0026#34;: [ { \u0026#34;data\u0026#34;: { \u0026#34;id\u0026#34;: \u0026#34;d51ca2a95d721427bbe27ed209766ec5\u0026#34;, \u0026#34;source\u0026#34;: \u0026#34;06e488a37fc9aa5b0e0805db4f16ae69\u0026#34;, \u0026#34;target\u0026#34;: \u0026#34;31150e7e5adf85b63f22fbd8255803d7\u0026#34;, \u0026#34;rate\u0026#34;: \u0026#34;0.236\u0026#34;, \u0026#34;percentRate\u0026#34;: \u0026#34;17.089\u0026#34;, \u0026#34;responseTime\u0026#34;: \u0026#34;0.152\u0026#34; } }, ... { \u0026#34;data\u0026#34;: { \u0026#34;id\u0026#34;: \u0026#34;1dda06d9904bcf727d1b6a113be58556\u0026#34;, \u0026#34;source\u0026#34;: \u0026#34;80f71758099020586131c3565075935d\u0026#34;, \u0026#34;target\u0026#34;: \u0026#34;4b64bda48e5a3c7e50ab1c63836c9469\u0026#34;, \u0026#34;rate\u0026#34;: \u0026#34;0.236\u0026#34;, \u0026#34;responseTime\u0026#34;: \u0026#34;0.022\u0026#34; } } ] } } 该值中包含了每个 node 和 edege 的信息，Node 即图中的每个节点，其中包含了节点的配置信息，Edge 即节点间的关系还有流量情况。前端可以根据该信息绘制服务拓扑图，我们下面将查看下 kiali 的后端，看看它是如何生成以上格式的 JSON 信息的。\n注：详细的 REST API 使用和字段说明请查看 swagger 生成的 API 文档。\n代码解析 下面将带大家了解 Kiali 的后端代码基本结构。\n路由配置\n服务拓扑图的路由信息保存在 kiali/routing/routes.go 文件中。\n{ \u0026#34;GraphNamespace\u0026#34;, \u0026#34;GET\u0026#34;, \u0026#34;/api/namespaces/{namespace}/graph\u0026#34;, handlers.GraphNamespace, true, }, { \u0026#34;GraphAppVersion\u0026#34;, \u0026#34;GET\u0026#34;, \u0026#34;/api/namespaces/{namespace}/applications/{app}/versions/{version}/graph\u0026#34;, handlers.GraphNode, true, }, { \u0026#34;GraphApp\u0026#34;, \u0026#34;GET\u0026#34;, \u0026#34;/api/namespaces/{namespace}/applications/{app}/graph\u0026#34;, handlers.GraphNode, true, }, { \u0026#34;GraphService\u0026#34;, \u0026#34;GET\u0026#34;, \u0026#34;/api/namespaces/{namespace}/services/{service}/graph\u0026#34;, handlers.GraphNode, true, }, { \u0026#34;GraphWorkload\u0026#34;, \u0026#34;GET\u0026#34;, \u0026#34;/api/namespaces/{namespace}/workloads/{workload}/graph\u0026#34;, handlers.GraphNode, true, } 直接查看 Swagger 生成的 API 文档也可以。\nPQL 查询语句构建\nkiali/handlers/graph.go 中处理 HTTP 请求，服务拓扑图中所有的指标信息都是从 Prometheus 中查询得到的。\nKiali 的服务状态拓扑是根据 namespace 来查询的，例如 default namespace 下的服务指标查询 PQL：\nround(sum(rate(istio_requests_total{reporter=\u0026#34;source\u0026#34;,source_workload_namespace=\u0026#34;default\u0026#34;,response_code=~\u0026#34;[2345][0-9][0-9]\u0026#34;} [600s])) by (source_workload_namespace,source_workload,source_app,source_version,destination_service_namespace,destination_service_name,destination_workload,destination_app,destination_version,response_code),0.001) 其中的参数都是通过页面选择传入的（构建的 PQL 中的选项在 kiali/graph/options/options.go 中定义）：\n reporter=\u0026#34;source\u0026#34;：metric 报告来源，源服务（source）是 Envoy 代理的下游客户端。在服务网格里，一个源服务通常是一个工作负载，但是入口流量的源服务有可能包含其他客户端，例如浏览器，或者一个移动应用。 …","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"9e5649e22ad4002c5c298dec0a42918d","permalink":"https://jimmysong.io/docs/istio-handbook/setup/istio-observability-tool-kiali/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/istio-handbook/setup/istio-observability-tool-kiali/","section":"istio-handbook","summary":"Istio 中有个 issue #9066 要求将 Istio 中默认使用的 Service Graph 替换成 Kiali。Kia","tags":null,"title":"安装可观察性工具 Kiali","type":"book"},{"authors":null,"categories":null,"content":"Merbridge 是由 DaoCloud 在 2022 年初开源的的一款利用 eBPF 加速 Istio 服务网格的插件。使用 Merbridge 可以在一定程度上优化数据平面的网络性能。\n使用条件 要想使用 Merbridge，你的系统必须满足以下条件：\n Istio 网格中的主机使用 Linux 5.7 及以上版本内核  原理 在 Istio 中的透明流量劫持详解中，我们谈到 Istio 默认使用 IPtables 拦截数据平面中的流量到 Envoy 代理，这种拦截方式通用性最强。因为 Pod 中所有的 inbound 和 outbound 流量都会先通过 Envoy 代理，尤其是 Pod 接收的流量，都要先通过 IPtables 将流量劫持到 Envoy 代理后再发往 Pod 中的应用容器的端口。\n   使用 IPtables 劫持流量发到当前 Pod 的应用端口  利用 eBPF 的 sockops 和 redir 能力，可以直接将数据包从 inbound socket 传输到 outbound socket。eBPF 提供了 bpf_msg_redirect_hash 函数可以直接转发应用程序的数据包。\n下图展示的是在不同主机上的 Pod 的利用 eBPF 来劫持流量的示意图。\n   使用 Merbridge 的在不同主机上的 Pod  Pod 内部的流量劫持使用的是 Merbridge，而不同主机间依然需要使用 IPtables 来转发流量。\n如果两个 Pod 位于同一台主机，那么流量转发全程都可以通过 Merbridge 完成。下图展示了的是在同一主机上使用 Merbridge 的示意图。\n   使用 Merbridge 的同一个主机上的 Pod  关于 Merbridge 原理的详细解释请参考 Istio 文档。\n如何使用 只需要在 Istio 集群执行一条命令，即可直接使用 eBPF 代替 iptables 做透明流量拦截，实现网络加速。而且这对 Istio 是无感的，你可以随时安装和卸载 Merbridge。使用下面的命令启用 Merbridge：\nkubectl apply -f https://raw.githubusercontent.com/merbridge/merbridge/main/deploy/all-in-one.yaml Merbridge 是以 DaemonSet 的方式运行在 Istio 网格的每个节点上。它运行在服务网格的下层，对于 Istio 是透明的，要启用它时，无需对 Istio 做任何改动。\n如果你想删除 Merbridge，请运行下面的命令：\nkubectl delete -f https://raw.githubusercontent.com/merbridge/merbridge/main/deploy/all-in-one.yaml 参考  Merbridge - Accelerate your mesh with eBPF - istio.io   下一章   ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"71ca9cb8fa3d38de82d7427a06396ac3","permalink":"https://jimmysong.io/docs/istio-handbook/ecosystem/merbridge/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/istio-handbook/ecosystem/merbridge/","section":"istio-handbook","summary":"Merbridge 是由 DaoCloud 在 2022 年初开源的的一款利用 eBPF 加速 Istio 服务网格的插件。使用","tags":null,"title":"Merbridge","type":"book"},{"authors":null,"categories":null,"content":"服务中的工作负载之间的通信是通过 Envoy 代理进行的。当一个工作负载使用 mTLS 向另一个工作负载发送请求时，Istio 会将流量重新路由到 sidecar 代理（Envoy）。\n然后，sidecar Envoy 开始与服务器端的 Envoy 进行 mTLS 握手。在握手过程中，调用者会进行安全命名检查，以验证服务器证书中的服务账户是否被授权运行目标服务。一旦 mTLS 连接建立，Istio 就会将请求从客户端的 Envoy 代理转发到服务器端的 Envoy 代理。在服务器端的授权后，sidecar 将流量转发到工作负载。\n我们可以在服务的目标规则中改变 mTLS 行为。支持的 TLS 模式有：DISABLE（无 TLS 连接）、SIMPLE（向上游端点发起 TLS 连接）、MUTUAL（通过出示客户端证书进行认证来使用 mTLS）和 ISTIO_MUTUAL（与 MUTUAL 类似，但使用 Istio 自动生成的证书进行 mTLS）。\n什么是 mTLS？ 在一个典型的 Kubernetes 集群中，加密的流量进入集群，经过一个负载均衡器终止 TLS 连接，从而产生解密的流量。然后，解密的流量被发送到集群内的相关服务。由于集群内的流量通常被认为是安全的，对于许多用例，这是一个可以接受的方法。\n但对于某些用例，如处理个人身份信息（PII），可能需要额外的保护。在这些情况下，我们希望确保 所有的 网络流量，甚至同一集群内的流量，都是加密的。这为防止窥探（读取传输中的数据）和欺骗（伪造数据来源）攻击提供了额外保障。这可以帮助减轻系统中其他缺陷的影响。\n如果手动实现这个完整的数据传输加密系统的话，需要对集群中的每个应用程序进行大规模改造。你需要告诉所有的应用程序终止自己的 TLS 连接，为所有的应用程序颁发证书，并为所有的应用程序添加一个新的证书颁发机构。\nIstio 的 mTLS 在应用程序之外处理这个问题。它安装了一个 sidecar，通过 localhost 连接与你的应用程序进行通信，绕过了暴露的网络流量。它使用复杂的端口转发规则（通过 IPTables）来重定向进出 Pod 的流量，使其通过 sidecar。代理中的 Envoy sidecar 处理所有获取 TLS 证书、刷新密钥、终止等逻辑。\nIstio 的这种工作方式虽然可以让你避免修改应用程序，但是当它可以工作时，能够工作得很好。而当它失败时，它可能是灾难性的，而且还难以调试。Istio 的 mTLS 值得一提的三个具体要点。\n  在严格模式（Strict Mode）下，也就是我们要做的，数据平面 Envoy 会拒绝任何传入的明文通信。\n  通常情况下，如果你对一个不存在的主机进行 HTTP 连接，你会得到一个失败的连接错误。你肯定 不会 得到一个 HTTP 响应。然而，在 Istio 中，你将 总是 成功地发出 HTTP 连接，因为你的连接是给 Envoy 本身的。如果 Envoy 代理不能建立连接，它将像大多数代理一样，返回一个带有 503 错误信息的 HTTP 响应体。\n  Envoy 代理对一些协议有特殊处理。最重要的是，如果你做一个纯文本的 HTTP 外发连接，Envoy 代理有复杂的能力来解析外发请求，了解各种头文件的细节，并做智能路由。\n  允许模式 允许模式（Permissive Mode）是一个特殊的选项，它允许一个服务同时接受纯文本流量和 mTLS 流量。这个功能的目的是为了改善 mTLS 的用户体验。\n默认情况下，Istio 使用允许模式配置目标工作负载。Istio 跟踪使用 Istio 代理的工作负载，并自动向其发送 mTLS 流量。如果工作负载没有代理，Istio 将发送纯文本流量。\n当使用允许模式时，服务器接受纯文本流量和 mTLS 流量，不会破坏任何东西。允许模式给了我们时间来安装和配置 sidecar，以逐步发送 mTLS 流量。\n一旦所有的工作负载都安装了 sidecar，我们就可以切换到严格的 mTLS 模式。要做到这一点，我们可以创建一个 PeerAuthentication 资源。我们可以防止非双向 TLS 流量，并要求所有通信都使用 mTLS。\n我们可以创建 PeerAuthentication 资源，首先在每个命名空间中分别执行严格模式。然后，我们可以在根命名空间（在我们的例子中是 istio-system）创建一个策略，在整个服务网格中执行该策略：\napiVersion:security.istio.io/v1beta1kind:PeerAuthenticationmetadata:name:defaultnamespace:istio-systemspec:mtls:mode:STRICT此外，我们还可以指定selector字段，将策略仅应用于网格中的特定工作负载。下面的例子对具有指定标签的工作负载启用STRICT模式：\napiVersion:security.istio.io/v1beta1kind:PeerAuthenticationmetadata:name:defaultnamespace:my-namespacespec:selector:matchLabels:app:customersmtls:mode:STRICT参考  An Istio/mutual TLS debugging story -fpcomplete.com  ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"4ba8fc9d16268c1984810e82c985e586","permalink":"https://jimmysong.io/docs/istio-handbook/security/mtls/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/istio-handbook/security/mtls/","section":"istio-handbook","summary":"服务中的工作负载之间的通信是通过 Envoy 代理进行的。当一个工作负载","tags":null,"title":"mTLS","type":"book"},{"authors":null,"categories":null,"content":"PeerAuthentication（对等认证）定义了流量将如何被隧道化（或不被隧道化）到 sidecar。\n示例 策略允许命名空间 foo 下所有工作负载的 mTLS 流量。\napiVersion:security.istio.io/v1beta1kind:PeerAuthenticationmetadata:name:defaultnamespace:foospec:mtls:mode:STRICT对于网格级别，根据你的 Istio 安装，将策略放在根命名空间。\n策略允许命名空间 foo 下的所有工作负载的 mTLS 和明文流量，但 finance 的工作负载需要 mTLS。\napiVersion:security.istio.io/v1beta1kind:PeerAuthenticationmetadata:name:defaultnamespace:foospec:mtls:mode:PERMISSIVE---apiVersion:security.istio.io/v1beta1kind:PeerAuthenticationmetadata:name:defaultnamespace:foospec:selector:matchLabels:app:financemtls:mode:STRICT政策允许所有工作负载严格 mTLS，但 8080 端口保留为明文。\napiVersion:security.istio.io/v1beta1kind:PeerAuthenticationmetadata:name:defaultnamespace:foospec:selector:matchLabels:app:financemtls:mode:STRICTportLevelMtls:8080:mode:DISABLE从命名空间（或网格）设置中继承 mTLS 模式的策略，并覆盖 8080 端口的设置。\napiVersion:security.istio.io/v1beta1kind:PeerAuthenticationmetadata:name:defaultnamespace:foospec:selector:matchLabels:app:financemtls:mode:UNSETportLevelMtls:8080:mode:DISABLE关于 PeerAuthentication 配置的详细用法请参考 Istio 官方文档。\n参考  PeerAuthentication- istio.io  ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"746cdff0d3838b0e0986756aef0be42f","permalink":"https://jimmysong.io/docs/istio-handbook/config-security/peer-authentication/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/istio-handbook/config-security/peer-authentication/","section":"istio-handbook","summary":"PeerAuthentication（对等认证）定义了流量将","tags":null,"title":"PeerAuthentication","type":"book"},{"authors":null,"categories":null,"content":"Wasm 是一种可执行代码的可移植二进制格式，依赖于一个开放的标准。它允许开发人员用自己喜欢的编程语言编写，然后将代码编译成 Wasm 模块。\n   代码编译成 wasm  Wasm 模块与主机环境隔离，并在一个称为虚拟机（VM） 的内存安全沙盒中执行。Wasm 模块使用一个 API 与主机环境进行通信。\nWasm 的主要目标是在网页上实现高性能应用。例如，假设我们要用 Javascript 构建一个网页应用程序。我们可以用 Go（或其他语言）写一些，并将其编译成一个二进制文件，即 Wasm 模块。然后，我们可以在与 Javascript 网页应用程序相同的沙盒中运行已编译的 Wasm 模块。\n最初，Wasm 被设计为在网络浏览器中运行。然而，我们可以将虚拟机嵌入到其他主机应用程序中，并执行它们。这就是 Envoy 的作用！\nEnvoy 嵌入了 V8 虚拟机的一个子集。V8 是一个用 C++ 编写的高性能 JavaScript 和 WebAssembly 引擎，它被用于 Chrome 和 Node.js 等。\n我们在本课程的前面提到，Envoy 使用多线程模式运行。这意味着有一个主线程，负责处理配置更新和执行全局任务。\n除了主线程之外，还有负责代理单个 HTTP 请求和 TCP 连接的 worker 线程。这些 worker 线程被设计为相互独立。例如，处理一个 HTTP 请求的 worker 线程不会受到其他处理其他请求的 worker 线程的影响。\n   Envoy 线程  每个线程拥有自己的资源副本，包括 Wasm 虚拟机。这样做的原因是为了避免任何昂贵的跨线程同步，即实现更高的内存使用率。\nEnvoy 在运行时将每个独特的 Wasm 模块（所有 *.wasm 文件）加载到一个独特的 Wasm VM。由于 Wasm VM 不是线程安全的（即，多个线程必须同步访问一个 Wasm VM），Envoy 为每个将执行扩展的线程创建一个单独的 Wasm VM 副本。因此，每个线程可能同时有多个 Wasm VM 在使用。\nProxy-Wasm 我们将使用的 SDK 允许我们编写 Wasm 扩展，这些扩展是 HTTP 过滤器，网络过滤器，或称为 Wasm 服务的专用扩展类型。这些扩展在 Wasm 虚拟机内的 worker 线程（HTTP 过滤器，网络过滤器）或主线程（Wasm 服务）上执行。正如我们提到的，这些线程是独立的，它们本质上不知道其他线程上发生的请求处理。\nHTTP 过滤器是处理 HTTP 协议的，它对 HTTP Header、body 等进行操作。同样，网络过滤器处理 TCP 协议，对数据帧和连接进行操作。我们也可以说，这两种插件类型是无状态的。\nEnvoy 还支持有状态的场景。例如，你可以编写一个扩展，将请求数据、日志或指标等统计信息在多个请求之间进行汇总——这意味着跨越了许多 worker 线程。对于这种情况，我们会使用 Wasm 服务类型。Wasm 服务类型运行在单个虚拟机上；这个虚拟机只有一个实例，它运行在 Envoy 主线程上。你可以用它来汇总无状态过滤器的指标或日志。\n下图显示了 Wasm 服务扩展是如何在主线程上执行的，而不是 HTTP 或网络过滤器，后者是在 worker 线程上执行。\n   API  事实上，Wasm 服务扩展是在主线程上执行的，并不影响请求延迟。另一方面，网络或 HTTP 过滤器会影响延迟。\n图中显示了在主线程上运行的 Wasm 服务扩展，它使用消息队列 API 订阅队列并接收由运行在 worker 线程上的 HTTP 过滤器或网络过滤器发送的消息。然后，Wasm 服务扩展可以聚合从 worker 线程上收到的数据。\nWasm 服务扩展并不是持久化数据的唯一方法。你也可以调用 HTTP 或 gRPC API。此外，我们可以使用定时器 API 在请求之外执行行动。\n我们提到的 API、消息队列、定时器和共享数据都是由 Proxy-Wasm 提供的。\nProxy-Wasm 是一个代理无关的 ABI（应用二进制接口）标准，它规定了代理（我们的主机）和 Wasm 模块如何互动。这些互动是以函数和回调的形式实现的。\nProxy-Wasm 中的 API 与代理无关，这意味着它们可以与 Envoy 代理以及任何其他代理（例如 MOSN）一起实现 Proxy-Wasm 标准。这使得你的 Wasm 过滤器可以在不同的代理之间移植，而且它们并不局限于 Envoy。\n   Proxy-wasm  当请求进入 Envoy 时，它们会经过不同的过滤器链，被过滤器处理，在链中的某个点，请求数据会流经本地 Proxy-Wasm 扩展。\n这个扩展使用 Proxy-Wasm 接口与运行在虚拟机内的扩展通信。 过滤器处理完数据后，该链就会继续，或停止，这取决于从扩展返回的结果。\n基于 Proxy-Wasm 规范，我们可以使用一些特定语言的 SDK 实现来编写扩展。\n在其中一个实验中，我们将使用 Go SDK for Proxy-Wasm 来编写 Go 中的 Proxy-Wasm 插件。\nTinyGo 是一个用于嵌入式系统和 WebAssembly 的编译器。它不支持使用所有的标准 Go 包。例如，不支持一些标准包，如 net 和其他。\n你还可以选择使用 Assembly Script、C++、Rust 或 Zig。\n配置 Wasm 扩展 Envoy 中的通用 Wasm 扩展配置看起来像这样。\n- name:envoy.filters.http.wasmtyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/udpa.type.v1.TypedStructtype_url:type.googleapis.com/envoy.extensions.filters.http.wasm.v3.Wasmvalue:config:vm_config:vm_id:\u0026#34;my_vm\u0026#34;runtime:\u0026#34;envoy.wasm.runtime.v8\u0026#34;configuration:\u0026#34;@type\u0026#34;: type.googleapis.com/google.protobuf.StringValuevalue:\u0026#39;{\u0026#34;plugin-config\u0026#34;: \u0026#34;some-value\u0026#34;}\u0026#39;code:local:filename:\u0026#34;my-plugin.wasm\u0026#34;configuration:\u0026#34;@type\u0026#34;: type.googleapis.com/google.protobuf.StringValuevalue:\u0026#39;{\u0026#34;vm-wide-config\u0026#34;: \u0026#34;some-value\u0026#34;}\u0026#39;vm_config 字段用于指定 Wasm 虚拟机、运行时，以及我们要执行的.wasm 扩展的实际指针。\nvm_id 字段在虚拟机之间进行通信时使用。然后这个 ID 可以用来通过共享数据 API 和队列在虚拟机之间共享数据。请注意，要在多个插件中重用虚拟机，你必须使用相同的 vm_id、运行时、配置和代码。\n下一个项目是 runtime。这通常被设置为 envoy.wasm.runtime.v8。例如，如果我们用 Envoy 编译 Wasm 扩展，我们会在这里使用 null 运行时。其他选项是 Wasm micro runtime、Wasm VM 或 Wasmtime；不过，这些在官方 Envoy 构建中都没有启用。\nvm_config 字段下的配置是用来配置虚拟机本身的。除了虚拟机 ID 和运行时外，另一个重要的部分是code字段。\ncode 字段是我们引用编译后的 Wasm 扩展的地方。这可以是一个指向本地文件的指针（例如，/etc/envoy/my-plugin.wasm）或一个远程位置（例如，https://wasm.example.com/my-plugin.wasm）。\nconfiguration 文件，一个在 vm_config 下，另一个在 config 层，用于为虚拟机和插件提供配置。然后当虚拟机或插件启动时，可以从 Wasm 扩展代码中读取这些值。\n要运行一个 Wasm 服务插件，我们必须在 bootstrap_extensions 字段中定义配置，并将 singleton 布尔字段的值设置为真。\nbootstrap_extensions:- name:envoy.bootstrap.wasmtyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.wasm.3.WasmServicesingleton:trueconfig:vm_config:{ ...}开发 Wasm 扩展 - Proxy-Wasm Go SDK API 在开发 Wasm 扩展时，我们将学习上下文、hostcall API 和入口点。\n上下文 上下文是 Proxy-Wasm SDK 中的一个接口集合，并与我们前面解释的概念相匹配。\n   上下文  例如，每个虚拟机中都有一个 VMContext，可以有一个或多个 PluginContexts。这意味着我们可以在同一个虚拟机上下文中运行不同的插件（即使用同一个 vm_id 时）。每个 PluginContext 对应于一个插件实例。那就是 TcpContext（TCP 网络过滤器）或 HttpContext（HTTP 过滤器）。\nVMContext 接口定义了两个函数：OnVMStart 函数和 NewPluginContext 函数。\ntype VMContext interface { OnVMStart(vmConfigurationSize int) OnVMStartStatus NewPluginContext(contextID uint32) PluginContext } 顾名思义，OnVMStart 在虚拟机创建后被调用。在这个函数中，我们可以使用 GetVMConfiguration hostcall 检索可选的虚拟机配置。这个函数的目的是执行任何虚拟机范围的初始化。\n作为开发者，我们需要实现 NewPluginContext 函数，在该函数中我们创建一个 PluginContext 的实例。\nPluginContext 接口定义了与 VMContext 类似的功能。下面是这个接口。\ntype PluginContext interface { OnPluginStart(pluginConfigurationSize int) OnPluginStartStatus OnPluginDone() bool OnQueueReady(queueID uint32) OnTick() NewTcpContext(contextID uint32) TcpContext NewHttpContext(contextID uint32) HttpContext } OnPluginStart 函数与我们前面提到的 OnVMStart 函数类似。它在插件被创建时被调用。在这个函数中，我们也可以使用 GetPluginConfiguration  API 来检索插件的特定配置。我们还必须实现 NewTcpContext 或 NewHttpContext，在代理中响应 HTTP/TCP 流时被调用。这个上下文还包含一些其他的函数，用于设置队列（OnQueueReady）或在流处理的同时做异步任务（OnTick）。\n 参考 Proxy Wasm Go SDK Github 仓库 中的 context.go 文件，以获得最新的接口定义。\n Hostcall API 这里实现的 hostcall API ，为我们提供了与 Wasm 插件的 Envoy 代理互动的方法。\nhostcall API 定义了读取配置的方法；设置共享队列并执行队列操作；调度 HTTP 调用，从请求和响应流中检索 Header、Trailer 和正文并操作这些值；配置指标；以及更多。\n …","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"5d04c0a359b62a4f17735fd766c89720","permalink":"https://jimmysong.io/docs/envoy-handbook/extending-envoy/wasm/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/envoy-handbook/extending-envoy/wasm/","section":"envoy-handbook","summary":"Wasm 是一种可执行代码的可移植二进制格式，依赖于一个开放的标准。","tags":null,"title":"WebAssembly（Wasm）","type":"book"},{"authors":null,"categories":null,"content":"Envoy 通过查询文件或管理服务器来动态发现资源。概括地讲，对应的发现服务及其相应的 API 被称作 xDS。Envoy 通过订阅（subscription）方式来获取资源，如监控指定路径下的文件、启动 gRPC 流或轮询 REST-JSON URL。后两种方式会发送 DiscoveryRequest 请求消息，发现的对应资源则包含在响应消息 DiscoveryResponse 中。下面，我们将具体讨论每种订阅类型。\n文件订阅 发现动态资源的最简单方式就是将其保存于文件，并将路径配置在 ConfigSource 中的 path 参数中。Envoy 使用 inotify（Mac OS X 上为 kqueue）来监控文件的变化，在文件被更新时，Envoy 读取保存的 DiscoveryResponse 数据进行解析，数据格式可以为二进制 protobuf、JSON、YAML 和协议文本等。\n 译者注：core.ConfigSource 配置格式如下：\n { \u0026#34;path\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;api_config_source\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;ads\u0026#34;: \u0026#34;{...}\u0026#34; } 文件订阅方式可提供统计数据和日志信息，但是缺少 ACK/NACK 更新的机制。如果更新的配置被拒绝，xDS API 则继续使用最后一个的有效配置。\ngRPC 流式订阅 单资源类型发现 每个 xDS API 可以单独配置 ApiConfigSource，指向对应的上游管理服务器的集群地址。每个 xDS 资源类型会启动一个独立的双向 gRPC 流，可能对应不同的管理服务器。API 交付方式采用最终一致性。可以参考后续聚合服务发现（ADS） 章节来了解必要的显式控制序列。\n 译者注：core.ApiConfigSource 配置格式如下：\n { \u0026#34;api_type\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;cluster_names\u0026#34;: [], \u0026#34;grpc_services\u0026#34;: [], \u0026#34;refresh_delay\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;request_timeout\u0026#34;: \u0026#34;{...}\u0026#34; } 类型 URL 每个 xDS API 都与给定的资源的类型存在 1:1 对应。关系如下：\n LDS： envoy.api.v2.Listener RDS： envoy.api.v2.RouteConfiguration CDS： envoy.api.v2.Cluster EDS： envoy.api.v2.ClusterLoadAssignment SDS：envoy.api.v2.Auth.Secret  类型 URL 的概念如下所示，其采用 type.googleapis.com/\u0026lt;resource type\u0026gt; 的形式，例如 CDS 对应于 type.googleapis.com/envoy.api.v2.Cluster。在 Envoy 的请求和管理服务器的响应中，都包括了资源类型 URL。\nACK/NACK 和版本 每个 Envoy 流以 DiscoveryRequest 开始，包括了列表订阅的资源、订阅资源对应的类型 URL、节点标识符和空的 version_info。EDS 请求示例如下：\nversion_info:node:{id:envoy }resource_names:- foo- bartype_url:type.googleapis.com/envoy.api.v2.ClusterLoadAssignmentresponse_nonce:管理服务器可立刻或等待资源就绪时发送 DiscoveryResponse 作为响应，示例如下：\nversion_info:Xresources:- foo ClusterLoadAssignment proto encoding- bar ClusterLoadAssignment proto encodingtype_url:type.googleapis.com/envoy.api.v2.ClusterLoadAssignmentnonce:AEnvoy 在处理 DiscoveryResponse 响应后，将通过流发送一个新的请求，请求包含应用成功的最后一个版本号和管理服务器提供的 nonce。如果本次更新已成功应用，则 version_info 的值设置为 X，如下序列图所示：\n   ACK 后的版本更新  在此序列图及后续中，将统一使用以下缩写格式：\n DiscoveryRequest： (V=version_info，R=resource_names，N=response_nonce，T=type_url) DiscoveryResponse： (V=version_info，R=resources，N=nonce，T=type_url)   译者注：在信息安全中，Nonce是一个在加密通信只能使用一次的数字。在认证协议中，它往往是一个随机或伪随机数，以避免重放攻击。Nonce也用于流密码以确保安全。如果需要使用相同的密钥加密一个以上的消息，就需要Nonce来确保不同的消息与该密钥加密的密钥流不同。（引用自维基百科）在本文中nonce是每次更新的数据包的唯一标识。\n 版本为 Envoy 和管理服务器提供了共享当前应用配置的概念和通过 ACK/NACK 来进行配置更新的机制。如果 Envoy 拒绝配置更新 X，则回复 error_detail 及前一个的版本号，在当前情况下为空的初始版本号，error_detail 包含了有关错误的更加详细的信息：\n   NACK 无版本更新  后续，API 更新可能会在新版本 Y 上成功：\n   ACK 紧接着 NACK  每个流都有自己的版本概念，但不存在跨资源类型的共享版本。在不使用 ADS 的情况下，每个资源类型可能具有不同的版本，因为 Envoy API 允许指向不同的 EDS/RDS 资源配置并对应不同的 ConfigSources。\n何时发送更新 管理服务器应该只向 Envoy 客户端发送上次 DiscoveryResponse 后更新过的资源。Envoy 则会根据接受或拒绝 DiscoveryResponse 的情况，立即回复包含 ACK/NACK 的 DiscoveryRequest 请求。如果管理服务器每次发送相同的资源集结果，而不是根据其更新情况，则会导致 Envoy 和管理服务器通讯效率大打折扣。\n在同一个流中，新的 DiscoveryRequests 将取代此前具有相同的资源类型 DiscoveryRequest 请求。这意味着管理服务器只需要响应给定资源类型最新的 DiscoveryRequest 请求即可。\n资源提示 DiscoveryRequest 中的 resource_names 信息作为资源提示出现。一些资源类型，例如 Cluster 和 Listener 将使用一个空的 resource_names，因为 Envoy 需要获取管理服务器对应于节点标识的所有 Cluster（CDS）和 Listener（LDS）。对于其他资源类型，如 RouteConfigurations（RDS）和 ClusterLoadAssignments（EDS），则遵循此前的 CDS/LDS 更新，Envoy 能够明确地枚举这些资源。\nLDS/CDS 资源提示信息将始终为空，并且期望管理服务器的每个响应都提供 LDS/CDS 资源的完整状态。缺席的 Listener 或 Cluster 将被删除。\n对于 EDS/RDS，管理服务器并不需要为每个请求的资源进行响应，而且还可能提供额外未请求的资源。resource_names 只是一个提示。Envoy 将默默地忽略返回的多余资源。如果请求的资源中缺少相应的 RDS 或 EDS 更新，Envoy 将保留对应资源的最后的值。管理服务器可能会依据 DiscoveryRequest 中 node 标识推断其所需的 EDS/RDS 资源，在这种情况下，提示信息可能会被丢弃。从相应的角度来看，空的 EDS/RDS DiscoveryResponse 响应实际上是表明在 Envoy 中为一个空的资源。\n当 Listener 或 Cluster 被删除时，其对应的 EDS 和 RDS 资源也需要在 Envoy 实例中删除。为使 EDS 资源被 Envoy 已知或跟踪，就必须存在应用过的 Cluster 定义（如通过 CDS 获取）。RDS 和 Listeners 之间存在类似的关系（如通过 LDS 获取）。\n对于 EDS/RDS ，Envoy 可以为每个给定类型的资源生成不同的流（如每个 ConfigSource 都有自己的上游管理服务器的集群）或当指定资源类型的请求发送到同一个管理服务器的时候，允许将多个资源请求组合在一起发送。虽然可以单个实现，但管理服务器应具备处理每个给定资源类型中对单个或多个 resource_names 请求的能力。下面的两个序列图对于获取两个 EDS 资源都是有效的 {foo，bar}：\n   一个流上多个 EDS 请求     不同流上的多个 EDS 请求  资源更新 如上所述，Envoy 可能会更新 DiscoveryRequest 中出现的 resource_names 列表，其中 DiscoveryRequest 是用来 ACK/NACK 管理服务器的特定的 DiscoveryResponse 。此外，Envoy 后续可能会发送额外的 DiscoveryRequests ，用于在特定 version_info 上使用新的资源提示来更新管理服务器。例如，如果 Envoy 在 EDS 版本 X 时仅知道集群 foo，但在随后收到的 CDS 更新时额外获取了集群 bar ，它可能会为版本 X 发出额外的 DiscoveryRequest 请求，并将 {foo，bar} 作为请求的 resource_names 。\n   CDS 响应导致 EDS 资源更新  这里可能会出现竞争状况；如果 Envoy 在版本 X 上发布了资源提示更新请求，但在管理服务器处理该请求之前发送了新的版本号为 Y 的响应，针对 version_info 为 X 的版本，资源提示更新可能会被解释为拒绝 Y 。为避免这种情况，通过使用管理服务器提供的 nonce，Envoy 可用来保证每个 DiscoveryRequest 对应到相应的 DiscoveryResponse ：\n   EDS 更新速率激发 nonces  管理服务器不应该为含有过期 nonce 的 DiscoveryRequest 发送 DiscoveryResponse 响应。在向 Envoy 发送的 DiscoveryResponse 中包含了的新 nonce ，则此前的 nonce 将过期。在资源新版本就绪之前，管理服务器不需要向 Envoy 发送更新。同版本的早期请求将会过期。在新版本就绪时，管理服务器可能会处理同一个版本号的多个 DiscoveryRequests请求。\n   请求变的陈旧  上述资源更新序列表明 Envoy 并不能期待其发出的每个 DiscoveryRequest 都得到 DiscoveryResponse 响应。\n最终一致性考虑 由于 Envoy 的 xDS API 采用最终一致性，因此在更新期间可能导致流量被丢弃。例如，如果通过 CDS/EDS 仅获取到了集群 X，而且 RouteConfiguration 引用了集群 X；在 CDS/EDS 更新集群 Y 配置之前，如果将 RouteConfiguration 将引用的集群调整为 Y ，那么流量将被吸入黑洞而丢弃，直至集群 Y 被 Envoy 实例获取。\n对某些应用程序，可接受临时的流量丢弃，客户端重试或其他 Envoy sidecar 会掩盖流量丢弃。那些对流量丢弃不能容忍的场景，可以通过以下方式避免流量丢失，CDS/EDS 更新同时携带 X 和 Y ，然后发送 RDS 更新从 X 切 …","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"0ca31feed76db86adcc56afa51f9a9fb","permalink":"https://jimmysong.io/docs/istio-handbook/data-plane/envoy-xds-protocol/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/istio-handbook/data-plane/envoy-xds-protocol/","section":"istio-handbook","summary":"Envoy 通过查询文件或管理服务器来动态发现资源。概括地讲，对应的发","tags":null,"title":"xDS 协议解析","type":"book"},{"authors":null,"categories":null,"content":"接下来，我们将部署可观察性、分布式追踪、数据可视化工具：\nkubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.10/samples/addons/prometheus.yaml kubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.10/samples/addons/grafana.yaml kubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.10/samples/addons/kiali.yaml kubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.10/samples/addons/extras/zipkin.yaml  如果你在安装 Kiali 的时候发现以下错误 No matches for kind \u0026#34;MonitoringDashboard\u0026#34; in version \u0026#34;monitoring.kiali.io/v1alpha1\u0026#34; 请重新运行以上命令。\n 要从 Google Cloud Shell 打开仪表盘，我们可以运行 getmesh istioctl dash kiali 命令，例如，然后点击 Web Preview 按钮，选择仪表盘运行的端口（Kiali 为 20001）。如果你使用你的终端，运行 Istio CLI 命令就可以了。\n下面是 Boutique 图表在 Kiali 中的样子：\n   Boutique 应用在 Kiali 中的样子  ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"38efda69af8234bc574fca0acf70ef33","permalink":"https://jimmysong.io/docs/istio-handbook/practice/observability/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/istio-handbook/practice/observability/","section":"istio-handbook","summary":"接下来，我们将部署可观察性、分布式追踪、数据可视化工具： kubectl apply","tags":null,"title":"部署可观察性工具","type":"book"},{"authors":null,"categories":null,"content":"每当你遇到配置问题时，你可以使用这组步骤来浏览和解决问题。在第一部分，我们要检查配置是否有效。如果配置是有效的，下一步就是看看运行时是如何处理配置的，为此，你需要对 Envoy 配置有基本的了解。\n配置 1. 配置是否有效？\nIstio CLI 有一个叫 validate 的命令，我们可以用它来验证 YAML 配置。YAML 最常见的问题是缩进和数组符号相关的问题。 要验证一个配置，请将 YAML 文件传递给 validate 命令，像这样：\n$ istioctl validate -f myresource.yaml validation succeed 如果资源是无效的，CLI 会给我们一个详细的错误。例如，如果我们拼错了一个字段名：\nunknown field \u0026#34;worloadSelector\u0026#34; in v1alpha3.ServiceEntry 我们可以使用另一个命令istioctl analyze。使用这个命令，我们可以检测 Istio 配置的潜在问题。我们可以针对本地的一组配置文件或实时集群运行它。同时，寻找来自 istiod 的任何警告或错误。\n下面是该命令的一个输出样本，它捕捉到了目的地主机名称中的一个错字：\n$ istioctl analyze Error [IST0101] (VirtualService customers.default) Referenced host not found: \u0026#34;cusomers.default.svc.cluster.local\u0026#34; Error [IST0101] (VirtualService customers.default) Referenced host+subset in destinationrule not found: \u0026#34;cusomers.default.svc.cluster.local+v1\u0026#34; Error: Analyzers found issues when analyzing namespace: default. See https://istio.io/docs/reference/config/analysis for more information about causes and resolutions. 2. 命名是否正确？资源是否在正确的命名空间？\n几乎所有的 Istio 资源都是命名空间范围的。确保它们与你正在处理的服务处于同一命名空间。将 Istio 资源放在同一命名空间中尤其重要，因为选择器也是有命名空间的。\n一个常见的错误配置是在应用程序的命名空间中发布 VirtualService（例如 default），然后使用 istio：ingressgateway 选择器来绑定到 istio-system 命名空间中的 ingress 网关部署。这只有在你的 VirtualService 也在 istio-system 命名空间中时才有效。\n同样地，不要在 istio-system 命名空间中部署引用应用程序命名空间中的 VirtualService 的 Sidecar 资源。相反，为每个需要入口的应用程序部署一组 Envoy 网关。\n3. 资源选择器是否正确？\n验证部署中的 pod 是否有正确的标签设置。正如上一步提到的，资源选择器与资源发布的命名空间绑定。\n在这一点上，我们应该有理由相信，配置是正确的。接下来的步骤是进一步研究运行时系统是如何处理配置的。\n运行时 Istio CLI 的一个实验性功能可以提供信息，帮助我们了解影响 Pod 或服务的配置。下面是一个针对 Pod 运行 describe 命令的例子，这个 Pod 的主机名称中有一个错字：\n$ istioctl x describe pod customers-v1-64455cd4c6-xvjzm.default Pod: customers-v1-64455cd4c6-xvjzm Pod Ports: 3000 (svc), 15090 (istio-proxy) -------------------- Service: customers Port: http 80/HTTP targets pod port 3000 DestinationRule: customers for \u0026#34;customers.default.svc.cluster.local\u0026#34; Matching subsets: v1 No Traffic Policy VirtualService: customers WARNING: No destinations match pod subsets (checked 1 HTTP routes) Route to cusomers.default.svc.cluster.local 1. Envoy 是否接受（ACK）该配置？\n你可以使用 istioctl proxy-status 命令来检查状态，看看 Envoy 是否接受配置。我们希望所有东西的状态都设置为 SYNCHED。任何其他值都可能表明有错误，你应该检查 Pilot 的日志。\n$ istioctl proxy-status NAME CDS LDS EDS RDS ISTIOD VERSION customers-v1... SYNCED SYNCED SYNCED SYNCED istiod-67b4c76c6-8lwxf 1.9.0 customers-v1... SYNCED SYNCED SYNCED SYNCED istiod-67b4c76c6-8lwxf 1.9.0 istio-egress... SYNCED SYNCED SYNCED NOT SENT istiod-67b4c76c6-8lwxf 1.9.0 istio-ingress... SYNCED SYNCED SYNCED SYNCED istiod-67b4c76c6-8lwxf 1.9.0 web-frontend-... SYNCED SYNCED SYNCED SYNCED istiod-67b4c76c6-8lwxf 1.9.0 列表显示所有连接到 Pilot 实例的代理。如果列表中缺少一个代理，这意味着它没有连接到 Pilot，也没有收到任何配置。如果任何一个代理被标记为STALE，可能有网络问题，或者我们需要扩展 Pilot。\n如果 Envoy 接受了配置，但我们仍然看到问题，我们需要确保配置在 Envoy 中的表现符合预期。\n2. 配置在 Envoy 中的表现和预期的一样吗？\n我们可以使用 proxy-config 命令来检索特定 Envoy 实例的信息。请参考下面的表格，我们可以检索不同的代理配置。\n   命令 描述     istioctl proxy-config cluster [POD] -n [NAMESPACE] 检索 cluster 配置   istioctl proxy-config bootstrap [POD] -n [NAMESPACE] 检索 bootstrap 配置   istioctl proxy-config listener [POD] -n [NAMESPACE] 检索 listener 配置   istioctl proxy-config route [POD] -n [NAMESPACE] 检索 route 配置   istioctl proxy-config endpoints [POD] -n [NAMESPACE] 检索 endpoint 配置    该命令从 Envoy 的管理端点（主要是 /config_dump）收集数据，它包含了很多有用的信息。\n另外，请参考显示 Envoy 和 Istio 资源之间映射的图。例如，许多 VirtualService 规则将表现为 Envoy 路由，而 DestinationRules 和 ServiceEntries 则表现为 Cluster。\n DestinationRules 不会出现在配置中，除非其主机的 ServiceEntry 首先存在。\n 让我们以客户的 VirtualService 为例。\napiVersion:networking.istio.io/v1alpha3kind:VirtualServicemetadata:name:customersspec:hosts:- \u0026#39;customers.default.svc.cluster.local\u0026#39;http:- route:- destination:host:customers.default.svc.cluster.localport:number:80subset:v1weight:80- destination:host:customers.default.svc.cluster.localport:number:80subset:v2weight:20timeout:5s如果你运行istioctl proxy-config routes [POD] -o json命令，你会看到加权目的地和超时是如何在配置中体现的：\n.. { \u0026#34;name\u0026#34;: \u0026#34;80\u0026#34;, \u0026#34;virtualHosts\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;customers.default.svc.cluster.local:80\u0026#34;, \u0026#34;domains\u0026#34;: [ \u0026#34;customers.default.svc.cluster.local\u0026#34;, ... ], \u0026#34;routes\u0026#34;: [ { \u0026#34;match\u0026#34;: {\u0026#34;prefix\u0026#34;: \u0026#34;/\u0026#34;}, \u0026#34;route\u0026#34;: { \u0026#34;weightedClusters\u0026#34;: { \u0026#34;clusters\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;outbound|80|v1|customers.default.svc.cluster.local\u0026#34;, \u0026#34;weight\u0026#34;: 80 }, { \u0026#34;name\u0026#34;: \u0026#34;outbound|80|v2|customers.default.svc.cluster.local\u0026#34;, \u0026#34;weight\u0026#34;: 20 } ] }, \u0026#34;timeout\u0026#34;: \u0026#34;5s\u0026#34;, ... 当你评估 VirtualServices 时，你要寻找主机名是否像你写的那样出现在 Envoy 配置中（例如customers.default.svc.cluster.local），以及路由是否存在（见输出中的 80-20 流量分割）。你也可以使用之前的例子，通过监听器、路由和集群（和端点）来追踪调用。\nEnvoy 过滤器会表现在你告诉 Istio 把它们放在哪里（EnvoyFilter 资源中的 applyTo 字段）。通常情况下，一个坏的过滤器会表现为 Envoy 拒绝配置（即不显示 SYNCED 状态）。在这种情况下，你需要检查 Istiod 日志中的错误。\n3. Istiod（Pilot）中是否有错误？\n从 Pilot 查看错误的最快方法是跟踪日志（使用 --follow 标志），然后应用配置。下面是一个来自 Pilot 的错误的例子，这是由于过滤器的内联代码中的一个错字而导致的。\n2020-11-20T21:49:16.017487Z warn ads ADS:LDS: ACK ERROR sidecar~10.120.1.8~web-frontend-58d497b6f8-lwqkg.default~default.svc.cluster.local-4 Internal:Error adding/updating listener (s) virtualInbound: script load error: [string\u0026#34;fction envoy_on_response (response_handle)...\u0026#34;]:1: \u0026#39;=\u0026#39; expected near \u0026#39;envoy_on_response\u0026#39; 如果配置根本没有出现在 Envoy 中（Envoy …","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"750af3c2897e0a1b91d412f4161ac558","permalink":"https://jimmysong.io/docs/istio-handbook/troubleshooting/checklist/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/istio-handbook/troubleshooting/checklist/","section":"istio-handbook","summary":"每当你遇到配置问题时，你可以使用这组步骤来浏览和解决问题。在","tags":null,"title":"调试清单","type":"book"},{"authors":null,"categories":null,"content":"Envoy 中访问日志的另一个特点是可以指定过滤器，决定是否需要写入访问日志。例如，我们可以有一个访问日志过滤器，只记录 500 状态代码，只记录超过 5 秒的请求，等等。下表显示了支持的访问日志过滤器。\n   访问日志过滤器名称 描述     status_code_filter 对状态代码值进行过滤。   duration_filter 对总的请求持续时间进行过滤，单位为毫秒。   not_health_check_filter 对非健康检查请求的过滤。   traceable_filter 对可追踪的请求进行过滤。   runtime_filter 对请求进行随机抽样的过滤器。   and_filter 对过滤器列表中每个过滤器的结果进行逻辑 “和” 运算。过滤器是按顺序进行评估的。   or_filter 对过滤器列表中每个过滤器的结果进行逻辑 “或” 运算。过滤器是按顺序进行评估的。   header_filter 根据请求头的存在或值来过滤请求。   response_flag_filter 过滤那些收到设置了 Envoy 响应标志的响应的请求。   grpc_status_filter 根据响应状态过滤 gRPC 请求。   extension_filter 使用一个在运行时静态注册的扩展过滤器。   metadata_filter 基于匹配的动态元数据的过滤器。    每个过滤器都有不同的属性，我们可以选择设置。这里有一个片段，显示了如何使用状态代码、Header 和一个 and 过滤器。\n...access_log:- name:envoy.access_loggers.stdouttyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.access_loggers.stream.v3.StdoutAccessLogfilter:and_filter:filters:header_filter:header:name:\u0026#34;:method\u0026#34;string_match:exact:\u0026#34;GET\u0026#34;status_code_filter:comparison:op:GEvalue:default_value:400...上面的片段为所有响应代码大于或等于 400 的 GET 请求写了一条日志条目到标准输出。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"34c4b0c594cd263697496fe8cd67d0e0","permalink":"https://jimmysong.io/docs/envoy-handbook/logging/access-log-filtering/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/envoy-handbook/logging/access-log-filtering/","section":"envoy-handbook","summary":"Envoy 中访问日志的另一个特点是可以指定过滤器，决定是否需要写入访","tags":null,"title":"访问日志过滤","type":"book"},{"authors":null,"categories":null,"content":"我们可以使用 VirtualService 资源在 Istio 服务网格中进行流量路由。通过 VirtualService，我们可以定义流量路由规则，并在客户端试图连接到服务时应用这些规则。例如向 dev.example.com 发送一个请求，最终到达目标服务。\n让我们看一下在集群中运行 customers 应用程序的两个版本（v1 和 v2）的例子。我们有两个 Kubernetes 部署，customers-v1 和 customers-v2。属于这些部署的 Pod 有一个标签 version：v1 或一个标签 version：v2 的设置。\n   路由到 Customers  我们想把 VirtualService 配置为将流量路由到应用程序的 V1 版本。70% 的传入流量应该被路由到 V1 版本。30% 的请求应该被发送到应用程序的 V2 版本。\n下面是上述情况下 VirtualService 资源的样子：\napiVersion:networking.istio.io/v1alpha3kind:VirtualServicemetadata:name:customers-routespec:hosts:- customers.default.svc.cluster.localhttp:- name:customers-v1-routesroute:- destination:host:customers.default.svc.cluster.localsubset:v1weight:70- name:customers-v2-routesroute:- destination:host:customers.default.svc.cluster.localsubset:v2weight:30在 hosts 字段下，我们要定义流量被发送到的目标主机。在我们的例子中，这就是 customers.default.svc.cluster.local Kubernetes 服务。\n下一个字段是 http，这个字段包含一个 HTTP 流量的路由规则的有序列表。destination 是指服务注册表中的一个服务，也是路由规则处理后请求将被发送到的目的地。Istio 的服务注册表包含所有的 Kubernetes 服务，以及任何用 ServiceEntry 资源声明的服务。\n我们也在设置每个目的地的权重（weight）。权重等于发送到每个子集的流量的比例。所有权重的总和应该是 100。如果我们有一个单一的目的地，权重被假定为 100。\n通过 gateways 字段，我们还可以指定我们想要绑定这个 VirtualService 的网关名称。比如说：\napiVersion:networking.istio.io/v1alpha3kind:VirtualServicemetadata:name:customers-routespec:hosts:- customers.default.svc.cluster.localgateways:- my-gatewayhttp:...上面的 YAML 将 customers-route VirtualService 绑定到名为 my-gateway 的网关上。这有效地暴露了通过网关的目标路由。\n当一个 VirtualService 被附加到一个网关上时，只允许在网关资源中定义的主机。下表解释了网关资源中的 hosts 字段如何作为过滤器，以及 VirtualService 中的 hosts 字段如何作为匹配。\n   Gateway配置  ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"ab68fcafef2f0c1645ef453852b451d2","permalink":"https://jimmysong.io/docs/istio-handbook/traffic-management/basic-routing/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/istio-handbook/traffic-management/basic-routing/","section":"istio-handbook","summary":"我们可以使用 VirtualService 资源在 Istio 服务网格中进行流量路由。通过 Virtu","tags":null,"title":"基本路由","type":"book"},{"authors":null,"categories":null,"content":"使用控制平面来更新 Envoy 比使用文件系统的配置更复杂。我们必须创建自己的控制平面，实现发现服务接口。这里有一个 xDS 服务器实现的简单例子。这个例子显示了如何实现不同的发现服务，并运行 Envoy 连接的 gRPC 服务器的实例来检索配置。\n   Envoy 的动态配置  Envoy 方面的动态配置与文件系统的配置类似。这一次，不同的是，我们提供了实现发现服务的 gRPC 服务器的位置。我们通过静态资源指定一个集群来做到这一点。\n...dynamic_resources:lds_config:resource_api_version:V3api_config_source:api_type:GRPCtransport_api_version:V3grpc_services:- envoy_grpc:cluster_name:xds_clustercds_config:resource_api_version:V3api_config_source:api_type:GRPCtransport_api_version:V3grpc_services:- envoy_grpc:cluster_name:xds_clusterstatic_resources:clusters:- name:xds_clustertype:STATICload_assignment:cluster_name:xds_clusterendpoints:- lb_endpoints:- endpoint:address:socket_address:address:127.0.0.1port_value:9090控制平面不需要在 Envoy 概念上操作。它可以抽象出配置。它也可以使用图形用户界面或不同的 YAML、XML 或任何其他配置文件来收集用户的输入。重要的是，无论高级别配置是如何进入控制平面的，它都需要被翻译成 Envoy xDS API。\n例如，Istio 是 Envoy 代理机群的控制平面，可以通过各种自定义资源定义（VirtualService、Gateway、DestinationRule…）进行配置。除了上层配置外，在 Istio 中，Kubernetes 环境和集群内运行的服务也被用来作为生成 Envoy 配置的输入。上层配置和环境中发现的服务可以一起作为控制平面的输入。控制平面可以接受这些输入，将其转化为 Envoy 可读的配置，并通过 gRPC 将其发送给 Envoy 实例。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"023368edd59b4cf0548e11fbbd1ba66a","permalink":"https://jimmysong.io/docs/envoy-handbook/dynamic-config/control-plane/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/envoy-handbook/dynamic-config/control-plane/","section":"envoy-handbook","summary":"使用控制平面来更新 Envoy 比使用文件系统的配置更复杂。我们必须创建","tags":null,"title":"来自控制平面的动态配置","type":"book"},{"authors":null,"categories":null,"content":"授权是对访问控制问题中访问控制部分的响应。一个（经过认证的）主体是否被允许对一个对象执行动作？用户 A 能否向服务 A 的路径 /hello 发送一个 GET 请求？\n请注意，尽管主体可以被认证，但它可能不被允许执行一个动作。你的公司 ID 卡可能是有效的、真实的，但我不能用它来进入另一家公司的办公室。如果我们继续之前的海关官员的比喻，我们可以说授权类似于你护照上的签证章。\n这就引出了下一个问题 —— 有认证而无授权（反之亦然）对我们没有什么好处。对于适当的访问控制，我们需要两者。让我给你举个例子：如果我们只认证主体而不授权他们，他们就可以做任何他们想做的事，对任何对象执行任何操作。相反，如果我们授权了一个请求，但我们没有认证它，我们就可以假装成其他人，再次对任何对象执行任何操作。\nIstio 允许我们使用 AuthorizationPolicy 资源在网格、命名空间和工作负载层面定义访问控制。AuthorizationPolicy 支持 DENY、ALLOW、AUDIT 和 CUSTOM 操作。\n每个 Envoy 代理实例都运行一个授权引擎，在运行时对请求进行授权。当请求到达代理时，引擎会根据授权策略评估请求的上下文，并返回 ALLOW 或 DENY。AUDIT 动作决定是否记录符合规则的请求。注意，AUDIT 策略并不影响请求被允许或拒绝。\n没有必要明确地启用授权功能。为了执行访问控制，我们可以创建一个授权策略来应用于我们的工作负载。\nAuthorizationPolicy 资源是我们可以利用 PeerAuthentication 策略和 RequestAuthentication 策略中的主体的地方。\n在定义 AuthorizationPolicy 的时候，我们需要考虑三个部分。\n 选择要应用该策略的工作负载 要采取的行动（拒绝、允许或审计） 采取该行动的规则  让我们看看下面这个例子如何与 AuthorizationPolicy 资源中的字段相对应。\napiVersion:security.istio.io/v1beta1kind:AuthorizationPolicymetadata:name:customers-denynamespace:defaultspec:selector:matchLabels:app:customersversion:v2action:DENYrules:- from:- source:notNamespaces:[\u0026#34;default\u0026#34;]使用selector和matchLabels，我们可以选择策略所适用的工作负载。在我们的案例中，我们选择的是所有设置了app: customers和version: v2标签的工作负载。action 字段被设置为DENY。\n最后，我们在规则栏中定义所有规则。我们例子中的规则是说，当请求来自默认命名空间之外时，拒绝对 customers v2 工作负载的请求（action）。\n除了规则中的 from 字段外，我们还可以使用 to 和 when 字段进一步定制规则。让我们看一个使用这些字段的例子。\napiVersion:security.istio.io/v1beta1kind:AuthorizationPolicymetadata:name:customers-denynamespace:defaultspec:selector:matchLabels:app:customersversion:v2action:DENYrules:- from:- source:notNamespaces:[\u0026#34;default\u0026#34;]- to:- operation:methods:[\u0026#34;GET\u0026#34;]- when:- key:request.headers [User-Agent]values:[\u0026#34;Mozilla/*\u0026#34;]我们在规则部分添加了to和when字段。如果我们翻译一下上面的规则，我们可以说，当客户的 GET 请求来自default命名空间之外，并且User Agent头的值与正则表达式Mozilla/* 相匹配时，我们会拒绝 customer v2 的工作负载。\n总的来说，to 定义了策略所允许的行动，from 定义了谁可以采取这些行动，when 定义了每个请求必须具备的属性，以便被策略所允许，selector 定义了哪些工作负载将执行该策略。\n如果一个工作负载有多个策略，则首先评估拒绝的策略。评估遵循这些规则：\n 如果有与请求相匹配的 DENY 策略，则拒绝该请求 如果没有适合该工作负载的 ALLOW 策略，则允许该请求。 如果有任何 ALLOW 策略与该请求相匹配，则允许该请求。 拒绝该请求  来源 我们在上述例子中使用的源是 notNamespaces。我们还可以使用以下任何一个字段来指定请求的来源，如表中所示。\n   来源 示例 释义     principals principals: [\u0026#34;my-service-account\u0026#34;] 任何是有 my-service-account 的工作负载   notPrincipals notPrincipals: [\u0026#34;my-service-account\u0026#34;] 除了 my-service-account 的任何工作负载   requestPrincipals requestPrincipals: [\u0026#34;my-issuer/hello\u0026#34;] 任何具有有效 JWT 和请求主体 my-issuer/hello 的工作负载   notRequestPrincipals notRequestPrincipals: [\u0026#34;*\u0026#34;] 任何没有请求主体的工作负载（只有有效的 JWT 令牌）。   namespaces namespaces: [\u0026#34;default\u0026#34;] 任何来自 default 命名空间的工作负载   notNamespaces notNamespaces: [\u0026#34;prod\u0026#34;] 任何不在 prod 命名空间的工作负载   ipBlocks ipBlocks: [\u0026#34;1.2.3.4\u0026#34;,\u0026#34;9.8.7.6/15\u0026#34;] 任何具有 1.2.3.4 的 IP 地址或来自 CIDR 块的 IP 地址的工作负载   notIpBlock ipBlocks: [\u0026#34;1.2.3.4/24\u0026#34;] Any IP address that’s outside of the CIDR block    操作 操作被定义在 to 字段下，如果多于一个，则使用 AND 语义。就像来源一样，操作是成对的，有正反两面的匹配。设置在操作字段的值是字符串：\n hosts 和 notHosts ports 和 notPorts methods 和 notMethods paths 和 notPath  所有这些操作都适用于请求属性。例如，要在一个特定的请求路径上进行匹配，我们可以使用路径。[\u0026#34;/api/*\u0026#34;,\u0026#34;/admin\u0026#34;] 或特定的端口 ports: [\u0026#34;8080\u0026#34;]，以此类推。\n条件 为了指定条件，我们必须提供一个 key 字段。key 字段是一个 Istio 属性的名称。例如，request.headers、source.ip、destination.port 等等。关于支持的属性的完整列表，请参考 授权政策条件。\n条件的第二部分是 values 或 notValues 的字符串列表。下面是一个 when 条件的片段：\n...- when:- key:source.ipnotValues:[\u0026#34;10.0.1.1\u0026#34;] 下一章   ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"59a8a9c10b0ec83d8c20bdc4ef83c3aa","permalink":"https://jimmysong.io/docs/istio-handbook/security/authz/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/istio-handbook/security/authz/","section":"istio-handbook","summary":"授权是对访问控制问题中访问控制部分的响应。一个（经过认证的）","tags":null,"title":"授权","type":"book"},{"authors":null,"categories":null,"content":"管理接口的统计输出的主要端点是通过 /stats 端点访问的。这个输入通常是用来调试的。我们可以通过向 /stats 端点发送请求或从管理接口访问同一路径来访问该端点。\n该端点支持使用 filter 查询参数和正则表达式来过滤返回的统计资料。\n另一个过滤输出的维度是使用 usedonly 查询参数。当使用时，它将只输出 Envoy 更新过的统计数据。例如，至少增加过一次的计数器，至少改变过一次的仪表，以及至少增加过一次的直方图。\n默认情况下，统计信息是以 StatsD 格式写入的。每条统计信息都写在单独的一行中，统计信息的名称（例如，cluster_manager.active_clusters）后面是统计信息的值（例如，15）。\n例如：\n... cluster_manager.active_clusters。15 cluster_manager.cluster_added: 3 cluster_manager.cluster_modified:4 ... format 查询参数控制输出格式。设置为 json 将以 JSON 格式输出统计信息。如果我们想以编程方式访问和解析统计信息，通常会使用这种格式。\n第二种格式是 Prometheus 格式（例如， format=prometheus）。这个选项以 Prometheus 格式格式化状态，可以用来与 Prometheus 服务器集成。另外，我们也可以使用 /stats/prometheus 端点来获得同样的输出。\n内存 /memory 端点将输出当前内存分配和堆的使用情况，单位为字节。下面是 /stats 端点打印出来的信息的一个子集。\n$ curl localhost:9901/memory { \u0026#34;allocated\u0026#34;: \u0026#34;5845672\u0026#34;, \u0026#34;heap_size\u0026#34;: \u0026#34;10485760\u0026#34;, \u0026#34;pageheap_unmapped\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;pageheap_free\u0026#34;: \u0026#34;3186688\u0026#34;, \u0026#34;total_thread_cache\u0026#34;: \u0026#34;80064\u0026#34;, \u0026#34;total_physical_bytes\u0026#34;: \u0026#34;12699350\u0026#34; } 重置计数器 向 /reset_counters 发送一个 POST 请求，将所有计数器重置为零。注意，这不会重置或放弃任何发送到 statsd 的数据。它只影响到 /stats  端点的输出。在调试过程中可以使用 /stats 端点和 /reset_counters  端点。\n服务器信息和状态 /server_info 端点输出运行中的 Envoy 服务器的信息。这包括版本、状态、配置路径、日志级别信息、正常运行时间、节点信息等。\n该 admin.v3.ServerInfo proto 解释了由端点返回的不同字段。\n/ready 端点返回一个字符串和一个错误代码，反映 Envoy 的状态。如果 Envoy 是活的，并准备好接受连接，那么它返回 HTTP 200 和字符串 LIVE。否则，输出将是一个 HTTP 503。这个端点可以作为准备就绪检查。\n/runtime 端点以 JSON 格式输出所有运行时值。输出包括活动的运行时覆盖层列表和每个键的层值堆栈。这些值也可以通过向 /runtime_modify 端点发送 POST 请求并指定键 / 值对来修改。例如，POST /runtime_modify?my_key_1=somevalue。\n/hot_restart_version 端点，加上 --hot-restart-version  标志，可以用来确定新的二进制文件和运行中的二进制文件是否热重启兼容。\n热重启是指 Envoy 能够 “热” 或 \u0026#34; 实时 \u0026#34; 重启自己。这意味着 Envoy 可以完全重新加载自己（和配置）而不放弃任何现有的连接。\nHystrix 事件流 /hystrix_event_stream 端点的目的是作为流源用于 Hystrix 仪表盘。向该端点发送请求将触发来自 Envoy 的统计流，其格式是 Hystrix 仪表盘所期望的。\n注意，我们必须在引导配置中配置 Hystrix 统计同步，以使端点工作。\n例如：\nstats_sinks:- name:envoy.stat_sinks.hystrixtyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.config.metrics.v3.HystrixSinknum_buckets:10争用 如果启用了互斥追踪功能，/contention 端点会转储当前 Envoy 互斥内容的统计信息。\nCPU 和堆分析器 我们可以使用 /cpuprofiler 和 /heapprofiler 端点来启用或禁用 CPU / 堆分析器。注意，这需要用 gperftools 编译 Envoy。Envoy 的 GitHub 资源库有文档说明。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"a4a2a6769ada9a0f0473910ee80c65c2","permalink":"https://jimmysong.io/docs/envoy-handbook/admin-interface/statistics/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/envoy-handbook/admin-interface/statistics/","section":"envoy-handbook","summary":"管理接口的统计输出的主要端点是通过 /stats 端点访问的。这个输入通常","tags":null,"title":"统计","type":"book"},{"authors":null,"categories":null,"content":"要想说明为什么要使用服务网格，那就要从微服务架构说起，可以说服务网格很大程度上是一种新一代的微服务架构，它解决了微服务中网络层操控性、弹性、可视性的问题。\n微服务架构 开发人员经常将云原生应用程序分解为多个执行特定动作的服务。你可能有一个只处理客户的服务和另一个处理订单或付款的服务。所有这些服务都通过网络相互沟通。如果一个新的付款需要被处理，请求会被发送到付款服务。如果客户数据需要更新，请求会被发送到客户服务，等等。\n   微服务架构  这种类型的架构被称为微服务架构。这种架构有几个好处。你可以有多个较小的团队从事个别服务。这些团队可以灵活地选择他们的技术栈和语言，并且通常有独立部署和发布服务的自主权。这种机制得以运作得益于在其背后通信的网络。随着服务数量的增加，它们之间的通信和网络通信也在增加。服务和团队的数量使得监控和管理通信逻辑变得相当复杂。由于我们也知道网络是不可靠的，它们会失败，所有这些的结合使得微服务的管理和监控相当复杂。\n服务网格概述 服务网格被定义为一个专门的基础设施层，用于管理服务与服务之间的通信，使其可管理、可见、可控制。在某些版本的定义中，你可能还会听到服务网格如何使服务间的通信安全和可靠。如果我必须用一个更直接的句子来描述服务网格，我会说，服务网格是关于服务之间的通信。\n但是，服务网格是如何帮助通信的呢？让我们思考一下通信逻辑和它通常所在的地方。在大多数情况下，开发人员将这种逻辑作为服务的一部分来构建。通信逻辑是处理入站或出站请求的任何代码，重试逻辑，超时，甚至可能是流量路由。因此，无论何时服务 A 调用服务 B，请求都要经过这个通信代码逻辑，这个逻辑决定如何处理这个请求。\n   通信逻辑  我们提到，如果我们采用微服务的方法，最终可能会有大量的服务。我们如何处理所有这些服务的通信逻辑呢？我们可以创建一个包含这种逻辑的共享库，并在多个地方重用它。假设我们对所有的服务都使用相同的堆栈或编程语言，共享库的方法可能会很有效。如果我们不这样做，我们将不得不重新实现这个库，这会带来巨大的工作量而且效率低下。你也可能使用自己本身不拥有代码库的服务。在这种情况下，我们无法控制通信逻辑或监控。\n第二个问题是配置。除了配置你的应用程序外，我们还必须维护通信逻辑配置。如果我们需要同时调整或更新多个服务，我们将不得不为每个服务单独进行调整。\n服务网格所做的是，它将这种通信逻辑、重试、超时等从单个服务中分离出来，并将其移到一个单独的基础设施层。在服务网格的情况下，基础设施层是一个网络代理的阵列。这些网络代理的集合（每个服务实例旁边都有一个）处理你的服务之间的所有通信逻辑。我们称这些代理为 sideecar，因为它们与每个服务并存。\n   Sidecar 代理  以前，我们让 Customer 服务直接与 Payment 服务通信，现在我们有一个 Customer 服务旁边的代理与 Payment 服务旁边的代理通信。服务网格控制平面以这样一种方式配置代理，即它们透明地拦截所有入站和出站请求。这些代理的集合（基础设施层）形成了一个网络网格，称为服务网格。\n将通信逻辑从业务和应用逻辑中分离出来，可以使开发人员专注于业务逻辑，而服务网格运维人员则专注于服务网格配置。\n服务网格的功能 服务网格为我们提供了一种一致的方式来连接、保护和观察微服务。网格内的代理捕获了网格内所有通信的请求和指标。每一次失败、每一次成功的调用、重试或超时都可以被捕获、可视化，并发出警报。此外，可以根据请求属性做出决定。例如，我们可以检查入站（或出站）请求并编写规则，将所有具有特定头值的请求路由到不同的服务版本。\n所有这些信息和收集到的指标使得一些场景可以合理地直接实现。开发人员和运营商可以配置和执行以下方案，而不需要对服务进行任何代码修改。\n mTLS 和自动证书轮换 使用指标识别性能和可靠性问题 在 Grafana 等工具中实现指标的可视化 使用 Jaeger 或 Zipkin（需要对代码进行小的修改，以便在服务之间传播跟踪头信息） 对服务进行调试和追踪 基于权重和请求的流量路由，金丝雀部署，A/B 测试 流量镜像 通过超时和重试提高服务的弹性 通过在服务之间注入故障和延迟来进行混沌测试 检测和弹出不健康的服务实例的断路器  ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"ccfbc4878d965aad540648c9645bf9af","permalink":"https://jimmysong.io/docs/istio-handbook/intro/why-service-mesh/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/istio-handbook/intro/why-service-mesh/","section":"istio-handbook","summary":"要想说明为什么要使用服务网格，那就要从微服务架构说起，可以说","tags":null,"title":"为什么要使用服务网格？","type":"book"},{"authors":null,"categories":null,"content":"第二种类型的健康检查被称为被动健康检查。异常点检测（outlier detection） 是一种被动的健康检查形式。说它 “被动” 是因为 Envoy 没有 “主动” 发送任何请求来确定端点的健康状况。相反，Envoy 观察不同端点的性能，以确定它们是否健康。如果端点被认为是不健康的，它们就会被移除或从健康负载均衡池中弹出。\n端点的性能是通过连续失败、时间成功率、延迟等来确定的。\n为了使异常点检测发挥作用，我们需要过滤器来报告错误、超时和重置。目前，有四个过滤器支持异常点检测。HTTP 路由器、TCP 代理、Redis 代理和 Thrift 代理。\n检测到的错误根据起源点分为两类。\n1. 来自外部的错误\n这些错误是针对事务的，发生在上游服务器上，是对收到的请求的回应。这些错误是在 Envoy 成功连接到上游主机后产生的。例如，端点响应的是 HTTP 500。\n2. 本地产生的错误\nEnvoy 产生这些错误是为了应对中断或阻止与上游主机通信的事件，例如超时、TCP 重置、无法连接到指定端口等。\n这些错误也取决于过滤器的类型。例如，HTTP 路由器过滤器可以检测两种错误。相反，TCP 代理过滤器不理解 TCP 层以上的任何协议，只报告本地产生的错误。\n在配置中，我们可以指定是否可以区分本地和外部产生的错误（使用 split_external_local_origin_errors 字段）。这允许我们通过单独的计数器跟踪错误，并配置异常点检测，对本地产生的错误做出反应，而忽略外部产生的错误，反之亦然。默认模式错误将不被分割（即 split_external_local_origin_errors 为 false）。\n端点弹出 当一个端点被确定为异常点时，Envoy 将检查它是否需要从健康负载均衡池中弹出。如果没有端点被弹出，Envoy 会立即弹出异常（不健康的）端点。否则，它会检查 max_ejection_percent 设置，确保被弹出的端点数量低于配置的阈值。如果超过 max_ejection_percent 的主机已经被弹出，该端点就不会被弹出了。\n每个端点被弹出的时间是预先确定的。我们可以使用 base_ejection_time 值来配置弹出时间。这个值要乘以端点连续被弹出的次数。如果端点继续失败，它们被弹出的时间会越来越长。这里的第二个设置叫做 max_ejection_time 。它控制端点被弹出的最长时间——也就是说，端点被弹出的最长时间在 max_ejection_time 值中被指定。\nEnvoy 在 internal 字段中指定的间隔时间内检查每个端点的健康状况。每检查一次端点是否健康，弹出的倍数就会被递减。经历弹出时间后，端点会自动返回到健康的负载均衡池中。\n现在我们了解了异常点检测和端点弹出的基本知识，让我们看看不同的异常点检测方法。\n检测类型 Envoy 支持以下五种异常点检测类型。\n1. 连续的 5xx\n这种检测类型考虑到了所有产生的错误。Envoy 内部将非 HTTP 过滤器产生的任何错误映射为 HTTP 5xx 代码。\n当错误类型被分割时，该检测类型只计算外部产生的错误，忽略本地产生的错误。如果端点是一个 HTTP 服务器，只考虑 5xx 类型的错误。\n如果一个端点返回一定数量的 5xx 错误，该端点会被弹出。consecutive_5xx 值控制连续 5xx 错误的数量。\nclusters:- name:my_cluster_nameoutlier_detection:interval:5sbase_ejection_time:15smax_ejection_time:50smax_ejection_percent:30consecutive_5xx:10...上述异常点检测，一旦它失败 10 次，将弹出一个失败的端点。失败的端点会被弹出 15 秒（base_ejection_time）。在多次弹出的情况下，单个端点被弹出的最长时间是 50 秒（max_ejection_time）。在一个失败的端点被弹出之前，Envoy 会检查是否有超过 30% 的端点已经被弹出（max_ejection_percent），并决定是否弹出这个失败的端点。\n2. 连续的网关故障\n连续网关故障类型与连续 5xx 类型类似。它将 5xx 错误的一个子集，称为 “网关错误”（如 502、503 或 504 状态代码）和本地源故障，如超时、TCP 复位等。\n这种检测类型考虑了分隔模式下的网关错误，并且只由 HTTP 过滤器支持。连续错误的数量可通过 contriable_gateway_failure 字段进行配置。\nclusters:- name:my_cluster_nameoutlier_detection:interval:5sbase_ejection_time:15smax_ejection_time:50smax_ejection_percent:30consecutive_gateway_failure:10...3. 连续的本地源失败\n这种类型只在分隔模式下启用（split_external_local_origin_errors 为 true），它只考虑本地产生的错误。连续失败的数量可以通过 contriable_local_origin_failure 字段进行配置。如果未提供，默认为 5。\nclusters:- name:my_cluster_nameoutlier_detection:interval:5sbase_ejection_time:15smax_ejection_time:50smax_ejection_percent:30consecutive_local_origin_failure:10...4. 成功率\n成功率异常点检测汇总了集群中每个端点的成功率数据。基于成功率，它将在给定的时间间隔内弹出端点。在默认模式下，所有的错误都被考虑，而在分隔模式下，外部和本地产生的错误被分别处理。\n通过 success_rate_request_volume 值，我们可以设置最小请求量。如果请求量小于该字段中指定的请求量，将不计算该主机的成功率。同样地，我们可以使用 success_rate_minimum_hosts 来设置具有最小要求的请求量的端点数量。如果具有最小要求的请求量的端点数量少于 success_rate_minimum_hosts 中设置的值，Envoy 将不会进行异常点检测。\nsuccess_rate_stdev_factor 用于确定弹出阈值。弹出阈值是平均成功率和该系数与平均成功率标准差的乘积之间的差。\n平均值 - (stdev * success_rate_stdev_factor) 这个系数被除以一千，得到一个双数。也就是说，如果想要的系数是 1.9，那么运行时间值应该是 1900。\n5. 故障率\n故障率异常点检测与成功率类似。不同的是，它不依赖于整个集群的平均成功率。相反，它将该值与用户在 failure_percentage_threshold  字段中配置的阈值进行比较。如果某个主机的故障率大于或等于这个值，该主机就会被弹出。\n可以使用 failure_percentage_minimum_hosts 和 failure_percentage_request_volume 配置最小主机和请求量。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"099da0cc49c653e3166ed492bb0c4ee4","permalink":"https://jimmysong.io/docs/envoy-handbook/cluster/outlier-detection/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/envoy-handbook/cluster/outlier-detection/","section":"envoy-handbook","summary":"第二种类型的健康检查被称为被动健康检查。异常点检测（outl","tags":null,"title":"异常点检测","type":"book"},{"authors":null,"categories":null,"content":"如果你听说过服务网格，并尝试过 Istio，你可能有以下问题。\n 为什么 Istio 要在 Kubernetes 上运行？ Kubernetes 和服务网格在云原生应用架构中分别扮演什么角色？ Istio 扩展了 Kubernetes 的哪些方面？它解决了哪些问题？ Kubernetes、Envoy 和 Istio 之间是什么关系？  本文将带大家了解 Kubernetes 和 Istio 的内部工作原理。此外，我会介绍 Kubernetes 中的负载均衡方法，并解释为什么有了 Kubernetes 后还需要 Istio。\nKubernetes 本质上是通过声明式配置来实现应用生命周期管理，而服务网格本质上是提供应用间的流量、安全管理和可观察性。如果你已经使用 Kubernetes 搭建了一个稳定的应用平台，那么如何设置服务间调用的负载均衡和流量控制？是否有这样一个通用的工具或者说平台（非 SDK），可以实现？这就需要用到服务网格了。\nEnvoy 引入了 xDS 协议，这个协议得到了各种开源软件的支持，比如 Istio、MOSN 等。Envoy 将 xDS 贡献给服务网格或云原生基础设施。Envoy 本质上是一个现代版的代理，可以通过 API 进行配置，在此基础上衍生出许多不同的使用场景，比如 API Gateway、服务网格中的 sidecar 代理和边缘代理。\n本文包含以下内容。\n kube-proxy 的作用描述。 Kubernetes 在微服务管理方面的局限性。 Istio 服务网格的功能介绍。 Kubernetes、Envoy 和 Istio 服务网格中一些概念的比较。  Kubernetes vs Service Mesh 下图展示的是 Kubernetes 与 Service Mesh 中的的服务访问关系，本文仅针对 sidecar per-pod 模式，详情请参考服务网格的实现模式。\n   kubernetes vs service mesh  流量转发 Kubernetes 集群中的每个节点都部署了一个 kube-proxy 组件，该组件与 Kubernetes API Server 进行通信，获取集群中的服务信息，然后设置 iptables 规则，将服务请求直接发送到对应的 Endpoint（属于同一组服务的 pod）。\n服务发现    服务发现  Istio 可以跟踪 Kubernetes 中的服务注册，也可以在控制平面中通过平台适配器与其他服务发现系统对接；然后生成数据平面的配置（使用 CRD，这些配置存储在 etcd 中），数据平面的透明代理。数据平面的透明代理以 sidecar 容器的形式部署在每个应用服务的 pod 中，这些代理都需要请求控制平面同步代理配置。代理之所以 “透明”，是因为应用容器完全不知道代理的存在。过程中的 kube-proxy 组件也需要拦截流量，只不过 kube-proxy 拦截的是进出 Kubernetes 节点的流量，而 sidecar 代理拦截的是进出 pod 的流量。\n服务网格的劣势 由于 Kubernetes 的每个节点上都运行着很多 pod，所以在每个 pod 中放入原有的 kube-proxy 路由转发功能，会增加响应延迟——由于 sidecar 拦截流量时跳数更多，消耗更多的资源。为了对流量进行精细化管理，将增加一系列新的抽象功能。这将进一步增加用户的学习成本，但随着技术的普及，这种情况会慢慢得到缓解。\n服务网格的优势 kube-proxy 的设置是全局的，无法对每个服务进行细粒度的控制，而 service mesh 通过 sidecar proxy 的方式将 Kubernetes 中的流量控制从服务层中抽离出来–可以实现更大的弹性。\nKube-proxy 的不足之处 首先，如果转发的 pod 不能正常服务，它不会自动尝试其他 pod。每个 pod 都有一个健康检查机制，当一个 pod 出现健康问题时，kubelet 会重启 pod，kube-proxy 会删除相应的转发规则。另外，节点 Port 类型的服务不能添加 TLS 或更复杂的消息路由机制。\nKube-proxy 实现了一个 Kubernetes 服务的多个 pod 实例之间的流量负载均衡，但如何对这些服务之间的流量进行精细化控制–比如将流量按百分比划分给不同的应用版本（这些应用版本都是同一个服务的一部分，但在不同的部署上），或者做金丝雀发布（灰度发布）和蓝绿发布？\nKubernetes 社区给出了一个使用 Deployment 做金丝雀发布的方法，本质上是通过修改 pod 的标签来给部署的服务分配不同的 pod。\nKubernetes Ingress vs Istio Gateway 如上所述，kube-proxy 只能在 Kubernetes 集群内路由流量。Kubernetes 集群的 pods 位于 CNI 创建的网络中。一个 ingress—— 一个在 Kubernetes 中创建的资源对象 - 被创建用于集群外部的通信。它由位于 Kubernetes 边缘节点上的入口控制器驱动，负责管理南北向流量。Ingress 必须与各种 Ingress 控制器对接，比如 nginx ingress 控制器和 traefik。Ingress 只适用于 HTTP 流量，使用简单。它只能通过匹配有限的字段来路由流量–如服务、端口、HTTP 路径等。这使得它无法对 TCP 流量进行路由，如 MySQL、Redis 和各种 RPC。这就是为什么你会看到人们在 ingress 资源注释中写 nginx 配置语言的原因。直接路由南北流量的唯一方法是使用服务的 LoadBalancer 或 NodePort，前者需要云厂商支持，后者需要额外的端口管理。\nIstio Gateway 的功能与 Kubernetes Ingress 类似，它负责进出集群的南北流量。Istio Gateway 描述了一个负载平衡器，用于承载进出网状结构边缘的连接。该规范描述了一组开放端口和这些端口所使用的协议，以及用于负载均衡的 SNI 配置等。Gateway 是一个 CRD 扩展，它也重用了 sidecar 代理的功能；详细配置请参见 Istio 网站。\nEnvoy Envoy 是 Istio 中默认的 sidecar 代理。Istio 基于 Envoy 的 xDS 协议扩展了其控制平面。在讨论 Envoy 的 xDS 协议之前，我们需要先熟悉 Envoy 的基本术语。以下是 Envoy 中的基本术语及其数据结构的列表，更多细节请参考 Envoy 文档。\n   Envoy proxy 架构图  基本术语 下面是您应该了解的 Envoy 里的基本术语：\n Downstream（下游）：下游主机连接到 Envoy，发送请求并接收响应，即发送请求的主机。 Upstream（上游）：上游主机接收来自 Envoy 的连接和请求，并返回响应，即接受请求的主机。 Listener（监听器）：监听器是命名网地址（例如，端口、unix domain socket 等)，下游客户端可以连接这些监听器。Envoy 暴露一个或者多个监听器给下游主机连接。 Cluster（集群）：集群是指 Envoy 连接的一组逻辑相同的上游主机。Envoy 通过服务发现来发现集群的成员。可以选择通过主动健康检查来确定集群成员的健康状态。Envoy 通过负载均衡策略决定将请求路由到集群的哪个成员。  在 Envoy 中可以设置多个监听器，每个监听器可以设置一个过滤链（过滤链表），而且过滤链是可扩展的，这样我们可以更方便地操纵流量的行为–比如设置加密、私有 RPC 等。\nxDS 协议是由 Envoy 提出的，是 Istio 中默认的 sidecar 代理，但只要实现了 xDS 协议，理论上也可以作为 Istio 中的 sidecar 代理 —— 比如蚂蚁集团开源的 MOSN。\n   Envoy proxy 架构图  Istio 是一个功能非常丰富的服务网格，包括以下功能。\n 流量管理。这是 Istio 最基本的功能。 策略控制。实现访问控制系统、遥测采集、配额管理、计费等功能。 可观察性。在 sidecar 代理中实现。 安全认证。由 Citadel 组件进行密钥和证书管理。  Istio 中的流量管理 Istio 中定义了以下 CRD 来帮助用户进行流量管理。\n 网关。网关描述了一个运行在网络边缘的负载均衡器，用于接收传入或传出的 HTTP/TCP 连接。 虚拟服务（VirtualService）。VirtualService 实际上是将 Kubernetes 服务连接到 Istio 网关。它还可以执行额外的操作，例如定义一组流量路由规则，以便在主机寻址时应用。 DestinationRule。DestinationRule 定义的策略决定了流量被路由后的访问策略。简单来说，它定义了流量的路由方式。其中，这些策略可以定义为负载均衡配置、连接池大小和外部检测（用于识别和驱逐负载均衡池中不健康的主机）配置。 EnvoyFilter。EnvoyFilter 对象描述了代理服务的过滤器，可以自定义 Istio Pilot 生成的代理配置。这种配置一般很少被主用户使用。 ServiceEntry。默认情况下，Istio 服务 Mesh 中的服务无法发现 Mesh 之外的服务。ServiceEntry 可以在 Istio 内部的服务注册表中添加额外的条目，从而允许 Mesh 中自动发现的服务访问并路由到这些手动添加的服务。  Kubernetes vs xDS vs Istio 在回顾了 Kubernetes 的 kube-proxy 组件、xDS 和 Istio 对流量管理的抽象后，现在我们仅从流量管理的角度来看看这三个组件 / 协议的比较（注意，三者并不完全等同）。\n   Kubernetes xDS Istio 服务网格     Endpoint Endpoint WorkloadEntry   Service Route VirtualService   kube-proxy Route DestinationRule   kube-proxy Listener EnvoyFilter   Ingress Listener Gateway   Service Cluster ServiceEntry    核心观点  Kubernetes 的本质是应用生命周期管理，具体来说就是部署和管理（伸缩、自动恢复、发布）。 Kubernetes 为微服务提供了一个可扩展、高弹性的部署和管理平台。 服务网格是基于透明代理，通过 sidecar 代理拦截服务之间的流量，然后通过控制平面配置管理它们的行为。 服务网格将流量管理与 Kubernetes 解耦，不需要 kube-proxy 组件来支持服务网格内的流量；通过提供更接近微服务应用层的抽象来管理服务间的流量、安全性和可观察性。 xDS 是服务网格的协议标准之一。 服务网格是 Kubernetes 中服务的一个更高层次的抽象。  总结 如果说 Kubernetes 管理的对象是一个 pod，那么服务网格管理的对象就是一个服务，所以用 Kubernetes 管理微服务，然后应用服务网格就可以了。如果你连服务都不想管理，那就用 Knative 这样的无服务器平台，不过这是后话。\n 下一章   ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"e4380c9314156fe4e15905e62f8ceefa","permalink":"https://jimmysong.io/docs/istio-handbook/intro/cloud-native-application-network/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/istio-handbook/intro/cloud-native-application-network/","section":"istio-handbook","summary":"如果你听说过服务网格，并尝试过 Istio，你可能有以下问题。","tags":null,"title":"云原生应用网络","type":"book"},{"authors":null,"categories":null,"content":"Bookinfo 示例是 Istio 官方为了演示 Istio 功能而开发的一个示例应用，该应用有如下特点：\n 使用微服务方式开发，共有四个微服务 多语言应用，使用了 Java、Python、Ruby 和 NodeJs 语言 为了演示流量管理的高级功能，有的服务同时推出了多个版本  Bookinfo 应用部署架构 以下为 Istio 官方提供的该应用的架构图。\n   Istio 的 Bookinfo 示例应用架构图  Bookinfo 应用分为四个单独的微服务，其中每个微服务的部署的结构中都注入了一个 Sidecar：\n productpage ：productpage 微服务会调用 details 和 reviews 两个微服务，用来生成页面。 details ：这个微服务包含了书籍的信息。 reviews ：这个微服务包含了书籍相关的评论。它还会调用 ratings 微服务。 ratings ：ratings 微服务中包含了由书籍评价组成的评级信息。  reviews 微服务有 3 个版本：\n v1 版本不会调用 ratings 服务。 v2 版本会调用 ratings 服务，并使用 1 到 5 个黑色星形图标来显示评分信息。 v3 版本会调用 ratings 服务，并使用 1 到 5 个红色星形图标来显示评分信息。  使用 kubernetes-vagrant-centos-cluster 部署的 Kubernetes 集群和 Istio 服务的话可以直接运行下面的命令部署 Bookinfo 示例：\n$ kubectl apply -n default -f \u0026lt;(istioctl kube-inject -f yaml/istio-bookinfo/bookinfo.yaml) $ istioctl create -n default -f yaml/istio-bookinfo/bookinfo-gateway.yaml 关于该示例的介绍和详细步骤请参考 Bookinfo 应用。\nBookinfo 示例及 Istio 服务整体架构 从 Bookinfo 应用部署架构中可以看到该应用的几个微服务之间的关系，但是并没有描绘应用与 Istio 控制平面、Kubernetes 平台的关系，下图中描绘的是应用和平台整体的架构。\n   Bookinfo 示例与 Istio 的整体架构图  从图中可以看出 Istio 整体架构的特点：\n 模块化：很多模块可以选择性的开启，如负责证书管理的 istio-citadel 默认就没有启用 可定制化：可观察性的组件可以定制化和替换  参考  Bookinfo 应用 - istio.io   下一章   ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"6eea608c5e7b01ffeb136a6b537618dc","permalink":"https://jimmysong.io/docs/istio-handbook/setup/bookinfo-sample/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/istio-handbook/setup/bookinfo-sample/","section":"istio-handbook","summary":"Bookinfo 示例是 Istio 官方为了演示 Istio 功能而开发的一个示例应用，该应用有如","tags":null,"title":"Bookinfo 示例","type":"book"},{"authors":null,"categories":null,"content":"到目前为止，我们已经谈到了向 Envoy 发送请求时产生的日志。然而，Envoy 也会在启动时和执行过程中产生日志。\n我们可以在每次运行 Envoy 时看到 Envoy 组件的日志。\n... [2021-11-03 17:22:43.361][1678][info][main] [source/server/server.cc:368] initializing epoch 0 (base id=0, hot restart version=11.104) [2021-11-03 17:22:43.361][1678][info][main] [source/server/server.cc:370] statically linked extensions: [2021-11-03 17:22:43.361][1678][info][main] [source/server/server.cc:372] envoy.filters.network: envoy.client_ssl_auth, envoy.echo, envoy.ext_authz, envoy.filters.network.client_ssl_auth ... 组件日志的默认格式字符串是 [%Y-%m-%d %T.%e][%t][%l][%n] [%g:%#] %v。格式字符串的第一部分代表日期和时间，然后是线程 ID（%t）、消息的日志级别（%l）、记录器名称（%n）、源文件的相对路径和行号（%g:%#），以及实际的日志消息（%v）。\n在启动 Envoy 时，我们可以使用 --log-format 命令行选项来定制格式。例如，如果我们想记录时间记录器名称、源函数名称和日志信息，那么我们可以这样写格式字符串：[%T.%e][%n][%！] %v。\n然后，在启动 Envoy 时，我们可以设置格式字符串，如下所示。\nfunc-e run -c someconfig.yaml --log-format \u0026#39;[%T.%e][%n][%!] %v\u0026#39; 如果我们使用格式字符串，日志条目看起来像这样：\n[17:43:15.963][main][initialize] response trailer map: 160 bytes: grpc-message,grpc-status [17:43:15.965][main][createRuntime] runtime: {} [17:43:15.965][main][initialize] No admin address given, so no admin HTTP server started. [17:43:15.966][config][initializeTracers] loading tracing configuration [17:43:15.966][config][initialize] loading 0 static secret(s) [17:43:15.966][config][initialize] loading 0 cluster(s) [17:43:15.966][config][initialize] loading 1 listener(s) [17:43:15.969][config][initializeStatsConfig] loading stats configuration [17:43:15.969][runtime][onRtdsReady] RTDS has finished initialization [17:43:15.969][upstream][maybeFinishInitialize] cm init: all clusters initialized [17:43:15.969][main][onRuntimeReady] there is no configured limit to the number of allowed active connections. Set a limit via the runtime key overload.global_downstream_max_connections [17:43:15.970][main][operator()] all clusters initialized. initializing init manager [17:43:15.970][config][startWorkers] all dependencies initialized. starting workers [17:43:15.971][main][run] starting main dispatch loop Envoy 具有多个日志记录器，对于每个日志记录器（例如 main、config、http...），我们可以控制日志记录级别（info、debug、trace）。如果我们启用 Envoy 管理界面并向 /logging 路径发送请求，就可以查看所有活动的日志记录器的名称。另一种查看所有可用日志的方法是通过源码。\n下面是 /logging 终端的默认输出的样子。\nactive loggers: admin: info alternate_protocols_cache: info aws: info assert: info backtrace: info cache_filter: info client: info config: info connection: info conn_handler: info decompression: info dns: info dubbo: info envoy_bug: info ext_authz: info rocketmq: info file: info filter: info forward_proxy: info grpc: info hc: info health_checker: info http: info http2: info hystrix: info init: info io: info jwt: info kafka: info key_value_store: info lua: info main: info matcher: info misc: info mongo: info quic: info quic_stream: info pool: info rbac: info redis: info router: info runtime: info stats: info secret: info tap: info testing: info thrift: info tracing: info upstream: info udp: info wasm: info 请注意，每个日志记录器的默认日志级别都被设置为 info。其他的日志级别有以下几种:\n trace debug info warning/warn error critical off  为了配置日志级别，我们可以使用 --log-level 选项或 --component-log-level 来分别控制每个组件的日志级别。组件的日志级别可以用 log_name:log_level 格式来写。如果我们要为多个组件设置日志级别，那么就用逗号来分隔它们。例如：upstream:critical,secret:error,router:trace。\n例如，要将 main 日志级别设置为 trace，config 日志级别设置为 error，并关闭所有其他日志记录器，我们可以键入以下内容。\nfunc-e run -c someconfig.yaml --log-level off --component-log-level main:trace, config:error 默认情况下，所有 Envoy 应用程序的日志都写到标准错误（stderr）。要改变这一点，我们可以使用 --log-path 选项提供一个输出文件。\nfunc-e run -c someconfig.yaml --log-path app-logs.log 在其中一个试验中，我们还将展示如何配置 Envoy，以便将应用日志写入谷歌云操作套件（以前称为 Stackdriver）。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"16d8b51d566d1e775edf177920d76426","permalink":"https://jimmysong.io/docs/envoy-handbook/logging/envoy-component-logs/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/envoy-handbook/logging/envoy-component-logs/","section":"envoy-handbook","summary":"到目前为止，我们已经谈到了向 Envoy 发送请求时产生的日志。然而，E","tags":null,"title":"Envoy 组件日志","type":"book"},{"authors":null,"categories":null,"content":"Istio 是目前最流行的服务网格的开源实现，它的架构目前来说有两种，一种是 Sidecar 模式，这也是 Istio 最传统的部署架构，另一种是 Proxyless 模式。\n   Istio 的部署架构  Sidecar 模式 Sidecar 模式是 Istio 开源之初就在使用的模式，这种模式将应用程序的功能划分为单独的进程运行在同一个最小调度单元中，比如 Kubernetes 的 Pod 中。这种架构分为两个部分：控制平面和数据平面，控制平面是一个单体应用 Istiod，数据平面是由注入在每个 Pod 中的 Envoy 代理组成。你可以在 Sidecar 中添加更多功能，而不需要修改应用程序代码。这也是服务网格最大的一个卖点之一，将原先的应用程序 SDK 中的功能转移到了 Sidecar 中，这样开发者就可以专注于业务逻辑，而 sidecar 就交由运维来处理。\n   Sidecar 模式  我们在看下应用程序 Pod 中的结构。Pod 中包含应用容器和 Sidecar 容器，sidecar 容器与控制平面通信，获取该 Pod 上的所有代理配置，其中还有个 Init 容器，它是在注入 Sidecar 之前启动的，用来修改 Pod 的 IPtables 规则，做流量拦截的。\nProxyless 模式 Proxyless 模式是 Istio 1.11 版本中支持的实验特性，Istio 官网中有篇博客介绍了这个特性。可以直接将 gRPC 服务添加到 Istio 中，不需要再向 Pod 中注入 Envoy 代理。这样做可以极大的提升应用性能，降低网络延迟。有人说这种做法又回到了原始的基于 SDK 的微服务模式，其实非也，它依然使用了 Envoy 的 xDS API，但是因为不再需要向应用程序中注入 Sidecar 代理，因此可以减少应用程序性能的损耗。\n   Proxyless 模式  总结 Istio 虽然从诞生之初就是用的是 Sidecar 模式，但是随着网格规模的增大，大量的 Sidecar 对系统造成的瓶颈，及如 eBPF 这里网络技术的演进，未来我们有可能看到部分服务网格的功能，如透明流量劫持、证书配置等下沉到内核层，Istio 的架构也不一定会一成不变。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"cf7b9b7ef51b3c32fd84e2aa25944ea1","permalink":"https://jimmysong.io/docs/istio-handbook/concepts/istio-architecture/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/istio-handbook/concepts/istio-architecture/","section":"istio-handbook","summary":"Istio 是目前最流行的服务网格的开源实现，它的架构目前来说有两种，","tags":null,"title":"Istio 架构解析","type":"book"},{"authors":null,"categories":null,"content":"本文将概述如何配置 Istio 的开发环境及编译和生成二进制文件和 Kubernetes 的 YAML 文件，更高级的测试、格式规范、原型和参考文档编写等请参考 Istio Dev Guide。\n依赖环境 Istio 开发环境依赖以下软件：\n Docker：测试和运行时 Go 1.11：程序开发 fpm 包构建工具：用来打包 Kubernetes 1.7.3+  设置环境变量 在编译过程中需要依赖以下环境变量，请根据你自己的\nexport ISTIO=$GOPATH/src/istio.io # DockerHub 的用户名 USER=jimmysong export HUB=\u0026#34;docker.io/$USER\u0026#34; # Docker 镜像的 tag，这里为了方便指定成了固定值，也可以使用 install/updateVersion.sh 来生成 tag export TAG=$USER # GitHub 的用户名 export GITHUB_USER=rootsongjc # 指定 Kubernetes 集群的配置文件地址 export KUBECONFIG=${HOME}/.kube/config 全量编译 编译过程中需要下载很多依赖包，请确认你的机器可以科学上网。\n执行下面的命令可以编译 Istio 所有组件的二进制文件。\nmake 以在 Mac 下编译为例，编译完成后所有的二进制文件将位于 $GOPATH/out/darwin_amd64/release。\n执行下面的命令构建镜像。\nmake docker 执行下面的命令将镜像推送到 DockerHub。\nmake push 也可以编译单独组件的镜像，详见开发指南。\n构建 YAML 文件 执行下面的命令可以生成 YAML 文件。\nmake generate_yaml 生成的 YAML 文件位于 repo 根目录的 install/kubernetes 目录下。\n参考  Istio Dev Guide - github.com   下一章   ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"68f63f0c64148df3d07a359f2ea7b499","permalink":"https://jimmysong.io/docs/istio-handbook/troubleshooting/istio-dev-env/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/istio-handbook/troubleshooting/istio-dev-env/","section":"istio-handbook","summary":"本文将概述如何配置 Istio 的开发环境及编译和生成二进制文件和 Kubernetes 的 YAML","tags":null,"title":"Istio 开发环境配置","type":"book"},{"authors":null,"categories":null,"content":"用于认证的 JSON Web Token（JWT）令牌格式，由 RFC 7519 定义。参见 OAuth 2.0 和 OIDC 1.0，了解在整个认证流程中如何使用。\n示例 JWT 的规格是由 https://example.com 签发，受众要求必须是 bookstore_android.apps.example.com 或 bookstore_web.apps.example.com。该令牌应呈现在 Authorization header（默认）。Json 网络密钥集（JWKS）将按照 OpenID Connect 协议被发现。\nissuer:https://example.comaudiences:- bookstore_android.apps.example.combookstore_web.apps.example.com这个例子在非默认位置（x-goog-iap-jwt-assertion header）指定了令牌。它还定义了 URI 来明确获取 JWKS。\nissuer:https://example.comjwksUri:https://example.com/.secret/jwks.jsonjwtHeaders:- \u0026#34;x-goog-iap-jwt-assertion\u0026#34;关于 JWTRule 配置的详细用法请参考 Istio 官方文档。\n参考  JWTRule - istio.io   下一章   ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"2eb50efeeff79d05c0544e08adeedc70","permalink":"https://jimmysong.io/docs/istio-handbook/config-security/jwt/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/istio-handbook/config-security/jwt/","section":"istio-handbook","summary":"用于认证的 JSON Web Token（JWT）令牌格式，由 RFC 7519 定义。参见","tags":null,"title":"JWTRule","type":"book"},{"authors":null,"categories":null,"content":"Listener 发现服务（LDS）是一个可选的 API，Envoy 将调用它来动态获取 Listener。Envoy 将协调 API 响应，并根据需要添加、修改或删除已知的 Listener。\nListener 更新的语义如下：\n 每个 Listener 必须有一个独特的名字。如果没有提供名称，Envoy 将创建一个 UUID。要动态更新的 Listener ，管理服务必须提供 Listener 的唯一名称。 当一个 Listener 被添加，在参与连接处理之前，会先进入“预热”阶段。例如，如果 Listener 引用 RDS 配置，那么在 Listener 迁移到 “active” 之前，将会解析并提取该配置。 Listener 一旦创建，实际上就会保持不变。因此，更新 Listener 时，会创建一个全新的 Listener （使用相同的监听套接字）。新增加的 Listener 都会通过上面所描述的相同“预热”过程。 当更新或删除 Listener 时，旧的 Listener 将被置于 “draining（逐出）” 状态，就像整个服务重新启动时一样。Listener 移除之后，该 Listener 所拥有的连接，经过一段时间优雅地关闭（如果可能的话）剩余的连接。逐出时间通过 --drain-time-s 选项设置。  注意\n Envoy 从 1.9 版本开始已不再支持 v1 API。  统计 LDS 的统计树是以 listener_manager.lds 为根，统计如下：\n   名字 类型 描述     config_reload Counter 因配置不同而导致配置重新加载的总次数   update_attempt Counter 尝试调用配置加载 API 的总次数   update_success Counter 调用配置加载 API 成功的总次数   update_failure Counter 调用配置加载 API 因网络错误的失败总数   update_rejected Counter 调用配置加载 API 因 schema/验证错误的失败总次数   version Gauge 来自上次成功调用配置加载API的内容哈希   control_plane.connected_state Gauge 布尔值，用来表示与管理服务器的连接状态，1表示已连接，0表示断开连接    参考  Listener discovery service(LDS) - envoyproxy.io  ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"1f92372829ad7798a8ff18ac378a14ff","permalink":"https://jimmysong.io/docs/istio-handbook/data-plane/envoy-lds/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/istio-handbook/data-plane/envoy-lds/","section":"istio-handbook","summary":"Listener 发现服务（LDS）是一个可选的 API，Envoy 将调用它来","tags":null,"title":"LDS（监听器发现服务）","type":"book"},{"authors":null,"categories":null,"content":"目的地指的是不同的子集（subset）或服务版本。通过子集，我们可以识别应用程序的不同变体。在我们的例子中，我们有两个子集，v1 和 v2，它们对应于我们 customer 服务的两个不同版本。每个子集都使用键/值对（标签）的组合来确定哪些 Pod 要包含在子集中。我们可以在一个名为 DestinationRule 的资源类型中声明子集。\n下面是定义了两个子集的 DestinationRule 资源的样子。\napiVersion:networking.istio.io/v1alpha3kind:DestinationRulemetadata:name:customers-destinationspec:host:customers.default.svc.cluster.localsubsets:- name:v1labels:version:v1- name:v2labels:version:v2让我们看看我们可以在 DestinationRule 中设置的流量策略。\nDestinationRule 中的流量策略 通过 DestinationRule，我们可以定义设置，如负载均衡配置、连接池大小、局部异常检测等，在路由发生后应用于流量。我们可以在trafficPolicy字段下设置流量策略设置。以下是这些设置：\n 负载均衡器设置 连接池设置 局部异常点检测 客户端 TLS 设置 端口流量策略  负载均衡器设置 通过负载均衡器设置，我们可以控制目的地使用哪种负载均衡算法。下面是一个带有流量策略的 DestinationRule 的例子，它把目的地的负载均衡算法设置为 round-robin。\napiVersion:networking.istio.io/v1alpha3kind:DestinationRulemetadata:name:customers-destinationspec:host:customers.default.svc.cluster.localtrafficPolicy:loadBalancer:simple:ROUND_ROBINsubsets:- name:v1labels:version:v1- name:v2labels:version:v2我们还可以设置基于哈希的负载均衡，并根据 HTTP 头、cookies 或其他请求属性提供会话亲和性。下面是一个流量策略的片段，它设置了基于哈希的负载均衡，并使用一个叫做 location 的 cookie 来实现亲和力。\ntrafficPolicy:loadBalancer:consistentHash:httpCookie:name:locationttl:4s连接池配置 这些设置可以在 TCP 和 HTTP 层面应用于上游服务的每个主机，我们可以用它们来控制连接量。\n下面是一个片段，显示了我们如何设置对服务的并发请求的限制。\nspec:host:myredissrv.prod.svc.cluster.localtrafficPolicy:connectionPool:http:http2MaxRequests:50异常点检测 异常点检测是一个断路器的实现，它跟踪上游服务中每个主机（Pod）的状态。如果一个主机开始返回 5xx HTTP 错误，它就会在预定的时间内被从负载均衡池中弹出。对于 TCP 服务，Envoy 将连接超时或失败计算为错误。\n下面是一个例子，它设置了 500 个并发的 HTTP2 请求（http2MaxRequests）的限制，每个连接不超过 10 个请求（maxRequestsPerConnection）到该服务。每 5 分钟扫描一次上游主机（Pod）（interval），如果其中任何一个主机连续失败 10 次（contracticalErrors），Envoy 会将其弹出 10 分钟（baseEjectionTime）。\ntrafficPolicy:connectionPool:http:http2MaxRequests:500maxRequestsPerConnection:10outlierDetection:consecutiveErrors:10interval:5mbaseEjectionTime:10m客户端 TLS 设置 包含任何与上游服务连接的 TLS 相关设置。下面是一个使用提供的证书配置 mTLS 的例子。\ntrafficPolicy:tls:mode:MUTUALclientCertificate:/etc/certs/cert.pemprivateKey:/etc/certs/key.pemcaCertificates:/etc/certs/ca.pem其他支持的 TLS 模式有 DISABLE（没有 TLS 连接），SIMPLE（在上游端点发起 TLS 连接），以及 ISTIO_MUTUAL（与 MUTUAL 类似，使用 Istio 的 mTLS 证书）。\n端口流量策略 使用 portLevelSettings 字段，我们可以将流量策略应用于单个端口。比如说：\ntrafficPolicy:portLevelSettings:- port:number:80loadBalancer:simple:LEAST_CONN- port:number:8000loadBalancer:simple:ROUND_ROBIN","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"ca01293c33e8067146ef161e31917973","permalink":"https://jimmysong.io/docs/istio-handbook/traffic-management/subset-and-destinationrule/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/istio-handbook/traffic-management/subset-and-destinationrule/","section":"istio-handbook","summary":"目的地指的是不同的子集（subset）或服务版本。通过子集，","tags":null,"title":"Subset 和 DestinationRule","type":"book"},{"authors":null,"categories":null,"content":"WorkloadEntry 使运维能够描述单个非 Kubernetes 工作负载的属性，如虚拟机或裸机服务器，以加入网格。WorkloadEntry 必须同时配置 Istio ServiceEntry，通过适当的标签选择工作负载，并提供 MESH_INTERNAL 服务的服务定义（主机名、端口属性等）。ServiceEntry 对象可以根据服务条目中指定的标签选择器来选择多个工作负载条目以及 Kubernetes pod。\n当工作负载连接到 istiod 时，自定义资源中的状态字段将被更新，以表明工作负载的健康状况以及其他细节，类似于 Kubernetes 更新 pod 状态的方式。\n示例 下面的例子声明了一个 WorkloadEntry 条目，代表 details.bookinfo.com 服务的一个虚拟机。这个虚拟机安装了 sidecar，并使用 details-legacy 服务账户进行引导。该服务通过 80 端口暴露给网格中的应用程序。通往该服务的 HTTP 流量被 Istio mTLS 封装，并被发送到目标端口 8080 的虚拟机上的 sidecar，后者又将其转发到同一端口的 localhost 上的应用程序。\napiVersion:networking.istio.io/v1alpha3kind:WorkloadEntrymetadata:name:details-svcspec:# 使用服务账户表明工作负载有一个用该服务账户引导的 sidecar 代理。有 sidecar 的 pod 会自动使用 istio mTLS 与工作负载通信。serviceAccount:details-legacyaddress:2.2.2.2labels:app:details-legacyinstance-id:vm1与其相关的 ServiceEntry 如下。\napiVersion:networking.istio.io/v1alpha3kind:ServiceEntrymetadata:name:details-svcspec:hosts:- details.bookinfo.comlocation:MESH_INTERNALports:- number:80name:httpprotocol:HTTPtargetPort:8080resolution:STATICworkloadSelector:labels:app:details-legacy下面的例子使用其完全限定的 DNS 名称声明了同一个虚拟机工作负载。ServiceEntry 的解析模式应改为 DNS，以表明客户端侧设备在转发请求之前应在运行时动态地解析 DNS 名称。\napiVersion:networking.istio.io/v1alpha3kind:WorkloadEntrymetadata:name:details-svcspec:# 使用服务账户表明工作负载有一个用该服务账户引导的 sidecar 代理。有 sidecar 的 pod 会自动使用 istio mTLS 与工作负载通信。serviceAccount:details-legacyaddress:vm1.vpc01.corp.netlabels:app:details-legacyinstance-id:vm1与其相关的 ServiceEntry 如下。\napiVersion:networking.istio.io/v1alpha3kind:ServiceEntrymetadata:name:details-svcspec:hosts:- details.bookinfo.comlocation:MESH_INTERNALports:- number:80name:httpprotocol:HTTPtargetPort:8080resolution:DNSworkloadSelector:labels:app:details-legacy配置项 下图是 WorkloadEntry 资源的配置拓扑图。\n  WorkloadEntry 资源配置拓扑图  WorkloadEntry 资源的顶级配置项如下：\n  address：字符串类型。与网络端点相关的地址，不含端口。当且仅当解析被设置为 DNS 时，可以使用域名，并且必须是完全限定的，没有通配符。对于 Unix 域套接字端点，使用 unix://absolute/path/to/socket 格式。\n  ports：与端点相关的端口集。如果指定了端口映射，必须是servicePortName 到这个端点的端口映射，这样，到服务端口的流量将被转发到映射到服务的 portName 的端点端口。如果省略，而目标端口被指定为服务的端口规范的一部分，那么到服务端口的流量将被转发到指定的目标端口上的一个端点。如果没有指定targetPort和端点的端口映射，到服务端口的流量将被转发到同一端口的端点上。\n注意1：不要用于unix://地址。\n注意2：端点的端口映射优先于 targetPort。\n  labels：与端点相关的一或多个标签。\n  network：字符串类型。network 使Istio能够将位于同一L3域/网络中的端点分组。同一网络中的所有端点都被认为是可以相互直连的。当不同网络中的端点不能直连时，可以使用Istio Gateway来建立连接（通常在Gateway服务器中使用AUTO_PASSTHROUGH模式）。这是一个高级配置，通常用于在多个集群上跨越Istio网格。\n  locality：字符串类型。与端点相关的位置。locality 对应于一个故障域（例如，国家/地区/区域）。例如，在US的一个端点，在US-East-1地区，在可用区az-1内，在数据中心机架r11内，其位置可以表示为us/us-east-1/az-1/r11。Istio将配置sidecar，使其路由到与sidecar相同地域的端点。如果该地区的端点都不可用，将选择父地区的端点（但在同一网络ID内）。例如，如果有两个端点在同一个网络中（networkID n1），比如e1的位置是us/us-east-1/az-1/r11，e2的位置是us/us-east-1/az-2/r12，来自us/us-east-1/az-1/r11位置的sidecar会选择同一位置的e1而不是来自不同位置的e2。端点e2可以是与网关（连接网络n1和n2）相关的IP，或与标准服务端点相关的IP。\n  weight：unit32 类型。与端点相关的负载均衡权重。权重较高的端点将按比例获得较高的流量。\n  serviceAccount：字符串类型。如果工作负载中存在 sidecar，则为与工作负载相关的服务账户。该服务账户必须与配置（WorkloadEntry 或 ServiceEntry）存在于同一命名空间。\n  关于 WorkloadEntry 配置的详细用法请参考 Istio 官方文档。\n参考  WorkloadEntry - istio.io  ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"52366ae0b614527ac9b8bcccaefeee9b","permalink":"https://jimmysong.io/docs/istio-handbook/config-networking/workload-entry/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/istio-handbook/config-networking/workload-entry/","section":"istio-handbook","summary":"WorkloadEntry 使运维能够描述单个非 Kubernetes 工作负载的属性，如虚拟机或裸机服务器","tags":null,"title":"WorkloadEntry","type":"book"},{"authors":null,"categories":null,"content":"分布式追踪是一种监测微服务应用程序的方法。使用分布式追踪，我们可以在请求通过被监控系统的不同部分时追踪它们。\n每当一个请求进入服务网格时，Envoy 都会生成一个唯一的请求 ID 和追踪信息，并将其作为 HTTP 头的一部分来存储。任何应用程序都可以将这些头信息转发给它所调用的其他服务，以便在系统中创建一个完整的追踪。\n分布式追踪是一个跨度（span）的集合。当请求流经不同的系统组件时，每个组件都会贡献一个跨度。每个跨度都有一个名称，开始和结束的时间戳，一组称为标签（tag）和日志（log）的键值对，以及一个跨度上下文。\n标签被应用于整个跨度，并用于查询和过滤。下面是我们在使用 Zipkin 时将看到的几个标签的例子。注意，其中有些是通用的，有些是 Istio 特有的。\n istio.mesh_id istio.canonical_service upstream_cluster http.url http.status_code zone  单个跨度与识别跨度、父跨度、追踪 ID 的上下文头一起被发送到一个叫做采集器的组件。采集器对数据进行验证、索引和存储。\n当请求流经 Envoy 代理时，Envoy 代理会自动发送各个跨度。请注意，Envoy 只能在边缘收集跨度。我们要负责在每个应用程序中生成任何额外的跨度，并确保我们在调用其他服务时转发追踪头信息。这样一来，各个跨度就可以正确地关联到一个单一的追踪中。\n使用 Zipkin 进行分布式追踪 Zipkin 是一个分布式跟踪系统。我们可以轻松地监控服务网格中发生的分布式事务，发现任何性能或延迟问题。\n为了让我们的服务参与分布式跟踪，我们需要在进行任何下游服务调用时传播服务的 HTTP 头信息。尽管所有的请求都要经过 Istio sidecar，但 Istio 没有办法将出站请求与产生这些请求的入站请求联系起来。通过在应用程序中传播相关的头信息可以帮助 Zipkin 将这些跟踪信息拼接起来。\nIstio 依赖于 B3 跟踪头（以 x-b3 开头的 header）和 Envoy 生成的请求 ID（x-request-id）。B3 头信息用于跨服务边界的跟踪上下文传播。\n以下是我们需要在我们的应用程序中对每个发出的请求进行传播的特定头文件名称：\nx-request-id x-b3-traceid x-b3-spanid x-b3-parentspanid x-b3-sampled x-b3-flags b3  如果你使用 Lightstep，你还需要转发名为 x-ot-span-context 的头。\n 传播头信息最常见的方法是从传入的请求中复制它们，并将它们包含在所有从你的应用程序发出的请求中。\n你用 Istio 服务网格得到的跟踪只在服务边界捕获。为了了解应用程序的行为并排除故障，你需要通过创建额外的跨度（span）来正确检测你的应用程序。\n要安装 Zipkin，我们可以使用 addons 文件夹中的 zipkin.yaml 文件。\n$ kubectl apply -f istio-1.9.0/samples/addons/extras/zipkin.yaml deployment.apps/zipkin created service/tracing created service/zipkin created 我们可以通过运行 getmesh istioctl dashboard zipkin 来打开 Zipkin 仪表板。在用户界面上，我们可以选择跟踪查询的标准。点击按钮，从下拉菜单中选择 serviceName，然后选择 customers.default service，点击搜索按钮（或按回车键），就可以搜索到 trace 信息。\n   Zipkin Dashboard  我们可以点击个别 trace 来深入挖掘不同的跨度。详细的视图将显示服务之间的调用时间，以及请求的细节，如方法、协议、状态码等。由于我们只有一个服务在运行（Nginx），所以你不会看到很多细节。稍后，我们将回到 Zipkin，更详细地探索这些 trace。\n   Zipkin trace 详情  ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"c4dab0282d1ea91612ab1a87f0c9f89f","permalink":"https://jimmysong.io/docs/istio-handbook/observability/zipkin/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/istio-handbook/observability/zipkin/","section":"istio-handbook","summary":"分布式追踪是一种监测微服务应用程序的方法。使用分布式追踪，我","tags":null,"title":"Zipkin","type":"book"},{"authors":null,"categories":null,"content":"断路是一种重要的模式，可以帮助服务的弹性。断路模式通过控制和管理对故障服务的访问来防止额外的故障。它允许我们快速失败，并尽快向下游反馈。\n让我们看一个定义断路的片段。\n...clusters:- name:my_cluster_name...circuit_breakers:thresholds:- priority:DEFAULTmax_connections:1000- priority:HIGHmax_requests:2000...我们可以为每条路由的优先级分别配置断路器的阈值。例如，较高优先级的路由应该有比默认优先级更高的阈值。如果超过了任何阈值，断路器就会断开，下游主机就会收到 HTTP 503 响应。\n我们可以用多种选项来配置断路器。\n1. 最大连接数（max_connections）\n指定 Envoy 与集群中所有端点的最大连接数。如果超过这个数字，断路器会断开，并增加集群的 upstream_cx_overflow 指标。默认值是 1024。\n2. 最大的排队请求（max_pending_requests）\n指定在等待就绪的连接池连接时被排队的最大请求数。当超过该阈值时，Envoy 会增加集群的 upstream_rq_pending_overflow 统计。默认值是 1024。\n3. 最大请求（max_requests）\n指定 Envoy 向集群中所有端点发出的最大并行请求数。默认值是 1024。\n4. 最大重试（max_retries）\n指定 Envoy 允许给集群中所有终端的最大并行重试次数。默认值是 3，如果这个断路器溢出，upstream_rq_retry_overflow 计数器就会递增。\n另外，我们可以将断路器与重试预算（retry_budget）相结合。通过指定重试预算，我们可以将并发重试限制在活动请求的数量上。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"a4b26c01a24bc115807b636c61d3c6de","permalink":"https://jimmysong.io/docs/envoy-handbook/cluster/circuit-breakers/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/envoy-handbook/cluster/circuit-breakers/","section":"envoy-handbook","summary":"断路是一种重要的模式，可以帮助服务的弹性。断路模式通过控制和","tags":null,"title":"断路器","type":"book"},{"authors":null,"categories":null,"content":"我们已经建立了一个新的 Docker 镜像，它使用了与当前运行的前端服务不同的标头。让我们看看如何部署所需的资源并将一定比例的流量路由到不同的前端服务版本。\n在我们创建任何资源之前，让我们删除现有的前端部署（kubectl delete deploy frontend），并创建一个版本标签设置为 original 。\napiVersion:apps/v1kind:Deploymentmetadata:name:frontendspec:selector:matchLabels:app:frontendversion:originaltemplate:metadata:labels:app:frontendversion:originalannotations:sidecar.istio.io/rewriteAppHTTPProbers:\u0026#34;true\u0026#34;spec:containers:- name:serverimage:gcr.io/google-samples/microservices-demo/frontend:v0.2.1ports:- containerPort:8080readinessProbe:initialDelaySeconds:10httpGet:path:\u0026#34;/_healthz\u0026#34;port:8080httpHeaders:- name:\u0026#34;Cookie\u0026#34;value:\u0026#34;shop_session-id=x-readiness-probe\u0026#34;livenessProbe:initialDelaySeconds:10httpGet:path:\u0026#34;/_healthz\u0026#34;port:8080httpHeaders:- name:\u0026#34;Cookie\u0026#34;value:\u0026#34;shop_session-id=x-liveness-probe\u0026#34;env:- name:PORTvalue:\u0026#34;8080\u0026#34;- name:PRODUCT_CATALOG_SERVICE_ADDRvalue:\u0026#34;productcatalogservice:3550\u0026#34;- name:CURRENCY_SERVICE_ADDRvalue:\u0026#34;currencyservice:7000\u0026#34;- name:CART_SERVICE_ADDRvalue:\u0026#34;cartservice:7070\u0026#34;- name:RECOMMENDATION_SERVICE_ADDRvalue:\u0026#34;recommendationservice:8080\u0026#34;- name:SHIPPING_SERVICE_ADDRvalue:\u0026#34;shippingservice:50051\u0026#34;- name:CHECKOUT_SERVICE_ADDRvalue:\u0026#34;checkoutservice:5050\u0026#34;- name:AD_SERVICE_ADDRvalue:\u0026#34;adservice:9555\u0026#34;- name:ENV_PLATFORMvalue:\u0026#34;gcp\u0026#34;resources:requests:cpu:100mmemory:64Milimits:cpu:200mmemory:128Mi将以上文件存储为 frontend-original.yaml 并使用 kubectl apply -f frontend-original.yaml 命令创建部署。\n现在我们准备创建一个 DestinationRule，定义两个版本的前端——现有的（original）和新的（v1）。\napiVersion:networking.istio.io/v1alpha3kind:DestinationRulemetadata:name:frontendspec:host:frontend.default.svc.cluster.localsubsets:- name:originallabels:version:original- name:v1labels:version:1.0.0将上述 YAML 保存为 frontend-dr.yaml，并使用 kubectl apply -f frontend-dr.yaml 创建它。\n接下来，我们将更新 VirtualService，并指定将所有流量路由到的子集。在这种情况下，我们将把所有流量路由到原始版本的前端。\napiVersion:networking.istio.io/v1alpha3kind:VirtualServicemetadata:name:frontend-ingressspec:hosts:- \u0026#39;*\u0026#39;gateways:- frontend-gatewayhttp:- route:- destination:host:frontendport:number:80subset:original将上述 YAML 保存为 frontend-vs.yaml，并使用 kubectl apply -f frontend-vs.yaml 更新 VirtualService 资源。\n现在我们将 VirtualService 配置为将所有进入的流量路由到 original 子集，我们可以安全地创建新的前端部署。\napiVersion:apps/v1kind:Deploymentmetadata:name:frontend-v1spec:selector:matchLabels:app:frontendversion:1.0.0template:metadata:labels:app:frontendversion:1.0.0annotations:sidecar.istio.io/rewriteAppHTTPProbers:\u0026#34;true\u0026#34;spec:containers:- name:serverimage:gcr.io/tetratelabs/boutique-frontend:1.0.0ports:- containerPort:8080readinessProbe:initialDelaySeconds:10httpGet:path:\u0026#34;/_healthz\u0026#34;port:8080httpHeaders:- name:\u0026#34;Cookie\u0026#34;value:\u0026#34;shop_session-id=x-readiness-probe\u0026#34;livenessProbe:initialDelaySeconds:10httpGet:path:\u0026#34;/_healthz\u0026#34;port:8080httpHeaders:- name:\u0026#34;Cookie\u0026#34;value:\u0026#34;shop_session-id=x-liveness-probe\u0026#34;env:- name:PORTvalue:\u0026#34;8080\u0026#34;- name:PRODUCT_CATALOG_SERVICE_ADDRvalue:\u0026#34;productcatalogservice:3550\u0026#34;- name:CURRENCY_SERVICE_ADDRvalue:\u0026#34;currencyservice:7000\u0026#34;- name:CART_SERVICE_ADDRvalue:\u0026#34;cartservice:7070\u0026#34;- name:RECOMMENDATION_SERVICE_ADDRvalue:\u0026#34;recommendationservice:8080\u0026#34;- name:SHIPPING_SERVICE_ADDRvalue:\u0026#34;shippingservice:50051\u0026#34;- name:CHECKOUT_SERVICE_ADDRvalue:\u0026#34;checkoutservice:5050\u0026#34;- name:AD_SERVICE_ADDRvalue:\u0026#34;adservice:9555\u0026#34;- name:ENV_PLATFORMvalue:\u0026#34;gcp\u0026#34;resources:requests:cpu:100mmemory:64Milimits:cpu:200mmemory:128Mi将上述 YAML 保存为 frontend-v1.yaml，然后用 kubectl apply -f frontend-v1.yaml 创建它。\n如果我们在浏览器中打开 INGRESS_HOST，我们仍然会看到原始版本的前端。让我们更新 VirtualService 中的权重，开始将 30% 的流量路由到 v1 的子集。\napiVersion:networking.istio.io/v1alpha3kind:VirtualServicemetadata:name:frontend-ingressspec:hosts:- \u0026#39;*\u0026#39;gateways:- frontend-gatewayhttp:- route:- destination:host:frontendport:number:80subset:originalweight:70- destination:host:frontendport:number:80subset:v1weight:30将上述 YAML 保存为 frontend-30.yaml，然后用 kubectl apply -f frontend-30.yaml 更新 VirtualService。\n如果我们刷新几次网页，我们会注意到来自前端 v1 的更新标头，看起来像下图中的样子。\n   更新后的Header  如果我们打开 Kiali 并点击 Graph，我们会发现有两个版本的前端在运行，如下图所示。\n   在 Kiali 中有两个版本  ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"98434b85c0301170da9e09e4f6b67a12","permalink":"https://jimmysong.io/docs/istio-handbook/practice/traffic-routing/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/istio-handbook/practice/traffic-routing/","section":"istio-handbook","summary":"我们已经建立了一个新的 Docker 镜像，它使用了与当前运行的前端服务不","tags":null,"title":"路由流量","type":"book"},{"authors":null,"categories":null,"content":"/logging 端点启用或禁用特定组件或所有记录器的不同日志级别。\n要列出所有的记录器，我们可以向 /logging 端点发送一个 POST 请求。\n$ curl -X POST localhost:9901/logging active loggers: admin: info alternate_protocols_cache: info aws: info assert: info backtrace: info cache_filter: info client: info config: info ... 输出将包含记录器的名称和每个记录器的日志级别。要改变所有活动日志记录器的日志级别，我们可以使用 level 参数。例如，我们可以运行下面的程序，将所有日志记录器的日志记录级别改为 debug。\n$ curl -X POST localhost:9901/logging?level=debug active loggers: admin: debug alternate_protocols_cache: debug aws: debug assert: debug backtrace: debug cache_filter: debug client: debug config: debug ... 要改变某个日志记录器的级别，我们可以用日志记录器的名称替换 level 查询参数名称。例如，要将 admin 日志记录器级别改为 warning，我们可以运行以下程序。\n$ curl -X POST localhost:9901/logging?admin=warning active loggers: admin: warning alternate_protocols_cache: info aws: info assert: info backtrace: info cache_filter: info client: info config: info 为了触发所有访问日志的重新开放，我们可以向 /reopen_logs 端点发送一个 POST 请求。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"0e2e0bc1c2e44127b133a36c180cc526","permalink":"https://jimmysong.io/docs/envoy-handbook/admin-interface/logging/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/envoy-handbook/admin-interface/logging/","section":"envoy-handbook","summary":"/logging 端点启用或禁用特定组件或所有记录器的不同日志级别。 要列出所","tags":null,"title":"日志","type":"book"},{"authors":null,"categories":null,"content":"我们将在这个实验中创建一个动态的 Envoy 配置，并通过单独的配置文件配置监听器、集群、路由和端点。\n让我们从最小的 Envoy 配置开始。\nnode:cluster:cluster-1id:envoy-instance-1dynamic_resources:lds_config:path:./lds.yamlcds_config:path:./cds.yamladmin:address:socket_address:address:127.0.0.1port_value:9901access_log:- name:envoy.access_loggers.filetyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.access_loggers.stream.v3.StdoutAccessLog将上述 YAML 保存到 envoy-proxy-1.yaml 文件。我们还需要创建空的（暂时的）cds.yaml 和 lds.yaml 文件。\ntouch {cds,lds}.yaml 我们现在可以用这个配置来运行 Envoy 代理：func-e run -c envoy-proxy-1.yaml。如果我们看一下生成的配置（比如 localhost:9901/config_dump），我们会发现它是空的，因为我们没有提供任何监听器或集群。\n接下来让我们创建监听器和路由配置。\nversion_info:\u0026#34;0\u0026#34;resources:- \u0026#34;@type\u0026#34;: type.googleapis.com/envoy.config.listener.v3.Listenername:listener_0address:socket_address:address:0.0.0.0port_value:10000filter_chains:- filters:- name:envoy.filters.network.http_connection_managertyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManagerstat_prefix:listener_httphttp_filters:- name:envoy.filters.http.routerrds:route_config_name:route_config_1config_source:path:./rds.yaml创建一个空的 rds.yaml 文件（touch rds.yaml），并将上述 YAML 保存为 lds.yaml。因为 Envoy 只关注文件路径的移动，所以保存文件不会触发配置重载。为了触发重载，让我们覆盖 lds.yaml 文件。\nmv lds.yaml tmp; mv tmp lds.yaml 上述命令触发了重载，我们应该从 Envoy 得到以下日志条目：\n[2021-09-07 19:04:06.710][2113][info][upstream] [source/server/lds_api.cc:78] lds: add/update listener \u0026#39;listener_0\u0026#39; 同样，如果我们向 localhost:10000 发送请求，我们会得到一个 HTTP 404。\n接下来让我们创建 rds.yaml 的内容。\nversion_info:\u0026#34;0\u0026#34;resources:- \u0026#34;@type\u0026#34;: type.googleapis.com/envoy.config.route.v3.RouteConfigurationname:route_config_1virtual_hosts:- name:vhdomains:[\u0026#34;*\u0026#34;]routes:- match:prefix:\u0026#34;/headers\u0026#34;route:cluster:instance_1强制重载：\nmv rds.yaml tmp; mv tmp rds.yaml 最后，我们还需要对集群进行配置。在这之前，让我们运行一个 httpbin 容器。\ndocker run -d -p 5050:80 kennethreitz/httpbin 现在我们更新集群（cds.yaml）并强制重载（mv cds.yaml tmp; mv tmp cds.yaml）：\nversion_info:\u0026#34;0\u0026#34;resources:- \u0026#34;@type\u0026#34;: type.googleapis.com/envoy.config.cluster.v3.Clustername:instance_1connect_timeout:5sload_assignment:cluster_name:instance_1endpoints:- lb_endpoints:- endpoint:address:socket_address:address:127.0.0.1port_value:5050当 Envoy 更新配置时，我们会得到以下日志条目：\n$ [2021-09-07 19:09:15.582][2113][info][upstream] [source/common/upstream/cds_api_helper.cc:65] cds: added/updated 1 cluster(s), skipped 0 unmodified cluster(s) 现在我们可以提出请求并验证流量是否到达集群中定义的端点。\n$ curl localhost:10000/headers { \u0026#34;headers\u0026#34;: { \u0026#34;Accept\u0026#34;: \u0026#34;*/*\u0026#34;, \u0026#34;Host\u0026#34;: \u0026#34;localhost:10000\u0026#34;, \u0026#34;User-Agent\u0026#34;: \u0026#34;curl/7.64.0\u0026#34;, \u0026#34;X-Envoy-Expected-Rq-Timeout-Ms\u0026#34;: \u0026#34;15000\u0026#34; } } 注意我们是如何将端点与集群配置在同一个文件中的。我们可以通过单独定义端点（eds.yaml）将两者分开。\n让我们从创建 eds.yaml 文件开始：\nversion_info:\u0026#34;0\u0026#34;resources:- \u0026#34;@type\u0026#34;: type.googleapis.com/envoy.config.endpoint.v3.ClusterLoadAssignmentcluster_name:instance_1endpoints:- lb_endpoints:- endpoint:address:socket_address:address:127.0.0.1port_value:5050将上述 YAML 保存为 eds.yaml。\n为了使用这个端点文件，我们需要更新集群（cds.yaml）以读取 eds.yaml 中的端点。\nversion_info:\u0026#34;0\u0026#34;resources:- \u0026#34;@type\u0026#34;: type.googleapis.com/envoy.config.cluster.v3.Clustername:instance_1connect_timeout:5stype:EDSeds_cluster_config:eds_config:path:./eds.yaml通过运行 mv cds.yaml tmp; mv tmp cds.yaml 来强制重载。Envoy 会重新加载配置，我们就可以像以前一样向 localhost:10000/headers 发送请求。现在的区别是，不同的配置在不同的文件中，可以分别更新。\n 下一章   ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"ce74faf0f212ecc3473896d9723550a4","permalink":"https://jimmysong.io/docs/envoy-handbook/dynamic-config/lab8/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/envoy-handbook/dynamic-config/lab8/","section":"envoy-handbook","summary":"我们将在这个实验中创建一个动态的 Envoy 配置，并通过单独的配置文件","tags":null,"title":"实验 8：来自文件系统的动态配置","type":"book"},{"authors":null,"categories":null,"content":"在这个实验中，我们将编写一个 Lua 脚本，为响应头添加一个头，并使用一个文件中定义的全局脚本。\n我们将创建一个 Envoy 配置和一个 Lua 脚本，在响应句柄上添加一个头。由于我们不会使用请求路径，所以我们不需要定义 envoy_on_request 函数。响应函数看起来像这样。\nfunction envoy_on_response(response_handle) response_handle:headers():add(\u0026#34;hello\u0026#34;, \u0026#34;world\u0026#34;) end 我们在从 headers() 函数返回的 header 对象上调用 add(\u0026lt;header-name\u0026gt;, \u0026lt;header-value\u0026gt;) 函数。\n让我们在 Envoy 配置中内联定义这个脚本。为了简化配置，我们将使用 direct_response，而不是集群。\nstatic_resources:listeners:- name:mainaddress:socket_address:address:0.0.0.0port_value:10000filter_chains:- filters:- name:envoy.filters.network.http_connection_managertyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManagerstat_prefix:ingress_httproute_config:name:some_routevirtual_hosts:- name:some_servicedomains:- \u0026#34;*\u0026#34;routes:- match:prefix:\u0026#34;/\u0026#34;direct_response:status:200body:inline_string:\u0026#34;200\u0026#34;http_filters:- name:envoy.filters.http.luatyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.http.lua.v3.Luainline_code:|function envoy_on_response(response_handle) response_handle:headers():add(\u0026#34;hello\u0026#34;, \u0026#34;world\u0026#34;) end- name:envoy.filters.http.router将上述 YAML 保存为 8-lab-1-lua-script.yaml 并运行它。\nfunc-e run -c 8-Lab-1-lua-script.yaml \u0026amp; 为了测试这个功能，我们可以向 localhost:10000 发送一个请求，并检查响应头。\n$ curl -v localhost:10000 ... \u0026gt; GET / HTTP/1.1 \u0026gt; Host: localhost:10000 \u0026gt; User-Agent: curl/7.64.0 \u0026gt; Accept: */* \u0026gt; \u0026lt; HTTP/1.1 200 OK \u0026lt; content-length: 3 \u0026lt; content-type: text/plain \u0026lt; hello: world \u0026lt; date: Tue, 23 Nov 2021 21:37:01 GMT \u0026lt; server: envoy \u0026lt; 200 输出应该包括我们在其他标准头文件中添加的 hello: world header。\n让我们来看看一个更复杂的情况。对于传入的请求，我们要检查它们是否有一个叫做 my-request-id 的头，如果这个头不存在，那么我们要为所有 GET 请求添加一个 my-request-id 头。\n因为我们想在 envoy_on_response 函数中检查方法和一个头，我们将使用动态元数据在 envoy_on_request 函数中存储这些值。然后，在响应函数中，我们可以读取元数据，检查头是否被设置，方法是否为 GET，并添加 my-request-id 头。\n下面是代码的样子。\nfunction envoy_on_request(request_handle) local headers = request_handle:headers() local metadata = request_handle:streamInfo():dynamicMetadata() metadata:set(\u0026#34;envoy.filters.http.lua\u0026#34;, \u0026#34;requestInfo\u0026#34;, { requestId = headers:get(\u0026#34;my-request-id\u0026#34;), method = headers:get(\u0026#34;:method\u0026#34;), }) end function envoy_on_response(response_handle) local requestInfoObj = response_handle:streamInfo():dynamicMetadata():get(\u0026#34;envoy.filters.http.lua\u0026#34;)[\u0026#34;requestInfo\u0026#34;] local requestId = requestInfoObj.requestId local method = requestInfoObj.method if (requestId == nil or requestId == \u0026#39;\u0026#39;) and (method == \u0026#39;GET\u0026#39;) then response_handle:logInfo(\u0026#34;Adding request ID header\u0026#34;) response_handle:headers():add(\u0026#34;my-request-id\u0026#34;, \u0026#34;some_id_here\u0026#34;) end end 注意，目前我们使用 some_id_here 作为 my-request-id的值，以后我们会创建一个函数，为我们生成一个 ID。下面是完整的 Envoy 配置的样子。\nstatic_resources:listeners:- name:mainaddress:socket_address:address:0.0.0.0port_value:10000filter_chains:- filters:- name:envoy.filters.network.http_connection_managertyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManagerstat_prefix:ingress_httproute_config:name:some_routevirtual_hosts:- name:some_servicedomains:- \u0026#34;*\u0026#34;routes:- match:prefix:\u0026#34;/\u0026#34;direct_response:status:200body:inline_string:\u0026#34;200\u0026#34;http_filters:- name:envoy.filters.http.luatyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.http.lua.v3.Luainline_code:|function envoy_on_request(request_handle) local headers = request_handle:headers() local metadata = request_handle:streamInfo():dynamicMetadata() metadata:set(\u0026#34;envoy.filters.http.lua\u0026#34;, \u0026#34;requestInfo\u0026#34;, { requestId = headers:get(\u0026#34;my-request-id\u0026#34;), method = headers:get(\u0026#34;:method\u0026#34;), }) end function envoy_on_response(response_handle) local requestInfoObj = response_handle:streamInfo():dynamicMetadata():get(\u0026#34;envoy.filters.http.lua\u0026#34;)[\u0026#34;requestInfo\u0026#34;] local requestId = requestInfoObj.requestId local method = requestInfoObj.method if (requestId == nil or requestId == \u0026#39;\u0026#39;) and (method == \u0026#39;GET\u0026#39;) then response_handle:logInfo(\u0026#34;Adding request ID header\u0026#34;) response_handle:headers():add(\u0026#34;my-request-id\u0026#34;, \u0026#34;some_id_here\u0026#34;) end end- name:envoy.filters.http.router将上述 YAML 保存为 8-lab-1-lua-script-1.yaml 并运行它。\nfunc-e run -c 8-Lab-1-lua-script-1.yaml \u0026amp; 让我们试一试几种情况。首先，我们将发送一个没有设置 my-request-id 头的 GET 请求。\n$ curl -v localhost:10000 ... [2021-11-23 22:59:35.932][2258][info][lua] [source/extensions/filters/http/lua/lua_filter.cc:795] script log: Adding request ID header \u0026lt; HTTP/1.1 200 OK \u0026lt; content-length: 3 \u0026lt; content-type: text/plain \u0026lt; my-request-id: some_id_here \u0026lt; date: Tue, 23 Nov 2021 22:59:35 GMT \u0026lt; server: envoy \u0026lt; * Connection #0 to host localhost left intact 200 我们知道 Lua 代码运行了，因为我们看到了日志条目和 my-request-id 头的设置。\n让我们试着发送一个 POST 请求。\n$ curl -X POST -v localhost:10000 ... \u0026gt; POST / HTTP/1.1 \u0026gt; Host: localhost:10000 \u0026gt; User-Agent: curl/7.64.0 \u0026gt; Accept: */* \u0026gt; \u0026lt; HTTP/1.1 200 OK \u0026lt; content-length: 3 \u0026lt; content-type: text/plain \u0026lt; date: Mon, 29 Nov 2021 23:49:58 GMT \u0026lt; server: envoy 注意到头信息中没有包括 my-request-id 头信息。最后，让我们也尝试发送一个 GET 请求，但也提供 my-request-id 头。在这种情况下，my-request-id 头信息也不应该被包含在响应中。\n$ curl -v -H \u0026#34;my-request-id: something\u0026#34; localhost:10000 ... \u0026gt; GET / HTTP/1.1 \u0026gt; Host: localhost:10000 \u0026gt; User-Agent: curl/7.64.0 \u0026gt; Accept: */* \u0026gt; my-request-id: …","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"fe6e1226451d8bd7b2653716a7650905","permalink":"https://jimmysong.io/docs/envoy-handbook/extending-envoy/lab16/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/envoy-handbook/extending-envoy/lab16/","section":"envoy-handbook","summary":"在这个实验中，我们将编写一个 Lua 脚本，为响应头添加一个头，并使","tags":null,"title":"实验16：使用 Lua 脚本扩展 Envoy","type":"book"},{"authors":null,"categories":null,"content":"原始目的地过滤器（envoy.filters.listener.original_dst）会读取 SO_ORIGINAL_DST 套接字选项。当一个连接被 iptables REDIRECT 或 TPROXY 目标（如果transparent选项被设置）重定向时，这个选项被设置。该过滤器可用于与 ORIGINAL_DST 类型的集群连接。\n当使用 ORIGINAL_DST 集群类型时，请求会被转发到由重定向元数据寻址的上游主机，而不做任何主机发现。因此，在集群中定义任何端点都是没有意义的，因为端点是从原始数据包中提取的，并不是由负载均衡器选择。\n我们可以将 Envoy 作为一个通用代理，使用这种集群类型将所有请求转发到原始目的地。\n要使用 ORIGINAL_DST 集群，流量需要通过 iptables REDIRECT 或 TPROXY 目标到达 Envoy。\n...listener_filters:- name:envoy.filters.listener.original_dsttyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.listener.original_dst.v3.OriginalDst...clusters:- name:original_dst_clusterconnect_timeout:5stype:ORIGNAL_DSTlb_policy:CLUSTER_PROVIDED ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"2896c1931786e37a2e3b0d7a84390621","permalink":"https://jimmysong.io/docs/envoy-handbook/listener/original-destination-listener-filter/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/envoy-handbook/listener/original-destination-listener-filter/","section":"envoy-handbook","summary":"原始目的地过滤器（envoy.filters.listene","tags":null,"title":"原始目的地监听器过滤器","type":"book"},{"authors":null,"categories":null,"content":"Kiali 是一个基于 Istio 的服务网格的管理控制台。它提供了仪表盘、可观察性，并让我们通过强大的配置和验证能力来操作网格。它通过推断流量拓扑来显示服务网格，并显示网格的健康状况。Kiali 提供了详细的指标，强大的验证，Grafana 访问，以及与 Jaeger 的分布式跟踪的强大集成。\n要安装 Kiali，请使用 addons 文件夹中的 kiali.yaml 文件：\n$ kubectl apply -f istio-1.9.0/samples/addons/kiali.yaml customresourcedefinition.apiextensions.k8s.io/monitoringdashboards.monito ring.kiali.io created serviceaccount/kiali created configmap/kiali created clusterrole.rbac.authorization.k8s.io/kiali-viewer created clusterrole.rbac.authorization.k8s.io/kiali created clusterrolebinding.rbac.authorization.k8s.io/kiali created service/kiali created deployment.apps/kiali created 注意，如果你看到任何错误，例如在版本 monitoringkiali.io/v1alpha 中没有匹配的 MonitoringDashboard，请再次重新运行 kubectl apply 命令。问题是，在安装 CRD（自定义资源定义）和由该 CRD 定义的资源时，可能存在一个匹配条件。\n我们可以用 getmesh istioctl dashboard kiali 打开 Kiali。\nKiali 可以生成一个像下图这样的服务图。\n   Kiali 界面  该图向我们展示了服务的拓扑结构，并将服务的通信方式可视化。它还显示了入站和出站的指标，以及通过连接 Jaeger 和 Grafana（如果安装了）的追踪。图中的颜色代表服务网格的健康状况。颜色为红色或橙色的节点可能需要注意。组件之间的边的颜色代表这些组件之间的请求的健康状况。节点形状表示组件的类型，如服务、工作负载或应用程序。\n节点和边的健康状况会根据用户的偏好自动刷新。该图也可以暂停以检查一个特定的状态，或重放以重新检查一个特定的时期。\nKiali 提供创建、更新和删除 Istio 配置的操作，由向导驱动。我们可以配置请求路由、故障注入、流量转移和请求超时，所有这些都来自用户界面。如果我们有任何现有的 Istio 配置已经部署，Kiali 可以验证它并报告任何警告或错误。\n 下一章   ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"86ac708f4bc6bded5a10295d0247c625","permalink":"https://jimmysong.io/docs/istio-handbook/observability/kiali/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/istio-handbook/observability/kiali/","section":"istio-handbook","summary":"Kiali 是一个基于 Istio 的服务网格的管理控制台。它提供了仪表盘、可观察","tags":null,"title":"Kiali","type":"book"},{"authors":null,"categories":null,"content":"路由发现服务（RDS）是 Envoy 里面的一个可选 API，用于动态获取路由配置。路由配置包括 HTTP header 修改、虚拟主机以及每个虚拟主机中包含的单个路由规则配置。每个 HTTP 连接管理器都可以通过 API 独立地获取自身的路由配置。\n注意：Envoy 从 1.9 版本开始已不再支持 v1 API。\n统计 RDS 的统计树以 http.\u0026lt;stat_prefix\u0026gt;.rds.\u0026lt;route_config_name\u0026gt;.*.为根，route_config_name名称中的任何:字符在统计树中被替换为_。统计树包含以下统计信息：\n   名字 类型 描述     config_reload Counter 因配置不同而导致配置重新加载的总次数   update_attempt Counter 尝试调用配置加载 API 的总次数   update_success Counter 调用配置加载 API 成功的总次数   update_failure Counter 调用配置加载 API 因网络错误的失败总数   update_rejected Counter 调用配置加载 API 因 schema/验证错误的失败总次数   version Gauge 来自上次成功调用配置加载API的内容哈希    ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"6478162900dc59552379fe248a1b5a65","permalink":"https://jimmysong.io/docs/istio-handbook/data-plane/envoy-rds/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/istio-handbook/data-plane/envoy-rds/","section":"istio-handbook","summary":"路由发现服务（RDS）是 Envoy 里面的一个可选 API，用于动态获取","tags":null,"title":"RDS（路由发现服务）","type":"book"},{"authors":null,"categories":null,"content":"Sidecar 模式是 Istio 服务网格采用的模式，在服务网格出现之前该模式就一直存在，尤其是当微服务出现后开始盛行，本文讲解 Sidecar 模式。\n什么是 Sidecar 模式 将应用程序的功能划分为单独的进程可以被视为 Sidecar 模式。Sidecar 设计模式允许你为应用程序添加许多功能，而无需额外第三方组件的配置和代码。\n就如 Sidecar 连接着摩托车一样，类似地在软件架构中， Sidecar 应用是连接到父应用并且为其扩展或者增强功能。Sidecar 应用与主应用程序松散耦合。\n让我用一个例子解释一下。想象一下假如你有6个微服务相互通信以确定一个包裹的成本。\n每个微服务都需要具有可观察性、监控、日志记录、配置、断路器等功能。所有这些功能都是根据一些行业标准的第三方库在每个微服务中实现的。\n但再想一想，这不是多余吗？它不会增加应用程序的整体复杂性吗？如果你的应用程序是用不同的语言编写时会发生什么——如何合并那些特定用于 .Net、Java、Python 等语言的第三方库。\n使用 Sidecar 模式的优势  通过抽象出与功能相关的共同基础设施到一个不同层降低了微服务代码的复杂度。 因为你不再需要编写相同的第三方组件配置文件和代码，所以能够降低微服务架构中的代码重复度。 降低应用程序代码和底层平台的耦合度。  Sidecar 模式如何工作 Sidecar 是容器应用模式的一种，也是在服务网格中发扬光大的一种模式，详见 Service Mesh 架构解析，其中详细描述了节点代理和 Sidecar 模式的服务网格架构。\n使用 Sidecar 模式部署服务网格时，无需在节点上运行代理（因此您不需要基础结构的协作），但是集群中将运行多个相同的 Sidecar 副本。从另一个角度看：我可以为一组微服务部署到一个服务网格中，你也可以部署一个有特定实现的服务网格。在 Sidecar 部署方式中，你会为每个应用的容器部署一个伴生容器。Sidecar 接管进出应用容器的所有流量。在 Kubernetes 的 Pod 中，在原有的应用容器旁边运行一个 Sidecar 容器，可以理解为两个容器共享存储、网络等资源，可以广义的将这个注入了 Sidecar 容器的 Pod 理解为一台主机，两个容器共享主机资源。\n参考  理解 Istio 服务网格中 Envoy 代理 Sidecar 注入及流量劫持 - jimmysong.io 微服务中的 Sidecar 设计模式解析 - cloudnative.to  ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"2bbbc77bbbc6b9ca66e4295bed316d6e","permalink":"https://jimmysong.io/docs/istio-handbook/concepts/sidecar-pattern/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/istio-handbook/concepts/sidecar-pattern/","section":"istio-handbook","summary":"Sidecar 模式是 Istio 服务网格采用的模式，在服务网格出现之前该模式就一直","tags":null,"title":"Sidecar 模式","type":"book"},{"authors":null,"categories":null,"content":"WorkloadGroup 描述了工作负载实例的集合。它提供了一个规范，工作负载实例可用于启动其代理，包括元数据和身份。它只适用于虚拟机等非 Kubernetes 工作负载，旨在模仿现有的用于 Kubernetes 工作负载的 sidecar 注入和部署规范模型，以引导 Istio 代理。\n示例 下面的例子声明了一个代表工作负载集合的工作负载组，这些工作负载将在 bookinfo 命名空间的 reviews 下注册。在引导过程中，这组标签将与每个工作负载实例相关联，端口 3550 和 8080 将与工作负载组相关联，并使用 default 服务账户。app.kubernetes.io/version 只是一个标签的例子。\napiVersion:networking.istio.io/v1alpha3kind:WorkloadGroupmetadata:name:reviewsnamespace:bookinfospec:metadata:labels:app.kubernetes.io/name:reviewsapp.kubernetes.io/version:\u0026#34;1.3.4\u0026#34;template:ports:grpc:3550http:8080serviceAccount:defaultprobe:initialDelaySeconds:5timeoutSeconds:3periodSeconds:4successThreshold:3failureThreshold:3httpGet:path:/foo/barhost:127.0.0.1port:3100scheme:HTTPShttpHeaders:- name:Lit-Headervalue:Im-The-Best配置项 下图是 WorkloadGroup 资源的配置拓扑图。\n  WorkloadGroup 资源配置拓扑图  WorkloadGroup 资源的顶级配置项如下：\n metadata：元数据，将用于所有相应的 WorkloadEntry。WorkloadGroup 的用户标签应在 metadata 中而不是在 template 中设置。 template：用于生成属于该 WorkloadGroup 的 WorkloadEntry 资源的模板。请注意，模板中不应设置 address 和 labels 字段，而空的 serviceAccount 的默认值为 default。工作负载身份（mTLS 证书）将使用指定服务账户的令牌进行引导。该组中的 WorkloadEntry 将与 WorkloadGroup 处于同一命名空间，并继承上述 metadata 字段中的标签和注释。 probe：ReadinessProbe 描述了用户必须为其工作负载的健康检查提供的配置。这个配置在语法和逻辑上大部分都与 K8s 一致。  关于 WorkloadGroup 配置的详细用法请参考 Istio 官方文档。\n参考  WorkloadGroup - istio.io  ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"6d76c63f42134e54b3fd856a88beefed","permalink":"https://jimmysong.io/docs/istio-handbook/config-networking/workload-group/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/istio-handbook/config-networking/workload-group/","section":"istio-handbook","summary":"WorkloadGroup 描述了工作负载实例的集合。它提供了一个规范，工作负载实例可","tags":null,"title":"WorkloadGroup","type":"book"},{"authors":null,"categories":null,"content":"在本节中，我们将为推荐服务引入 5 秒的延迟。Envoy 将为 50% 的请求注入延迟。\napiVersion:networking.istio.io/v1alpha3kind:VirtualServicemetadata:name:recommendationservicespec:hosts:- recommendationservicehttp:- route:- destination:host:recommendationservicefault:delay:percentage:value:50fixedDelay:5s将上述 YAML 保存为 recommendation-delay.yaml，然后用 kubectl apply -f recommendation-delay.yaml 创建 VirtualService。\n我们可以在浏览器中打开 INGRESS_HOST，然后点击其中一个产品。推荐服务的结果显示在屏幕底部的”Other Products You Might Light“部分。如果我们刷新几次页面，我们会注意到，该页面要么立即加载，要么有一个延迟加载页面。这个延迟是由于我们注入了 5 秒的延迟。\n我们可以打开 Grafana（getmesh istioctl dash grafana）和 Istio 服务仪表板。确保从服务列表中选择recommendationsservice，在 Reporter 下拉菜单中选择 source，并查看显示延迟的 Client Request Duration，如下图所示。\n   Recommendations 服务延迟  同样地，我们可以注入一个中止。在下面的例子中，我们为发送到产品目录服务的 50% 的请求注入一个 HTTP 500。\napiVersion:networking.istio.io/v1alpha3kind:VirtualServicemetadata:name:productcatalogservicespec:hosts:- productcatalogservicehttp:- route:- destination:host:productcatalogservicefault:abort:percentage:value:50httpStatus:500将上述 YAML 保存为 productcatalogservice-abort.yaml，然后用 kubectl apply -f productcatalogservice-abort.yaml 更新 VirtualService。\n如果我们刷新几次产品页面，我们应该得到如下图所示的错误信息。\n   注入错误  请注意，错误信息说，失败的原因是故障过滤器中止。如果我们打开 Grafana（getmesh istioctl dash grafana），我们也会注意到图中报告的错误。\n我们可以通过运行 kubectl delete vs productcatalogservice 来删除 VirtualService。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"ee50c7c47c0101dbbaae3f0c9dcd0d39","permalink":"https://jimmysong.io/docs/istio-handbook/practice/fault-injection/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/istio-handbook/practice/fault-injection/","section":"istio-handbook","summary":"在本节中，我们将为推荐服务引入 5 秒的延迟。Envoy 将为 50% 的","tags":null,"title":"错误注入","type":"book"},{"authors":null,"categories":null,"content":"弹性（Resiliency）是指在面对故障和对正常运行的挑战时，提供和保持可接受的服务水平的能力。这不是为了避免故障。它是以一种没有停机或数据丢失的方式来应对它们。弹性的目标是在故障发生后将服务恢复到一个完全正常的状态。\n使服务可用的一个关键因素是在提出服务请求时使用超时（timeout）和重试（retry）策略。我们可以在 Istio 的 VirtualService 上配置这两者。\n使用超时字段，我们可以为 HTTP 请求定义一个超时。如果请求的时间超过了超时字段中指定的值，Envoy 代理将放弃请求，并将其标记为超时（向应用程序返回一个 HTTP 408）。连接将保持开放，除非触发了异常点检测。下面是一个为路由设置超时的例子：\n...- route:- destination:host:customers.default.svc.cluster.localsubset:v1timeout:10s...除了超时之外，我们还可以配置更细化的重试策略。我们可以控制一个给定请求的重试次数，每次尝试的超时时间，以及我们想要重试的具体条件。\n例如，我们可以只在上游服务器返回任何 5xx 响应代码时重试请求，或者只在网关错误（HTTP 502、503 或 504）时重试，或者甚至在请求头中指定可重试的状态代码。重试和超时都发生在客户端。当 Envoy 重试一个失败的请求时，最初失败并导致重试的端点就不再包含在负载均衡池中了。假设 Kubernetes 服务有 3 个端点（Pod），其中一个失败了，并出现了可重试的错误代码。当 Envoy 重试请求时，它不会再向原来的端点重新发送请求。相反，它将把请求发送到两个没有失败的端点中的一个。\n下面是一个例子，说明如何为一个特定的目的地设置重试策略。\n...- route:- destination:host:customers.default.svc.cluster.localsubset:v1retries:attempts:10perTryTimeout:2sretryOn:connect-failure,reset...上述重试策略将尝试重试任何连接超时（connect-failure）或服务器完全不响应（reset）的失败请求。我们将每次尝试的超时时间设置为 2 秒，尝试的次数设置为 10 次。注意，如果同时设置重试和超时，超时值将是请求等待的最长时间。如果我们在上面的例子中指定了 10 秒的超时，那么即使重试策略中还剩下一些尝试，我们也只能最多等待 10 秒。\n关于重试策略的更多细节，请参阅 x-envoy-retry-on 文档。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"dfd5d5384615723eaa2ba031c6c50dca","permalink":"https://jimmysong.io/docs/istio-handbook/traffic-management/resiliency/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/istio-handbook/traffic-management/resiliency/","section":"istio-handbook","summary":"弹性（Resiliency）是指在面对故障和对正常运行的挑战","tags":null,"title":"弹性","type":"book"},{"authors":null,"categories":null,"content":"负载均衡是一种在单个上游集群的多个端点之间分配流量的方式。在众多端点之间分配流量的原因是为了最好地利用可用资源。\n为了实现资源的最有效利用，Envoy 提供了不同的负载均衡策略，可以分为两组：全局负载均衡和分布式负载均衡。不同的是，在全局负载均衡中，我们使用单一的控制平面来决定端点之间的流量分配。Envoy 决定负载如何分配（例如，使用主动健康检查、分区感知路由、负载均衡策略）。\n在多个端点之间分配负载的技术之一是一致性哈希。服务器使用请求的一部分来创建一个哈希值来选择一个端点。在模数散列中，哈希值被认为是一个巨大的数字。为了得到发送请求的端点索引，我们将哈希值与可用端点的数量取余数（index=hash % endpointCount）。如果端点的数量是稳定的，这种方法效果很好。然而，如果端点被添加或删除（即它们不健康，我们扩大或缩小它们的规模，等等），大多数请求将在一个与以前不同的端点上结束。\n一致性哈希是一种方法，每个端点根据某些属性被分配多个有价值的值。然后，每个请求被分配到具有最接近哈希值的端点。这种方法的价值在于，当我们添加或删除端点时，大多数请求最终会被分配到与之前相同的端点。拥有这种 “粘性” 是有帮助的，因为它不会干扰端点持有的任何缓存。\n负载均衡策略 Envoy 使用其中一个负载均衡策略来选择一个端点发送流量。负载均衡策略是可配置的，可以为每个上游集群分别指定。请注意，负载均衡只在健康的端点上执行。如果没有定义主动或被动的健康检查，则假定所有端点都是健康的。\n我们可以使用 lb_policy 字段和其他针对所选策略的字段来配置负载均衡策略。\n加权轮询（默认） 加权轮询（ROUND_ROBIN）以轮询顺序选择端点。如果端点是加权的，那么就会使用加权的轮询顺序。这个策略给我们提供了一个可预测的请求在所有端点的分布。权重较高的端点将在轮转中出现得更频繁，以实现有效的加权。\n加权的最小请求 加权最小请求（LEAST_REQUEST）算法取决于分配给端点的权重。\n如果所有的端点权重相等，算法会随机选择 N 个可用的端点（choice_count），并挑选出活动请求最少的一个。\n如果端点的权重不相等，该算法就会转入一种模式，即使用加权的循环计划，其中的权重是根据选择时端点的请求负荷动态调整。\n以下公式用于动态计算权重。\nweight = load_balancing_weight / (active_requests + 1)^active_request_bias active_request_bias 是可配置的（默认为 1.0）。主动请求偏差越大，主动请求就越积极地降低有效权重。\n如果 active_request_bias 被设置为 0，那么算法的行为就像轮询一样，在挑选时忽略了活动请求数。\n我们可以使用 least_request_lb_config 字段来设置加权最小请求的可选配置。\n...lb_policy:LEAST_REQUESTleast_request_lb_config:choice_count:5active_request_bias:0.5...环形哈希 环形哈希（或模数散列）算法（RING_HASH）实现了对端点的一致性哈希。每个端点地址（默认设置）都被散列并映射到一个环上。Envoy 通过散列一些请求属性，并在环上顺时针找到最近的对应端点，将请求路由到一个端点。哈希键默认为端点地址；然而，它可以使用 hash_key 字段改变为任何其他属性。\n我们可以通过指定最小（minimum_ring_size）和最大（maximum_ring_size）的环形哈希算法，并使用统计量（min_hashes_per_host 和 max_hashes_per_host）来确保良好的分布。环越大，请求的分布就越能反映出所需的权重。最小环大小默认为 1024 个条目（限制在 8M 个条目），而最大环大小默认为 8M（限制在 8M）。\n我们可以使用 ring_hash_lb_config 字段设置环形哈希的可选配置。\n...lb_policy:RING_HASHring_hash_lb_config:minimum_ring_size:2000maximum_ring_size:10000...Maglev 与环形哈希算法一样，maglev（MAGLEV）算法也实现了对端点的一致性哈希。该算法产生一个查找表，允许在一个恒定的时间内找到一个项目。Maglev 的设计是为了比环形哈希算法的查找速度更快，并且使用更少的内存。你可以在下面这篇文章中阅读更多关于它的内容 Maglev: A Fast and Reliable Software Network Load Balancer。\n我们可以使用 maglev_lb_config 字段来设置 maglev 算法的可选配置。\n...lb_policy:MAGLEVmaglev_lb_config:table_size:69997...默认的表大小是 65537，但它可以被设置为任何素数，只要它不大于 5000011。\n原始目的地 原始目的地（ORIGINAL_DESTINATION）是一个特殊用途的负载均衡器，只能与原始目的地集群一起使用。我们在谈到原始目的地集群类型时已经提到了原始目的地负载均衡器。\n随机 顾名思义，随机（RANDOM）算法会挑选一个可用的随机端点。如果你没有配置主动健康检查策略，随机算法的表现比轮询算法更好。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"e7cb74305697309429cab75c6a1b0ad9","permalink":"https://jimmysong.io/docs/envoy-handbook/cluster/load-balancing/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/envoy-handbook/cluster/load-balancing/","section":"envoy-handbook","summary":"负载均衡是一种在单个上游集群的多个端点之间分配流量的方式。在","tags":null,"title":"负载均衡","type":"book"},{"authors":null,"categories":null,"content":"集群端点（/clusters）将显示配置的集群列表，并包括以下信息：\n 每个主机的统计数据 每个主机的健康状态 断路器设置 每个主机的权重和位置信息   这里的主机指的是每个被发现的属于上游集群的主机。\n 下面的片段显示了信息的模样（注意，输出是经过裁剪的）。\n{ \u0026#34;cluster_statuses\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;api_google_com\u0026#34;, \u0026#34;host_statuses\u0026#34;: [ { \u0026#34;address\u0026#34;: { \u0026#34;socket_address\u0026#34;: { \u0026#34;address\u0026#34;: \u0026#34;10.0.0.1\u0026#34;, \u0026#34;port_value\u0026#34;: 8080 } }, \u0026#34;stats\u0026#34;: [ { \u0026#34;value\u0026#34;: \u0026#34;23\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;cx_total\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;rq_error\u0026#34; }, { \u0026#34;value\u0026#34;: \u0026#34;51\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;rq_success\u0026#34; }, ... ], \u0026#34;health_status\u0026#34;: { \u0026#34;eds_health_status\u0026#34;: \u0026#34;HEALTHY\u0026#34; }, \u0026#34;weight\u0026#34;: 1, \u0026#34;locality\u0026#34;: {} } ], \u0026#34;circuit_breakers\u0026#34;: { \u0026#34;thresholds\u0026#34;: [ { \u0026#34;max_connections\u0026#34;: 1024, \u0026#34;max_pending_requests\u0026#34;: 1024, \u0026#34;max_requests\u0026#34;: 1024, \u0026#34;max_retries\u0026#34;: 3 }, { \u0026#34;priority\u0026#34;: \u0026#34;HIGH\u0026#34;, \u0026#34;max_connections\u0026#34;: 1024, \u0026#34;max_pending_requests\u0026#34;: 1024, \u0026#34;max_requests\u0026#34;: 1024, \u0026#34;max_retries\u0026#34;: 3 } ] }, \u0026#34;observability_name\u0026#34;: \u0026#34;api_google_com\u0026#34; }, ...  为了获得 JSON 输出，我们可以在发出请求或在浏览器中打开 URL 时附加 ?format=json。\n 主机统计 输出包括每个主机的统计数据，如下表所解释。\n   指标名称 描述     cx_total 连接总数   cx_active 有效连接总数   cx_connect_fail 连接失败总数   rq_total 请求总数   rq_timeout 超时的请求总数   rq_success 有非 5xx 响应的请求总数   rq_error 有 5xx 响应的请求总数   rq_active 有效请求总数    主机健康状况 主机的健康状况在 health_status 字段下报告。健康状态中的值取决于健康检查是否被启用。假设启用了主动和被动（断路器）健康检查，该表显示了可能包含在 health_status 字段中的布尔字段。\n   字段名称 描述     failed_active_health_check 真，如果该主机目前未能通过主动健康检查。   failed_outlier_check 真，如果该宿主目前被认为是一个异常，并已被弹出。   failed_active_degraded_check 如果主机目前通过主动健康检查被标记为降级，则为真。   pending_dynamic_removal 如果主机已经从服务发现中移除，但由于主动健康检查正在稳定，则为真。   pending_active_hc 真，如果该主机尚未被健康检查。   excluded_via_immediate_hc_fail 真，如果该主机应被排除在恐慌、溢出等计算之外，因为它被明确地通过协议信号从轮换中取出，并且不打算被路由到。   active_hc_timeout 真，如果主机由于超时而导致活动健康检查失败。   eds_health_status 默认情况下，设置为healthy（如果不使用 EDS）。否则，它也可以被设置为unhealthy 或 degraded。    请注意，表中的字段只有在设置为真时才会被报告。例如，如果主机是健康的，那么健康状态将看起来像这样。\n\u0026#34;Health_status\u0026#34;:{ \u0026#34;eds_health_status\u0026#34;:\u0026#34;HEALTHY\u0026#34; } 如果配置了主动健康检查，而主机是失败的，那么状态将看起来像这样。\n\u0026#34;Health_status\u0026#34;:{ \u0026#34;failed_active_health_check\u0026#34;: true, \u0026#34;eds_health_status\u0026#34;:\u0026#34;HEALTHY\u0026#34; }  ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"8c66f48932886efae67d6b9d5d180cba","permalink":"https://jimmysong.io/docs/envoy-handbook/admin-interface/clusters/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/envoy-handbook/admin-interface/clusters/","section":"envoy-handbook","summary":"集群端点（/clusters）将显示配置的集群列表，并包括以","tags":null,"title":"集群","type":"book"},{"authors":null,"categories":null,"content":"在这个实验中，我们将学习使用日志过滤器，根据一些请求属性，只记录某些请求。\n让我们想出几个我们想用过滤器来实现的日志要求。\n 将所有带有 HTTP 404 状态码的请求记录到一个名为 not_found.log 的日志文件中。 将所有带有 env=debug 头值的请求记录到一个名为 debug.log 的日志文件中。 将所有 POST 请求记录到标准输出  基于这些要求，我们将有两个访问记录器记录到文件（not_found.log 和 debug.log）和一个写到 stdout 的访问记录器。\naccess_log 字段是一个数组，我们可以在它下面定义多个记录器。在各个日志记录器里面，我们可以使用 filter 字段来指定何时将字符串写到日志中。\n对于第一个要求，我们将使用状态码过滤器，而对于第二个要求，我们将使用 Header 过滤器。下面是配置的样子。\nstatic_resources:listeners:- name:listener_0address:socket_address:address:0.0.0.0port_value:10000filter_chains:- filters:- name:envoy.filters.network.http_connection_managertyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManagerstat_prefix:ingress_httpaccess_log:- name:envoy.access_loggers.filetyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.access_loggers.file.v3.FileAccessLogpath:./debug.logfilter:header_filter:header:name:envstring_match:exact:debug- name:envoy.access_loggers.filetyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.access_loggers.file.v3.FileAccessLogpath:./not_found.logfilter:status_code_filter:comparison:value:default_value:404runtime_key:ingress_http_status_code_filter- name:envoy.access_loggers.stdouttyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.access_loggers.stream.v3.StdoutAccessLogfilter:header_filter:header:name:\u0026#34;:method\u0026#34;string_match:exact:POSThttp_filters:- name:envoy.filters.http.routerroute_config:name:my_first_routevirtual_hosts:- name:direct_response_servicedomains:[\u0026#34;*\u0026#34;]routes:- match:prefix:\u0026#34;/404\u0026#34;direct_response:status:404body:inline_string:\u0026#34;404\u0026#34;- match:prefix:\u0026#34;/\u0026#34;direct_response:status:200body:inline_string:\u0026#34;200\u0026#34;将上述 YAML 保存为 6-lab-1-logging-filters.yaml 并在后台运行 Envoy。\nfunc-e run -c 6-lab-1-logging-filters.yaml \u0026amp; 在 Envoy 运行的情况下，如果我们发送一个请求到 http://localhost:10000，那么我们会注意到没有任何东西被写入标准输出或日志文件。\n接下来让我们试试 POST 请求。\n$ curl -X POST localhost:10000 [2021-11-03T21:52:36.398Z] \u0026#34;POST / HTTP/1.1\u0026#34; 200 - 0 3 0 - \u0026#34;-\u0026#34; \u0026#34;curl/7.64.0\u0026#34; \u0026#34;528335ae-8f0d-4d22-934a-02d4702a9c62\u0026#34; \u0026#34;localhost:10000\u0026#34; \u0026#34;-\u0026#34; 你会注意到，日志条目被写到了配置中定义的标准输出。\n接下来，让我们发送一个头信息 env: debug 与以下请求。\ncurl -H \u0026#34;env: debug\u0026#34; localhost:10000 像第一个例子一样，没有任何东西会被写入标准输出（这不是一个 POST 请求）。然而，如果我们在 debug.log 文件中查看，那么我们会看到日志条目。\n$ cat debug.log [2021-11-03T21:54:49.357Z] \u0026#34;GET / HTTP/1.1\u0026#34; 200 - 0 3 0 - \u0026#34;-\u0026#34; \u0026#34;curl/7.64.0\u0026#34; \u0026#34;ea2a11d6-6ccb-4f13-9686-4d30dbc3136e\u0026#34; \u0026#34;localhost:10000\u0026#34; \u0026#34;-\u0026#34; 同样地，让我们向 /404 发送一个请求，并查看 not_found.log 文件。\n$ curl localhost:10000/404 404 $ cat not_found.log [2021-11-03T21:55:37.891Z] \u0026#34;GET /404 HTTP/1.1\u0026#34; 404 - 0 3 0 - \u0026#34;-\u0026#34; \u0026#34;curl/7.64.0\u0026#34; \u0026#34;59bf1a1a-62b2-49e4-9226-7c49516ec390\u0026#34; \u0026#34;localhost:10000\u0026#34; \u0026#34;-\u0026#34; 在满足多个过滤条件的情况下（例如，我们有一个 POST 请求*，*我们将请求发送到 /404），在这种情况下，日志将被写入标准输出和 not_found.log。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"6751faaa9826cbfb43d9cde63abf01d5","permalink":"https://jimmysong.io/docs/envoy-handbook/logging/lab12/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/envoy-handbook/logging/lab12/","section":"envoy-handbook","summary":"在这个实验中，我们将学习使用日志过滤器，根据一些请求属性，只","tags":null,"title":"实验 12：使用日志过滤器","type":"book"},{"authors":null,"categories":null,"content":"在这个实验中，我们将使用 TinyGo、proxy-wasm-go-sdk 和 func-e CLI 来构建和测试一个 Envoy Wasm 扩展。\n我们将写一个简单的 Wasm 模块，为响应头添加一个头。稍后，我们将展示如何读取配置和添加自定义指标。我们将使用 Golang 并使用 TinyGo 编译器进行编译。\n安装 TinyGo 让我们下载并安装 TinyGo。\nwget https://github.com/tinygo-org/tinygo/releases/download/v0.21.0/tinygo_0.21.0_amd64.deb sudo dpkg -i tinygo_0.21.0_amd64.deb 你可以运行 tinygo version 来检查安装是否成功。\n$ tinygo version tinygo version 0.21.0 linux/amd64 (using go version go1.17.2 and LLVM version 11.0.0) 为 Wasm 模块搭建脚手架 我们将首先为我们的扩展创建一个新的文件夹，初始化 Go 模块，并下载 SDK 依赖。\n$ mkdir header-filter \u0026amp;\u0026amp; cd header-filter $ go mod init header-filter $ go mod edit -require=github.com/tetratelabs/proxy-wasm-go-sdk@main $ go mod download github.com/tetratelabs/proxy-wasm-go-sdk 接下来，让我们创建 main.go 文件，其中有我们 WASM 扩展的代码。\npackage main import ( \u0026#34;github.com/tetratelabs/proxy-wasm-go-sdk/proxywasm\u0026#34; \u0026#34;github.com/tetratelabs/proxy-wasm-go-sdk/proxywasm/types\u0026#34; ) func main() { proxywasm.SetVMContext(\u0026amp;vmContext{}) } type vmContext struct { // Embed the default VM context here,  // so that we don\u0026#39;t need to reimplement all the methods.  types.DefaultVMContext } // Override types.DefaultVMContext. func (*vmContext) NewPluginContext(contextID uint32) types.PluginContext { return \u0026amp;pluginContext{} } type pluginContext struct { // Embed the default plugin context here,  // so that we don\u0026#39;t need to reimplement all the methods.  types.DefaultPluginContext } // Override types.DefaultPluginContext. func (*pluginContext) NewHttpContext(contextID uint32) types.HttpContext { return \u0026amp;httpHeaders{contextID: contextID} } type httpHeaders struct { // Embed the default http context here,  // so that we don\u0026#39;t need to reimplement all the methods.  types.DefaultHttpContext contextID uint32 } func (ctx *httpHeaders) OnHttpRequestHeaders(numHeaders int, endOfStream bool) types.Action { proxywasm.LogInfo(\u0026#34;OnHttpRequestHeaders\u0026#34;) return types.ActionContinue } func (ctx *httpHeaders) OnHttpResponseHeaders(numHeaders int, endOfStream bool) types.Action { proxywasm.LogInfo(\u0026#34;OnHttpResponseHeaders\u0026#34;) return types.ActionContinue } func (ctx *httpHeaders) OnHttpStreamDone() { proxywasm.LogInfof(\u0026#34;%d finished\u0026#34;, ctx.contextID) } 将上述内容保存在一个名为 main.go的文件中。\n让我们建立过滤器，检查是否一切正常。\ntinygo build -o main.wasm -scheduler=none -target=wasi main.go 构建命令应该成功运行并生成一个名为 main.wasm的文件。\n我们将使用 func-e 来运行一个本地 Envoy 实例来测试我们构建的扩展。\n首先，我们需要一个 Envoy 配置，它将配置扩展。\nstatic_resources:listeners:- name:mainaddress:socket_address:address:0.0.0.0port_value:10000filter_chains:- filters:- name:envoy.http_connection_managertyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManagerstat_prefix:ingress_httpcodec_type:autoroute_config:name:local_routevirtual_hosts:- name:local_servicedomains:- \u0026#34;*\u0026#34;routes:- match:prefix:\u0026#34;/\u0026#34;direct_response:status:200body:inline_string:\u0026#34;hello world\\n\u0026#34;http_filters:- name:envoy.filters.http.wasmtyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/udpa.type.v1.TypedStructtype_url:type.googleapis.com/envoy.extensions.filters.http.wasm.v3.Wasmvalue:config:vm_config:runtime:\u0026#34;envoy.wasm.runtime.v8\u0026#34;code:local:filename:\u0026#34;main.wasm\u0026#34;- name:envoy.filters.http.routeradmin:address:socket_address:address:127.0.0.1port_value:9901将上述内容保存到 8-lab-2-wasm-config.yaml 文件。\nEnvoy 的配置在 10000 端口设置了一个监听器，返回一个直接响应（HTTP 200），正文是 hello world。在 http_filters 部分，我们配置了 envoy.filters.http.wasm 过滤器，并引用了我们之前建立的本地 WASM 文件（main.wasm）。\n让我们在后台用这个配置运行 Envoy。\nfunc-e run -c 8-Lab-2-wasm-config.yaml \u0026amp; Envoy 实例的启动应该没有任何问题。一旦启动，我们就可以向 Envoy 监听的端口（10000）发送一个请求。\n$ curl -v localhost:10000 ... \u0026lt; HTTP/1.1 200 OK \u0026lt; content-length: 13 \u0026lt; content-type: text/plain \u0026lt; my-new-header: some-value-here \u0026lt; date: Mon, 22 Jun 2021 17:02:31 GMT \u0026lt; server: envoy \u0026lt; hello world 输出显示了两个日志条目：一个来自 OnHttpRequestHeaders 处理器，第二个来自 OnHttpResponseHeaders 处理器。最后一行是过滤器中的直接响应配置所返回的响应示例。\n你可以通过用 fg 把进程带到前台，然后按 CTRL+C 停止代理。\n在 HTTP 响应上设置附加头信息 让我们打开 main.go 文件，在响应头信息中添加一个头信息。我们将更新 OnHttpResponseHeaders 函数来做到这一点。\n我们将调用 AddHttpResponseHeader 函数来添加一个新的头。更新 OnHttpResponseHeaders 函数，使其看起来像这样。\nfunc (ctx *httpHeaders) OnHttpResponseHeaders(numHeaders int, endOfStream bool) types.action { proxywasm.LogInfo(\u0026#34;OnHttpResponseHeaders\u0026#34;) err := proxywasm.AddHttpResponseHeader(\u0026#34;my-new-header\u0026#34;, \u0026#34;some-value-here\u0026#34;) if err != nil { proxywasm.LogCriticalf(\u0026#34; failed to add response header: %v\u0026#34;, err) } return types.ActionContinue } 让我们重新建立扩展。\ntinygo build -o main.wasm -scheduler=none -target=wasi main.go 现在我们可以用更新后的扩展来重新运行 Envoy 代理。\nfunc-e run -c 8-Lab-2-wasm-config.yaml \u0026amp; 现在，如果我们再次发送一个请求（确保添加 -v 标志），我们将看到被添加到响应中的头。\n$ curl -v localhost:10000 ... \u0026lt; HTTP/1.1 200 OK \u0026lt; content-length: 13 \u0026lt; content-type: text/plain \u0026lt; my-new-header: some-value-here \u0026lt; date: Mon, 22 Jun 2021 17:02:31 GMT \u0026lt; server: envoy \u0026lt; hello world 从配置中读取数值 在代码中硬编码这样的值从来不是一个好主意。让我们看看我们如何读取额外的头文件。\n1. 将 additionalHeaders 和 contextID 添加到 pluginContext 结构体中：\ntype pluginContext struct { // 在这里嵌入默认的插件上下文。 // 这样我们就不需要重新实现所有的方法了。  types.DefaultPluginContext additionalHeaders map[string]string contextID uint32 } 2. 更新 NewPluginContext 函数以初始化数值。\nfunc (*vmContext) …","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"bf0a7b6da2a886a254332d8158c4ebb4","permalink":"https://jimmysong.io/docs/envoy-handbook/extending-envoy/lab17/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/envoy-handbook/extending-envoy/lab17/","section":"envoy-handbook","summary":"在这个实验中，我们将使用 TinyGo、proxy-wasm-","tags":null,"title":"实验17：使用 Wasm 和 Go 扩展 Envoy","type":"book"},{"authors":null,"categories":null,"content":"原始源过滤器（envoy.filters.listener.original_src）在 Envoy 的上游（接收 Envoy 请求的主机）一侧复制了连接的下游（连接到 Envoy 的主机）的远程地址。\n例如，如果我们用 10.0.0.1 发送请求给 Envoy 连接到上游，源 IP 是 10.0.0.1 。这个地址是由代理协议过滤器决定的（接下来解释），或者它可以来自于可信的 HTTP Header。\n- name:envoy.filters.listener.original_srctyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions. filters.listener.original_src.v3.OriginalSrcmark:100该过滤器还允许我们在上游连接的套接字上设置 SO_MARK 选项。SO_MARK 选项用于标记通过套接字发送的每个数据包，并允许我们做基于标记的路由（我们可以在以后匹配标记）。\n上面的片段将该标记设置为 100。使用这个标记，我们可以确保非本地地址在绑定到原始源地址时可以通过 Envoy 代理路由回来。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"a01004c5858d90be8df8318bd9b86128","permalink":"https://jimmysong.io/docs/envoy-handbook/listener/original-source-listener-filter/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/envoy-handbook/listener/original-source-listener-filter/","section":"envoy-handbook","summary":"原始源过滤器（envoy.filters.listener.","tags":null,"title":"原始源监听器过滤器","type":"book"},{"authors":null,"categories":null,"content":"集群发现服务（CDS）是一个可选的 API，Envoy 将调用该 API 来动态获取集群管理器的成员。Envoy 还将根据 API 响应协调集群管理，根据需要完成添加、修改或删除已知的集群。\n关于 Envoy 是如何通过 CDS 从 pilot-discovery 服务中获取的 cluster 配置，请参考 Service Mesh深度学习系列part3—istio源码分析之pilot-discovery模块分析（续）一文中的 CDS 服务部分。\n注意\n 在 Envoy 配置中静态定义的 cluster 不能通过 CDS API 进行修改或删除。 Envoy 从 1.9 版本开始已不再支持 v1 API。  统计 CDS 的统计树以 cluster_manager.cds. 为根，统计如下：\n   名字 类型 描述     config_reload Counter 因配置不同而导致配置重新加载的总次数   update_attempt Counter 尝试调用配置加载 API 的总次数   update_success Counter 调用配置加载 API 成功的总次数   update_failure Counter 调用配置加载 API 因网络错误的失败总数   update_rejected Counter 调用配置加载 API 因 schema/验证错误的失败总次数   version Gauge 来自上次成功调用配置加载API的内容哈希   control_plane.connected_state Gauge 布尔值，用来表示与管理服务器的连接状态，1表示已连接，0表示断开连接    ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"741f9eeedad7f75d0a65ba7c05fdf44c","permalink":"https://jimmysong.io/docs/istio-handbook/data-plane/envoy-cds/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/istio-handbook/data-plane/envoy-cds/","section":"istio-handbook","summary":"集群发现服务（CDS）是一个可选的 API，Envoy 将调用该","tags":null,"title":"CDS（集群发现服务）","type":"book"},{"authors":null,"categories":null,"content":"ServiceEntry 可以在 Istio 的内部服务注册表中添加额外的条目，这样网格中自动发现的服务就可以访问 / 路由到这些手动指定的服务。服务条目描述了服务的属性（DNS 名称、VIP、端口、协议、端点）。这些服务可以是网格的外部服务（如 Web API），也可以是不属于平台服务注册表的网格内部服务（如与 Kubernetes 中的服务通信的一组虚拟机）。此外，服务条目的端点也可以通过使用 workloadSelector 字段动态选择。这些端点可以是使用 WorkloadEntry 对象声明的虚拟机工作负载或 Kubernetes pod。在单一服务下同时选择 pod 和 VM 的能力允许将服务从 VM 迁移到 Kubernetes，而不必改变与服务相关的现有 DNS 名称。\n示例 下面的例子声明了一些内部应用程序通过 HTTPS 访问的外部 API。Sidecar 检查了 ClientHello 消息中的 SNI 值，以路由到适当的外部服务。\napiVersion:networking.istio.io/v1alpha3kind:ServiceEntrymetadata:name:external-svc-httpsspec:hosts:- api.dropboxapi.com- www.googleapis.com- api.facebook.comlocation:MESH_EXTERNALports:- number:443name:httpsprotocol:TLSresolution:DNS下面的配置在 Istio 的注册表中添加了一组运行在未被管理的虚拟机上的 MongoDB 实例，因此这些服务也可以被视为网格中的任何其他服务。相关的 DestinationRule 被用来启动与数据库实例的 mTLS 连接。\napiVersion:networking.istio.io/v1alpha3kind:ServiceEntrymetadata:name:external-svc-mongoclusterspec:hosts:- mymongodb.somedomain# 未使用addresses:- 192.192.192.192/24# VIPports:- number:27018name:mongodbprotocol:MONGOlocation:MESH_INTERNALresolution:STATICendpoints:- address:2.2.2.2- address:3.3.3.3相关的 DestinationRule。\napiVersion:networking.istio.io/v1alpha3kind:DestinationRulemetadata:name:mtls-mongoclusterspec:host:mymongodb.somedomaintrafficPolicy:tls:mode:MUTUALclientCertificate:/etc/certs/myclientcert.pemprivateKey:/etc/certs/client_private_key.pemcaCertificates:/etc/certs/rootcacerts.pem下面的例子在一个 VirtualService 中使用 ServiceEntry 和 TLS 路由的组合，根据 SNI 值将流量引导到内部出口（egress）防火墙。\napiVersion:networking.istio.io/v1alpha3kind:ServiceEntrymetadata:name:external-svc-redirectspec:hosts:- wikipedia.org- \u0026#34;*.wikipedia.org\u0026#34;location:MESH_EXTERNALports:- number:443name:httpsprotocol:TLSresolution:NONE相关的 VirtualService，根据 SNI 值进行路由。\napiVersion:networking.istio.io/v1alpha3kind:VirtualServicemetadata:name:tls-routingspec:hosts:- wikipedia.org- \u0026#34;*.wikipedia.org\u0026#34;tls:- match:- sniHosts:- wikipedia.org- \u0026#34;*.wikipedia.org\u0026#34;route:- destination:host:internal-egress-firewall.ns1.svc.cluster.local带有 TLS 匹配的 VirtualService 是为了覆盖默认的 SNI 匹配。在没有 VirtualService 的情况下，流量将被转发到维基百科的域。\n下面的例子演示了专用出口（egress）网关的使用，所有外部服务流量都通过该网关转发。exportTo 字段允许控制服务声明对网格中其他命名空间的可见性。默认情况下，服务会被输出到所有命名空间。下面的例子限制了对当前命名空间的可见性，用 . 表示，所以它不能被其他命名空间使用。\napiVersion:networking.istio.io/v1alpha3kind:ServiceEntrymetadata:name:external-svc-httpbinnamespace :egressspec:hosts:- httpbin.comexportTo:- \u0026#34;.\u0026#34;location:MESH_EXTERNALports:- number:80name:httpprotocol:HTTPresolution:DNS定义一个网关来处理所有的出口（egress）流量。\napiVersion:networking.istio.io/v1alpha3kind:Gatewaymetadata:name:istio-egressgatewaynamespace:istio-systemspec:selector:istio:egressgatewayservers:- port:number:80name:httpprotocol:HTTPhosts:- \u0026#34;*\u0026#34;和相关的 VirtualService，从 Sidecar 路由到网关服务（istio-egressgateway.istio-system.svc.cluster.local），以及从网关路由到外部服务。请注意，VirtualService 被导出到所有命名空间，使它们能够通过网关将流量路由到外部服务。迫使流量通过像这样一个受管理的中间代理是一种常见的做法。\napiVersion:networking.istio.io/v1alpha3kind:VirtualServicemetadata:name:gateway-routingnamespace:egressspec:hosts:- httpbin.comexportTo:- \u0026#34;*\u0026#34;gateways:# 同时应用于网格内的网关和所有 sidecar- mesh- istio-egressgatewayhttp:- match:- port:80gateways:- mesh# 应用于网格内的所有 sidecarroute:- destination:host:istio-egressgateway.istio-system.svc.cluster.local- match:- port:80gateways:- istio-egressgateway# 仅应用于网关route:- destination:host:httpbin.com下面的例子演示了在外部服务的主机中使用通配符。如果连接必须被路由到应用程序请求的 IP 地址（即应用程序解析 DNS 并试图连接到一个特定的 IP），发现模式必须被设置为 NONE。\napiVersion:networking.istio.io/v1alpha3kind:ServiceEntrymetadata:name:external-svc-wildcard-examplespec:hosts:- \u0026#34;*.bar.com\u0026#34;location:MESH_EXTERNALports:- number:80name:httpprotocol:HTTPresolution:NONE下面的例子演示了一个通过客户主机上的 Unix 域套接字提供的服务。解析必须设置为 STATIC 以使用 Unix 地址端点。\napiVersion:networking.istio.io/v1alpha3kind:ServiceEntrymetadata:name:unix-domain-socket-examplespec:hosts:- \u0026#34;example.unix.local\u0026#34;location:MESH_EXTERNALports:- number:80name:httpprotocol:HTTPresolution:STATICendpoints:- address:unix:///var/run/example/socket对于基于 HTTP 的服务，可以创建一个由多个 DNS 可寻址端点支持的 VirtualService。在这种情况下，应用程序可以使用 HTTP_PROXY 环境变量来透明地将 VirtualService 的 API 调用重新路由到所选择的后端。例如，下面的配置创建了一个不存在的外部服务，名为foo.bar.com，由三个域名支持：us.foo.bar.com:8080，uk.foo.bar.com:9080和in.foo.bar.com:7080。\napiVersion:networking.istio.io/v1alpha3kind:ServiceEntrymetadata:name:external-svc-dnsspec:hosts:- foo.bar.comlocation:MESH_EXTERNALports:- number:80name:httpprotocol:HTTPresolution:DNSendpoints:- address:us.foo.bar.comports:http:8080- address:uk.foo.bar.comports:http:9080- address:in.foo.bar.comports:http:7080有了HTTP_PROXY=http://localhost/，从应用程序到 http://foo.bar.com 的调用将在上面指定的三个域中进行负均衡。换句话说，对 http://foo.bar.com/baz 的调用将被转译成 http://uk.foo.bar.com/baz。\n下面的例子说明了包含主题（subject）替代名称的 ServiceEntry 的用法，其格式符合 SPIFFE 标准。\napiVersion:networking.istio.io/v1alpha3kind:ServiceEntrymetadata:name:httpbinnamespace :httpbin-nsspec:hosts:- httpbin.comlocation:MESH_INTERNALports:- number:80name:httpprotocol:HTTPresolution:STATICendpoints:- address:2.2.2.2- address:3.3.3.3subjectAltNames:- \u0026#34;spiffe://cluster.local/ns/httpbin-ns/sa/httpbin-service-account\u0026#34;下面的例子演示了使用带有workloadSelector的ServiceEntry来处理服务details.bookinfo.com从 VM 到 Kubernetes 的 …","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"ae4a7837a6f5193d127b732f1a57d428","permalink":"https://jimmysong.io/docs/istio-handbook/config-networking/service-entry/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/istio-handbook/config-networking/service-entry/","section":"istio-handbook","summary":"ServiceEntry 可以在 Istio 的内部服务注册表中添加额外的条目，这样网格中自动发","tags":null,"title":"ServiceEntry","type":"book"},{"authors":null,"categories":null,"content":"这一节将带大家了解 Istio 为数据平面自动注入 Sidecar 的详细过程。\nSidecar 注入过程概览 如 Istio 官方文档中对 Istio sidecar 注入的描述，你可以使用 istioctl 命令手动注入 Sidecar，也可以为 Kubernetes 集群自动开启 sidecar 注入，这主要用到了 Kubernetes 的准入控制器中的 webhook，参考 Istio 官网中对 Istio sidecar 注入的描述。\n   Sidecar 注入流程图  手动注入 sidecar 与自动注入 sidecar 的区别 不论是手动注入还是自动注入，sidecar 的注入过程有需要遵循如下步骤：\n Kubernetes 需要了解待注入的 sidecar 所连接的 Istio 集群及其配置； Kubernetes 需要了解待注入的 sidecar 容器本身的配置，如镜像地址、启动参数等； Kubernetes 根据 sidecar 注入模板和以上配置填充 sidecar 的配置参数，将以上配置注入到应用容器的一侧；  Istio 和 sidecar 配置保存在 istio 和 istio-sidecar-injector 这两个 ConfigMap 中，其中包含了 Go template，所谓自动 sidecar 注入就是将生成 Pod 配置从应用 YAML 文件期间转移到 mutable webhook 中。\nInit 容器 Init 容器是一种专用容器，它在应用程序容器启动之前运行，用来包含一些应用镜像中不存在的实用工具或安装脚本。\n一个 Pod 中可以指定多个 Init 容器，如果指定了多个，那么 Init 容器将会按顺序依次运行。只有当前面的 Init 容器必须运行成功后，才可以运行下一个 Init 容器。当所有的 Init 容器运行完成后，Kubernetes 才初始化 Pod 和运行应用容器。\nInit 容器使用 Linux Namespace，所以相对应用程序容器来说具有不同的文件系统视图。因此，它们能够具有访问 Secret 的权限，而应用程序容器则不能。\n在 Pod 启动过程中，Init 容器会按顺序在网络和数据卷初始化之后启动。每个容器必须在下一个容器启动之前成功退出。如果由于运行时或失败退出，将导致容器启动失败，它会根据 Pod 的 restartPolicy 指定的策略进行重试。然而，如果 Pod 的 restartPolicy 设置为 Always，Init 容器失败时会使用 RestartPolicy 策略。\n在所有的 Init 容器没有成功之前，Pod 将不会变成 Ready 状态。Init 容器的端口将不会在 Service 中进行聚集。 正在初始化中的 Pod 处于 Pending 状态，但应该会将 Initializing 状态设置为 true。Init 容器运行完成以后就会自动终止。\n关于 Init 容器的详细信息请参考 Init 容器 - Kubernetes 中文指南/云原生应用架构实践手册。\nIstio 中的 sidecar 注入过程详解 Istio 中提供了以下两种 sidecar 注入方式：\n 使用 istioctl 手动注入。 基于 Kubernetes 的 突变 webhook 准入控制器（mutating webhook addmission controller 的自动 sidecar 注入方式。  不论是手动注入还是自动注入，sidecar 的注入过程都需要遵循如下步骤：\n Kubernetes 需要了解待注入的 sidecar 所连接的 Istio 集群及其配置； Kubernetes 需要了解待注入的 sidecar 容器本身的配置，如镜像地址、启动参数等； Kubernetes 根据 sidecar 注入模板和以上配置填充 sidecar 的配置参数，将以上配置注入到应用容器的一侧；  使用下面的命令可以手动注入 sidecar。\nistioctl kube-inject -f ${YAML_FILE} | kuebectl apply -f - 该命令会使用 Istio 内置的 sidecar 配置来注入，下面使用 Istio详细配置请参考 Istio 官网。\n注入完成后您将看到 Istio 为原有 pod template 注入了 initContainer 及 sidecar proxy相关的配置。\nSidecar 注入示例分析 以 Istio 官方提供的 bookinfo 中 productpage 的 YAML 为例，关于 bookinfo 应用的详细 YAML 配置请参考 bookinfo.yaml。\n下文将从以下几个方面讲解：\n Sidecar 容器的注入 iptables 规则的创建 路由的详细过程  apiVersion:apps/v1kind:Deploymentmetadata:name:productpage-v1labels:app:productpageversion:v1spec:replicas:1selector:matchLabels:app:productpageversion:v1template:metadata:labels:app:productpageversion:v1spec:serviceAccountName:bookinfo-productpagecontainers:- name:productpageimage:docker.io/istio/examples-bookinfo-productpage-v1:1.15.0imagePullPolicy:IfNotPresentports:- containerPort:9080volumeMounts:- name:tmpmountPath:/tmpvolumes:- name:tmpemptyDir:{}再查看下 productpage 容器的 Dockerfile。\nFROMpython:3.7.4-slimCOPY requirements.txt ./RUN pip install --no-cache-dir -r requirements.txtCOPY test-requirements.txt ./RUN pip install --no-cache-dir -r test-requirements.txtCOPY productpage.py /opt/microservices/COPY tests/unit/* /opt/microservices/COPY templates /opt/microservices/templatesCOPY static /opt/microservices/staticCOPY requirements.txt /opt/microservices/ARG flood_factorENV FLOOD_FACTOR ${flood_factor:-0}EXPOSE9080WORKDIR/opt/microservicesRUN python -m unittest discoverUSER1CMD [\u0026#34;python\u0026#34;, \u0026#34;productpage.py\u0026#34;, \u0026#34;9080\u0026#34;]我们看到 Dockerfile 中没有配置 ENTRYPOINT，所以 CMD 的配置 python productpage.py 9080 将作为默认的 ENTRYPOINT，记住这一点，再看下注入 sidecar 之后的配置。\n$ istioctl kube-inject -f samples/bookinfo/platform/kube/bookinfo.yaml 我们只截取其中与 productpage 相关的 Deployment 配置中的部分 YAML 配置。\ncontainers:- image:docker.io/istio/examples-bookinfo-productpage-v1:1.15.0# 应用镜像name:productpageports:- containerPort:9080- args:- proxy- sidecar- --domain- $(POD_NAMESPACE).svc.cluster.local- --configPath- /etc/istio/proxy- --binaryPath- /usr/local/bin/envoy- --serviceCluster- productpage.$(POD_NAMESPACE)- --drainDuration- 45s- --parentShutdownDuration- 1m0s- --discoveryAddress- istiod.istio-system.svc:15012- --zipkinAddress- zipkin.istio-system:9411- --proxyLogLevel=warning- --proxyComponentLogLevel=misc:error- --connectTimeout- 10s- --proxyAdminPort- \u0026#34;15000\u0026#34;- --concurrency- \u0026#34;2\u0026#34;- --controlPlaneAuthPolicy- NONE- --dnsRefreshRate- 300s- --statusPort- \u0026#34;15020\u0026#34;- --trust-domain=cluster.local- --controlPlaneBootstrap=falseimage:docker.io/istio/proxyv2:1.5.1# sidecar proxyname:istio-proxyports:- containerPort:15090name:http-envoy-promprotocol:TCPinitContainers:- command:- istio-iptables- -p- \u0026#34;15001\u0026#34;- -z- \u0026#34;15006\u0026#34;- -u- \u0026#34;1337\u0026#34;- -m- REDIRECT- -i- \u0026#39;*\u0026#39;- -x- \u0026#34;\u0026#34;- -b- \u0026#39;*\u0026#39;- -d- 15090,15020image:docker.io/istio/proxyv2:1.5.1# init 容器name:istio-initIstio 给应用 Pod 注入的配置主要包括：\n Init 容器 istio-init：用于 pod 中设置 iptables 端口转发 Sidecar 容器 istio-proxy：运行 sidecar 代理，如 Envoy 或 MOSN。  接下来将分别解析下这两个容器。\nInit 容器解析 Istio 在 pod 中注入的 Init 容器名为 istio-init，我们在上面 Istio 注入完成后的 YAML 文件中看到了该容器的启动命令是：\nistio-iptables -p 15001 -z 15006 -u 1337 -m REDIRECT -i \u0026#39;*\u0026#39; -x \u0026#34;\u0026#34; -b \u0026#39;*\u0026#39; -d 15090,15020 我们再检查下该容器的 Dockerfile 看看 ENTRYPOINT 是怎么确定启动时执行的命令。\n# 前面的内容省略# The pilot-agent will bootstrap Envoy.ENTRYPOINT [\u0026#34;/usr/local/bin/pilot-agent\u0026#34;]我们看到 istio-init 容器的入口是 /usr/local/bin/istio-iptables 命令行，该命令行工具的代码的位置在 Istio 源码仓库的 tools/istio-iptables 目录。\n注意：在 Istio 1.1 版本时还是使用 isito-iptables.sh 命令行来操作 IPtables。\nInit 容器启动入口 Init …","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"701101d65743d205c3818582031017bd","permalink":"https://jimmysong.io/docs/istio-handbook/concepts/istio-sidecar-injector/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/istio-handbook/concepts/istio-sidecar-injector/","section":"istio-handbook","summary":"这一节将带大家了解 Istio 为数据平面自动注入 Sidecar 的详细过程。 Sidecar 注入过","tags":null,"title":"Sidecar 自动注入过程详解","type":"book"},{"authors":null,"categories":null,"content":"为了帮助我们提高服务的弹性，我们可以使用故障注入功能。我们可以在 HTTP 流量上应用故障注入策略，在转发目的地的请求时指定一个或多个故障注入。\n有两种类型的故障注入。我们可以在转发前延迟（delay）请求，模拟缓慢的网络或过载的服务，我们可以中止（abort） HTTP 请求，并返回一个特定的 HTTP 错误代码给调用者。通过中止，我们可以模拟一个有故障的上游服务。\n下面是一个中止 HTTP 请求并返回 HTTP 404 的例子，针对 30% 的传入请求。\n- route:- destination:host:customers.default.svc.cluster.localsubset:v1fault:abort:percentage:value:30httpStatus:404如果我们不指定百分比，所有的请求将被中止。请注意，故障注入会影响使用该 VirtualService 的服务。它并不影响该服务的所有消费者。\n同样地，我们可以使用 fixedDelay 字段对请求应用一个可选的延迟。\n- route:- destination:host:customers.default.svc.cluster.localsubset:v1fault:delay:percentage:value:5fixedDelay:3s上述设置将对 5% 的传入请求应用 3 秒的延迟。\n注意，故障注入将不会触发我们在路由上设置的任何重试策略。例如，如果我们注入了一个 HTTP 500 的错误，配置为在 HTTP 500 上的重试策略将不会被触发。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"c008da1d29706e8f1960d42914b3b0b6","permalink":"https://jimmysong.io/docs/istio-handbook/traffic-management/fault-injection/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/istio-handbook/traffic-management/fault-injection/","section":"istio-handbook","summary":"为了帮助我们提高服务的弹性，我们可以使用故障注入功能。我们可","tags":null,"title":"错误注入","type":"book"},{"authors":null,"categories":null,"content":"代理协议监听器过滤器（envoy.filters.listener.proxy_protocol）增加了对 HAProxy 代理协议的支持。\n代理使用其 IP 堆栈连接到远程服务器，并丢失初始连接的源和目的地信息。PROXY 协议允许我们在不丢失客户端信息的情况下链接代理。该协议定义了一种在主 TCP 流之前通过 TCP 通信连接的元数据的方式。元数据包括源 IP 地址。\n使用这个过滤器，Envoy 可以从 PROXY 协议中获取元数据，并将其传播到 x-forwarded-for 头中。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"b173a15f77637d160c7795e5803e3be3","permalink":"https://jimmysong.io/docs/envoy-handbook/listener/proxy-protocol-listener-filter/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/envoy-handbook/listener/proxy-protocol-listener-filter/","section":"envoy-handbook","summary":"代理协议监听器过滤器（envoy.filters.liste","tags":null,"title":"代理协议监听器过滤器","type":"book"},{"authors":null,"categories":null,"content":"为了演示弹性功能，我们将在产品目录服务部署中添加一个名为 EXTRA_LATENCY 的环境变量。这个变量会在每次调用服务时注入一个额外的休眠。\n通过运行 kubectl edit deploy productcatalogservice 来编辑产品目录服务部署。这将打开一个编辑器。滚动到有环境变量的部分，添加 EXTRA_LATENCY 环境变量。\n...spec:containers:- env:- name:EXTRA_LATENCYvalue:6s...保存并推出编辑器。\n如果我们刷新页面，我们会发现页面需要 6 秒的时间来加载）——那是由于我们注入的延迟。\n让我们给产品目录服务添加一个 2 秒的超时。\napiVersion:networking.istio.io/v1alpha3kind:VirtualServicemetadata:name:productcatalogservicespec:hosts:- productcatalogservicehttp:- route:- destination:host:productcatalogservicetimeout:2s将上述 YAML 保存为 productcatalogservice-timeout.yaml，并使用 kubectl apply -f productcatalogservice-timeout.yaml 创建 VirtualService。\n如果我们刷新页面，我们会注意到一个错误信息的出现：\nrpc error: code = Unavailable desc = upstream request timeout could not retrieve products 该错误表明对产品目录服务的请求超时了。我们修改了服务，增加了 6 秒的延迟，并将超时设置为 2 秒。\n让我们定义一个重试策略，有三次尝试，每次尝试的超时为 1 秒。\napiVersion:networking.istio.io/v1alpha3kind:VirtualServicemetadata:name:productcatalogservicespec:hosts:- productcatalogservicehttp:- route:- destination:host:productcatalogserviceretries:attempts:3perTryTimeout:1s由于我们在产品目录服务部署中留下了额外的延迟，我们仍然会看到错误。让我们打开 Zipkin 中的追踪，看看重试策略的作用。\n使用 getmesh istioctl dash zipkin 来打开 Zipkin 仪表盘。点击 + 按钮，选择 serviceName 和 frontend.default。为了只得到至少一秒钟的响应（这就是我们的 perTryTimeout），选择 minDuration，在文本框中输入 1s。点击搜索按钮，显示所有追踪。\n点击 Filter 按钮，从下拉菜单中选择 productCatalogService.default。你应该看到花了 1 秒钟的 trace。这些 trace 对应于我们之前定义的 perTryTimeout。\n   Zipkin 中的 trace  运行 kubectl delete vs productcatalogservice 删除 VirtualService。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"01a3c76d872f3018987b4fc516078572","permalink":"https://jimmysong.io/docs/istio-handbook/practice/resiliency/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/istio-handbook/practice/resiliency/","section":"istio-handbook","summary":"为了演示弹性功能，我们将在产品目录服务部署中添加一个名为 EXTRA_LATENCY 的","tags":null,"title":"弹性","type":"book"},{"authors":null,"categories":null,"content":"/listeners 端点列出了所有配置的监听器。这包括名称以及每个监听器的地址和监听的端口。\n例如：\n$ curl localhost:9901/listeners http_8080::0.0.0.0:8080 http_hello_world_9090::0.0.0.0:9090 对于 JSON 输出，我们可以在 URL 上附加 ?format=json。\n$ curl localhost:9901/listeners?format=json { \u0026#34;listener_statuses\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;http_8080\u0026#34;, \u0026#34;local_address\u0026#34;: { \u0026#34;socket_address\u0026#34;: { \u0026#34;address\u0026#34;: \u0026#34;0.0.0.0\u0026#34;, \u0026#34;port_value\u0026#34;: 8080 } } }, { \u0026#34;name\u0026#34;: \u0026#34;http_hello_world_9090\u0026#34;, \u0026#34;local_address\u0026#34;: { \u0026#34;socket_address\u0026#34;: { \u0026#34;address\u0026#34;: \u0026#34;0.0.0.0\u0026#34;, \u0026#34;port_value\u0026#34;: 9090 } } } ] } 监听器排空 发生排空（draining）的一个典型场景是在热重启排空期间。它涉及到在 Envoy 进程关闭之前，通过指示监听器停止接受传入的请求来减少打开连接的数量。\n默认情况下，如果我们关闭 Envoy，所有的连接都会立即关闭。要进行优雅的关闭（即不关闭现有的连接），我们可以使用 /drain_listeners 端点，并加入一个可选的 graceful 查询参数。\nEnvoy 根据通过 --drain-time-s 和 --drain-strategy 指定的配置来排空连接。\n如果没有提供，排空时间默认为 10 分钟（600 秒）。该值指定了 Envoy 将排空连接的时间——即在关闭它们之前等待多久。\n排空策略参数决定了排空序列中的行为（例如，在热重启期间），连接是通过发送 “Connection:CLOSE”（HTTP/1.1）或 GOAWAY 帧（HTTP/2）。\n有两种支持的策略：渐进（默认）和立即。当使用渐进策略时，随着排空时间的推移，排空的请求的百分比慢慢增加到 100%。即时策略将使所有的请求在排空序列开始后立即排空。\n排空是按监听器进行的。然而，它必须在网络过滤器层面得到支持。目前支持优雅排空的过滤器是 Redis、Mongo 和 HTTP 连接管理器。\n端点的另一个选项是使用 inboundonly 查询参数（例如，/drain_listeners?inboundonly）排空所有入站监听器的能力。这使用监听器上的 traffic_direction 字段来确定流量方向。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"021c0f02913fc89ea7eb9f5d6d291768","permalink":"https://jimmysong.io/docs/envoy-handbook/admin-interface/listeners-and-draining/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/envoy-handbook/admin-interface/listeners-and-draining/","section":"envoy-handbook","summary":"/listeners 端点列出了所有配置的监听器。这包括名称以及每个监听器的地址","tags":null,"title":"监听器和监听器的排空","type":"book"},{"authors":null,"categories":null,"content":"本实验涵盖了如何配置 Envoy 使用独立的 gRPC 访问日志服务（ALS）。我们将使用一个基本的 gRPC 服务器，它实现了 StreamAccessLogs 函数，并将收到的日志从 Envoy 输出到标准输出。\n让我们先把 ALS 服务器作为一个 Docker 容器来运行。\ndocker run -dit -p 5000:5000 gcr.io/tetratelabs/envoy-als:0.1.0 ALS 服务器默认监听端口为 5000，所以如果我们看一下 Docker 容器的日志，它们应该类似于下面的内容。\n$ docker logs [container-id] Creating new ALS server 2021/11/05 20:24:03 Listening on :5000 输出告诉我们，ALS 正在监听 5000 端口。\n在 Envoy 配置中，有两个要求需要配置。首先是使用 access_log 字段的访问日志和一个名为 HttpGrpcAccessLogConfig 的日志类型。其次，在访问日志配置中，我们必须引用 gRPC 服务器。为此定义一个 Envoy 集群。\n下面是配置记录器并指向名为 grpc_als_cluster的 Envoy 集群的片段。\n...access_log:- name:envoy.access_loggers.http_grpctyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.access_loggers.grpc.v3.HttpGrpcAccessLogConfigcommon_config:log_name:\u0026#34;mygrpclog\u0026#34;transport_api_version:V3grpc_service:envoy_grpc:cluster_name:grpc_als_cluster...下一个片段是集群配置，在这一点上我们应该已经很熟悉了。在我们的例子中，我们在同一台机器上运行 gRPC 服务器，端口为 5000。\n...clusters:- name:grpc_als_clusterconnect_timeout:5stype:STRICT_DNShttp2_protocol_options:{}load_assignment:cluster_name:grpc_als_clusterendpoints:- lb_endpoints:- endpoint:address:socket_address:address:127.0.0.1port_value:5000...让我们把这两块放在一起，得出一个使用 gRPC 访问日志服务的 Envoy 配置示例。\nstatic_resources:listeners:- name:listener_0address:socket_address:address:0.0.0.0port_value:10000filter_chains:- filters:- name:envoy.filters.network.http_connection_managertyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManagerstat_prefix:ingress_httpaccess_log:- name:envoy.access_loggers.http_grpctyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.access_loggers.grpc.v3.HttpGrpcAccessLogConfigcommon_config:log_name:\u0026#34;mygrpclog\u0026#34;transport_api_version:V3grpc_service:envoy_grpc:cluster_name:grpc_als_clusterhttp_filters:- name:envoy.filters.http.routerroute_config:name:my_first_routevirtual_hosts:- name:direct_response_servicedomains:[\u0026#34;*\u0026#34;]routes:- match:prefix:\u0026#34;/404\u0026#34;direct_response:status:404body:inline_string:\u0026#34;404\u0026#34;- match:prefix:\u0026#34;/\u0026#34;direct_response:status:200body:inline_string:\u0026#34;200\u0026#34;clusters:- name:grpc_als_clusterconnect_timeout:5stype:STRICT_DNShttp2_protocol_options:{}load_assignment:cluster_name:grpc_als_clusterendpoints:- lb_endpoints:- endpoint:address:socket_address:address:127.0.0.1port_value:5000将上述 YAML 保存为 6-lab-2-grpc-als.yaml，并使用 func-e 启动 Envoy 代理。\nfunc-e run -c 6-lab-2-grpc-als.yaml \u0026amp; 我们正在后台运行 Docker 容器和 Envoy，所以我们现在可以用 curl 向 Envoy 代理发送几个请求。\n$ curl localhost:10000 200 代理的回应是 200，因为那是我们在配置中所定义的。你会注意到没有任何日志输出到标准输出，这是预期的。\n要看到这些日志，我们必须看一下 Docker 容器的日志。你可以使用 docker ps 来获取容器 ID，然后运行 logs 命令。\n$ docker logs 96f Creating new ALS server 2021/11/05 20:24:03 Listening on :5000 2021/11/05 20:33:52 Received value 2021/11/05 20:33:52 {\u0026#34;identifier\u0026#34;:{\u0026#34;node\u0026#34;:{\u0026#34;userAgentName\u0026#34;:\u0026#34;envoy\u0026#34;,\u0026#34;userAgentBuildVersion\u0026#34;:{\u0026#34;version\u0026#34;:{\u0026#34;majorNumber\u0026#34;:1,\u0026#34;minorNumber\u0026#34;:20},\u0026#34;metadata\u0026#34;:{\u0026#34;fields\u0026#34;:{\u0026#34;build.type\u0026#34;:{\u0026#34;stringValue\u0026#34;:\u0026#34;RELEASE\u0026#34;},\u0026#34;revision.sha\u0026#34;:{\u0026#34;stringValue\u0026#34;:\u0026#34;96701cb24611b0f3aac1cc0dd8bf8589fbdf8e9e\u0026#34;},\u0026#34;revision.status\u0026#34;:{\u0026#34;stringValue\u0026#34;:\u0026#34;Clean\u0026#34;},\u0026#34;ssl.version\u0026#34;:{\u0026#34;stringValue\u0026#34;:\u0026#34;BoringSSL\u0026#34;}}}},\u0026#34;extensions\u0026#34;:[{\u0026#34;name\u0026#34;:\u0026#34;envoy.matching.common_inputs.environment_variable\u0026#34;,\u0026#34;category\u0026#34;:\u0026#34;envoy.matching.common_inputs\u0026#34;},{\u0026#34;name\u0026#34;:\u0026#34;envoy.access_loggers.file\u0026#34;,\u0026#34;category\u0026#34;:\u0026#34;envoy.access_loggers\u0026#34;},{\u0026#34;name\u0026#34;:\u0026#34;envoy.access_loggers.http_grpc\u0026#34;,\u0026#34;category\u0026#34;:\u0026#34;envoy.access_loggers\u0026#34;},{\u0026#34;name\u0026#34;:\u0026#34;envoy.access_loggers.open_telemetry\u0026#34;,\u0026#34;category\u0026#34;:\u0026#34;envoy.access_loggers\u0026#34;},{\u0026#34;name\u0026#34;:\u0026#34;envoy.access_loggers.stderr\u0026#34;,\u0026#34;category\u0026#34;:\u0026#34;envoy.access_loggers\u0026#34;},{\u0026#34;name\u0026#34;:\u0026#34;envoy.access_loggers.stdout\u0026#34;,\u0026#34;category\u0026#34;:\u0026#34;envoy.access_loggers\u0026#34;},{\u0026#34;name\u0026#34;:\u0026#34;envoy.acc... ... 然后我们会注意到从 Envoy 代理发送到我们 gRPC 服务器的日志条目。gRPC 服务器中的代码很简单，只是将收到的值转换为一个字符串并输出。\n下面是完整的 StreamAccessLogs 函数的样子。\nfunc (s *server) StreamAccessLogs(stream v3.AccessLogService_StreamAccessLogsServer) error { for { in, err := stream.Recv() log.Println(\u0026#34;Received value\u0026#34;) if err == io.EOF { return nil } if err != nil { return err } str, _ := s.marshaler.MarshalToString(in) log.Println(str) } } 在这一点上，我们可以从收到的数据流中解析具体的数值，并决定如何格式化它们以及将它们发送到哪里。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"7d3c11c657cd0c4fea99b76ace154544","permalink":"https://jimmysong.io/docs/envoy-handbook/logging/lab13/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/envoy-handbook/logging/lab13/","section":"envoy-handbook","summary":"本实验涵盖了如何配置 Envoy 使用独立的 gRPC 访问日志服务（ALS）。我","tags":null,"title":"实验 13：使用 gRPC 访问日志服务（ALS）记录日志","type":"book"},{"authors":null,"categories":null,"content":"在这个实验中，我们将演示如何使用断路器。我们将运行一个 Python HTTP 服务器和一个 Envoy 代理在它前面。要启动在 8000 端口监听的 Python 服务器，请运行：\npython3 -m http.server 8000 接下来，我们将用下面的断路器创建 Envoy 配置：\n...circuit_breakers:thresholds:max_connections:20max_requests:100max_pending_requests:20因此，如果我们超过了 20 个连接或 100 个请求或 20 个待处理请求，断路器就会断开。下面是完整的 Envoy 配置：\nstatic_resources:listeners:- name:listener_0address:socket_address:address:0.0.0.0port_value:10000filter_chains:- filters:- name:envoy.filters.network.http_connection_managertyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManagerstat_prefix:listener_httphttp_filters:- name:envoy.filters.http.routerroute_config:name:routevirtual_hosts:- name:vhdomains:[\u0026#34;*\u0026#34;]routes:- match:prefix:\u0026#34;/\u0026#34;route:cluster:python_serverclusters:- name:python_serverconnect_timeout:5scircuit_breakers:thresholds:max_connections:20max_requests:100max_pending_requests:20load_assignment:cluster_name:python_serverendpoints:- lb_endpoints:- endpoint:address:socket_address:address:127.0.0.1port_value:8000admin:address:socket_address:address:127.0.0.1port_value:9901将上述配置保存为 3-lab-1-circuit-breaker.yaml，然后运行 Envoy 代理。\nfunc-e run -c 3-lab-1-circuit-breaker.yaml 为了向代理发送多个并发请求，我们将使用一个名为 hey 的工具。默认情况下，hey 运行 50 个并发，发送 200 个请求，所以我们在请求 http://localhost:1000，甚至不需要传入任何参数。\nhey http://localhost:10000 ... Status code distribution: [200] 104 responses [503] 96 responses hey 将输出许多统计数字，但我们感兴趣的是状态码的分布。它显示我们在收到了 104 个 HTTP 200 响应，在 96 个 HTTP 503 响应 —— 这就是断路器断开的地方。\n我们可以使用 Envoy 的管理接口（运行在 9901 端口）来查看详细的指标，比如说：\n... envoy_cluster_upstream_cx_overflow{envoy_cluster_name=\u0026#34;python_server\u0026#34;} 104 envoy_cluster_upstream_rq_pending_overflow{envoy_cluster_name=\u0026#34;python_server\u0026#34;} 96  下一章   ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"3ee88c536e27312cf75f3f0bed9eaf05","permalink":"https://jimmysong.io/docs/envoy-handbook/cluster/lab7/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/envoy-handbook/cluster/lab7/","section":"envoy-handbook","summary":"在这个实验中，我们将演示如何使用断路器。我们将运行一个 Python HTTP 服","tags":null,"title":"实验7：断路器","type":"book"},{"authors":null,"categories":null,"content":"EDS 只是 Envoy 中众多的服务发现方式的一种。要想了解 EDS 首先我们需要先知道什么是 Endpoint。\nEndpoint\nEndpoint 即上游主机标识。它的数据结构如下：\n{ \u0026#34;address\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;health_check_config\u0026#34;: \u0026#34;{...}\u0026#34; } 其中包括端点的地址和健康检查配置。详情请参考 Endpoints。\n终端发现服务（EDS）是一个基于 gRPC 或 REST-JSON API 服务器的 xDS 管理服务，在 Envoy 中用来获取集群成员。集群成员在 Envoy 的术语中被称为“终端”。对于每个集群，Envoy 都会通过发现服务来获取成员的终端。由于以下几个原因，EDS 是首选的服务发现机制：\n Envoy 对每个上游主机都有明确的了解（与通过 DNS 解析的负载均衡进行路由相比而言），并可以做出更智能的负载均衡决策。 在每个主机的发现 API 响应中携带的额外属性通知 Envoy 负载均衡权重、金丝雀状态、区域等。这些附加属性在负载均衡、统计信息收集等过程中会被 Envoy 网格全局使用。  Envoy 提供了 Java 和 Go 语言版本的 EDS 和其他发现服务的参考 gRPC 实现。\n通常，主动健康检查与最终一致的服务发现服务数据结合使用，以进行负载均衡和路由决策。\n参考  服务发现 - cloudnative.to  ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"2e51e17d8b6e89a1785330fc9902e5c6","permalink":"https://jimmysong.io/docs/istio-handbook/data-plane/envoy-eds/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/istio-handbook/data-plane/envoy-eds/","section":"istio-handbook","summary":"EDS 只是 Envoy 中众多的服务发现方式的一种。要想了解 EDS 首先我们需要先","tags":null,"title":"EDS（端点发现服务）","type":"book"},{"authors":null,"categories":null,"content":"在 Pod 中的 init 容器启动之后，就向 Pod 中增加了 iptables 规则，本文将向你介绍 Istio 中的基于 iptables 的透明流量劫持的全过程。\n查看 iptables nat 表中注入的规则 Init 容器通过向 iptables nat 表中注入转发规则来劫持流量的，下图显示的是 productpage 服务中的 iptables 流量劫持的详细过程。\n   Envoy sidecar 流量劫持流程示意图  Init 容器启动时命令行参数中指定了 REDIRECT 模式，因此只创建了 NAT 表规则，接下来我们查看下 NAT 表中创建的规则，这是全文中的重点部分，前面讲了那么多都是为它做铺垫的。\nproductpage 访问 reviews Pod，入站流量处理过程对应于图示上的步骤：1、2、3、4、Envoy Inbound Handler、5、6、7、8、应用容器。\nreviews Pod 访问 rating 服务的出站流量处理过程对应于图示上的步骤是：9、10、11、12、Envoy Outbound Handler、13、14、15。\n上图中关于流量路由部分，包含：\n productpage 服务请求访问 http://reviews.default.svc.cluster.local:9080/，当流量进入 reviews Pod 内部时，流量是如何被 iptables 劫持到 Envoy 代理被 Inbound Handler 处理的； reviews 请求访问 ratings 服务的 Pod，应用程序发出的出站流量被 iptables 劫持到 Envoy 代理的 Outbound Handler 的处理。  在阅读下文时，请大家确立以下已知点：\n 首先，productpage 发出的对 reivews 的访问流量，是在 Envoy 已经通过 EDS 选择出了要请求的 reviews 服务的某个 Pod，知晓了其 IP 地址，直接向该 IP 发送的 TCP 连接请求。 reviews 服务有三个版本，每个版本有一个实例，三个版本中的 sidecar 工作步骤类似，下文只以其中一个 Pod 中的 sidecar 流量转发步骤来说明。 所有进入 reviews Pod 的 TCP 流量都根据 Pod 中的 iptables 规则转发到了 Envoy 代理的 15006 端口，然后经过 Envoy 的处理确定转发给 Pod 内的应用容器还是透传。  iptables 规则注入解析 为了查看 iptables 配置，我们需要登陆到 sidecar 容器中使用 root 用户来查看，因为 kubectl 无法使用特权模式来远程操作 docker 容器，所以我们需要登陆到 productpage pod 所在的主机上使用 docker 命令登陆容器中查看。\n如果您使用 minikube 部署的 Kubernetes，可以直接登录到 minikube 的虚拟机中并切换为 root 用户。查看 iptables 配置，列出 NAT（网络地址转换）表的所有规则，因为在 Init 容器启动的时候选择给 istio-iptables 传递的参数中指定将入站流量重定向到 sidecar 的模式为 REDIRECT，因此在 iptables 中将只有 NAT 表的规格配置，如果选择 TPROXY 还会有 mangle 表配置。iptables 命令的详细用法请参考 iptables 命令。\n我们仅查看与 productpage 有关的 iptables 规则如下，因为这些规则是运行在该容器特定的网络空间下，因此需要使用 nsenter 命令进入其网络空间。进入的时候需要指定进程 ID（PID），因此首先我们需要找到 productpage 容器的 PID。对于在不同平台上安装的 Kubernetes，查找容器的方式会略有不同，例如在 GKE 上，执行 docker ps -a 命令是查看不到任何容器进程的。下面已 minikube 和 GKE 两个典型的平台为例，指导你如何进入容器的网络空间。\n在 minikube 中查看容器中的 iptabes 规则 对于 minikube，因为所有的进程都运行在单个节点上，因此你只需要登录到 minikube 虚拟机，切换为 root 用户然后查找 productpage 进程即可，参考下面的步骤。\n# 进入 minikube 并切换为 root 用户，minikube 默认用户为 docker $ minikube ssh $ sudo -i # 查看 productpage pod 的 istio-proxy 容器中的进程 $ docker top `docker ps|grep \u0026#34;istio-proxy_productpage\u0026#34;|cut -d \u0026#34; \u0026#34; -f1` UID PID PPID C STIME TTY TIME CMD 1337 10576 10517 0 08:09 ? 00:00:07 /usr/local/bin/pilot-agent proxy sidecar --domain default.svc.cluster.local --configPath /etc/istio/proxy --binaryPath /usr/local/bin/envoy --serviceCluster productpage.default --drainDuration 45s --parentShutdownDuration 1m0s --discoveryAddress istiod.istio-system.svc:15012 --zipkinAddress zipkin.istio-system:9411 --proxyLogLevel=warning --proxyComponentLogLevel=misc:error --connectTimeout 10s --proxyAdminPort 15000 --concurrency 2 --controlPlaneAuthPolicy NONE --dnsRefreshRate 300s --statusPort 15020 --trust-domain=cluster.local --controlPlaneBootstrap=false 1337 10660 10576 0 08:09 ? 00:00:33 /usr/local/bin/envoy -c /etc/istio/proxy/envoy-rev0.json --restart-epoch 0 --drain-time-s 45 --parent-shutdown-time-s 60 --service-cluster productpage.default --service-node sidecar~172.17.0.16~productpage-v1-7f44c4d57c-ksf9b.default~default.svc.cluster.local --max-obj-name-len 189 --local-address-ip-version v4 --log-format [Envoy (Epoch 0)] [%Y-%m-%d %T.%e][%t][%l][%n] %v -l warning --component-log-level misc:error --concurrency 2 # 使用 nsenter 进入 sidecar 容器的命名空间（以上任何一个都可以） $ nsenter -n --target 10660 # 查看 NAT 表中规则配置的详细信息。 $ iptables -t nat -L 在 GKE 中查看容器的 iptables 规则 如果你在 GKE 中安装的多节点的 Kubernetes 集群，首先你需要确定这个 Pod 运行在哪个节点上，然后登陆到那台主机，使用下面的命令查找进程的 PID，你会得到类似下面的输出。\n$ ps aux|grep \u0026#34;productpage\u0026#34; chronos 4268 0.0 0.6 43796 24856 ? Ss Apr22 0:00 python productpage.py 9080 chronos 4329 0.9 0.6 117524 24616 ? Sl Apr22 13:43 /usr/local/bin/python /opt/microservices/productpage.py 9080 root 361903 0.0 0.0 4536 812 pts/0 S+ 01:54 0:00 grep --colour=auto productpage 然后在终端中输出 iptables -t nat -L 即可查看 iptables 规则。\niptables 流量劫持过程详解 经过上面的步骤，你已经可以查看到 init 容器向 Pod 中注入的 iptables 规则，如下所示。\n# PREROUTING 链：用于目标地址转换（DNAT），将所有入站 TCP 流量跳转到 ISTIO_INBOUND 链上。 Chain PREROUTING (policy ACCEPT 2701 packets, 162K bytes) pkts bytes target prot opt in out source destination 2701 162K ISTIO_INBOUND tcp -- any any anywhere anywhere # INPUT 链：处理输入数据包，非 TCP 流量将继续 OUTPUT 链。 Chain INPUT (policy ACCEPT 2701 packets, 162K bytes) pkts bytes target prot opt in out source destination # OUTPUT 链：将所有出站数据包跳转到 ISTIO_OUTPUT 链上。 Chain OUTPUT (policy ACCEPT 79 packets, 6761 bytes) pkts bytes target prot opt in out source destination 15 900 ISTIO_OUTPUT tcp -- any any anywhere anywhere # POSTROUTING 链：所有数据包流出网卡时都要先进入 POSTROUTING 链，内核根据数据包目的地判断是否需要转发出去，我们看到此处未做任何处理。 Chain POSTROUTING (policy ACCEPT 79 packets, 6761 bytes) pkts bytes target prot opt in out source destination # ISTIO_INBOUND 链：将所有入站流量重定向到 ISTIO_IN_REDIRECT 链上。目的地为 15090（Prometheus 使用）和 15020（Ingress gateway 使用，用于 Pilot 健康检查）端口的流量除外，发送到以上两个端口的流量将返回 iptables 规则链的调用点，即 PREROUTING 链的后继 POSTROUTING 后直接调用原始目的地。 Chain ISTIO_INBOUND (1 references) pkts bytes target prot opt in out source destination 0 0 RETURN tcp -- any any anywhere anywhere tcp dpt:ssh 2 120 RETURN tcp -- any any anywhere anywhere tcp dpt:15090 2699 162K RETURN tcp -- …","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"8d9d4c7734e7171ccb1f14c1059f0c16","permalink":"https://jimmysong.io/docs/istio-handbook/concepts/transparent-traffic-hijacking/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/istio-handbook/concepts/transparent-traffic-hijacking/","section":"istio-handbook","summary":"在 Pod 中的 init 容器启动之后，就向 Pod 中增加了 iptables 规则，本文将向你介绍","tags":null,"title":"Istio 中的透明流量劫持过程详解","type":"book"},{"authors":null,"categories":null,"content":"Sidecar 描述了 sidecar 代理的配置，该代理负责协调与它所连接的工作负载实例的 inbound 和 outbound 通信。默认情况下，Istio 会对网格中的所有 sidecar 代理进行必要的配置，以达到网格中的每个工作负载实例，并接受与工作负载相关的所有端口的流量。Sidecar 配置提供了一种方法来微调代理在转发工作负载的流量时将接受的一组端口和协议。此外，还可以限制代理在转发工作负载实例的 outbound 流量时可以到达的服务集。\n网格中的服务和配置被组织到一个或多个命名空间（例如，Kubernetes 命名空间）。命名空间中的 Sidecar 配置将适用于同一命名空间中的一个或多个工作负载实例，并通过 workloadSelector 字段进行选择。在没有 workloadSelector 的情况下，它将适用于同一命名空间的所有工作负载实例。在确定应用于工作负载实例的 Sidecar 配置时，将优先考虑具有选择该工作负载实例的 workloadSelector 的资源，而不是没有任何 workloadSelector 的 Sidecar 配置。\n注意事项 在配置 Sidecar 时需要注意以下事项。\n注意一 每个命名空间只能有一个没有任何 workloadSelector 的 Sidecar 配置，该配置为该命名空间的所有 pod 指定了默认配置。建议对整个命名空间的 sidecar 命名为 defatult。如果在一个给定的命名空间中存在多个无选择器的 Sidecar 配置，则系统的行为未定义。如果两个或更多带有 workloadSelector 的 Sidecar 配置选择了同一个工作负载实例，那么系统的行为将无法定义。\n注意二 MeshConfig 根命名空间中的 Sidecar 配置将被默认应用于所有没有 Sidecar 配置的命名空间。这个全局默认 Sidecar 配置不应该有任何 workloadSelector。\n示例 下面的例子在根命名空间 istio-config 中声明了一个全局默认 Sidecar 配置，该配置将所有命名空间中的 sidecar 配置为只允许向同一命名空间中的其他工作负载以及 istio-system 命名空间中的服务输出流量。\napiVersion:networking.istio.io/v1alpha3kind:Sidecarmetadata:name:defaultnamespace:istio-configspec:egress:- hosts:- \u0026#34;./*\u0026#34;- \u0026#34;istio-system/*\u0026#34;下面的例子在 prod-us1 命名空间中声明了一个 Sidecar 配置，它覆盖了上面定义的全局默认值，并配置了命名空间中的 sidecar，以允许向 prod-us1、prod-apis 和 istio-system 命名空间的公开服务输出流量。\napiVersion:networking.istio.io/v1alpha3kind:Sidecarmetadata:name:defaultnamespace:prod-us1spec:egress:- hosts:- \u0026#34;prod-us1/*\u0026#34;- \u0026#34;prod-apis/*\u0026#34;- \u0026#34;istio-system/*\u0026#34;下面的例子在 prod-us1 命名空间为所有标签为 app: ratings 的 pod 声明了一个 Sidecar 配置，这些 pod 属于 rating.prod-us1 服务。该工作负载接受 9080 端口的入站 HTTP 流量。然后，该流量被转发到在 Unix 域套接字上监听的附加工作负载实例。在出口方向，除了 istio-system 命名空间外，sidecar 只为 prod-us1 命名空间的服务代理绑定在 9080 端口的 HTTP 流量。\napiVersion:networking.istio.io/v1alpha3kind:Sidecarmetadata:name:ratingsnamespace:prod-us1spec:workloadSelector:labels:app:ratingsingress:- port:number:9080protocol:HTTPname:somenamedefaultEndpoint:unix:///var/run/someuds.sockegress:- port:number:9080protocol:HTTPname:egresshttphosts:- \u0026#34;prod-us1/*\u0026#34;- hosts:- \u0026#34;istio-system/*\u0026#34;如果工作负载的部署没有基于 IPTables 的流量捕获，Sidecar 配置是配置连接到工作负载实例的代理上的端口的唯一方法。下面的例子在 prod-us1 命名空间中为属于 productpage.prod-us1 服务的标签为 app: productpage 的所有 pod 声明了一个 Sidecar 配置。假设这些 pod 的部署没有 IPtable 规则（即 istio-init 容器），并且代理元数据 ISTIO_META_INTERCEPTION_MODE 被设置为 NONE，下面的规范允许这样的 pod 在 9080 端口接收 HTTP 流量（在 Istio mutual TLS 内包装），并将其转发给在 127.0.0.1:8080 监听的应用程序。它还允许应用程序与 127.0.0.1:3306 上的 MySQL 数据库进行通信，然后被代理到 mysql.foo.com:3306 上的外部托管 MySQL 服务。\napiVersion:networking.istio.io/v1alpha3kind:Sidecarmetadata:name:no-ip-tablesnamespace:prod-us1spec:workloadSelector:labels:app:productpageingress:- port:number:9080#绑定到 proxy_instance_ip:9080（0.0.0.0:9080，如果实例没有可用的单播 IP）。protocol:HTTPname:somenamedefaultEndpoint:127.0.0.1:8080captureMode:NONE#如果为整个代理设置了元数据，则不需要。egress:- port:number:3306protocol:MYSQLname:egressmysqlcaptureMode:NONE#如果为整个代理设置了元数据，则不需要。bind:127.0.0.1hosts:- \u0026#34;*/mysql.foo.com\u0026#34;以及路由到 mysql.foo.com:3306 的相关 ServiceEntry。\napiVersion:networking.istio.io/v1alpha3kind:ServiceEntrymetadata:name:external-svc-mysqlnamespace:ns1spec:hosts:- mysql.foo.comports:- number:3306name:mysqlprotocol:MYSQLlocation:MESH_EXTERNALresolution:DNS也可以在一个代理中混合和匹配流量捕获模式。例如有一个设置，内部服务在 192.168.0.0/16 子网。因此，在虚拟机上设置 IPTables 以捕获 192.168.0.0/16 子网的所有出站流量。假设虚拟机在 172.16.0.0/16 子网有一个额外的网络接口，用于入站流量。下面的 Sidecar 配置允许虚拟机在 172.16.1.32:80（虚拟机的 IP）上为从 172.16.0.0/16 子网到达的流量暴露一个监听器。\n注意：虚拟机中代理上的 ISTIO_META_INTERCEPTION_MODE 元数据应包含 REDIRECT 或 TPROXY 作为其值，这意味着基于 IPTables 的流量捕获是激活的。\napiVersion:networking.istio.io/v1alpha3kind:Sidecarmetadata:name:partial-ip-tablesnamespace:prod-us1spec:workloadSelector:labels:app:productpageingress:- bind:172.16.1.32port:number:80#绑定到 172.16.1.32:80protocol:HTTPname:somenamedefaultEndpoint:127.0.0.1:8080captureMode:NONEegress:#使用系统检测到的默认值设置配置，以处理到 192.168.0.0/16 子网的服务的出站流量，基于服务注册表提供的信息- captureMode:IPTABLEShosts:- \u0026#34;*/*\u0026#34;配置项 下图是 Sidecar 资源的配置拓扑图。\n  Sidecar 资源配置拓扑图  Sidecar 资源的顶级配置项如下：\n workloadSelector：用于选择应用该 Sidecar 配置的特定 pod/VM 集合的标准。如果省略，Sidecar 配置将被应用于同一命名空间的所有工作负载实例。 ingress：Ingress 指明用于处理连接工作负载实例的入站流量的 sidecar 配置。如果省略，Istio 将根据从编排平台获得的工作负载信息（例如，暴露的端口、服务等）自动配置 sidecar。如果指定，当且仅当工作负载实例与服务相关联时，将配置入站端口。 egress：Egress 指定 sidecar 的配置，用于处理从附加工作负载实例到网格中其他服务的出站流量。如果不指定，则继承系统检测到的全命名空间或全局默认 Sidecar 的默认值。 outboundTrafficPolicy：出站流量策略的配置。如果你的应用程序使用一个或多个事先不知道的外部服务，将策略设置为ALLOW_ANY，将导致 sidecar 将任何来自应用程序的未知流量路由到其请求的目的地。如果没有指定，则继承系统检测到的来自全命名空间或全局默认 Sidecar 的默认值。  关于 Sidecar 配置的详细用法请参考 Istio 官方文档。\n参考  Sidecar - Istio 官网文档  ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"6977824532274584a8d44e8a853a299a","permalink":"https://jimmysong.io/docs/istio-handbook/config-networking/sidecar/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/istio-handbook/config-networking/sidecar/","section":"istio-handbook","summary":"Sidecar 描述了 sidecar 代理的配置，该代理负责协调与它所连接的工作负载实例","tags":null,"title":"Sidecar","type":"book"},{"authors":null,"categories":null,"content":"TLS 监听器过滤器让我们可以检测到传输的是 TLS 还是明文。 如果传输是 TLS，它会检测服务器名称指示（SNI）和 / 或客户端的应用层协议协商（ALPN）。\n什么是 SNI？\nSNI 或服务器名称指示（Server Name Indication）是对 TLS 协议的扩展，它告诉我们在 TLS 握手过程的开始，哪个主机名正在连接。我们可以使用 SNI 在同一个 IP 地址和端口上提供多个 HTTPS 服务（使用不同的证书）。如果客户端以主机名 “hello.com” 进行连接，服务器可以出示该主机名的证书。同样地，如果客户以 “example.com” 连接，服务器就会提供该证书。\n什么是 ALPN？\nALPN 或应用层协议协商是对 TLS 协议的扩展，它允许应用层协商应该在安全连接上执行哪种协议，而无需进行额外的往返请求。使用 ALPN，我们可以确定客户端使用的是 HTTP/1.1 还是 HTTP/2。\n我们可以使用 SNI 和 ALPN 值来匹配过滤器链，使用 server_names（对于 SNI）和 / 或 application_protocols（对于 ALPN）字段。\n下面的片段显示了我们如何使用 application_protocols 和 server_names 来执行不同的过滤器链。\n...listener_filters:- name:\u0026#34;envoy.filters.listener.tls_inspector\u0026#34;typed_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.listener.tls_inspector.v3.TlsInspectorfilter_chains:- filter_chain_match:application_protocols:[\u0026#34;h2c\u0026#34;]filters:- name:some_filter... - filter_chain_match:server_names:\u0026#34;something.hello.com\u0026#34;transport_socket:...filters:- name:another_filter... ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"d20eb8299fabbb694460510656f56c9e","permalink":"https://jimmysong.io/docs/envoy-handbook/listener/tls-inspector-listener-filter/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/envoy-handbook/listener/tls-inspector-listener-filter/","section":"envoy-handbook","summary":"TLS 监听器过滤器让我们可以检测到传输的是 TLS 还是明文。 如果传输是","tags":null,"title":"TLS 检查器监听器过滤器","type":"book"},{"authors":null,"categories":null,"content":"分接式过滤器（Tap Filter）的目的是根据一些匹配的属性来记录 HTTP 流量。有两种方法来配置分接式过滤器。\n  使用 Envoy 配置里面的 static_config 字段\n  使用 admin_config 字段并指定配置 ID。\n  不同的是，我们在静态配置（static_config）中一次性提供所有东西——匹配配置和输出配置。当使用管理配置（admin_config）时，我们只提供配置 ID，然后在运行时使用 /tap 管理端点来配置过滤器。\n正如我们所提到的，过滤器的配置被分成两部分：匹配配置和输出配置。\n我们可以用匹配配置指定匹配谓词，告诉分接式过滤器要分接哪些请求并写入配置的输出。\n例如，下面的片段显示了如何使用 any_match 来匹配所有的请求，无论其属性如何。\ncommon_config:static_config:match:any_match:true...我们也有一个选项，可以在请求和响应 Header、Trailer 和正文上进行匹配。\nHeader/Trailer 匹配 Header/Trailer 匹配器使用 HttpHeadersMatch  proto，在这里我们指定一个头数组来匹配。例如，这个片段匹配任何请求头 my-header 被精确设置为 hello 的请求。\ncommon_config:static_config:match:http_request_headers_match:headers:name:\u0026#34;my-header\u0026#34;string_match:exact:\u0026#34;hello\u0026#34;... 请注意，在 string_match 中，我们可以使用其他匹配器（例如 prefix、surffix、safe_regex），正如前面解释的那样。\n 正文匹配 通用请求和响应正文（body）匹配使用 HttpGenericBodyMatch 来指定字符串或二进制匹配。顾名思义，字符串匹配（string_match）是在 HTTP 正文中寻找一个字符串，而二进制匹配（binary_match）是在 HTTP 正文中寻找一串字节的位置。\n例如，如果响应体包含字符串 hello，则下面的片段可以匹配。\ncommon_config:static_config:match:http_response_generic_body_match:patterns:string_match:\u0026#34;hello\u0026#34;...匹配谓词 我们可以用 or_match、and_match 和 not_match 等匹配谓词来组合多个 Header、Trailer 和正文匹配器。\nor_match 和 and_match 使用 MatchSet 原语，描述逻辑 OR 或逻辑 AND。我们在匹配集内的 rules 字段中指定构成一个集合的规则列表。\n下面的例子显示了如何使用 and_match 来确保响应体包含 hello 这个词，以及请求头 my-header 被设置为 hello。\ncommon_config:static_config:match:and_match:rules:- http_response_generic_body_match:patterns:- string_match:\u0026#34;hello\u0026#34;- http_request_headers_match:headers:name:\u0026#34;my-header\u0026#34;string_match:exact:\u0026#34;hello\u0026#34;...如果我们想实现逻辑 OR，那么我们可以用 or_match 字段替换 and_match 字段。字段内的配置将保持不变，因为两个字段都使用 MatchSet  proto。\n让我们使用与之前相同的例子来说明 not_match 是如何工作的。假设我们想过滤所有没有设置头信息 my-header: hello 的请求，以及响应体不包括 hello 这个字符串的请求。\n下面是我们如何写这个配置。\ncommon_config:static_config:match:not_match:and_match:rules:- http_response_generic_body_match:patterns:- string_match:\u0026#34;hello\u0026#34;- http_request_headers_match:headers:name:\u0026#34;my-header\u0026#34;string_match:exact:\u0026#34;hello\u0026#34;...not_match  字段和父 match 字段一样使用 MatchPredicate  原语。匹配字段是一个递归结构，它允许我们创建复杂的嵌套匹配配置。\n这里要提到的最后一个字段是 any_match。这是一个布尔字段，当设置为 true 时，将总是匹配。\n输出配置 一旦请求被过滤出来，我们需要告诉过滤器将输出写入哪里。目前，我们可以配置一个单一的输出沉积。\n下面是一个输出配置示例。\n...output_config:sinks:- format:JSON_BODY_AS_STRINGfile_per_tap:path_prefix:tap...使用 file_per_tap，我们指定要为每个被监听的数据流输出一个文件。path_prefix 指定了输出文件的前缀。文件用以下格式命名：\n\u0026lt;path_prefix\u0026gt;_\u0026lt;id\u0026gt;.\u0026lt;pb | json\u0026gt; id 代表一个标识符，使我们能够区分流实例的记录跟踪。文件扩展名（pb 或 json）取决于格式选择。\n捕获输出的第二个选项是使用 streaming_admin 字段。这指定了 /tap 管理端点将流式传输被捕获的输出。请注意，要使用 /tap 管理端点进行输出，还必须使用 admin_config 字段配置分接式过滤器。如果我们静态地配置了分接式过滤器，我们就不会使用 /tap 端点来获取输出。\n格式选择 我们有多种输出格式的选项，指定消息的书写方式。让我们看看不同的格式，从默认格式开始，JSON_BODY_AS_BYTES。\nJSON_BODY_AS_BYTES 输出格式将消息输出为 JSON，任何响应的 body 数据将在 as_bytes 字段中，其中包含 base64 编码的字符串。\n例如，下面是分接输出的示例。\n{ \u0026#34;http_buffered_trace\u0026#34;: { \u0026#34;request\u0026#34;: { \u0026#34;headers\u0026#34;: [ { \u0026#34;key\u0026#34;: \u0026#34;:authority\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;localhost:10000\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;:path\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;/\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;:method\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;GET\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;:scheme\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;http\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;user-agent\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;curl/7.64.0\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;accept\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;*/*\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;my-header\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;hello\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;x-forwarded-proto\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;http\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;x-request-id\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;67e3e8ac-429a-42fb-945b-ec25927fdcc1\u0026#34; } ], \u0026#34;trailers\u0026#34;: [] }, \u0026#34;response\u0026#34;: { \u0026#34;headers\u0026#34;: [ { \u0026#34;key\u0026#34;: \u0026#34;:status\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;200\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;content-length\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;5\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;content-type\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;text/plain\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;date\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;Mon, 29 Nov 2021 19:31:43 GMT\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;server\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;envoy\u0026#34; } ], \u0026#34;body\u0026#34;: { \u0026#34;truncated\u0026#34;: false, \u0026#34;as_bytes\u0026#34;: \u0026#34;aGVsbG8=\u0026#34; }, \u0026#34;trailers\u0026#34;: [] } } } 注意 body 中的 as_bytes 字段。该值是 body 数据的 base64 编码表示（本例中为 hello）。\n第二种输出格式是 JSON_BODY_AS_STRING。与之前的格式不同的是，在 JSON_BODY_AS_STRING 中，body 数据是以字符串的形式写在 as_string 字段中。当我们知道 body 是人类可读的，并且不需要对数据进行 base64 编码时，这种格式很有用。\n... \u0026#34;body\u0026#34;: { \u0026#34;truncated\u0026#34;: false, \u0026#34;as_string\u0026#34;: \u0026#34;hello\u0026#34; }, ... 其他三种格式类型是 PROTO_BINARY、PROTO_BINARY_LENGTH_DELIMITED 和 PROTO_TEXT。\nPROTO_BINARY 格式以二进制 proto 格式写入输出。这种格式不是自限性的，这意味着如果分接写了多个没有任何长度信息的二进制消息，那么数据流将没有用处。如果我们在每个文件中写一个消息，那么输出格式将更容易解析。\n我们也可以使用 PROTO_BINARY_LENGTH_DELIMITED 格式，其中消息被写成序列元组。每个元组是消息长度（编码为 32 位 protobuf varint 类型），后面是二进制消息。\n最后，我们还可以使用 PROTO_TEXT 格式，在这种格式下，输出结果以下面的 protobuf 格式写入。\nhttp_buffered_trace { request { headers { key: \u0026#34;:authority\u0026#34; value: \u0026#34;localhost:10000\u0026#34; } headers { key: \u0026#34;:path\u0026#34; value: \u0026#34;/\u0026#34; } headers { key: \u0026#34;:method\u0026#34; value: \u0026#34;GET\u0026#34; } headers { key: \u0026#34;:scheme\u0026#34; value: \u0026#34;http\u0026#34; } headers { key: \u0026#34;user-agent\u0026#34; value: \u0026#34;curl/7.64.0\u0026#34; } headers { key: \u0026#34;accept\u0026#34; value: \u0026#34;*/*\u0026#34; } headers { key: \u0026#34;debug\u0026#34; value: \u0026#34;true\u0026#34; } headers { key: \u0026#34;x-forwarded-proto\u0026#34; value: \u0026#34;http\u0026#34; } headers { key: \u0026#34;x-request-id\u0026#34; value: \u0026#34;af6e0879-e057-4efc-83e4-846ff4d46efe\u0026#34; } } response { headers { key: \u0026#34;:status\u0026#34; value: \u0026#34;500\u0026#34; } headers { key: \u0026#34;content-length\u0026#34; value: \u0026#34;5\u0026#34; } headers { key: \u0026#34;content-type\u0026#34; value: \u0026#34;text/plain\u0026#34; } headers { key: \u0026#34;date\u0026#34; value: \u0026#34;Mon, 29 Nov 2021 22:32:40 GMT\u0026#34; } headers { key: \u0026#34;server\u0026#34; value: \u0026#34;envoy\u0026#34; } body { as_bytes: \u0026#34;hello\u0026#34; } }}静态配置分接式过滤器 我们把匹配的配置和输出配置（使用 file_per_tap 字段）结合起来，静态地配置分接式过滤器。\n下面是一个通过静态配置来配置分接式过滤器的片段。\n- name:envoy.filters.http.taptyped_config:\u0026#34;@type\u0026#34;: …","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"882383193bf0edc8142a25a2c72e5c14","permalink":"https://jimmysong.io/docs/envoy-handbook/admin-interface/tap-filter/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/envoy-handbook/admin-interface/tap-filter/","section":"envoy-handbook","summary":"分接式过滤器（Tap Filter）的目的是根据一些匹配的属性","tags":null,"title":"分接式过滤器","type":"book"},{"authors":null,"categories":null,"content":"在这个实验中，我们将学习如何将 Envoy 应用日志发送到 Google Cloud Logging。我们将在 GCP 中运行的虚拟机（VM）实例上运行 Envoy，配置 Envoy 实例，将应用日志发送到 GCP 中的云日志。配置 Envoy 实例将允许我们在日志资源管理器中查看 Envoy 的日志，获得日志分析，并使用其他谷歌云功能。\n为了使用谷歌云的日志收集、分析和其他工具，我们需要安装云日志代理（Ops Agent）。\n在这个演示中，我们将在一个单独的虚拟机上安装 Ops 代理。另外，请注意，其他云供应商可能使用不同的日志工具和服务。\n安装 Ops 代理 在 Google Cloud，在你的地区创建一个新的虚拟机实例。一旦创建了虚拟机，我们就可以通过 SSH 进入该实例并安装 Ops 代理。\n在虚拟机实例中，运行以下命令来安装 Ops 代理。\ncurl -sSO https://dl.google.com/cloudagents/add-google-cloud-ops-agent-repo.sh sudo bash add-google-cloud-ops-agent-repo.sh --also-install 安装 Ops 代理的另一个选择是按照以下步骤进行。\n 从 GCP 的导航页面，选择监测。 在 \u0026#34;监控 \u0026#34; 导航页面，选择 \u0026#34; 仪表盘 \u0026#34;。 在仪表板表中，找到并点击虚拟机实例。 选择一个没有安装代理的实例旁边的复选框（例如，代理栏显示未检测到）。 点击安装代理按钮，在打开的窗口中，点击在在 Cloud Shell 中运行按钮，开始安装。  安装代理的命令将在 Cloud Shell 中打开。你需要做的最后一件事是按回车键开始安装。\n下面是成功安装的命令和输出在 Cloud Shell 中的样子。\n$ :\u0026gt; agents_to_install.csv \u0026amp;\u0026amp; \\ → echo \u0026#39;\u0026#34;projects/envoy-project/zones/us-west1-a/instances/envoy-instance\u0026#34;,\u0026#34;[{\u0026#34;\u0026#34;type\u0026#34;\u0026#34;:\u0026#34;\u0026#34;ops-agent\u0026#34;\u0026#34;}]\u0026#34;\u0026#39; \u0026gt;\u0026gt; agents_to_install.csv \u0026amp;\u0026amp; \\ → curl -sSO https://dl.google.com/cloudagents/mass-provision-google-cloud-ops-agents.py \u0026amp;\u0026amp; \\ → python3 mass-provision-google-cloud-ops-agents.py --file agents_to_install.csv 2021-11-03T19:04:31.577710Z Processing instance: projects/peterjs-project/zones/us-west1-a/instances/some-instance. ---------------------Getting output------------------------- Progress: |==================================================| 100.0% [1/1] (100.0%) completed; [1/1] (100.0%) succeeded; [0/1] (0.0%) failed; Instance: projects/envoy-project/zones/us-west1-a/instances/envoy-instance successfully runs ops-agent. See log file in: ./google_cloud_ops_agent_provisioning/20211103-190431_576419/envoy-project_us-west1-a_envoy-instance.log SUCCEEDED: [1/1] (100.0%) FAILED: [0/1] (0.0%) COMPLETED: [1/1] (100.0%) See script log file: ./google_cloud_ops_agent_provisioning/20211103-190431_576419/wrapper_script.log 随着安装的进展，虚拟机实例仪表板中的代理列将显示待定。一旦代理安装完成，该值将变为 Ops Agent，这表明 Ops Agent 已成功安装。\n现在我们可以通过 SSH 进入虚拟机实例，安装 func-e（用于运行 Envoy），创建一个基本的 Envoy 配置，并运行它，这样 Envoy 的应用日志就会被发送到 GCP 的云端日志。\n安装 func-e 要在虚拟机上安装 func-e，请运行：\ncurl https://func-e.io/install.sh | sudo bash -s -- -b /usr/local/bin 我们可以运行 func-e --version 来检查安装是否成功。\n发送 Envoy 应用日志到云端日志 让我们创建一个我们将在本实验中使用的原始 Envoy 配置。\nstatic_resources:listeners:- name:listener_0address:socket_address:address:0.0.0.0port_value:10000filter_chains:- filters:- name:envoy.filters.network.http_connection_managertyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManagerstat_prefix:ingress_httphttp_filters:- name:envoy.filters.http.routerroute_config:name:my_first_routevirtual_hosts:- name:direct_response_servicedomains:[\u0026#34;*\u0026#34;]routes:- match:prefix:\u0026#34;/\u0026#34;direct_response:status:200body:inline_string:\u0026#34;200\u0026#34;将上述 YAML 保存为 6-lab-3-gcp-logging.yaml。\n让我们来配置 Ops Agent，为 Envoy 创建一个新的接收器，描述如何检索日志。\nlogging:receivers:envoy:type:filesinclude_paths:- /var/log/envoy.logservice:pipelines:default_pipeline:receivers:[envoy]将上述内容保存到虚拟机实例上的 /etc/google-cloud-ops-agent/config.yaml 文件中。要重新启动 Ops Agent，请运行 sudo service google-cloud-ops-agent restart。\n在 Ops Agent 使用新配置的情况下，我们可以运行 Envoy，并告诉它把日志写到 /var/log/envoy.log 文件中，代理会在那里接收。\nsudo func-e run -c 6-lab-3-gcp-logging.yaml --log-path /var/log/envoy.log 接下来，我们可以点击日志，然后点击 GCP 中的日志资源管理器，查看虚拟机上运行的 Envoy 实例的日志。\n   GCP的日志资源管理器中的 Envoy 日志   下一章   ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"03195933182888e5e89158fd7f67bce7","permalink":"https://jimmysong.io/docs/envoy-handbook/logging/lab14/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/envoy-handbook/logging/lab14/","section":"envoy-handbook","summary":"在这个实验中，我们将学习如何将 Envoy 应用日志发送到 Google Cloud Loggi","tags":null,"title":"实验 14：将 Envoy 的日志发送到 Google Cloud Logging","type":"book"},{"authors":null,"categories":null,"content":"EnvoyFilter 提供了一种机制来定制 Istio Pilot 生成的 Envoy 配置。使用 EnvoyFilter 来修改某些字段的值，添加特定的过滤器，甚至添加全新的 listener、cluster 等。这个功能必须谨慎使用，因为不正确的配置可能破坏整个网格的稳定性。与其他 Istio 网络对象不同，EnvoyFilter 是累加应用。对于特定命名空间中的特定工作负载，可以存在任意数量的 EnvoyFilter。这些 EnvoyFilter 的应用顺序如下：配置根命名空间中的所有 EnvoyFilter，其次是工作负载命名空间中的所有匹配 EnvoyFilter。\n注意一 该 API 的某些方面与 Istio 网络子系统的内部实现以及 Envoy 的 xDS API 有很深的关系。虽然 EnvoyFilter API 本身将保持向后兼容，但通过该机制提供的任何 Envoy 配置应在 Istio 代理版本升级时仔细审查，以确保废弃的字段被适当地删除和替换。\n注意二 当多个 EnvoyFilter 被绑定到特定命名空间的同一个工作负载时，所有补丁将按照创建顺序处理。如果多个 EnvoyFilter 的配置相互冲突，则其行为将无法确定。\n注意三 要将 EnvoyFilter 资源应用于系统中的所有工作负载（sidecar 和 gateway）上，请在 config 根命名空间中定义该资源，不要使用 workloadSelector。\n示例 下面的例子在名为 istio-config 的根命名空间中声明了一个全局默认的 EnvoyFilter 资源，在系统中的所有 sidecar 上添加了一个自定义的协议过滤器，用于 outbound 端口 9307。该过滤器应在终止 tcp_proxy 过滤器之前添加，以便生效。此外，它为 gateway 和 sidecar 的所有 HTTP 连接设置了 30 秒的空闲超时。\napiVersion:networking.istio.io/v1alpha3kind:EnvoyFiltermetadata:name:custom-protocolnamespace:istio-config# 如在 meshConfig 资源中定义的。spec:configPatches:- applyTo:NETWORK_FILTERmatch:context:SIDECAR_OUTBOUND# 将会匹配所有 sidecar 中的所有 outbound listenerlistener:portNumber:9307filterChain:filter:name:\u0026#34;envoy.filters.network.tcp_proxy\u0026#34;patch:operation:INSERT_BEFOREvalue:# 这是完整的过滤器配置，包括名称和 typed_config 部分。name:\u0026#34;envoy.config.filter.network.custom_protocol\u0026#34;typed_config:...- applyTo:NETWORK_FILTER# HTTP 连接管理器是 Envoy 的一个过滤器。match:# 省略了上下文，因此这同时适用于 sidecar 和 gateway。listener:filterChain:filter:name:\u0026#34;envoy.filters.network.http_connection_manager\u0026#34;patch:operation:MERGEvalue:name:\u0026#34;envoy.filters.network.http_connection_manager\u0026#34;typed_config:\u0026#34;@type\u0026#34;: \u0026#34;type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\u0026#34;common_http_protocol_options:idle_timeout:30s下面的例子启用了 Envoy 的 Lua 过滤器，用于处理所有到达 bookinfo 命名空间中的对 reviews 服务 pod 的 8080 端口的 HTTP 调用，标签为 app: reviews。Lua 过滤器调用外部服务internal.org.net:8888，这需要在 Envoy 中定义一个特殊的 cluster。该 cluster 也被添加到 sidecar 中，作为该配置的一部分。\napiVersion:networking.istio.io/v1alpha3kind:EnvoyFiltermetadata:name:reviews-luanamespace:bookinfospec:workloadSelector:labels:app:reviewsconfigPatches:# 第一个补丁将 lua过 滤器添加到监听器/http 连接管理器。- applyTo:HTTP_FILTERmatch:context:SIDECAR_INBOUNDlistener:portNumber:8080filterChain:filter:name:\u0026#34;envoy.filters.network.http_connection_manager\u0026#34;subFilter:name:\u0026#34;envoy.filters.http.router\u0026#34;patch:operation:INSERT_BEFOREvalue:# lua 过滤器配置name:envoy.luatyped_config:\u0026#34;@type\u0026#34;: \u0026#34;type.googleapis.com/envoy.extensions.filters.http.lua.v3.Lua\u0026#34;inlineCode:|function envoy_on_request(request_handle) -- 向上游主机进行 HTTP 调用，header、body 和 time欧特 如下。 local headers, body = request_handle:httpCall( \u0026#34;lua_cluster\u0026#34;, { [\u0026#34;:method\u0026#34;] = \u0026#34;POST\u0026#34;, [\u0026#34;:path\u0026#34;] = \u0026#34;/acl\u0026#34;, [\u0026#34;:authority\u0026#34;] = \u0026#34;internal.org.net\u0026#34; }, \u0026#34;authorize call\u0026#34;, 5000) end# 第二个补丁添加了被 lua 代码引用的 cluster，cds 匹配被省略，因为正在添加一个新的 cluster。- applyTo:CLUSTERmatch:context:SIDECAR_OUTBOUNDpatch:operation:ADDvalue:# cluster 配置name:\u0026#34;lua_cluster\u0026#34;type:STRICT_DNSconnect_timeout:0.5slb_policy:ROUND_ROBINload_assignment:cluster_name:lua_clusterendpoints:- lb_endpoints:- endpoint:address:socket_address:protocol:TCPaddress:\u0026#34;internal.org.net\u0026#34;port_value:8888下面的例子覆盖了 SNI 主机 app.example.com 在 istio-system 命名空间的 ingress gateway 的监听器中的 HTTP 连接管理器的某些字段（HTTP 空闲超时和X-Forward-For信任跳数）。\napiVersion:networking.istio.io/v1alpha3kind:EnvoyFiltermetadata:name:hcm-tweaksnamespace:istio-systemspec:workloadSelector:labels:istio:ingressgatewayconfigPatches:- applyTo:NETWORK_FILTER# HTTP 连接管理器是 Envoy 中的一个过滤器。 match:context:GATEWAYlistener:filterChain:sni:app.example.comfilter:name:\u0026#34;envoy.filters.network.http_connection_manager\u0026#34;patch:operation:MERGEvalue:typed_config:\u0026#34;@type\u0026#34;: \u0026#34;type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\u0026#34;xff_num_trusted_hops:5common_http_protocol_options:idle_timeout:30s下面的例子插入了一个产生 istio_operationId 属性的 attributegen 过滤器，该属性被 istio.stats fiter 消费。filterClass:STATS 对这种依赖关系进行编码。\napiVersion:networking.istio.io/v1alpha3kind:EnvoyFiltermetadata:name:reviews-request-operationnamespace:mynsspec:workloadSelector:labels:app:reviewsconfigPatches:- applyTo:HTTP_FILTERmatch:context:SIDECAR_INBOUNDpatch:operation:ADDfilterClass:STATS# 这个过滤器将在 Istio 统计过滤器之前运行。value:name:istio.request_operationtyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/udpa.type.v1.TypedStructtype_url:type.googleapis.com/envoy.extensions.filters.http.wasm.v3.Wasmvalue:config:configuration:|{ \u0026#34;attributes\u0026#34;: [ { \u0026#34;output_attribute\u0026#34;: \u0026#34;istio_operationId\u0026#34;, \u0026#34;match\u0026#34;: [ { \u0026#34;value\u0026#34;: \u0026#34;ListReviews\u0026#34;, \u0026#34;condition\u0026#34;: \u0026#34;request.url_path == \u0026#39;/reviews\u0026#39; \u0026amp;\u0026amp; request.method == \u0026#39;GET\u0026#39;\u0026#34; }] }] }vm_config:runtime:envoy.wasm.runtime.nullcode:local:{inline_string:\u0026#34;envoy.wasm.attributegen\u0026#34;}下面的例子在 myns 命名空间中插入了一个 http ext_authz 过滤器。\napiVersion:networking.istio.io/v1alpha3kind:EnvoyFiltermetadata:name:myns-ext-authznamespace:mynsspec:configPatches:- applyTo:HTTP_FILTERmatch:context:SIDECAR_INBOUNDpatch:operation:ADDfilterClass:AUTHZ# 该过滤器将在 Istio authz 过滤器之后运行。value:name:envoy.filters.http.ext_authztyped_config:\u0026#34;@type\u0026#34;: …","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"8df10c02c27e968510d23a906da62472","permalink":"https://jimmysong.io/docs/istio-handbook/config-networking/envoy-filter/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/istio-handbook/config-networking/envoy-filter/","section":"istio-handbook","summary":"EnvoyFilter 提供了一种机制来定制 Istio Pilot 生成的 Envoy 配置。使用 EnvoyFilter 来修改某些字段","tags":null,"title":"EnvoyFilter","type":"book"},{"authors":null,"categories":null,"content":"本文以 Istio 官方的 bookinfo 示例来讲解在进入 Pod 的流量被 iptables 转交给 Envoy sidecar 后，Envoy 是如何做路由转发的，详述了 Inbound 和 Outbound 处理过程。\n下面是 Istio 官方提供的 bookinfo 的请求流程图，假设 bookinfo 应用的所有服务中没有配置 DestinationRule。\n   Bookinfo 示例  我们将要解析的是 reviews-v1 这个 Pod 中的 Inbound 和 Outbound 流量。\n理解 Inbound Handler Inbound Handler 的作用是将 iptables 拦截到的 downstream 的流量转发给 Pod 内的应用程序容器。在我们的实例中，假设其中一个 Pod 的名字是 reviews-v1-545db77b95-jkgv2，运行 istioctl proxy-config listener reviews-v1-545db77b95-jkgv2 --port 15006 查看该 Pod 中 15006 端口上的监听器情况 ，你将看到下面的输出。\nADDRESS PORT MATCH DESTINATION 0.0.0.0 15006 Addr: *:15006 Non-HTTP/Non-TCP 0.0.0.0 15006 Trans: tls; App: istio-http/1.0,istio-http/1.1,istio-h2; Addr: 0.0.0.0/0 InboundPassthroughClusterIpv4 0.0.0.0 15006 Trans: raw_buffer; App: http/1.1,h2c; Addr: 0.0.0.0/0 InboundPassthroughClusterIpv4 0.0.0.0 15006 Trans: tls; App: TCP TLS; Addr: 0.0.0.0/0 InboundPassthroughClusterIpv4 0.0.0.0 15006 Trans: raw_buffer; Addr: 0.0.0.0/0 InboundPassthroughClusterIpv4 0.0.0.0 15006 Trans: tls; Addr: 0.0.0.0/0 InboundPassthroughClusterIpv4 0.0.0.0 15006 Trans: tls; App: istio,istio-peer-exchange,istio-http/1.0,istio-http/1.1,istio-h2; Addr: *:9080 Cluster: inbound|9080|| 0.0.0.0 15006 Trans: raw_buffer; Addr: *:9080 Cluster: inbound|9080|| 下面列出了以上输出中各字段的含义：\n ADDRESS：下游地址 PORT：Envoy 监听器监听的端口 MATCH：请求使用的传输协议或匹配的下游地址 DESTINATION：路由目的地  reviews Pod 中的 Iptables 将入站流量劫持到 15006 端口上，从上面的输出我们可以看到 Envoy 的 Inbound Handler 在 15006 端口上监听，对目的地为任何 IP 的 9080 端口的请求将路由到 inbound|9080|| Cluster 上。\n从该 Pod 的 Listener 列表的最后两行中可以看到，0.0.0.0:15006/TCP 的 Listener（其实际名字是 virtualInbound）监听所有的 Inbound 流量，其中包含了匹配规则，来自任意 IP 的对 9080 端口的访问流量，将会路由到 inbound|9080|| Cluster，如果你想以 Json 格式查看该 Listener 的详细配置，可以执行 istioctl proxy-config listeners reviews-v1-545db77b95-jkgv2 --port 15006 -o json 命令，你将获得类似下面的输出。\n[ /*省略部分内容*/ { \u0026#34;name\u0026#34;: \u0026#34;virtualInbound\u0026#34;, \u0026#34;address\u0026#34;: { \u0026#34;socketAddress\u0026#34;: { \u0026#34;address\u0026#34;: \u0026#34;0.0.0.0\u0026#34;, \u0026#34;portValue\u0026#34;: 15006 } }, \u0026#34;filterChains\u0026#34;: [ /*省略部分内容*/ { \u0026#34;filterChainMatch\u0026#34;: { \u0026#34;destinationPort\u0026#34;: 9080, \u0026#34;transportProtocol\u0026#34;: \u0026#34;tls\u0026#34;, \u0026#34;applicationProtocols\u0026#34;: [ \u0026#34;istio\u0026#34;, \u0026#34;istio-peer-exchange\u0026#34;, \u0026#34;istio-http/1.0\u0026#34;, \u0026#34;istio-http/1.1\u0026#34;, \u0026#34;istio-h2\u0026#34; ] }, \u0026#34;filters\u0026#34;: [ /*省略部分内容*/ { \u0026#34;name\u0026#34;: \u0026#34;envoy.filters.network.http_connection_manager\u0026#34;, \u0026#34;typedConfig\u0026#34;: { \u0026#34;@type\u0026#34;: \u0026#34;type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\u0026#34;, \u0026#34;statPrefix\u0026#34;: \u0026#34;inbound_0.0.0.0_9080\u0026#34;, \u0026#34;routeConfig\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;inbound|9080||\u0026#34;, \u0026#34;virtualHosts\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;inbound|http|9080\u0026#34;, \u0026#34;domains\u0026#34;: [ \u0026#34;*\u0026#34; ], \u0026#34;routes\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;match\u0026#34;: { \u0026#34;prefix\u0026#34;: \u0026#34;/\u0026#34; }, \u0026#34;route\u0026#34;: { \u0026#34;cluster\u0026#34;: \u0026#34;inbound|9080||\u0026#34;, \u0026#34;timeout\u0026#34;: \u0026#34;0s\u0026#34;, \u0026#34;maxStreamDuration\u0026#34;: { \u0026#34;maxStreamDuration\u0026#34;: \u0026#34;0s\u0026#34;, \u0026#34;grpcTimeoutHeaderMax\u0026#34;: \u0026#34;0s\u0026#34; } }, \u0026#34;decorator\u0026#34;: { \u0026#34;operation\u0026#34;: \u0026#34;reviews.default.svc.cluster.local:9080/*\u0026#34; } } ] } ], \u0026#34;validateClusters\u0026#34;: false }, /*省略部分内容*/ } } ], /*省略部分内容*/ ], \u0026#34;listenerFilters\u0026#34;: [ /*省略部分内容*/ ], \u0026#34;listenerFiltersTimeout\u0026#34;: \u0026#34;0s\u0026#34;, \u0026#34;continueOnListenerFiltersTimeout\u0026#34;: true, \u0026#34;trafficDirection\u0026#34;: \u0026#34;INBOUND\u0026#34; } ] 既然 Inbound Handler 的流量中将来自任意地址的对该 Pod 9080 端口的流量路由到 inbound|9080|| Cluster，那么我们运行 istioctl pc cluster reviews-v1-545db77b95-jkgv2 --port 9080 --direction inbound -o json 查看下该 Cluster 配置，你将获得类似下面的输出。\n[ { \u0026#34;name\u0026#34;: \u0026#34;inbound|9080||\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;ORIGINAL_DST\u0026#34;, \u0026#34;connectTimeout\u0026#34;: \u0026#34;10s\u0026#34;, \u0026#34;lbPolicy\u0026#34;: \u0026#34;CLUSTER_PROVIDED\u0026#34;, \u0026#34;circuitBreakers\u0026#34;: { \u0026#34;thresholds\u0026#34;: [ { \u0026#34;maxConnections\u0026#34;: 4294967295, \u0026#34;maxPendingRequests\u0026#34;: 4294967295, \u0026#34;maxRequests\u0026#34;: 4294967295, \u0026#34;maxRetries\u0026#34;: 4294967295, \u0026#34;trackRemaining\u0026#34;: true } ] }, \u0026#34;cleanupInterval\u0026#34;: \u0026#34;60s\u0026#34;, \u0026#34;upstreamBindConfig\u0026#34;: { \u0026#34;sourceAddress\u0026#34;: { \u0026#34;address\u0026#34;: \u0026#34;127.0.0.6\u0026#34;, \u0026#34;portValue\u0026#34;: 0 } }, \u0026#34;metadata\u0026#34;: { \u0026#34;filterMetadata\u0026#34;: { \u0026#34;istio\u0026#34;: { \u0026#34;services\u0026#34;: [ { \u0026#34;host\u0026#34;: \u0026#34;reviews.default.svc.cluster.local\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;reviews\u0026#34;, \u0026#34;namespace\u0026#34;: \u0026#34;default\u0026#34; } ] } } } } ] 我们看其中的 TYPE 为 ORIGINAL_DST，将流量发送到原始目标地址（Pod IP），因为原始目标地址即当前 Pod，你还应该注意到 upstreamBindConfig.sourceAddress.address 的值被改写为了 127.0.0.6，而且对于 Pod 内流量是通过 lo 网卡发送的，这刚好呼应了上文中的 iptables ISTIO_OUTPUT 链中的第一条规则，根据该规则，流量将被透传到 Pod 内的应用容器。\n理解 Outbound Handler 在本示例中 reviews 会向 ratings 服务发送 HTTP 请求，请求的地址是：http://ratings.default.svc.cluster.local:9080/，Outbound Handler 的作用是将 iptables 拦截到的本地应用程序向外发出的流量，经由 Envoy 代理路由到上游。\nEnvoy 监听在 15001 端口上监听所有 Outbound 流量，Outbound Handler 处理，然后经过 virtualOutbound Listener、0.0.0.0_9080 Listener，然后通过 Route 9080 找到上游的 cluster，进而通过 EDS 找到 Endpoint 执行路由动作。\nratings.default.svc.cluster.local:9080 路由\n运行 istioctl proxy-config routes reviews-v1-545db77b95-jkgv2 --name 9080 -o json 查看 route 配置，因为 sidecar 会根据 HTTP header 中的 domains 来匹配 VirtualHost，所以下面只列举了 ratings.default.svc.cluster.local:9080 这一个 VirtualHost。\n[ { \u0026#34;name\u0026#34;: \u0026#34;9080\u0026#34;, \u0026#34;virtualHosts\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;ratings.default.svc.cluster.local:9080\u0026#34;, \u0026#34;domains\u0026#34;: [ \u0026#34;ratings.default.svc.cluster.local\u0026#34;, \u0026#34;ratings.default.svc.cluster.local:9080\u0026#34;, \u0026#34;ratings\u0026#34;, \u0026#34;ratings:9080\u0026#34;, \u0026#34;ratings.default.svc\u0026#34;, \u0026#34;ratings.default.svc:9080\u0026#34;, \u0026#34;ratings.default\u0026#34;, \u0026#34;ratings.default:9080\u0026#34;, \u0026#34;10.8.8.106\u0026#34;, \u0026#34;10.8.8.106:9080\u0026#34; ], \u0026#34;routes\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;match\u0026#34;: { \u0026#34;prefix\u0026#34;: \u0026#34;/\u0026#34; }, \u0026#34;route\u0026#34;: { \u0026#34;cluster\u0026#34;: …","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"38fa563aab9a11c0704587df31bedca8","permalink":"https://jimmysong.io/docs/istio-handbook/concepts/sidecar-traffic-routing-deep-dive/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/istio-handbook/concepts/sidecar-traffic-routing-deep-dive/","section":"istio-handbook","summary":"本文以 Istio 官方的 bookinfo 示例来讲解在进入 Pod 的流量被 iptables 转交给 Envoy sidecar 后，E","tags":null,"title":"Istio 中的 Sidecar 的流量路由详解","type":"book"},{"authors":null,"categories":null,"content":"SDS（秘钥发现服务）是 Envoy 1.8.0 版本起开始引入的服务。可以在 bootstrap.static_resource 的 secret 配置中为 Envoy 指定 TLS 证书（secret）。也可以通过秘钥发现服务（SDS）远程获取。Istio 预计将在 1.1 版本中支持 SDS。\nSDS 带来的最大的好处就是简化证书管理。要是没有该功能的话，我们就必须使用 Kubernetes 中的 secret 资源创建证书，然后把证书挂载到代理容器中。如果证书过期，还需要更新 secret 和需要重新部署代理容器。使用 SDS，中央 SDS 服务器将证书推送到所有 Envoy 实例上。如果证书过期，服务器只需将新证书推送到 Envoy 实例，Envoy 可以立即使用新证书而无需重新部署。\n如果 listener server 需要从远程获取证书，则 listener server 不会被标记为 active 状态，在获取证书之前不会打开其端口。如果 Envoy 由于连接失败或错误的响应数据而无法获取证书，则 listener server 将被标记为 active，并且打开端口，但是将重置与端口的连接。\n上游集群的处理方式类似，如果需要通过 SDS 从远程获取集群客户端证书，则不会将其标记为 active 状态，在获得证书之前也它不会被使用。如果 Envoy 由于连接失败或错误的响应数据而无法获取证书，则集群将被标记为 active，可以处理请求，但路由到该集群的请求都将被拒绝。\n使用 SDS 的静态集群需定义 SDS 集群（除非使用不需要集群的 Google gRPC），则必须在使用静态集群之前定义 SDS 集群。\nEnvoy 代理和 SDS 服务器之间的连接必须是安全的。可以在同一主机上运行 SDS 服务器，使用 Unix Domain Socket 进行连接。否则，需要代理和 SDS 服务器之间的 mTLS。在这种情况下，必须静态配置 SDS 连接的客户端证书。\nSDS Server SDS server 需要实现 SecretDiscoveryService 这个 gRPC 服务。遵循与其他 xDS 相同的协议。\nSDS 配置 SDS 支持静态配置也支持动态配置。\n静态配置\n可以在static_resources 的 secrets 中配置 TLS 证书。\n动态配置\n从远程 SDS server 获取 secret。\n 通过 Unix Domain Socket 访问 gRPC SDS server。 通过 UDS 访问 gRPC SDS server。 通过 Envoy gRPC 访问 SDS server。  ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"025de10fb2ee2f3d8fefd8522eeea6e8","permalink":"https://jimmysong.io/docs/istio-handbook/data-plane/envoy-sds/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/istio-handbook/data-plane/envoy-sds/","section":"istio-handbook","summary":"SDS（秘钥发现服务）是 Envoy 1.8.0 版本起开始引入的服务。可以在 bootstrap.static_resource 的","tags":null,"title":"SDS（秘钥发现服务）","type":"book"},{"authors":null,"categories":null,"content":"在前面，我们了解了如何利用流量的比例（weight 字段）在多个子集之间进行流量路由。在某些情况下，纯粹的基于权重的流量路由或分割已经足够了。然而，在有些场景和情况下，我们可能需要对流量如何被分割和转发到目标服务进行更细化的控制。\nIstio 允许我们使用传入请求的一部分，并将其与定义的值相匹配。例如，我们可以匹配传入请求的 URI前缀，并基于此路由流量。\n   属性 描述     uri 将请求 URI 与指定值相匹配   schema 匹配请求的 schema（HTTP、HTTPS…）   method 匹配请求的 method（GET、POST…）   authority 匹配请求 authority 头   headers 匹配请求头。头信息必须是小写的，并以连字符分隔（例如：x-my-request-id）。注意，如果我们使用头信息进行匹配，其他属性将被忽略（uri、schema、method、authority）。    上述每个属性都可以用这些方法中的一种进行匹配：\n  精确匹配：例如，exact: \u0026#34;value\u0026#34; 匹配精确的字符串\n  前缀匹配：例如，prefix: \u0026#34;value\u0026#34; 只匹配前缀\n  这则匹配：例如，regex：\u0026#34;value\u0026#34; 根据 ECMAscript 风格的正则进行匹配\n  例如，假设请求的 URI 看起来像这样：https://dev.example.com/v1/api。为了匹配该请求的 URI，我们会这样写：\nhttp:- match:- uri:prefix:/v1上述片段将匹配传入的请求，并且请求将被路由到该路由中定义的目的地。\n另一个例子是使用正则并在头上进行匹配。\nhttp:- match:- headers:user-agent:regex:\u0026#39;.*Firefox.*\u0026#39;上述匹配将匹配任何用户代理头与 Regex 匹配的请求。\n重定向和重写请求 在头信息和其他请求属性上进行匹配是有用的，但有时我们可能需要通过请求 URI 中的值来匹配请求。\n例如，让我们考虑这样一种情况：传入的请求使用 /v1/api 路径，而我们想把请求路由到 /v2/api 端点。\n这样做的方法是重写所有传入的请求和与 /v1/api 匹配的 authority/host headers 到 /v2/api。\n例如：\n...http:- match:- uri:prefix:/v1/apirewrite:uri:/v2/apiroute:- destination:host:customers.default.svc.cluster.local...即使目标服务不在 /v1/api 端点上监听，Envoy 也会将请求重写到 /v2/api。\n我们还可以选择将请求重定向或转发到一个完全不同的服务。下面是我们如何在头信息上进行匹配，然后将请求重定向到另一个服务：\n...http:- match:- headers:my-header:exact:helloredirect:uri:/helloauthority:my-service.default.svc.cluster.local:8000...redirect 和 destination 字段是相互排斥的。如果我们使用 redirect，就不需要设置 destination。\nAND 和 OR 语义 在进行匹配时，我们可以使用 AND 和 OR 两种语义。让我们看一下下面的片段：\n...http:- match:- uri:prefix:/v1headers:my-header:exact:hello...上面的片段使用的是 AND 语义。这意味着 URI 前缀需要与 /v1 相匹配，并且头信息 my-header 有一个确切的值 hello。\n要使用 OR 语义，我们可以添加另一个 match 项，像这样：\n...http:- match:- uri:prefix:/v1...- match:- headers:my-header:exact:hello...在上面的例子中，将首先对 URI 前缀进行匹配，如果匹配，请求将被路由到目的地。如果第一个不匹配，算法会转移到第二个，并尝试匹配头。如果我们省略路由上的匹配字段，它将总是评估为 true。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"6ea2e19e7015c60c1d5dd880bc91fa92","permalink":"https://jimmysong.io/docs/istio-handbook/traffic-management/advanced-routing/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/istio-handbook/traffic-management/advanced-routing/","section":"istio-handbook","summary":"在前面，我们了解了如何利用流量的比例（weight 字段）在多","tags":null,"title":"高级路由","type":"book"},{"authors":null,"categories":null,"content":"/healthcheck/fail 可用于失败的入站健康检查。端点 /healthcheck/ok 用于恢复失败的端点。\n这两个端点都需要使用 HTTP 健康检查过滤器。我们可能会在关闭服务器前或进行完全重启时使用它来排空服务器。当调用失败的健康检查选项时，所有的健康检查都将失败，无论其配置如何。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"388d87ab67b55bde59b0cd25d27ab37b","permalink":"https://jimmysong.io/docs/envoy-handbook/admin-interface/healthchecks/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/envoy-handbook/admin-interface/healthchecks/","section":"envoy-handbook","summary":"/healthcheck/fail 可用于失败的入站健康检查。端点 /healthcheck/ok 用于恢复失败的端点。 这两个","tags":null,"title":"健康检查","type":"book"},{"authors":null,"categories":null,"content":"在这个实验中，我们将学习如何配置原始目的地过滤器。要做到这一点，我们需要启用 IP 转发，然后更新 iptables 规则，以捕获所有流量并将其重定向到 Envoy 正在监听的端口。\n我们将使用一个 Linux 虚拟机，而不是 Google Cloud Shell。\n让我们从启用 IP 转发开始。\n# 启用IP转发功能 sudo sysctl -w net.ipv4.ip_forward=1 接下来，我们需要配置 iptables 来捕获所有发送到 80 端口的流量，并将其重定向到 10000 端口。Envoy 代理将在 10000 端口进行监听。\n首先，我们需要确定我们将在 iptables 命令中使用的网络接口名称。我们可以使用 ip link show 命令列出网络接口。例如：\njimmy@instance-1:~$ ip link show 1: lo: \u0026lt; LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00 2: ens4: \u0026lt; BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1460 qdisc mq state UP mode DEFAULT group default qlen 1000 link/ether 42:01:0a:8a:00:2e brd ff:ff:ff:ff:ff:ff 输出结果告诉我们，我们有两个网络接口：环回接口和一个名为 ens4 的接口。这是我们将在 iptables 命令中使用的接口名称。\n# 捕获所有来自外部的80端口的流量并将其重定向到10000端口 sudo iptables -t nat -A PREROUTING -i ens4 -p tcp --dport 80 -j REDIRECT --to port 10000 最后，我们将运行另一条 iptables to 命令，防止从虚拟机发出请求时出现路由循环。设置这个规则将允许我们从虚拟机上运行 curl tetrate.io，并且仍然被重定向到 10000 端口。\n# 使我们能够从同一个实例中运行curl（即防止路由循环）。 sudo iptables -t nat -A OUTPUT -p tcp -m owner !--uid-owner root --dport 80 --j REDIRECT --to port 10000 在修改了 iptables 规则后，我们可以创建以下 Envoy 配置。\nstatic_resources:listeners:- name:inboundaddress:socket_address:address:0.0.0.0port_value:10000listener_filters:- name:envoy.filters.listener.original_dsttyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.listener.original_dst.v3.OriginalDstfilter_chains:- filters:- name:envoy.filters.network.http_connection_managertyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManagerstat_prefix:ingress_httpaccess_log:- name:envoy.access_loggers.filetyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.access_loggers.file.v3.FileAccessLogpath:./envoy.loghttp_filters:- name:envoy.filters.http.routerroute_config:virtual_hosts:- name:proxydomains:[\u0026#34;*\u0026#34;]routes:- match:prefix:\u0026#34;/\u0026#34;route:cluster:original_dst_clusterclusters:- name:original_dst_clustertype:ORIGINAL_DSTconnect_timeout:5slb_policy:CLUSTER_PROVIDEDoriginal_dst_lb_config:use_http_header:true这个配置看起来与我们已经看到的配置相似。我们在 listenener_filters 中添加了 original_dst 过滤器，启用了对一个文件的访问日志，并将所有流量路由到一个叫做 original_dst_cluster 的集群。这个集群的类型设置为 ORIGINAL_DST，将请求发送到原始目的地。\n此外，我们将 use_http_header 字段设置为 true。当设置为 true 时，我们可以使用 x-envoy-original-dst-host 头来覆盖目标地址。请注意，这个标头默认情况下是没有经过处理的，所以启用它允许将流量路由到任意的主机，这可能会产生安全问题。我们在这里只是把它作为一个例子。\n   原始DST过滤器  对于透明代理的情况，这就是我们所需要的。我们不希望做任何解析。我们希望将请求代理到原始目的地。\n将上述 YAML 保存为 5-lab-1-originaldst.yaml。\n为了运行它，我们将使用 func-e CLI。让我们在虚拟机上安装 CLI。\ncurl https://func-e.io/install.sh | sudo bash -s -- -b /usr/local/bin 现在我们可以用我们创建的配置运行 Envoy 代理。\nsudo func-e run -c 5-lab-1-originaldst.yaml  注意，在这种情况下，我们用 sudo 运行 func-e，所以我们可以用同一台机器来测试代理，防止路由循环（见第二条 iptables 规则）。\n 我们可以向 tetrate.io 发送一个请求，如果我们查看 envoy.log 文件，我们会看到以下条目。\n[2021-07-07T21:22:57.294Z] \u0026#34;GET / HTTP/1.1\u0026#34; 301 - 0 227 34 34 \u0026#34;-\u0026#34; \u0026#34;curl/7.64.0\u0026#34; \u0026#34;5fd04969-27b0-4d37-b56c-c273a410da46\u0026#34; \u0026#34;tedrate.io\u0026#34; \u0026#34;75.119.195.116:80\u0026#34; 日志条目显示，iptables 捕获了该请求，并将其重定向到 Envoy 正在监听的端口 10000 。然后，Envoy 将该请求代理到原来的目的地。\n我们也可以从虚拟机的外部提出请求。从第二个终端：这一次，我们使用 Google Cloud Shell，而且我们不在虚拟机中。我们可以向虚拟机的 IP 地址发送请求，并提供 x-envoy-original-dst-host 头，我们希望 Envoy 将请求发送给该 IP 地址。\n 我在这个例子中使用 google.com。要获得 IP 地址，你可以运行 nslookup google.com 并使用该命令中的 IP 地址。\n $ curl -H \u0026#34;x-envoy-original-dst-host: 74.125.199.139\u0026#34; [vm-ip-address] \u0026lt;HTML\u0026gt;\u0026lt;HEAD\u0026gt;\u0026lt;meta http-equiv=\u0026#34;content-type\u0026#34; content=\u0026#34;text/html;charset=utf-8\u0026#34;\u0026gt; \u0026lt;TITLE\u0026gt;301 Moved\u0026lt;/TITLE\u0026gt;\u0026lt;/HEAD\u0026gt;\u0026lt;BODY\u0026gt; \u0026lt;H1\u0026gt;301 Moved\u0026lt;/H1\u0026gt; The document has moved \u0026lt;A HREF=\u0026#34;http://www.google.com/\u0026#34;\u0026gt;here\u0026lt;/A\u0026gt;. \u0026lt;/BODY\u0026gt;\u0026lt;/HTML\u0026gt; 你会注意到响应被代理到了 google.com。我们也可以检查虚拟机上的 envoy.log 来查看日志条目。\n要清理 iptables 规则并禁用 IP 转发，请运行：\n# 禁用IP转发功能 sudo sysctl -w net.ipv4.ip_forward=0 # 从nat表中删除所有规则 sudo iptables -t nat -F ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"3d02a8e2889020e3e0603eb527e9a0fc","permalink":"https://jimmysong.io/docs/envoy-handbook/listener/lab9/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/envoy-handbook/listener/lab9/","section":"envoy-handbook","summary":"在这个实验中，我们将学习如何配置原始目的地过滤器。要做到这一","tags":null,"title":"实验 9：原始目的地过滤器","type":"book"},{"authors":null,"categories":null,"content":"虽然 Envoy 本质上采用了最终一致性模型，但 ADS 提供了对 API 更新推送进行排序的机会，并确保单个管理服务器对 Envoy 节点的 API 更新具有亲和力。ADS 允许管理服务器在单个双向 gRPC 流上传递一个或多个 API 及其资源。否则，一些 API（如 RDS 和 EDS）可能需要管理多个流并连接到不同的管理服务器。\nADS 通过适当得排序 xDS 可以无中断的更新 Envoy 的配置。例如，假设 foo.com 已映射到集群 X。我们希望将路由表中将该映射更改为在集群 Y。为此，必须首先提供 X、Y 这两个集群的 CDS/EDS 更新。\n如果没有 ADS，CDS/EDS/RDS 流可能指向不同的管理服务器，或者位于需要协调的不同 gRPC流连接的同一管理服务器上。EDS 资源请求可以跨两个不同的流分开，一个用于 X，一个用于 Y。ADS 将这些流合并到单个流和单个管理服务器，从而无需分布式同步就可以正确地对更新进行排序。使用 ADS，管理服务器将在单个流上提供 CDS、EDS 和 RDS 更新。\n 下一章   ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"1ba0e1f1da760faba6b2af5cb53e102e","permalink":"https://jimmysong.io/docs/istio-handbook/data-plane/envoy-ads/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/istio-handbook/data-plane/envoy-ads/","section":"istio-handbook","summary":"虽然 Envoy 本质上采用了最终一致性模型，但 ADS 提供了对 API 更新推送进行","tags":null,"title":"ADS（聚合发现服务）","type":"book"},{"authors":null,"categories":null,"content":"ProxyConfig 暴露了代理级别的配置选项。ProxyConfig 可以在每个工作负载、命名空间或整个网格的基础上进行配置。ProxyConfig 不是一个必要的资源；有默认值。\n注意：ProxyConfig 中的字段不是动态配置的——更改需要重新启动工作负载才能生效。\n对于任何命名空间，包括根配置命名空间，只有一个无工作负载选择器的 ProxyConfig 资源是有效的。\n对于带有工作负载选择器的资源，只有一个选择任何特定工作负载的资源才有效。\n对于网格级别的配置，请将资源放在你的 Istio 安装的根配置命名空间中，而不使用工作负载选择器：\napiVersion:networking.istio.io/v1beta1kind:ProxyConfigmetadata:name:my-proxyconfignamespace:istio-systemspec:concurrency:0image:type:distroless对于命名空间级别的配置，将资源放在所需的命名空间中，不需要工作负载选择器：\napiVersion:networking.istio.io/v1beta1kind:ProxyConfigmetadata:name:my-ns-proxyconfignamespace:user-namespacespec:concurrency:0对于工作负载级别的配置，在 ProxyConfig 资源上设置选择器字段：\napiVersion:networking.istio.io/v1beta1kind:ProxyConfigmetadata:name:per-workload-proxyconfignamespace:examplespec:selector:labels:app:ratingsconcurrency:0image:type:debug如果 ProxyConfig CR 被定义为与工作负载相匹配，它将与 proxy.istio.io/config 注释合并（如果存在的话），对于重叠的字段，CR 优先于注释。同样地，如果定义了一个网格的 ProxyConfig CR，并且设置了 MeshConfig.DefaultConfig，那么这两个资源将被合并，对于重叠的字段，CR 优先。\n配置项 下图是 ProxyConfig 的资源配置拓扑图。\n  ProxyConfig 资源配置拓扑图  ProxyConfig 的顶级配置如下。\n selector：可选的。选择器指定应用此 ProxyConfig 资源的 pod/VM 的集合。如果不设置，ProxyConfig 资源将被应用于定义该资源的命名空间中的所有工作负载。 concurrency：要运行的工作线程的数量。如果没有设置，默认为 2。如果设置为 0，这将被配置为使用机器上的所有内核，使用 CPU 请求和限制来选择一个值，限制优先于请求。 environmentVariables：代理的额外环境变量。以ISTIO_META_ 开头的名字将被包含在生成的引导配置中，并被发送到XDS 服务器。 image：指定代理镜像的细节。  ProxyImage 配置 用于构建代理镜像 URL：$hub/$imagename/$tag-$imagetype 。例如：docker.io/istio/proxyv2:1.11.1 或 docker.io/istio/proxyv2:1.11.1-distroless 。\n imageType：Istio 会发布 default、debug 和 distroless 镜像。如果这些镜像类型（例如：centos）被发布到指定的镜像仓库，则允许使用其他值。支持的值：default、debug、distroless。  参考  ProxyConfig - istio.io   下一章   ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"5237b1fdd7ad24f7bd71b8a75d4c8c35","permalink":"https://jimmysong.io/docs/istio-handbook/config-networking/proxy-config/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/istio-handbook/config-networking/proxy-config/","section":"istio-handbook","summary":"ProxyConfig 暴露了代理级别的配置选项。ProxyConfig 可以在每个","tags":null,"title":"ProxyConfig","type":"book"},{"authors":null,"categories":null,"content":"通过 ServiceEntry 资源，我们可以向 Istio 的内部服务注册表添加额外的条目，并使不属于我们网格的外部服务或内部服务看起来像是我们服务网格的一部分。\n当一个服务在服务注册表中时，我们就可以使用流量路由、故障注入和其他网格功能，就像我们对其他服务一样。\n下面是一个 ServiceEntry 资源的例子，它声明了一个可以通过 HTTPS 访问的外部 API（api.external-svc.com）。\napiVersion:networking.istio.io/v1alpha3kind:ServiceEntrymetadata:name:external-svcspec:hosts:- api.external-svc.comports:- number:443name:httpsprotocol:TLSresolution:DNSlocation:MESH_EXTERNALhosts 字段可以包含多个外部 API，在这种情况下，Envoy sidecar 会根据下面的层次结构来进行检查。如果任何一项不能被检查，Envoy 就会转到层次结构中的下一项。\n HTTP Authority 头（在HTTP/2中）和 Host 头（HTTP/1.1中） SNI IP 地址和端口  如果上述数值都无法检查，Envoy 会根据 Istio 的安装配置，盲目地转发请求或放弃该请求。\n与 WorkloadEntry 资源一起，我们可以处理虚拟机工作负载向 Kubernetes 迁移的问题。在 WorkloadEntry 中，我们可以指定在虚拟机上运行的工作负载的细节（名称、地址、标签），然后使用 ServiceEntry 中的 workloadSelector 字段，使虚拟机成为 Istio 内部服务注册表的一部分。\n例如，假设 customers 的工作负载正在两个虚拟机上运行。此外，我们已经有在 Kubernetes 中运行的 Pod，其标签为 app: customers。\n让我们这样来定义 WorkloadEntry 资源：\napiVersion:networking.istio.io/v1alpha3kind:WorkloadEntrymetadata:name:customers-vm-1spec:serviceAccount:customersaddress:1.0.0.0labels:app:customersinstance-id:vm1---apiVersion:networking.istio.io/v1alpha3kind:WorkloadEntrymetadata:name:customers-vm-2spec:serviceAccount:customersaddress:2.0.0.0labels:app:customersinstance-id:vm2现在我们可以创建一个 ServiceEntry 资源，该资源同时跨越 Kubernetes 中运行的工作负载和虚拟机：\napiVersion:networking.istio.io/v1alpha3kind:ServiceEntrymetadata:name:customers-svcspec:hosts:- customers.comlocation:MESH_INTERNALports:- number:80name:httpprotocol:HTTPresolution:STATICworkloadSelector:labels:app:customers在位置字段中设置 MESH_INTERNAL，我们是说这个服务是网格的一部分。这个值通常用于包括未管理的基础设施（VM）上的工作负载的情况。这个字段的另一个值，MESH_EXTERNAL，用于通过 API 消费的外部服务。MESH_INTERNAL 和 MESH_EXTERNAL 设置控制了网格中的 sidecar 如何尝试与工作负载进行通信，包括它们是否会默认使用 Istio 双向 TLS。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"74f6a771085405ded072e6ff614cad5b","permalink":"https://jimmysong.io/docs/istio-handbook/traffic-management/seviceentry/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/istio-handbook/traffic-management/seviceentry/","section":"istio-handbook","summary":"通过 ServiceEntry 资源，我们可以向 Istio 的内部服务注册表添加额外的条目，并使","tags":null,"title":"ServiceEntry","type":"book"},{"authors":null,"categories":null,"content":"在这个实验中，我们将看一个例子，说明我们如何设置 Envoy，以便在一个 IP 地址上用不同的证书为多个网站服务。\n我们将使用自签名的证书进行测试，但如果使用真实签名的证书，其过程是相同的。\n使用 openssl，我们将在 certs 文件夹中为 www.hello.com 和 www.example.com 创建自签名证书。\n$ mkdir certs \u0026amp;\u0026amp; cd certs $ openssl req -nodes -new -x509 -keyout www_hello_com.key -out www_hello_com.cert -subj \u0026#34;/C=US/ST=Washington/L=Seattle/O=Hello LLC/OU=Org/CNwww.hello.com\u0026#34; $ openssl req -nodes -new -x509 -keyout www_example_com.key -out www_example_com.cert -subj \u0026#34;/C=US/ST=Washington/L=Seattle/O=Example LLC/OU=Org/CN www.example.com\u0026#34; 对于每个通用名称，我们最终会有两个文件：私钥和证书（例如，www_example_com.key 和 www_example_com.cert）。\n我们将为每个过滤链的匹配分别配置 transport_socket 字段。下面是一个如何定义 TLS 传输套接字并提供密钥和证书的片段。\n...transport_socket:name:envoy.transport_sockets.tlstyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.DownstreamTlsContextcommon_tls_context:tls_certificates:- certificate_chain:filename:certs/www_hello_com.certprivate_key:filename:certs/www_hello_com.key...因为我们想根据 SNI 使用不同的证书，我们将在 TLS 监听器中添加一个 TLS 检查器过滤器，使用 filter_chain_match 和 server_names 字段来根据 SNI 进行匹配。\n下面是两个过滤链匹配部分：注意每个过滤链匹配都有自己的 transport_socket，有指向证书和密钥文件的指针。\nlistener_filters:- name:\u0026#34;envoy.filters.listener.tls_inspector\u0026#34;typed_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.listener.tls_inspector.v3.TlsInspectorfilter_chains:- filter_chain_match:server_names:\u0026#34;www.example.com\u0026#34;filters:transport_socket:name:envoy.transport_sockets.tls...http:filters:...- filter_chain_match:server_names:\u0026#34;www.hello.com\u0026#34;filters:transport_socket:name:envoy.transport_sockets.tls...http:filters:...你可以在 5-lab-2-tls_match.yaml 文件中找到完整的配置，用 func-e run -c 5-lab-2-tls_match.yaml 运行它。由于我们将只使用 openssl 进行连接，我们不需要集群端点的运行。\n为了检查 SNI 匹配是否正常工作，我们可以使用 openssl 并连接到提供服务器名称的 Envoy 监听器。例如：\n$ openssl s_client -connect 0.0.0.0:443 -servername www.example.comCONNECTED(00000003)depth=0 C = US, ST = Washington, L = Seattle, O = Example LLC, OU = Org, CN = www.example.comverify error:num=18:self signed certificateverify return:1depth=0 C = US, ST = Washington, L = Seattle, O = Example LLC, OU = Org, CN = www.example.comverify return:1---Certificate chain0s:C = US, ST = Washington, L = Seattle, O = Example LLC, OU = Org, CN = www.example.comi:C = US, ST = Washington, L = Seattle, O = Example LLC, OU = Org, CN = www.example.com...该命令将根据所提供的服务器名称返回正确的对等证书。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"2b336ae15a4d4ae3f50794fe972362da","permalink":"https://jimmysong.io/docs/envoy-handbook/listener/lab10/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/envoy-handbook/listener/lab10/","section":"envoy-handbook","summary":"在这个实验中，我们将看一个例子，说明我们如何设置 Envoy，","tags":null,"title":"实验 10：TLS 检查器过滤器","type":"book"},{"authors":null,"categories":null,"content":"在这个实验中，我们将展示如何使用和配置 HTTP 分接式过滤器。我们将配置一个 /error 路由，它返回一个直接的响应，其主体为 error 值。然后，在分接式过滤器中，我们将配置匹配器，以匹配任何响应主体包含 error 字符串和请求头 debug: true 的请求。如果这两个条件都为真，那么我们就分接该请求并将输出写入一个前缀为 tap_debug 的文件。\n让我们从创建匹配配置开始。我们将使用两个匹配器，一个用来匹配请求头（http_request_headers_match），另一个用来匹配响应体（http_response_generic_body_match）。我们将用逻辑上的 AND 来组合这两个条件。\n下面是匹配配置的样子。\n- name:envoy.filters.http.taptyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.http.tap.v3.Tapcommon_config:static_config:match:and_match:rules:- http_request_headers_match:headers:name:debugstring_match:exact:\u0026#34;true\u0026#34;- http_response_generic_body_match:patterns:- string_match:error我们将使用 JSON_BODY_AS_STRING 格式，并将输出写入以 tap_debug 为前缀的文件。\noutput_config:sinks:- format:JSON_BODY_AS_STRINGfile_per_tap:path_prefix:tap_debug让我们把这两块放在一起，创建一个完整的配置。我们将使用 direct_response，所以我们不需要设置或运行任何额外的服务。\nstatic_resources:listeners:- name:listener_0address:socket_address:address:0.0.0.0port_value:10000filter_chains:- filters:- name:envoy.filters.network.http_connection_managertyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManagerstat_prefix:ingress_httphttp_filters:- name:envoy.filters.http.taptyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.http.tap.v3.Tapcommon_config:static_config:match:and_match:rules:- http_request_headers_match:headers:name:debugstring_match:exact:\u0026#34;true\u0026#34;- http_response_generic_body_match:patterns:- string_match:erroroutput_config:sinks:- format:JSON_BODY_AS_STRINGfile_per_tap:path_prefix:tap_debug- name:envoy.filters.http.routerroute_config:name:local_routevirtual_hosts:- name:local_servicedomains:[\u0026#34;*\u0026#34;]routes:- match:path:\u0026#34;/\u0026#34;direct_response:status:200body:inline_string:hello- match:path:\u0026#34;/error\u0026#34;direct_response:status:500body:inline_string:error将上述 YAML 保存为 7-lab-1-tap-filter-1.yaml 文件，并使用 func-e CLI 运行它：\nfunc-e run -c 7-lab-1-tap-filter-1.yaml \u0026amp; 如果我们发送一个请求到 http://localhost:10000，我们会收到符合第一条路由的响应（HTTP 200 和 body hello）。这个请求不会被分接，因为我们没有提供任何头信息，响应体也没有包含 error 值。\n让我们试着设置 debug 头并向 /error 端点发送一个请求。\n$ curl -H \u0026#34;debug: true\u0026#34; localhost:10000/error error 这一次，在同一个文件夹中创建了一个包含被挖掘的请求内容的 JSON 文件。下面是该文件的内容应该是这样的。\n{ \u0026#34;http_buffered_trace\u0026#34;: { \u0026#34;request\u0026#34;: { \u0026#34;headers\u0026#34;: [ { \u0026#34;key\u0026#34;: \u0026#34;:authority\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;localhost:10000\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;:path\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;/error\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;:method\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;GET\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;:scheme\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;http\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;user-agent\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;curl/7.64.0\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;accept\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;*/*\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;debug\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;true\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;x-forwarded-proto\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;http\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;x-request-id\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;4855ee5d-7798-4c50-8692-a6989e72ca9b\u0026#34; } ], \u0026#34;trailers\u0026#34;: [] }, \u0026#34;response\u0026#34;: { \u0026#34;headers\u0026#34;: [ { \u0026#34;key\u0026#34;: \u0026#34;:status\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;500\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;content-length\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;5\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;content-type\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;text/plain\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;date\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;Mon, 29 Nov 2021 22:38:32 GMT\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;server\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;envoy\u0026#34; } ], \u0026#34;body\u0026#34;: { \u0026#34;truncated\u0026#34;: false, \u0026#34;as_string\u0026#34;: \u0026#34;error\u0026#34; }, \u0026#34;trailers\u0026#34;: [] } } } 输出显示了所有的请求头和 Trailer，以及我们收到的响应。\n我们将在下一个例子中使用同样的场景，但我们将使用 /tap 管理端点来实现它。\n首先，让我们创建 Envoy 配置。\nstatic_resources:listeners:- name:listener_0address:socket_address:address:0.0.0.0port_value:10000filter_chains:- filters:- name:envoy.filters.network.http_connection_managertyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManagerstat_prefix:ingress_httphttp_filters:- name:envoy.filters.http.taptyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.http.tap.v3.Tapcommon_config:admin_config:config_id:my_tap_id- name:envoy.filters.http.routerroute_config:name:local_routevirtual_hosts:- name:local_servicedomains:[\u0026#34;*\u0026#34;]routes:- match:path:\u0026#34;/\u0026#34;direct_response:status:200body:inline_string:hello- match:path:\u0026#34;/error\u0026#34;direct_response:status:500body:inline_string:erroradmin:address:socket_address:address:0.0.0.0port_value:9901这一次，我们使用 admin_config 字段并指定配置 ID。此外，我们要使管理接口使用 /tap 端点。\n将上述 YAML 保存为 7-lab-1-tap-filter-2.yaml，并使用 func-e CLI 运行它。\nfunc-e run -c 7-lab-1-tap-filter-2.yaml \u0026amp; 如果我们尝试向 / 和 error 路径发送请求，我们会得到预期的响应。我们必须用 tap 配置向 /tap 端点发送一个 POST 请求，以启用请求分接。\n让我们使用这个符合任何请求的 tap 配置。\n{ \u0026#34;config_id\u0026#34;: \u0026#34;my_tap_id\u0026#34;, \u0026#34;tap_config\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;any_match\u0026#34;: true }, \u0026#34;output_config\u0026#34;: { \u0026#34;sinks\u0026#34;: [ { \u0026#34;streaming_admin\u0026#34;: {} } ] } } } 注意，我们提供的是与我们在 Envoy 配置中定义的 ID 相匹配的配置 ID。如果我们提供了一个无效的配置 ID，那么在向 /tap 端点发送 POST 请求时，我们会得到一个错误。\nUnknown config id \u0026#39;some_tap_id\u0026#39;. No extension has registered with this id. 我们还使用 streaming_admin 字段作为输出汇，这意味着如果 /tap 的 POST 请求被接受，那么 Envoy 将流式处理序列化的 JSON 信息，直到我们终止请求。\n让我们把上述 JSON 保存到 tap-config-any.json，然后用 cURL 向 /tap 端点发送一个 POST 请求。\ncurl -X POST -d @tap-config-any.json http://localhost:9901/tap 我们将打开第二个终端窗口，向 localhost:10000 发送一个 cURL 请求，以测试配置。由于我们对所有的请求都进行了匹配，我们将在第一个终端窗口中看到流式分接的输出。\n{ \u0026#34;http_buffered_trace\u0026#34;: { \u0026#34;request\u0026#34;: { \u0026#34;headers\u0026#34;: [ { \u0026#34;key\u0026#34;: \u0026#34;:authority\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;localhost:10000\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;:path\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;/\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;:method\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;POST\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;:scheme\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;http\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;user-agent\u0026#34;, \u0026#34;value\u0026#34;: …","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"2bd69459f63f1070de91c6a02f244e18","permalink":"https://jimmysong.io/docs/envoy-handbook/admin-interface/lab15/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/envoy-handbook/admin-interface/lab15/","section":"envoy-handbook","summary":"在这个实验中，我们将展示如何使用和配置 HTTP 分接式过滤器。我们将","tags":null,"title":"实验15：使用 HTTP 分接式过滤器","type":"book"},{"authors":null,"categories":null,"content":"默认情况下，注入的 sidecar 代理的配置方式是，它们接受所有端口的流量，并且在转发流量时可以到达网格中的任何服务。\n在某些情况下，你可能想改变这种配置，配置代理，所以它只能使用特定的端口和访问某些服务。要做到这一点，你可以在 Istio 中使用 Sidecar 资源。\nSidecar 资源可以被部署到 Kubernetes 集群内的一个或多个命名空间，但如果没有定义工作负载选择器，每个命名空间只能有一个 sidecar 资源。\nSidecar 资源由三部分组成，一个工作负载选择器、一个入口（ingress）监听器和一个出口（egress）监听器。\n工作负载选择器 工作负载选择器决定了哪些工作负载会受到 sidecar 配置的影响。你可以决定控制一个命名空间中的所有 sidecar，而不考虑工作负载，或者提供一个工作负载选择器，将配置只应用于特定的工作负载。\n例如，这个 YAML 适用于默认命名空间内的所有代理，因为没有定义选择器。\napiVersion:networking.istio.io/v1alpha3kind:Sidecarmetadata:name:default-sidecarnamespace:defaultspec:egress:- hosts:- \u0026#34;default/*\u0026#34;- \u0026#34;istio-system/*\u0026#34;- \u0026#34;staging/*\u0026#34;在 egress 部分，我们指定代理可以访问运行在 default、istio-system 和 staging 命名空间的服务。要将资源仅应用于特定的工作负载，我们可以使用 workloadSelector 字段。例如，将选择器设置为 `version: v1 将只适用于有该标签设置的工作负载。\napiVersion:networking.istio.io/v1alpha3kind:Sidecarmetadata:name:default-sidecarnamespace:defaultspec:workloadSelector:labels:version:v1egress:- hosts:- \u0026#34;default/*\u0026#34;- \u0026#34;istio-system/*\u0026#34;- \u0026#34;staging/*\u0026#34;入口和出口监听器 资源的入口（ingress）监听器部分定义了哪些入站流量被接受。同样地，通过出口（egress）监听器，你可以定义出站流量的属性。\n每个入口监听器都需要一个端口设置，以便接收流量（例如，下面的例子中的 3000）和一个默认的端点。默认端点可以是一个回环 IP 端点或 Unix 域套接字。端点配置了流量将被转发到哪里。\n...ingress:- port:number:3000protocol:HTTPname:somenamedefaultEndpoint:127.0.0.1:8080...上面的片段将入口监听器配置为在端口 3000 上监听，并将流量转发到服务监听的端口 8080 上的回环 IP。此外，我们可以设置 bind 字段，以指定一个 IP 地址或域套接字，我们希望代理监听传入的流量。最后，字段 captureMode 可以用来配置如何以及是否捕获流量。\n出口监听器有类似的字段，但增加了hosts 字段。通过 hosts 字段，你可以用 namespace/dnsName 的格式指定服务主机。例如， myservice.default 或 default/*。在 hosts 字段中指定的服务可以是来自网格注册表的实际服务、外部服务（用 ServiceEntry 定义），或虚拟服务。\negress:- port:number:8080protocol:HTTPhosts:- \u0026#34;staging/*\u0026#34;通过上面的 YAML，sidecar 代理了运行在 staging 命名空间的服务的 8080 端口的流量。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"ee67757be0c111f377df927eead3f0b8","permalink":"https://jimmysong.io/docs/istio-handbook/traffic-management/sidecar/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/istio-handbook/traffic-management/sidecar/","section":"istio-handbook","summary":"默认情况下，注入的 sidecar 代理的配置方式是，它们接受所有端口的流量","tags":null,"title":"Sidecar","type":"book"},{"authors":null,"categories":null,"content":"在这个实验中，我们将学习如何使用 TLS 检查器过滤器来选择一个特定的过滤器链。单个过滤器链将根据 transport_protocol 和 application_protocol将流量分配到不同的上游集群。\n我们将为我们的上游主机使用 mendhak/http-https-echo Docker 镜像。这些容器可以被配置为监听 HTTP/HTTPS 并将响应回传。\n我们将运行镜像的三个实例来代表非 TLS HTTP、TLS HTTP/1.1 和 TLS HTTP/2 协议。\n# non-TLS HTTP docker run -dit -p 8080:8080 -t mendhak/http-https-echo:18 # TLS HTTP1.1 docker run -dit -e HTTPS_PORT=443 -p 443:443 -t mendhak/http-https-echo:18 # TLS HTTP2 docker run -dit -e HTTPS_PORT=8443 -p 8443:8443 -t mendhak/http-https-echo:18 为了确保这三个容器都在运行，我们可以用 curl 发送几个请求，看看是否得到了回应。从输出中，我们还可以检查主机名是否与实际的 Docker 容器 ID 相符。\n# non-TLS HTTP $ curl http://localhost:8080 { \u0026#34;path\u0026#34;: \u0026#34;/\u0026#34;, \u0026#34;headers\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;localhost:8080\u0026#34;, \u0026#34;user-agent\u0026#34;: \u0026#34;curl/7.64.0\u0026#34;, \u0026#34;accept\u0026#34;: \u0026#34;*/*\u0026#34; }, \u0026#34;method\u0026#34;: \u0026#34;GET\u0026#34;, \u0026#34;body\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;fresh\u0026#34;: false, \u0026#34;hostname\u0026#34;: \u0026#34;localhost\u0026#34;, \u0026#34;ip\u0026#34;: \u0026#34;::ffff:172.18.0.1\u0026#34;, \u0026#34;ips\u0026#34;: [], \u0026#34;protocol\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;query\u0026#34;: {}, \u0026#34;subdomains\u0026#34;: [], \u0026#34;xhr\u0026#34;: false, \u0026#34;os\u0026#34;: { \u0026#34;hostname\u0026#34;: \u0026#34;100db0dce742\u0026#34; }, \u0026#34;connection\u0026#34;: {} }  注意在向其他两个容器发送请求时，我们将使用 -k 标志来告诉 curl 跳过对服务器 TLS 证书的验证。此外，我们可以使用 --http1.1 和 --http2 标志来发送 HTTP1.1 或 HTTP2 请求。\n # HTTP1.1 $ curl -k --http1.1 https://localhost:443 { \u0026#34;path\u0026#34;: \u0026#34;/\u0026#34;, \u0026#34;headers\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;localhost\u0026#34;, \u0026#34;user-agent\u0026#34;: \u0026#34;curl/7.64.0\u0026#34;, \u0026#34;accept\u0026#34;: \u0026#34;*/*\u0026#34; }, \u0026#34;method\u0026#34;: \u0026#34;GET\u0026#34;, \u0026#34;body\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;fresh\u0026#34;: false, \u0026#34;hostname\u0026#34;: \u0026#34;localhost\u0026#34;, \u0026#34;ip\u0026#34;: \u0026#34;::ffff:172.18.0.1\u0026#34;, \u0026#34;ips\u0026#34;: [], \u0026#34;protocol\u0026#34;: \u0026#34;https\u0026#34;, \u0026#34;query\u0026#34;: {}, \u0026#34;subdomains\u0026#34;: [], \u0026#34;xhr\u0026#34;: false, \u0026#34;os\u0026#34;: { \u0026#34;hostname\u0026#34;: \u0026#34;51afc40f7506\u0026#34; }, \u0026#34;connection\u0026#34;: { \u0026#34;servername\u0026#34;: \u0026#34;localhost\u0026#34; } } $ curl -k --http2 https://localhost:8443 { \u0026#34;path\u0026#34;: \u0026#34;/\u0026#34;, \u0026#34;headers\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;localhost:8443\u0026#34;, \u0026#34;user-agent\u0026#34;: \u0026#34;curl/7.64.0\u0026#34;, \u0026#34;accept\u0026#34;: \u0026#34;*/*\u0026#34; }, \u0026#34;method\u0026#34;: \u0026#34;GET\u0026#34;, \u0026#34;body\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;fresh\u0026#34;: false, \u0026#34;hostname\u0026#34;: \u0026#34;localhost\u0026#34;, \u0026#34;ip\u0026#34;: \u0026#34;::ffff:172.18.0.1\u0026#34;, \u0026#34;ips\u0026#34;: [], \u0026#34;protocol\u0026#34;: \u0026#34;https\u0026#34;, \u0026#34;query\u0026#34;: {}, \u0026#34;subdomains\u0026#34;: [], \u0026#34;xhr\u0026#34;: false, \u0026#34;os\u0026#34;: { \u0026#34;hostname\u0026#34;: \u0026#34;40e7143e6a55\u0026#34; }, \u0026#34;connection\u0026#34;: { \u0026#34;servername\u0026#34;: \u0026#34;localhost\u0026#34; } } 一旦我们验证了容器的正确运行，我们就可以创建 Envoy 配置。我们将使用 tls_inspector 和 filter_chain_match 字段来检查传输协议是否是 TLS，以及应用协议是否是 HTTP1.1（http/1.1）或 HTTP2（h2）。基于这些信息，我们会有不同的集群，将流量转发到上游主机（Docker 容器）。记住 HTTP 运行在端口 8080，TLS HTTP/1.1 运行在端口 443，TLS HTTP2 运行在端口 8443。\nstatic_resources:listeners:- address:socket_address:address:0.0.0.0port_value:10000listener_filters:- name:\u0026#34;envoy.filters.listener.tls_inspector\u0026#34;typed_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.listener.tls_inspector.v3.TlsInspectorfilter_chains:- filter_chain_match:# Match TLS and HTTP2transport_protocol:tlsapplication_protocols:[h2]filters:- name:envoy.filters.network.tcp_proxytyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.network.tcp_proxy.v3.TcpProxycluster:service-tls-http2stat_prefix:https_passthrough- filter_chain_match:# Match TLS and HTTP1.1transport_protocol:tlsapplication_protocols:[http/1.1]filters:- name:envoy.filters.network.tcp_proxytyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.network.tcp_proxy.v3.TcpProxycluster:service-tls-http1.1stat_prefix:https_passthrough- filter_chain_match:# No matches here, go to HTTP upstreamfilters:- name:envoy.filters.network.tcp_proxytyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.network.tcp_proxy.v3.TcpProxycluster:service-httpstat_prefix:ingress_httpclusters:- name:service-tls-http2type:STRICT_DNSlb_policy:ROUND_ROBINload_assignment:cluster_name:service-tls-http2endpoints:- lb_endpoints:- endpoint:address:socket_address:address:127.0.0.1port_value:8443- name:service-tls-http1.1type:STRICT_DNSlb_policy:ROUND_ROBINload_assignment:cluster_name:service-tls-http1.1endpoints:- lb_endpoints:- endpoint:address:socket_address:address:127.0.0.1port_value:443- name:service-httptype:STRICT_DNSlb_policy:ROUND_ROBINload_assignment:cluster_name:service-httpendpoints:- lb_endpoints:- endpoint:address:socket_address:address:127.0.0.1port_value:8080admin:address:socket_address:address:0.0.0.0port_value:9901将上述 YAML 保存为 tls.yaml，并使用 func-e run -c tls.yaml 运行它。\n为了测试这一点，我们可以像以前一样发出类似的 curl 请求，并检查主机名是否与正在运行的 Docker 容器相符。\n$ curl http://localhost:10000 | jq \u0026#39;.os.hostname\u0026#39; \u0026#34;100db0dce742\u0026#34; $ curl -k --http1.1 https://localhost:10000 | jq \u0026#39;.os.hostname\u0026#39; \u0026#34;51afc40f7506\u0026#34; $ curl -k --http2 https://localhost:10000 | jq \u0026#39;.os.hostname\u0026#39; \u0026#34;40e7143e6a55\u0026#34; 另外，我们可以检查各个容器的日志，看看请求是否被正确发送。\n 下一章   ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"47041317a6b921ba6857beb75e95f601","permalink":"https://jimmysong.io/docs/envoy-handbook/listener/lab11/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/envoy-handbook/listener/lab11/","section":"envoy-handbook","summary":"在这个实验中，我们将学习如何使用 TLS 检查器过滤器来选择一个特定","tags":null,"title":"实验 11：匹配传输和应用协议","type":"book"},{"authors":null,"categories":null,"content":"EnvoyFilter 资源允许你定制由 Istio Pilot 生成的 Envoy 配置。使用该资源，你可以更新数值，添加特定的过滤器，甚至添加新的监听器、集群等等。小心使用这个功能，因为不正确的定制可能会破坏整个网格的稳定性。\n过滤器是叠加应用的，这意味着对于特定命名空间中的特定工作负载，可以有任何数量的过滤器。根命名空间（例如 istio-system）中的过滤器首先被应用，然后是工作负载命名空间中的所有匹配过滤器。\n下面是一个 EnvoyFilter 的例子，它在请求中添加了一个名为 api-version 的头。\napiVersion:networking.istio.io/v1alpha3kind:EnvoyFiltermetadata:name:api-header-filternamespace:defaultspec:workloadSelector:labels:app:web-frontendconfigPatches:- applyTo:HTTP_FILTERmatch:context:SIDECAR_INBOUNDlistener:portNumber:8080filterChain:filter:name:\u0026#34;envoy.http_connection_manager\u0026#34;subFilter:name:\u0026#34;envoy.router\u0026#34;patch:operation:INSERT_BEFOREvalue:name:envoy.luatyped_config:\u0026#34;@type\u0026#34;: \u0026#34;type.googleapis.com/envoy.extensions.filters.http.lua.v3.Lua\u0026#34;inlineCode:|function envoy_on_response(response_handle) response_handle:headers():add(\u0026#34;api-version\u0026#34;, \u0026#34;v1\u0026#34;) end如果你向 $GATEWAY_URL 发送一个请求，你可以注意到 api-version 头被添加了，如下所示：\n$ curl -s -I -X HEAD http://$GATEWAY_URL HTTP/1.1 200 OK x-powered-by: Express content-type: text/html; charset=utf-8 content-length: 2471 etag: W/\u0026#34;9a7-hEXE7lJW5CDgD+e2FypGgChcgho\u0026#34; date: Tue, 17 Nov 2020 00:40:16 GMT x-envoy-upstream-service-time: 32 api-version: v1 server: istio-envoy  下一章   ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"5c8612f44ebc7041c3dcd55d9e7464f7","permalink":"https://jimmysong.io/docs/istio-handbook/traffic-management/envoyfilter/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/docs/istio-handbook/traffic-management/envoyfilter/","section":"istio-handbook","summary":"EnvoyFilter 资源允许你定制由 Istio Pilot 生成的 Envoy 配置。使用该资源，你可以更新数","tags":null,"title":"EnvoyFilter","type":"book"},{"authors":null,"categories":null,"content":"Envoy 基础教程，本手册梳理了 Envoy 基础知识，适用于初学者，帮你快速掌握 Envoy 代理。\n关于本书 Envoy 是一个开源的边缘和服务代理，专为云原生应用而设计。Envoy 与每个应用程序一起运行，通过提供网络相关的功能，如重试、超时、流量路由和镜像、TLS 终止等，以一种平台无关的方式抽象出网络。由于所有的网络流量都流经 Envoy 代理，因此很容易观察到流量和问题区域，调整性能，并准确定位延迟来源。\n本书为 Tetrate 出品的《Envoy 基础教程》的文字内容，其配套的 8 节实验及 19 个测试，请访问 Tetrate 学院。\n关于作者 宋净超（Jimmy Song），CNCF Ambassador，云原生社区创始人，个人网站 jimmysong.io。\n许可证 您可以使用署名 - 非商业性使用 - 相同方式共享 4.0 (CC BY-NC-SA 4.0) 协议共享。\n","date":1652832000,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"4a01e92eddc6da383ffd966fd08ce545","permalink":"https://jimmysong.io/docs/book/envoy-handbook/","publishdate":"2022-05-18T00:00:00Z","relpermalink":"/docs/book/envoy-handbook/","section":"book","summary":"从零开始学习Envoy网络代理","tags":["handbook"],"title":"Envoy基础教程🔥","type":"publication"},{"authors":null,"categories":null,"content":"关于本书 本书的主题包括：\n 服务网格概念解析 控制平面和数据平面的原理 Istio 架构详解 基于 Istio 的自定义扩展 迁移到 Istio 服务网格 构建云原生应用网络  书中部分内容来自 Tetrate 出品的 Istio 基础教程，请访问 Tetrate 学院，解锁全部教程及测试，获得 Tetrate 认证的 Istio 认证。\n致谢 感谢 ServiceMesher 及云原生社区先后负责翻译了 Envoy 及 Istio 官方文档，为本书的成书提供了大量参考资料。\n许可证 本书所有内容支持使用署名 - 非商业性使用 - 相同方式共享 4.0 (CC BY-NC-SA 4.0) 协议共享。\n参与本书 请参考 Istio 文档样式指南。\n","date":1652832000,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"cdeb6daaa562734b2afb9bbddb006311","permalink":"https://jimmysong.io/docs/book/istio-handbook/","publishdate":"2022-05-18T00:00:00Z","relpermalink":"/docs/book/istio-handbook/","section":"book","summary":"云原生应用网络构建指南","tags":["handbook"],"title":"Istio基础教程🔥","type":"publication"},{"authors":["Varun Talwar"],"categories":["Envoy"],"content":"今天，Envoy 社区宣布了一个令人兴奋的新项目。 Envoy Gateway。该项目将行业领导者联合起来，精简由 Envoy 驱动的应用网关的好处。这种方法使 Envoy Gateway 能够立即为快速创新打下坚实的基础。该项目将提供一套服务来管理 Envoy 代理机群，通过易用性来推动采用，并通过定义明确的扩展机制来支持众多的用例。\n我们为什么要这样做？ Tetrate 是 Envoy Proxy 的第一贡献者（按提交量计算），也是 Envoy Gateway 指导小组的成员，其贡献者涵盖技术和管理领域。我们相信，我们强大的伙伴关系和在开源软件方面的深厚经验将有助于确保 Envoy Gateway 的成功。Tetrate 推动了 EG 计划，因为我们致力于上游项目，因为我们相信这将降低 Envoy Proxy 用户的进入门槛，也因为这与我们开发服务网格作为零信任架构基础的使命相一致。Tetrate 将大力投资建设 Envoy Gateway 的安全功能，包括支持 OAuth2 和 Let’s Encrypt 集成等 API 功能。\n对上游项目的承诺 Tetrate 从第一天起就站在服务网格的最前沿，始终相信上游项目和它们的社区。因此，我们一直在为 Istio 和 Envoy 的上游项目提供帮助和支持。我们看到不同的人在使用 Envoy，并创建他们自己的控制平面和 API 网关实现，导致碎片化，创新速度慢，功能差距大，以及缺乏对一个代码库的支持。由于我们与 Matt Klein 和 Envoy 社区长期以来关系密切，当我们提议将其纳入 Envoy 的标准化实现，并将其整合到一个官方的上游实现中时，我们得到了 Matt 和其他 CNCF 项目的强烈支持。我们一直在幕后与其他指导委员会成员（Ambassador Labs、Fidelity Investments、Project Contour 和 VMware 辛勤工作，以定义 Envoy Gateway。\n我们知道，艰苦的工作才刚刚开始，我们致力于这个项目以及 CNCF 内其他几个项目的长期成功。\n实现控制平面的标准化 在很短的时间内，Envoy 已经成为现代云原生应用的首选网络层。随着 Envoy 获得关注，大量的上游项目开始利用它来实现服务网格、入口、出口和 API 网关功能。这些项目中有许多能力重叠、功能差距、专有特性，或者缺乏社区多样性。这种支离破碎的状态是由于 Envoy 社区没有提供控制平面的实现而产生的副作用。\n因此，创新的速度降低了，企业被要求辨别利用 Envoy 作为其应用网络数据平面的最佳方法。现在，社区正在提供 Envoy Gateway，更多的用户可以享受 Envoy 的好处，而无需决定控制平面。Envoy Gateway 的目标是：\n “…… 通过支持众多入口和 L7/L4 流量路由使用案例的表达式、可扩展、面向角色的 API，降低采用障碍，吸引更多用户使用 Envoy；并为供应商建立增值产品提供共同基础，而无需重新设计基本交互。”\n 易用性和运营效率 Envoy Proxy 是由 xDS API 驱动的，这些 APIs 暴露了大量的功能，并被控制平面广泛采用。虽然这些 API 功能丰富，但对于用户来说，要快速学习并开始利用 Envoy 的功能是非常困难的。Envoy Gateway 将为用户抽象出这些复杂的功能，同时支持现有的运营和应用管理模式。\nEnvoy Gateway 将利用 Gateway API 来实现这些目标，而不是开发一个新的项目专用 API。Gateway API 是一个由 Kubernetes 网络特别兴趣小组管理的项目，正在迅速成为提供用户接口以管理应用网络基础设施和流量路由的首选方法。这个开源项目有一个丰富、多样的社区，有几个知名的实施方案。我们期待着作为社区的一部分开展工作，使 Envoy Gateway 成为业界首选的网关 API 实现。\n为什么这比传统的 API 网关更好？ 传统的代理不是轻量级的、开放的，也不是动态可编程的、类似 xDS 的 API，因此 Envoy 很适合成为当今动态后端的 API 网关 —— 尤其是在增加安全功能的情况下。我们设想将 Envoy 网关作为不断发展的 API 管理领域的一个关键组成部分。API 网关是 API 管理的核心组件，提供透明地策略执行和生成详细遥测数据的功能。这种遥测技术提供了强大的可观察性，为企业提供了更好的洞察力，以排除故障、维护和优化其 API。\n在我们看来，由于 Envoy 的设计、功能设置、安装基础和社区，它是业内最好的 API 网关。有了 Envoy Gateway，企业可以在将 Envoy 嵌入其 API 管理策略方面增加信心。\n无边界的零信任 当你的所有应用服务都在一个服务网格中运行时，实现零信任架构就不那么难了。然而，现实中不都是服务网格。服务在虚拟机上运行，在无代理容器中运行，作为无服务器函数运行，等等。Envoy Gateway 将突破这些运行时的界限，为跨异构环境的统一策略执行提供基础。\n这一基础的关键是 Envoy Gateway 的可扩展性，它提供了暴露 Envoy 和非 Envoy 安全功能的灵活性。这些扩展点将被用来提供实现零信任架构所需的功能，包括用户和应用认证、授权、加密和速率限制。Envoy Gateway 将很快成为寻求实现零信任架构的组织的一个关键组件。\n同样，Tetrate 致力于上游项目和它们的长期可行性。这一举措又一次证明了这一点，并表明上游的 Envoy 和 Istio 现在正成为构建服务网格的事实上的支柱。Envoy Gateway 将使服务网格成为主流，架构师们应该把网格看作是 ZTA 的基础。为了帮助架构师进行论证，我们最近出版了《服务网格手册》。我们很快就会发布一种带有上游 Envoy Gateway 和 Istio 的架构方法，可以看作是你的应用网络的基础。\n探索 Envoy Gateway 在 Tetrate，我们正在领导基于 Envoy Gateway 和 Istio 的零信任架构的定义，并将在后续博文中阐述设想的架构。如果你想和我们一起讨论架构，并了解更多关于如何为传统和云原生应用程序进行架构，请加入 tetrate-community Slack 频道。\n要了解更多关于 Tetrate 的信息，请访问 tetrate.io。\n","date":1652757120,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"eccb7acc0538e2eba6679f0c086d34c5","permalink":"https://jimmysong.io/docs/translation/the-gateway-to-a-new-frontier/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/docs/translation/the-gateway-to-a-new-frontier/","section":"translation","summary":"在我们看来，由于Envoy的设计、功能设置、安装基础和社区，它是业内最好的API网关。有了Envoy Gateway，企业可以在将Envoy嵌入其API管理策略方面增加信心。","tags":["Enovy","开源","网关","Tetrate"],"title":"Envoy API Gateway——推动网关的进一步发展","type":"translation"},{"authors":["Matt Klein"],"categories":["Envoy"],"content":"Envoy Gateway 简介 今天，我们很高兴地宣布 Envoy Gateway 成为 Envoy 代理家族的新成员，该项目旨在大幅降低将 Envoy 作为 API 网关的使用门槛。\n历史 Envoy 在 2016 年秋天开源，令我们惊讶的是，它很快就引领了整个行业。用户被这个项目的许多不同方面所吸引，包括它的包容性社区、可扩展性、API 驱动的配置模型、强大的可观察性输出和越来越广泛的功能集。\n尽管在其早期历史中，Envoy 成为了服务网格的代名词，但它在 Lyft 的首次使用实际上是作为 API 网关 / 边缘代理，提供深入的可观察性输出，帮助 Lyft 从单体架构迁移到微服务架构。\n在过去的 5 年多时间里，我们看到 Envoy 被大量的终端用户采用，既可以作为 API 网关，也可以作为服务网格中的 sidecar 代理。同时，我们看到围绕 Envoy 出现了一个庞大的供应商生态系统，在开源和专有领域提供了大量的解决方案。Envoy 的供应商生态系统对项目的成功至关重要；如果没有对所有在 Envoy 上兼职或全职工作的员工的资助，这个项目肯定不会有今天的成就。\nEnvoy 作为许多不同的架构类型和供应商解决方案的组成部分，其成功的另一面是它本质上位于架构底层；Envoy 并不是一个容易学习的软件。虽然该项目在世界各地的大型工程组织中取得了巨大的成功，但在较小和较简单的用例中，它只被轻度采用，在这些用例中，nginx 和 HAProxy 仍占主导地位。\nEnvoy Gateway 项目的诞生是出于这样的信念：将 Envoy 作为 API 网关的角色推向大众需要两个主要条件：\n 一个简化的部署模型和 API 层，旨在满足轻量级使用 将现有的 CNCF API 网关项目（Contour 和 Emissary）合并为一个共同的核，可以提供最好的用户体验，同时仍然允许供应商在 Envoy Proxy 和 Envoy Gateway 的基础上建立增值解决方案。  我们坚信，如果社区汇聚在单一的以 Envoy 为核心的 API 网关周围，它将会：\n 减少围绕安全、控制平面技术细节和其他共同关切的重复工作。 允许供应商专注于在 Envoy Proxy 和 Envoy Gateway 的基础上以扩展、管理平面 UI 等形式分层提供增值功能。 众人拾柴火焰高，让全世界更多的用户享受到 Envoy 的好处，无论组织的大小。更多的用户为更多的潜在客户提供了良性循环，为 Envoy 的核心项目提供了更多的支持，也为所有人提供了更好的整体体验。  项目概要 总得来说，Envoy Gateway 可以被认为是 Envoy Proxy 核心的一个封装器。它不会以任何方式改变核心代理、xDS、go-control-plane 等（除了潜在的驱动功能、bug 修复和一般改进以外）。它将提供以下功能：\n 为网关用例提供简化的 API。该 API 将是带有一些 Envoy 特定扩展的 Kubernetes Gateway API。之所以选择这个 API，是因为在 Kubernetes 上作为 Ingress Controller 部署是该项目最初的重点，而且该 API 得到了业界的广泛认可。 开箱即用，让用户能够尽可能快地启动和运行。这包括提供控制器资源、控制平面资源、代理实例等的生命周期管理功能。 可扩展的 API 平面。虽然该项目将致力于使常见的 API 网关功能开箱即用（例如，速率限制、认证、Let’s Encrypt 集成等），但供应商将能够提供所有 API 的 SaaS 版本，提供额外的 API 和增值功能，如 WAF、增强的可观察性、混乱工程等。 高质量的文档和入门指南。我们对 Envoy Gateway 的主要目标是使最常见的网关用例对普通用户来说可以信手拈来。  关于 API，我们认为导致混乱的主要是在针对高级用例时，在其他项目中有效地重新实现 Envoy 的 xDS API。这种模式导致用户不得不学习多个复杂的 API（最终转化为 xDS），才能完成工作。因此，Envoy Gateway 致力于 “硬性规定”，即 Kubernetes Gateway API（以及该 API 中任何允许的扩展）是唯一被支持的额外 API。更高级的用例将由 “xDS 模式” 提供服务，其中现有的 API 资源将为最终用户自动翻译，然后他们可以切换到直接利用 xDS API。这将导致一个更清晰的主 API，同时为那些可能超越主 API 的表达能力并希望通过 xDS 利用 Envoy 的全部功能的组织提供了路径。\n关于 API 的标准化 虽然 Envoy Gateway 的目标是提供一个参考实现，以便在 Kubernetes 中作为 Ingress Controller 轻松运行 Envoy，但这项工作最主要的目的是 API 标准化。随着行业在特定的 Envoy Kubernetes Gateway API 扩展上的趋同，它将允许供应商轻松地提供替代的 SaaS 实现，如果用户超越了参考实现，想要额外的支持和功能等，这可能是最好的。显然，围绕定义 API 扩展，确定哪些 API 是必需的，哪些是可选的，等等，还有很多工作要做。这是我们标准化之旅的开始，我们渴望与所有感兴趣的人一起深入研究。\n接下来的计划 今天，我们感谢 Envoy Gateway 的最初赞助商（Ambassador Labs、Fidelity、Tetrate 和 VMware），很高兴能与大家一起开始这个新的旅程。该项目是非常早期的，到目前为止的重点是商定 目标和高水平的设计，所以现在是参与的好时机，无论是作为终端用户还是作为系统集成商。\n我们还想非常清楚地说明，Contour 和 Emissary 的现有用户不会被抛在后面。该项目（以及 VMware 和 Ambassador Labs）完全致力于确保这些项目的用户最终能够顺利地迁移到 Envoy Gateway，无论是通过翻译和替换，还是通过这些项目成为 Envoy Gateway 核心的包装物。\n我们对通过 Envoy Gateway 项目将 Envoy 带给更大的用户群感到非常兴奋，我们希望你能加入我们的旅程。\n","date":1652706720,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"035010e997a726066dee0d3b0bd17560","permalink":"https://jimmysong.io/docs/translation/introducing-envoy-gateway/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/docs/translation/introducing-envoy-gateway/","section":"translation","summary":"今天，我们很高兴地宣布 Envoy Gateway 成为 Envoy 代理家族的新成员，该项目旨在大幅降低将 Envoy 作为 API 网关的使用门槛。","tags":["Enovy","开源","网关"],"title":"开源项目 Envoy Gateway 简介","type":"translation"},{"authors":["Liam Byrne"],"categories":["Service Mesh"],"content":"什么是 Wasm 插件？ 你可以使用 Wasm 插件在数据路径上添加自定义代码，轻松地扩展服务网格的功能。可以用你选择的语言编写插件。目前，有 AssemblyScript（TypeScript-ish）、C++、Rust、Zig 和 Go 语言的 Proxy-Wasm SDK。\n在这篇博文中，我们描述了如何使用 Wasm 插件来验证一个请求的有效载荷。这是 Wasm 与 Istio 的一个重要用例，也是你可以使用 Wasm 扩展 Istio 的许多方法的一个例子。您可能有兴趣阅读我们关于在 Istio 中使用 Wasm 的博文，并观看我们关于在 Istio 和 Envoy 中使用 Wasm 的免费研讨会的录音。\n何时使用 Wasm 插件？ 当你需要添加 Envoy 或 Istio 不支持的自定义功能时，你应该使用 Wasm 插件。使用 Wasm 插件来添加自定义验证、认证、日志或管理配额。\n在这个例子中，我们将构建和运行一个 Wasm 插件，验证请求 body 是 JSON，并包含两个必要的键 ——id 和 token。\n编写 Wasm 插件 这个示例使用 tinygo 来编译成 Wasm。确保你已经安装了 tinygo 编译器。\n配置 Wasm 上下文 首先配置 Wasm 上下文，这样 tinygo 文件才能操作 HTTP 请求：\npackage main import ( \u0026#34;github.com/tetratelabs/proxy-wasm-go-sdk/proxywasm\u0026#34; \u0026#34;github.com/tetratelabs/proxy-wasm-go-sdk/proxywasm/types\u0026#34; \u0026#34;github.com/tidwall/gjson\u0026#34; ) func main() { // SetVMContext 是配置整个 Wasm VM 的入口。请确保该入口在 main 函数中调用，否则 VM 将启动失败。 \tproxywasm.SetVMContext(\u0026amp;vmContext{}) } // vmContext 实现 proxy-wasm-go SDK 的 types.VMContext 接口。 type vmContext struct { // 在这里嵌入默认的虚拟机环境，我们不需要实现所有方法。 \ttypes.DefaultVMContext } // 复写 types.DefaultVMContext func (*vmContext) NewPluginContext(contextID uint32) types.PluginContext { return \u0026amp;pluginContext{} } // pluginContext 实现 proxy-wasm-go SDK 的 types.PluginContext 接口 type pluginContext struct { // 在这里侵入默认的插件上下文，我们不需要实现所有方法。 \ttypes.DefaultPluginContext } // 复写 types.DefaultPluginContext func (ctx *pluginContext) NewHttpContext(contextID uint32) types.HttpContext { return \u0026amp;payloadValidationContext{} } // payloadValidationContext 实现 proxy-wasm-go SDK 的 types.HttpContext 接口 type payloadValidationContext struct { // 在这里嵌入默认的根 http 上下文，我们不需要实现所有方法。 \ttypes.DefaultHttpContext totalRequestBodySize int } 验证负载 内容类型头是通过实现 OnHttpRequestHeaders 来验证的，一旦从客户端收到请求头，就会调用该头。\nproxywasm.SendHttpResponse 用于响应 403 forbidden 的错误代码和信息，如果内容类型丢失的话。\nfunc (ctx *payloadValidationContext) OnHttpRequestHeaders(numHeaders int, endOfStream bool) types.Action { contentType, err := proxywasm.GetHttpRequestHeader(\u0026#34;content-type\u0026#34;) if err != nil || contentType != \u0026#34;application/json\u0026#34; { // 如果 header 没有期望的 content type，返回 403 响应 \tif err := proxywasm.SendHttpResponse(403, nil, []byte(\u0026#34;content-type must be provided\u0026#34;), -1); err != nil { proxywasm.LogErrorf(\u0026#34;failed to send the 403 response: %v\u0026#34;, err) } // 终止 ActionPause 对流量的进一步处理 \treturn types.ActionPause } // ActionContinue 让主机继续处理 body \treturn types.ActionContinue } 请求主体是通过实现 OnHttpRequestBody 来验证的，每次从客户端接收到请求的一个块时，都会调用该请求。这是通过等待直到 endOfStream 为真并记录所有收到的块的总大小来完成的。一旦收到整个主体，就会使用 proxywasm.GetHttpRequestBody 读取，然后可以使用 golang 进行验证。\n这个例子使用 gjson，因为 tinygo 不支持 golang 的默认 JSON 库。它检查有效载荷是否是有效的 JSON，以及键 id 和 token 是否存在。\nfunc (ctx *payloadValidationContext) OnHttpRequestBody(bodySize int, endOfStream bool) types.Action { ctx.totalRequestBodySize += bodySize if !endOfStream { // OnHttpRequestBody 等待收到到 body 的全部才开始处理。 \treturn types.ActionPause } body, err := proxywasm.GetHttpRequestBody(0, ctx.totalRequestBodySize) if err != nil { proxywasm.LogErrorf(\u0026#34;failed to get request body: %v\u0026#34;, err) return types.ActionContinue } if !validatePayload(body) { // 如果验证失败，发送 403 响应。 \tif err := proxywasm.SendHttpResponse(403, nil, []byte(\u0026#34;invalid payload\u0026#34;), -1); err != nil { proxywasm.LogErrorf(\u0026#34;failed to send the 403 response: %v\u0026#34;, err) } // 终止流量 \treturn types.ActionPause } return types.ActionContinue } // validatePayload 验证给定的 json 负载 // 注意该函数使用 gjson 解析 json，因为 TinyGo 不支持 encoding/json func validatePayload(body []byte) bool { if !gjson.ValidBytes(body) { proxywasm.LogErrorf(\u0026#34;body is not a valid json: %v\u0026#34;, body) return false } jsonData := gjson.ParseBytes(body) // 验证 json。检查示例中是否存在必须的键 \tfor _, requiredKey := range []string{\u0026#34;id\u0026#34;, \u0026#34;token\u0026#34;} { if !jsonData.Get(requiredKey).Exists() { proxywasm.LogErrorf(\u0026#34;required key (%v) is missing: %v\u0026#34;, requiredKey, jsonData) return false } } return true } 编译成 Wasm 使用 tinygo 编译器编译成 Wasm：\ntinygo build -o main.wasm -scheduler=none -target=wasi main.go 部署 Wasm 插件 打包到 Docker 中部署到 Envoy 对于开发，这个插件可以在 Docker 中部署到 Envoy。下面的 Envoy 配置文件将设置 Envoy 监听 localhost:18000，运行所提供的 Wasm 插件，并在成功后响应 HTTP 200 和文本 hello from server。突出显示的部分是配置 Wasm 插件。\nstatic_resources:listeners:- name:mainaddress:socket_address:address:0.0.0.0port_value:18000filter_chains:- filters:- name:envoy.http_connection_managertyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManagerstat_prefix:ingress_httpcodec_type:autoroute_config:name:local_routevirtual_hosts:- name:local_servicedomains:- \u0026#34;*\u0026#34;routes:- match:prefix:\u0026#34;/\u0026#34;route:cluster:web_servicehttp_filters:- name:envoy.filters.http.wasmtyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/udpa.type.v1.TypedStructtype_url:type.googleapis.com/envoy.extensions.filters.http.wasm.v3.Wasmvalue:config:vm_config:runtime:\u0026#34;envoy.wasm.runtime.v8\u0026#34;code:local:filename:\u0026#34;./main.wasm\u0026#34;- name:envoy.filters.http.router- name:staticreplyaddress:socket_address:address:127.0.0.1port_value:8099filter_chains:- filters:- name:envoy.http_connection_managertyped_config:\u0026#34;@type\u0026#34;: …","date":1652447520,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"8af1f26bf600892899df1a8391cdf016","permalink":"https://jimmysong.io/docs/translation/validating-a-request-payload-with-wasm/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/docs/translation/validating-a-request-payload-with-wasm/","section":"translation","summary":"本文是一个使用 Go 语言开发 Wasm 插件验证请求负载，并将其部署到 Istio 或 Envoy 上的教程。","tags":["wasm","istio","envoy"],"title":"使用 WebAssembly 验证请求负载","type":"translation"},{"authors":null,"categories":null,"content":"本报告译自 O’Reilly 出品的 The Future of Observablity with OpeTelemetry，作者 Ted Young，译者 Jimmy Song。\n关于本书 本书内容包括：\n OpenTelemetry 如何满足库作者、应用程序拥有者、运维和响应者的需求 应用程序的不同角色如何围绕 OpenTelemetry 来协同和独立工作 关于在组织中采用和管理 OpenTelemetry 的实用建议  关于作者 Ted Young 是 OpenTelemetry 项目的联合创始人之一。在过去的二十年里，他设计并建立了各种大规模的分布式系统，包括可视化 FX 管道和容器调度系统。他目前在 Lightstep 公司担任开发者教育总监，住在俄勒冈州波特兰的一个小农场里。\n许可证 您可以使用署名 - 非商业性使用 - 相同方式共享 4.0 (CC BY-NC-SA 4.0) 协议共享。\n","date":1651795200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"3d78837fcc0503a24f74c332111a3100","permalink":"https://jimmysong.io/docs/book/opentelemetry-obervability/","publishdate":"2022-05-06T00:00:00Z","relpermalink":"/docs/book/opentelemetry-obervability/","section":"book","summary":"利用格式化数据从根本上改变可观察性实践","tags":["ebook","translation"],"title":"OpenTelemetry可观察性指南🔥","type":"publication"},{"authors":["Ariane van der Steldt","Radha Kumari"],"categories":["Envoy"],"content":"Slack 有一个全球客户群，在高峰期有数百万同时连接的用户。用户之间的大部分通信涉及到向对方发送大量的微小信息。在 Slack 的大部分历史中，我们一直使用 HAProxy 作为所有传入流量的负载均衡器。今天，我们将讨论我们在使用 HAProxy 时所面临的问题，我们如何用 Envoy Proxy 来解决这些问题，迁移所涉及的步骤，以及结果是什么。让我们开始吧！\nSlack 的 Websockets 为了即时传递信息，我们使用 websocket 连接，这是一种双向的通信链接，负责让你看到 “有几个人在打字……\u0026#34;，然后是他们打的东西，速度几乎是光速的。websocket 连接被摄取到一个叫做 “wss”（WebSocket 服务）的系统中，可以通过 wss-primary.slack.com 和 wss-backup.slack.com（这不是网站，如果去访问，只会得到一个 HTTP 404）从互联网上访问。\n   显示websockets工作原理的图表  Websocket 连接一开始是普通的 HTTPS 连接，然后客户端发出协议切换请求，将连接升级为 Websocket。在 Slack，我们有不同的 websocket 服务，专门用于消息、在线（列出哪些联系人在线）和其他服务。其中一个 websocket 端点是专门为需要与 Slack 互动的应用程序制作的（因为应用程序也想要实时通信）。\n   解释流量如何被路由到后端服务的流程图  过去，我们在多个 AWS Region 有一组专门用于 websockets 的 HAProxy 实例，以终止靠近用户的 websocket 连接，并将请求转发给相应的后端服务。\n迁移到 Envoy Proxy 的动机 虽然我们从 Slack 开始就一直在使用 HAproxy，并且知道如何大规模地操作它，但有一些操作上的挑战让我们考虑替代方案，比如 Envoy Proxy。\n热重启 在 Slack，后端服务端点列表的变化是一个常见的事件（由于实例被添加或删除）。HAProxy 提供两种方法来更新其配置，以适应端点列表的变化。一种是使用 HAProxy Runtime API。我们在其中一套 HAProxy 实例中使用了这种方法，我们的经验在另一篇博文中有所描述 —— 在 Slack 的可怕的、恐怖的、没有好处的、非常糟糕的一天。另一种方法，我们用于 websockets 负载均衡器（LB），是将后端渲染到 HAProxy 配置文件中，然后重新加载 HAProxy。\n每次 HAProxy 重载时，都会创建一组新的进程来处理新进入的连接。我们会让旧的进程持续运行很多小时，以便让长寿的 websocket 连接耗尽，避免用户频繁断开连接。然而，我们不能有太多的 HAProxy 进程，每个进程都运行着它自己 “当时” 的配置副本 —— 我们希望实例能更快地汇聚到新版本的配置上。我们不得不定期收割旧的 HAProxy 进程，并限制 HAProxy 重新加载的频率，以防底层后端出现混乱。\n无论我们使用哪种方法，都需要一些额外的基础设施来管理 HAProxy 的重新加载。\nEnvoy 允许我们使用动态配置的集群和端点，这意味着如果端点列表发生变化，它不需要重新加载。如果代码或配置确实发生了变化，Envoy 有能力在不放弃任何连接的情况下热重启自己。Envoy 通过 inotify 观察文件系统配置的更新。在热重启过程中，Envoy 还将统计数据从父进程复制到子进程中，因此仪表和计数器不会被重置。\n这一切都使 Envoy 的运营开销大大减少，而且不需要额外的服务来管理配置变化或重新启动。\n负载均衡功能 Envoy 提供了一些先进的负载均衡功能，如：\n 内置支持区域感知路由的功能 通过异常值检测进行被动健康检查 恐慌路由：Envoy 通常只将流量路由到健康的后端，但是如果健康主机的百分比低于某个阈值，它可以被配置为将流量发送到所有的后端，不管是健康的还是不健康的。这在我们 2021 年 1 月 4 日的故障中非常有帮助，这次故障是由我们基础设施中的一个广泛的网络问题引起的。  由于上述原因，在 2019 年，我们决定将我们的入口负载均衡层从 HAproxy 迁移到 Envoy Proxy，从 websockets 堆栈开始。迁移的主要目标是提高可操作性，获得 Envoy 提供的新功能，以及更加标准化。通过在整个 Slack 中从 HAProxy 迁移到 Envoy，我们的团队将不再需要了解两个软件的怪异之处，不再需要维护两种不同的配置，不再需要管理两个构建和发布管道，诸如此类。那时，我们已经在使用 Envoy Proxy 作为我们服务网格中的数据平面。我们内部也有经验丰富的 Envoy 开发人员，所以我们可以随时获得 Envoy 的专业知识。\n生成 Envoy 配置 这次迁移的第一步是审查我们现有的 websocket 层配置，并生成一个同等的 Envoy 配置。在迁移过程中，管理 Envoy 配置是我们最大的挑战之一。Envoy 有丰富的功能集，其配置与 HAProxy 的配置有很大的不同。Envoy 配置涉及四个主要概念：\n Listener，接收请求，又称 TCP 套接字、SSL 套接字或 unix 域套接字。 Cluster，代表我们发送请求的内部服务，如消息服务器和存在服务器 Route，将 Listener 和 Cluster 连接在一起 Filter，它对请求进行操作  Slack 的配置管理主要是通过 Chef 完成的。当我们开始使用 Envoy 时，我们把 Envoy 配置作为 chef 模板文件来部署，但它的管理变得很麻烦，而且容易出错。为了解决这个问题，我们建立了 chef 库和自定义资源来生成 Envoy 配置。\n   Chef 资源的结构和流程图  在 Chef 内部，配置是一个单例，模拟了每个主机只有一个 Envoy 配置的情况。所有的 Chef 资源都在这个单例上操作，添加监听器、路由或集群。在 Chef 运行的最后，envoy.yaml 被生成、验证，然后安装 —— 我们从不写中间配置，因为这些配置可能是无效的。\n这个例子展示了我们如何创建一个有两条路由的 HTTP 监听器，将流量路由到两个动态集群。\n   调用Chef资源以创建带有集群和路由的监听器的例子  要在 Envoy 中复制我们复杂的 HAProxy 配置需要一些努力。大部分需要的功能在 Envoy 中已经有了，所以只需要在 chef 库中加入对它的支持就可以了。我们实现了一些缺失的 Envoy 功能（有些是上游贡献的，有些是内部维护的扩展）。\n对我们的新配置进行测试和验证 测试新的 Envoy websockets 层是一个迭代的过程。我们经常用手工编码的 Envoy 配置做原型，并在本地的开发机器上测试，每个监听器、路由和集群都有一个。手工编码的修改一旦成功，就会被移到 chef 库中。\nHTTP 路由是用 curl 测试的：\n 基于头和 cookie 的特定路由到特定后端 基于路径、前缀和查询参数的路由到特定后端 SSL 证书  当事情没有达到预期效果时，我们在机器上使用 Envoy 调试日志。调试日志清楚地解释了为什么 Envoy 选择将一个特定的请求路由到一个特定的集群。Envoy 的调试日志非常有用，但也很冗长，而且很昂贵（你真的不想在生产环境中启用这个功能）。调试日志可以通过 Curl 启用，如下所示。\ncurl -X POST http://localhost:\u0026lt;envoy_admin_port\u0026gt;/logging?level=debug Envoy 管理接口在初始调试时也很有用，特别是这些端点：\n clusters：显示所有配置的集群，包括每个集群中所有上游主机的信息以及每个主机的统计数据。 /certs：以 JSON 格式显示所有加载的 TLS 证书，包括文件名、序列号、主体替代名称和到期前的天数。 /listeners：显示所有配置的监听器及其名称和地址。  我们的 Chef 库使用 -mode validate 命令行选项运行 Envoy，作为一个验证步骤，以防止安装无效的配置。这也可以手动完成。\nsudo /path/to/envoy/binary -c \u0026lt;/path/to/envoy.yaml\u0026gt; --mode validate Envoy 提供 JSON 格式的监听器日志。我们将这些日志录入我们的日志管道（当然是在对日志进行 PII 处理后），这对调试工作经常很有帮助。\n一旦对开发环境中的配置有信心，我们就准备做一些更多的测试 – 在生产中！\u0026#34;。\n迁移至生产 为了将迁移过程中的风险降到最低，我们建立了一个新的 Envoy websocket 栈，其配置与现有的 HAProxy 层相当。这意味着我们可以逐步、有控制地将流量转移到新的 Envoy 堆栈，并且在必要时可以快速切换回 HAProxy。缺点是我们的 AWS 成本 —— 我们在迁移过程中使用了双倍的资源，但我们愿意花费时间和资源为我们的客户透明地进行迁移。\n我们通过 NS1 管理我们的 DNS 记录 wss-primary.slack.com 和 wss-backup.slack.com。我们使用加权路由将流量从 haproxy-wss 转移到 envoy-wss NLB DNS 名称。第一批区域是以 10%、25%、50%、75% 和 100% 的步骤单独上线的。由于我们对新的 Envoy 层和上线过程有信心，所以最后的区域上线速度更快（25%、50%、75%、100% 只需两天，而之前的一个区域需要一周的时间）。\n尽管迁移工作很顺利，没有出现故障，但还是出现了一些小问题，比如超时值和 header 的差异。在迁移过程中，我们多次恢复、修复，并再次上线。\n   流程图显示DNS迁移过程中涉及的组件和步骤  经过漫长而激动人心的 6 个月，迁移完成了，整个 HAProxy websocket 堆栈在全球范围内被 Envoy Proxy 取代，对客户的影响为零。\n哪些进展顺利，哪些不顺利 迁移本身是相对平淡和无聊的。枯燥是一件好事：刺激意味着事情的中断，枯燥意味着一切顺利。\n我们发现，旧的 HAProxy 配置随着时间的推移而有机地增长。它在很大程度上是由 HAProxy 使用的模型形成的 —— 一个包括所有监听器的大型配置。Envoy 的配置模型比 HAProxy 的模型使用更多的定义范围。一旦一个监听器被输入，只有该监听器内的规则适用于请求。一旦输入一个路由，只有该路由上的规则适用。这使得将规则与相关的请求联系起来更加容易。\n我们花了很长时间从旧的 HAProxy 配置中提取重要的东西，这实际上是技术债务。通常很难弄清楚为什么会有某个规则，哪些是有意的，哪些是无意的，以及其他服务所依赖的行为是什么。例如，有些服务应该只在两个虚拟主机（vhosts）中的一个下，但实际上在 HAProxy 的两个 vhosts 下都可用。我们不得不复制这个错误，因为现有的代码依赖于这种行为。\n我们在 HAProxy 堆栈中错过了一些细微的东西。有时这些是很重要的 —— 我们破坏了 Slack 的每日活跃用户（DAU）指标（哎呀！）。也有很多小问题需要解决。负载均衡器的行为很复杂，除了花时间调试外，没有真正的办法解决这个问题。\n我们开始迁移时，没有为负载均衡器的配置提供测试框架。我们没有自动测试来验证测试的 URL 路由到正确的端点以及与请求和响应头相关的行为，而是有…… 一个 HAProxy 配置。在迁移过程中，测试是很有帮助的，因为它们可以提供很多关于预期行为的原因的背景。因为我们缺乏测试，所以我们经常不得不向服务所有者询问，以了解他们所依赖的行为。\n我们建立的 Chef 资源有意只支持 Envoy 功能的一个子集。这使我们的库更简单 —— 我们只需要考虑我们实际使用的功能。缺点是，每次我们想使用新的 Envoy 功能时，都必须在我 …","date":1649394000,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"bac836b0677d3b9584eb09dae8470576","permalink":"https://jimmysong.io/docs/translation/migrating-millions-of-concurrent-websockets-to-envoy/","publishdate":"2022-04-08T13:00:00+08:00","relpermalink":"/docs/translation/migrating-millions-of-concurrent-websockets-to-envoy/","section":"translation","summary":"本文是 Slack 花半年时间从 HAProxy 迁移到 Envoy 上的经验分享。","tags":["Enovy","slcak","haproxy","WebSockets"],"title":"Slack 将数百万个并发的 Websockets 迁移到 Envoy 上经验分享","type":"translation"},{"authors":null,"categories":null,"content":"本书译自 The Developer Advocacy Handbook，副标题「开发者布道师的自我修养」为译者自拟。\n 作者：Christian Heilmann 译者：宋净超（Jimmy Song） 开始阅读 下载电子书  译者序 随着开源软件的流行，企业的开发者布道（Developer Advocacy）的需求不断增长，开发者布道师（Developer Advocate）的职业前景也越来越广阔。这本书将揭开大众口中的“布道师”的神秘面纱，让你了解这一角色的工作内容；对于各位布道师来说，这本书也将成为你整个布道生涯的得力助手，指导你布道的方方面面。\n作者 Christian Heilmann 是一个德国人，从事开发者布道工作已有十余年，译者本身也有多年从事开发者布道工作的经验。目前在中文互联网上，对“布道师”这一角色还存在很多猜疑和误解，相信这本书可以化解开发者对布道师这一角色的疑虑。虽然书中提到的一些技术在中国可能并不适用，但是中国也有同类出色的替代品，我相信人情总是相通的，技术只是手段而已。\n由于笔者完全凭个人兴趣，利用业余时间翻译此书，翻译过程中难免遗漏和错误，欢迎提交 Issue 评论和 PR 修改。如果你有关于开发者布道、开源、云原生及开源相关的问题，欢迎与我交流。\n许可证 本书所有内容支持使用署名 - 非商业性使用 - 相同方式共享 4.0 (CC BY-NC-SA 4.0) 协议共享。\n","date":1648080000,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"6e2457fbeab76e72de47a3f208acdd81","permalink":"https://jimmysong.io/docs/book/developer-advocacy-handbook/","publishdate":"2022-03-24T00:00:00Z","relpermalink":"/docs/book/developer-advocacy-handbook/","section":"book","summary":"开发者布道师的自我修养","tags":["ebook","translation"],"title":"开发者布道手册","type":"publication"},{"authors":null,"categories":null,"content":"本书译自美国国家标准标准与技术研究院（NIST）Special Publication 800-204C。\n关于本书 作者：Ramaswamy Chandramouli\n审阅\n 计算机安全司信息技术实验室 美国商务部 Gina M. Raimondo，秘书 国家标准和技术研究所 James K. Olthoff，履行负责标准和技术的商务部副部长兼国家标准和技术研究所所长的非专属职能和职责  原出版物可在：https://doi.org/10.6028/NIST.SP.800-204C 免费获取，中文版请在此阅读。\n","date":1647216000,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"77e3afee877672a563686411fc92a285","permalink":"https://jimmysong.io/docs/book/service-mesh-devsecops/","publishdate":"2022-03-14T00:00:00Z","relpermalink":"/docs/book/service-mesh-devsecops/","section":"book","summary":"NIST特别发布","tags":["ebook","translation"],"title":"利用服务网格实施微服务的DevSecOps","type":"publication"},{"authors":null,"categories":null,"content":"本书起始于2017年3月，记录了本人从零开始学习和使用Kubernetes的心路历程，着重于经验分享和总结，同时也会有相关的概念解析，希望能够帮助大家少踩坑，少走弯路，还会指引大家关注Kubernetes生态周边，如微服务构建、DevOps、大数据应用、服务网格（Service Mesh）、云原生等领域。\n开始之前 在阅读本书之前希望您掌握以下知识和准备以下环境：\n Linux 操作系统原理 Linux 常用命令 Docker 容器原理及基本操作 一台可以上网的电脑，Mac/Windows/Linux 皆可 安装 Docker  本书主题 本书的主题不局限于Kubernetes，还包括以下几大主题：\n 云原生开源组件 云原生应用与微服务架构 基于Kubernetes的Service Mesh架构 Kubernetes与微服务结合实践  起初写作本书时，安装的所有组件、所用示例和操作等皆基于 Kubernetes 1.6+ 版本，同时我们也将密切关注Kubernetes的版本更新，随着它的版本更新升级，本书中的Kubernetes版本和示例也将随之更新。\n使用方式 您可以通过以下方式使用本书：\n GitHub GitBook 在线浏览 下载本书的发行版 按照说明自行编译成离线版本 Fork 一份添加你自己的笔记自行维护，有余力者可以一起参与进来  注意：本书中的 Service Mesh 相关内容已不再维护，请转至 istio-handbook 浏览。\n快速开始 如果您想要学习Kubernetes和云原生应用架构但是又不想自己从头开始搭建和配置一个集群，那么可以直接使用kubernetes-vagrant-centos-cluster项目直接在本地部署一个3节点的分布式集群及其他如Heapster、EFK、Istio等可选组件，或者使用更加轻量级的cloud-native-sandbox在个人电脑上使用Docker运行单节点的Kubernetes、Istio等组件。\n贡献与致谢 感谢大家对本书做出的贡献！\n 查看贡献者列表 查看如何贡献 查看文档的组织结构与使用方法  ","date":1646784000,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"28cfdf5e03d31fe45dbec103a514ab86","permalink":"https://jimmysong.io/docs/book/kubernetes-handbook/","publishdate":"2022-03-09T00:00:00Z","relpermalink":"/docs/book/kubernetes-handbook/","section":"book","summary":"云原生应用架构实战手册","tags":["handbook"],"title":"Kubernetes中文指南🔥","type":"publication"},{"authors":["Thomas Graf"],"categories":["service mesh"],"content":"译者注：本文作者是 Isovalent 联合创始人\u0026amp;CTO，原文标题 How eBPF will solve Service Mesh - Goodbye Sidecars，作者回顾了Linux 内核的连接性，实现服务网格的几种模式，以及如何使用 eBPF 实现无 Sidecar 的服务网格。\n 什么是服务网格？ 随着分布式应用的引入，额外的可见性、连接性和安全性要求也浮出水面。应用程序组件通过不受信任的网络跨越云和集群边界进行通信，负载均衡、弹性变得至关重要，安全必须发展到发送者和接收者都可以验证彼此的身份的模式。在分布式应用的早期，这些要求是通过直接将所需的逻辑嵌入到应用中来解决的。服务网格将这些功能从应用程序中提取出来，作为基础设施的一部分提供给所有应用程序使用，因此不再需要修改每个应用程序。\n   服务网格示意图  纵观今天服务网格的功能设置，可以总结为以下几点：\n 弹性连接：服务与服务之间的通信必须能够跨越边界，如云、集群和场所。通信必须是有弹性的和容错的。 L7 流量管理：负载均衡、速率限制和弹性必须是 L7 感知的（HTTP、REST、gRPC、WebSocket 等）。 基于身份的安全：依靠网络标识符来实现安全已经不够了，发送和接收服务都必须能够根据身份而不是网络标识符来验证对方。 可观察性和跟踪：追踪和指标形式的可观察性对于理解、监控和排除应用程序的稳定性、性能和可用性至关重要。 透明：该功能必须以透明的方式提供给应用程序，即不需要改变应用程序代码。  在早期，服务网格的功能通常是以库的形式实现的，要求网格中的每个应用程序都要链接到以应用程序的语言框架编写的库。类似的事情也发生在互联网的早期：曾几何时，应用程序还需要运行自己的 TCP/IP 协议栈！正如我们将在这篇文章中讨论的那样，服务网格正在发展成为一种内核责任，就像网络堆栈一样。\n   基于库的服务网格模型  今天，服务网格通常使用一种叫做 sidecar 模型的架构来实现。这种架构将实现上述功能的代码封装到第四层代理中，服务间的流量被重定向到这个所谓的 sidecar 代理。它之所以被称为 “挎斗”，是因为每个应用程序都有一个代理，就像挎斗附着在摩托车上一样。\n   基于 Sidecar 的服务网格模型  这种架构的优点是，服务不再需要自己实现服务网格的功能。如果许多服务是用不同的语言编写部署的，或者如果你正在运行不可变的第三方应用程序，这就很有好处。\n这种模式的缺点是有大量的代理，许多额外的网络连接，以及复杂的重定向逻辑，将网络流量输入代理。除此之外，在什么类型的网络流量可以被重定向到第四层代理上也有限制。代理（Proxy）在其能支持的网络协议方面是有限的。\n连接性转移到内核中的历史 几十年来，在应用程序之间提供安全可靠的连接一直是操作系统的责任。有些人可能还记得早期 Unix 和 Linux 时代的 TCP 包装器和 tcpd。tcpd 允许用户在不修改应用程序的情况下透明地添加日志、访问控制、主机名验证和欺骗保护。它使用了 libwrap，而且，在一个有趣的平行于服务网格的故事中，这个库也是以前应用程序提供这些功能的链接对象。tcpd 所带来的是能够在不修改现有应用程序的情况下将这些功能透明地添加到现有应用程序中。最终，所有这些功能都进入了 Linux 本身，并以一种更有效、更强大的方式提供给所有应用程序。今天，这已经发展到了我们所知道的 iptables。\n然而，iptables 显然不适合解决现代应用的连接性、安全性和可观察性要求，因为它只在网络层面上操作，对应用协议层缺乏任何了解。自然，阻力最小的路径是回到库模型，然后是 sidecar 模型。现在，我们正处于这样一个阶段：为了最佳的透明度、效率和安全性，在操作系统中原生地支持这种模式是有意义的。\n   服务网格的进化  在 tcpd 时代，曾经的连接记录现在是追踪。IP 层面的访问控制已经演变成应用协议层面的授权，例如使用 JWT。主机名验证已被更强大的认证所取代，如 mTLS。网络负载均衡已经扩展到 L7 流量管理。HTTP 重试是新的 TCP 重传。过去用黑洞路由解决的问题今天被称为断路。这些都不是根本性的新问题，但所需的环境和控制已经发生了变化。\n扩展内核命名空间概念 Linux 内核已经有一个概念，可以共享共同的功能，并使其对系统上运行的许多应用程序可用。这个概念被称为命名空间（Namespace），它构成了我们今天所知的容器技术的基础。命名空间（内核的那种，不是 Kubernetes 的命名空间）存在于各种抽象中，包括文件系统、用户管理、挂载设备、进程、网络等。这就是允许单个容器呈现不同的文件系统视图、不同的用户集，以及允许多个容器绑定到单个主机上的同一网络端口。在 cgroups 的帮助下，这个概念得到了扩展，可以对 CPU、内存和网络等资源进行管理和优先排序。从云原生应用开发者的角度来看，cgroups 和资源被紧密地整合到我们所知的 “容器” 概念中。\n符合逻辑的是，如果我们认为服务网格是操作系统的责任，那么它必须符合并整合命名空间和 cgroup 的概念。这看起来会是这样的。\n   Service Mesh Namespace  不出所料，这看起来非常自然，而且可能是大多数用户从简单的角度所期望的。应用程序保持不变，它们继续使用套接字进行通信，就像以前那样。理想的服务网格是作为 Linux 的一部分透明地提供的。它就在那里，就像今天的 TCP 一样。\n注入 Sidecar 的成本 如果我们仔细研究一下 sidecar 模型，我们会发现它实际上是在试图模仿这种模型。应用程序继续使用套接字，一切都被塞进 Linux 内核的网络命名空间。然而，这比它看起来要复杂得多，需要许多额外的步骤来透明地注入 sidecar 代理。\n   注入 Sidecar 的成本  这种额外的复杂性在延迟和额外资源消耗方面付出了巨大的代价。早期的基准测试表明，这对延迟的影响高达 3-4 倍，而且所有代理都需要大量的额外内存。在这篇文章的后面，我们将研究这两点，因为我们将其与基于 eBPF 的模型进行比较。\n用 eBPF 解锁内核服务网格 为什么我们以前没有在内核中创建一个服务网格？有些人半开玩笑地说，kube-proxy 是最初的服务网格（见我们已经构建了相当多的服务网格 - Tim Hockin, Google）。这句话是有一定道理的。Kube-proxy 是一个很好的例子，说明了 Linux 内核在依靠传统的基于网络的 iptables 功能实现服务网格时，可以达到多么接近。然而，这还不够，L7 上下文是缺失的。Kube-proxy 完全在网络数据包层面运作。现代应用需要 L7 流量管理、跟踪、认证和额外的可靠性保证。Kube-proxy 不能在网络层面上提供这些。\neBPF 改变了这个模式。它允许动态地扩展 Linux 内核的功能。我们一直在使用 eBPF 为 Cilium 建立一个高效的网络、安全和可观察性数据通路，并将其直接嵌入到 Linux 内核。应用这个相同的概念，我们也可以在内核层面上解决服务网格的要求。事实上，Cilium 已经实现了各种所需的概念，如基于身份的安全、L3-L7 可观察性和授权、加密和负载均衡。缺少的部分现在正在向 Cilium 涌来。在本博客的末尾，你会发现如何加入由 Cilium 社区推动的 Cilium 服务网格测试项目的细节。\n   eBPF 服务网格架构  有人可能想知道为什么 Linux 内核社区不直接解决这些需求。eBPF 有一个巨大的优势，eBPF 代码可以在运行时插入到现有的 Linux 内核中，类似于 Linux 内核模块，但与内核模块不同，它可以以安全和可移植的方式进行。这使得 eBPF 的实现能够随着服务网格社区的发展而继续发展。新的内核版本需要几年时间才能进入用户手中。eBPF 是一项关键技术，它使 Linux 内核能够跟上快速发展的云原生技术栈。\n无 Sidecar 的基于 eBPF 的 L7 追踪和度量 让我们看看 L7 追踪和指标可观察性，作为一个具体的例子，说明基于 eBPF 的服务网格对保持低延迟和提高观察性有巨大的影响。应用程序团队依靠应用程序的可见性和监控作为基本要求这些，这包括请求跟踪、HTTP 响应率和服务延迟信息等能力。然而，这种可观察性应该没有明显的成本（延迟、复杂性、资源…）。\n   基于 eBPF 的可视性  在下面的基准测试中，我们可以看到早期的测量结果，即通过 eBPF 或 sidecar 方法实现 HTTP 可见性对延迟的影响。该设置是在两个不同节点上运行的两个 pod 之间通过固定数量的连接每秒稳定运行 10K 个 HTTP 请求，并测量请求的平均延时。\n   基于 eBPF 的延迟基准测试 vs 基于 Sidecar 的 L7 可视性  我们故意不提这些测量中使用的具体代理，因为它并不重要。对于我们测试过的所有代理，结果几乎都是一样的。要明确的是，这不是关于 Envoy、Linkerd、Nginx 或其他代理是否更快。所提到的代理有差异，但与首先注入代理的成本相比，它们是微不足道的。几乎没有开销是来自代理本身的逻辑。开销是通过注入代理，将网络流量重定向到它，终止连接和启动新的连接而增加的。\n这些早期的测量结果表明，基于 eBPF 的内核方法是非常有前途的，可以实现完全透明的服务网格的愿望，而且没有明显的开销。\n使用 eBPF 加速的 per-node 代理 越来越多的用例可以用这种仅有 eBPF 的方法来覆盖，从而完全取消 L4 代理。有些用例，仍然需要代理。例如，当连接需要拼接时，当 TLS 终止被执行时，或对于某些形式的 HTTP 授权。\n我们的 eBPF 服务网格工作将继续关注那些从性能角度可以获得最大收益的领域。如果你必须执行 TLS 终止，你可能不介意在流量流入集群时用代理终止一次连接。然而，你会更关心在每个连接的路径中注入两个代理的影响，以提取 HTTP 指标和跟踪数据。\n当一个用例不能用纯 eBPF 的方法来实现时，网格可以回退到每个节点的代理模型，直接将代理与内核的套接字层结合起来。\n   eBPF per-node Proxy  eBPF 不依赖网络级的重定向，而是直接在套接字级别注入代理，保持路径短。在 Cilium 的案例中，正在使用 Envoy 代理，尽管从架构的角度来看，任何代理都可以被整合到这个模型。从概念上讲，这允许将 Linux 内核网络命名空间的概念直接扩展到 Envoy 监听器配置的概念，并将 Envoy 变成一个多用户代理。\nSidecar 与 per-Node 代理 即使需要代理，代理的成本也会根据部署的架构而有所不同。让我们来看看每个节点的代理模式与 sidecar 模式的比较。\n每个连接的代理 所需的网络连接数将因是否有代理而不同。最简单的情况是无 sidecar 模式，这意味着网络连接的数量没有变化。一个单一的连接将为请求提供服务，eBPF 将提供服务网格功能，如跟踪或现有连接上的负载均衡。\n   基于 eBPF 的模型  用 sidecar 模型提供同样的功能需要在连接中注入两次代理，这导致需要维护三个连接。这导致了开销的增加和所有额外的套接字缓冲区所需内存的倍增，表现为更高的服务间延迟。这就是我们之前在无 sidecar L7 可见性部分看到的 sidecar 开销。\n   基于 Sidecar 代理的模型  切换到 per-node 的代理模式使我们能够摆脱其中一个代理，因为我们不再依赖在每个工作负载中运行一个 sidecar。比起不需要额外的连接，这还是不够理想，但比起总是需要两个额外的连接要好。\n   Per-node 代理模式  所需的代理总数 在每个工作负载中运行一个 sidecar 会导致大量的代理。即使每个单独的代理实例在其内存占用方面是相当优化的，但实例的数量之多将导致总的影响很大。此外，每个代理维护的数据结构，如路由和端点 …","date":1639054980,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"1a93bf36a16fc3ff16d4019eebfb278b","permalink":"https://jimmysong.io/docs/translation/ebpf-solve-service-mesh-sidecar/","publishdate":"2021-12-09T21:03:00+08:00","relpermalink":"/docs/translation/ebpf-solve-service-mesh-sidecar/","section":"translation","summary":"本文回顾了Linux 内核的连接性，实现服务网格的几种模式，以及如何使用 eBPF 实现无 Sidecar 的服务网格。","tags":["Istio","ebpf","service mesh"],"title":"告别 Sidecar——使用 eBPF 解锁内核级服务网格","type":"translation"},{"authors":["胡渐飞"],"categories":["Istio"],"content":"Istio 1.12 中新的 WebAssembly 基础设施使其能够轻松地将额外的功能注入网格部署中。\n经过三年的努力，Istio 现在有了一个强大的扩展机制，可以将自定义和第三方 Wasm 模块添加到网格中的 sidecar。Tetrate 工程师米田武（Takeshi Yoneda）和周礼赞（Lizan Zhou）在实现这一目标方面发挥了重要作用。这篇文章将介绍 Istio 中 Wasm 的基础知识，以及为什么它很重要，然后是关于建立自己的 Wasm 插件并将其部署到网格的简短教程。\n为什么 Istio 中的 Wasm 很重要 使用 Wasm，开发人员可以更容易的扩展网格和网关。在 Tetrate，我们相信这项技术正在迅速成熟，因此我们一直在投资上游的 Istio，使配置 API、分发机制和从 Go 开始的可扩展性体验更加容易。我们认为这将使 Istio 有一个全新的方向。\n有何期待：新的插件配置 API，可靠的获取和安装机制 有一个新的顶级 API，叫做 WasmPlugin，可以让你配置要安装哪些插件，从哪里获取它们（OCI 镜像、容器本地文件或远程 HTTP 资源），在哪里安装它们（通过 Workload 选择器），以及一个配置结构体来传递给插件实例。\nistio-agent 中的镜像提取机制（在 Istio 1.9 中引入），从远程 HTTP 源可靠地检索 Wasm 二进制文件，已被扩展到支持从任何 OCI 注册处检索 Wasm OCI 镜像，包括 Docker Hub、Google Container Registry（GCR）、Amazon Elastic Container Registry（Amazon ECR）和其他地方。\n这意味着你可以创建自己的 Wasm 插件，或者从任何注册处选择现成的插件，只需几行配置就可以扩展 Istio 的功能。Istio 会在幕后做所有的工作，为你获取、验证、安装和配置它们。\nIstio Wasm 扩展 Istio 的扩展机制使用 Proxy-Wasm 应用二进制接口（ABI）规范，该规范由周礼赞和米田武带头制定，提供了一套代理无关的流媒体 API 和实用功能，可以用任何有合适 SDK 的语言来实现。截至目前，Proxy-Wasm 的 SDK 有 AssemblyScript（类似 TypeScript）、C++、Rust、Zig 和 Go（使用 TinyGo WebAssembly 系统接口「WASI」，米田武也是其主要贡献者）。\n如何获取：Tetrate Istio Distro 获得 Istio 的最简单方法是使用 Tetrate 的开源 get-mesh CLI 和 Tetrate Istio Distro，这是一个简单、安全的上游 Istio 的企业级发行版。\nWasm 实战：构建你自己的速率限制 WebAssembly 插件 在我们之前关于 Envoy 中的 Wasm 扩展的博客中，我们展示了如何开发 WebAssembly 插件来增强服务网格的能力。新的 Wasm 扩展 API 让它变得更加简单。本教程将解释如何使用 Istio Wasm 扩展 API 来实现 Golang 中的速率限制。\n先决条件  熟悉 Istio 和 Envoy 中的 Wasm。 安装 TinyGo 0.21.0 并使用 Golang 构建 Wasm 扩展。  说明 在这个例子中，我们将在集群中部署两个应用程序（sleep 和 httpbin）。我们将从一个容器向另一个容器发送几个请求，而不部署任何 Wasm 扩展。\n接下来，我们将在 Go 中创建一个 Wasm 模块，为响应添加一个自定义头，并拒绝任何请求率超过每秒两个的请求。\n我们将把 Wasm 模块推送到 Docker 镜像仓库，并使用新的 WasmPlugin 资源，告诉 Istio 从哪里下载 Wasm 模块，以及将该模块应用于哪些工作负载。\n第 1 步：安装 Istio 并部署应用程序 首先，我们将下载并安装 Istio 1.12，并标记 Kubernetes 的 default 命名空间，以便自动注入 sidecar。\ncurl -L https://git.io/getLatestIstio | ISTIO_VERSION=1.12 sh cd istio-1.12/ ./bin/istioctl install --set profile=demo -y kubectl label namespace default istio-injection=enabled --overwrite 接下来，我们将部署 httpbin 和 sleep 应用程序的示例。\nkubectl apply -f samples/httpbin/httpbin.yaml kubectl apply -f samples/sleep/sleep.yaml 应用程序部署并运行后，我们将每秒从 sleep 容器向 httpbin 容器发送 4 个请求。\n$ SLEEP_POD=$(kubectl get pod -l app=sleep -o jsonpath={.items..metadata.name}) $ kubectl exec ${SLEEP_POD} -c sleep -- sh -c \u0026#39;for i in $(seq 1 3); do curl --head -s httpbin:8000/headers; sleep 0.25; done\u0026#39; HTTP/1.1 200 OK server: envoy date: Tue, 16 Nov 2021 22:18:32 GMT content-type: application/json content-length: 523 access-control-allow-origin: * access-control-allow-credentials: true x-envoy-upstream-service-time: 2 HTTP/1.1 200 OK server: envoy date: Tue, 16 Nov 2021 22:18:32 GMT content-type: application/json content-length: 523 access-control-allow-origin: * access-control-allow-credentials: true x-envoy-upstream-service-time: 4 HTTP/1.1 200 OK server: envoy date: Tue, 16 Nov 2021 22:18:32 GMT content-type: application/json content-length: 523 access-control-allow-origin: * access-control-allow-credentials: true x-envoy-upstream-service-time: 1 你会发现所有的请求都成功了，并返回了 HTTP 200。\n第 2 步：开发、编译和推送 Wasm 模块 我们将使用 Golang 和 Proxy Wasm Golang SDK 来开发 Wasm 模块。我们将使用 SDK 资源库中的一个现有例子，叫做 istio-rate-limiting。要开始，请先克隆 Github 仓库。\ngit clone https://github.com/tetratelabs/wasm-rate-limiting cd wasm-rate-limiting/ 我们来看看 main.go 中的代码。这就是我们使用 Proxy Wasm Golang SDK 实现速率限制逻辑的地方。Wasm 模块做了两件事。\n 在响应中添加一个自定义的头。 执行 2 个请求 / 秒的速率限制，拒绝超额的请求。  下面是 main.go 的片段，显示了功能是如何实现的。\n// Modify the header func (ctx *httpHeaders) OnHttpResponseHeaders(numHeaders int, endOfStream bool) types.Action { for key, value := range additionalHeaders { proxywasm.AddHttpResponseHeader(key, value) } return types.ActionContinue } // Perform rate limiting func (ctx *httpHeaders) OnHttpRequestHeaders(int, bool) types.Action { current := time.Now().UnixNano() // We use nanoseconds() rather than time.Second() because the proxy-wasm has the known limitation. \t// TODO(incfly): change to time.Second() once https://github.com/proxy-wasm/proxy-wasm-cpp-host/issues/199 \t// is resolved and released. \tif current \u0026gt; ctx.pluginContext.lastRefillNanoSec+1e9 { ctx.pluginContext.remainToken = 2 ctx.pluginContext.lastRefillNanoSec = current } proxywasm.LogCriticalf(\u0026#34;Current time %v, last refill time %v, the remain token %v\u0026#34;, current, ctx.pluginContext.lastRefillNanoSec, ctx.pluginContext.remainToken) if ctx.pluginContext.remainToken == 0 { if err := proxywasm.SendHttpResponse(403, [][2]string{ {\u0026#34;powered-by\u0026#34;, \u0026#34;proxy-wasm-go-sdk!!\u0026#34;}, }, []byte(\u0026#34;rate limited, wait and retry.\u0026#34;), -1); err != nil { proxywasm.LogErrorf(\u0026#34;failed to send local response: %v\u0026#34;, err) proxywasm.ResumeHttpRequest() } return types.ActionPause } ctx.pluginContext.remainToken -= 1 return types.ActionContinue } 在 OnHttpResponseHeaders 函数中，我们正在迭代 extraHeaders 变量，并将头文件添加到响应中。\n在 OnHttpRequestHeaders 函数中，我们得到当前的时间戳，将其与最后一次补给时间的时间戳进行比较（对于速率限制器），如果需要的话，就补给令牌。\n如果没有剩余的令牌，我们就发送一个带有额外头的 403 响应（由：proxy-wasm-go-sdk！！）。\n让我们用 tinygo 将 Golang 程序编译成 Wasm 模块，并将其打包成一个 Docker 镜像。\ntinygo build -o main.wasm -scheduler=none -target=wasi main.go 我们构建一个 Docker 镜像，并将其推送到镜像仓库（用你自己 …","date":1637748180,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"743f475d861ee760cd0240e71082dcae","permalink":"https://jimmysong.io/docs/translation/istio-wasm-extensions-and-ecosystem/","publishdate":"2021-11-24T18:03:00+08:00","relpermalink":"/docs/translation/istio-wasm-extensions-and-ecosystem/","section":"translation","summary":"Istio 1.12 中新的 WebAssembly 基础设施使其能够轻松地将额外的功能注入网格部署中。","tags":["Envoy","Wasm","Istio","Tetrate"],"title":"Istio 1.12 引入 Wasm 插件配置 API 用于扩展 Istio 生态","type":"translation"},{"authors":null,"categories":["Istio"],"content":"译者注：本文译自 Istio 官方博客，博客原标题 gRPC Proxyless Service Mesh，其实是 Istio 1.11 版本中支持的实验特性，可以直接将 gRPC 服务添加到 Istio 中，而不需要再向 Pod 中注入 Envoy 代理。本文中还给出了一个 Demo 性能测试数据，这种做法可以极大的提升应用性能，降低网络延迟。\n Istio 使用一组发现 API（统称为 xDS API 来动态配置其 Envoy sidecar 代理。这些 API 的目标是成为一个 通用的数据平面 API。gRPC 项目对 xDS API 有很好的支持，也就是说你可以管理 gRPC 工作负载，而不需要同时部署 Envoy sidecar。你可以在 Megan Yahya 的 KubeCon EU 2021 演讲中了解更多关于该集成的信息。关于 gRPC 支持的最新情况，可以在他们的提案中找到，还有实现状态。\nIstio 1.11 增加了实验性支持，可以直接将 gRPC 服务添加到网格中。我们支持基本的服务发现，一些基于 VirtualService 的流量策略，以及双向 TLS。\n支持的功能 与 Envoy 相比，目前 gRPC 内的 xDS API 的实现在某些方面是有限的。以下功能应该可以使用，尽管这不是一个详尽的列表，其他功能可能部分可用。\n 基本的服务发现。你的 gRPC 服务可以接触到在网格中注册的其他 pod 和虚拟机。 DestinationRule  Subset：你的 gRPC 服务可以根据标签选择器将流量分割到不同的实例组。 目前唯一支持的 Istio loadBalancer 是 ROUND_ROBIN，consistentHash 将在未来的 Istio 版本中加入（支持 gRPC）。 tls 设置被限制为 DISABLE 或 ISTIO_MUTUAL。其他模式将被视为 DISABLE。   VirtualService  Header 匹配和 URI 匹配的格式为 /ServiceName/RPCName。 覆盖目标主机和子集。 加权的流量转移。   PeerAuthentication  只支持 DISABLE 和 STRICT。其他模式将被视为 DISABLE。 在未来的版本中可能会有对 auto-mTLS 的支持。    其他功能包括故障、重试、超时、镜像和重写规则，可能会在未来的版本中支持。其中一些功能正等待在 gRPC 中实现，而其他功能则需要在 Istio 中支持。gRPC 中 xDS 功能的状态可以在这里找到。Istio 的支持状况将存在于未来的官方文档中。\n这个功能是实验性的。标准的 Istio 功能将随着时间的推移和整体设计的改进而得到支持。\n架构概述    gRPC服务如何与istiod通信的示意图  虽然不使用 proxy 进行数据面通信，但它仍然需要一个 agent 来进行初始化和与控制面的通信。首先，agent 在启动时生成一个引导文件，与为 Envoy 生成引导文件的方式相同。这告诉 gRPC 库如何连接到 istiod，在哪里可以找到数据面通信的证书，以及向控制面发送什么元数据。接下来，agent 作为一个 xDS proxy，代表应用程序与 istiod 进行连接和认证。最后，agent 获取并轮换数据平面通信中使用的证书。\n对应用程序代码的修改 本节介绍了 gRPC 在 Go 中的 xDS 支持。其他语言也有类似的 API。\n为了启用 gRPC 中的 xDS 功能，你的应用程序必须做一些必要的修改。你的 gRPC 版本应该至少是 1.39.0。\n客户端 下面的导入将在 gRPC 中注册 xDS 解析器和均衡器。它应该被添加到你的主包或调用 grpc.Dial 的同一个包中。\nimport _ \u0026#34;google.golang.org/grpc/xds\u0026#34; 当创建一个 gRPC 连接时，URL 必须使用 xds:/// scheme。\nconn, err := grpc.DialContext(ctx, \u0026#34;xds:///foo.ns.svc.cluster.local:7070\u0026#34;) 此外，为了支持（m）TLS，必须向 DialContext 传递一个特殊的 TransportCredentials 选项。FallbackCreds 允许我们在 istiod 不发送安全配置时成功。\nimport \u0026#34;google.golang.org/grpc/credentials/xds\u0026#34; ... creds, err := xds.NewClientCredentials(xds.ClientOptions{ FallbackCreds: insecure.NewCredentials() }) // handle err conn, err := grpc.DialContext( ctx, \u0026#34;xds:///foo.ns.svc.cluster.local:7070\u0026#34;, grpc.WithTransportCredentials(creds), ) 服务端 为了支持服务器端的配置，如 mTLS，必须做一些修改。\n首先，我们使用一个特殊的构造函数来创建 GRPCServer。\nimport \u0026#34;google.golang.org/grpc/xds\u0026#34; ... server = xds.NewGRPCServer() RegisterFooServer(server, \u0026amp;fooServerImpl) 如果你的 protoc 生成的 Go 代码已经过期，你可能需要重新生成，以便与 xDS 服务器兼容。你生成的 RegisterFooServer 函数应该像下面这样。\nfunc RegisterFooServer(s grpc.ServiceRegistrar, srv FooServer) { s.RegisterService(\u0026amp;FooServer_ServiceDesc, srv) } 最后，与客户端的变化一样，我们必须启用安全支持。\ncreds, err := xds.NewServerCredentials(xdscreds.ServerOptions{FallbackCreds: insecure.NewCredentials()}) // handle err server = xds.NewGRPCServer(grpc.Creds(creds)) 在你的 Kubernetes 部署中 假设你的应用代码是兼容的，Pod 只需要注释 inject.istio.io/templates：grpc-agent。这增加了一个运行上述代理的 sidecar 容器，以及一些环境变量，gRPC 使用这些变量来寻找引导文件并启用某些功能。\n对于 gRPC 服务端，你的 Pod 也应该用 proxy.istio.io/config: \u0026#39;{\u0026#34;holdApplicationUntilProxyStarts\u0026#34;: true}\u0026#39; 来注释，以确保在你的 gRPC 服务端初始化之前，代理中的 xDS 代理和引导文件已经准备就绪。\n例子 在本指南中，你将部署 echo，一个已经支持服务器端和客户端无代理的 gRPC 的应用。通过这个应用程序，你可以尝试一些支持的流量策略，启用 mTLS。\n先决条件 本指南要求在进行之前安装 Istio（1.11+）控制平面。\n部署应用程序 创建一个支持注入的命名空间 echo-grpc。接下来部署两个 echo 应用程序的实例以及服务。\n$ kubectl create namespace echo-grpc $ kubectl label namespace echo-grpc istio-injection=enabled $ kubectl -n echo-grpc apply -f samples/grpc-echo/grpc-echo.yaml 确保两个 Pod 正在运行。\n$ kubectl -n echo-grpc get pods NAME READY STATUS RESTARTS AGE echo-v1-69d6d96cb7-gpcpd 2/2 Running 0 58s echo-v2-5c6cbf6dc7-dfhcb 2/2 Running 0 58s 测试 gRPC 解析器 首先，将 17171 端口转发到其中一个 Pod 上。这个端口是一个非 xDS 支持的 gRPC 服务端，允许从端口转发的 Pod 发出请求。\n$ kubectl -n echo-grpc port-forward $(kubectl -n echo-grpc get pods -l version=v1 -ojsonpath=\u0026#39;{.items[0].metadata.name}\u0026#39;) 17171 \u0026amp; 接下来，我们可以发送一批 5 个请求。\n$ grpcurl -plaintext -d \u0026#39;{\u0026#34;url\u0026#34;: \u0026#34;xds:///echo.echo-grpc.svc.cluster.local:7070\u0026#34;, \u0026#34;count\u0026#34;: 5}\u0026#39; :17171 proto.EchoTestService/ForwardEcho | jq -r \u0026#39;.output | join(\u0026#34;\u0026#34;)\u0026#39; | grep Hostname Handling connection for 17171 [0 body] Hostname=echo-v1-7cf5b76586-bgn6t [1 body] Hostname=echo-v2-cf97bd94d-qf628 [2 body] Hostname=echo-v1-7cf5b76586-bgn6t [3 body] Hostname=echo-v2-cf97bd94d-qf628 [4 body] Hostname=echo-v1-7cf5b76586-bgn6t 你也可以使用类似 Kubernetes 名称解析的短名称。\n$ grpcurl -plaintext -d \u0026#39;{\u0026#34;url\u0026#34;: \u0026#34;xds:///echo:7070\u0026#34;}\u0026#39; :17171 proto.EchoTestService/ForwardEcho | jq -r \u0026#39;.output | join (\u0026#34;\u0026#34;)\u0026#39; | grep Hostname [0 body] Hostname=echo-v1-7cf5b76586-ltr8q $ grpcurl -plaintext -d \u0026#39;{\u0026#34;url\u0026#34;: \u0026#34;xds:///echo.echo-grpc:7070\u0026#34;}\u0026#39; :17171 proto.EchoTestService/ForwardEcho | jq -r \u0026#39;.output | join(\u0026#34;\u0026#34;)\u0026#39; | grep Hostname [0 body] Hostname=echo-v1-7cf5b76586-ltr8q $ grpcurl -plaintext -d \u0026#39;{\u0026#34;url\u0026#34;: \u0026#34;xds:///echo.echo-grpc.svc:7070\u0026#34;}\u0026#39; :17171 proto.EchoTestService/ForwardEcho | jq -r \u0026#39;.output | join(\u0026#34;\u0026#34;)\u0026#39; | grep Hostname [0 body] Hostname=echo-v2-cf97bd94d-jt5mf 用目的地规则创建子集 首先，为每个版本的工作负载创建一个子集。\n$ cat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: echo-versions namespace: echo-grpc spec: host: echo.echo-grpc.svc.cluster.local subsets: - name: v1 labels: version: v1 …","date":1637632980,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"033c33c2e82444714e4dfbd09760b1eb","permalink":"https://jimmysong.io/docs/translation/grpc-proxyless-service-mesh/","publishdate":"2021-11-23T10:03:00+08:00","relpermalink":"/docs/translation/grpc-proxyless-service-mesh/","section":"translation","summary":"译者注：本文译自 Istio 官方博客，博客原标题 gRPC Proxyless Service Mesh，其实是","tags":["Istio","grpc","service mesh"],"title":"基于 gRPC 和 Istio 的无 sidecar 代理的服务网格","type":"translation"},{"authors":["Liz Rice"],"categories":["Service Mesh"],"content":"今天有几个服务网格的产品和项目，承诺简化应用微服务之间的连接，同时提供额外的功能，如安全连接、可观察性和流量管理。但正如我们在过去几年中反复看到的那样，对服务网格的兴奋已经被对额外的复杂性和开销的实际担忧所抑制。让我们来探讨一下 eBPF 是如何让我们精简服务网格，使服务网格的数据平面更有效率，更容易部署。\nSidecar 问题 今天的 Kubernetes 服务网格解决方案要求你在每一个应用 pod 上添加一个代理 sidecar 容器，如 Envoy 或 Linkerd-proxy。这是正确的：即使在一个非常小的环境中，比如说有 20 个服务，每个服务运行五个 pod，分布在三个节点上，你也有 100 个代理容器。无论代理的实现多么小和有效，这种纯粹的重复都会耗费资源。\n每个代理使用的内存与它需要能够通信的服务数量有关。Pranay Singhal 写了他配置 Istio 的经验，将每个代理的消耗从 1GB 左右减少到更合理的 60-70MB。但是，即使在我们的小环境中，在三个节点上有 100 个代理，这种优化配置仍然需要每个节点 2GB 左右。\n   来自redhat.com/architect/why-when-service-mesh——每个微服务都有自己的代理sidecar  为什么我们需要所有这些 sidecar？这种模式允许代理容器与 pod 中的应用容器共享一个网络命名空间。网络命名空间是 Linux 内核的结构，它允许容器和 pod 拥有自己独立的网络堆栈，将容器化的应用程序相互隔离。这使得应用之间互不相干，这就是为什么你可以让尽可能多的 pod 在 80 端口上运行一个 web 应用 —— 网络命名空间意味着它们各自拥有自己的 80 端口。代理必须共享相同的网络命名空间，这样它就可以拦截和处理进出应用容器的流量。\n引入 eBPF eBPF 是一种内核技术，允许自定义程序在内核中运行。这些程序在响应事件时运行，有成千上万个可能的事件，eBPF 程序可以被附加到这些事件上。这些事件包括轨迹点、进入或退出任何功能（在内核或用户空间）或对服务网格来说很重要的 —— 抵达的网络数据包。\n重要的是，每个节点只有一个内核；在一个节点上运行的所有容器（也就是所有的 pod）共享同一个内核。如果你在内核中添加一个 eBPF 程序到一个事件中，它将被触发，无论哪个进程引起该事件，无论它是在应用容器中运行还是直接运行在主机上。\n   每台主机一个内核  这就是为什么 eBPF 对于 Kubernetes 中的任何一种 instrumentation 来说都是如此令人兴奋的技术 —— 你只需要在每个节点上添加一次 instrumentation ，所有的应用程序 pod 都会被覆盖。无论你是在寻求可观察性、安全性还是网络，由 eBPF 驱动的解决方案都可以在不需要 sidecar 的情况下对应用进行检测。\n基于 eBPF 的 Cilium 项目（最近 以孵化级别加入云计算基金会）将这种 “无 sidecar\u0026#34; 模式带到了服务网格的世界。除了传统的 sidecar 模型，Cilium 还支持每个节点使用一个 Envoy 代理实例运行服务网格的数据平面。使用我们前面的例子，这就把代理实例的数量从 100 个减少到只有 3 个。\n   用无sidecar代理模式减少代理实例  减少 YAML 在 sidecar 模型中，指定每个应用 pod 的 YAML 需要被修改以添加 sidecar 容器。这通常是自动化的 —— 例如，使用一个 mutating webhook，在每个应用 pod 部署的时候注入 sidecar。\n以 Istio 为例，这需要标记 Kubernetes 命名空间和 / 或 pod，以定义是否应该注入 sidecar—— 当然也需要为集群启用 mutating webhook。\n但如果出了问题怎么办？如果命名空间或 pod 的标签不正确，那么 sidecar 将不会被注入，pod 将不会被连接到服务网格。更糟糕的是，如果攻击者破坏了集群，并能够运行一个恶意的工作负载 —— 例如，一个加密货币矿工，他们将不太可能标记它，以便它加入服务网格。它不会通过服务网格提供的流量观察能力而被发现。\n相比之下，在支持 eBPF 的无 sidecar 代理模型中，pod 不需要任何额外的 YAML 就可以被检测。相反，一个 CRD 被用来在集群范围内配置服务网格。即使是已经存在的 pod 也可以成为服务网格的一部分，而不需要重新启动。\n如果攻击者试图通过直接在主机上运行工作负载来绕过 Kubernetes 编排，eBPF 程序可以检测并控制这一活动，因为这一切都可以从内核看到。\neBPF 支持的网络效率 支持 eBPF 的网络允许数据包走捷径，绕过内核的部分网络堆栈，这可以使 Kubernetes 网络的性能得到显著改善。让我们看看这在服务网格数据平面中是如何应用的。\n   在eBPF加速、无sidecar的服务网格模型中，网络数据包通过的路径要短得多  在服务网格的情况下，代理在传统网络中作为 sidecar 运行，数据包到达应用程序的路径相当曲折：入站数据包必须穿越主机 TCP/IP 栈，通过虚拟以太网连接到达 pod 的网络命名空间。从那里，数据包必须穿过 pod 的网络堆栈到达代理，代理将数据包通过回环接口转发到应用程序。考虑到流量必须在连接的两端流经代理，与非服务网格流量相比，这将导致延迟的显著增加。\n基于 eBPF 的 Kubernetes CNI 实现，如 Cilium，可以使用 eBPF 程序，明智地钩住内核中的特定点，沿着更直接的路线重定向数据包。这是可能的，因为 Cilium 知道所有的 Kubernetes 端点和服务的身份。当数据包到达主机时，Cilium 可以将其直接分配到它所要去的代理或 Pod 端点。\n网络中的加密 如果一个网络解决方案能够意识到 Kubernetes 服务，并在这些服务的端点之间提供网络连接，那么它能够提供服务网格数据平面的能力就不足为奇。但这些能力可以超越基本的连接。一个例子是透明加密。\n通常使用服务网格来确保所有的应用流量都是经过认证和加密的。这是通过双向 TLS（mTLS）实现的；服务网格代理组件作为网络连接的端点，并与其远程对等物协商一个安全的 TLS 连接。这种连接对代理之间的通信进行加密，而不需要对应用程序做任何改变。\n但在应用层管理的 TLS 并不是实现组件间认证和加密流量的唯一方法。另一个选择是在网络层加密流量，使用 IPSec 或 WireGuard。因为它在网络层操作，这种加密不仅对应用程序完全透明，而且对代理也是透明的 —— 它可以在有或没有服务网格时启用。如果你使用服务网格的唯一原因是提供加密，你可能想考虑网络级加密。它不仅更简单，而且还可以用来验证和加密节点上的任何流量 —— 它不只限于那些启用了 sidecar 的工作负载。\neBPF 是服务网格的数据平面 现在，eBPF 在 Linux 生产发行版使用的内核版本中得到广泛支持，企业可以利用它来获得更有效的网络解决方案，并作为服务网格的更有效的数据平面。\n去年，我代表 CNCF 的技术监督委员会，对服务网格领域的整合和清晰化做了一些 预测。在同一主题演讲中，我谈到 eBPF 有可能成为更多项目和更广泛部署能力的基础。这两个想法现在正结合在一起，因为 eBPF 似乎是服务网格数据平面的自然路径。\n","date":1635310800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"6def5edfce48b22b3f0fe6a02b6c1ec3","permalink":"https://jimmysong.io/docs/translation/how-ebpf-streamlines-the-service-mesh/","publishdate":"2021-10-27T13:00:00+08:00","relpermalink":"/docs/translation/how-ebpf-streamlines-the-service-mesh/","section":"translation","summary":"本文探讨一下eBPF是如何让我们精简服务网格，使服务网格的数据平面更有效率，更容易部署。","tags":["Service Mesh","ebpf"],"title":"eBPF 如何简化服务网格","type":"translation"},{"authors":["宣超","胡凯","谭崇康","唐博","林文炜","张凯豪"],"categories":null,"content":"讲师分享  云原生社区 meetup 第八期上海站开场，郭旭东 云原生 2.0 华为云赋能 “新云原生企业”，张凯豪 蚂蚁万级规模 K8s 集群 etcd 架构优化实践 —ETCD on OceanBase，宣超 新一代开源 HCI 底层原理剖析，胡凯 攀登规模化的高峰 —— 蚂蚁集团大规模 Sigma 集群 ApiServer 优化实践，唐博，谭崇康 云原生分布式存储 Rook 及其在企业中应用的未来，林文炜  ","date":1634965200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"93a357e6fddc8d2c7fa611b7e08c92b2","permalink":"https://jimmysong.io/docs/event/cloud-native-meetup-shanghai-08/","publishdate":"2021-10-23T20:00:00+08:00","relpermalink":"/docs/event/cloud-native-meetup-shanghai-08/","section":"event","summary":"本次活动聚焦于 OceanBase、Rook、Sigma 等。","tags":["OceanBase","Rook"],"title":"云原生社区 meetup 第八期上海站","type":"event"},{"authors":["Srini Penchikala"],"categories":["service mesh"],"content":"主要收获  了解采用服务网格技术的新兴架构趋势，特别是多云、多集群和多租户模式，如何在异构基础设施（裸机、虚拟机和 Kubernetes）中部署服务网格解决方案，以及从边缘计算层到网格的应用 / 服务连接。 了解服务网格生态系统中的一些新模式，如多集群服务网格、媒体服务网格（Media Service Mesh）和混沌网格，以及经典的微服务反模式，如 “死星（Death Star） “架构。 获取最新的关于在部署领域使用服务网格的创新总结，在 Pod（K8s 集群）和 VM（非 K8s 集群）之间进行快速实验、混乱工程和金丝雀部署。 探索服务网格扩展领域的创新，包括：增强身份管理，以确保微服务连接的安全性，包括自定义证书授权插件，自适应路由功能，以提高服务的可用性和可扩展性，以及增强 sidecar 代理。 了解操作方面即将出现的情况，如配置多集群功能和将 Kubernetes 工作负载连接到托管在虚拟机基础设施上的服务器，以及管理多集群服务网格中所有功能和 API 的开发者门户。  在过去的几年里，服务网格技术有了长足的发展。服务网格在各组织采用云原生技术方面发挥着重要作用。通过提供四种主要能力 —— 连接性、可靠性、可观察性和安全性，服务网格已经成为 IT 组织的技术和基础设施现代化工作的核心组成部分。服务网格使开发和运维团队能够在基础设施层面实现这些能力，因此，当涉及到跨领域的非功能需求时，应用团队不需要重新发明轮子。\n自本文第一版于 2020 年 2 月发表以来，服务网格技术经历了重大创新，在不断发展的服务网格领域出现了一些新的架构趋势、技术能力和服务网格项目。\n在过去的一年里，服务网格产品的发展远远超过了原有的 Kubernetes 解决方案，没有托管在 Kubernetes 平台上的应用无法利用服务网格。并非所有的组织都将其所有的业务和 IT 应用程序过渡到 Kubernetes 云平台。因此，自服务网格诞生以来，一直需要这项技术在不同的 IT 基础设施环境中工作。\n随着微服务架构的不断采用，应用系统在云供应商、基础设施（Kubernetes、虚拟机、裸机服务器）、地域，甚至在服务网格集成环境中要管理的工作负载类型方面，都已实现解耦和分布式。\n让我们从服务网格的历史开始说起，了解服务网格是如何产生的。\n2016 年前后，“服务网格 \u0026#34; 这个词出现在微服务、云计算和 DevOps 的领域。Buoyant 团队在 2016 年用这个词来解释他们的产品 Linkerd。和云计算领域的许多概念一样，相关的模式和技术其实有很长的历史。\n服务网格的到来主要是由于 IT 领域内的一场风暴。开发人员开始使用多语言（polyglot）方法构建分布式系统，并需要动态服务发现。运维部门开始使用短暂的基础设施，并希望优雅地处理不可避免的通信故障和执行网络策略。平台团队开始接受像 Kubernetes 这样的容器编排系统，并希望使用现代 API 驱动的网络代理（如 Envoy）在系统中和周围动态地路由流量。\n本文旨在回答软件架构师和技术负责人的相关问题，如：什么是服务网格？我是否需要服务网格？如何评估不同的服务网格产品？\n服务网格模式 服务网格模式专注于管理分布式软件系统中所有服务之间的通信。\n背景介绍 该模式的背景有两个方面。首先，工程师们已经采用了微服务架构模式，并通过将多个（理想情况下是单一用途且可独立部署的）服务组合在一起构建他们的应用。第二，组织已经接受了云原生平台技术，如容器（如 Docker）、编排器（如 Kubernetes）和网关。\n意图 服务网格模式试图解决的问题包括：\n 消除了将特定语言的通信库编译到单个服务中的需求，以处理服务发现、路由和应用层（第 7 层）非功能通信要求。 外部化服务通信配置，包括外部服务的网络位置、安全凭证和服务质量目标。 提供对其他服务的被动和主动监测。 在整个分布式系统中分布式地执行策略。 提供可观察性的默认值，并使相关数据的收集标准化。  启用请求记录 配置分布式追踪 收集指标    结构 服务网格模式主要侧重于处理传统上被称为 “东西向 “的基于远程过程调用（RPC）的流量：请求 / 响应类型的通信，源自数据中心内部，在服务之间传播。这与 API 网关或边缘代理相反，后者被设计为处理 “南北 “流量。来自外部的通信，进入数据中心内的一个终端或服务。\n服务网格的特点 服务网格的实施通常会提供以下一个或多个功能：\n 规范化命名并增加逻辑路由，（例如，将代码级名称 “用户服务 \u0026#34; 映射到平台特定位置 “AWS-us-east-1a/prod/users/v4”。 提供流量整形和流量转移 保持负载均衡，通常采用可配置的算法 提供服务发布控制（例如，金丝雀释放和流量分割） 提供按请求的路由（例如，影子流量、故障注入和调试重新路由）。 增加基线可靠性，如健康检查、超时 / 截止日期、断路和重试（预算）。 通过透明的双向传输级安全（TLS）和访问控制列表（ACL）等策略，提高安全性 提供额外的可观察性和监测，如顶线指标（请求量、成功率和延迟），支持分布式追踪，以及 “挖掘” 和检查实时服务间通信的能力。 使得平台团队能够配置 \u0026#34; 理智的默认值”，以保护系统免受不良通信的影响。  服务网格的能力可分为以下四个方面：\n 连接性 可靠性 安全性 可观察性  让我们看看服务网格技术在这些领域都能提供哪些功能。\n连接性\n 流量控制（路由，分流） 网关（入口、出口） 服务发现 A/B 测试、金丝雀 服务超时、重试  可靠性\n 断路器 故障注入 / 混沌测试  安全性\n 服务间认证（mTLS） 证书管理 用户认证（JWT） 用户授权（RBAC） 加密  可观察性\n 监测 遥测、仪表、计量 分布式追踪 服务图表  服务网格架构：内部原理 服务网格由两部分组成：数据平面和控制平面。Matt Klein，Envoy Proxy 的作者，写了一篇关于 “ 服务网格数据平面与控制平面 “的深入探讨。\n广义上讲，数据平面 “执行工作”，负责 “有条件地翻译、转发和观察流向和来自 [网络终端] 的每个网络数据包”。在现代系统中，数据平面通常以代理的形式实现，（如 Envoy、HAProxy 或 MOSN），它作为 “sidecar” 与每个服务一起在进程外运行。Linkerd 使用了一种 微型代理方法，该方法针对服务网格的使用情况进行了优化。\n控制平面 “监督工作”，并将数据平面的所有单个实例 —— 一组孤立的无状态 sidecar 代理变成一个分布式系统。控制平面不接触系统中的任何数据包 / 请求，相反，它允许人类运维人员为网格中所有正在运行的数据平面提供策略和配置。控制平面还能够收集和集中数据平面的遥测数据，供运维人员使用。\n控制平面和数据平面的结合提供了两方面的优势，即策略可以集中定义和管理，同时，同样的政策可以以分散的方式，在 Kubernetes 集群的每个 pod 中本地执行。这些策略可以与安全、路由、断路器或监控有关。\n下图取自 Istio 架构文档，虽然标注的技术是 Istio 特有的，但这些组件对所有服务网格的实现都是通用的。\n   Istio 架构  Istio 架构，展示了控制平面和代理数据平面的交互方式（由 Istio 文档提供）。\n使用案例 服务网格可以实现或支持多种用例。\n动态服务发现和路由 服务网格提供动态服务发现和流量管理，包括用于测试的流量影子（复制），以及用于金丝雀发布和 A/B 实验的流量分割。\n服务网格中使用的代理通常是 “应用层 \u0026#34; 感知的（在 OSI 网络堆栈的第 7 层运行）。这意味着流量路由决策和指标的标记可以利用 HTTP 头或其他应用层协议元数据。\n服务间通信可靠性 服务网格支持跨领域的可靠性要求的实施和执行，如请求重试、超时、速率限制和断路。服务网格经常被用来补偿（或封装）处理分布式计算的八个谬误。应该注意的是，服务网格只能提供 wire-level 的可靠性支持（如重试 HTTP 请求），最终服务应该对相关的业务影响负责，如避免多个（非幂等的）HTTP POST 请求。\n流量的可观察性 由于服务网格处于系统内处理的每个请求的关键路径上，它还可以提供额外的 “可观察性”，例如请求的分布式追踪、HTTP 错误代码的频率以及全局和服务间的延迟。虽然在企业领域是一个被过度使用的短语，但服务网格经常被提议作为一种方法来捕获所有必要的数据，以实现整个系统内流量的统一界面视图。\n通信安全 服务网格还支持跨领域安全要求的实施和执行，如提供服务身份（通过 x509 证书），实现应用级服务 / 网络分割（例如，“服务 A\u0026#34; 可以与 “服务 B “通信，但不能与 “服务 C “通信），确保所有通信都经过加密（通过 TLS），并确保存在有效的用户级身份令牌或 “护照 “。\n反模式 当反模式的使用出现时，这往往是一个技术成熟的标志。服务网格也不例外。\n太多的流量管理层次 当开发人员不与平台或运维团队协商，并在现在通过服务网格实现的代码中重复现有的通信处理逻辑时，就会出现这种反模式。例如，除了服务网格提供的 wire-level 重试策略外，应用程序还在代码中还实现了重试策略。这种反模式会导致重复的事务等问题。\n服务网格银弹 在 IT 领域没有 “银弹 “这样的东西，但供应商有时会被诱惑给新技术贴上这个标签。服务网格不会解决微服务、Kubernetes 等容器编排器或云网络的所有通信问题。服务网格的目的只是促进服务件（东西向）的通信，而且部署和运行服务网格有明显的运营成本。\n企业服务总线（ESB）2.0 在前微服务面向服务架构（SOA）时代，企业服务总线（ESB）实现了软件组件之间的通信系统。有些人担心 ESB 时代的许多错误会随着服务网格的使用而重演。\n通过 ESB 提供的集中的通信控制显然有价值。然而，这些技术的发展是由供应商推动的，这导致了多种问题，例如：ESB 之间缺乏互操作性，行业标准的定制扩展（例如，将供应商的特定配置添加到 WS-* 兼容模式中），以及高成本。ESB 供应商也没有做任何事情来阻止业务逻辑与通信总线的集成和紧耦合。\n大爆炸部署 在整个 IT 界有一种诱惑，认为大爆炸式的部署方法是最容易管理的方法，但正如 Accelerate 和 DevOps 报告的研究，事实并非如此。由于服务网格的全面推广意味着这项技术处于处理所有终端用户请求的关键路径上，大爆炸式的部署是非常危险的。\n死星建筑 当企业采用微服务架构，开发团队开始创建新的微服务或在应用中利用现有的服务时，服务间的通信成为架构的一个关键部分。如果没有一个良好的治理模式，这可能会导致不同服务之间的紧密耦合。当整个系统在生产中出现问题时，也将很难确定哪个服务出现了问题。\n如果缺乏服务沟通战略和治理模式，该架构就会变成所谓的 “死星架构”。\n关于这种架构反模式的更多信息，请查看关于云原生架构采用的第一部分、第二部分和第三部分的文章。\n特定领域的服务网格 服务网格的本地实现和过度优化有时会导致服务网格部署范围过窄。开发人员可能更喜欢针对自己的业务领域的服务网格，但这种方法弊大于利。我们不希望实现过于细化的服务网格范围，比如为组织中的每个业务或功能域（如财务、人力资源、会计等）提供专用的服务网格。这就违背了拥有像服务网格这样的通用服务协调解决方案的目的，即企业级服务发现或跨域服务路由等功能。\n服务网格的实现和产品 以下是一份非详尽的当前服务网格实施清单。\n Linkerd (CNCF 毕业项目) Istio Consul Kuma（CNCF 沙盒项目） AWS App Mesh NGINX Service Mesh AspenMesh Kong Solo Gloo Mesh Tetrate Service Bridge Traefik Mesh（原名 Maesh） Meshery Open Service MEsh（CNCF 沙盒项目）  另外，像 DataDog …","date":1633917600,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"feac45c6dd58fb80b684c98415e8b557","permalink":"https://jimmysong.io/docs/translation/service-mesh-ultimate-guide-e2/","publishdate":"2021-10-11T10:00:00+08:00","relpermalink":"/docs/translation/service-mesh-ultimate-guide-e2/","section":"translation","summary":"本文是 InfoQ 自 2020 年 2 月发表的服务网格终极指南后的第二版，发布于 2021 年 9 月。","tags":["service mesh"],"title":"服务网格终极指南第二版——下一代微服务开发","type":"translation"},{"authors":["董志勇","汪晟杰","黄金浩","田甜","邓洪超"],"categories":null,"content":"讲师分享  云原生社区 meetup 第七期深圳站开场致辞 - 宋净超 使用 IAST 构建高效的 DevSecOps 流程 - 董志勇 云原生场景下的开发和调试-汪晟杰，黄金浩 Envoy 在腾讯游戏云原生平台应用 - 田甜 使用 KubeVela 构建混合云应用管理平台 - 邓洪超  ","date":1632546000,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"d1047a20179fc46964691ba7b24a0887","permalink":"https://jimmysong.io/docs/event/cloud-native-meetup-shenzhen-07/","publishdate":"2021-09-25T18:00:00+08:00","relpermalink":"/docs/event/cloud-native-meetup-shenzhen-07/","section":"event","summary":"本次活动关注于 DevSecOps、Envoy、KubeVela、Nocalhost 等。","tags":["DevSecOps","Envoy","KubeVela","Nocalhost"],"title":"云原生社区 meetup 第七期深圳站","type":"event"},{"authors":["林杨","白西原","张卫滨","何昌钦"],"categories":null,"content":"话题  欢迎来到云原生社区大连站 Connecting, Controlling and Observing Dubbo Microservices with Flomesh，林杨 基于云原生技术的服务最大化可用性，白西原（乐天创研） Jutopia 一站式云原生机器学习平台，何昌钦 分布式系统的发展与趋势分析，张卫滨  ","date":1629608400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"a801c020c0b65a1030b6aa97ee9a11b7","permalink":"https://jimmysong.io/docs/event/cloud-native-meetup-dalian-06/","publishdate":"2021-08-22T18:00:00+08:00","relpermalink":"/docs/event/cloud-native-meetup-dalian-06/","section":"event","summary":"本次活动关注服务网格、机器学习。","tags":["Flomesh","Jutopia"],"title":"云原生社区 meetup 第六期大连站","type":"event"},{"authors":null,"categories":null,"content":"近日美国国家安全局（NSA）和网络安全与基础设施安全署（CISA）发布了一份网络安全技术报告 Kubernetes Hardening Guidance（查看英文原版 PDF）。\n笔者将本资料翻译为《Kubernetes 加固指南》（或译作《Kubernetes 强化指南》）中文版。\n","date":1628380800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"ff33cba030ab96ff0c0640954bf14026","permalink":"https://jimmysong.io/docs/book/kubernetes-hardening-guidance/","publishdate":"2021-08-08T00:00:00Z","relpermalink":"/docs/book/kubernetes-hardening-guidance/","section":"book","summary":"NSA 和 CISA 联合出品","tags":["ebook","translation"],"title":"Kubernetes加固指南","type":"publication"},{"authors":null,"categories":["Istio"],"content":"本文译自 Istio 官方博客 Security Best Practices。\nIstio 的安全功能提供了强大的身份、策略、透明的 TLS 加密以及认证、授权和审计（AAA）工具来保护你的服务和数据。然而，为了充分安全地利用这些功能，必须注意遵循最佳实践。建议在继续阅读之前，先回顾一下安全概述。\n双向 TLS Istio 将尽可能使用双向 TLS 对流量进行自动加密。然而，代理在默认情况下被配置为许可模式（Permissive Mode），这意味着他们将接受双向 TLS 和明文流量。\n虽然这是为了增量采用或允许来自没有 Istio sidecar 的客户端的流量的需要，但它也削弱了安全立场。建议在可能的情况下迁移到严格模式（Strict Mode），以强制使用双向 TLS。\n然而，仅靠双向 TLS 并不足以保证流量的安全，因为它只提供认证，而不是授权。这意味着，任何拥有有效证书的人仍然可以访问一个服务。\n为了完全锁定流量，建议配置授权策略。这允许创建细粒度的策略来允许或拒绝流量。例如，你可以只允许来自 app 命名空间的请求访问 hello-world 服务。\n授权策略 Istio 授权在 Istio 安全中起着关键作用。它需要努力配置正确的授权策略，以最好地保护你的集群。了解这些配置的影响是很重要的，因为 Istio 无法确定所有用户的正确授权。请全程关注本节内容。\n应用默认拒绝的授权策略 我们建议你按照 default-deny 模式定义你的 Istio 授权策略，以增强集群的安全态势。默认拒绝授权模式意味着你的系统默认拒绝所有请求，而你定义了允许请求的条件。如果你错过了一些条件，流量将被意外地拒绝，而不是流量被意外地允许。后者通常是一个安全事件，而前者可能会导致糟糕的用户体验、服务中断或不符合你的 SLO/SLA。\n例如，在 HTTP 流量的授权任务中，名为 allow-nothing 的授权策略确保所有流量在默认情况下被拒绝。从这里开始，其他授权策略根据特定条件允许流量。\n在路径规范化上定制你的系统 Istio 授权策略可以基于 HTTP 请求中的 URL 路径。路径规范化（又称 URI 规范化）对传入请求的路径进行修改和标准化，从而使规范化后的路径能够以标准方式进行处理。语法上不同的路径在路径规范化后可能是等同的。\nIstio 支持以下请求路径的规范化方案，然后再根据授权策略进行评估和路由请求：\n   选项 描述 示例     NONE 不做任何规范化处理。Envoy 收到的任何信息都会被原封不动地转发给任何后端服务。 ../%2Fa../b 由授权政策评估并发送给你的服务。   BASE 这是目前 Istio 默认安装中使用的选项。这在 Envoy 代理上应用了 normalize_path选项，该选项遵循 RFC 3986，有额外的规范化处理，将反斜线转换成正斜线。 /a/../b 被规范化为 /b。\\da 被规范化微 /da。   MERGE_SLASHES 斜线在 BASE 规范化之后被合并。 /a//b 被规范化为 /a/b。   DECODE_AND_MERGE_SLASHES 最严格的设置，当你默认允许所有流量。这个设置是推荐的，但要注意的是，你需要彻底测试你的授权策略路径。百分比编码的斜线和反斜线字符（%2F、%2f、%5C 和 %5c）在 MERGE_SLASHES 被规范化之前被解码为 / 或 \\。 /a%2fb 被规范化为 /a/b。    该配置是通过 mesh 配置中的 pathNormalization字段指定的。\n为了强调这一点，规范化算法是按照以下顺序进行的：\n 百分比解码 %2F、%2f、%5C 和 %5c。 RFC 3986 和其他由 Envoy 的 normalize_path选项实现的规范化。 合并斜线   虽然这些规范化选项代表了来自 HTTP 标准和常见行业惯例的建议，但应用程序可以以它选择的任何方式解释一个 URL。当使用拒绝策略时，请确保你了解你的应用程序的行为方式。\n 配置的例子 确保 Envoy 规范化请求路径以符合你的后端服务的期望，对你的系统安全至关重要。下面的例子可以作为你配置系统的参考。规范化的 URL 路径，如果选择了 NONE，则是原始的 URL 路径将：\n 用来对照授权策略进行检查 转发到后端应用程序     你的应用程序 选择     依靠代理进行规范化处理 BASE, MERGE_SLASHES or DECODE_AND_MERGE_SLASHES   根据 RFC 3986 规范化请求路径，不合并斜线 BASE   根据 RFC 3986 规范化请求路径，合并斜线，但不对百分比编码的斜线进行解码 MERGE_SLASHES   根据 RFC 3986 规范化请求路径，合并斜线，并对百分比编码的斜线进行解码 DECODE_AND_MERGE_SLASHES   处理请求路径的方式与 RFC 3986 不兼容 NONE    如何配置 你可以使用 istioctl 来更新 mesh 配置：\n$ istioctl upgrade --set meshConfig.pathNormalization.normalization=DECODE_AND_MERGE_SLASHES 或通过改变你的 Operator 重写文件\n$ cat \u0026lt;\u0026lt;EOF \u0026gt; iop.yamlapiVersion:install.istio.io/v1alpha1kind:IstioOperatorspec:meshConfig:pathNormalization:normalization:DECODE_AND_MERGE_SLASHESEOF$ istioctl install -f iop.yaml另外，如果你想直接编辑 Mesh 配置，你可以将 pathNormalization添加到 mesh 配置中，该配置是 istio-\u0026lt;REVISION_ID\u0026gt; 的 CongfigMap，在 istio-system 命名空间。例如，如果你选择 DECODE_AND_MERGE_SLASHES 选项，你修改 mesh 配置如下：\napiVersion:v1data:mesh:|-... pathNormalization: normalization: DECODE_AND_MERGE_SLASHES ... 不太常见的规范化配置 大小写规范化 在某些环境中，以不区分大小写的方式比较授权策略中的路径可能是有用的。例如，将 https://myurl/get 和 https://myurl/GeT 等同对待。在这些情况下，可以使用下面的 EnvoyFilter。这个过滤器将改变用于比较的路径和呈现给应用程序的路径。\napiVersion:networking.istio.io/v1alpha3kind:EnvoyFiltermetadata:name:ingress-case-insensitivenamespace:istio-systemspec:configPatches:- applyTo:HTTP_FILTERmatch:context:GATEWAYlistener:filterChain:filter:name:\u0026#34;envoy.filters.network.http_connection_manager\u0026#34;subFilter:name:\u0026#34;envoy.filters.http.router\u0026#34;patch:operation:INSERT_BEFOREvalue:name:envoy.luatyped_config:\u0026#34;@type\u0026#34;: \u0026#34;type.googleapis.com/envoy.extensions.filters.http.lua.v3.Lua\u0026#34;inlineCode:|function envoy_on_request(request_handle) local path = request_handle:headers():get(\u0026#34;:path\u0026#34;) request_handle:headers():replace(\u0026#34;:path\u0026#34;, string.lower(path)) end 了解流量采集的限制 Istio sidecar 的工作原理是捕获入站流量和出站流量，并通过 sidecar 代理引导它们。\n然而，并不是所有的流量都被捕获：\n 重定向只处理基于 TCP 的流量。任何 UDP 或 ICMP 数据包都不会被捕获或修改。 Sidecar 使用的许多端口以及 22 号端口的入站捕获被禁用。这个列表可以通过 traffic.sidecar.istio.io/excludeInboundPorts 等选项来扩展。 出站捕获同样可以通过 traffic.sidecar.istio.io/excludeOutboundPorts 等设置或其他方式减少。  一般来说，应用程序和其 sidecar 代理之间的安全边界最小。对 sidecar 的配置是以每个模块为基础的，并且两者都在同一个网络 / 进程命名空间中运行。因此，应用程序可能有能力删除重定向规则，并删除、改变、终止或替换 sidecar 代理。这允许一个 pod 故意绕过它的 sidecar 的出站流量或故意让入站流量绕过它的 sidecar。\n因此，依靠 Istio 无条件地捕获所有流量是不安全的。相反，安全边界是客户端不能绕过另一个 pod 的 sidecar。\n例如，如果我在 9080 端口运行 review 应用程序，我可以假设来自 productpage 应用程序的所有流量将被 sidecar 代理捕获，其中 Istio 认证和授权策略可能适用。\n利用 NetworkPolicy 进行深度防御 为了进一步确保流量安全，Istio 策略可以与 Kubernetes 网络策略分层。这实现了一个强大的深度防御策略，可以用来进一步加强你的网格的安全性。\n例如，你可以选择只允许流量到我们 review 应用程序的 9080 端口。如果集群中的 Pod 被破坏或存在安全漏洞，这可能会限制或阻止攻击者的进展。\n确保出口流量的安全 一个常见的误解是，像 outboundTrafficPolicy: REGISTRY_ONLY 作为一个安全策略，防止所有对未申报服务的访问。然而，如上所述，这并不是一个强大的安全边界，应该被认为是尽力而为。\n虽然这对防止意外的依赖性很有用，但如果你想保证出口流量的安全，并强制要求所有出站流量通过代理，你应该依靠 Egress Gateway。当与网络策略相结合时，你可以强制所有的流量，或一些子集，通过出口网关。这确保了即使客户意外地或恶意地绕过他们的 sidecar，该请求也会被阻止。\n当使用 TLS 发起时，在目的地规则中配置 TLS 验证 Istio 提供了从一个 sidecar 代理或网关发起 TLS 的能力。这使得发送纯文本 HTTP 流量的应用程序能够透明地 “升级 “到 HTTPS。\n在配置 DestinationRule 的 tls 设置时，必须注意指定 caCertificates 字段。如果没有设置，服务器的证书将不会被验证。\n例如：\napiVersion:networking.istio.io/v1beta1kind:DestinationRulemetadata:name:google-tlsspec:host:google.comtrafficPolicy:tls:mode:SIMPLEcaCertificates:/etc/ssl/certs/ca-certificates.crt网关 在运行 Istio 网关时，涉及一些资源：\n Gateways，它控制网关的端口和 TLS 设置。 VirtualServices，控制路由逻辑。这些都是通过在网关字段中的直接引用和在网关和 VirtualService 的 hosts …","date":1627880742,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"5b387140459a3ad5a69afef9bd7e8c7a","permalink":"https://jimmysong.io/docs/translation/istio-security-best-practices/","publishdate":"2021-08-02T13:05:42+08:00","relpermalink":"/docs/translation/istio-security-best-practices/","section":"translation","summary":"本文列举了 Istio 安全的最佳实践。","tags":["Istio","安全"],"title":"Istio 安全最佳实践","type":"translation"},{"authors":["粟伟","梁举","王天宜","石建伟"],"categories":null,"content":"话题  开场演讲：欢迎来到云原生社区成都站，宋净超，云原生社区创始人、Tetrate 布道师 Amazon EKS Distro 开源项目解析 \u0026amp; 演示，粟伟，亚马逊云科技资深解决方案架构师 面向量化投资的 AI 平台 ——AI 赋能投资：打造以大数据 + AI 为核心的下一代投资平台，梁举，宽邦科技 CEO 基于 TiDB 的云原生数据库实践，王天宜，TiDB 社区部门架构师 Layotto: 开启服务网格 + 应用运行时新篇章，石建伟（卓与），蚂蚁集团高级技术专家  ","date":1625288400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"7984aaeb6bd7f2e4bd98deb176fe136e","permalink":"https://jimmysong.io/docs/event/cloud-native-meetup-chengdu-05/","publishdate":"2021-07-03T18:00:00+08:00","relpermalink":"/docs/event/cloud-native-meetup-chengdu-05/","section":"event","summary":"本次活动关注 EKS Distro、TiDB、AI 和服务网格。","tags":["Layotto","TiDB"],"title":"云原生社区 meetup 第五期成都站","type":"event"},{"authors":["王院生","黄国锋","吴凌峰","敖小剑"],"categories":null,"content":"开场致辞 讲师：宋净超（Tetrate 布道师、云原生社区创始人）\n讲师介绍：Tetrate 云原生布道师，云原生社区创始人，CNCF Ambassador。\n有了 Nginx 和 Kong，为什么还需要 Apache APISIX？ 讲师：王院生\n个人介绍：支流科技联合创始人 CTO\n演讲概要\n在云原生时代，k8s 和微服务已经成为主流，在带来巨大生产力提升的同时，也增加了系统的复杂度。如何发布、管理和可视化服务，成为了一个重要的问题。每次修改配置都要 reload 的 Nginx、依赖 postgres 才能工作的 Kong，都不是云原生时代的理想之选。这正是我们创造 Apache APISIX 的原因：没有 reload、毫秒内全集群生效、不依赖数据库、极致性能、支持 Java 和 Go 开发插件。\n听众收益\n更好的理解 API 网关、服务网格，以及各个开源项目的优劣势\n云原生时代的研发效能 讲师：黄国峰\n个人介绍：腾讯 PCG 工程效能专家。10 多年的软件和互联网从业经验；现任腾讯工程效能部，负责持续集成、研发流程和构建系统等平台；曾任职唯品会高级经理，负责架构团队。在云原生平台下的研发效能方向有丰富的理论知识和实践经验。\n演讲概要\n云原生时代，软件研发的逻辑彻底改变了。传统的软件开发在本机编码 / 调试、部署到测试环境测试、再发布到生产环境；而云原生时代的开发，基于不可变设施，研发流程从编码、构建、持续测试、持续集成到持续部署，整个过程几乎完全代码化。\n听众收益\n 了解云原生开发的新挑战和难点 了解腾讯云原生开发实践的流程和思路 了解腾讯云原生开发中的遇到的坑和解决思路  37 手游 Go 微服务架构演进和云原生实践 讲师：吴凌峰\n个人介绍：任职于三七互娱集团 37 手游技术部基础架构组，负责平台 golang 基础框架以及 DevOps、CI/CD 生态建设，从业以来一直专注于云原生、DevOps 和容器化等技术应用和推广，在 golang 工程化领域有一定的心得。\n演讲概要\nGolang 微服务应用和云原生的概念近年越来越火热，传统技术栈公司随着业务规模增长，在云原生技术应用落地探索和转型的过程中一定会遇到很多共通的问题以及有各自不同的思考，包括如何更好地提升我们的开发效率、提升服务稳定性、降低运维成本？面对不断增长的服务数量和不断变长变复杂的调用关系网，怎样才能更好地观测、管理和保证核心服务高可用，本次演讲分享将会围绕 37 手游转型为 Go 微服务架构以及建设云原生 DevOps 体系的历程、过程中的领悟和思考展开。\n听众收益\n 了解 Golang 云原生微服务框架的关键技术和优化实践经验 了解云原生观测体系如链路追踪、监控等 Golang 微服务落地实践经验 了解混合云混合部署 DevOps 和 CI/CD 体系的企业实践经验  死生之地不可不察：论 API 标准化对 Dapr 的重要性 讲师：敖小剑\n个人介绍：资深码农，十九年软件开发经验，微服务专家，Service Mesh 布道师，Servicemesher 社区联合创始人，Dapr Maintainer。专注于基础架构，Cloud Native 拥护者，敏捷实践者，坚守开发一线打磨匠艺的架构师。曾在亚信、爱立信、唯品会、蚂蚁金服等任职，对基础架构和微服务有过深入研究和实践。目前就职阿里云，在云原生应用平台全职从事 Dapr 开发。\n演讲概要\nDapr 作为新兴的云原生项目，以 “应用运行时” 之名致力于围绕云原生应用的各种分布式需求打造一个通用而可移植的抽象能力层。这个愿景有着令人兴奋而向往的美好前景：一个受到普通认可和遵循的云原生业界标准，基于此开发的云原生应用可以在不同的厂家的云上自由的部署和迁移，恍惚间一派云原生下世界大同的美景。然而事情往往没这么简单，API 的标准化之路异常的艰辛而痛苦，Dapr 的分布式能力抽象在实践中会遇到各种挑战和困扰。\n听众收益\n 了解 Dapr 的愿景和分布式能力抽象层的重要 了解 Dapr API 在抽象和实现时遇到的实际问题，尤其是取舍之间的艰难 了解目前 Dapr 在 API 抽象上正在进行的努力和新近准备增加的 API  ","date":1621659600,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"c58a8a96d593ceb233458c9ee06a93b0","permalink":"https://jimmysong.io/docs/event/cloud-native-meetup-guangzhou-04/","publishdate":"2021-05-22T20:00:00+08:00","relpermalink":"/docs/event/cloud-native-meetup-guangzhou-04/","section":"event","summary":"本次活动关注于 Dapr 和 APISIX 等。","tags":["Dapr","APISIX"],"title":"云原生社区 meetup 第四期广州站","type":"event"},{"authors":["周强","王佰平","孙健波","张义飞"],"categories":null,"content":"开场致辞 讲师：宋净超（Tetrate 布道师、云原生社区创始人）\n讲师介绍：Tetrate 云原生布道师，云原生社区创始人，CNCF Ambassador。\n使用 Chaos Mesh 来保障云原生系统的健壮性 讲师：周强\n公司：PingCAP\n讲师介绍：周强，PingCAP 工程效率负责人，Chaos Mesh 负责人，专注稳定性和性能测试平台。在混沌工程领域有 4 年的从业经验，领导开发云原生混沌测试平台 Chaos Mesh。\n演讲概要:\n在云原生的世界中，错误无处不在，混沌工程在提高系统稳定性方面起着至关重要的作用。通过执行混沌工程实验，我们可以了解系统的弱点并主动解决。我们开发了云原生混沌工程平台 Chaos Mesh，并在内部使用 Chaos Mesh 来提升云原生分布式数据库 TiDB 的健壮性。目前 Chaos Mesh 已加入 CNCF Sandbox 项目，该平台依托于 k8s 基础设施，通过对 pod/container 进行诸如杀节点、IO 错误和延时注入、时间回退、内核分配内存失败等等来进行混沌测试。主题大纲:\n 在分布式领域会遇到的质量和稳定性问题 混沌工程在提升系统稳定性方面的作用和意义 Chaos Mesh 项目简介 混沌工程主要的测试方式和使用案例 混沌工程平台的构建实践  听众收益：\n 了解在构建分布式系统可能出现的问题和风险 了解混沌工程的使用经验和踩过的坑，观众后续可以通过混沌工程来进行相关实践，提升产品质量 通过 case study 可以帮助大家构建分布式混沌测试平台  Envoy 在轻舟微服务中落地实践 讲师：王佰平\n公司：网易\n讲师介绍：网易数帆资深工程师，负责轻舟 Envoy 网关与轻舟 Service Mesh 数据面开发、功能增强、性能优化等工作。对于 Envoy 数据面开发、增强、落地具有较为丰富的经验。\n演讲概要：\nEnvoy 是由 Lyft 开源的高性能数据和服务代理，以其可观察性和高扩展性著称。如何充分利用 Envoy 的特性，为业务构建灵活易扩展、稳定高性能的基础设施（服务网格、API 网关）是 Envoy 落地生产实践必须考虑的问题。本次分享主要介绍 Envoy 在轻舟微服务网关与轻舟微服务网格中落地实践经验和构建此类基础设施时轻舟关注的核心问题。希望能够给大家带来一些帮助。\n听众收益：\n 了解 Envoy 本身架构与关键特性； 在生产实践当中微服务网关与服务网格关注的核心问题以及轻舟 Envoy 的解决之道； 为期望实现集群流量全方位治理和观察的听众提供些许借鉴。  KubeVela：阿里巴巴新一代应用交付管理系统实践 讲师：孙健波\n公司：阿里巴巴\n讲师介绍：孙健波 (花名：天元) 阿里云技术专家，云原生应用模型 OAM (Open Application Model) 核心成员和主要制定者，KubeVela 项目作者，致力于推动云原生应用标准化，负责大规模云原生应用交付与应用管理相关工作。曾参与编写《Docker 容器与容器云》技术书籍。\n演讲概要：\n 云原生应用交付面临的问题与挑战 社区中常见的应用交付解决方案对比 基于 KubeVela 的标准化应用交付管理核心原理 阿里巴巴基于 KubeVela 的应用交付实践  听众收益：\n随着 “云原生” 的普及，基础设施逐渐成熟的今天，越来越多的应用开发者们开始追求快速的构建交付应用。然而使用场景的不同往往意味着应用交付的环境会有巨大的差异。就比如，K8s 中不同的工作负载类型需要对接不同的灰度发布、流量管理等实现方案，不同的部署环境（公有云、私有化部署等）也常常需要对接不同的日志监控体系。如何才能将应用管理和交付变得标准化，使得应用研发不再需要花大量精力对接不同的交付平台？这已经逐渐成为云原生应用管理领域的一大痛点。本次分享将针对这些问题为大家介绍如何基于 KubeVela 构建标准化的应用交付管理平台，介绍阿里巴巴在此基础上的实践经验。\nEnvoy 在阿里巴巴内部的落地实践 讲师：张义飞\n公司：阿里巴巴\n讲师介绍：阿里巴巴云原生部门高级工程师，主要负责阿里巴巴内部 ServiceMesh 数据面的落地。Envoy/Istio 社区 Member，给 envoy 社区贡献了 Dubbo Proxy filter，metadata 优化等。\n演讲概要：\n  介绍 Envoy 在大规模场景下存在的问题以及如何优化\n  介绍 Envoy 实现自定义协议的最佳实践\n   扩展自定义协议 扩展连接池 扩展 Cluster    介绍 Envoy 在阿里巴巴内部落地遇到的一些困难\n  听众收益：\n Envoy 在大规模场景下存在的一些问题 机器数量过多导致的内存问题 机器全量下发导致的 CPU 问题  ","date":1618635600,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"c40acfee850749d333afb24c0092c194","permalink":"https://jimmysong.io/docs/event/cloud-native-meetup-hangzhou-03/","publishdate":"2021-04-17T20:00:00+08:00","relpermalink":"/docs/event/cloud-native-meetup-hangzhou-03/","section":"event","summary":"本次活动关注于 ChaosMesh、Envoy 和 KubeVela。","tags":["ChaosMesh","Envoy","KubeVela"],"title":"云原生社区 meetup 第三期杭州站","type":"event"},{"authors":null,"categories":null,"content":"2021 年 3 月，《Quarkus 实战——专为 Kubernetes 而优化的 Java 解决方案》，已经由机械工业出版社出版上市，可点击查看详情购买，下面是本书封面及封底。\n   译者序 Quarkus 是一款有别于传统 Java 架构的新技术框架，它是建立在我们熟知的技术栈上，使用了诸多成熟的技术，如 JPA，JAX-RS、Eclipse Vert.x、Eclipse MicroProfile 和 CDI 等，并将之和 Kubernetes 紧密融合在一起。用户可以借助 Kubernetes 的高效的调度运维能力，最大限度地节约资源。\n云原生的星星之火，自社区 Kubernetes 爆红之后，变成燎原之势。云原生相关的技术如雨后春笋般涌出。刘岩，宋净超和我都是云原生社区的成员，也钟爱布道各种相关技术，是这一领域的狂热爱好者。我们共同的爱好之一，就是时刻关注有好的国外技术或者成熟技术的优秀书籍发布。\n在这一过程中，我们机缘巧合地发现了这本书，恰好这本书在国内还没有进行翻译，满怀热情的我们就此踏上了研究 Quarkus 之旅。\n这本采用十分简单的抛出问题，提出解决方案，和引发讨论的方式，将 Quarkus 的技术点描绘的细致透彻。通过本书，用户可以自学相关内容，借助 Quarkus，提高 Java 相关研发的工作效率，让你在快节奏的微服务构建和基于云的应用程序开发领域立于不败之地。\n在整个翻译过程中，我们得到华章出版社和李忠明编辑的全力帮助，在此表示衷心感谢。\n最后，感谢大家有缘阅读到此书，希望我们三人的绵薄之力可以帮助到崇尚云原生技术的你，在 Quarkus 的技术道路上，能够享受到和我们一样的欣喜。\n译者：张晓宇、刘岩、宋净超\n","date":1617148800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"c8dd8e154fb2f6ff5a1da83d9217e955","permalink":"https://jimmysong.io/docs/book/quarkus-cookbook/","publishdate":"2021-03-31T00:00:00Z","relpermalink":"/docs/book/quarkus-cookbook/","section":"book","summary":"专为 Kubernetes 而优化的 Java 解决方案","tags":["printed"],"title":"Quarkus实战","type":"publication"},{"authors":["宋净超","孙召昌","赵新","张城","刘硕然"],"categories":null,"content":"Istio 1.8——还是从前那个少年 讲师：宋净超（Tetrate 布道师、云原生社区创始人）\n个人介绍：Tetrate 布道师、CNCF Ambassador、云原生社区 创始人、电子工业出版社优秀译者、出品人。Kubernetes、Istio 等技术的早期使用及推广者。曾就职于科大讯飞、TalkingData 和蚂蚁集团。\n议题简介：带你回顾 Istio 的发展历程，看他是否还是从前那个少年，“没有一丝丝改变”，能够经历时间的考验。带你一起来了解 Istio 1.8 的新特性，看它是如何作为传统和现代应用的桥接器，成为云原生应用中的中流砥柱。同时也会为你分享云原生社区的规划，为了推行云原生，我们在行动。\n百度服务网格在金融行业的大规模落地实践 讲师：孙召昌（百度高级研发工程师）\n个人介绍：百度高级研发工程师，现就职于百度基础架构部云原生团队，参与了服务网格产品的研发工作和大规模落地实践，对云原生、微服务、Service Mesh等方向有深入的研究和实践经验。\n议题简介：百度服务网格技术在金融行业大规模落地过程的实践经验和思考，主要包括：\n 支持传统微服务应用的平滑迁移，兼容SpringCloud和Dubbo应用； 灵活对接多种注册中心，支持百万级别的服务注册和发现； 提供丰富的流量治理策略，包括自定义路由、全链路灰度等； 实现业务无侵入的指标统计和调用链展示，满足用户的可观察性需求。  Apache/Dubbo-go 在云原生时代的实践与探索 讲师：赵新（于雨）\n个人介绍：于雨（GitHub ID AlexStocks），dubbogo 社区负责人，一个有十多年服务端基础架构研发经验的一线程序员，陆续改进过 Redis/Muduo/Pika/Dubbo/Dubbo-go/Sentinel-go 等知名项目，目前在蚂蚁集团可信原生部从事容器编排和 Service Mesh 工作。\n议题简介：\n 基于Kubernetes 的微服务通信能力 基于 MOSN 的云原生 Service Mesh 能力 基于应用级注册的服务自省能力 dubbo-go 3.0 规划  合影、中场休息、签售 中场休息时会有《云原生操作系统 Kubernetes》作者之一张城为大家现场签售。\n云原生下的可观察性 讲师：张城（元乙）\n个人介绍：阿里云技术专家，负责阿里巴巴集团、蚂蚁金服、阿里云等日志采集基础设施，服务数万内外部客户，日流量数十PB。同时负责云原生相关的日志/监控解决方案，包括系统组件，负载均衡，审计，安全，Service Mesh，事件，应用等监控方案。目前主要关注可观察性、AIOps、大规模分析引擎等方向。\n议题简介：近年来随着云原生技术的普及，PaaS和SaaS化的程度越来越高，传统的监控系统正在朝可观察性系统的方向演进。在这背景下OpenTelemetry诞生，OpenTelemetry为我们带来了Metric、Tracing、Logging的统一标准，便于我们构建一个统一的可观察性平台。\n云原生分布式存储解决方案实践 讲师：刘硕然（OPPO）\n个人介绍：OPPO互联网云平台分布式文件存储技术负责人，ChubaoFS初创成员及项目维护者。\n议题简介：ChubaoFS是云原生的分布式存储系统，目前已经在多家公司生产环境为大规模容器平台的云原生应用提供分布式存储解决方案。主要特点包括高可用，高可扩展，多租户，文件及对象双接口等。与云原生社区的生态也有非常紧密的结合，目前监控使用Prometheus，部署支持Helm，使用支持CSI driver。\n","date":1608440400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"1945b774f78bb1667f9376e845ed281d","permalink":"https://jimmysong.io/docs/event/cloud-native-meetup-beijing-02/","publishdate":"2020-12-20T18:00:00+08:00","relpermalink":"/docs/event/cloud-native-meetup-beijing-02/","section":"event","summary":"本次活动关注 Istio、云原生存储、可观察性、DubboGo 等。","tags":["Istio","DubboGo","SpringCloud","ChubaoFS","OpenTelemetry"],"title":"云原生社区 meetup 第二期北京站","type":"event"},{"authors":["宋净超","高洪涛","杨可奥","侯诗军","程亮"],"categories":null,"content":" 演讲幻灯片及视频回放请点击页面上方的按钮。   开场演讲 讲师：宋净超\n公司：Tetrate\n讲师介绍：Tetrate 云原生布道师，云原生社区创始人，CNCF Ambassador，作家和摄影师，热衷于开源和分享。\nKubernetes 在 UCloud 内部的应用 讲师：高鹏\n公司：UCloud\n讲师介绍：高鹏 UCloud 后台研发工程师，负责内部云原生平台的建设。\n演讲概要：在 Kubernetes 的实际应用中，我们会碰到各种各样的问题，比如复杂的网络结构、持久化存储的实现、多租户的权限模型、集群的升级、Istio 的使用、镜像仓库的高可用、CI/CD、监控告警、日志、Operator 等等等等。这次分享将介绍 UCloud 内部对 Kubernetes 以及云原生生态的应用实践，和大家分析每个选择背后的原因，碰到的问题以及解决方案。\n使用 Apache SkyWalking Adapter 实现 K8s HPA 讲师：高洪涛\n公司：Tetrate\n讲师介绍：高洪涛 美国 servicemesh 服务商 Tetrate 创始工程师。原华为软件开发云技术专家，对云 APM 有深入的理解，并有丰富的 APM 产品设计，研发与实施经验。对分布式数据库，容器调度，微服务，ServicMesh 等技术有深入的了解。目前为 Apache SkyWalking 核心贡献者，参与该开源项目在软件开发云的商业化进程。前当当网系统架构师，开源达人，曾参与 Apache ShardingSphere，Elastic-Job 等知名开源项目。对开源项目的管理，推广和社区运营有丰富的经验。积极参与技术分享，曾在多个技术大会中做过分享，包括 ArchSummit， Top100，Oracle 嘉年华等。在多个媒体发表过文章，如 InfoQ。\n演讲概要：Apache SkyWalking 作为云原生可观测性工具在 Kubernetes 领域内有诸多应用，包括监控微服务的性能，观测基于 ISTIO 的 Service Mesh 服务等。本次分享将带来使用 SkyWalking 的 Adapter 来实现 Kubernetes 的 HPA 功能。应用该能力后，应用或服务将会根据 SkyWalking 分析的指标进行横向扩展。\nChaos Mesh - 让应用与混沌在 Kubernetes 上共舞 讲师：杨可奥\n公司：PingCAP\n讲师介绍：杨可奥，是 Chaos Mesh 的核心开发者之一；也是当前 Chaos Mesh 的 maintainer。在混沌工程的实践和实现上拥有一定经验和见解。除了 Chaos Mesh 之外还维护有多个受欢迎的开源项目，如 pprof-rs。他也曾多次主持或参与社区活动，对云环境下的应用有自己的见解和热爱。\n演讲概要：在生产环境中，各种各样的故障随时会发生，一个稳健的应用应当时刻处于能够应对故障的状态。对于云环境下的应用来说，这一点尤为重要。近些年来，混沌工程逐渐成为了一个稳定性保障和测试的重要话题。而 Chaos Mesh 以 Kubernetes 为平台，提供了云环境下的混沌工程实践方案。在这次分享中将介绍混沌工程和 Chaos Mesh 这一实践方案，并对其动机、Operator 模式、部分实现进行讲解。\n云原生技术在风控 SaaS 领域的踩坑与最佳实践 讲师：侯诗军\n公司：同盾科技\n讲师介绍：侯诗军 – 同盾科技云原生计算部门负责人，香港理工大学与华中科技大学双硕士，十几年来一直专注于云计算技术与管理领域，是 kube-router 等 kubernetes 组件的源代码贡献者，拥有多项云技术专利。16 年加入同盾科技，带领云原生团队从 0 到 N 进行了集团层面的自动化运维、K8S 云平台、DevOps 研发体系变革，将公司在线业务 100% 容器化。\n演讲概要：同盾是国内智能风控领域的头部领军企业，全国独角兽 top50。在中国、印尼雅加达、北美、新加坡拥有 8 个混合云数据中心，共计服务器数量 5000 + 台，在线运转的容器数量 2 万 + 个。截至目前已有超过 1 万家客户选择了同盾的产品及服务，API 单日调用量最低 1 亿次以上，平均响应时间最高 200MS 以内。截止 2019 年，同盾科技全部 (1000 个 app) 在线业务已 100% 容器化。本次议题分别从自动化运维、CI/CD、镜像、云原生网络、域名切换、监控与日志、机器学习、容器安全、混合云弹性、多云管理等方面来讲述公司的踩坑与最佳实践。\n云原生监控体系建设 讲师：程亮\n公司：VIPKID\n讲师介绍： VIPKID 资深架构师，曾任百度高级研发工程师，阿里巴巴技术专家。目前负责大班课后端总架构，VIPKID 监控系统。\n演讲概要：\nvipkid 的传统监控体系介绍：传统的机器监控，zabbix，falcon；日志监控，钉钉，邮件直接上报；业务监控\n基于 k8s 发布之后的监控体系：从 19 开始，vipkid 开始基于 k8s 的发布流程改造，响应的基于 thanos 的监控体系升级。基于公司内部的 CMDBCMD 系统，开发 k8s 的 opertor，自动化适配 vm（虚拟机）发布项目及监控。\n基于日志链路监控：基于流量 CDN-LB-WAF-NG 等链路信息的日志监控\n业务监控的全新规划: 对于研发 RD 和测试 QA 同学来说，更加关注线上业务的正确性。构建业务监控平台，支持线上业务指标追踪。\n","date":1606539600,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"8600100d5ffa5c951169c42f8f325783","permalink":"https://jimmysong.io/docs/event/cloud-native-meetup-shanghai-01/","publishdate":"2020-11-28T20:00:00+08:00","relpermalink":"/docs/event/cloud-native-meetup-shanghai-01/","section":"event","summary":"云原生社区举办的第一届城市站 meetup。","tags":["Istio","SkyWalking","DubboGo","ChaosMesh"],"title":"云原生社区 meetup 第一期上海站","type":"event"},{"authors":null,"categories":null,"content":"   《云原生模式》 图书封面  当我们在讨论云原生时究竟在讨论什么？这些年来我一直在思索这个问题，大家的观点可能不尽相同。三年前从我翻译了第一本云原生领域书籍开始，陆续参与翻译和创作了一系列云原生作品，同时通过对云原生领域的开源项目、社区、基金会、应用云化过程的参与和观察，我得出了下面的结论：云原生是一种行为方式和设计理念，究其本质，凡是能够提高云上资源利用率和应用交付效率的行为或方式都是云原生的。云计算的发展史就是一部云原生化的历史。云原生是云计算适应社会分工的必然结果，将系统资源、底层基础设施和应用编排交由云平台管理，让开发者专注于业务逻辑，这不正是云计算长久以来孜孜以求的吗？云原生应用追求的是快速构建高容错性、弹性的分布式应用，追求的极致的研发效率和友好的上线与运维体验，随云云原生的理念应运而生，它们天生适合部署在云上，可以最大限度利用云计算带来的红利。\n在此之前我曾翻译过几本云原生主题的图书，其中《Cloud Native Go》的作者 Kevin Hoffman，《云原生 Java》的作者 Josh Long，他们都是来自 Pivotal 或曾在 Pivotal 工作多年，当看到此书时，我惊奇的发现，作者 Cornelia Davis 同样来自这家公司，Pivotal 真可谓是云原生的黄埔军校，此书的内容跟以往的云原生书籍有所不同，对于模式的梳理标新立异，因此我立马联系了电子工业出版社的张春雨编辑，经他了解到张若飞正在翻译此书，此前我已与他合作翻译了《云原生 Java》，本书算是我跟他的第二次合作，他翻译图书时的精准和高效着实让我佩服，我们各自翻译了本书一半的内容。 人人都在讨论云原生，但是究竟如何实现却莫衷一是。本书列举了构建云原生应用的 12 种模式，主要关注的是云原生应用的数据、服务与交互，即应用层面的设计模式，这些模式穿插于本书的第二部分各个章节中，基本覆盖了云原生应用的各个方面，并将理论结合实践，带领读者使用 Java 来实现一个云原生应用。\n同时还要感谢云原生社区的成员及志愿者们，对于云原生在中国的发展做出的贡献，你们的鼓励和支持是在云原生领域不断努力和探索的动力。本书在翻译过程中难免有一些纰漏，还望读者指正。\n","date":1596931200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"89c91f15f5e3f5c3fb85d7856f0fa83d","permalink":"https://jimmysong.io/docs/book/cloud-native-patterns/","publishdate":"2020-08-09T00:00:00Z","relpermalink":"/docs/book/cloud-native-patterns/","section":"book","summary":"张若飞、宋净超 译","tags":["printed"],"title":"云原生模式","type":"publication"},{"authors":["靳文祥","付铖","黄家琦","王国云"],"categories":null,"content":"讲师与演讲话题 蚂蚁集团 API Gateway Mesh 的思考与实践 主讲人: 贾岛\n在 Service Mesh 微服务架构中，我们常常会听到东西流量和南北流量两个术语。蚂蚁集团开源的Service Mesh Sidecar MOSN 已经多次与大家见面交流了，以往的议题重点在东西流量的服务发现与路由，那么蚂蚁集团在南北流量上的思考是怎样的？本次分享，将从蚂蚁集团 API 网关发展历程来看，Mesh 化的网关架构是怎样的，解决了什么问题，双十一的实践表现，以及我们对未来的思考。\n酷家乐的 Istio 与 Knative 踩坑实录 主讲人: 付铖\n酷家乐在部分业务模块，自2018年使用了 Istio 进行服务治理，自2019年使用了 Knative 作为 FaaS 基础设施，在实践过程中解决了大量问题，也积累了不少第一手经验。本次分享，将重点讨论服务网格的性能损耗，存量业务迁移难题，函数计算的冷启动时间问题以及解决方案等。\n云原生开放智能网络代理 MOSN 金融级云原生架构助推器-蚂蚁集团 主讲人：肖涵（涵畅）\n圆桌环节：Service Mesh 落地的务实与创新 主讲人: 鲁直 、涵畅 、张超盟 、付铖 、王国云\n蚂蚁集团 Service Mesh 技术风险思考和实践 主讲人: 嘉祁\nServish Mesh 是微服务架构与云原生碰撞出的火花，对于传统的中间件体系与运维支撑能力是极大的挑战。本次分享的主题主要关注于在蚂蚁集团内部如何应对这些挑战，并建设相应的技术风险能力来保障其稳定。\n网易严选的 Service Mesh 实践 主讲人: 王国云\n网易严选在2016年选择了 Service Mesh 作为未来微服务改造的基础架构，并在过去几年支持了业务的持续快速增长。本次分享主要介绍 Service Mesh 在严选的落地和演进情况，讨论 Service Mesh 在混合云架构下落地遇到的挑战和我们的解决方案，同时也希望和大家交流一下在架构方面的一些思考。\n","date":1577163600,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"adfcbdf724f50e2689e5ae03c2c517f3","permalink":"https://jimmysong.io/docs/event/service-mesh-meetup-09/","publishdate":"2019-12-24T13:00:00+08:00","relpermalink":"/docs/event/service-mesh-meetup-09/","section":"event","summary":"这是第七届 Service Mesh Meetup。","tags":["Service Mesh"],"title":"Service Mesh Meetup #9 杭州站","type":"event"},{"authors":["曾凡松","黄挺","雷志远","周涛","曹寅","宋顺","李云","王成昌","吴天龙"],"categories":null,"content":"本期为 Service Mesh Meetup#8 特别场，联合 CNCF、阿里巴巴及蚂蚁集团共同举办。\n不是任何一朵云都撑得住双 11。\n成交 2684 亿，阿里巴巴核心系统 100% 上云。\n蚂蚁集团的核心交易链路大规模上线 Service Mesh。\n这次，让双 11 狂欢继续，让云原生经得起双 11 大考，也让云原生走到开发者身边。\n你将收获 3 大经验加持：\n 双 11 洗礼下的阿里巴巴 K8s 超大规模实践经验 蚂蚁集团首次 Service Mesh 大规模落地经验 阿里巴巴超大规模神龙裸金属 K8s 集群运维实践经验  讲师与演讲话题 释放云原生价值，双 11 洗礼下的阿里巴巴 K8s 超大规模实践 主讲人：曾凡松（逐灵） 、汪萌海（木苏）\n2019 双 11 点燃了全球人民的购物热情，而阿里经济体核心系统全面上云则刷爆了国内的技术圈子，引起了众多热爱云计算、云原生技术专家的热议。阿里巴巴是首个在超大规模体量公司内大规模使用 K8s 的公司，借此机会将为大家带来阿里巴巴在生产场景中大规模应用 K8s 的实践经验，包括在大规模应用管理上的经验教训；当前如何通过云原生方式高效管理应用；以及对未来应用管理发展趋势的基本看法。\n蚂蚁集团双 11 Service Mesh 超大规模落地实践 主讲人: 黄挺（鲁直） 、雷志远（碧远）\nService Mesh 在过去几年中取得了巨大的关注，但是业界大规模的落地却比较少，目前蚂蚁集团已经在双十一的核心交易链路中大规模上线 Service Mesh，在本次分享中，我们将详细分享我们如何做大规模的落地，在大规模落地 Service Mesh 的时候遇到了什么样的问题，对应的解法又是什么，分享我们在大规模落地 Service Mesh 之后取得的收益，希望能够给有志于尝试 Service Mesh 的公司带来更多的参考。\n阿里巴巴超大规模神龙裸金属 K8s 集群运维实践 主讲人: 周涛 (广侯）\n2019 年是云原生大规模落地的元年，阿里巴巴集团底层基础设施全部搬迁到公有云上，并基于阿里云最新一代的神龙裸金属服务器共同形成了云原生的最佳组合。本次分享将介绍大规模的神龙裸金属云原生集群如何管理和运维的实践。\n深入Kubernetes的“无人区” — 蚂蚁集团双十一的调度系统 主讲人: 曹寅\n蚂蚁集团今年已全面落地 Kubernetes, 支撑双 11 大促。这次分享主要介绍我们在落地过程中面对的各项新技术挑战，及应对这些问题形成的最佳实践，话题包含了规模化 Kubernetes 实践、计算型业务的统一调度与资源混部、蚂蚁双大促分时调度以及 Service Mesh 落地等。\n服务网格在“路口”的产品思考与实践 主讲人: 宋顺（齐天）\n这次分享主要介绍在云原生概念如火如荼而金融行业还处在数字化转型初期的当下，蚂蚁集团服务网格从技术到产品化过程中的思考与实践。内容包括：规模化场景下如何平滑过渡、如何兼顾性能与稳定性、如何支持多语言多协议、无侵入、异构服务统一治理等方面的思考和实践。\n阿里集团核心应用落地 Service Mesh 的挑战与机遇 主讲人: 李云（至简）\n将分享阿里巴巴集团在核心应用落地 Service Mesh 时如何做到“在飞行的飞机上换引擎”，以及在落地的过程中面临的挑战。那些挑战虽然在现阶段的落地过程中并没有全面解决，但给后面的发展和技术突破重点指引了方向。 此外，Service Mesh 作为云原生技术的关键内容，还将分享在这个技术趋势下的发展思考，与大家交流什么是“三位一体”，以及未来 Service Mesh 应当“长成什么模样”。\n蚂蚁集团云原生 PaaS 实践之路 主讲人: 王成昌（晙曦）\n已实现异地多活单元化架构的蚂蚁集团、网商银行基础设施正全面拥抱云原生。本次分享将首次披露蚂蚁集团 PaaS 产品层的建设思路：SOFAStack-CAFE - 承担着海量应用管理、变更保障、容灾多活的应用 PaaS，如何结合实际金融级技术风险保障诉求，基于 Kubernetes 构建云原生运维体系。内容将包括多集群联邦、核心运维能力下沉、发布变更体系、运行时安全、应用交付模式、SOFAMesh 运维体系支撑等方面的实践探索。\n函数计算在双十一小程序场景的应用 主讲人: 吴天龙 （木吴）\n小程序是轻量级的快速迭代的移动应用，对小程序开发者的开发效率有很高的要求。使用函数计算，开发者无需关心后端服务的搭建运维，只需要编写函数就能够提供稳定可靠并且弹性伸缩的服务。双 11 中很多推广活动都是以小程序的方式提供给用户的，在活动时间会有集中的访问，这对后端服务的稳定和弹性是很大的考验。使用函数计算提供的预留实例，可提前为活动高峰预留一部分资源，结合极速的弹性伸缩，轻松应对活动高峰。\n","date":1574571600,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"ca3bd72ddaabcc37ae049c1d121828d2","permalink":"https://jimmysong.io/docs/event/service-mesh-meetup-08/","publishdate":"2019-11-24T13:00:00+08:00","relpermalink":"/docs/event/service-mesh-meetup-08/","section":"event","summary":"这是第八届 Service Mesh Meetup。","tags":["Service Mesh"],"title":"Service Mesh Meetup #8 北京站","type":"event"},{"authors":["赵化冰","肖涵","杨川胡","杨彪"],"categories":null,"content":"本期 Meetup 邀请社区大咖，从服务网格下微服务架构设计、在 5G 时代的应用、如何使用开源的 Traefik 构建云原生边缘路由及蚂蚁集团的服务网格代理演进角度给大家带来精彩分享。\n讲师与演讲话题 服务网格技术在5G网络管理平台中的落地实践 赵化冰（中兴通讯网管软件资深专家）\n在通信网络向5G演进的过程中，电信行业借鉴了IT行业的微服务架构和云原生相关技术对5G网络功能进行重构，以提供敏捷、灵活、易于扩展的业务能力。 本演讲主题将介绍在5G网络管理平台的微服务架构中落地微服务网格的产品实践，包括多网络平面支持、API网关和网格Ingress的定位、Consul Registry的性能增强等等。\n蚂蚁集团网络代理的演进之路 肖涵（蚂蚁集团高级技术专家）\n从网络硬件设备到自研平台，从传统服务治理到 Service Mesh，本次分享将介绍蚂蚁集团网络代理在接入层以及 Service Mesh 化道路上是如何一步步支撑秒级百万支付，千万红包请求的。\n进击的Traefik——云原生边缘路由探秘 杨川胡（ 知群后台负责人）\nTraefik 是一个云原生的边缘路由器，开源的反向代理和负载均衡器，寄予厚望的 2.0 版本历时一年的开发终于发布了，此处大版本的更新新增了许多新的特性，特别是大家期望的对 TCP 的支持，在当前 topic 中我们将来探索 Traefik 2.0 有哪些值得我们关注的新特性。\nService Mesh下微服务的架构设计 杨彪（美团高级技术专家）\n当下Service Mesh技术可以说是炙手可热，它通过容器编排、持续交付DevOps、以及微服务等理论和方法来构建和运行云原生应用。然而Service Mesh毕竟发展才短短的2年，对于我们刚刚熟悉和稳定的微服务架构又将带来哪些挑战，业务系统架构是否有必要升级到Service Mesh，我们的微服务架构设计将发生什么改变，这些问题我将在本次大会交流讨论，敬请期待。\n","date":1572066000,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"73d940fabe0ee00971625962ad5ac976","permalink":"https://jimmysong.io/docs/event/service-mesh-meetup-07/","publishdate":"2019-10-26T13:00:00+08:00","relpermalink":"/docs/event/service-mesh-meetup-07/","section":"event","summary":"这是第七届 Service Mesh Meetup。","tags":["Service Mesh"],"title":"Service Mesh Meetup #7 成都站","type":"event"},{"authors":null,"categories":null,"content":"Serverless（无服务器架构）是指服务端逻辑由开发者实现，应用运行在无状态的计算容器中，由事件触发，完全被第三方管理，其业务层面的状态则存储在数据库或其他介质中。\nServerless 是云原生技术发展的高级阶段，可以使开发者更聚焦在业务逻辑，而减少对基础架构的关注。\n关于本书 本书是本人学习和实践 Serverless 过程中所整理的资料，目前主要关注的 Serverless 开源项目是 Knative。\n使用方式 您可以通过以下方式使用本书：\n 在线浏览  ","date":1571616000,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"fc81e90d61dedf88880100181148ebb8","permalink":"https://jimmysong.io/docs/book/serverless-handbook/","publishdate":"2019-10-21T00:00:00Z","relpermalink":"/docs/book/serverless-handbook/","section":"book","summary":"无服务器架构实践手册","tags":["handbook"],"title":"Serverless实战","type":"publication"},{"authors":null,"categories":null,"content":"本书为 Cloud Native Infrastructure 中文版，作者 Justin Garrison 和 Kris Nova ，英文版发行于 2017 年 11 月，已可以在网上免费获得，本书是关于创建和管理基础架构，以适用于云原生应用全生命周期管理的模式和实践。\n阅读完这本书后，您将会有如下收获：\n 理解为什么说云原生基础架构是高效运行云原生应用所必须的 根据准则来决定您的业务何时以及是否应该采用云原生 了解部署和管理基础架构和应用程序的模式 设计测试以证明您的基础架构可以按预期工作，即使在各种边缘情况下也是如此 了解如何以策略即代码的方式保护基础架构  使用方式 您可以通过以下方式使用本书：\n GitHub 在线阅读 英文原版  许可证 本书英文版版权属于 O’Reilly，中文版版权归属于机械工业出版，基于署名-非商业性使用-相同方式共享 4.0（CC BY-NC-SA 4.0）分享，本书为本人自行翻译，目的在于学习和巩固云原生知识，如有需要请购买纸质书。\n","date":1571616000,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"ab6cb03678341ec789ecf55430a12ff9","permalink":"https://jimmysong.io/docs/book/cloud-native-infrastructure/","publishdate":"2019-10-21T00:00:00Z","relpermalink":"/docs/book/cloud-native-infrastructure/","section":"book","summary":"专为 Kubernetes 而优化的 Java 解决方案","tags":["ebook","translation"],"title":"云原生基础架构","type":"publication"},{"authors":null,"categories":null,"content":"Google 有许多通用工程实践，几乎涵盖所有语言和项目。此文档为长期积累的最佳实践，是集体经验的结晶。我们尽可能地将其公之于众，您的组织和开源项目也会从中受益。\n当前包含以下文档：\nGoogle 代码审查指南，实则两套指南：\n 代码审查者指南 代码开发者指南  译者序 此仓库翻译自 google/eng-practices，目前为止的主要内容为 Google 总结的如何进行 Code Review（代码审查） 指南，根据原 Github 仓库的标题判断以后会追加更多 Google 工程实践的内容。\n 本文档的 Github 地址，点击页面底部的 Edit this page 链接可以编辑页面。 其他语言的版本。 感谢 Ta-Ching Chen 的审阅。 译者：Jimmy Song 本网站使用 Hugo 构建。 本网站使用 hugo-book 主题。  许可证 本项目中的文档适用于 CC-By 3.0 许可证，该许可证鼓励您共享这些文档。\n","date":1569801600,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"415ed3757cbd85008bb45b82a1580dd7","permalink":"https://jimmysong.io/docs/book/google-engineering-practices/","publishdate":"2019-09-30T00:00:00Z","relpermalink":"/docs/book/google-engineering-practices/","section":"book","summary":"谷歌代码审查实践指南","tags":["ebook","translation"],"title":"谷歌工程实践","type":"publication"},{"authors":["张波","彭泽文","涂小刚","敖小剑"],"categories":null,"content":"讲师与演讲话题 虎牙直播在微服务改造方面的实践 张波 虎牙基础保障部中间件团队负责人\n本次主要分享虎牙注册中心、名字服务、DNS 的改造实践，以及如何通过 Nacos 实现与 istio 打通实现，使微服务平滑过渡到 service mesh。\nService Mesh 在蚂蚁集团的生产级安全实践 彭泽文 蚂蚁集团高级开发工程师\n介绍通过 Envoy SDS（Secret Discovery Service）实现 Sidecar 证书管理的落地方案；分享如何为可信身份服务构建敏感信息数据下发通道，以及 Service Mesh Sidecar 的 TLS 生产级落地实践。\n基于 Kubernetes 的微服务实践 涂小刚 慧择网运维经理\n介绍如何跟据现有业务环境情况制定容器化整体解决方案，导入业务进入 K8S 平台，容器和原有业务环境互通。制订接入规范、配置中心对接 K8S 服务、网络互通方案、DNS 互通方案、jenkins-pipeline 流水线构建方案、日志采集方案、监控方案等。\nService Mesh 发展趋势（续）：棋到中盘路往何方 敖小剑 蚂蚁集团高级技术专家\n继续探讨 Service Mesh 发展趋势：深度分析 Istio 的重大革新 Mixer v2，Envoy 支持 Web Assembly 的意义所在，以及在 Mixer v2 出来之前的权宜之计; 深入介绍 Google Traffic Director 对虚拟机模式的创新支持方式，以及最近围绕 SMI 发生的故事。\n","date":1565499600,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"e43a0b86d60910a000c4731c733e3067","permalink":"https://jimmysong.io/docs/event/service-mesh-meetup-06/","publishdate":"2019-08-11T13:00:00+08:00","relpermalink":"/docs/event/service-mesh-meetup-06/","section":"event","summary":"这是第六届 Service Mesh Meetup。","tags":["Service Mesh"],"title":"Service Mesh Meetup #6 广州站","type":"event"},{"authors":null,"categories":null,"content":"   《未来架构》图书封面  这本书的第一作者是张亮，现就职于京东金融，为了丰富全书的内容，张亮要求了圈内的好友吴晟、敖小剑和我，共同创作了这本宏大命题的《未来架构》，下面引述的他介绍的成书原由。\n成书缘由 身处互联网行业的我们一向处在变革的最前端，受到世界浪潮的洗礼，不停歇地追赶着这一波又一波的技术潮流，才不会落在时代脚步之后。特别是近几年来，互联网架构不断演化，经历了从集中式架构到分布式架构，再到云原生架构的过程。云原生因能解决传统应用升级缓慢、架构臃肿、不能快速迭代等问题而逐渐成为这个时代舞台的主角。\n身处在这个变化浪潮中，我看着它改变着互联网架构的航行方向，并给越来越多的公司和个人带来新的思想和发展，也用我这些年走过的路、积累的经验、沉淀的眼界去学习它、读懂它，并让它融入我的知识体系网，来更新大脑里那张探索不断、充满指南针意义的架构地图。\n2017、2018 年，我与这些变化同进同退，让 Elastic-Job、Sharding-Sphere 成为业界里大家认可的项目、让所负责的开源项目开始走向国际化、也认识了更多的良师益友…… 这种种的经历和发展，触动我开始将所闻、所见、所知、所感的珠玑落到了笔尖，串联成了这本书：《未来架构 —— 从服务化到云原生》。\n这本书里有你想认识的分布式、服务化、服务网格、容器、编排治理、云原生、云数据库……\n这本书里既有我多年深思熟虑的见解和沉淀良久的经验，也有我弃笔又拾笔的挣扎，因为我需要让书的内容对读者负责……\n这本书里更有这些资深大咖的精彩章节叙述：Apache 孵化器项目 SkyWalking 创始人 \u0026amp; APM 专家吴晟、CNCF Ambassador \u0026amp; 云原生布道师 \u0026amp; 云原生社区创始人宋净超、Service Mesh 布道师敖小剑。\n目录  第 1 章 云原生 第 2 章 远程通信 第 3 章 配置 第 4 章 服务治理 第 5 章 观察分布式服务 第 6 章 侵入式服务治理方案 第 7 章 云原生生态的基石 Kubernetes 第 8 章 跨语言服务治理方案 Service Mesh 第 9 章 云原生数据架构 第 10 章 分布式数据库中间件生态圈 ShardingSphere  寄托期翼 书的封页是张亮老师选择的老特拉福德球场前矗立的曼联 Holy Trinity 雕像作为背景图。\n1958 年 2 月 6 日，曼联队在南斯拉夫参加欧冠杯获得半决赛权后，回程途中遭遇慕尼黑空难，曼联战队瞬间消失在夜空。为了曼联的复兴，幸存下来的曼联队的主帅马特・巴斯比强忍悲痛，用血泪和汗水重建曼联。1968 年 5 月 29 日，在慕尼黑空难整整 10 年后，巴斯比带领他的新战队终于捧起了欧洲冠军杯，告慰了那些故去的亡魂！这座 Holy Trinity 雕像变成了永恒的纪念！\n信仰、永不言弃、坚持不懈、创造奇迹、浴火重生…，是我从这座雕塑中感受到的力量。每个人的一生一定都会经历高峰和低谷，见过山川和沙漠，也希望这本书不仅仅能为大家带来互联网架构的干货知识，也能寄托我对大家的祝福：希望在这十万长征路上正在不懈拼搏的你，能够拥有自己的信仰和希望，即使要途径无数沙漠和海洋，也能在经历千帆后柳暗花明！\n","date":1554076800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"3fcfce4a4eaa9356b969ffe6d6a97952","permalink":"https://jimmysong.io/docs/book/future-architecture/","publishdate":"2019-04-01T00:00:00Z","relpermalink":"/docs/book/future-architecture/","section":"book","summary":"张亮、吴晟、敖小剑、宋净超 著","tags":["printed"],"title":"未来架构——从服务化到云原生","type":"publication"},{"authors":["郑德惠","陈逸凡","崔秀龙","宋净超","敖小剑"],"categories":null,"content":"讲师与演讲话题 唯品会 Service Mesh 的实践分享 郑德惠 唯品会Java资深开发工程师，内部Service Mesh框架负责人，唯品会开源项目vjtools重要开发者，10年电信与互联网后台开发经验。\nSOFAMosn 持续演进路径及实践案例 陈逸凡 花名无钩，蚂蚁集团资深开发工程师。专注于网络接入层，高性能服务器研发，SOFAMosn团队核心成员\n在网格的边缘试探——企业 Istio 试水指南 崔秀龙 HPE 软件分析师，Kubernetes 权威指南作者之一，Kubernetes、Istio 项目成员\nRoundtable：回顾2018，Service Mesh 蓄势待发 主持人：宋净超，ServiceMesher 社区联合创始人\n","date":1546750800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"71aa318f87a07770754a4650d2134da8","permalink":"https://jimmysong.io/docs/event/service-mesh-meetup-05/","publishdate":"2019-01-06T13:00:00+08:00","relpermalink":"/docs/event/service-mesh-meetup-05/","section":"event","summary":"这是第五届 Service Mesh Meetup。","tags":["Service Mesh"],"title":"Service Mesh Meetup #5 广州站","type":"event"},{"authors":["吴晟","敖小剑","冯玮","徐运元"],"categories":null,"content":"讲师与演讲话题 Observability and Istio telemetry 吴晟 Apache SkyWalking创始人、Apache Sharding-Sphere原型作者、比特大陆资深技术专家、CNCF OpenTracing标准化委员会成员\n蚂蚁集团 Service Mesh 渐进式迁移方案 敖小剑 蚂蚁集团高级技术专家，十六年软件开发经验，微服务专家，Service Mesh布道师，Servicemesher社区联合创始人\n张瑜标 阿里巴巴技术专家、前京东Hadoop负责人、Hadoop代码贡献者、现负责UC 基于Kubernetes自研的PaaS平台整体的稳定性\n探讨和实践基于Isito的微服务治理事件监控 徐运元 谐云科技云平台架构师，致力于容器 PaaS 平台、企业级容器云平台的方案设计和技术落地\nEnvoy、Contour与Kubernetes实践 冯玮 七牛容器云平台产品架构师，曾在百度和华为从事公有云领域高性能分布式计算和存储平台的架构设计和产品研发\n","date":1543122000,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"15d6465073096faa5872f9ca72446fd0","permalink":"https://jimmysong.io/docs/event/service-mesh-meetup-04/","publishdate":"2018-11-25T18:00:00+08:00","relpermalink":"/docs/event/service-mesh-meetup-04/","section":"event","summary":"这是第四届 Service Mesh Meetup。","tags":["Service Mesh"],"title":"Service Mesh Meetup #4 上海站","type":"event"},{"authors":["张超盟","朱经惠","邵俊雄","杨文"],"categories":null,"content":"讲师与演讲话题 张超盟（华为）——Kubernetes容器应用基于Istio的灰度发布实践\nTopic摘要：随着1.0版本在上月底的发布，标志着Istio作为最火热的ServcieMesh框架已经逐渐成熟。本次议题中将以典型的灰度发布为例，分享华为云容器服务在Istio的实践，以及Istio和Kubernetes的完美结合释放云原生应用的核心优势，加速企业微服务技术转型。\n讲师简介：华为云微服务平台架构师，现负责华为云容器服务Istio产品化工作。参与华为PaaS平台产品设计研发，在Kubernetes容器服务、微服务架构、云服务目录、大数据、APM、DevOpS工具等多个领域有深入研究与实践。曾供职于趋势科技。\n朱经惠 （联邦车网）——Istio控制平面组件原理解析\nTopic摘要：网上有很多关于Istio的介绍，但主要的关注是数据平面。所以这次独辟蹊径，给大家解密Istio里强大的控制平面：管理生命周期的Pilot-Agent，配置中心Pilot-Discovery， 生成遥测报告的Mixer以及安全证书管理的Istio_Ca。通过本次分享您将了解其工作原理和现存的问题。\n讲师简介：朱经惠，ETC车宝平台工程师。喜欢开源，个人开源项目《Jaeger PHP Client》；喜欢研究源码，对NSQ，Jaeger，Istio（控制平面）等go语言开源项目进行过研究。除了代码还喜欢爬山和第二天睡醒后全身酸疼的感觉。\n邵俊雄（蚂蚁集团）——SOFAMesh 的通用协议扩展\nTopic摘要：介绍蚂蚁集团在SOFAMesh上开发对SOFA RPC与HSF这两个RPC框架的支持过程中总结出来的一个通用协议扩展方案。\n讲师介绍：蚂蚁集团中间件团队高级技术专家，目前主要负责 SOFAMesh 的开发工作。\n杨文（JEX）——Kubernetes、Service Mesh、CI/CD 实践\nTopic摘要：本次主题我将跟大家分享我们在提升研发团队工程效率上的一些思考和实践，包括如何构建自动化 CI/CD 平台，如何提升持续交付能力，以及我们在这一系列演化过程中所踩过一些坑。\n讲师介绍：JEX 技术VP，前小恩爱技术总监，开源爱好者，TiDB、logkit 等多个开源项目的 Contributor，Go 夜读发起人。\n","date":1535173200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"a626ff20443cad1d9fe4d02a4f4607ec","permalink":"https://jimmysong.io/docs/event/service-mesh-meetup-03/","publishdate":"2018-08-25T18:00:00+08:00","relpermalink":"/docs/event/service-mesh-meetup-03/","section":"event","summary":"这是第三届 Service Mesh Meetup。","tags":["Service Mesh"],"title":"Service Mesh Meetup #3 深圳站","type":"event"},{"authors":["张亮","吴晟","朵晓东","丁振凯"],"categories":null,"content":"讲师与演讲话题 张亮（京东金融数据研发负责人）：Service Mesh的延伸 —— 论道Database Mesh\n个人简介：张亮，京东金融数据研发负责人。热爱开源，目前主导两个开源项目Elastic-Job和Sharding-Sphere(Sharding-JDBC)。擅长以java为主分布式架构以及以Kubernetes和Mesos为主的云平台方向，推崇优雅代码，对如何写出具有展现力的代码有较多研究。2018年初加入京东金融，现担任数据研发负责人。目前主要精力投入在将Sharding-Sphere打造为业界一流的金融级数据解决方案之上。\n随着Service Mesh概念的推广与普及，云原生、低接入成本以及分布式组件下移等理念，已逐渐被认可。在Service Mesh依旧处于高速迭代的发展期的同时，以它的理念为参考，其他的Mesh思想也在崭露萌芽。 Database Mesh即是Service Mesh的其中一种延伸，虽然理念与Service Mesh相近，但数据库与无状态的服务却有着巨大的差别。Database Mesh与分布式数据库（如NoSQL和NewSQL）的功能范畴并非重叠而是互补，它更加关注数据库之上的中间啮合层。本次将与您一起交流Database Mesh的一些思考，以及探讨如何与现有产品相结合，实现更加强大与优雅的云原生数据库解决方案。\n 吴晟（Apache SkyWalking创始人）：Observability on Service Mesh —— Apache SkyWalking 6.0\n个人简介：Apache SkyWalking 创始人，PPMC和Committer，比特大陆资深技术专家，Tetrate.io Founding Engineer，专注APM和自动化运维相关领域。Microsoft MVP。CNCF OpenTracing标准化委员会成员。Sharding-Sphere PMC 成员。\nAPM在传统意义上，都是通过语言探针，对应用性能进行整体分析。但随着Cloud Native, K8s容器化之后，以Istio为代表的Service Mesh的出现，为可观测性和APM提供了一种新的选择。SkyWalking作为传统上提供多语言自动探针的Apache开源项目，在service mesh的大背景下，也开始从新的角度提供可观测性支持。\nSkyWalking和Tetrate Inc. Istio核心团队合作，从Mixer接口提取遥感数据，提供SkyWalking语言探针一样的功能，展现service mesh风格探针的强大力量。之后，也会和更多的mesh实现进行合作，深入在此领域的运用。\n 朵晓东（蚂蚁集团，高级技术专家）：蚂蚁集团开源的Service Mesh数据平面SOFA MOSN深层揭秘\n个人简介：蚂蚁集团高级技术专家，专注云计算技术及产品。Apache Kylin创始团队核心成员；蚂蚁金融云PaaS创始团队核心成员，Antstack网络产品负责人；SOFAMesh创始团队核心成员。\nService Mesh技术体系在蚂蚁落地过程中，我们意识到Mesh结合云原生在多语言，流量调度等各方面的优势，同时面对蚂蚁内部语言体系与运维构架深度融合，7层流量调度规则方式复杂多样，金融级安全要求等诸多特征带来的问题和挑战，最终选择结合蚂蚁自身情况自研Golang版本数据平面MOSN，同时拥抱开源社区，支持作为Envoy替代方案与Istio集成工作。本次session将从功能、构架、跨语言、安全、性能、开源等多方面分享Service Mesh在蚂蚁落地过程中在数据平面的思考和阶段成果。\n 丁振凯（新浪微博，微博搜索架构师）：微博Service Mesh实践 - WeiboMesh\n个人简介：微博搜索架构师，主要负责搜索泛前端架构工作。主导搜索结果和热搜榜峰值应对及稳定性解决方案，以及微服务化方案落地。在Web系统架构方面拥有比较丰富的实践和积累。喜欢思考，深究技术本质。去年十一鹿晗关晓彤事件中一不小心成为网红工程师，并成功登上自家热搜榜。\nWeiboMesh源自于微博内部对异构体系服务化的强烈需求以及对历史沉淀的取舍权衡，它没有把历史作为包袱，而是巧妙的结合自身实际情况完成了对Service Mesh规范的实现。目前WeiboMesh在公司内部已经大规模落地，并且已经开源，WeiboMesh是非常接地气的Service Mesh实现。本次分享主要介绍微博在跨语言服务化面临的问题及WeiboMesh方案介绍，并结合业务实例分析WeiboMesh的独到之处。\n","date":1532840400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"57e36a41d7270d13e1609109d6ec7d26","permalink":"https://jimmysong.io/docs/event/service-mesh-meetup-02/","publishdate":"2018-07-29T13:00:00+08:00","relpermalink":"/docs/event/service-mesh-meetup-02/","section":"event","summary":"这是第二届 Service Mesh Meetup。","tags":["Service Mesh"],"title":"Service Mesh Meetup #2 北京站","type":"event"},{"authors":null,"categories":null,"content":"   《Python云原生》图书封面  Cloud Native Python 介绍 随着当今商业的迅速发展，企业为了支撑自身的迅速扩张，仅仅通过自有的基础设施是远远不够的。因此，他们一直在追求利用云的弹性来构建支持高度可扩展应用程序的平台。\n这本书能够帮助您一站式的了解使用Python构建云原生应用架构的所有信息。本书中我们首先向您介绍云原生应用架构和他们能够帮助您解决哪些问题。然后您将了解到如何使用REST API和Python构建微服务，通过事件驱动的方式构建Web层。接下来，您将了解到如何与数据服务进行交互，并使用React构建Web视图，之后我们将详细介绍应用程序的安全性和性能。然后，您还将了解到如何Docker容器化您的服务。最后，您将学习如何在AWS和Azure平台上部署您的应用程序。在您部署了应用程序后，我们将围绕关于应用程序故障排查的一系列概念和技术来结束这本书。\n本书中涵盖哪些内容  第1章 介绍云原生应用架构和微服务，讨论云原生架构的基本概念和构建应用程序开发环境。 第2章 使用Python构建微服务，构建自己的微服务知识体系并根据您的用例进行扩展。 第3章 使用Python构建Web应用程序，构建一个初始的Web应用程序并与微服务集成。 第4章 与数据服务交互，教您如何将应用程序迁移到不同的数据库服务。 第5章 使用React构建Web视图。 第6章 使用Flux创建可扩展UI，帮助您理解如何使用Flux创建可扩展的应用程序。 第7章 事件溯源和CQRS，讨论如何以事件形式存储合约（transaction）。 第8章 保护Web应用程序，让您的应用程序免于受到外部威胁。 第9章 持续交付，应用程序频繁发布的相关知识。 第10章 Docker容器化您的服务，讨论容器服务和在Docker中运行应用程序。 第11章 将应用程序部署到AWS平台上，教您如何在AWS上构建基础设施并建立应用程序的生产环境。 第12章 将应用程序部署到Azure平台上，讨论如何在Azure上构建基础设施并建立应用程序的生产环境。 第13章 监控云应用，了解不同的基础设施和应用的监控工具。  使用本书您需要哪些工具和环境 您需要在系统上安装Python。一个文本编辑器，最好是Vim、Sublime或者Notepad++。在有一个章节中您需要下载POSTMAN，这是一个功能强大的API测试套件，可以作为作为Chrome扩展插件来安装。您可以从这里下载。\n除此之外，如果您还有如下网站的账号那就更好了：\n Jenkins Docker Amazon Web Services Terraform  目标读者 本书适用于具有Python基础知识、熟悉命令行和基于HTTP的应用程序基本原理的开发人员。对于那些想要了解如何构建、测试和扩展Python开发的应用程序的人员来说本书是个理想选择。不需要有使用Python构建微服务的经验。\n","date":1530403200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"ee1ffade370d7cff3745bdfe71a1a446","permalink":"https://jimmysong.io/docs/book/cloud-native-python/","publishdate":"2018-07-01T00:00:00Z","relpermalink":"/docs/book/cloud-native-python/","section":"book","summary":"构建应对海量用户数据的高可扩展Web应用","tags":["printed"],"title":"Python云原生","type":"publication"},{"authors":null,"categories":null,"content":"   云原生 Java 图书封面  这是我翻译的第三本云原生相关技术书籍，前两本分别是：\n Cloud Native Go 中文版 Cloud Native Python 中文版     照片拍摄于 2018 年 11 月 3 日，北京\n本书介绍 传统企业与如亚马逊、Netflix和Etsy这类企业之间的区别是什么？这些公司有完善云原生开发方法，这些方法使得他们能够保持优势并领先于竞争对手。本实践指南向Java/JVM开发人员展示如何使用Spring Boot、Spring Cloud和Cloud Foundry更快更好得构建软件。\n很多组织都已踏足云计算、测试驱动开发、微服务与持续集成和交付领域。本书作者Josh Long和Kenny Bastani将带您深入研究这些工具和方法，并帮助您将传统应用程序转变为真正的云原生应用程序。\n本书中包含以下四大部分：\n 基础知识：了解云原生思维背后的动机；配置和测试Spring Boot应用程序；将您的传统应用程序迁移至云端 微服务：使用Spring构建HTTP和RESTful服务；在分布式系统中路由请求；建立更接近数据的边缘服务 数据整合：使用Spring Data管理数据，并将分布式服务与Spring支持的事件驱动的，以消息传递为中心的架构集成 生产：让您的系统可观测；使用服务代理来连接有状态的服务；了解持续交付背后的重要思想  如果您正在构建云原生应用程序，这本书将是使用Java生态系统的基本指南。本书中包含了所有内容——构建弹性服务、管理数据流（通过REST和异步事件）、测试、部署和可观察性的关键任务。\n——Daniel Bryant，SpectoLabs的软件开发者和CTO\n我预测无论是刚开始云原生之旅还是已经接近云原生的目标，所有参与其中的人都将从这本云原生Java的洞察和经验中受益。\n——Dava Syer博士，Spring框架的贡献者，Spring Boot和Spring Cloud的贡献者和联合创始人\n作者信息 Josh Long是一名Spring布道师，同时也是InfoQ.com的Java queue编辑，以及包括Spring Recipes第二版（Apress出版社出版）在内的多本书籍的主要作者。Josh在许多国际行业会议上发表过演讲，包括TheServiceSide Java Symposium、SpringOne、OSCON、JavaZone、Devoxx、Java2Days等。当他没在编写SpringSource的代码的时候，不是泡在Java用户组就是在咖啡店里喝咖啡。Josh喜欢能够推动技术发展的解决方案。他的兴趣包括可扩展性、BPM、网格计算、移动计算和所谓的“智能”系统等。您可以在http://blog.springsource.org或http://joshlong.com上浏览他的博客。\nKenny Bastani是Pivotal的Spring布道师。作为一名开源贡献者和博客作者，Kenny关注图数据库、微服务等，并喜欢吸引一群充满热情的软件开发人员。Kenny还是OSCON、SpringOne Platform和GOTO等行业会议的常客。他维护了一个关于软件架构的个人博客，并提供用于构建事件驱动的微服务和无服务器架构的教程和开源参考示例。\n目录 序言（James Watters） xvii\n序言（Rod Johnson） xix\n前言 xxi\n第Ⅰ部分　基础知识\n 第1 章　云原生应用程序 3 第2 章　训练营：Spring Boot 和Cloud Foundry 21 第3 章　符合十二要素程序风格的配置 67 第４章　测试 85 第5 章　迁移遗留的应用程序 115  第Ⅱ部分　Web 服务\n 第6 章　REST API 137 第7 章　路由 179 第8 章　边缘服务 197  第Ⅲ部分　数据整合\n 第9 章　数据管理 251 第10 章　消息系统 303 第11 章　批处理和任务 325 第12 章　数据集成 363  第IV 部分　生产\n 第13 章　可观测的系统 411 第14 章　服务代理 469 第15 章　持续交付 497  第V 部分 附录\n附录A 在Java EE 中使用Spring Boot 527\n","date":1530403200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"5e4ce4787f40566e2879959ee0cdac40","permalink":"https://jimmysong.io/docs/book/cloud-native-java/","publishdate":"2018-07-01T00:00:00Z","relpermalink":"/docs/book/cloud-native-java/","section":"book","summary":"张若飞、宋净超 译","tags":["printed"],"title":"云原生Java","type":"publication"},{"authors":["敖小剑","刘超","唐鹏程","徐运元"],"categories":null,"content":"讲师分享  云原生社区 meetup 第七期深圳站开场致辞 - 宋净超 使用 IAST 构建高效的 DevSecOps 流程 - 董志勇 云原生场景下的开发和调试-汪晟杰，黄金浩 Envoy 在腾讯游戏云原生平台应用 - 田甜 使用 KubeVela 构建混合云应用管理平台 - 邓洪超  ","date":1530334800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"83056fece6b9c5f9d8c261c13b6a6a6e","permalink":"https://jimmysong.io/docs/event/service-mesh-meetup-01/","publishdate":"2018-06-30T18:00:00+08:00","relpermalink":"/docs/event/service-mesh-meetup-01/","section":"event","summary":"这是第一届 Service Mesh Meetup。","tags":["Service Mesh"],"title":"Service Mesh Meetup #1 杭州站","type":"event"},{"authors":["敖小剑","章耿","董一韬","宋国恒","曹杰","俞仁杰","陈龙"],"categories":null,"content":"SOFAStack（Scalable Open Financial Architecture Stack）是蚂蚁集团自主研发并开源的金融级分布式架构，包含了构建金融级云原生架构所需的各个组件，是在金融场景里锤炼出来的最佳实践。SOFAStack 官方网站：https://www.sofastack.tech/\n参加此次 Meetup 您将获得：\n 基于 SOFAStack 快速构建微服务 金融场景下的分布式事务最佳实践 基于 Kubernetes 的云原生部署体验 云上的 Service Mesh 基本使用场景体验 基于 Serverless 轻松构建云上应用  如何注册：此活动须提前注册。请将 SOFAStack Cloud Native Workshop 添加到您 KubeCon + CloudNativeCon + Open Source Summit 的注册表里。您可以使用 KCCN19COMATF 折扣码获取 KubeCon 半价门票！\n如果对此活动有任何疑问，请发送邮件至 jingchao.sjc@antfin.com。\n活动详情 9:00 - 9:20 开场演讲 SOFAStack 云原生开源体系介绍 by 余淮\n9:20 - 10:10 使用 SOFAStack 快速构建微服务 by 玄北\n基于 SOFA 技术栈构建微服务应用。通过本 workshop ，您可以了解在 SOFA 体系中如何上报应用监控数据、服务链路数据以及发布及订阅服务。\n10:15 - 11:05 SOFABoot 动态模块实践 by 卫恒\n在本 workshop 中，您可以基于 SOFADashboard 的 ARK 管控能力来实现 SOFAArk 提供的合并部署和动态模块推送的功能。\n11:10 - 12:00 使用 Seata 保障支付一致性 by 屹远\n微服务架构下，分布式事务问题是一个业界难题。通过本workshop，您可以了解到分布式架构下，分布式事务问题产生的背景，以及常见的分布式事务解决方案；并亲身体验到如何使用开源分布式事务框架Seata的AT模式、TCC模式解决业务数据的最终一致性问题。\n12:00 - 13:00 午餐时间\n13:00 - 13:30 蚂蚁集团的云原生探索与实践 by 首仁\n13:30 - 14:40 通过 Serverless 快速上云 by 隐秀\n作为云原生技术前进方向之一，Serverless 架构让您进一步提高资源利用率，更专注于业务研发。通过我们的 workshop，您可以体验到快速创建 Serveless 应用、根据业务请求秒级 0-1-N 自动伸缩、通过日志查看器快速排错、按时间触发应用等产品新功能。\n14:50 - 16:00 使用 CloudMesh 轻松实践 Service Mesh by 敖小剑\nService Mesh 将服务间通信能力下沉到基础设施，让应用解耦并轻量化。但 Service Mesh 本身的复杂度依然存在，CloudMesh 通过将 Service Mesh 托管在云上，使得您可以轻松的实践 Service Mesh 技术。通过我们的 workshop，您可以快速部署应用到 CloudMesh ，对服务进行访问，通过监控查看流量，体验服务治理、Sidecar管理和对服务的新版本进行灰度发布等实用功能。\n","date":1529805600,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"95540d21f15586a7e48215064ab94210","permalink":"https://jimmysong.io/docs/event/sofastack-cloud-native-workshop/","publishdate":"2018-06-24T20:00:00+08:00","relpermalink":"/docs/event/sofastack-cloud-native-workshop/","section":"event","summary":"这是 KubeCon 第一次在中国举办，蚂蚁集团参与了同场活动，进行了 SOFAStack 云原生动手实验。","tags":["SOFAStack","KubeCon"],"title":"SOFAStack Cloud Native Workshop","type":"event"},{"authors":null,"categories":null,"content":"此书是 Kubernetes Handbook 的前传，本书中主要介绍 Docker 1.13 新特性和帮助大家了解 Docker 集群的管理和使用。\n GitHub 在线访问地址  容器是微服务的最佳载体，Kubernetes 是微服务的最佳运行平台，Istio 是 Kubernetes 上最佳的 service mesh。\n2017年4月 Docker 项目改名为 moby，10月的 DockerCon 在哥本哈根 Docker 公司宣布拥抱 Kubernetes，对于容器编排领域 Kubernetes 已经成为事实上的王者。\n关于 本书中引用了一些公开的分享与链接并加以整理。\n本书作于2017年初，现已停止更新。\n","date":1507680000,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"cc08d3e8dc3067f862ae62792c803e47","permalink":"https://jimmysong.io/docs/book/docker-handbook/","publishdate":"2017-10-11T00:00:00Z","relpermalink":"/docs/book/docker-handbook/","section":"book","summary":"Docker容器化实战手册","tags":["handbook"],"title":"Docker指南","type":"publication"},{"authors":null,"categories":null,"content":"本手册将指导你如何使用Hugo构建静态网站用于个人博客或者项目展示。\n手把手教你如何从0开始构建一个静态网站，这不需要有太多的编程和开发经验和时间投入，也基本不需要多少成本（除了个性化域名），使用GitHub和Hugo模板即可快速构建和上线一个网站。\n 在线浏览 Github地址  目标读者 本文档适用于以下用户：\n 对网站构建不太了解，没有web开发经验的用户 希望快速构建一个网站，个性化需求不多，不想花费太多精力打理，更新不多的用户 希望学习网站构建、网站模板、web开发的用户 对Go语言感兴趣，希望从事开源项目的用户👏  贡献者 感谢此书的贡献者。\n","date":1507593600,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"1f2a1c51e7fa860e7a702c6589f0a744","permalink":"https://jimmysong.io/docs/book/hugo-handbook/","publishdate":"2017-10-10T00:00:00Z","relpermalink":"/docs/book/hugo-handbook/","section":"book","summary":"静态网站构建指南","tags":["handbook"],"title":"Hugo实战手册","type":"publication"},{"authors":null,"categories":null,"content":"   Cloud Native Go 图书封面   本书作者：Kevin Hoffman \u0026amp; Dan Nemeth 译者：宋净超、吴迎松、徐蓓、马超译 出版社：电子工业出版社 全名：Cloud Native Go - 基于Go和React的web云原生应用构建指南  本书已由电子工业出版社出版，可以在京东上购买。\n   照片拍摄于 2017 年 9 月 12 日，北京\n简介 Cloud Native Go向开发人员展示如何构建大规模云应用程序，在满足当今客户的强大需求的同时还可以动态扩展来处理几乎任何规模的数据量、流量或用户。\nKevin Hoffman和Dan Nemeth详细描述了现代云原生应用程序，阐明了与快速、可靠的云原生开发相关的因素、规则和习惯。他们还介绍了Go这种“简单优雅”的高性能语言，它特别适合于云开发。\n在本书中你将使用Go语言创建微服务，使用ReactJS和Flux添加前端Web组件，并掌握基于Go的高级云原生技术。Hoffman和Nemeth展示了如何使用Wercker、Docker和Dockerhub等工具构建持续交付管道; 自动推送应用程序到平台上; 并系统地监控生产中的应用程序性能。\n 学习“云之道”：为什么开发好的云软件基本上是关于心态和规则 了解为什么使用Go语言是云本地微服务开发的理想选择 规划支持持续交付和部署的云应用程序 设计服务生态系统，然后以test-first的方式构建它们 将正在进行的工作推送到云 使用事件源和CQRS模式来响应大规模和高吞吐量 安全的基于云的Web应用程序：做与不做的选择 使用第三方消息传递供应商创建响应式云应用程序 使用React和Flux构建大规模，云友好的GUI 监控云中的动态扩展，故障转移和容错  章节简介如下图。\n   关于作者 Kevin Hoffman通过现代化和以多种不同语言构建云原生服务的方式帮助企业将其应用程序引入云端。他10岁时开始编程，在重新组装的CommodoreVIC-20上自习BASIC。从那时起，他已经沉迷于构建软件，并花了很多时间学习语言、框架和模式。他已经构建了从遥控摄影无人机、仿生性安全系统、超低延迟金融应用程序到移动应用程序等一系列软件。他在构建需要与Pivotal Cloud Foundry配合使用的自定义组件时爱上了Go语言。\nKevin 是流行的幻想书系列（The Sigilord Chronicles ）的作者，他热切地期待着最终能够将自己对构建软件的热爱与对构建幻想世界的热爱结合起来。\nDan Nemeth目前在Pivotal担任咨询解决方案架构师，负责支持Pivotal Cloud Foundry。他从Commodore 64开始就一直在开发软件，从1995年起开始专业编码，使用ANSIC编写了用于本地ISP的CGI脚本。从那时起，他职业生涯的大部分时间里是作为独立顾问为从金融到制药行业提供解决方案，并使用当时流行的各种语言和框架。Dan最近接受了Go作为自己的归宿，并热情地将它用于所有的项目。\n如果你发现Dan没在电脑前，他很可能就是在靠近安纳波利斯的水域玩帆船或飞钓。\n目录  第1章 云之道 第2章 开始 第3章 Go入门 第4章 持续交付 第5章 在Go中构建微服务 第6章 运用后端服务 第7章 构建数据服务 第8章 事件溯源和CQRS 第9章 使用Go构建web应用程序 第10章 云安全 第11章 使用WebSockets 第12章 使用React构建Web视图 第13章 使用Flux构建可扩展的UI 第14章 创建完整应用World of FluxCraft 第15章 结论 附录A 云应用的故障排查 索引  ","date":1501545600,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"0e73bfe8c6dd89864805484bce53c620","permalink":"https://jimmysong.io/docs/book/cloud-native-go/","publishdate":"2017-08-01T00:00:00Z","relpermalink":"/docs/book/cloud-native-go/","section":"book","summary":"宋净超、吴迎松、徐蓓、马超译","tags":["printed"],"title":"Cloud Native Go","type":"publication"},{"authors":null,"categories":null,"content":"本书是 Migrating to Cloud Native Application Architectures 的中文版，本书英文版发布于 2015 年 2 月，中文版由 Jimmy Song 翻译，发布于 2017 年 7 月。\n GitHub Gitbook 阅读  译者序 云时代的云原生应用大势已来，将传统的单体架构应用迁移到云原生架构，你准备好了吗？\n俗话说“意识决定行动”，在迁移到云原生应用之前，我们大家需要先对 Cloud Native（云原生）的概念、组织形式并对实现它的技术有一个大概的了解，这样才能指导我们的云原生架构实践。\nPivotal 是云原生应用的提出者，并推出了 Pivotal Cloud Foundry 云原生应用平台和 Spring 开源 Java 开发框架，成为云原生应用架构中先驱者和探路者。\n原书作于2015年，其中的示例主要针对 Java 应用，实际上也适用于任何应用类型，云原生应用架构适用于异构语言的程序开发，不仅仅是针对 Java 语言的程序开发。截止到本人翻译本书时，云原生应用生态系统已经初具规模，CNCF 成员不断发展壮大，基于 Cloud Native 的创业公司不断涌现，kubernetes 引领容器编排潮流，和 Service Mesh 技术（如 Linkerd 和 Istio） 的出现，Go 语言的兴起（参考另一本书 Cloud Native Go）等为我们将应用迁移到云原生架构的提供了更多的方案选择。\n简介 当前很多企业正在采用云原生应用架构，这可以帮助其IT转型，成为市场竞争中真正敏捷的力量。 O’Reilly 的报告中定义了云原生应用架构的特性，如微服务和十二因素应用程序。\n本书中作者Matt Stine还探究了将传统的单体应用和面向服务架构（SOA）应用迁移到云原生架构所需的文化、组织和技术变革。本书中还有一个迁移手册，其中包含将单体应用程序分解为微服务，实施容错模式和执行云原生服务的自动测试的方法。\n本书中讨论的应用架构包括：\n 十二因素应用程序：云原生应用架构模式的集合 微服务：独立部署的服务，每个服务只做一件事情 自助服务的敏捷基础设施：快速，可重复和一致地提供应用环境和后台服务的平台 基于API的协作：发布和版本化的API，允许在云原生应用架构中的服务之间进行交互 抗压性：根据压力变强的系统  关于作者 Matt Stine，Pivotal的技术产品经理，拥有15年企业IT和众多业务领域的经验。Matt 强调精益/敏捷方法、DevOps、架构模式和编程范例，他正在探究使用技术组合帮助企业IT部门能够像初创公司一样工作。\n","date":1499731200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"486df942c684c0eed9030caa9246ec19","permalink":"https://jimmysong.io/docs/book/migrating-to-cloud-native-application-architectures/","publishdate":"2017-07-11T00:00:00Z","relpermalink":"/docs/book/migrating-to-cloud-native-application-architectures/","section":"book","summary":"第一本介绍云原生的出版物","tags":["ebook","translation"],"title":"迁移到云原生应用架构","type":"publication"},{"authors":null,"categories":null,"content":"认证及培训 随着云原生生态的不断发展壮大，业界缺乏相应的人才储备及知识积累，各种认证及培训则如雨后春笋般出现。\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"3164ebfa52c4544d747b1c96a39de4d3","permalink":"https://jimmysong.io/docs/cloud-native/intro/certification/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/docs/cloud-native/intro/certification/","section":"cloud-native","summary":"认证及培训 随着云原生生态的不断发展壮大，业界缺乏相应的人才储","tags":null,"title":"","type":"cloud-native"},{"authors":null,"categories":null,"content":"速率限制的统计 无论是使用全局还是局部的速率限制，Envoy 都会发出下表中描述的指标。我们可以在配置过滤器时使用 stat_prefix 字段来设置统计信息的前缀。\n当使用局部速率限制器时，每个度量名称的前缀是 \u0026lt;stat_prefix\u0026gt;.http_local_rate_limit.\u0026lt;metric_name\u0026gt;，当使用全局速率限制器时，前缀是 cluster.\u0026lt;route_target_cluster\u0026gt;.ratelimit.\u0026lt;metric_name\u0026gt;。\n   速率限制器 指标名称 描述     局部 enabled 速率限制器被调用的请求总数   局部 / 全局 ok 来自令牌桶的低于限制的响应总数   局部 rate_limited 没有可用令牌的答复总数（但不一定强制执行）   局部 enforced 有速率限制的请求总数（例如，返回 HTTP 429）。   全局 over_limit 速率限制服务的超限答复总数   全局 error 与速率限制服务联系的错误总数   全局 failure_mode_allowed 属于错误但由于 failure_mode_deny 设置而被允许的请求总数    ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"ee5f05a328dbe72eb78961199616dc1a","permalink":"https://jimmysong.io/docs/envoy-handbook/hcm/rate-limiting-stats/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/docs/envoy-handbook/hcm/rate-limiting-stats/","section":"envoy-handbook","summary":"速率限制的统计 无论是使用全局还是局部的速率限制，Envoy 都","tags":null,"title":"","type":"envoy-handbook"},{"authors":null,"categories":null,"content":"什么是 Istio? ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"7b0c84618afbfe86a73ad1cbd17325d9","permalink":"https://jimmysong.io/docs/istio-handbook/concepts/what-is-istio/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/docs/istio-handbook/concepts/what-is-istio/","section":"istio-handbook","summary":"什么是 Istio?","tags":null,"title":"","type":"istio-handbook"},{"authors":null,"categories":null,"content":"Istio sidecar proxy 配置 假如您使用 kubernetes-vagrant-centos-cluster 部署了 Kubernetes 集群并开启了 Istio，再部署 bookinfo 示例，那么在 default 命名空间下有一个名字类似于 ratings-v1-7c9949d479-dwkr4 的 Pod，使用下面的命令查看该 Pod 的 Envoy sidecar 的全量配置：\nkubectl -n default exec ratings-v1-7c9949d479-dwkr4 -c istio-proxy curl http://localhost:15000/config_dump \u0026gt; dump-rating.json 将 Envoy 的运行时配置 dump 出来之后你将看到一个长 6000 余行的配置文件。\n下图展示的是 Envoy 的配置。\n   Istio 会在为 Service Mesh 中的每个 Pod 注入 Sidecar 的时候同时为 Envoy 注入 Bootstrap 配置，其余的配置是通过 Pilot 下发的，注意整个数据平面即 Service Mesh 中的 Envoy 的动态配置应该是相同的。您也可以使用上面的命令检查其他 sidecar 的 Envoy 配置是否跟最上面的那个相同。\n使用下面的命令检查 Service Mesh 中的所有有 Sidecar 注入的 Pod 中的 proxy 配置是否同步。\n$ istioctl proxy-status PROXY CDS LDS EDS RDS PILOT VERSION details-v1-876bf485f-sx7df.default SYNCED SYNCED SYNCED (100%) SYNCED istio-pilot-5bf6d97f79-6lz4x 1.0.0 ... istioctl 这个命令行工具就像 kubectl 一样有很多神器的魔法，通过它可以高效的管理 Istio 和 debug。\n参考  kubernetes-vagrant-centos-cluster - github.com  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"524aef55e25bd4393201361cd43af228","permalink":"https://jimmysong.io/docs/istio-handbook/data-plane/istio-sidecar-proxy-config/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/docs/istio-handbook/data-plane/istio-sidecar-proxy-config/","section":"istio-handbook","summary":"Istio sidecar proxy 配置 假如您使用 kubernetes-vagrant-centos-cluster 部署了 Kubernetes 集群并开启了 Istio，再部署","tags":null,"title":"","type":"istio-handbook"},{"authors":null,"categories":null,"content":"Kubernetes Hardening Guidance National Security Agency Cybersecurity and Infrastructure Security Agency\nCybersecurity Technical Report\nNotices and history Document change history\nAugust 2021 1.0 Initial release\nDisclaimer of warranties and endorsement\nThe information and opinions contained in this document are provided “as is” and without any warranties or guarantees. Reference herein to any specific commercial products, process, or service by trade name, trademark, manufacturer, or otherwise, does not necessarily constitute or imply its endorsement, recommendation, or favoring by the United States Government, and this guidance shall not be used for advertising or product endorsement purposes.\nTrademark recognition\nKubernetes is a registered trademark of The Linux Foundation. ▪ SELinux is a registered trademark of the National Security Agency. ▪ AppArmor is a registered trademark of SUSE LLC. ▪ Windows and Hyper-V are registered trademarks of Microsoft Corporation. ▪ ETCD is a registered trademark of CoreOS, Inc. ▪ Syslog-ng is a registered trademark of One Identity Software International Designated Activity Company. ▪ Prometheus is a registered trademark of The Linux Foundation. ▪ Grafana is a registered trademark of Raintank, Inc. dba Grafana Labs ▪ Elasticsearch and ELK Stack are registered trademarks of Elasticsearch B.V.\nCopyright recognition\nInformation, examples, and figures in this document are based on Kubernetes Documentation by The Kubernetes Authors, published under a Creative Commons Attribution 4.0 license.\nPublication information Author(s)\nNational Security Agency (NSA) Cybersecurity Directorate Endpoint Security\nCybersecurity and Infrastructure Security Agency (CISA)\nContact information\nClient Requirements / General Cybersecurity Inquiries: Cybersecurity Requirements Center, 410-854-4200, Cybersecurity_Requests@nsa.gov\nMedia inquiries / Press Desk: Media Relations, 443-634-0721, MediaRelations@nsa.gov\nFor incident response resources, contact CISA at CISAServiceDesk@cisa.dhs.gov.\nPurpose\nNSA and CISA developed this document in furtherance of their respective cybersecurity missions, including their responsibilities to develop and issue cybersecurity specifications and mitigations. This information may be shared broadly to reach all appropriate stakeholders.\nExecutive summary Kubernetes® is an open-source system that automates the deployment, scaling, and management of applications run in containers, and is often hosted in a cloud environment. Using this type of virtualized infrastructure can provide several flexibility and security benefits compared to traditional, monolithic software platforms. However, securely managing everything from microservices to the underlying infrastructure introduces other complexities. The hardening guidance detailed in this report is designed to help organizations handle associated risks and enjoy the benefits of using this technology.\nThree common sources of compromise in Kubernetes are supply chain risks, malicious threat actors, and insider threats.\nSupply chain risks are often challenging to mitigate and can arise in the container build cycle or infrastructure acquisition. Malicious threat actors can exploit vulnerabilities and misconfigurations in components of the Kubernetes architecture, such as the control plane, worker nodes, or containerized applications. Insider threats can be administrators, users, or cloud service providers. Insiders with special access to an organization’s Kubernetes infrastructure may be able to abuse these privileges.\nThis guidance describes the security challenges associated with setting up and securing a Kubernetes cluster. It includes hardening strategies to avoid common misconfigurations and guide system administrators and developers of National Security Systems on how to deploy Kubernetes with example configurations for the recommended hardening measures and mitigations. This guidance details the following mitigations:\n Scan containers and Pods for vulnerabilities or misconfigurations. Sun containers and Pods with the least privileges possible. Use network separation to control the amount of damage a compromise can cause. Use firewalls to limit unneeded network connectivity and encryption to protect confidentiality. Use strong authentication and authorization to limit user and administrator access as well as to limit the attack surface. Use log auditing so that administrators can monitor activity and be alerted to potential malicious activity. Periodically review all Kubernetes settings and use vulnerability scans to help ensure risks are appropriately accounted for and security patches are applied.  For additional security hardening guidance, see the Center for Internet Security Kubernetes benchmarks, the Docker and Kubernetes Security Technical Implementation Guides, the Cybersecurity and Infrastructure Security Agency (CISA) analysis report, and Kubernetes documentation [1], [2], [3], [6].\nIntroduction Kubernetes, …","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1652965920,"objectID":"0eff61ecdb5db16ac492ae8211e11b40","permalink":"https://jimmysong.io/docs/kubernetes-hardening-guidance/kubernetes-hardening-guidance-english/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/docs/kubernetes-hardening-guidance/kubernetes-hardening-guidance-english/","section":"kubernetes-hardening-guidance","summary":"Kubernetes Hardening Guidance National Security Agency Cybersecurity and Infrastructure Security Agency\nCybersecurity Technical Report\nNotices and history Document change history\nAugust 2021 1.0 Initial release\nDisclaimer of warranties and endorsement","tags":null,"title":"","type":"kubernetes-hardening-guidance"}]