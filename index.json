[{"content":"注意\n 本书正在编写中。  ","relpermalink":"/hugo-in-action/","summary":"静态网站","title":"Hugo 实战"},{"content":"注意\n 《Cilium 中文指南》除特殊说明，默认基于 Cilium 1.11 稳定版本。  《Cilium 中文指南》当前内容译自 Cilium 官方文档，节选了以下章节：\n 概念：描述了 Cilium 的组件以及部署 Cilium 的不同模式。 提供高层次的 运行一个完整的 Cilium 部署并理解其行为所需的高层次理解。 开始：快速开始使用 Cilium。 策略：详细介绍了策略语言结构和支持的格式。 内部原理：介绍了一些组件的内部细节。  其他未翻译部分主要涉及命令、API 及运维，本指南未来会加入笔者个人观点及其他内容。\n大纲   Cilium 和 Hubble 简介\n  多集群（集群网格）\n  Cilium 概念\n  网络\n  网络安全\n  eBPF 数据路径\n  网络策略\n  Kubernetes 集成\n   开始阅读   ","relpermalink":"/cilium-handbook/","summary":"Cilium 中文指南","title":"Cilium 中文指南"},{"content":"《什么是 eBPF —— 新一代网络、安全和可观测性工具介绍》译自 O’Reilly 发布的报告 “What is eBPF”，作者是 Liz Rice，由 JImmy Song 翻译，英文原版可以在 O’Reilly 网站上获取。\n  《什么是 eBPF》中文版封面  译者序 最近两年来关于 eBPF 的讨论在云原生社区里越来越多，尤其是当谈到 Cilium 的商业化，使用 eBPF 来优化 Istio 服务网格，甚至扬言干掉 Sidecar 时，eBPF 更是赚足了眼球。\n这本报告是由基于 Cilium 的创业公司 Isovalent 的 Liz Rice 撰写，由 O’Reilly 发布，相信可以为你揭开 eBPF 技术的神秘面纱，带你了解什么是 eBPF 还有它的强大之处。更重要的是它在云原生环境中，在服务网格、可观测性和安全中的应用。\n关于作者 Liz Rice 是云原生网络和安全专家，Isovalent 的首席开源官，是基于 eBPF 的 Cilium网络项目的创建者。她在 2019-2022 年担任 CNCF 的技术监督委员会（TOC）主席，并在 2018 …","relpermalink":"/what-is-ebpf/","summary":"新一代网络、安全和可观测性工具简介。","title":"什么是 eBPF？"},{"content":"关于本教程\n 本教程迁移自《Kubernetes 中文指南——云原生应用架构实战手册》，原手册使用 Gitbook 发布，内容涵盖 容器、Kubernetes、服务网格、Serverless 等云元生的多个领域，因内容过于宽泛，且 Gitbook 项目已停止维护，现将其中的 Kubernetes 教程部分独立成书，并使用 Hugo 重新构建。  云原生是一种行为方式和设计理念，究其本质，凡是能够提高云上资源利用率和应用交付效率的行为或方式都是云原生的。云计算的发展史就是一部云原生化的历史。Kubernetes 开启了云原生的序幕，服务网格 Istio 的出现，引领了后 Kubernetes 时代的微服务，Serverless 的兴起，使得云原生从基础设施层不断向应用架构层挺进，我们正处于一个云原生的新时代。\n  《Kubernetes 基础教程》封面  Kubernetes 是 Google 于 2014 年 6 月基于其内部使用的 Borg 系统开源出来的容器编排调度引擎，Google 将其作为初始和核心项目贡献给 CNCF（云原生计算基金会），近年来逐渐发展出了云原生生态。 …","relpermalink":"/kubernetes-handbook/","summary":"云原生应用架构实战手册","title":"Kubernetes 基础教程"},{"content":"Kubernetes Hardening Guidance（查看英文原版 PDF） 是由美国国家安全局（NSA）于 2021 年 8 月发布的，其中文版《Kubernetes 加固指南》（或译作《Kubernetes 强化指南》），译者 Jimmy Song。\n  《Kubernetes加固指南》封面  许可证 您可以使用署名 - 非商业性使用 - 相同方式共享 4.0 (CC BY-NC-SA 4.0) 协议共享。\n交流群 欢迎加入云原生社区可观测性讨论组（微信群）参与讨论交流，加入前请先填写入群申请问卷后联系 Jimmy Song 入群。\n 开始阅读   ","relpermalink":"/kubernetes-hardening-guidance/","summary":"本指南译自美国国家安全局（NSA）于 2021 年 8 月发布的的 Kubernetes Hardening Guidance。","title":"Kubernetes 加固指南"},{"content":"注意\n 本书内容基于 Envoy 1.20 版本，部分命令在更新版本中可能遇到兼容性问题。  本手册梳理了 Envoy 基础知识，适用于初学者，帮你快速掌握 Envoy 代理。\n  《Envoy基础教程》封面  关于本书 Envoy 是一个开源的边缘和服务代理，专为云原生应用而设计。Envoy 与每个应用程序一起运行，通过提供网络相关的功能，如重试、超时、流量路由和镜像、TLS 终止等，以一种平台无关的方式抽象出网络。由于所有的网络流量都流经 Envoy 代理，因此很容易观察到流量和问题区域，调整性能，并准确定位延迟来源。\n本书为 Tetrate 出品的《Envoy 基础教程》的教程及实验内容，其他测试及考核证书的获取，请访问 Tetrate 学院。\n本书大纲   Envoy 简介\n  HTTP 连接管理器\n  集群\n  动态配置\n  监听器子系统\n  日志\n  管理接口\n  扩展 Envoy\n  关于作者 宋净超（Jimmy Song），CNCF Ambassador，云原生社区创始人，个人网站 jimmysong.io。\n许可证 您可以使用署名 - 非商业性使用 - …","relpermalink":"/envoy-handbook/","summary":"本手册梳理了 Envoy 基础知识，适用于初学者，帮你快速掌握 Envoy 代理。","title":"Envoy 基础教程"},{"content":"在 Envoy 介绍章节中，你将了解 Envoy 的概况，并通过一个例子了解 Envoy 的构建模块。在本章结束时，你将通过运行 Envoy 的示例配置来了解 Envoy。\n本章大纲   什么是 Envoy？\n  Envoy 的构建模块\n   阅读本章   ","relpermalink":"/envoy-handbook/intro/","summary":"在 Envoy 介绍章节中，你将了解 Envoy 的概况，并通过一个例子了解 Envoy 的构建模块。在本章结束时，你将通过运行 Envoy 的示例配置来了解 Envoy。 本章大纲 什么是 Envoy？ Envoy 的构建模块 阅读本章","title":"Envoy 简介"},{"content":"注意\n 本教程基于 Istio 1.11 或更新版本。  Istio 是由 Google、IBM、Lyft 等共同开源的 Service Mesh（服务网格）框架，于2017 年开源。\n  《Istio 基础教程》封面  关于本书 本书的主题包括：\n 服务网格概念解析 控制平面和数据平面的原理 Istio 架构详解 基于 Istio 的自定义扩展 迁移到 Istio 服务网格 构建云原生应用网络  书中部分内容来自 Tetrate 出品的 Istio 基础教程，请访问 Tetrate 学院，解锁全部教程及测试，获得 Tetrate 认证的 Istio 认证。\n本书大纲   服务网格概述\n  概念原理\n  数据平面\n  安装\n  流量管理\n  流量管理配置\n  可观测性\n  安全\n  安全配置\n  扩展 Istio\n  高级功能\n  问题排查\n  Istio 生态\n  实战案例\n  关于作者 宋净超（Jimmy Song），CNCF Ambassador，云原生社区创始人，个人网站 jimmysong.io。\n许可证 您可以使用署名 - 非商业性使用 - 相同方式共享 4.0 (CC …","relpermalink":"/istio-handbook/","summary":"云原生应用网络构建指南。","title":"Istio 基础教程"},{"content":" 软件正在吞噬世界。\n——Mark Andreessen\n 近些年来，在一些长期由领导者支配的行业中，这些领导者的领先地位已经岌岌可危，这都是由以这些行业为核心业务的软件公司造成的。像Square、Uber、Netflix、Airbnb和特斯拉这样的公司能够持续快速增长，并且拥有傲人的市场估值，成为它们所在行业的新领导者。这些创新公司有什么共同点？\n  快速创新\n  持续可用的服务\n  弹性可扩展的Web\n  以移动为核心的用户体验\n  将软件迁移到云上是一种自演化，使用了云原生应用架构是这些公司能够如此具有破坏性的核心原因。对于云，我们指的是一个任何能够按需、自助弹性提供和释放计算、网络和存储资源的计算环境。云的定义包括公有云（例如 Amazon Web Services、Google Cloud和Microsoft Azure）和私有云（例如 VMware vSphere和 OpenStack）。\n本章中我们将探讨云原生应用架构的创新性，然后验证云原生应用架构的主要特性。\n 开始阅读   ","relpermalink":"/migrating-to-cloud-native-application-architectures/the-rise-of-cloud-native/","summary":"软件正在吞噬世界。 ——Mark Andreessen 近些年来，在一些长期由领导者支配的行业中，这些领导者的领先地位已经岌岌可危，这都是由以这些行业为核心业务的软件公司造成的。像Square、Uber、Netflix、Ai","title":"第一章：云原生的崛起"},{"content":"Google 有许多通用工程实践，几乎涵盖所有语言和项目。此文档为长期积累的最佳实践，是集体经验的结晶。我们尽可能地将其公之于众，您的组织和开源项目也会从中受益。\n  《谷歌工程实践》封面  当前包含以下文档：\nGoogle 代码审查指南，实则两套指南：\n 代码审查者指南 代码开发者指南  译者序 此仓库翻译自 google/eng-practices，目前为止的主要内容为 Google 总结的如何进行 Code Review（代码审查） 指南，根据原 Github 仓库的标题判断以后会追加更多 Google 工程实践的内容。\n许可证 您可以使用署名 - 非商业性使用 - 相同方式共享 4.0 (CC BY-NC-SA 4.0) 协议共享。\n交流群 欢迎加入云原生社区微信讨论群，加入前请先填写入群申请问卷后联系 Jimmy Song 入群。\n 开始阅读   ","relpermalink":"/eng-practices/","summary":"谷歌代码审查实践指南","title":"谷歌工程实践"},{"content":"本书译自美国国家标准标准与技术研究院（NIST）Special Publication 800-204C。\n  《利用服务网格为基于微服务的应用程序实施 DevSecOps》封面  本书大纲   声明\n  执行摘要\n  第一章：简介\n  第二章：实施 DevSecOps 原语的参考平台\n  第三章：DevSecOps 组织准备、关键基本要素和实施\n  第四章：为参考平台实施 DevSecOps 原语\n  第五章：摘要和结论\n  关于本书 作者：Ramaswamy Chandramouli\n计算机安全司信息技术实验室\n美国商务部\nGina M. Raimondo，秘书\n国家标准和技术研究所\nJames K. Olthoff，履行负责标准和技术的商务部副部长兼国家标准和技术研究所所长的非专属职能和职责\n本出版物可在：https://doi.org/10.6028/NIST.SP.800-204C 免费获取。\n 开始阅读   ","relpermalink":"/service-mesh-devsecops/","summary":"本书译自美国国家标准标准与技术研究院（NIST）Special Publication 800-204C。","title":"利用服务网格为基于微服务的应用程序实施 DevSecOps"},{"content":"本书是 Migrating to Cloud Native Application Architectures 的中文版，本书英文版发布于 2015 年 2 月，中文版由 Jimmy Song 翻译，发布于 2017 年 7 月。\n  《迁移到云原生应用架构》 图书封面  译者序 云时代的云原生应用大势已来，将传统的单体架构应用迁移到云原生架构，你准备好了吗？\n俗话说“意识决定行动”，在迁移到云原生应用之前，我们大家需要先对 Cloud Native（云原生）的概念、组织形式并对实现它的技术有一个大概的了解，这样才能指导我们的云原生架构实践。\n英文版作于2015年，其中的示例主要针对 Java 应用，实际上也适用于任何应用类型，云原生应用架构适用于异构语言的程序开发，不仅仅是针对 Java 语言的程序开发。截止到本人翻译本书时，云原生应用生态系统已经初具规模，CNCF 成员不断发展壮大，基于 Cloud Native 的创业公司不断涌现，Kubernetes 引领容器编排潮流，和 Service Mesh 技术（如 Linkerd 和 Istio） 的出现，Go 语言的兴起等为我们将应 …","relpermalink":"/migrating-to-cloud-native-application-architectures/","summary":"本书是 Migrating to Cloud Native Application Architectures 的中文版。","title":"迁移到云原生应用架构"},{"content":"本书为 Cloud Native Infrastructure 中文版，作者 Justin Garrison 和 Kris Nova，英文版发行于 2017 年 11 月，已可以在网上免费获得，本书是关于创建和管理基础架构，以适用于云原生应用全生命周期管理的模式和实践。\n  《云原生基础架构》封面  阅读完这本书后，您将会有如下收获：\n 理解为什么说云原生基础架构是高效运行云原生应用所必须的 根据准则来决定您的业务何时以及是否应该采用云原生 了解部署和管理基础架构和应用程序的模式 设计测试以证明您的基础架构可以按预期工作，即使在各种边缘情况下也是如此 了解如何以策略即代码的方式保护基础架构  本书大纲   前言\n  介绍\n  第 1 章：什么是云原生基础架构？\n  第 2 章：采纳云原生基础架构的时机\n  第 3 章：云原生部署的演变\n  第 4 章：设计基础架构应用程序\n  第 5 章：开发基础架构应用程序\n  第 6 章：测试云原生基础架构\n  第 7 章：管理云原生应用程序\n  第 8 章：保护应用程序\n  第 9 章：实施云原生基础架构\n  附录 A： …","relpermalink":"/cloud-native-infra/","summary":"《云原生基础架构》，Cloud Native Infrastructure 中文版。","title":"云原生基础架构"},{"content":"本报告译自 O’Reilly 出品的 The Future of Observablity with OpeTelemetry，作者 Ted Young，译者 Jimmy Song。\n  《OpenTelemetry 可观测性的未来》封面  关于本书 本书内容包括：\n OpenTelemetry 如何满足库作者、应用程序拥有者、运维和响应者的需求 应用程序的不同角色如何围绕 OpenTelemetry 来协同和独立工作 关于在组织中采用和管理 OpenTelemetry 的实用建议  关于作者 Ted Young 是 OpenTelemetry 项目的联合创始人之一。在过去的二十年里，他设计并建立了各种大规模的分布式系统，包括可视化 FX 管道和容器调度系统。他目前在 Lightstep 公司担任开发者教育总监，住在俄勒冈州波特兰的一个小农场里。\n许可证 您可以使用署名 - 非商业性使用 - 相同方式共享 4.0 (CC BY-NC-SA 4.0) 协议共享。\n交流群 欢迎加入云原生社区可观测性讨论组（微信群）参与讨论交流，加入前请先填写入群申请问卷后联系 Jimmy Song 入群。 …","relpermalink":"/opentelemetry-obervability/","summary":"本报告译自 O'Reilly 出品的 The Future of Observablity with OpeTelemetry，作者 Ted Young，译者 Jimmy Song。","title":"OpenTelemetry 可观测性的未来"},{"content":"欢迎来到云原生导览——一站式了解云原生技术体系。\n导览图   关于云原生\n  Kubernetes\n  服务网格\n  社区\n   开始阅读   ","relpermalink":"/cloud-native-handbook/","summary":"云原生技术体系导览","title":"云原生导览"},{"content":"本章将介绍什么是云原生、云原生应用。\n本章大纲   什么是云原生？\n  云原生的设计哲学\n  什么是云原生应用？\n  云原生快速入门\n   阅读本章   ","relpermalink":"/cloud-native-handbook/intro/","summary":"本章介绍什么是云原生。","title":"关于云原生"},{"content":"简介 代码审查是除了代码作者之外，其他人检查代码的过程。\nGoogle 通过 Code Review 来维护代码和产品质量。\n此文档是 Google Code Review 流程和政策的规范说明。\n此页面是我们进行 Code Review 流程的概述。本指南还有另外两套文档：\n 如何进行 Code Review：针对代码审查者的详细指南。 代码开发者指南：针对 CL 开发者的的详细指南。  代码审查者应该关注哪些方面？ 代码审查时应该关注以下方面：\n 设计：代码是否经过精心设计并适合您的系统？ 功能：代码的行为是否与作者的意图相同？代码是否可以正常响应用户的行为？ 复杂度：代码能更简单吗？将来其他开发人员能轻松理解并使用此代码吗？ 测试：代码是否具有正确且设计良好的自动化测试？ 命名：开发人员是否为变量、类、方法等选择了明确的名称？ 注释：注释是否清晰有用？ 风格：代码是否遵守了风格指南？ 文档：开发人员是否同时更新了相关文档？  参阅 如何进行 Code Review 获取更多资料。\n选择最合适审查者 一般而言，您希望找到能在合理的时间内回复您的评论的最合适的审查者。\n最合适的审查者 …","relpermalink":"/eng-practices/review/","summary":"简介 代码审查是除了代码作者之外，其他人检查代码的过程。 Google 通过 Code Review 来维护代码和产品质量。 此文档是 Google Code Review 流程和政策的规范说明。 此页面是我们进行 Code Review 流程的概述。本指南还有另外两套文档： 如何进行 Code Review","title":"代码审查指南"},{"content":"在 HCM 章节中，我们将对 HTTP 连接管理器过滤器进行扩展。我们将学习过滤器的排序，以及 HTTP 路由和匹配如何工作。我们将向你展示如何分割流量、操作 Header 信息、配置超时、实现重试、请求镜像和速率限制。\n在本章结束时，你将对 HCM 过滤器有一个很好的理解，以及如何路由和拆分 HTTP 流量，操纵 Header 等等。\n本章大纲   HCM 简介\n  HTTP 路由\n  请求匹配\n  流量分割\n  Header 操作\n  修改响应\n  生成请求 ID\n  超时\n  重试\n  请求镜像\n  速率限制\n  速率限制的统计\n  局部速率限制\n  全局速率限制\n  实验1：请求匹配\n  实验2：流量分割\n  实验3：Header操作\n  实验4：重试\n  实验5：局部速率限制\n  实验6：全局速率限制\n   阅读本章   ","relpermalink":"/envoy-handbook/hcm/","summary":"在 HCM 章节中，我们将对 HTTP 连接管理器过滤器进行扩展。我们将学习过滤器的排序，以及 HTTP 路由和匹配如何工作。我们将向你展示如何分割流量、操作 Header 信息、配置超时、实现重试、请求镜像和速率限制。 在本章结束时，你将对 HCM","title":"HTTP 连接管理器"},{"content":"本章将介绍什么是云原生、云原生应用。\n本章大纲   什么是 Kubernetes?\n  Kubernetes 的历史\n  Sidecar 模式\n   阅读本章   ","relpermalink":"/cloud-native-handbook/kubernetes/","summary":"关于Kubernetes。","title":"Kubernetes"},{"content":"本章将介绍服务网格。\n 阅读本章   ","relpermalink":"/cloud-native-handbook/service-mesh/","summary":"本章介绍服务网格。","title":"服务网格"},{"content":"概念一章对 Cilium 和 Hubble 的所有方面进行了更深入的介绍。如果你想了解 Cilium 和 Hubble 的概要介绍，请参阅 Cilium 和 Hubble 简介。\n本章大纲   组件概览\n  Cilium 术语说明\n  可观测性\n   阅读本章   ","relpermalink":"/cilium-handbook/concepts/","summary":"概念一章对 Cilium 和 Hubble 的所有方面进行了更深入的介绍。如果你想了解 Cilium 和 Hubble 的概要介绍，请参阅 Cilium 和 Hubble 简介。 本章大纲 组件概览 Cilium 术语说明 可观测性 阅读本章","title":"Cilium 概念"},{"content":"Kubernetes 最初源于谷歌内部的 Borg，提供了面向应用的容器集群部署和管理系统。Kubernetes 的目标旨在消除编排物理 / 虚拟计算，网络和存储基础设施的负担，并使应用程序运营商和开发人员完全将重点放在以容器为中心的原语上进行自助运营。Kubernetes 也提供稳定、兼容的基础（平台），用于构建定制化的 workflows 和更高级的自动化任务。\nKubernetes 具备完善的集群管理能力，包括多层次的安全防护和准入机制、多租户应用支撑能力、透明的服务注册和服务发现机制、内建负载均衡器、故障发现和自我修复能力、服务滚动升级和在线扩容、可扩展的资源自动调度机制、多粒度的资源配额管理能力。Kubernetes 还提供完善的管理工具，涵盖开发、部署测试、运维监控等各个环节。\nBorg 简介 Borg 是谷歌内部的大规模集群管理系统，负责对谷歌内部很多核心服务的调度和管理。Borg 的目的是让用户能够不必操心资源管理的问题，让他们专注于自己的核心业务，并且做到跨多个数据中心的资源利用率最大化。\nBorg 主要由 BorgMaster、Borglet、borgcfg …","relpermalink":"/kubernetes-handbook/architecture/","summary":"Kubernetes 最初源于谷歌内部的 Borg，提供了面向应用的容器集群部署和管理系统。Kubernetes 的目标旨在消除编排物理 / 虚拟计算，网络和存储基础设施的负担，并使应用程序运营商和开发人员完全将重点放在以容器为","title":"Kubernetes 架构"},{"content":" 从客户给我们下达订单开始，一直到我们收到现金为止，我们一直都关注时间线。而且我们正在通过删除非附加值的废物来减少这个时间表。\n—— Taichi Ohno\n Taichi Ohno被公认为精益制造之父。虽然精益制造的实践无法完全适用于软件开发领域，但它们的原则是一致的。 这些原则可以指导我们很好地寻求典型的企业IT组织采用云原生应用架构所需的变革，并且接受作为这一转变所带来的部分的文化和组织转型。\n 开始阅读   ","relpermalink":"/migrating-to-cloud-native-application-architectures/changes-needed/","summary":"从客户给我们下达订单开始，一直到我们收到现金为止，我们一直都关注时间线。而且我们正在通过删除非附加值的废物来减少这个时间表。 —— Taichi Ohno Taichi Ohno被公认为精益制造之父。虽然精益制造的实践无法完全适用于软件","title":"第二章：在变革中前行"},{"content":"现在我们已经定义了云原生应用架构，并简要介绍了企业在采用它们时必须考虑做出的变化，现在是深入研究技术细节的时候了。对每个技术细节的深入讲解已经超出了本报告的范围。本章中仅是对采用云原生应用架构后，需要做的特定工作和采用的模式的一系列简短的介绍，文中还给出了一些进一步深入了解这些方法的链接。\n 开始阅读   ","relpermalink":"/migrating-to-cloud-native-application-architectures/migration-cookbook/","summary":"现在我们已经定义了云原生应用架构，并简要介绍了企业在采用它们时必须考虑做出的变化，现在是深入研究技术细节的时候了。对每个技术细节的深入讲解已经超出了本报告的范围。本章中仅是对采用云原生应用架构后，需要","title":"第三章：迁移指南"},{"content":"云原生应用由多个松散耦合的组件（称为微服务，通常以容器形式实现）组成，在需要零信任概念的无边界网络环境中运行（企业内部或云），并由来自不同地点的用户访问（例如，校园、家庭办公室等）。云原生应用不只是指在云中运行的应用。它们还指具有设计和运行时架构的一类应用，如微服务，以及用于提供所有应用服务（包括安全）的专用基础设施。将 零信任原则 纳入这类应用提供了一些技术，其中对所有受保护资源的访问是通过基于身份的保护和基于网络的保护（如微分）来强制执行的。\n由于业务原因，云原生应用程序需要敏捷和安全的更新和部署技术，以及应对网络安全事件的必要弹性。因此，它们需要一种与传统的单层或多层应用不同的应用开发、部署和运行时监控范式（统称为软件生命周期范式）。DevSecOps（开发、安全和运维）是这类应用的促进范式，因为它通过（a）持续集成、持续交付 / 持续部署（CI/CD）管道（在第 3 节中解释）等基本要素促进了敏捷和安全的开发、交付、部署和运维；（b）整个生命周期的安全测试；以及（c）运行时的持续监控，所有这些都由自动化工具支持。事实上，满足上述目标的范式最初被赋予了 DevOps 这个术语，以 …","relpermalink":"/service-mesh-devsecops/intro/","summary":"云原生应用由多个松散耦合的组件（称为微服务，通常以容器形式实现）组成，在需要零信任概念的无边界网络环境中运行（企业内部或云），并由来自不同地点的用户访问（例如，校园、家庭办公室等）。云原生应用不只是指","title":"第一章：简介"},{"content":"在本章中，我们将学习集群以及如何管理它们。在 Envoy 的集群配置部分，我们可以配置一些功能，如负载均衡、健康检查、连接池、异常点检测等。\n在本章结束时，你将了解集群和端点是如何工作的，以及如何配置负载均衡策略、异常点检测和断路。\n本章大纲   服务发现\n  主动健康检查\n  异常点检测\n  断路器\n  负载均衡\n  实验7：断路器\n   阅读本章   ","relpermalink":"/envoy-handbook/cluster/","summary":"在本章中，我们将学习集群以及如何管理它们。在 Envoy 的集群配置部分，我们可以配置一些功能，如负载均衡、健康检查、连接池、异常点检测等。 在本章结束时，你将了解集群和端点是如何工作的，以及如何配置负载均衡策略、","title":"集群"},{"content":"本节页面的内容为开发人员进行代码审查的最佳实践。这些指南可帮助您更快地完成审核并获得更高质量的结果。您不必全部阅读它们，但它们适用于每个 Google 开发人员，并且许阅读全文通常会很有帮助。\n 写好 CL 描述 小型 CL 如何处理审查者的评论  另请参阅代码审查者指南，它为代码审阅者提供了详细的指导。\n","relpermalink":"/eng-practices/review/developer/","summary":"本节页面的内容为开发人员进行代码审查的最佳实践。这些指南可帮助您更快地完成审核并获得更高质量的结果。您不必全部阅读它们，但它们适用于每个 Google 开发人员，并且许阅读全文通常会很有帮助。 写好 CL 描述 小型 CL 如何处","title":"开发者指南"},{"content":"本节是基于过往经验编写的 Code Review 最佳方式建议。其中分为了很多独立的部分，共同组成完整的文档。虽然您不必阅读文档，但通读一遍会对您自己和团队很有帮助。\n Code Review 标准 Code Review 要点 查看 CL 的步骤 Code Review 速度 如何撰写 Code Review 评论 处理 Code Review 中的抵触  另请参阅代码开发者指南，该指南为正在进行 Code Review 的开发开发者提供详细指导。\n","relpermalink":"/eng-practices/review/reviewer/","summary":"本节是基于过往经验编写的 Code Review 最佳方式建议。其中分为了很多独立的部分，共同组成完整的文档。虽然您不必阅读文档，但通读一遍会对您自己和团队很有帮助。 Code Review 标准 Code Review 要点 查看 CL 的步骤 Code Review 速度 如何撰写 Code Review 评论 处理","title":"审查者指南"},{"content":"本章大纲   路由\n  IP 地址管理（IPAM）\n  IP 地址伪装\n  IPv4 分片处理\n   阅读本章   ","relpermalink":"/cilium-handbook/networking/","summary":"本章大纲 路由 IP 地址管理（IPAM） IP 地址伪装 IPv4 分片处理 阅读本章","title":"网络"},{"content":"本章大纲   介绍\n  基于身份\n  策略执行\n  代理注入\n   阅读本章   ","relpermalink":"/cilium-handbook/security/","summary":"本章大纲 介绍 基于身份 策略执行 代理注入 阅读本章","title":"网络安全"},{"content":"如第 1.1 节所述，参考平台是一个容器编排和管理平台。在现代应用环境中，平台是物理（裸机）或虚拟化（如虚拟机、容器）基础设施上的一个抽象层。在实施 DevSecOps 原语之前，平台只是包含了应用代码，其中包含了应用逻辑和服务网状代码，而服务网状代码又提供应用服务。本节将考虑以下内容：\n 一个容器编排和资源管理平台，容纳了应用程序代码和大部分的服务网格代码 服务网格的软件架构  本章大纲   2.1 容器编排和资源管理平台\n  2.2 服务网格架构\n   开始阅读   ","relpermalink":"/service-mesh-devsecops/reference-platform/","summary":"如第 1.1 节所述，参考平台是一个容器编排和管理平台。在现代应用环境中，平台是物理（裸机）或虚拟化（如虚拟机、容器）基础设施上的一个抽象层。在实施 DevSecOps 原语之前，平台只是包含了应用代码，其中包含了应用逻辑和服务","title":"第二章：实施 DevSecOps 原语的参考平台"},{"content":"在本章中，我们将学习如何使用动态配置来配置 Envoy 代理。到现在为止，我们一直在使用静态配置。本章将教我们如何在运行时从文件系统或通过网络使用发现服务为单个资源提供配置。\n在本章结束时，你将了解静态和动态配置之间的区别。\n本章大纲   动态配置简介\n  来自文件系统的动态配置\n  来自控制平面的动态配置\n  实验 8：来自文件系统的动态配置\n   阅读本章   ","relpermalink":"/envoy-handbook/dynamic-config/","summary":"在本章中，我们将学习如何使用动态配置来配置 Envoy 代理。到现在为止，我们一直在使用静态配置。本章将教我们如何在运行时从文件系统或通过网络使用发现服务为单个资源提供配置。 在本章结束时，你将了解静态和动态配置之","title":"动态配置"},{"content":"本章将介绍什么是云原生、云原生应用。\n 阅读本章   ","relpermalink":"/cloud-native-handbook/community/","summary":"社区","title":"社区"},{"content":"本章大纲   介绍\n  数据包流程\n  eBPF Map\n  Iptables 用法\n   阅读本章   ","relpermalink":"/cilium-handbook/ebpf/","summary":"本章大纲 介绍 数据包流程 eBPF Map Iptables 用法 阅读本章","title":"eBPF 数据路径"},{"content":"DevSecOps 在早期就将安全纳入了软件工程流程。它将安全流程和工具集成到 DevOps 的所有开发工作流程（或后面解释的管道）中，并使之自动化，从而实现无缝和连续。换句话说，它可以被看作是三个过程的组合。开发 + 安全 + 运维。\n本节讨论了 DevSecOps 的以下方面：\n 组织对 DevSecOps 的准备情况 开发安全运维平台 开发安全运维的基本构件或关键原语  本章大纲   3.1 组织对 DevSecOps 的准备情况\n  3.2 DevSecOps 平台\n  3.3 DevSecOps 关键原语和实施任务\n   开始阅读   ","relpermalink":"/service-mesh-devsecops/devsecops/","summary":"DevSecOps 在早期就将安全纳入了软件工程流程。它将安全流程和工具集成到 DevOps 的所有开发工作流程（或后面解释的管道）中，并使之自动化，从而实现无缝和连续。换句话说，它可以被看作是三个过程的组合。开发 + 安全 + 运维。 本节","title":"第三章：DevSecOps 组织准备、关键基本要素和实施"},{"content":"在监听器子系统章节中，我们将学习 Envoy 代理的监听器子系统。我们将介绍过滤器、过滤器链匹配、以及不同的监听器过滤器。\n在本章节结束时，你将了解监听器子系统的不同部分，并知道过滤器和过滤器链匹配是如何工作的。\n本章大纲   监听器过滤器\n  Header 操作\n  HTTP 检查器监听器过滤器\n  原始目的地监听器过滤器\n  原始源监听器过滤器\n  代理协议监听器过滤器\n  TLS 检查器监听器过滤器\n  实验 9：原始目的地过滤器\n  实验 10：TLS 检查器过滤器\n  实验 11：匹配传输和应用协议\n   阅读本章   ","relpermalink":"/envoy-handbook/listener/","summary":"在监听器子系统章节中，我们将学习 Envoy 代理的监听器子系统。我们将介绍过滤器、过滤器链匹配、以及不同的监听器过滤器。 在本章节结束时，你将了解监听器子系统的不同部分，并知道过滤器和过滤器链匹配是如何工作的。 本","title":"监听器子系统"},{"content":"Kubernetes 作为云原生应用的基础调度平台，相当于云原生的操作系统，为了便于系统的扩展，Kubernetes 中开放的以下接口，可以分别对接不同的后端，来实现自己的业务逻辑：\n 容器运行时接口（CRI）：提供计算资源 容器网络接口（CNI）：提供网络资源 容器存储接口（CSI），提供存储资源  以上三种资源相当于一个分布式操作系统的最基础的几种资源类型，而 Kuberentes 是将他们粘合在一起的纽带。\n本节大纲   容器运行时接口（CRI）\n  容器网络接口（CNI）\n  容器存储接口（CSI）\n  ","relpermalink":"/kubernetes-handbook/architecture/open-interfaces/","summary":"Kubernetes 作为云原生应用的基础调度平台，相当于云原生的操作系统，为了便于系统的扩展，Kubernetes 中开放的以下接口，可以分别对接不同的后端，来实现自己的业务逻辑： 容器运行时接口（CRI）：提供计算资源 容","title":"开放接口"},{"content":"各种 CI/CD 管道都涉及到参考平台（即基于微服务的应用，有提供基础设施服务的服务网格）。虽然参考应用是基于微服务的应用，但 DevSecOps 的原语可以应用于单体应用以及既在企业内部又基于云的应用（如混合云、单一公有云和多云）。\n在第 2.1 节中，我们提到了我们参考应用环境中的五种代码类型。我们还提到，也可以为这五种代码类型中的每一种创建单独的 CI/CD 管道。这五种代码类型在参考平台组件中的位置将被讨论，然后是描述相关 CI/CD 管道的单独章节：\n 参考平台中的代码类型和相关的 CI/CD 管道（4.1 节） 应用程序代码和应用服务代码的 CI/CD 管道（4.2 节） 基础设施即代码（IaC）的 CI/CD 管道（4.3 节） 策略即代码的 CI/CD 管道（4.4 节） 可观测性即代码的 CI/CD 管道（4.5 节）  所有 CI/CD 管道的实施问题，无论代码类型如何，都将在以下章节中讨论：\n 确保 CI/CD 管道的安全（4.6 节） CI/CD 管道中的工作流模型（4.7 节） CI/CD 管道中的安全测试（4.8 节）  本节还将考虑 DevSecOps 的 …","relpermalink":"/service-mesh-devsecops/implement/","summary":"各种 CI/CD 管道都涉及到参考平台（即基于微服务的应用，有提供基础设施服务的服务网格）。虽然参考应用是基于微服务的应用，但 DevSecOps 的原语可以应用于单体应用以及既在企业内部又基于云的应用（如混合云、单一公有云和多云）","title":"第四章：为参考平台实施 DevSecOps 原语"},{"content":"在日志章节中，我们将学习 Envoy 中不同类型的日志和方法。\n无论是作为流量网关还是作为服务网格中的 Sidecar，Envoy 都处于独特的地位，可以揭示你的网络中正在发生的事情。日志记录是了解系统状态的重要方式，无论是分析、审计还是故障排除。日志记录也有一个数量问题，有可能会泄露秘密。\n在本章结束时，你将了解 Envoy 中存在哪些日志选项，包括如何对日志进行结构化和过滤，以及它们可以被写到哪里。\n本章大纲   访问日志\n  配置访问记录器\n  访问日志过滤\n  Envoy 组件日志\n  实验 12：使用日志过滤器\n  实验 13：使用 gRPC 访问日志服务（ALS）记录日志\n  实验 14：将 Envoy 的日志发送到 Google Cloud Logging\n   阅读本章   ","relpermalink":"/envoy-handbook/logging/","summary":"在日志章节中，我们将学习 Envoy 中不同类型的日志和方法。 无论是作为流量网关还是作为服务网格中的 Sidecar，Envoy 都处于独特的地位，可以揭示你的网络中正在发生的事情。日志记录是了解系统状态的重要方式，","title":"日志"},{"content":"本文为托管云原生应用的参考平台实施 DevSecOps 原语提供全面指导。它包括对参考平台的概述，并描述了基本的 DevSecOps 原语（即 CI/CD 管道）、其构建模块、管道的设计和执行，以及自动化在 CI/CD 管道中有效执行工作流程的作用。\n参考平台的架构除了应用代码和提供应用服务的代码外还包括用于基础设施、运行时策略和持续监测应用健康状况的功能元素，可以通过具有独立 CI/CD 管道类型的声明性代码来部署。还介绍了这些代码的运行时行为、实现高安全性的好处，以及使用风险管理工具和仪表盘指标的管道内的工件来提供持续授权操作（C-ATO）。\n","relpermalink":"/service-mesh-devsecops/summary-and-conclusion/","summary":"本文为托管云原生应用的参考平台实施 DevSecOps 原语提供全面指导。它包括对参考平台的概述，并描述了基本的 DevSecOps 原语（即 CI/CD 管道）、其构建模块、管道的设计和执行，以及自动化在 CI/CD 管道中有效执行工作流程的作用。 参考平台的架构","title":"第五章：摘要和结论"},{"content":"在管理接口章节中，我们将学习 Envoy 所暴露的管理接口，以及我们可以用来检索配置和统计的不同端点，以及执行其他管理任务。\n在本章结束时，你将了解如何启用管理接口以及我们可以通过它执行的不同任务。\n本章大纲   启用管理接口\n  配置转储\n  统计\n  日志\n  集群\n  监听器和监听器的排空\n  分接式过滤器\n  健康检查\n  实验15：使用 HTTP 分接式过滤器\n   阅读本章   ","relpermalink":"/envoy-handbook/admin-interface/","summary":"在管理接口章节中，我们将学习 Envoy 所暴露的管理接口，以及我们可以用来检索配置和统计的不同端点，以及执行其他管理任务。 在本章结束时，你将了解如何启用管理接口以及我们可以通过它执行的不同任务。 本章大纲 启用管理","title":"管理接口"},{"content":"在本章节中，我们将了解扩展 Envoy 的不同方法。\n我们将更详细地介绍使用 Lua 和 Wasm 过滤器来扩展 Envoy 的功能。\n在本章结束时，你将了解扩展 Envoy 的不同方法以及如何使用 Lua 和 Wasm 过滤器。\n本章大纲   可扩展性概述\n  Lua 过滤器\n  WebAssembly（Wasm）\n  实验16：使用 Lua 脚本扩展 Envoy\n  实验17：使用 Wasm 和 Go 扩展 Envoy\n   阅读本章   ","relpermalink":"/envoy-handbook/extending-envoy/","summary":"在本章节中，我们将了解扩展 Envoy 的不同方法。 我们将更详细地介绍使用 Lua 和 Wasm 过滤器来扩展 Envoy 的功能。 在本章结束时，你将了解扩展 Envoy 的不同方法以及如何使用 Lua 和 Wasm 过滤器。 本章大纲 可扩展性概述 Lua 过滤器 WebAssemb","title":"扩展 Envoy"},{"content":"以下列举的内容都是 Kubernetes 中的对象（Object），这些对象都可以在 YAML 文件中作为一种 API 类型来配置。\n Pod Node Namespace Service Volume PersistentVolume Deployment Secret StatefulSet DaemonSet ServiceAccount ReplicationController ReplicaSet Job CronJob SecurityContext ResourceQuota LimitRange HorizontalPodAutoscaling Ingress ConfigMap Label CustomResourceDefinition Role ClusterRole  我将它们简单的分类为以下几种资源对象： …","relpermalink":"/kubernetes-handbook/objects/","summary":"以下列举的内容都是 Kubernetes 中的对象（Object），这些对象都可以在 YAML 文件中作为一种 API 类型来配置。 Pod Node Namespace Service Volume PersistentVolume Deployment Secret StatefulSet DaemonSet ServiceAccount ReplicationController ReplicaSet Job CronJob SecurityContext ResourceQuota LimitRange HorizontalPodAutoscaling Ingress ConfigMap Label CustomResourceDefinition Role ClusterRole 我将它们简单的分类为以下几种资源对象： 类别 名称 资源对象 Po","title":"Kubernetes 中的资源对象"},{"content":"本章将为你介绍什么是服务网格，为什么服务网格对于云原生应用十分重要。\n本章大纲   什么是服务网格？\n  为什么要使用服务网格？\n  云原生应用网络\n   阅读本章   ","relpermalink":"/istio-handbook/intro/","summary":"本章将为你介绍什么是服务网格，为什么服务网格对于云原生应用十分重要。 本章大纲 什么是服务网格？ 为什么要使用服务网格？ 云原生应用网络 阅读本章","title":"服务网格概述"},{"content":"本章记录了用于在 Cilium 中配置网络策略的策略语言。安全策略可以通过以下机制指定和导入：\n 使用 Kubernetes NetworkPolicy、CiliumNetworkPolicy 和 CiliumClusterwideNetworkPolicy 资源。更多细节请参见网络策略一节。在这种模式下，Kubernetes 将自动向所有代理分发策略。 通过代理的 CLI 或 API 参考直接导入到代理中。这种方法不会自动向所有代理分发策略。用户有责任在所有需要的代理中导入策略。  本章内容包括：\n 策略执行模式 规则基础 三层示例 四层示例 七层示例 拒绝政策 主机策略 七层协议可视性 在策略中使用 Kubernetes 构造 端点生命周期 故障排除   阅读本章   ","relpermalink":"/cilium-handbook/policy/","summary":"本章记录了用于在 Cilium 中配置网络策略的策略语言。安全策略可以通过以下机制指定和导入： 使用 Kubernetes NetworkPolicy、CiliumNetworkPolicy 和 CiliumClusterwideNetworkPolicy 资源。更多细节请参见网络策略一节。在这种模式","title":"网络策略"},{"content":"在这一章中，你将了解：\n 服务网格的基本概念 Istio 的架构 stio 中的主要部署模式——Sidecar 模式 透明流量拦截——Istio 的核心设计思想  本章大纲   服务网格架构\n  服务网格的实现模式\n  Istio 简介\n  Istio 架构解析\n  Sidecar 模式\n  Sidecar 自动注入过程详解\n  Istio 中的透明流量劫持过程详解\n  Istio 中的 Sidecar 的流量路由详解\n   阅读本章   ","relpermalink":"/istio-handbook/concepts/","summary":"在这一章中，你将了解： 服务网格的基本概念 Istio 的架构 stio 中的主要部署模式——Sidecar 模式 透明流量拦截——Istio 的核心设计思想 本章大纲 服务网格架构 服务网格的实现模式 Istio 简介 Istio 架构解析 Sidecar 模式 Sidecar 自动注入过","title":"概念原理"},{"content":"为了管理异构和不同配置的主机，为了便于 Pod 的运维管理，Kubernetes 中提供了很多集群管理的配置和管理功能，通过 namespace 划分的空间，通过为 node 节点创建 label 和 taint 用于 pod 的调度等。\n本节大纲   Node\n  Namespace\n  Label\n  Annotation\n  Taint 和 Toleration（污点和容忍）\n  垃圾收集\n  资源调度\n  服务质量等级（QoS）\n  ","relpermalink":"/kubernetes-handbook/cluster/","summary":"为了管理异构和不同配置的主机，为了便于 Pod 的运维管理，Kubernetes 中提供了很多集群管理的配置和管理功能，通过 namespace 划分的空间，通过为 node 节点创建 label 和 taint 用于 pod 的调度等。 本节大纲 Node Namespace Label Annotation Taint 和 Tolerat","title":"集群资源管理"},{"content":"Kubernetes 中内建了很多 controller（控制器），这些相当于一个状态机，用来控制 Pod 的具体状态和行为。\n本节大纲   Deployment\n  StatefulSet\n  DaemonSet\n  ReplicationController 和 ReplicaSet\n  Job\n  CronJob\n  Ingress 控制器\n  Horizontal Pod Autoscaling\n  准入控制器（Admission Controller）\n  ","relpermalink":"/kubernetes-handbook/controllers/","summary":"Kubernetes 中内建了很多 controller（控制器），这些相当于一个状态机，用来控制 Pod 的具体状态和行为。 本节大纲 Deployment StatefulSet DaemonSet ReplicationController 和 ReplicaSet Job CronJob Ingress 控制器 Horizontal Pod Autoscaling 准入控制器（Admission Controller）","title":"控制器"},{"content":"本章大纲   介绍\n  概念\n  要求\n  网络策略\n  端点 CRD\n  端点切片 CRD\n  Kubernetes 兼容性\n  故障排除\n   阅读本章   ","relpermalink":"/cilium-handbook/kubernetes/","summary":"本章大纲 介绍 概念 要求 网络策略 端点 CRD 端点切片 CRD Kubernetes 兼容性 故障排除 阅读本章","title":"Kubernetes 集成"},{"content":"Istio 中的数据平面默认使用的是 Envoy 代理，你也可以根据 xDS 构建其他数据平面，例如蚂蚁开源的 MOSN。\n本章大纲   Envoy 中的基本术语\n  Envoy 代理配置详解\n  xDS 协议解析\n  LDS（监听器发现服务）\n  RDS（路由发现服务）\n  CDS（集群发现服务）\n  EDS（端点发现服务）\n  SDS（秘钥发现服务）\n  ADS（聚合发现服务）\n   阅读本章   ","relpermalink":"/istio-handbook/data-plane/","summary":"Istio 中的数据平面默认使用的是 Envoy 代理，你也可以根据 xDS 构建其他数据平面，例如蚂蚁开源的 MOSN。 本章大纲 Envoy 中的基本术语 Envoy 代理配置详解 xDS 协议解析 LDS（监听器发现服务） RDS（路由发现服务） CDS（集群发现服","title":"数据平面"},{"content":"应用的资源使用率通常都有高峰和低谷的时候，如何削峰填谷，提高集群的整体资源利用率，让 service 中的 Pod 个数自动调整呢？这就有赖于 Horizontal Pod Autoscaling 了，顾名思义，使 Pod 水平自动缩放。这个 Object（跟 Pod、Deployment 一样都是 API resource）也是最能体现 kubernetes 之于传统运维价值的地方，不再需要手动扩容了，终于实现自动化了，还可以自定义指标，没准未来还可以通过人工智能自动进化呢！\nHPA 属于 Kubernetes 中的 autoscaling SIG（Special Interest Group），其下有两个 feature：\n Arbitrary/Custom Metrics in the Horizontal Pod Autoscaler#117 Monitoring Pipeline Metrics HPA API #118  Kubernetes 自 1.2 版本引入 HPA 机制，到 1.6 版本之前一直是通过 kubelet 来获取监控指标来判断是否需要扩缩容，1.6 版本 …","relpermalink":"/kubernetes-handbook/controllers/hpa/","summary":"应用的资源使用率通常都有高峰和低谷的时候，如何削峰填谷，提高集群的整体资源利用率，让 service 中的 Pod 个数自动调整呢？这就有赖于 Horizontal Pod Autoscaling 了，顾名思义，使 Pod 水平自动缩放。这个 Object（跟 Pod、Deployme","title":"Horizontal Pod Autoscaling"},{"content":"Kubernetes 中为了实现服务实例间的负载均衡和不同服务间的服务发现，创造了 Serivce 对象，同时又为从集群外部访问集群创建了 Ingress 对象。\n本节大纲   Service\n  拓扑感知路由\n  Ingress\n  Gateway\n  ","relpermalink":"/kubernetes-handbook/service-discovery/","summary":"Kubernetes 中为了实现服务实例间的负载均衡和不同服务间的服务发现，创造了 Serivce 对象，同时又为从集群外部访问集群创建了 Ingress 对象。 本节大纲 Service 拓扑感知路由 Ingress Gateway","title":"服务发现与路由"},{"content":"有两种方法可以在单个 Kubernetes 集群上安装 Istio：\n 使用 Istioctl（istioctl） 使用 Istio Operator  在本章中，我们将使用 Istio Operator 在一个 Kubernetes 集群上安装 Istio。\n本章大纲   安装 Istio\n  GetMesh\n  Kiali\n  Bookinfo 示例\n   阅读本章   ","relpermalink":"/istio-handbook/setup/","summary":"有两种方法可以在单个 Kubernetes 集群上安装 Istio： 使用 Istioctl（istioctl） 使用 Istio Operator 在本章中，我们将使用 Istio Operator 在一个 Kubernetes 集群上安装 Istio。 本章大纲 安装 Istio GetMesh Kiali Bookinfo 示例 阅读本章","title":"安装"},{"content":"Kubernetes 中提供了良好的多租户认证管理机制，如 RBAC、ServiceAccount 还有各种策略等。\n另外还有一个 CNCF 孵化项目 SPIFFE 旨在为所有的分布式应用提供身份支持，目前已应用在了 Envoy、Istio 等应用中。\n本节大纲   ServiceAccount\n  基于角色的访问控制（RBAC）\n  NetworkPolicy\n  SPIFFE\n  SPIRE\n  SPIRE Kubernetes 工作负载注册器\n  SVID 身份颁发过程\n  ","relpermalink":"/kubernetes-handbook/auth/","summary":"Kubernetes 中提供了良好的多租户认证管理机制，如 RBAC、ServiceAccount 还有各种策略等。 另外还有一个 CNCF 孵化项目 SPIFFE 旨在为所有的分布式应用提供身份支持，目前已应用在了 Envoy、Istio 等应用中。 本","title":"身份与权限认证"},{"content":"Kubernetes 中的网络可以说对初次接触 Kubernetes 或者没有网络方面经验的人来说可能是其中最难的部分。Kubernetes 本身并不提供网络功能，只是把网络接口开放出来，通过插件的形式实现。\n网络要解决的问题 既然 Kubernetes 中将容器的联网通过插件的方式来实现，那么该如何解决容器的联网问题呢？\n如果您在本地单台机器上运行 docker 容器的话会注意到所有容器都会处在 docker0 网桥自动分配的一个网络 IP 段内（172.17.0.1/16）。该值可以通过 docker 启动参数 --bip 来设置。这样所有本地的所有的容器都拥有了一个 IP 地址，而且还是在一个网段内彼此就可以互相通信了。\n但是 Kubernetes 管理的是集群，Kubernetes 中的网络要解决的核心问题就是每台主机的 IP 地址网段划分，以及单个容器的 IP 地址分配。概括为：\n 保证每个 Pod 拥有一个集群内唯一的 IP 地址 保证不同节点的 IP 地址划分不会重复 保证跨节点的 Pod 可以互相通信 保证不同节点的 Pod 可以与跨节点的主机互相通信  为了解决该问 …","relpermalink":"/kubernetes-handbook/networking/","summary":"Kubernetes 中的网络可以说对初次接触 Kubernetes 或者没有网络方面经验的人来说可能是其中最难的部分。Kubernetes 本身并不提供网络功能，只是把网络接口开放出来，通过插件的形式实现。 网络要解决的问题 既然 Kubernetes 中将容器的联网","title":"网络"},{"content":"在本章中，我们将开始使用 Istio 服务网格在服务之间进行流量路由。我们将学习如何设置一个 Ingress 资源以允许流量进入我们的集群，以及一个 Egress 资源以使流量流出集群。\n使用流量路由，我们将学习如何部署新版本的服务，并在已发布的生产版本的服务旁边运行，而不干扰生产流量。随着两个服务版本的部署，我们将逐步发布（金丝雀发布）新版本，并开始将一定比例的传入流量路由到最新版本。\n流量管理是 Isito 中的最基础功能，使用 Istio 的流量管理模型，本质上是将流量与基础设施扩容解耦，让运维人员可以通过 Pilot 指定流量遵循什么规则，而不是指定哪些 pod/VM 应该接收流量——Pilot 和智能 Envoy 代理会帮我们搞定。\n所谓流量管理是指：\n 控制服务之间的路由：通过在 VirtualService 中的规则条件匹配来设置路由，可以在服务间拆分流量。 控制路由上流量的行为：设定好路由之后，就可以在路由上指定超时和重试机制，例如超时时间、重试次数等；做错误注入、设置断路器等。可以由 VirtualService 和 DestinationRule 共同完成。 显式地 …","relpermalink":"/istio-handbook/traffic-management/","summary":"在本章中，我们将开始使用 Istio 服务网格在服务之间进行流量路由。我们将学习如何设置一个 Ingress 资源以允许流量进入我们的集群，以及一个 Egress 资源以使流量流出集群。 使用流量路由，我们将学习如何部署新版本的服务，并在已发布","title":"流量管理"},{"content":"为了管理存储，Kubernetes 提供了以下资源对象：\n Secret：用于管理敏感信息 ConfigMap：存储配置 Volume、PV、PVC、StorageClass 等：用来管理存储卷  本节将为你讲解 Kubernetes 中的存储对象。\n本节大纲   Secret\n  ConfigMap\n  ConfigMap 的热更新\n  Volume\n  持久化卷（Persistent Volume）\n  Storage Class\n  本地持久化存储\n  ","relpermalink":"/kubernetes-handbook/storage/","summary":"为了管理存储，Kubernetes 提供了以下资源对象： Secret：用于管理敏感信息 ConfigMap：存储配置 Volume、PV、PVC、StorageClass 等：用来管理存储卷 本节将为你讲解 Kubernetes 中","title":"存储"},{"content":"流量管理配置章节描述如何配置 Istio 服务网格中与流量管理相关的一些资源对象。\n本章大纲   Gateway\n  VirtualService\n  DestinationRule\n  WorkloadEntry\n  WorkloadGroup\n  ServiceEntry\n  Sidecar\n  EnvoyFilter\n  ProxyConfig\n   阅读本章   ","relpermalink":"/istio-handbook/config-networking/","summary":"流量管理配置章节描述如何配置 Istio 服务网格中与流量管理相关的一些资源对象。 本章大纲 Gateway VirtualService DestinationRule WorkloadEntry WorkloadGroup ServiceEntry Sidecar EnvoyFilter ProxyConfig 阅读本章","title":"流量管理配置"},{"content":"Kubernetes 是一个高度开放可扩展的架构，可以通过自定义资源类型（CRD）来定义自己的类型，还可以自己来扩展 API 服务，用户的使用方式跟 Kubernetes 的原生对象无异。\n本节大纲   使用自定义资源扩展 API\n  使用 CRD 扩展 Kubernetes API\n  Aggregated API Server\n  APIService\n  服务目录（Service Catalog）\n  ","relpermalink":"/kubernetes-handbook/extend/","summary":"Kubernetes 是一个高度开放可扩展的架构，可以通过自定义资源类型（CRD）来定义自己的类型，还可以自己来扩展 API 服务，用户的使用方式跟 Kubernetes 的原生对象无异。 本节大纲 使用自定义资源扩展 API 使用 CRD 扩展 Kubernetes API Aggregated API Server APIService 服务目录（S","title":"扩展集群"},{"content":"组织需要部署多个 Kubernetes 集群来为不同的业务提供隔离，增强可用性和可扩展性。\n什么是多集群？ 多集群是一种在多个 Kubernetes 集群上或跨集群部署应用的策略，目的是提高可用性、隔离性和可扩展性。多集群对于确保遵守不同的和相互冲突的法规非常重要，因为单个集群可以进行调整，以遵守特定地域或认证的法规。软件交付的速度和安全性也可以提高，单个开发团队将应用程序部署到隔离的集群中，并有选择地暴露哪些服务可用于测试和发布。\n配置多集群访问 你可以使用 kubectl config 命令配置要访问的集群，详见配置对多集群的访问。\n集群联邦 集群联邦（Federation）是指通过 Federation API 资源来统一管理多个集群的资源，如定义 Deployment 如何部署到不同集群上，及其所需的副本数等。这些集群可能位于不同的可用区、地区或者供应商。实施集群联邦一般是为了达到以下目的：\n 简化管理多个集群的 Kubernetes 组件 (如 Deployment、Service 等）； 在多个集群之间分散工作负载（Pod），以提升应用（服务）的可靠性； 跨集群的资源编排， …","relpermalink":"/kubernetes-handbook/multi-cluster/","summary":"组织需要部署多个 Kubernetes 集群来为不同的业务提供隔离，增强可用性和可扩展性。 什么是多集群？ 多集群是一种在多个 Kubernetes 集群上或跨集群部署应用的策略，目的是提高可用性、隔离性和可扩展性。多集群对于确保遵守不同的和相互冲","title":"多集群管理"},{"content":"在本章中，我们将学习一些监控（Prometheus）、追踪（Zipkin）和数据可视化工具（Grafana）。\n什么是可观测性？ 由于采用了 sidecar 部署模式，即 Envoy 代理运行在应用实例旁边并拦截流量，这些代理也收集指标。\nEnvoy 代理收集的指标可以帮助我们获得系统状态的可视性。获得系统的这种可视性是至关重要的，因为我们需要了解正在发生的事情，并授权运维人员对应用程序进行故障排除、维护和优化。\nIstio 生成三种类型的遥测数据，为网格中的服务提供可观测性：\n 指标度量（Metric） 分布式跟踪 访问日志  指标度量 Istio 基于四个黄金信号生成指标：延迟、流量、错误和饱和度。\n延迟表示服务一个请求所需的时间。这个指标应该分成成功请求（如 HTTP 200）和失败请求（如 HTTP 500）的延迟。\n流量是衡量对系统的需求有多大，它是以系统的具体指标来衡量的。例如，每秒的 HTTP 请求，或并发会话，每秒的检索量，等等。\n错误用来衡量请求失败的比率（例如 HTTP 500）。\n饱和度衡量一个服务中最紧张的资源有多满。例如，线程池的利用率。\n这些指标是在不同的层 …","relpermalink":"/istio-handbook/observability/","summary":"在本章中，我们将学习一些监控（Prometheus）、追踪（Zipkin）和数据可视化工具（Grafana）。 什么是可观测性？ 由于采用了 sidecar 部署模式，即 Envoy 代理运行在应用实例旁边并拦截流量，这些代理也收集","title":"可观测性"},{"content":"Kubernetes 中的各个资源对象的配置指南。\n本节大纲   配置 Pod 的 liveness 和 readiness 探针\n  配置 Pod 的 Service Account\n  Secret 配置\n  管理 namespace 中的资源配额\n  ","relpermalink":"/kubernetes-handbook/config/","summary":"Kubernetes 中的各个资源对象的配置指南。 本节大纲 配置 Pod 的 liveness 和 readiness 探针 配置 Pod 的 Service Account Secret 配置 管理 namespace 中的资源配额","title":"资源对象配置"},{"content":"Kubernetes 中的 kubectl 及其他管理命令使用。\n本节大纲   Docker 用户过渡到 kubectl 命令行指南\n  Kubectl 命令概览\n  Kubectl 命令技巧大全\n  使用 etcdctl 访问 Kubernetes 数据\n  ","relpermalink":"/kubernetes-handbook/cli/","summary":"Kubernetes 中的 kubectl 及其他管理命令使用。 本节大纲 Docker 用户过渡到 kubectl 命令行指南 Kubectl 命令概览 Kubectl 命令技巧大全 使用 etcdctl 访问 Kubernetes 数据","title":"命令使用"},{"content":"在本章中，我们将学习 Istio 的安全功能。具体来说，就是认证和授权策略、安全命名和身份。\n在 Istio 中，有多个组件参与提供安全功能：\n 用于管理钥匙和证书的证书颁发机构（CA）。 Sidecar 和周边代理：实现客户端和服务器之间的安全通信，它们作为政策执行点（Policy Enforcement Point，简称PEP）工作 Envoy 代理扩展：管理遥测和审计 配置 API 服务器：分发认证、授权策略和安全命名信息  本章大纲   认证\n  对等认证和请求认证\n  mTLS\n  授权\n   阅读本章   ","relpermalink":"/istio-handbook/security/","summary":"在本章中，我们将学习 Istio 的安全功能。具体来说，就是认证和授权策略、安全命名和身份。 在 Istio 中，有多个组件参与提供安全功能： 用于管理钥匙和证书的证书颁发机构（CA）。 Sidecar 和周边代理：实现客户端和服务器之间的安全","title":"安全"},{"content":"Kubernetes 支持多租户，这就需要对集群的安全性进行管理。\n本节大纲   管理集群中的 TLS\n  Kublet 的认证授权\n  TLS Bootstrap\n  IP 伪装代理\n  创建用户认证授权的 kubeconfig 文件\n  使用 kubeconfig 或 token 进行用户身份认证\n  Kubernetes 中的用户与身份认证授权\n  Kubernetes 集群安全性配置最佳实践\n  ","relpermalink":"/kubernetes-handbook/security/","summary":"Kubernetes 支持多租户，这就需要对集群的安全性进行管理。 本节大纲 管理集群中的 TLS Kublet 的认证授权 TLS Bootstrap IP 伪装代理 创建用户认证授权的 kubeconfig 文件 使用 kubeconfig 或 token 进行用户身份认证 Kubernetes 中的用户与身份认证授权 Kubernetes 集群安全性配置最佳实践","title":"集群安全性管理"},{"content":"安全一节描述如何配置 Istio mesh 中与安全性相关的配置，其中包括：\n AuthorizationPolicy RequestAuthentication PeerAuthentication JWTRule  本章大纲   AuthorizationPolicy\n  RequestAuthentication\n  PeerAuthentication\n  JWTRule\n   阅读本章   ","relpermalink":"/istio-handbook/config-security/","summary":"安全一节描述如何配置 Istio mesh 中与安全性相关的配置，其中包括： AuthorizationPolicy RequestAuthentication PeerAuthentication JWTRule 本章大纲 AuthorizationPolicy RequestAuthentication PeerAuthentication JWTRule 阅读本章","title":"安全配置"},{"content":"在本章中，我们将了解如何扩展 Istio。\n本章大纲   扩展性概述\n   阅读本章   ","relpermalink":"/istio-handbook/extensibility/","summary":"在本章中，我们将了解如何扩展 Istio。 本章大纲 扩展性概述 阅读本章","title":"扩展 Istio"},{"content":"根据用户部署和暴露服务的方式不同，有很多种方式可以用来访问 Kubernetes 集群。\n 最简单也是最直接的方式是使用 kubectl 命令。 其次可以使用 kubeconfig 文件来认证授权访问 API server。 通过各种 proxy 经过端口转发访问 Kubernetes 集群中的服务 使用 Ingress，在集群外访问 Kubernetes 集群内的 service  本节大纲   访问集群\n  使用 kubeconfig 文件配置跨集群认证\n  通过端口转发访问集群中的应用程序\n  使用 service 访问群集中的应用程序\n  从外部访问 Kubernetes 中的 Pod\n  Lens - Kubernetes IDE\n  Kubernator - 更底层的 Kubernetes UI\n  ","relpermalink":"/kubernetes-handbook/access/","summary":"根据用户部署和暴露服务的方式不同，有很多种方式可以用来访问 Kubernetes 集群。 最简单也是最直接的方式是使用 kubectl 命令。 其次可以使用 kubeconfig 文件来认证授权访问 API server。 通过各种 proxy 经过端口转发访问 Kubernetes 集群中的服务 使用 Ing","title":"访问 Kubernetes 集群"},{"content":"理论上只要可以使用主机名做服务注册的应用都可以迁移到 Kubernetes 集群上。看到这里你可能不禁要问，为什么使用 IP 地址做服务注册发现的应用不适合迁移到 kubernetes 集群？因为这样的应用不适合自动故障恢复，因为目前 Kubernetes 中不支持固定 Pod 的 IP 地址，当 Pod 故障后自动转移到其他节点的时候该 Pod 的 IP 地址也随之变化。\n将传统应用迁移到 Kubernetes 中可能还有很长的路要走，但是直接开发云原生应用，Kubernetes 就是最佳运行时环境了。\n本节大纲   适用于 Kubernetes 的应用开发部署流程\n  迁移传统应用到 Kubernetes 步骤详解——以 Hadoop YARN 为例\n  使用StatefulSet部署有状态应用\n  ","relpermalink":"/kubernetes-handbook/devops/","summary":"理论上只要可以使用主机名做服务注册的应用都可以迁移到 Kubernetes 集群上。看到这里你可能不禁要问，为什么使用 IP 地址做服务注册发现的应用不适合迁移到 kubernetes 集群？因为这样的应用不适合自动故障恢复，因为目前 Kubernetes 中不支持固定 Pod","title":"在 Kubernetes 中开发部署应用"},{"content":"讲解如何在原生 Kubernetes 的基础上做定制开发。\n本节大纲   SIG 和工作组\n  配置 Kubernetes 开发环境\n  测试 Kubernetes\n  client-go 示例\n  Operator\n  Operator SDK\n  Kubebuilder\n  高级开发指南\n  参与 Kubernetes 社区贡献\n  Minikube\n   阅读本章   ","relpermalink":"/kubernetes-handbook/develop/","summary":"讲解如何在原生 Kubernetes 的基础上做定制开发。 本节大纲 SIG 和工作组 配置 Kubernetes 开发环境 测试 Kubernetes client-go 示例 Operator Operator SDK Kubebuilder 高级开发指南 参与 Kubernetes 社区贡献 Minikube 阅读本章","title":"开发指南"},{"content":"在本章中，我们将了解在多个集群上安装 Istio 的不同方法，以及如何将运行在虚拟机上的工作负载纳入服务网格。\n当决定在多集群场景下运行 Istio 时，有多种组合需要考虑。在高层次上，我们需要决定以下几点：\n 单个集群或多个集群 单个网络或多个网络 单个控制平面或多个控制平面 单个网格或多个网格  上述模式的任何组合都是可能的，然而，并非所有的模式都有意义。在本章中，我们将重点讨论涉及多个集群的场景。\n本章大纲   多集群部署\n  虚拟机负载\n   阅读本章   ","relpermalink":"/istio-handbook/advanced/","summary":"在本章中，我们将了解在多个集群上安装 Istio 的不同方法，以及如何将运行在虚拟机上的工作负载纳入服务网格。 当决定在多集群场景下运行 Istio 时，有多种组合需要考虑。在高层次上，我们需要决定以下几点： 单个集群或多个集群","title":"高级功能"},{"content":"本章介绍了在使用 Istio 时可能遇到的问题的几种排查方法。\n本章大纲   Envoy 基础问题\n  Envoy 示例\n  调试清单\n  Istio 开发环境配置\n   阅读本章   ","relpermalink":"/istio-handbook/troubleshooting/","summary":"本章介绍了在使用 Istio 时可能遇到的问题的几种排查方法。 本章大纲 Envoy 基础问题 Envoy 示例 调试清单 Istio 开发环境配置 阅读本章","title":"问题排查"},{"content":"Istio 服务网格自 2017 年 5 月开源以来，已围绕其周边诞生了诸多开源项目，这些项目有的是对 Istio 本身的扩展，有的是可以与 Istio 集成，还有的是 Istio 周边的开源工具。\n本章将分门别类为读者介绍 Istio 生态中的开源项目，以帮助读者对 Istio 的开源生态有个更直观的了解，另一方面也可以作为读者个人的选型参考。\nIstio 周边开源项目 下表中列举 Istio 生态中的开源项目，按照开源时间排序。\n   项目名称 开源时间 类别 描述 主导公司 Star 数量 与 Istio 的关系     Envoy 2016年 9 月 网络代理 云原生高性能边缘/中间服务代理 Lyft 18300 默认的数据平面   Istio 2017 年 5 月 服务网格 连接、保护、控制和观察服务。 Google 28400 控制平面   Emissary Gateway 2018 年 2 月 网关 用于微服务的 Kubernetes 原生 API 网关，基于 Envoy 构建 Ambassador 3500 可连接 Istio   APISIX 2019 年 6 月 网 …","relpermalink":"/istio-handbook/ecosystem/","summary":"Istio 服务网格自 2017 年 5 月开源以来，已围绕其周边诞生了诸多开源项目，这些项目有的是对 Istio 本身的扩展，有的是可以与 Istio 集成，还有的是 Istio 周边的开源工具。 本章将分门别类为读者介绍 Istio 生态中的开源项目，以帮助读者对 Istio 的开","title":"Istio 生态"},{"content":"在本章中，我们将部署名为 Online Boutique 的微服务演示应用程序，并尝试不同的Istio功能。\nOnline Boutique 是一个云原生微服务演示应用程序。Online Boutique 由一个 10 层的微服务应用组成。该应用是一个基于 Web 的电子商务应用，用户可以浏览商品，将其添加到购物车，并购买商品。\n本章大纲   创建集群\n  部署 Online Boutique 应用\n  部署可观测性工具\n  路由流量\n  错误注入\n  弹性\n   阅读本章   ","relpermalink":"/istio-handbook/practice/","summary":"在本章中，我们将部署名为 Online Boutique 的微服务演示应用程序，并尝试不同的Istio功能。 Online Boutique 是一个云原生微服务演示应用程序。Online Boutique 由一个 10 层的微服务应用组成。该应用是一个基于 Web 的电子商务应用，用户可以浏","title":"实战案例"},{"content":"Istio 利用 WebAssembly 技术实现扩展，使用 Proxy-Wasm 沙盒 API 替换了 Istio 老版本中的 Mixer。\nWebAssembly 沙盒的目标：  效率：降低扩展增加的延迟以及对 CPU 和内存的开销； 功能：扩展可以执行策略，收集遥测数据，并执行有效载荷的变异； 隔离：插件的编程错误或崩溃不会影响其他插件； 配置：使用与其他 Istio AP I一致的 API 对插件进行配置，可以动态地配置扩展； 运维：可以使用金丝雀部署，或者使用 log-only、fail-open 或 fail-close 模式部署扩展； 开发者：可以用多种编程语言编写扩展；  架构 Istio 扩展（Proxy-Wasm 插件）由以下组成部分：\n 过滤器服务提供商接口（Filter Service Provider Interface，简称 SPI） ：用于为过滤器构建 Proxy-Wasm 插件； 沙盒：嵌入在 Envoy 中的 V8 Wasm 运行时； 主机 API：用于处理请求头、尾和元数据； 呼出 API：针对 gRPC 和 HTTP 请求； 统计和记录 API：用 …","relpermalink":"/istio-handbook/extensibility/overview/","summary":"Istio 利用 WebAssembly 技术实现扩展，使用 Proxy-Wasm 沙盒 API 替换了 Istio 老版本中的 Mixer。 WebAssembly 沙盒的目标： 效率：降低扩展增加的延迟以及对 CPU 和内存的开销； 功能：扩展可以执行策略，收集遥测数据，并执行有效载荷的变异； 隔离：插件的编程","title":"Istio 的扩展性概述"},{"content":"Linux 内核在网络堆栈中支持一组 BPF 钩子（hook），可用于运行 BPF 程序。Cilium 数据路径使用这些钩子来加载 BPF 程序，这些程序一起使用时会创建更高级别的网络结构。\n以下是 Cilium 使用的钩子列表和简要说明。有关每个钩子细节的更详尽的文档，请参阅 BPF 和 XDP 参考指南。\n  XDP：XDP BPF 钩子位于网络驱动程序中的最早可能点，并在数据包接收时触发 BPF 程序的运行。这实现了可能的最佳数据包处理性能，因为程序在任何其他处理发生之前直接在数据包数据上运行。此钩子非常适合运行丢弃恶意或意外流量的过滤程序以及其他常见的 DDOS 保护机制。\n  流量控制入口/出口：附加到流量控制（traffic control，简称 TC）入口钩子的 BPF 程序附加到网络接口，与 XDP 相同，但将在网络堆栈完成数据包的初始处理后运行。该钩子在三层网络之前运行，但可以访问与数据包关联的大部分元数据。这非常适合进行本地节点处理，例如应用三层/四层端点策略并将流量重定向到端点。对于面向网络的设备，TC 入口钩子可以与上面的 XDP 钩子耦合。完成此操作后，可以合 …","relpermalink":"/cilium-handbook/ebpf/intro/","summary":"Linux 内核在网络堆栈中支持一组 BPF 钩子（hook），可用于运行 BPF 程序。Cilium 数据路径使用这些钩子来加载 BPF 程序，这些程序一起使用时会创建更高级别的网络结构。 以下是 Cilium 使用的钩子列表和简要说明。有关每个钩子","title":"eBPF 数据路径介绍"},{"content":"Cilium 能为 Kubernetes 集群提供什么？ 在 Kubernetes 集群中运行 Cilium 时提供以下功能：\n CNI 插件支持，为 pod 连接 提供 联网。 NetworkPolicy 资源的基于身份的实现，用于隔离三层和四层网络 pod 的连接。 以 CustomResourceDefinition 形式对 NetworkPolicy 的扩展，扩展策略控制以添加：  针对以下应用协议的入口和出口执行七层策略：  HTTP Kafka   对 CIDR 的出口支持以保护对外部服务的访问 强制外部无头服务自动限制为服务配置的 Kubernetes 端点集   ClusterIP 实现为 pod 到 pod 的流量提供分布式负载平衡 完全兼容现有的 kube-proxy 模型  Pod 间连接 在 Kubernetes 中，容器部署在称为 pod 的单元中，其中包括一个或多个可通过单个 IP 地址访问的容器。使用 Cilium，每个 pod 从运行 pod 的 Linux 节点的节点前缀中获取一个 IP 地址。有关其他详细信息，请参阅 IP 地址管理（IPAM）。在没 …","relpermalink":"/cilium-handbook/kubernetes/intro/","summary":"Cilium 能为 Kubernetes 集群提供什么？ 在 Kubernetes 集群中运行 Cilium 时提供以下功能： CNI 插件支持，为 pod 连接 提供 联网。 NetworkPolicy 资源的基于身份的实现，用于隔离三层和四层网络 pod 的连接。 以 CustomResourceDefinition 形式对 NetworkPolicy 的扩展，扩展策略控制以添加： 针对以下应用协议的入","title":"Kubernetes 集成介绍"},{"content":"Cilium 在多个层面上提供安全性。可以单独使用或组合使用。\n 基于身份：端点之间的连接策略（三层），例如任何带有标签的端点 role=frontend 都可以连接到任何带有标签的端点 role=backend。 限制传入和传出连接的可访问端口（四层），例如带标签的端点 role=frontend只能在端口 443（https）上进行传出连接，端点role=backend 只能接受端口 443（https）上的连接。 应用程序协议级别的细粒度访问控制，以保护 HTTP 和远程过程调用（RPC）协议，例如带有标签的端点 role=frontend 只能执行 REST API 调用 GET /userdata/[0-9]+，所有其他与 role=backend API 的交互都受到限制。  ","relpermalink":"/cilium-handbook/security/intro/","summary":"Cilium 在多个层面上提供安全性。可以单独使用或组合使用。 基于身份：端点之间的连接策略（三层），例如任何带有标签的端点 role=frontend 都可以连接到任何带有标签的端点 role=backend。 限制传入和传出连接的可访问端口（","title":"介绍"},{"content":"封装 当没有提供配置时，Cilium 会自动在此模式下运行，因为它是对底层网络基础设施要求最低的模式。\n在这种模式下，所有集群节点使用基于 UDP 的封装协议 VXLAN 或 Geneve。Cilium 节点之间的所有流量都被封装。\n对网络的要求   封装依赖于正常的节点到节点的连接。这意味着如果 Cilium 节点已经可以互相到达，那么所有的路由要求都已经满足了。\n  底层网络和防火墙必须允许封装数据包：\n   封装方式 端口范围 / 协议     VXLAN（默认） 8472/UDP   Geneve 6081/UDP      封装模式的优点 封装模式具有以下优点：\n  简单\n连接集群节点的网络不需要知道 PodCIDR。集群节点可以产生多个路由或链路层域。只要集群节点可以使用 IP/UDP 相互访问，底层网络的拓扑就无关紧要。\n  寻址空间\n由于不依赖于任何底层网络限制，如果相应地配置了 PodCIDR 大小，可用的寻址空间可能会更大，并且允许每个节点运行任意数量的 Pod。\n  自动配置\n当与 Kubernetes 等编排系统一起运行时，集群中所有节点的列表（包括它们关联的分 …","relpermalink":"/cilium-handbook/networking/routing/","summary":"封装 当没有提供配置时，Cilium 会自动在此模式下运行，因为它是对底层网络基础设施要求最低的模式。 在这种模式下，所有集群节点使用基于 UDP 的封装协议 VXLAN 或 Geneve。Cilium 节点之间的所有流量都被封装","title":"路由"},{"content":"本文将为你简要介绍 Cilium 和 Hubble。\n什么是 Cilium？ Cilium 是开源软件，用于透明地保护使用 Docker 和 Kubernetes 等 Linux 容器管理平台部署的应用服务之间的网络连接。\nCilium 的基础是一种新的 Linux 内核技术，称为 eBPF，它使强大的安全可视性和控制逻辑动态插入 Linux 本身。由于 eBPF 在 Linux 内核内运行，Cilium 安全策略的应用和更新无需对应用程序代码或容器配置进行任何改动。\n什么是 Hubble？ Hubble 是一个完全分布式的网络和安全可观测性平台。它建立在 Cilium 和 eBPF 之上，以完全透明的方式实现对服务的通信行为以及网络基础设施的深度可视性。\n通过建立在 Cilium 之上，Hubble 可以利用 eBPF 实现可视性。依靠 eBPF，所有的可视性都是可编程的，并允许采用一种动态的方法，最大限度地减少开销，同时按照用户的要求提供深入和详细的可视性。Hubble 的创建和专门设计是为了最好地利用这些新的 eBPF 力量。\nHubble 可以回答诸如以下问题。\n服务依赖和拓扑 …","relpermalink":"/cilium-handbook/intro/","summary":"本文将为你简要介绍 Cilium 和 Hubble。 什么是 Cilium？ Cilium 是开源软件，用于透明地保护使用 Docker 和 Kubernetes 等 Linux 容器管理平台部署的应用服务之间的网络连接。 Cilium 的基础是一种新的 Linux 内核技术，称为 eBPF，它使强大的安全可","title":"Cilium 和 Hubble 简介"},{"content":"Cilium 代理（agent）和 Cilium 网络策略的配置决定了一个端点（Endpoint）是否接受来自某个来源的流量。代理可以进入以下三种策略执行模式：\n  default\n如果任何规则选择了一个 Endpoint 并且该规则有一个入口部分，那么该端点就会在入口处进入默认拒绝状态。如果任何规则选择了一个 Endpoint 并且该规则有一个出口部分，那么该端点就会在出口处进入默认拒绝状态。这意味着端点开始时没有任何限制，一旦有规则限制其在入口处接收流量或在出口处传输流量的能力，那么端点就会进入白名单模式，所有流量都必须明确允许。\n  always\n在 always 模式下，即使没有规则选择特定的端点，也会在所有端点上启用策略执行。如果你想配置健康实体，在启动 cilium-agent 时用 enable-policy=always 检查整个集群的连接性，你很可能想启用与健康端点的通信。\n  never\n在 “never\u0026#34; 模式下，即使规则选择了特定的端点，所有端点上的策略执行也被禁用。换句话说，所有流量都允许来自任何来源（入口处）或目的地（出口处）。 …","relpermalink":"/cilium-handbook/policy/intro/","summary":"Cilium 代理（agent）和 Cilium 网络策略的配置决定了一个端点（Endpoint）是否接受来自某个来源的流量。代理可以进入以下三种策略执行模式： default 如果任何规则选择了一个 Endpoint 并且该规则有一个入口部分，那么该端点就会","title":"网络策略模式"},{"content":"本文将为你介绍 Cilium 和 Hubble 部署中包含的组件。\nCilium 包含以下组件：\n 代理 客户端 Operator CNI 插件  Hubble 包含以下组件：\n 服务器 中继器 客户端 图形用户界面 eBPF  另外你还需要一个数据库来存储代理的状态。\n下图展示的 Cilium 部署的组件。\n   Cilium 组件示意图  Cilium 代理\nCilium 代理（cilium-agent）在集群的每个节点上运行。在高层次上，代理接受通过 Kubernetes 或 API 的配置，描述网络、服务负载均衡、网络策略、可视性和监控要求。\nCilium 代理监听来自编排系统（如 Kubernetes）的事件，以了解容器或工作负载的启动和停止时间。它管理 eBPF 程序，Linux 内核用它来控制这些容器的所有网络访问。\n客户端（CLI）\nCilium CLI 客户端（cilium）是一个命令行工具，与 Cilium 代理一起安装。它与运行在同一节点上的 Cilium 代理的 REST API 互动。CLI 允许检查本地代理的状态。它还提供工具，直接访问 eBPF map 以 …","relpermalink":"/cilium-handbook/concepts/overview/","summary":"本文将为你介绍 Cilium 和 Hubble 部署中包含的组件。 Cilium 包含以下组件： 代理 客户端 Operator CNI 插件 Hubble 包含以下组件： 服务器 中继器 客户端 图形用户界面 eBPF 另外你还需要一个数据库来存储代理的状态。 下图展示的 Cilium 部署的组件。 Cilium 组件示意图 Cilium 代","title":"组件概览"},{"content":"在过去的几年里，eBPF 已经从相对默默无闻变成了现代基础设施建设中最热门的技术领域之一。就我个人而言，自从看到 Thomas Graf 在 DockerCon 17 的黑带会议（Black Blet）1 上谈到 eBPF 时，我就对它的可能性感到兴奋。在云原生计算基金会（CNCF），我在技术监督委员会（TOC）的同事把 eBPF 作为我们预测 2021 年将会起飞的重点技术之一来关注。超过 2500 人报名参加了当年的 eBPF 峰会线上会议，世界上最先进的几家软件工程公司共同创建了 eBPF 基金会。显然，人们对这项技术有很大的兴趣。\n在这个简短的报告中，我希望能给你一些启示，为什么人们对 eBPF 如此兴奋，以及它在现代计算环境中提供的工具能力。你会了解到 eBPF 是什么以及为什么它如此强大。还有一些代码实例，以使这种感觉更加具象化（但如果你愿意，你可以跳过这些）。\n你将了解到在建立支持 eBPF 的工具时涉及的内容，以及为什么 eBPF 在如此短的时间内变得如普遍。\n在这份简短的报告中，难免无法了解所有的细节，但如果你想更深入地了解，我将给你一些参考信息。\n扩展的伯克利数据包 …","relpermalink":"/what-is-ebpf/introduction/","summary":"在过去的几年里，eBPF 已经从相对默默无闻变成了现代基础设施建设中最热门的技术领域之一。就我个人而言，自从看到 Thomas Graf 在 DockerCon 17 的黑带会议（Black Blet）1 上谈到 eBPF 时，我就对它的可能性感到兴奋。在云原生","title":"第一章：eBPF 简介"},{"content":"Kubernetes 是云原生时代的 POSIX 在单机时代，POSIX 是类 UNIX 系统的通用 API，而在云原生时代，Kubernetes 是云操作系统的的 POSIX，它定义了基于云的分布式系统的 API。下表将 Kubernetes 与 POSIX 进行了对比。\n   对比项 Linux Kubernetes     隔离单元 进程 Pod   硬件 单机 数据中心   并发 线程 容器   资源管理 进程内存\u0026amp;CPU 内存、CPU Limit/Request   存储 文件 ConfigMap、Secret、Volume   网络 端口绑定 Service   终端 tty、pty、shell kubectl exec   网络安全 IPtables NetworkPolicy   权限 用户、文件权限 ServiceAccount、RBAC    注意\n 我们不能说 Linux 就是 POSIX，只能说 Linux 是 UNIX 兼容的。  参考  Kubernetes is the POSIX of the cloud - home.robusta.dev  ","relpermalink":"/cloud-native-handbook/kubernetes/what-is-kubernetes/","summary":"Kubernetes 是云原生时代的 POSIX 在单机时代，POSIX 是类 UNIX 系统的通用 API，而在云原生时代，Kubernetes 是云操作系统的的 POSIX，它定义了基于云的分布式系统的 API。下表将 Kubernetes 与 POSIX 进行了对比。 对比项 Linux Kubernetes 隔","title":"什么是 Kubernetes?"},{"content":"首先我们来阐述下将应用迁移到云原生架构的动机。\n速度 天下武功，唯快不破，市场竞争亦是如此。想象一下，能够快速创新、实验并交付软件的企业，与使用传统软件交付模式的企业，谁将在市场竞争中胜出呢？\n在传统企业中，为应用提供环境和部署新版本花费的时间通常以天、周或月来计算。这种速度严重限制了每个发行版可以承担的风险，因为修复这些错误往往跟发行一个新版本有差不多的耗时。\n互联网公司经常提到它们每天几百次发布的实践。为什么频繁发布如此重要？如果你可以每天实现几百次发布，你们就可以几乎立即从错误的版本恢复过来。如果你可以立即从错误中恢复过来，你就能够承受更多的风险。如果你可以承受更多的风险，你就可以做更疯狂的试验 —— 这些试验结果可能会成为你接下来的竞争优势。\n基于云基础设置的弹性和自服务的特性天生就适应于这种工作方式。通过调用云服务 API 来提供新的应用程序环境比基于表单的手动过程要快几个数量级。然后通过另一个 API 调用将代码部署到新的环境中。将自服务和 hook 添加到团队的 CI/CD 服务器环境中进一步加快了速度。现在，我们可以回答精益大师 Mary Poppendick 提出的问 …","relpermalink":"/migrating-to-cloud-native-application-architectures/the-rise-of-cloud-native/why-cloud-native-application-architectures/","summary":"首先我们来阐述下将应用迁移到云原生架构的动机。 速度 天下武功，唯快不破，市场竞争亦是如此。想象一下，能够快速创新、实验并交付软件的企业，与使用传统软件交付模式的企业，谁将在市场竞争中胜出呢？ 在传统企业中","title":"1.1 为何使用云原生应用架构"},{"content":"由于微服务通常是以容器的形式实现的，因此容器编排和资源管理平台被用于服务的部署、运维和维护。\n一个典型的协调和资源管理平台由各种逻辑（形成抽象层）和物理工件组成，用于部署容器。例如，在 Kubernetes 中，容器在最小的部署单元内运行，称为 Pod。一个 Pod 理论上可以承载一组容器，但通常情况下，一个 Pod 内只运行一个容器。一组 Pod 被定义在所谓的节点内，节点可以是物理机或虚拟机（VM）。一组节点构成了一个集群。通常情况下，需要单个微服务的多个实例来分配工作负载，以达到预期的性能水平。集群是一个资源池（节点），用于分配微服务的工作负载。使用的技术之一是横向扩展，即访问频率较高的微服务被分配更多的实例或分配到具有更多资源（如 CPU 和 / 或内存）的节点。\n","relpermalink":"/service-mesh-devsecops/reference-platform/container-orchestration-and-resource-management-platform/","summary":"由于微服务通常是以容器的形式实现的，因此容器编排和资源管理平台被用于服务的部署、运维和维护。 一个典型的协调和资源管理平台由各种逻辑（形成抽象层）和物理工件组成，用于部署容器。例如，在 Kubernetes 中，容器在最小的","title":"2.1 容器编排和资源管理平台"},{"content":"企业 IT 采用云原生架构所需的变革根本不是技术性的，而是企业文化和组织的变革，围绕消除造成浪费的结构、流程和活动。 在本节中，我们将研究必要的文化转变。\n从信息孤岛到 DevOps 企业 IT 通常被组织成以下许多孤岛：\n 软件开发 质量保证 数据库管理 系统管理 IT 运营 发布管理 项目管理  创建这些孤岛是为了让那些了解特定领域的人员来管理和指导那些执行该专业领域工作的人员。 这些孤岛通常具有不同的管理层次，工具集、沟通风格、词汇表和激励结构。这些差异启发了企业 IT 目标的不同范式，以及如何实现这一目标。\n但这里面存在很多矛盾，例如开发和运维分别对软件变更持有的观念就是个经常被提起的例子。开发的任务通常被视为通过开发软件功能为组织提供额外的价值。 这些功能本身就是向 IT 生态系统引入变更。 所以开发的使命可以被描述为 “交付变化”，而且经常根据有多少次变更来进行激励。\n相反，IT 运营的使命可以被描述为 “防止变更”。 IT 运营通常负责维护 IT 系统所需的可用性、弹性、性能和耐用性。因此，他们经常以维持关键绩效指标（KPI）来进行激励，例如平均故障间隔时间（MTBF）和 …","relpermalink":"/migrating-to-cloud-native-application-architectures/changes-needed/cultural-change/","summary":"企业 IT 采用云原生架构所需的变革根本不是技术性的，而是企业文化和组织的变革，围绕消除造成浪费的结构、流程和活动。 在本节中，我们将研究必要的文化转变。 从信息孤岛到 DevOps 企业 IT 通常被组织成以下许多孤岛： 软件开发","title":"2.1 文化变革"},{"content":"在和客户讨论分解数据、服务和团队后，客户经常向我提出这样的问题，“太棒了！但是我们要怎样实现呢？” 这是个好问题。如何拆分已有的单体应用并把他们迁移上云呢？\n事实证明，我已经看到了很多成功的例子，使用增量迁移这种相当可复制的模式，我现在向我所有的客户推荐这种模式。SoundCloud 和 Karma 就是公开的例子。\n本节中，我们将讲解如何一步步地将单体服务分解并将它们迁移到云上。\n新功能使用微服务形式 您可能感到很惊奇，第一步不是分解单体应用。我们假设您依然要在单体应用中构建服务。 事实上，如果您没有任何新的功能来构建，那么您甚至不应该考虑这个分解。（鉴于我们的主要动机是速度，您如何维持原状还能获取速度呢？）\n 团队决定，处理架构变化的最佳方法不是立即分解 Mothership 架构，而是不添加任何新的东西。我们所有的新功能以微服务形式构建…\n——Phil Calcado, SoundCloud\n 所以不要继续再向单体应用中增加代码，将所有的新功能以微服务的形式构建。这是第一步就要考虑好的，因为从头开始构架一个服务比分解一个单体应用并提出服务出来容易和快速的多。\n然而有一点不可避免， …","relpermalink":"/migrating-to-cloud-native-application-architectures/migration-cookbook/decomposition-recipes/","summary":"在和客户讨论分解数据、服务和团队后，客户经常向我提出这样的问题，“太棒了！但是我们要怎样实现呢？” 这是个好问题。如何拆分已有的单体应用并把他们迁移上云呢？ 事实证明，我已经看到了很多成功的例子，使用增量","title":"3.1 分解架构"},{"content":"DevSecOps 是一种软件开发、部署和生命周期管理方法，它涉及到从整个应用程序或平台的一次大型发布转变为持续集成、持续交付和持续部署（CI/CD）方法。这种转变又要求公司的 IT 部门的结构和工作流程发生变化。最明显的变化是组织一个 DevSecOps 小组，由软件开发人员、安全专家和 IT 运维专家组成，负责应用程序（即微服务）的每一部分。这个较小的团队不仅能促进最初的敏捷开发和部署的效率和效果，还能促进后续的生命周期管理活动，如监控应用行为、开发补丁、修复错误或扩展应用。这种具有三个领域专业知识的跨职能团队的组成，构成了在组织中引入 DevSecOps 的关键成功因素。\n","relpermalink":"/service-mesh-devsecops/devsecops/organizational-preparedness-for-devsecops/","summary":"DevSecOps 是一种软件开发、部署和生命周期管理方法，它涉及到从整个应用程序或平台的一次大型发布转变为持续集成、持续交付和持续部署（CI/CD）方法。这种转变又要求公司的 IT 部门的结构和工作流程发生变化。最明显的变","title":"3.1 组织对 DevSecOps 的准备情况"},{"content":"对上述五类代码（即应用、应用服务、基础设施、策略和监控）的简要描述如下：\n 应用程序代码和应用服务代码：前者包含一组特定业务事务的数据和应用逻辑，而后者包含所有服务的代码，如网络连接、负载均衡和网络弹性。 基础设施即代码（IaC）：用于提供和配置基础设施资源的代码，它以可重复和一致的方式承载 应用程序的部署。这种代码是用一种声明性语言编写的，当执行时，为正在部署的应用程序提供和配置基础设施。这种类型的代码就像在应用程序的微服务中发现的任何其他代码，只是它提供的是基础设施服务（例如，配置服务器）而不是事务服务（例如，在线零售应用程序的支付处理）。 策略即代码：描述了许多策略，包括安全策略，作为 可执行模块。一个例子是授权策略，它的代码包含了策略（如允许、拒绝等）和适用领域（如 RESTAPI 的方法，GET、PUT 等，路径等动词或工件）这段代码可以用特殊用途的策略语言（如 Rego）或常规应用中使用的语言（如 Go）编写。这段代码可能与 IaC 的配置代码有一些重合。然而，对于实施与特定于应用领域的关键安全服务相关的策略，需要一个单独的策略作为代码，驻留在参考平台的策略执行点（PEP） …","relpermalink":"/service-mesh-devsecops/implement/description-of-code-types-and-reference-platform-components/","summary":"对上述五类代码（即应用、应用服务、基础设施、策略和监控）的简要描述如下： 应用程序代码和应用服务代码：前者包含一组特定业务事务的数据和应用逻辑，而后者包含所有服务的代码，如网络连接、负载均衡和网络弹性。","title":"4.1 代码类型和参考平台组件的描述"},{"content":"还记得有一次告警电话半夜把我吵醒，发现是生产环境离线了。原来是系统瘫痪了，我们不得不要赔钱，这是我的错。\n从那一刻起，我就一直痴迷于构建坚如磐石的基础架构和基础架构管理系统，这样我就不会重蹈覆辙了。在我的职业生涯中，我为 Terraform、Kubernetes，一些编程语言和 Kops 做出过贡献，并创建了 Kubicorn。我不仅见证了系统基础架构的发展，而且我也帮助它完善。随着基础架构行业的发展，我们发现企业基础架构现在正以新的、令人兴奋的方式通过应用层管理。到目前为止，Kubernetes 是这种管理基础架构的新范例的最成熟的例子。\n我与人合著了这本书，部分地介绍了将基础架构作为云原生软件的新范例。此外，我希望鼓励基础架构工程师开始编写云原生应用程序。在这本书中，我们探讨了管理基础架构的丰富历史，并为云原生技术的未来定义了管理基础架构的模式。我们解释了基础架构由软件化 API 驱动的重要性。我们还探索了创建复杂系统的第一个基础架构组件的引导问题，并教授了扩展和测试基础架构的重要性。\n我于 2017 年加入 Heptio，担任资深布道师，并且很高兴能与行业中最聪明的系统工程师密切 …","relpermalink":"/cloud-native-infra/foreword/","summary":"还记得有一次告警电话半夜把我吵醒，发现是生产环境离线了。原来是系统瘫痪了，我们不得不要赔钱，这是我的错。 从那一刻起，我就一直痴迷于构建坚如磐石的基础架构和基础架构管理系统，这样我就不会重蹈覆辙了。在我","title":"前言"},{"content":"以下是关于本书的声明。\n许可 本出版物由 NIST 根据 2014 年《联邦信息安全现代化法案》（FISMA）（44 U.S.C. §3551 etseq）规定的法定职责编写，公共法律（P.L.）113-283。NIST 负责制定信息安全标准和准则，包括联邦信息系统的最低要求，但这些标准和准则在未经对国家安全系统行使策略权力的适当联邦官员明确批准的情况下，不得适用于这些系统。本准则与管理和预算办公室（OMB）A-130 号通知的要求一致。\n本出版物中的任何内容都不应被视为与商务部长根据法定授权对联邦机构的强制性和约束性标准和准则相抵触。这些准则也不应被解释为改变或取代商务部长、OMB 主任或任何其他联邦官员的现有权力。本出版物可由非政府组织在自愿的基础上使用，在美国不受版权限制。但是，请注明出处，NIST 将对此表示感谢。\n国家标准和技术研究所特别出版物 800-204C Natl.Inst. Stand.Technol.Spec.800-204C, 45 pages (March 2022) CODEN: NSPUE2\n本出版物可从以下网站免费获取。 …","relpermalink":"/service-mesh-devsecops/preface/","summary":"以下是关于本书的声明。 许可 本出版物由 NIST 根据 2014 年《联邦信息安全现代化法案》（FISMA）（44 U.S.C. §3551 etseq）规定的法定职责编写，公共法律（P.L.）113-283。NIST 负责制定信息安全标准","title":"声明"},{"content":"文件变更历史\n英文版\n   日期 版本 描述     2021 年 8 月 1.0 首次发布    中文版\n   日期 版本 描述     2021 年 8 月 8 日 1.0 首次发布    担保和认可的免责声明\n本文件中的信息和意见是 “按原样” 提供的，没有任何保证或担保。本文件以商品名称、商标、制造商或其他方式提及任何具体的商业产品、程序或服务，并不一定构成或暗示美国政府对其的认可、推荐或青睐，而且本指南不得用于广告或产品代言的目的。\n关于中文版\n中文版为 Jimmy Song 个人翻译，翻译过程中完全遵照原版，未做任何删减。其本人与本书的原作者没有任何组织或利益上的联系，翻译本书仅为交流学习之用。\n商标认可\n Kubernetes 是 Linux 基金会的注册商标。 SELinux 是美国国家安全局的注册商标。 AppArmor 是 SUSE LLC 的注册商标。 Windows 和 Hyper-V 是微软公司的注册商标。 ETCD 是 CoreOS, Inc. 的注册商标。 Syslog-ng 是 One Identity Software International …","relpermalink":"/kubernetes-hardening-guidance/notices-and-hitory/","summary":"文件变更历史 英文版 日期 版本 描述 2021 年 8 月 1.0 首次发布 中文版 日期 版本 描述 2021 年 8 月 8 日 1.0 首次发布 担保和认可的免责声明 本文件中的信息和意见是 “按原样” 提供的，没有任何保证或担保。本文件以","title":"通知和历史"},{"content":"Service Mesh 又译作 “服务网格”，作为服务间通信的基础设施层。Buoyant 公司的 CEO Willian Morgan 在他的这篇文章 WHAT’S A SERVICE MESH? AND WHY DO I NEED ONE? 中解释了什么是 Service Mesh，为什么云原生应用需要 Service Mesh。\n服务网格是用于处理服务间通信的专用基础设施层。它负责通过包含现代云原生应用程序的复杂服务拓扑来可靠地传递请求。实际上，服务网格通常通过一组轻量级网络代理来实现，这些代理与应用程序代码一起部署，而不需要感知应用程序本身。—— Willian Morgan Buoyant CEO\n服务网格（Service Mesh）这个术语通常用于描述构成这些应用程序的微服务网络以及应用之间的交互。随着规模和复杂性的增长，服务网格越来越难以理解和管理。它的需求包括服务发现、负载均衡、故障恢复、指标收集和监控以及通常更加复杂的运维需求，例如 A/B 测试、金丝雀发布、限流、访问控制和端到端认证等。\n服务网格的特点 服务网格有如下几个特点：\n 应用程序间通讯的中间层 轻量级网络 …","relpermalink":"/cloud-native-handbook/service-mesh/what-is-service-mesh/","summary":"Service Mesh 又译作 “服务网格”，作为服务间通信的基础设施层。Buoyant 公司的 CEO Willian Morgan 在他的这篇文章 WHAT’S A SERVICE MESH? AND WHY DO I NEED ONE? 中解释了什么是 Service Mesh，为什么云原生应用需要 Service Mesh。 服务网格是用于处理服","title":"什么是服务网格？"},{"content":"云原生（Cloud Native）这个词汇由来已久，以致于何时出现已无据可考。云原生开始大规模出现在受众视线中，与 Pivotal 提出的云原生应用的理念有着莫大的关系。我们现在谈到云原生，更多的指的是一种文化，而不具象为哪些技术体系。\n Pivotal 推出过 Pivotal Cloud Foundry 云原生应用平台和 Spring 开源 Java 开发框架，成为云原生应用架构中先驱者和探路者。Pivotal 是云原生应用平台第一股，2018 年在纽交所上市，2019 年底被 VMWare 以 27 亿美元收购，加入到 VMware 新的产品线 Tanzu。\n Pivotal 最初的定义 早在 2015 年 Pivotal 公司的 Matt Stine 写了一本叫做 迁移到云原生应用架构 的小册子，其中探讨了云原生应用架构的几个主要特征：\n 符合 12 因素应用 面向微服务架构 自服务敏捷架构 基于 API 的协作 抗脆弱性  笔者已于 2017 年翻译了本书，详见 迁移到云原生应用架构。\nCNCF 最初的定义 到了 2015 年 Google 主导成立了云原生计算基金 …","relpermalink":"/cloud-native-handbook/intro/what-is-cloud-native/","summary":"云原生（Cloud Native）这个词汇由来已久，以致于何时出现已无据可考。云原生开始大规模出现在受众视线中，与 Pivotal 提出的云原生应用的理念有着莫大的关系。我们现在谈到云原生，更多的指的是一种文化，而不具","title":"什么是云原生？"},{"content":"CNCF，全称Cloud Native Computing Foundation（云原生计算基金会），成立于 2015 年7月21日（于美国波特兰OSCON 2015上宣布），其最初的口号是坚持和整合开源技术来让编排容器作为微服务架构的一部分，其作为致力于云原生应用推广和普及的一支重要力量，不论您是云原生应用的开发者、管理者还是研究人员都有必要了解。\nCNCF作为一个厂商中立的基金会，致力于Github上的快速成长的开源技术的推广，如Kubernetes、Prometheus、Envoy等，帮助开发人员更快更好的构建出色的产品。CNCF 维护了一个全景图项目，详见 GitHub。\n关于CNCF的使命与组织方式请参考CNCF章程，概括的讲CNCF的使命包括以下三点：\n 容器化包装。 通过中心编排系统的动态资源管理。 面向微服务。  CNCF这个角色的作用是推广技术，形成社区，开源项目管理与推进生态系统健康发展。\n另外CNCF组织由以下部分组成：\n 会员：白金、金牌、银牌、最终用户、学术和非赢利成员，不同级别的会员在治理委员会中的投票权不同。 理事会：负责事务管理 TOC（技术监督委员 …","relpermalink":"/cloud-native-handbook/community/cncf/","summary":"CNCF，全称Cloud Native Computing Foundation（云原生计算基金会），成立于 2015 年7月21日（于美国波特兰OSCON 2015上宣布），其最初的口号是坚持和整合开源技术来让编排容器作为微服务架构的一部分，","title":"云原生计算基金会（CNCF）"},{"content":"软件开发的模式又一次改变了。开源软件和公有云供应商已经从根本上改变了我们构建和部署软件的方式。有了开源软件，我们的应用不再需要从头开始编码。而通过使用公有云供应商，我们不再需要配置服务器或连接网络设备。所有这些都意味着，你可以在短短几天甚至几小时内从头开始构建和部署一个应用程序。\n但是，仅仅因为部署新的应用程序很容易，并不意味着操作和维护它们也变得更容易。随着应用程序变得更加复杂，更加异质，最重要的是，更加分布式，看清大局，以及准确地指出问题发生的地方，变得更加困难。\n但有些事情并没有改变：作为开发者和运维，我们仍然需要能够从用户的角度理解应用程序的性能，我们仍然需要对用户的事务有一个端到端的看法。我们也仍然需要衡量和说明在处理这些事务的过程中资源是如何被消耗的。也就是说，我们仍然需要可观测性。\n但是，尽管对可观测性的需求没有改变，我们实施可观测性解决方案的方式必须改变。本报告详细介绍了关于可观测性的传统思维方式对现代应用的不足：继续将可观测性作为一系列工具（尤其是作为 “三大支柱”）来实施，几乎不可能以可靠的方式运行现代应用。\nOpenTelemetry 通过提供一种综合的方法来收集 …","relpermalink":"/opentelemetry-obervability/foreword/","summary":"前言","title":"前言"},{"content":"代码审查的主要目的是确保逐步改善 Google 代码库的整体健康状况。代码审查的所有工具和流程都是为此而设计的。\n为了实现此目标，必须做出一系列权衡。\n首先，开发人员必须能够对任务进行改进。如果开发者从未向代码库提交过代码，那么代码库的改进也就无从谈起。此外，如果审核人员对代码吹毛求疵，那么开发人员以后也很难再做出改进。\n另外，审查者有责任确保随着时间的推移，CL 的质量不会使代码库的整体健康状况下降。这可能很棘手，因为通常情况下，代码库健康状况会随着时间的而下降，特别是在对团队有严格的时间要求时，团队往往会采取捷径来达成他们的目标。\n此外，审查者应对正在审核的代码负责并拥有所有权。审查者希望确保代码库保持一致、可维护及 Code Review 要点中所提及的所有其他内容。\n因此，我们将以下规则作为 Code Review 中期望的标准：\n一般来说，审核人员应该倾向于批准 CL，只要 CL 确实可以提高系统的整体代码健康状态，即使 CL 并不完美。\n这是所有 Code Review 指南中的高级原则。\n当然，也有一些限制。例如，如果 CL 添加了审查者认为系统中不需要的功能，那么即使代 …","relpermalink":"/eng-practices/review/reviewer/standard/","summary":"代码审查的主要目的是确保逐步改善 Google 代码库的整体健康状况。代码审查的所有工具和流程都是为此而设计的。 为了实现此目标，必须做出一系列权衡。 首先，开发人员必须能够对任务进行改进。如果开发者从未向代码库提交过","title":"Code Review 标准"},{"content":"CL 描述是进行了哪些更改以及为何更改的公开记录。CL 将作为版本控制系统中的永久记录，可能会在长时期内被除审查者之外的数百人阅读。\n开发者将来会根据描述搜索您的 CL。有人可能会仅凭有关联性的微弱印象，但没有更多具体细节的情况下，来查找你的改动。如果所有重要信息都在代码而不是描述中，那么会让他们更加难以找到你的 CL 。\n首行  正在做什么的简短摘要。 完整的句子，使用祈使句。 后面跟一个空行。  CL 描述的第一行应该是关于这个 CL 是做什么的简短摘要，后面跟一个空白行。这是将来大多数的代码搜索者在浏览代码的版本控制历史时，最常被看到的内容，因此第一行应该提供足够的信息，以便他们不必阅读 CL 的整个描述就可以获得这个 CL 实际上是做了什么的信息。\n按照传统，CL 描述的第一行应该是一个完整的句子，就好像是一个命令（一个命令句）。例如，“Delete the FizzBuzz RPC and replace it with the new system.”而不是“Deleting the FizzBuzz RPC and replacing it with the new …","relpermalink":"/eng-practices/review/developer/cl-descriptions/","summary":"CL 描述是进行了哪些更改以及为何更改的公开记录。CL 将作为版本控制系统中的永久记录，可能会在长时期内被除审查者之外的数百人阅读。 开发者将来会根据描述搜索您的 CL。有人可能会仅凭有关联性的微弱印象，但没有","title":"写好 CL 描述"},{"content":"标签 标签（Label）是一种通用的、灵活的和高度可扩展的方式，可以用来处理大量资源，因为我们可以用它来对事物任意分组和创建集合。每当需要描述、解决或选择某物时，它都是基于标签完成的：\n 端点 被分配了从容器运行时、编排系统或其他来源派生的标签。 网络策略 根据标签选择允许通信的 端点对，策略本身也由标签标识。  什么是标签？ 标签是一对由 key 和 value 组成的字符串。可以将标签格式化为具有 key=value 的单个字符串。key 部分是强制性的，并且必须是唯一的。这通常是通过使用反向域名概念来实现的，例如 io.cilium.mykey=myvalue。value 部分是可选的，可以省略，例如 io.cilium.mykey.\n键名通常应由字符集组成 [a-z0-9-.]。\n当使用标签选择资源时，键和值都必须匹配，例如，当一个策略应该应用于所有带有标签 my.corp.foo 的端点时，标签 my.corp.foo=bar 不会与该选择器匹配。\n标签来源 标签可以来自各种来源。例如，端点 将通过本地容器运行时派生与容器关联的标签，以及与 Kubernetes …","relpermalink":"/cilium-handbook/concepts/terminology/","summary":"标签 标签（Label）是一种通用的、灵活的和高度可扩展的方式，可以用来处理大量资源，因为我们可以用它来对事物任意分组和创建集合。每当需要描述、解决或选择某物时，它都是基于标签完成的： 端点 被分配了从容器","title":"Cilium 术语说明"},{"content":"IP 地址管理（IPAM）负责分配和管理由 Cilium 管理的网络端点（容器和其他）使用的 IP 地址。Cilium 支持以下各种 IPAM 模式，以满足不同用户的需求。\n 集群范围（默认） Kubernetes 主机范围 Azure IPAM AWS ENI Google Kubernetes Engine CRD 支持  集群范围 集群范围 IPAM 模式将 PodCIDR 分配给每个节点，并使用每个节点上的主机范围分配器分配 IP。因此它类似于 Kubernetes 主机范围模式。区别在于 Kubernetes 不是通过 Kubernetes v1.Node资源分配每个节点的 PodCIDR，而是 Cilium Operator 通过 v2.CiliumNode 资源管理每个节点的 PodCIDR。这种模式的优点是它不依赖于 Kubernetes 被配置为分发每个节点的 PodCIDR。\n架构    架构图  如果无法将 Kubernetes 配置为分发 PodCIDR 或需要更多控制，这将非常有用。\n在这种模式下，Cilium 代理将在启动时等待，直到 PodCIDRs 范围 …","relpermalink":"/cilium-handbook/networking/ipam/","summary":"IP 地址管理（IPAM）负责分配和管理由 Cilium 管理的网络端点（容器和其他）使用的 IP 地址。Cilium 支持以下各种 IPAM 模式，以满足不同用户的需求。 集群范围（默认） Kubernetes 主机范围 Azure IPAM AWS ENI Google Kubernetes Engine CRD 支持 集群范围 集群范围 IPAM","title":"IP 地址管理（IPAM）"},{"content":"部署 标准 Cilium Kubernetes 部署的配置包括几个 Kubernetes 资源：\n DaemonSet  资源：描述部署到每个 Kubernetes 节点的 Cilium pod 。这个 pod 运行 cilium-agent 和相关的守护进程。这个 DaemonSet 的配置包括指示 Cilium docker 容器的确切版本（例如 v1.0.0）的镜像标签和传递给 cilium-agent 的命令行选项。 资源：描述传递给 cilium-agent 的ConfigMap 常用配置值，例如 kvstore 端点和凭据、启用/禁用调试模式等。 ServiceAccount、ClusterRole 和 ClusterRoleBindings 资源：当启用 Kubernetes RBAC 时，cilium-agent` 用于访问 Kubernetes API 服务器的身份和权限。 资源：如果 Secret 需要，描述用于访问 etcd kvstore 的凭据。  现有 Pod 的联网 如果在部署 Cilium 之前 pod 已经在运行 DaemonSet，这些 pod 仍将 …","relpermalink":"/cilium-handbook/kubernetes/concepts/","summary":"部署 标准 Cilium Kubernetes 部署的配置包括几个 Kubernetes 资源： DaemonSet 资源：描述部署到每个 Kubernetes 节点的 Cilium pod 。这个 pod 运行 cilium-agent 和相关的守护进程。这个 DaemonSet 的配置包括指示 Cilium docker 容器的确切版本（例如 v1.0.0）的镜像标签和传递给 cilium-agent 的命令行选项。 资源","title":"概念"},{"content":"Kubernetes 等容器管理系统部署了一个网络模型，该模型为每个 pod（容器组）分配一个单独的 IP 地址。这确保了架构的简单性，避免了不必要的网络地址转换 （NAT），并为每个单独的容器提供了全范围的端口号以供使用。这种模型的逻辑结果是，根据集群的大小和 pod 的总数，网络层必须管理大量的 IP 地址。\n在传统上，安全策略基于 IP 地址过滤器。下面是一个简单的例子。如果所有具有标签 role=frontend 的 pod 应该被允许发起与所有具有标签 role=backend 的 pod 的连接，那么每个运行至少一个具有标签 role=backend 的 pod 的集群节点必须安装一个相应的过滤器，允许所有 role=frontend pod 的所有 IP 地址发起与所有本地 role=backend pod 的 IP 地址的连接。所有其他的连接请求都应该被拒绝。这可能看起来像这样。如果目标地址是 10.1.1.2，那么只有当源地址是下列之一时才允许连接 [10.1.2.2,10.1.2.3,20.4.9.1]。\n每次启动或停止带有 role=frontend …","relpermalink":"/cilium-handbook/security/identity/","summary":"Kubernetes 等容器管理系统部署了一个网络模型，该模型为每个 pod（容器组）分配一个单独的 IP 地址。这确保了架构的简单性，避免了不必要的网络地址转换 （NAT），并为每个单独的容器提供了全范围的端口号以供使用。这种模","title":"基于身份"},{"content":"虽然监控数据路径状态提供对数据路径状态的自省，但默认情况下它只会提供对三层/四层数据包事件的可视性。如果配置了 七层示例，则可以查看七层协议，但这需要编写每个选定端点的完整策略。 为了在不配置完整策略的情况下获得对应用程序的更多可视性，Cilium 提供了一种在与 Kubernetes 一起运行时通过注解来规定可视性的方法。\n可视性信息由注解中以逗号分隔的元组列表表示：\n\u0026lt;{Traffic Direction}/{L4 Port}/{L4 Protocol}/{L7 Protocol}\u0026gt;\n例如：\n\u0026lt;Egress/53/UDP/DNS\u0026gt;,\u0026lt;Egress/80/TCP/HTTP\u0026gt;  为此，你可以在 Kubernetes YAML 中或通过命令行提供注释，例如：\nkubectl annotate pod foo -n bar io.cilium.proxy-visibility=\u0026#34;\u0026lt;Egress/53/UDP/DNS\u0026gt;,\u0026lt;Egress/80/TCP/HTTP\u0026gt;\u0026#34; Cilium 将拾取 pod 已收到这些注释，并将透明地将流量重定向到代理，以便显示 cilium monitor 流量的输出被重 …","relpermalink":"/cilium-handbook/policy/visibility/","summary":"虽然监控数据路径状态提供对数据路径状态的自省，但默认情况下它只会提供对三层/四层数据包事件的可视性。如果配置了 七层示例，则可以查看七层协议，但这需要编写每个选定端点的完整策略。 为了在不配置完整策略的情","title":"七层可视性"},{"content":"端点到端点 首先，我们使用可选的七层出口和入口策略显示本地端点到端点的流程。随后是启用了套接字层强制的同一端点到端点流。为 TCP 流量启用套接字层实施后，启动连接的握手将遍历端点策略对象，直到 TCP 状态为 ESTABLISHED。然后在建立连接后，只需要七层策略对象。\n   端点到端点的流程  端点到出口 接下来，我们使用可选的 overlay 网络显示本地端点到出口。在可选的覆盖网络中，网络流量被转发到与 overlay 网络对应的 Linux 网络接口。在默认情况下，overlay 接口名为 cilium_vxlan。与上面类似，当启用套接字层强制并使用七层代理时，我们可以避免在端点和 TCP 流量的七层策略之间运行端点策略块。如果启用，可选的 L3 加密块将加密数据包。\n   端点到出口的流程  入口到端点 最后，我们还使用可选的 overlay 网络显示到本地端点的入口。与上述套接字层强制类似，可用于避免代理和端点套接字之间的一组策略遍历。如果数据包在接收时被加密，则首先将其解密，然后通过正常流程进行处理。\n   入口到端点流程  这样就完成了数据路径概述。更多 BPF  …","relpermalink":"/cilium-handbook/ebpf/lifeofapacket/","summary":"端点到端点 首先，我们使用可选的七层出口和入口策略显示本地端点到端点的流程。随后是启用了套接字层强制的同一端点到端点流。为 TCP 流量启用套接字层实施后，启动连接的握手将遍历端点策略对象，直到 TCP 状态为 ESTA","title":"数据包流程"},{"content":"现在我们将探索云原生应用架构的几个主要特征，和这些特征是如何解决我们前面提到的使用云原生应用架构的动机。\n12 因素应用 12 因素应用是一系列云原生应用架构的模式集合，最初由 Heroku 提出。这些模式可以用来说明什么样的应用才是云原生应用。它们关注速度、安全、通过声明式配置扩展、可横向扩展的无状态 / 无共享进程以及部署环境的整体松耦合。如 Cloud Foundry、Heroku 和 Amazon ElasticBeanstalk 都对部署 12 因素应用进行了专门的优化。\n在 12 因素的背景下，应用（或者叫 app）指的是独立可部署单元。组织中经常把一些互相协作的可部署单元称作一个应用。\n12 因素应用遵循以下模式：\n代码库\n每个可部署 app 在版本控制系统中都有一个独立的代码库，可以在不同的环境中部署多个实例。\n依赖\nApp 应该使用适当的工具（如 Maven、Bundler、NPM）来对依赖进行显式的声明，而不该在部署环境中隐式的实现依赖。\n配置\n配置或其他随发布环境（如部署、staging、生产）而变更的部分应当作为操作系统级的环境变量注入。\n后端服务\n后端服务，例 …","relpermalink":"/migrating-to-cloud-native-application-architectures/the-rise-of-cloud-native/defining-cloud-native-architectures/","summary":"现在我们将探索云原生应用架构的几个主要特征，和这些特征是如何解决我们前面提到的使用云原生应用架构的动机。 12 因素应用 12 因素应用是一系列云原生应用架构的模式集合，最初由 Heroku 提出。这些模式可以用来说明什么样的","title":"1.2 云原生架构的定义"},{"content":"在看了基于微服务的应用所需的各种应用服务后，考虑一下提供这些服务的服务网格的架构。服务网格由两个主要部分组成：控制平面和数据平面。\n2.2.1 控制平面 控制平面有几个组件。虽然服务网格的数据面主要由作为容器运行在与应用容器相同的 Pod 中的代理组成，但控制面组件在它们自己的 Pod、节点和相关集群中运行。以下是控制平面的 各种功能：\n Envoy sidecar 代理的服务发现和配置 自动化的密钥和证书管理 用于策略定义和收集遥测数据的 API 服务网格组件的配置摄取 管理一个到服务网格的入站连接（入站网关） 管理来自服务网格的出站连接（出口网关） 将 sidecar 代理注入那些托管应用程序微服务容器的 Pod、节点或命名空间中  总的来说，控制平面帮助管理员用配置数据填充数据平面组件，这些数据是由控制平面的策略产生的。上述功能 3 的策略可能包括网络路由策略、负载均衡策略、蓝绿部署的策略、金丝雀部署、超时、重试和断路能力。这后三项被统称为网络基础设施服务的弹性能力的特殊名称。最后要说的是与安全相关的策略（例如，认证和授权策略、TLS 建立策略等）。这些策略规则由一个模块解析，该 …","relpermalink":"/service-mesh-devsecops/reference-platform/service-mesh-software-architecture/","summary":"在看了基于微服务的应用所需的各种应用服务后，考虑一下提供这些服务的服务网格的架构。服务网格由两个主要部分组成：控制平面和数据平面。 2.2.1 控制平面 控制平面有几个组件。虽然服务网格的数据面主要由作为容器运行在","title":"2.2 服务网格架构"},{"content":"在本节中，我们将探讨采用云原生应用架构的组织在创建团队时需要进行的变革。这个重组背后的理论是著名的康威定律。我们的解决方案是在长周期的产品开发中，创建一个包含了各方面专业员工的团队，而不是将他们分离在单一的团队中，例如测试人员。\n业务能力团队  设计系统的组织，最终产生的设计等同于组织之内、之间的沟通结构。\n——Melvyn Conway\n 我们已经讨论了 “从孤岛到 DevOps” 将 IT 组织成专门的信息孤岛的做法。我们很自然地创造了这些孤岛，并把个体放到与这些孤岛一致的团队中。但是当我们需要构建一个新的软件的时候会怎么样？\n一个很常见的做法是成立一个项目团队。该团队向项目经理汇报，然后项目经理与各种孤岛合作，为项目所需的各个专业领域寻求 “资源”。正如康威定律所言，这些团队将很自然地在系统中构筑起各种孤岛，我们最终得到的是联合各种孤岛相对应的孤立模块的架构：\n 数据访问层 服务层 Web MVC 层 消息层 等等  这些层次中的每一层都跨越了多个业务能力领域，使得在其之上的创新和部署独立于其他业务能力的新功能变的非常困难。\n寻求迁移到将业务能力分离的微服务等云原生架构的公司经常 …","relpermalink":"/migrating-to-cloud-native-application-architectures/changes-needed/organizational-change/","summary":"在本节中，我们将探讨采用云原生应用架构的组织在创建团队时需要进行的变革。这个重组背后的理论是著名的康威定律。我们的解决方案是在长周期的产品开发中，创建一个包含了各方面专业员工的团队，而不是将他们分离在","title":"2.2 组织变革"},{"content":"DevSecOps 是一个敏捷的、自动化的开发和部署过程，它使用称为 CI/CD 管道的原语，在自动化工具的帮助下，将软件从构建阶段带到部署阶段，最后到运行时间 / 操作阶段。这些管道是将开发者的源代码带过各个阶段的工作流程，如构建、测试、打包、交付，以及在各个阶段由测试工具支持的部署。\nDevSecOps 平台是指各种 CI/CD 管道（针对每种代码类型）运行的资源集合。至少，这个平台由以下部分组成。\n(a) 管道软件\n CI 软件 —— 从代码库中提取代码，调用构建软件，调用测试工具，并将测试后的工件存储到图像注册表中。 CD 软件 —— 拉出工件、软件包，并根据 IaC 中的计算、网络和存储资源描述，部署软件包。  (b) SDLC 软件\n 构建工具（例如，IDE） 测试工具（SAST、DAST、SCA）  (c) 存储库\n 源代码库（如 GitHub） 容器镜像存储库或注册表  (d) 可观测性或监测工具\n 日志和日志聚合工具 产生指标的工具 追踪工具（应用程序的调用顺序） 可视化工具（结合上述数据生成仪表盘 / 警报）。  在 DevSecOps 平台中，通过内置的设计功能（ …","relpermalink":"/service-mesh-devsecops/devsecops/devsecops-platform/","summary":"DevSecOps 是一个敏捷的、自动化的开发和部署过程，它使用称为 CI/CD 管道的原语，在自动化工具的帮助下，将软件从构建阶段带到部署阶段，最后到运行时间 / 操作阶段。这些管道是将开发者的源代码带过各个阶段的工作流程，如构建、","title":"3.2 DevSecOps 平台"},{"content":"当我们开始构建由微服务组成的分布式系统时，我们还会遇到在开发单体应用时通常不会遇到的非功能性要求。有时，使用物理定律就可以解决这些问题，例如一致性、延迟和网络分区问题。然而，脆弱性和易控性的问题通常可以使用相当通用的模式来解决。在本节中，我们将介绍帮助我们解决这些问题的方法。\n这些方法来自于 Spring Cloud 项目和 Netflix OSS 系列项目的组合。\n版本化和分布式配置 在 “12 因素应用 “中我们讨论过通过操作系统级环境变量为应用注入对应的配置，强调了这种配置管理方式的重要性。这种方式特别适合简单的系统，但是，当系统扩大后，有时我们还需要附加的配置能力：\n 为调试一个生产上的问题而变更运行的应用程序日志级别 更改 message broker 中接收消息的线程数 报告所有对生产系统配置所做的更改以支持审计监管 运行中的应用切换功能开关 保护配置中的机密信息（如密码）  为了支持这些特性，我们需要配置具有以下特性的配置管理方法：\n 版本控制 可审计 加密 在线刷新  Spring Cloud 项目中包含的一个可提供这些功能的配置服务器。此配置服务器通过 Git 本地仓 …","relpermalink":"/migrating-to-cloud-native-application-architectures/migration-cookbook/distributed-systems-recipes/","summary":"当我们开始构建由微服务组成的分布式系统时，我们还会遇到在开发单体应用时通常不会遇到的非功能性要求。有时，使用物理定律就可以解决这些问题，例如一致性、延迟和网络分区问题。然而，脆弱性和易控性的问题通常可","title":"3.2 使用分布式系统"},{"content":"应用程序代码和应用服务代码驻留在容器编排和资源管理平台中，而实现与之相关的工作流程的 CI/CD 软件通常驻留在同一平台中。应使用第 4.6 节所述的步骤对该管道进行保护，该管道控制下的应用程序代码应接受第 4.8 节所述的安全测试。此外，应用程序所在的调度平台本身应使用运行时安全工具（如 Falco）进行保护，该工具可以实时读取操作系统内核日志、容器日志和平台日志，并根据威胁检测规则引擎对其进行处理，以提醒用户注意恶意行为（例如，创建有特权的容器、未经授权的用户读取敏感文件等）。它们通常有一套默认（预定义）的规则，可以在上面添加自定义规则。在平台上安装它们，可以为集群中的每个节点启动代理，这些代理可以监控在该节点的各个 Pod 中运行的容器。这种类型的工具的优点是，它补充了现有平台的本地安全措施，如访问控制模型和 Pod 安全策略，通过实际检测它们的发生来 防止漏洞。\n","relpermalink":"/service-mesh-devsecops/implement/ci-cd-pipeline-for-application-code-and-application-services-code/","summary":"应用程序代码和应用服务代码驻留在容器编排和资源管理平台中，而实现与之相关的工作流程的 CI/CD 软件通常驻留在同一平台中。应使用第 4.6 节所述的步骤对该管道进行保护，该管道控制下的应用程序代码应接受第 4.8 节所述的安全","title":"4.2 应用程序代码和应用服务代码的 CI/CD 管道"},{"content":"HCM 是一个网络级的过滤器，将原始字节转译成 HTTP 级别的消息和事件（例如，收到的 Header，收到的 Body 数据等）。\nHCM 过滤器还处理标准的 HTTP 功能。它支持访问记录、请求 ID 生成和跟踪、Header 操作、路由表管理和统计等功能。\n从协议的角度来看，HCM 原生支持 HTTP/1.1、WebSockets、HTTP/2 和 HTTP/3（仍在 Alpha 阶段）。\nEnvoy 代理被设计成一个 HTTP/2 复用代理，这体现在描述 Envoy 组件的术语中。\nHTTP/2 术语\n在 HTTP/2 中，流是已建立的连接中的字节的双向流动。每个流可以携带一个或多个消息（message）。消息是一个完整的帧（frame）序列，映射到一个 HTTP 请求或响应消息。最后，帧是 HTTP/2 中最小的通信单位。每个帧都包含一个帧头（frame header），它至少可以识别该帧所属的流。帧可以携带有关 HTTP Header、消息有效载荷等信息。\n无论流来自哪个连接（HTTP/1.1、HTTP/2 或 HTTP/3），Envoy …","relpermalink":"/envoy-handbook/hcm/introduction/","summary":"HCM 是一个网络级的过滤器，将原始字节转译成 HTTP 级别的消息和事件（例如，收到的 Header，收到的 Body 数据等）。 HCM 过滤器还处理标准的 HTTP 功能。它支持访问记录、请求 ID 生成和跟踪、Header 操作、路由表管理和统计","title":"HTTP 连接管理器（HCM）简介"},{"content":"前面提到的路由器过滤器（envoy.filters.http.router）就是实现 HTTP 转发的。路由器过滤器几乎被用于所有的 HTTP 代理方案中。路由器过滤器的主要工作是查看路由表，并对请求进行相应的路由（转发和重定向）。\n路由器使用传入请求的信息（例如，host  或 authority 头），并通过虚拟主机和路由规则将其与上游集群相匹配。\n所有配置的 HTTP 过滤器都使用包含路由表的路由配置（route_config）。尽管路由表的主要消费者将是路由器过滤器，但其他过滤器如果想根据请求的目的地做出任何决定，也可以访问它。\n一组虚拟主机构成了路由配置。每个虚拟主机都有一个逻辑名称，一组可以根据请求头被路由到它的域，以及一组指定如何匹配请求并指出下一步要做什么的路由。\nEnvoy 还支持路由级别的优先级路由。每个优先级都有其连接池和断路设置。目前支持的两个优先级是 DEFAULT 和 HIGH。如果我们没有明确提供优先级，则默认为 DEFAULT。\n这里有一个片段，显示了一个路由配置的例子。\nroute_config:name:my_route_config# 用于统计的名 …","relpermalink":"/envoy-handbook/hcm/http-routing/","summary":"前面提到的路由器过滤器（envoy.filters.http.router）就是实现 HTTP 转发的。路由器过滤器几乎被用于所有的 HTTP 代理方案中。路由器过滤器的主要工作是查看路由表，并对请求进行相应的路由（转发","title":"HTTP 路由"},{"content":"作者\nCybersecurity and Infrastructure Security Agency (CISA)\nNational Security Agency (NSA) Cybersecurity Directorate Endpoint Security\n联系信息\n客户要求 / 一般网络安全问题。\n网络安全需求中心，410-854-4200，Cybersecurity_Requests@nsa.gov。\n媒体咨询 / 新闻台\n媒体关系，443-634-0721，MediaRelations@nsa.gov。\n关于事件响应资源，请联系 CISA：CISAServiceDesk@cisa.dhs.gov。\n中文版\n关于本书中文版的信息请联系 Jimmy Song：jimmysong@jimmysong.io。\n宗旨\n国家安全局和 CISA 制定本文件是为了促进其各自的网络安全，包括其制定和发布网络安全规范和缓解措施的责任。这一信息可以被广泛分享，以触达所有适当的利益相关者。\n","relpermalink":"/kubernetes-hardening-guidance/publication-information/","summary":"作者 Cybersecurity and Infrastructure Security Agency (CISA) National Security Agency (NSA) Cybersecurity Directorate Endpoint Security 联系信息 客户要求 / 一般网络安全问题。 网络安全需求中心，410-854-4200，Cybersecurity_Requests@nsa.gov。 媒体咨询 / 新闻台 媒体关系，","title":"出版信息"},{"content":"基础设施技术的历史向来引人入胜。由于基础设施的规模巨大，它已经经历了一次快速的颠覆性变革。除了计算机和互联网的早期，基础设施可谓日新月异。这些创新使基础架构更快，更可靠，更有价值。\n有些公司的人将基础设施推到了极限，他们已经找到了自动化和抽象的方法，提取基础设施更多商业价值。通过提供灵活的可用资源，他们将曾经是昂贵的成本中心转变为所需的商业公用事业。\n然而，公共事业公司很少为企业提供财务价值，这意味着基础设施往往被忽略并被视为不必要的成本。这使得投入创新或改进的时间和金钱很少。\n这样一个如此简单且令人着迷的业务栈怎么可以被轻易忽略？当基础设施出现故障时，业务显然会受到重视，那么为什么基础设施很难改善呢？\n基础设施已经达到了使消费者都感到无聊的成熟度。然而，它的潜力和新挑战又激发了实施者和工程师们新的激情。\n扩展基础设施并使用新的业务方式让来自不同行业的工程师都能找到解决方案。开源软件（OSS）和社区间的互相协作，这股力量又促使了创新的激增。\n如果管理得当，今天基础设施和应用方面的挑战将会不一样。这使得基础设施建设者和维护人员可以取得进展并开展新的有意义的工作。\n有些公司克服了诸如可扩展 …","relpermalink":"/cloud-native-infra/introduction/","summary":"基础设施技术的历史向来引人入胜。由于基础设施的规模巨大，它已经经历了一次快速的颠覆性变革。除了计算机和互联网的早期，基础设施可谓日新月异。这些创新使基础架构更快，更可靠，更有价值。 有些公司的人将基础设","title":"介绍"},{"content":"IT 行业正在向微服务架构和云原生解决方案发展。由于使用不同的技术开发了成百上千的微服务，这些系统可能变得复杂，难以调试。\n作为一个应用开发者，你考虑的是业务逻辑——购买产品或生成发票。然而，任何像这样的业务逻辑都会导致不同服务之间的多个服务调用。每个服务可能都有它的超时、重试逻辑和其他可能需要调整或微调的网络特定代码。\n如果在任何时候最初的请求失败了，就很难通过多个服务来追踪，准确地指出失败发生的地方，了解请求为什么失败。是网络不可靠吗？是否需要调整重试或超时？或者是业务逻辑问题或错误？\n服务可能使用不一致的跟踪和记录机制，使这种调试的复杂性增加。这些问题使你很难确定问题发生在哪里，以及如何解决。如果你是一个应用程序开发人员，而调试网络问题不属于你的核心技能，那就更是如此。\n将网络问题从应用程序堆栈中抽离出来，由另一个组件来处理网络部分，让调试网络问题变得更容易。这就是 Envoy 所做的事情。\n在每个服务实例旁边都有一个 Envoy 实例在运行。这种类型的部署也被称为 Sidecar 部署。Envoy 的另一种模式是边缘代理，用于构建 API 网关。\nEnvoy 和应用程序形成一个 …","relpermalink":"/envoy-handbook/intro/what-is-envoy/","summary":"IT 行业正在向微服务架构和云原生解决方案发展。由于使用不同的技术开发了成百上千的微服务，这些系统可能变得复杂，难以调试。 作为一个应用开发者，你考虑的是业务逻辑——购买产品或生成发票。然而，任何像这样的业","title":"什么是 Envoy？"},{"content":"云原生应用已经发展成为一个标准化的架构，由以下部分组成。\n 多个松散耦合的组件被称为微服务（通常或典型地以容器形式实现）。 一个应用服务基础设施，为用户、服务和设备提供安全通信、认证和授权等服务（例如，服务网格）。  由于安全、商业竞争力和其固有的结构（松散耦合的应用组件），这类应用需要一个不同的应用、部署和运行时监控范式 —— 统称为软件生命周期范式。DevSecOps（分别由开发、安全和运维的首字母缩写组成）是这些应用的开发、部署和运维的促进范式之一，其基本要素包括持续集成、持续交付和持续部署（CI/CD）管道。\nCI/CD 管道是将开发人员的源代码通过各个阶段的工作流程，如构建、功能测试、安全扫描漏洞、打包和部署，由带有反馈机制的自动化工具支持。在本文中，应用环境中涉及的整个源代码集被分为五种代码类型：\n 应用代码，它体现了执行一个或多个业务功能的应用逻辑。 应用服务代码，用于服务，如会话建立、网络连接等。 基础设施即代码，它是以声明性代码的形式存在的计算、网络和存储资源。 策略即代码，这是运行时策略（例如，零信任），以声明性代码的形式表达。 可观测性即代码，用于持续监测应用程序 …","relpermalink":"/service-mesh-devsecops/executive-summary/","summary":"云原生应用已经发展成为一个标准化的架构，由以下部分组成。 多个松散耦合的组件被称为微服务（通常或典型地以容器形式实现）。 一个应用服务基础设施，为用户、服务和设备提供安全通信、认证和授权等服务（例如，服务","title":"执行摘要"},{"content":"云原生一词已经被过度的采用，很多软件都号称是云原生，很多打着云原生旗号的会议也如雨后春笋般涌现。\n云原生本身甚至不能称为是一种架构，它首先是一种基础设施，运行在其上的应用称作云原生应用，只有符合云原生设计哲学的应用架构才叫云原生应用架构。\n云原生的设计理念 云原生系统的设计理念如下:\n 面向分布式设计（Distribution）：容器、微服务、API 驱动的开发； 面向配置设计（Configuration）：一个镜像，多个环境配置； 面向韧性设计（Resistancy）：故障容忍和自愈； 面向弹性设计（Elasticity）：弹性扩展和对环境变化（负载）做出响应； 面向交付设计（Delivery）：自动拉起，缩短交付时间； 面向性能设计（Performance）：响应式，并发和资源高效利用； 面向自动化设计（Automation）：自动化的 DevOps； 面向诊断性设计（Diagnosability）：集群级别的日志、metric 和追踪； 面向安全性设计（Security）：安全端点、API Gateway、端到端加密；  以上的设计理念很多都是继承自分布式应用的设计理念。虽然有如 …","relpermalink":"/cloud-native-handbook/intro/cloud-native-philosophy/","summary":"云原生一词已经被过度的采用，很多软件都号称是云原生，很多打着云原生旗号的会议也如雨后春笋般涌现。 云原生本身甚至不能称为是一种架构，它首先是一种基础设施，运行在其上的应用称作云原生应用，只有符合云原生设","title":"云原生的设计哲学"},{"content":"云原生社区是由 宋净超（Jimmy Song） 于 2020 年 5 月发起的，企业中立的云原生终端用户社区。社区秉持 “共识、共治、共建、共享” 的原则。社区的宗旨是：连接、中立、开源。立足中国，面向世界，企业中立，关注开源，回馈开源。了解更多请访问云原生社区官网：https://cloudnative.to。\n成立背景  Software is eating the world. —— Marc Andreessen\n “软件正在吞噬这个世界” 已被大家多次引用，随着云原生（Cloud Native）的崛起，我们想说的是 “Cloud Native is eating the software”。随着越来越多的企业将服务迁移上云，企业原有的开发模式以及技术架构已无法适应云的应用场景，其正在被重塑，向着云原生的方向演进。\n那么什么是云原生？云原生是一系列架构、研发流程、团队文化的最佳实践组合，以此支撑更快的创新速度、极致的用户体验、稳定可靠的用户服务、高效的研发效率。开源社区与云原生的关系密不可分，正是开源社区尤其是终端用户社区的存在，极大地促进了以容器、服务网格、微服务等为代表的云 …","relpermalink":"/cloud-native-handbook/community/cnc/","summary":"云原生社区是由 宋净超（Jimmy Song） 于 2020 年 5 月发起的，企业中立的云原生终端用户社区。社区秉持 “共识、共治、共建、共享” 的原则。社区的宗旨是：连接、中立、开源。立足中国，面向世界，企业中立，关注开","title":"云原生社区（中国）"},{"content":"Istio 是一个服务网格的开源实现。Istio 支持以下功能。\n流量管理\n利用配置，我们可以控制服务间的流量。设置断路器、超时或重试都可以通过简单的配置改变来完成。\n可观测性\nIstio 通过跟踪、监控和记录让我们更好地了解你的服务，它让我们能够快速发现和修复问题。\n安全性\nIstio 可以在代理层面上管理认证、授权和通信的加密。我们可以通过快速的配置变更在各个服务中执行政策。\nIstio 组件 Istio 服务网格有两个部分：数据平面和控制平面。\n在构建分布式系统时，将组件分离成控制平面和数据平面是一种常见的模式。数据平面的组件在请求路径上，而控制平面的组件则帮助数据平面完成其工作。\nIstio 中的数据平面由 Envoy 代理组成，控制服务之间的通信。网格的控制平面部分负责管理和配置代理。\n   Istio 架构  Envoy（数据平面） Envoy 是一个用 C++ 开发的高性能代理。Istio 服务网格将 Envoy 代理作为一个 sidecar 容器注入到你的应用容器旁边。然后该代理拦截该服务的所有入站和出站流量。注入的代理一起构成了服务网格的数据平面。\nEnvoy 代理也 …","relpermalink":"/cloud-native-handbook/service-mesh/what-is-istio/","summary":"Istio 是一个服务网格的开源实现。Istio 支持以下功能。 流量管理 利用配置，我们可以控制服务间的流量。设置断路器、超时或重试都可以通过简单的配置改变来完成。 可观测性 Istio 通过跟踪、监控和记录让我们更好地了解你的","title":"什么是 Istio?"},{"content":"可观测性行业正处在巨变中。\n传统上，我们观察系统时使用的是一套筒仓式的、独立的工具，其中大部分包含结构不良（或完全非结构化）的数据。这些独立的工具也是垂直整合的。工具、协议和数据格式都属于一个特定的后端或服务，不能互换。这意味着更换或采用新的工具需要耗费时间来更换整个工具链，而不仅仅是更换后端。\n这种孤立的技术格局通常被称为可观测性的 “三大支柱”：日志、度量和（几乎没有）追踪（见图 1-1）。\n日志\n记录构成事务的各个事件。\n度量\n记录构成一个事务的事件的集合。\n追踪\n测量操作的延迟和识别事务中的性能瓶颈，或者类似的东西。传统上，许多组织并不使用分布式追踪，许多开发人员也不熟悉它。\n   图 1-1：可观测性的 \u0026amp;ldquo;三大支柱\u0026amp;rdquo;。  我们用这种方法工作了很久，以致于我们不常质疑它。但正如我们将看到的，“三大支柱” 并不是一种正确的结构化的可观测性方法。事实上，这个术语只是描述了某些技术碰巧被实现的方式，它掩盖了关于如何实际使用我们的工具的几个基本事实。\n什么是事务和资源？ 在我们深入探讨不同可观测性范式的利弊之前，重要的是要定义我们所观察的是什么。我们最感兴趣的分 …","relpermalink":"/opentelemetry-obervability/history/","summary":"第 1 章：可观测性的历史","title":"第 1 章：可观测性的历史"},{"content":"注意：在考虑这些要点时，请谨记 “Code Review 标准”。\n设计 审查中最重要的是 CL 的整体设计。CL 中各种代码的交互是否有意义？此变更是属于您的代码库（codebase）还是属于库（library）？它是否与您系统的其他部分很好地集成？现在是添加此功能的好时机吗？\n功能 这个 CL 是否符合开发者的意图？开发者的意图对代码的用户是否是好的？ “用户”通常都是最终用户（当他们受到变更影响时）和开发者（将来必须“使用”此代码）。\n大多数情况下，我们希望开发者能够很好地测试 CL，以便在审查时代码能够正常工作。但是，作为审查者，仍然应该考虑边缘情况，寻找并发问题，尝试像用户一样思考，并确保您单纯透过阅读方式审查时，代码没有包含任何 bug。\n当要检查 CL 的行为会对用户有重大影响时，验证 CL 的变化将变得十分重要。例如 UI 变更。当您只是阅读代码时，很难理解某些变更会如何影响用户。如果在 CL 中打 patch 或自行尝试这样的变更太不方便，您可以让开发人员为您提供功能演示。\n另一个在代码审查期间特别需要考虑功能的时机，就是如果 CL 中存在某种并行编程，理论上可能导致 …","relpermalink":"/eng-practices/review/reviewer/looking-for/","summary":"注意：在考虑这些要点时，请谨记 “Code Review 标准”。 设计 审查中最重要的是 CL 的整体设计。CL 中各种代码的交互是否有意义？此变更是属于您的代码库（codebase）还是属于库（library）？它是否与您系","title":"Code Review 要点"},{"content":"由于 eBPF 允许在 Linux 内核中运行自定义代码，在解释 eBPF 之前我需要确保你对内核的作用有所了解。然后我们将讨论为什么在修改内核行为这件事情上，eBPF 改变了游戏规则。\nLinux 内核 Linux 内核是应用程序和它们所运行的硬件之间的软件层。应用程序运行在被称为用户空间的非特权层，它不能直接访问硬件。相反，应用程序使用系统调用（syscall）接口发出请求，要求内核代表它行事。这种硬件访问可能涉及到文件的读写，发送或接收网络流量，或者只是访问内存。内核还负责协调并发进程，使许多应用程序可以同时运行。\n应用程序开发者通常不直接使用系统调用接口，因为编程语言给了我们更高级别的抽象和标准库，开发者更容易掌握这些接口。因此，很多人都不知道在程序运行时内核做了什么。如果你想了解内核调用频率，你可以使用 strace 工具来显示程序所做的所有系统调用。这里有一个例子，用 cat 从文件中读取 hello 这个词并将其写到屏幕上涉及到 100 多个系统调用：\nliz@liz-ebpf-demo-1:~$ strace -c cat liz.txt hello % time …","relpermalink":"/what-is-ebpf/changing-the-kernel-is-hard/","summary":"由于 eBPF 允许在 Linux 内核中运行自定义代码，在解释 eBPF 之前我需要确保你对内核的作用有所了解。然后我们将讨论为什么在修改内核行为这件事情上，eBPF 改变了游戏规则。 Linux 内核 Linux 内核是应用程序和它们所运行的硬件之间的软","title":"第二章：修改内核很困难"},{"content":"为什么提交小型 CL? 小且简单的 CL 是指：\n 审查更快。审查者更容易抽多次五分钟时间来审查小型 CL，而不是留出 30 分钟来审查一个大型 CL。 审查得更彻底。如果是大的变更，审查者和提交者往往会因为大量细节的讨论翻来覆去而感到沮丧——有时甚至到了重要点被遗漏或丢失的程度。 不太可能引入错误。 由于您进行的变更较少，您和您的审查者可以更轻松有效地推断 CL 的影响，并查看是否已引入错误。 如果被拒绝，减少浪费的工作。 如果您写了一个巨大的 CL，您的评论者说整个 CL 的方向都错误了，你就浪费了很多精力和时间。 更容易合并。 处理大型 CL 需要很长时间，在合并时会出现很多冲突，并且必须经常合并。 更容易设计好。 打磨一个小变更的设计和代码健康状况比完善一个大变更的所有细节要容易得多。 减少对审查的阻碍。 发送整体变更的自包含部分可让您在等待当前 CL 审核时继续编码。 更简单的回滚。 大型 CL 更有可能触及在初始 CL 提交和回滚 CL 之间更新的文件，从而使回滚变得复杂（中间的 CL 也可能需要回滚）。  请注意，审查者可以仅凭 CL 过大而自行决定完全拒绝您的变更。通常 …","relpermalink":"/eng-practices/review/developer/small-cls/","summary":"为什么提交小型 CL? 小且简单的 CL 是指： 审查更快。审查者更容易抽多次五分钟时间来审查小型 CL，而不是留出 30 分钟来审查一个大型 CL。 审查得更彻底。如果是大的变更，审查者和提交者往往会因为大量细节的讨论翻来覆去","title":"小型 CL"},{"content":"所有 BPF Map 都是有使用容量上限的。超出限制的插入将失败，从而限制了数据路径的可扩展性。下表显示了映射的默认值。每个限制都可以在源代码中更改。如果需要，将根据要求添加配置选项。\n   Map 名称 范围 默认限制 规模影响     连接跟踪 节点或端点 1M TCP/256k UDP 最大 1M 并发 TCP 连接，最大 256k 预期 UDP 应答   NAT 节点 512k 最大 512k NAT 条目   邻居表 节点 512k 最大 512k 邻居条目   端点 节点 64k 每个节点最多 64k 个本地端点 + 主机 IP   IP 缓存 节点 512k 最大 256k 端点（IPv4+IPv6），最大 512k 端点（IPv4 或 IPv6）跨所有集群   负载均衡器 节点 64k 跨所有集群的所有服务的最大 64k 累积后端   策略 端点 16k 特定端点的最大允许身份 + 端口 + 协议对 16k   代理 Map 节点 512k 最大 512k 并发重定向 TCP 连接到代理   隧道 节点 64k 跨所有集群最多 32k 节点（IPv4+IPv6） …","relpermalink":"/cilium-handbook/ebpf/maps/","summary":"所有 BPF Map 都是有使用容量上限的。超出限制的插入将失败，从而限制了数据路径的可扩展性。下表显示了映射的默认值。每个限制都可以在源代码中更改。如果需要，将根据要求添加配置选项。 Map 名称 范围 默认限制 规模影响 连接","title":"eBPF Map"},{"content":"用于 Pod 的 IPv4 地址通常是从 RFC1918 私有地址块分配的，因此不可公开路由。Cilium 会自动将离开集群的所有流量的源 IP 地址伪装成节点的 IPv4 地址，因为节点的 IP 地址已经可以在网络上路由。\n   IP 地址伪装示意图  对于 IPv6 地址，只有在使用 iptables 实现模式时才会执行伪装。\n可以使用 IPv4 选项和离开主机的 IPv6 流量禁用此行为。enable-ipv4-masquerade: false``enable-ipv6-masquerade: false\n配置   设置可路由 CIDR\n默认行为是排除本地节点的 IP 分配 CIDR 内的任何目标。如果 pod IP 可在更广泛的网络中路由，则可以使用以下选项指定该网络：ipv4-native-routing-cidr: 10.0.0.0/8 ，在这种情况下，该 CIDR 内的所有目的地都 不会 被伪装。\n  设置伪装接口\n请参阅实现模式以配置伪装接口。\n  实现模式 基于 eBPF 基于 eBPF 的实现是最有效的实现。它需要 Linux 内核 4.19， …","relpermalink":"/cilium-handbook/networking/masquerading/","summary":"用于 Pod 的 IPv4 地址通常是从 RFC1918 私有地址块分配的，因此不可公开路由。Cilium 会自动将离开集群的所有流量的源 IP 地址伪装成节点的 IPv4 地址，因为节点的 IP 地址已经可以在网络上路由。 IP 地址伪装示意图 对于 IPv6 地址，只有在","title":"IP 地址伪装"},{"content":"所有安全策略的描述都是假设基于会话协议的有状态策略执行。这意味着策略的意图是描述允许的连接建立方向。如果策略允许A =\u0026gt; B，那么从 B 到 A 的回复数据包也会被自动允许。但是，并不自动允许 B 向 A 发起连接。如果希望得到这种结果，那么必须明确允许这两个方向。\n安全策略可以在 ingress 或 egress 处执行。对于 ingress，这意味着每个集群节点验证所有进入的数据包，并确定数据包是否被允许传输到预定的终端。相应地，对于 egress，每个集群节点验证出站数据包，并确定是否允许将数据包传输到预定目的地。\n为了在多主机集群中执行基于身份的安全，发送端点的身份被嵌入到集群节点之间传输的每个网络数据包中。然后，接收集群节点可以提取该身份，并验证一个特定的身份是否被允许与任何本地端点进行通信。\n默认安全策略 如果没有加载任何策略，默认行为是允许所有通信，除非明确启用了策略执行。一旦加载了第一条策略规则，就会自动启用策略执行，然后任何通信必须是白名单，否则相关数据包将被丢弃。\n同样，如果一个端点不受制于四层策略，则允许与所有端口进行通信。将至少一个 四层策略与一个端点相关联，将 …","relpermalink":"/cilium-handbook/security/policyenforcement/","summary":"所有安全策略的描述都是假设基于会话协议的有状态策略执行。这意味着策略的意图是描述允许的连接建立方向。如果策略允许A =\u003e B，那么从 B 到 A 的回复数据包也会被自动允许。但是，并不自动允许 B 向 A 发起连接。如果希","title":"策略执行"},{"content":"本节指定 Cilium 端点的生命周期。\nCilium 中的端点状态包括：\n restoring：端点在 Cilium 启动之前启动，Cilium 正在恢复其网络配置。 waiting-for-identity：Cilium 正在为端点分配一个唯一的身份。 waiting-to-regenerate：端点接收到一个身份并等待（重新）生成其网络配置。 regenerating：正在（重新）生成端点的网络配置。这包括为该端点编程 eBPF。 ready：端点的网络配置已成功（重新）生成。 disconnecting：正在删除端点。 disconnected：端点已被删除。     端点状态生命周期  可以使用 cilium endpoint list 和 cilium endpoint get CLI 命令查询端点的状态。\n当端点运行时，它会在 waiting-for-identity、waiting-to-regenerate、regenerating 和 ready 状态之间转换。进入 waiting-for-identity 状态的转换表明端点改变了它的身份。 …","relpermalink":"/cilium-handbook/policy/lifecycle/","summary":"本节指定 Cilium 端点的生命周期。 Cilium 中的端点状态包括： restoring：端点在 Cilium 启动之前启动，Cilium 正在恢复其网络配置。 waiting-for-identity：Cilium 正在为端点分配一个唯一的身","title":"端点生命周期"},{"content":"集群网格将网络数据路径扩展到多个集群。它允许所有连接集群中的端点进行通信，同时提供完整的策略执行。负载均衡可通过 Kubernetes 注解获得。\n请参阅如何设置集群网格的说明。\n","relpermalink":"/cilium-handbook/clustermesh/","summary":"集群网格将网络数据路径扩展到多个集群。它允许所有连接集群中的端点进行通信，同时提供完整的策略执行。负载均衡可通过 Kubernetes 注解获得。 请参阅如何设置集群网格的说明。","title":"多集群（集群网格）"},{"content":"可观测性由 Hubble 提供，它可以以完全透明的方式深入了解服务的通信和行为以及网络基础设施。Hubble 能够在多集群（集群网格） 场景中提供节点级别、集群级别甚至跨集群的可视性。有关 Hubble 的介绍以及它与 Cilium 的关系，请阅读 Cilium 和 Hubble 简介部分。\n默认情况下，Hubble API 的范围仅限于 Cilium 代理运行的每个单独节点。换句话说，网络可视性仅提供给本地 Cilium 代理观察到的流量。在这种情况下，与 Hubble API 交互的唯一方法是使用 Hubble CLI（hubble）查询通过本地 Unix Domain Socket 提供的 Hubble API。Hubble CLI 二进制文件默认安装在 Cilium 代理 pod 上。\n部署 Hubble Relay 后，Hubble 提供完整的网络可视性。在这种情况下，Hubble Relay 服务提供了一个 Hubble API，它在 ClusterMesh 场景中涵盖整个集群甚至多个集群。可以通过将 Hubble CLI（hubble）指向 Hubble Relay 服务 …","relpermalink":"/cilium-handbook/concepts/observability/","summary":"可观测性由 Hubble 提供，它可以以完全透明的方式深入了解服务的通信和行为以及网络基础设施。Hubble 能够在多集群（集群网格） 场景中提供节点级别、集群级别甚至跨集群的可视性。有关 Hubble 的介绍以及它与 Cilium 的关系，请阅","title":"可观测性"},{"content":"Kubernetes 版本 以下列出的所有 Kubernetes 版本都经过 e2e 测试，并保证与此 Cilium 版本兼容。此处未列出的旧 Kubernetes 版本不支持 Cilium。较新的 Kubernetes 版本未列出，这取决于新版本的的向后兼容性。\n 1.16 1.17 1.18 1.19 1.20 1.21 1.22 1.23  系统要求 Cilium 需要 Linux 内核 \u0026gt;= 4.9。有关所有系统要求的完整详细信息，请参阅系统要求。\n在 Kubernetes 中启用 CNI CNI（容器网络接口）是 Kubernetes 用来委托网络配置的插件层。必须在 Kubernetes 集群中启用 CNI 才能安装 Cilium。这是通过将 --network-plugin=cni 参数在所有节点上传递给 kubelet 来完成的。有关更多信息，请参阅Kubernetes CNI 网络插件文档。\n启用自动节点 CIDR 分配（推荐） Kubernetes 具有自动分配每个节点 IP CIDR 的能力。如果启用，Cilium 会自动使用此功能。这是在 Kubernetes  …","relpermalink":"/cilium-handbook/kubernetes/requirements/","summary":"Kubernetes 版本 以下列出的所有 Kubernetes 版本都经过 e2e 测试，并保证与此 Cilium 版本兼容。此处未列出的旧 Kubernetes 版本不支持 Cilium。较新的 Kubernetes 版本未列出，这取决于新版本的的向后兼容性。 1.16 1.17 1.18 1.19 1.20 1.21 1.22 1.23 系统要求 Cilium 需要 Linux 内核 \u003e= 4.9。有","title":"要求"},{"content":"在这一章中，让我们来谈谈编写 eBPF 代码。我们需要考虑在内核中运行的 eBPF 程序本身，以及与之交互的用户空间代码。\n内核和用户空间代码 首先，你可以用什么编程语言来编写 eBPF 程序？\n内核接受字节码形式的 eBPF 程序 1。人工编写这种字节码是可能的，就像用汇编语言写应用程序代码一样——但对人类来说，使用一种可以被编译（即自动翻译）为字节码的高级语言通常更实用。\n由于一些原因，eBPF 程序不能用任意的高级语言编写。首先，语言编译器需要支持发出内核所期望的 eBPF 字节码格式。其次，许多编译语言都有运行时特性——例如 Go 的内存管理和垃圾回收，使它们不适合。在撰写本文时，编写 eBPF 程序的唯一选择是 C（用 clang/llvm 编译）和最新的 Rust。迄今为止，绝大多数的 eBPF 代码都是用 C 语言发布的，考虑到它是 Linux 内核的语言，这是有道理的。\n至少，用户空间的程序需要加载到内核中，并将其附加到正确的事件中。有一些实用程序，如 bpftool，可以帮助我们解决这个问题，但这些都是低级别的工具，假定你有丰富的 eBPF 知识，它们是为 eBPF  …","relpermalink":"/what-is-ebpf/ebpf-programs/","summary":"在这一章中，让我们来谈谈编写 eBPF 代码。我们需要考虑在内核中运行的 eBPF 程序本身，以及与之交互的用户空间代码。 内核和用户空间代码 首先，你可以用什么编程语言来编写 eBPF 程序？ 内核接受字节码形式的 eBPF 程序 1。人工编写这","title":"第三章：eBPF 程序"},{"content":"现在我们将一些问题转移到了云中的 DevOps 平台。\n分解单体应用 传统的 n 层单体式应用部署到云中后很难维护，因为它们经常对云基础设施提供的部署环境做出不可靠的假设，这些假设云很难提供。 例如以下要求：\n 可访问已挂载的共享文件系统 P2P 应用服务器集群 共享库 配置文件位于常用的配置文件目录  大多数这些假设都出于这样的事实：单体应用通常都部署在长期运行的基础设施中，并与其紧密结合。不幸的是，单体应用并不太适合弹性和短暂（非长期支持）生命周期的基础设施。\n但是即使我们可以构建一个不需要这些假设的单体应用，我们依然有一些问题需要解决：\n 单体式应用的变更周期耦合，使独立业务能力无法按需部署，阻碍创新速度。 嵌入到单体应用中的服务不能独立于其他服务进行扩展，因此负载更难于优化。 新加入组织的开发人员必须适应新的团队，经常学习新的业务领域，并且一次就熟悉一个非常大的代码库。 这样会增加 3-6 个月的适应时间，才能实现真正的生产力。 尝试通过堆积开发人员来扩大开发组织，增加了昂贵的沟通和协调成本。 技术栈需要长期承诺。引进新技术太过冒险，可能会对整体产生不利影响。  细心的读者会注 …","relpermalink":"/migrating-to-cloud-native-application-architectures/changes-needed/technical-change/","summary":"现在我们将一些问题转移到了云中的 DevOps 平台。 分解单体应用 传统的 n 层单体式应用部署到云中后很难维护，因为它们经常对云基础设施提供的部署环境做出不可靠的假设，这些假设云很难提供。 例如以下要求： 可访问已挂载的共","title":"2.3 技术变革"},{"content":"所涉及的关键原语和实施任务是。\n 管道和 CI/CD 管道的概念 CI/CD 管道的构建块 设计和执行 CI/CD 管道 自动化的战略 CI/CD 管道中对安全自动化工具的要求  3.3.1 管道的概念和 CI/CD 管道 DevSecOps 作为一种敏捷应用开发、部署和运维的方法论或框架，与其他方法论一样，是由 各个阶段 组成的。信息在各阶段中的顺序和流动被称为工作流，其中一些阶段可以平行执行，而其他阶段则必须遵循一个顺序。每个阶段可能需要调用一个独特的工作来执行该阶段的活动。\nDevSecOps 在流程工作流中引入的一个独特概念是 管道 的概念。有了管道，就不需要为启动 / 执行流程的每个阶段单独编写作业。相反，只有一个作业从初始阶段开始，自动触发与其他阶段有关的活动 / 任务（包括顺序的和并行的），并创建一个无错误的智能工作流程。\nDevSecOps 中的管道被称为 CI/CD 管道，这是基于它所完成的总体任务和它所包含的两个单独阶段。CD 可以表示持续交付或持续部署阶段。根据这后一个阶段，CI/CD 可以涉及以下任务：\n 构建、测试、安全和交付：经过测试的修改后的代码被交付到暂 …","relpermalink":"/service-mesh-devsecops/devsecops/key-primitives-and-implementation-tasks/","summary":"所涉及的关键原语和实施任务是。 管道和 CI/CD 管道的概念 CI/CD 管道的构建块 设计和执行 CI/CD 管道 自动化的战略 CI/CD 管道中对安全自动化工具的要求 3.3.1 管道的概念和 CI/CD 管道 DevSecOps 作为一种敏捷应用开发、部署和运维的方法论或框架，与其他方法","title":"3.3 DevSecOps 关键原语和实施任务"},{"content":"本章中我们讨论了两种帮助我们迁移到云原生应用架构的方法：\n分解原架构\n我们使用以下方式分解单体应用：\n 所有新功能都使用微服务形式构建。 通过隔离层将微服务与单体应用集成。 通过定义有界上下文来分解服务，逐步扼杀单体架构。  使用分布式系统\n分布式系统由以下部分组成：\n 版本化，分布式，通过配置服务器和管理总线刷新配置。 动态发现远端依赖。 去中心化的负载均衡策略 通过熔断器和隔板阻止级联故障 通过 API 网关集成到特定的客户端上  还有很多其他的模式，包括自动化测试、持续构建与发布管道等。欲了解更多信息，请阅读 Toby Clemson 的 《 Testing Strategies in a Microservice Architecture》，以及 Jez Humbl 和 David Farley（AddisonWesley）的《Continuous Delivery: Reliable Software Releases through Build, Test, and Deployment Automation》。\n","relpermalink":"/migrating-to-cloud-native-application-architectures/migration-cookbook/summary/","summary":"本章中我们讨论了两种帮助我们迁移到云原生应用架构的方法： 分解原架构 我们使用以下方式分解单体应用： 所有新功能都使用微服务形式构建。 通过隔离层将微服务与单体应用集成。 通过定义有界上下文来分解服务，逐步扼杀","title":"3.3 本章小结"},{"content":"为应用程序分配基础设施的传统方法包括最初用配置参数和持续的任务配置计算和网络资源，如补丁管理（如操作系统和库），建立符合合规法规（如数据隐私），并进行漂移（当前配置不再提供预期的操作状态）纠正。\n基础设施即代码（IaC）是一种声明式的代码，它对计算机指令进行编码，这些指令封装了通过服务的管理 API 在公共云服务或私有数据中心部署虚拟基础设施所需的 参数。换句话说，基础设施是以声明式的方式定义的，并使用用于应用程序代码的相同的源代码控制工具（如 GitOps）进行版本控制。根据特定的 IaC 工具，这种语言可以是脚本语言（如 JavaScript、Python、TypeScript 等）或专有配置语言（如 HCL），可能与标准化语言（如 JSON）兼容也可能不兼容。基本指令包括告诉系统如何配置和管理 基础设施（无论是单个计算实例还是完整的服务器，如物理服务器或虚拟机）、容器、存储、网络连接、连接拓扑和负载均衡器。在某些情况下，基础设施可能是短暂的，基础设施的寿命（无论是不可变的还是可变的）不需要继续配置管理。配置可以与应用程序代码的单个提交相联系，使用的工具可以将应用程序代码和基础设施 …","relpermalink":"/service-mesh-devsecops/implement/ci-cd-pipeline-for-infrastructure-as-code/","summary":"为应用程序分配基础设施的传统方法包括最初用配置参数和持续的任务配置计算和网络资源，如补丁管理（如操作系统和库），建立符合合规法规（如数据隐私），并进行漂移（当前配置不再提供预期的操作状态）纠正。 基础设","title":"4.3 基础设施即代码的 CI/CD 管道"},{"content":"在这一节中，我们将解释 Envoy 的基本构建模块。\nEnvoy 配置的根被称为引导配置。它包含了一些字段，我们可以在这里提供静态或动态的资源和高级别的 Envoy 配置（例如，Envoy 实例名称、运行时配置、启用管理界面等等）。\n为了开始学习，我们将主要关注静态资源，在教程的后面，我们将介绍如何配置动态资源。\nEnvoy 输出许多统计数据，这取决于启用的组件和它们的配置。我们会在整个教程中提到不同的统计信息，在教程后面的专门模块中，我们会更多地讨论统计信息。\n下图显示了通过这些概念的请求流。\n   Envoy 构建块  这一切都从监听器开始。Envoy 暴露的监听器是命名的网络位置，可以是一个 IP 地址和一个端口，也可以是一个 Unix 域套接字路径。Envoy 通过监听器接收连接和请求。考虑一下下面的 Envoy 配置。\nstatic_resources:listeners:- name:listener_0address:socket_address:address:0.0.0.0port_value:10000filter_chains:[{}]通过上面的 Envoy 配 …","relpermalink":"/envoy-handbook/intro/envoy-building-blocks/","summary":"在这一节中，我们将解释 Envoy 的基本构建模块。 Envoy 配置的根被称为引导配置。它包含了一些字段，我们可以在这里提供静态或动态的资源和高级别的 Envoy 配置（例如，Envoy 实例名称、运行时配置、启用管理界面等等）。 为了开","title":"Envoy 的构建模块"},{"content":"基础架构是指支持应用程序的所有软件和硬件，包括数据中心、操作系统、部署流水线、配置管理以及支持应用程序生命周期所需的任何系统或软件。\n已经有无数的时间和金钱花在了基础架构上。通过多年来不断的技术演化和实践提炼，有些公司已经能够运行大规模的基础架构和应用程序，并且拥有卓越的敏捷性。高效运行的基础架构可以使得迭代更快，缩短投向市场的时间，从而加速业务发展。\n使用云原生基础架构是有效运行云原生应用程序的要求。如果没有正确的设计和实践来管理基础架构，即使是最好的云原生应用程序也会浪费。本书中的实践并不一定需要有巨大的基础架构规模，但如果您想从云计算中获取回报，您应该听从开创了这些模式的人的经验。\n在我们探索如何构建云中运行的应用程序的基础架构之前，我们需要了解我们是如何走到这一步。首先，我们将讨论采用云原生实践的好处。接下来，我们将看一下基础架构的简历，然后讨论下一阶段的功能，称为 “云原生”，以及它与您的应用程序、运行的平台及业务之间的关系。\n在明白了这一点后，我们将向您展示解决方案及实现。\n云原生的优势 采用本书中的模式有很多好处。它们仿照谷歌、Netflix 和亚马逊这些成功的公司 —— …","relpermalink":"/cloud-native-infra/what-is-cloud-native-infrastructure/","summary":"基础架构是指支持应用程序的所有软件和硬件，包括数据中心、操作系统、部署流水线、配置管理以及支持应用程序生命周期所需的任何系统或软件。 已经有无数的时间和金钱花在了基础架构上。通过多年来不断的技术演化和实","title":"第 1 章：什么是云原生基础架构？"},{"content":"Kubernetes® 是一个开源系统，可以自动部署、扩展和管理在容器中运行的应用程序，并且通常托管在云环境中。与传统的单体软件平台相比，使用这种类型的虚拟化基础设施可以提供一些灵活性和安全性的好处。然而，安全地管理从微服务到底层基础设施的所有方面，会引入其他的复杂性。本报告中详述的加固指导旨在帮助企业处理相关风险并享受使用这种技术的好处。\nKubernetes 中三个常见的破坏源是供应链风险、恶意威胁者和内部威胁。\n供应链风险往往是具有挑战性的，可以在容器构建周期或基础设施收购中出现。恶意威胁者可以利用 Kubernetes 架构的组件中的漏洞和错误配置，如控制平面、工作节点或容器化应用程序。内部威胁可以是管理员、用户或云服务提供商。对组织的 Kubernetes 基础设施有特殊访问权的内部人员可能会滥用这些特权。\n本指南描述了与设置和保护 Kubernetes 集群有关的安全挑战。包括避免常见错误配置的加固策略，并指导国家安全系统的系统管理员和开发人员如何部署 Kubernetes，并提供了建议的加固措施和缓解措施的配置示例。本指南详细介绍了以下缓解措施：\n 扫描容器和 Pod 的 …","relpermalink":"/kubernetes-hardening-guidance/executive-summary/","summary":"Kubernetes® 是一个开源系统，可以自动部署、扩展和管理在容器中运行的应用程序，并且通常托管在云环境中。与传统的单体软件平台相比，使用这种类型的虚拟化基础设施可以提供一些灵活性和安全性的好处。然","title":"执行摘要"},{"content":"本文参考的是 OAM 规范中对云原生应用的定义，并做出了引申。\n云原生应用是一个相互关联但又不独立的组件（service、task、worker）的集合，这些组件与配置结合在一起并在适当的运行时实例化后，共同完成统一的功能目的。\n云原生应用模型 下图是 OAM 定义的云原生应用模型示意图，为了便于理解，图中相同颜色的部分为同一类别的对象定义。\n   云原生应用模型  OAM 的规范中定义了以下对象，它们既是 OAM 规范中的基本术语也是云原生应用的基本组成。\n Workload（工作负载）：应用程序的工作负载类型，由平台提供。 Component组件）：定义了一个 Workload 的实例，并以基础设施中立的术语声明其运维特性。 Trait（特征）：用于将运维特性分配给组件实例。 ApplicationScope（应用作用域）：用于将组件分组成具有共同特性的松散耦合的应用。 ApplicationConfiguration（应用配置）：描述 Component 的部署、Trait 和 ApplicationScope。  关注点分离 下图是不同角色对于该模型的关注点示意图。\n   云原 …","relpermalink":"/cloud-native-handbook/intro/define-cloud-native-app/","summary":"本文参考的是 OAM 规范中对云原生应用的定义，并做出了引申。 云原生应用是一个相互关联但又不独立的组件（service、task、worker）的集合，这些组件与配置结合在一起并在适当的运行时实例化后，共同完","title":"什么是云原生应用？"},{"content":"Kubernetes 一词来自希腊语，意思是 “飞行员” 或 “舵手”。这个名字很贴切，Kubernetes 可以帮助你在波涛汹涌的容器海洋中航行。\nKubernetes 是做什么的？什么是 Docker？什么是容器编排？Kubernetes 是如何工作和扩展的？你可能还有很多其他的问题，本文将一一为你解答。\n这篇文章适合初学者，尤其是那些工作忙碌，没有办法抽出太多时间来了解 Kubernetes 和云原生的开发者们，希望本文可以帮助你进入 Kubernetes 的世界。\n简而言之，Kubernetes 提供了一个平台或工具来帮助你快速协调或扩展容器化应用，特别是在 Docker 容器。让我们深入了解一下这些概念。\n容器和容器化 那么什么是容器呢？\n要讨论容器化首先要谈到虚拟机 (VM)，顾名思义，虚拟机就是可以远程连接的虚拟服务器，比如 AWS 的 EC2 或阿里云的 ECS。\n接下来，假如你要在虚拟机上运行一个网络应用 —— 包括一个 MySQL 数据库、一个 Vue 前端和一些 Java 库，在 Ubuntu 操作系统 (OS) 上运行。你不用熟悉其中的每一个技术 —— 你只要记 …","relpermalink":"/cloud-native-handbook/intro/quick-start/","summary":"Kubernetes 一词来自希腊语，意思是 “飞行员” 或 “舵手”。这个名字很贴切，Kubernetes 可以帮助你在波涛汹涌的容器海洋中航行。 Kubernetes 是做什么的？什么是 Docker？什么是容器编排？Kubernetes 是如何工作","title":"云原生快速入门"},{"content":"你可能参加过各种云原生、服务网格相关的 meetup，在社区里看到很多人在分享和讨论 Istio，但是对于自己是否真的需要 Istio 感到踌躇，甚至因为它的复杂性而对服务网格的前景感到怀疑。那么，在你继阅读 Istio SIG 后续文章之前，请先仔细阅读本文，审视一下自己公司的现状，看看你是否有必要使用服务网格，处于 Istio 应用的哪个阶段。\n本文不是对应用服务网格的指导，而是根据社区里经常遇到的问题而整理。在使用 Istio 之前，请先考虑下以下因素：\n 你的团队里有多少人？ 你的团队是否有使用 Kubernetes、Istio 的经验？ 你有多少微服务？ 这些微服务使用什么语言？ 你的运维、SRE 团队是否可以支持服务网格管理？ 你有采用开源项目的经验吗？ 你的服务都运行在哪些平台上？ 你的应用已经容器化并使用 Kubernetes 管理了吗？ 你的服务有多少是部署在虚拟机、有多少是部署到 Kubernetes 集群上，比例如何？ 你的团队有制定转移到云原生架构的计划吗？ 你想使用 Istio 的什么功能？ Istio 的稳定性是否能够满足你的需求？ …","relpermalink":"/cloud-native-handbook/service-mesh/do-you-need-a-service-mesh/","summary":"你可能参加过各种云原生、服务网格相关的 meetup，在社区里看到很多人在分享和讨论 Istio，但是对于自己是否真的需要 Istio 感到踌躇，甚至因为它的复杂性而对服务网格的前景感到怀疑。那么，在你继阅读 Istio SIG 后续","title":"你是否需要 Istio？"},{"content":"眯着眼睛看图表并不是寻找相关性的最佳方式。目前在运维人员头脑中进行的大量工作实际上是可以自动化的。这使运维人员可以在识别问题、提出假设和验证根本原因之间迅速行动。\n为了建立更好的工具，我们需要更好的数据。遥测必须具备以下两个要求以支持高质量的自动分析：\n 所有的数据点都必须用适当的索引连接在一个图上。 所有代表常见操作的数据点必须有明确的键和值。  在这一章中，我们将从一个基本的构件开始浏览现代遥测数据模型：属性。\n属性：定义键和值 最基本的数据结构是属性（attribute），定义为一个键和一个值。OpenTelemetry 的每个数据结构都包含一个属性列表。分布式系统的每个组件（HTTP 请求、SQL 客户端、无服务器函数、Kubernetes Pod）在 OpenTelemetry 规范中都被定义为一组特定的属性。这些定义被称为 OpenTelemetry 语义约定。表 2-1 显示了 HTTP 约定的部分列表。\n   属性 类型 描述 示例     http.method string HTTP 请求类型 GET; POST; HEAD   http.target string …","relpermalink":"/opentelemetry-obervability/the-value-of-structured-data/","summary":"第 2 章：结构化数据的价值","title":"第 2 章：结构化数据的价值"},{"content":"IT 行业正在向微服务架构和云原生解决方案发展。由于使用不同的技术开发了成百上千的微服务，这些系统可能变得复杂，难以调试。\n作为一个应用开发者，你考虑的是业务逻辑——购买产品或生成发票。然而，任何像这样的业务逻辑都会导致不同服务之间的多个服务调用。每个服务可能都有它的超时、重试逻辑和其他可能需要调整或微调的网络特定代码。\n如果在任何时候最初的请求失败了，就很难通过多个服务来追踪，准确地指出失败发生的地方，了解请求为什么失败。是网络不可靠吗？是否需要调整重试或超时？或者是业务逻辑问题或错误？\n服务可能使用不一致的跟踪和记录机制，使这种调试的复杂性增加。这些问题使你很难确定问题发生在哪里，以及如何解决。如果你是一个应用程序开发人员，而调试网络问题不属于你的核心技能，那就更是如此。\n将网络问题从应用程序堆栈中抽离出来，由另一个组件来处理网络部分，让调试网络问题变得更容易。这就是 Envoy 所做的事情。\n在每个服务实例旁边都有一个 Envoy 实例在运行。这种类型的部署也被称为 Sidecar 部署。Envoy 的另一种模式是边缘代理，用于构建 API 网关。\nEnvoy 和应用程序形成一个 …","relpermalink":"/cloud-native-handbook/service-mesh/what-is-envoy/","summary":"IT 行业正在向微服务架构和云原生解决方案发展。由于使用不同的技术开发了成百上千的微服务，这些系统可能变得复杂，难以调试。 作为一个应用开发者，你考虑的是业务逻辑——购买产品或生成发票。然而，任何像这样的业","title":"什么是 Envoy？"},{"content":"总结 现在您已经知道了 Code Review 要点，那么管理分布在多个文件中的评论的最有效方法是什么？\n 变更是否有意义？它有很好的描述吗？ 首先看一下变更中最重要的部分。整体设计得好吗？ 以适当的顺序查看 CL 的其余部分。  第一步：全面了解变更 查看 CL 描述和 CL 大致上用来做什么事情。这种变更是否有意义？如果在最初不应该发生这样的变更，请立即回复，说明为什么不应该进行变更。当您拒绝这样的变更时，向开发人员建议应该做什么也是一个好主意。\n例如，您可能会说“看起来你已经完成一些不错的工作，谢谢！但实际上，我们正朝着删除您在这里修改的 FooWidget 系统的方向演进，所以我们不想对它进行任何新的修改。不过，您来重构下新的 BarWidget 类怎么样？“\n请注意，审查者不仅拒绝了当前的 CL 并提供了替代建议，而且他们保持礼貌地这样做。这种礼貌很重要，因为我们希望表明，即使不同意，我们也会相互尊重。\n如果您获得了多个您不想变更的 CL，您应该考虑重整开发团队的开发过程或外部贡献者的发布过程，以便在编写CL之前有更多的沟通。最好在他们完成大量工作之前说“不”，避免已经投入心 …","relpermalink":"/eng-practices/review/reviewer/navigate/","summary":"总结 现在您已经知道了 Code Review 要点，那么管理分布在多个文件中的评论的最有效方法是什么？ 变更是否有意义？它有很好的描述吗？ 首先看一下变更中最重要的部分。整体设计得好吗？ 以适当的顺序查看 CL 的其余部分。 第一步：全","title":"查看 CL 的步骤"},{"content":"当您发送 CL 进行审查时，您的审查者可能会对您的 CL 发表一些评论。以下是处理审查者评论的一些有用信息。\n不是针对您 审查的目标是保持代码库和产品的质量。当审查者对您的代码提出批评时，请将其视为在帮助您、代码库和 Google，而不是对您或您的能力的个人攻击。\n有时，审查者会感到沮丧并在评论中表达他们的挫折感。对于审查者来说，这不是一个好习惯，但作为开发人员，您应该为此做好准备。问问自己，“审查者试图与我沟通的建设性意见是什么？”然后像他们实际说的那样操作。\n永远不要愤怒地回应代码审查评论。这严重违反了专业礼仪且将永远存在于代码审查工具中。如果您太生气或恼火而无法好好的回应，那么请离开电脑一段时间，或者做一些别的事情，直到您感到平静，可以礼貌地回答。\n一般来说，如果审查者没有以建设性和礼貌的方式提供反馈，请亲自向他们解释。如果您无法亲自或通过视频通话与他们交谈，请向他们发送私人电子邮件。以友善的方式向他们解释您不喜欢的东西以及您希望他们以怎样不同的方式来做些什么。如果他们也以非建设性的方式回复此私人讨论，或者没有预期的效果，那么请酌情上报给您的经理。\n修复代码 如果审查者说他们不了 …","relpermalink":"/eng-practices/review/developer/handling-comments/","summary":"当您发送 CL 进行审查时，您的审查者可能会对您的 CL 发表一些评论。以下是处理审查者评论的一些有用信息。 不是针对您 审查的目标是保持代码库和产品的质量。当审查者对您的代码提出批评时，请将其视为在帮助您、代码库和","title":"如何处理审查者的评论"},{"content":"根据所使用的 Linux 内核版本，eBPF 数据路径可以完全在 eBPF 中实现不同的功能集。如果某些所需功能不可用，则使用旧版 iptables 实现提供该功能。有关详细信息，请参阅 IPsec 要求。\nkube-proxy 互操作性 下图显示了 kube-proxy 安装的 iptables 规则和 Cilium 安装的 iptables 规则的集成。\n   图片   下一章   ","relpermalink":"/cilium-handbook/ebpf/iptables/","summary":"根据所使用的 Linux 内核版本，eBPF 数据路径可以完全在 eBPF 中实现不同的功能集。如果某些所需功能不可用，则使用旧版 iptables 实现提供该功能。有关详细信息，请参阅 IPsec 要求。 kube-proxy 互操作性 下图显示了 kube-proxy 安装的 iptables 规则和 Cilium 安装的 iptables 规","title":"Iptables 用法"},{"content":"默认情况下，Cilium 将 eBPF 数据路径配置为执行 IP 分片跟踪，以允许不支持分片的协议（例如 UDP）通过网络透明地传输大型消息。IP 分片跟踪在 eBPF 中使用 LRU（最近最少使用）映射实现，需要 Linux 4.10 或更高版本。可以使用以下选项配置此功能：\n --enable-ipv4-fragment-tracking：启用或禁用 IPv4 分片跟踪。默认启用。 --bpf-fragments-map-max：控制使用 IP 分片的最大活动并发连接数。对于默认值，请参阅eBPF Maps。  注意\n当使用 kube-proxy 运行 Cilium 时，碎片化的 NodePort 流量可能会由于内核错误而中断，其中路由 MTU 不受转发数据包的影响。纤毛碎片跟踪需要第一个逻辑碎片首先到达。由于内核错误，可能会在外部封装层上发生额外的碎片，从而导致数据包重新排序并导致无法跟踪碎片。\n内核错误已被 修复 并向后移植到所有维护的内核版本。如果您发现连接问题，请确保您的节点上的内核包最近已升级，然后再报告问题。\n提示\n 这是一个测试版功能。如果你遇到任何问题，请提供反馈并 …","relpermalink":"/cilium-handbook/networking/fragmentation/","summary":"默认情况下，Cilium 将 eBPF 数据路径配置为执行 IP 分片跟踪，以允许不支持分片的协议（例如 UDP）通过网络透明地传输大型消息。IP 分片跟踪在 eBPF 中使用 LRU（最近最少使用）映射实现，需要 Linux 4.10 或更高版本。可以","title":"IPv4 分片处理"},{"content":"Cilium 能够透明地将四层代理注入任何网络连接中。这被用作执行更高级别网络策略的基础（请参阅基于 DNS 和 七层示例）。\n可以注入以下代理：\n Envoy  注入 Envoy 代理 注意\n 此功能目前正处于测试阶段。  如果你有兴趣编写 Envoy 代理的 Go 扩展，请参考开发者指南。\n   Envoy 代理注入示意图  如上所述，该框架允许开发人员编写少量 Go 代码（绿色框），专注于解析新的 API 协议，并且该 Go 代码能够充分利用 Cilium 功能，包括高性能重定向 Envoy、丰富的七层感知策略语言和访问日志记录，以及通过 kTLS 对加密流量的可视性（即将推出）。总而言之，作为开发者的你只需要关心解析协议的逻辑，Cilium + Envoy + eBPF 就完成了繁重的工作。\n本指南基于假设的 r2d2 协议（参见 proxylib/r2d2/r2d2parser.go）的简单示例，该协议可用于很久以前在遥远的星系中与简单的协议机器人对话。但它也指向了 cilium/proxylib 目录中已经存在的其他真实协议，例如 Memcached …","relpermalink":"/cilium-handbook/security/proxy/","summary":"Cilium 能够透明地将四层代理注入任何网络连接中。这被用作执行更高级别网络策略的基础（请参阅基于 DNS 和 七层示例）。 可以注入以下代理： Envoy 注入 Envoy 代理 注意 此功能目前正处于测试阶段。 如果你有兴趣编写 Envoy 代理的 Go 扩展，请参","title":"代理注入"},{"content":"策略规则到端点映射 确定哪些策略规则当前对端点有效，并且可以将来自 cilium endpoint list 和 cilium endpoint get 的数据与 cilium policy get 配对。cilium endpoint get 将列出适用于端点的每个规则的标签。可以传递标签列表给 cilium policy get 以显示确切的源策略。请注意，不能单独获取没有标签的规则（无标签会返回节点上的完整策略）。具有相同标签的规则将一起返回。\n在下面的示例中，其中一个 deathstar pod 的端点 id 是 568。我们可以打印应用于它的所有策略：\n$ # Get a shell on the Cilium pod $ kubectl exec -ti cilium-88k78 -n kube-system -- /bin/bash $ # print out the ingress labels $ # clean up the data $ # fetch each policy via each set of labels $ # (Note that while …","relpermalink":"/cilium-handbook/policy/troubleshooting/","summary":"策略规则到端点映射 确定哪些策略规则当前对端点有效，并且可以将来自 cilium endpoint list 和 cilium endpoint get 的数据与 cilium policy get 配对。cilium endpoint get 将列出适用于端点的每个规则的标签。可以传递标签列表给 cilium policy get 以显示确切的源策略。请注意，","title":"故障排除"},{"content":"现在你已经看到了 eBPF 编程的例子，了解到它是如何工作的。虽然基础示例使得 eBPF 看起来相对简单，但也有一些复杂的地方使得 eBPF 编程充满挑战。\n长期以来，有个问题使得编写和发布 eBPF 程序相对困难，那就是内核兼容性。\n跨内核的可移植性 eBPF 程序可以访问内核数据结构，而这些结构可能在不同的内核版本中发生变化。这些结构本身被定义在头文件中，构成了 Linux 源代码的一部分。在过去编译 eBPF 程序时，必须基于你想运行这些程序的内核兼容的头文件集。\nBCC 对可移植性的处理方法 为了解决跨内核的可移植性问题，BCC 1（BPF 编译器集合，BPF Compiler Collection）项目采取了在运行时编译 eBPF 代码的方法，在目标机器上就地进行。这意味着编译工具链需要安装到每个你想让代码运行 2 的目标机器上，而且你必须在工具启动之前等待编译完成，而且文件系统上必须有内核头文件（实际上并不总是这样）。这就引出了 BPF CO-RE。\nCO-RE CO-RE（Compile Once, Run Everyone，编译一次，到处运行）方法由以下元素组成。 …","relpermalink":"/what-is-ebpf/ebpf-complexity/","summary":"现在你已经看到了 eBPF 编程的例子，了解到它是如何工作的。虽然基础示例使得 eBPF 看起来相对简单，但也有一些复杂的地方使得 eBPF 编程充满挑战。 长期以来，有个问题使得编写和发布 eBPF 程序相对困难，那就是内核兼容性。 跨内核的","title":"第四章：eBPF 的复杂性"},{"content":"这一章将介绍 Kubernetes 的设计理念及基本概念。\nKubernetes 设计理念与分布式系统 分析和理解 Kubernetes 的设计理念可以使我们更深入地了解 Kubernetes 系统，更好地利用它管理分布式部署的云原生应用，另一方面也可以让我们借鉴其在分布式系统设计方面的经验。\n分层架构 Kubernetes 设计理念和功能其实就是一个类似 Linux 的分层架构，如下图所示\n   Kubernetes 分层架构示意图   核心层：Kubernetes 最核心的功能，对外提供 API 构建高层的应用，对内提供插件式应用执行环境 应用层：部署（无状态应用、有状态应用、批处理任务、集群应用等）和路由（服务发现、DNS 解析等） 管理层：系统度量（如基础设施、容器和网络的度量），自动化（如自动扩展、动态 Provision 等）以及策略管理（RBAC、Quota、PSP、NetworkPolicy 等） 接口层：kubectl 命令行工具、客户端 SDK 以及集群联邦 生态系统：在接口层之上的庞大容器集群管理调度的生态系统，可以划分为两个范畴  Kubernetes 外部：日 …","relpermalink":"/kubernetes-handbook/architecture/perspective/","summary":"这一章将介绍 Kubernetes 的设计理念及基本概念。 Kubernetes 设计理念与分布式系统 分析和理解 Kubernetes 的设计理念可以使我们更深入地了解 Kubernetes 系统，更好地利用它管理分布式部署的云原生应用，另一方面也可以让我们借鉴其在分布式系统设计方面的经","title":"Kubernetes 的设计理念"},{"content":"从理论上讲，DevSecOps 原语可以应用于许多应用架构，但最适合于基于微服务的架构，由于应用是由相对较小的、松散耦合的模块组成的，被称为微服务，因此允许采用敏捷开发模式。即使在基于微服务的架构中，DevSecOps 原语的实现也可以采取不同的形式，这取决于平台。在本文中，所选择的平台是一个容器编排和资源管理平台（如 Kubernetes）。该平台是物理（裸机）或虚拟化（如虚拟机、容器）基础设施上的一个抽象层。为了在本文中明确提及该平台或应用环境，它被称为 DevSecOps 原语参考平台，或简称为 参考平台。\n在描述参考平台的 DevSecOps 原语的实现之前，我们假设在部署服务网格组件方面采用了以下 尽职调查：\n 用于部署和管理基于服务网格的基础设施（如网络路由）、策略执行和监控组件的安全设计模式 测试证明这些服务网格组件在应用的各个方面（如入口、出口和内部服务）的各种情况下都能按预期工作。  为参考平台实施 DevSecOps 原语所提供的指导与 (a) DevSecOps 管道中使用的工具和 (b) 提供应用服务的服务网格软件无关，尽管来自 Istio 等服务网格产品的例子 …","relpermalink":"/service-mesh-devsecops/intro/scope/","summary":"从理论上讲，DevSecOps 原语可以应用于许多应用架构，但最适合于基于微服务的架构，由于应用是由相对较小的、松散耦合的模块组成的，被称为微服务，因此允许采用敏捷开发模式。即使在基于微服务的架构中，D","title":"1.1 范围"},{"content":"本章中，我们讨论了希望通过软件赋予我们业务的能力并迁移到云原生应用架构的动机：\n速度\n比我们的竞争对手更快速得创新、试验并传递价值。\n安全\n在保持稳定性、可用性和持久性的同时，具有快速行动的能力。\n扩展\n根据需求变化弹性扩展。\n移动性\n客户可以随时随地通过任何设备无缝的跟我们交互。\n我们还研究了云原生应用架构的独特特征，以及如何赋予我们这些能力：\n12因素应用\n一套优化应用设计速度，安全性和规模的模式。\n微服务\n一种架构模式，可帮助我们将部署单位与业务能力保持一致，使每个业务能够独立和自主地移动，这样一来也更快更安全。\n自服务敏捷基础设施\n云平台使开发团队能够在应用和服务抽象层面上运行，提供基础架构级速度，安全性和扩展性。\n基于API的协作\n将服务间交互定义为可自动验证协议的架构模式，通过简化的集成工作实现速度和安全性。\n抗脆弱性\n随着速度和规模扩大，系统的压力随之增加，系统的响应能力和安全性也随之提高。\n在下一章中，我们将探讨大多数企业为采用云原生应用架构而需要做出哪些改变。\n 下一章   ","relpermalink":"/migrating-to-cloud-native-application-architectures/the-rise-of-cloud-native/summary/","summary":"本章中，我们讨论了希望通过软件赋予我们业务的能力并迁移到云原生应用架构的动机： 速度 比我们的竞争对手更快速得创新、试验并传递价值。 安全 在保持稳定性、可用性和持久性的同时，具有快速行动的能力。 扩展 根据需求","title":"1.3：本章小结"},{"content":"本章中，我们探讨了大多数企业采用云原生应用架构所需要做出的变革。从宏观总体上看是权力下放和自治：\nDevOps\n技能集中化转变为跨职能团队。\n持续交付\n发行时间表和流程的权力下放。\n自治\n决策权力下放。\n我们将这种权力下放编成两个主要的团队结构：\n业务能力团队\n自主决定设计、流程和发布时间表的跨职能团队。\n平台运营团队\n为跨职能团队提供他们所需要运行平台。\n而在技术上，我们也分散自治：\n单体应用到微服务\n将个人业务能力的控制分配给单个自主服务。\n有界上下文\n将业务领域模型的内部一致子集的控制分配到微服务。\n容器化\n将对应用包装的控制分配给业务能力团队。\n编排\n将服务集成控制分配给服务端点。\n所有这些变化造就了无数的自治单元，辅助我们以期望的创新速度安全前行。\n在最后一章中，我们将通过一组操作手册，深入研究迁移到云原生应用架构的技术细节。\n 下一章   ","relpermalink":"/migrating-to-cloud-native-application-architectures/changes-needed/summary/","summary":"本章中，我们探讨了大多数企业采用云原生应用架构所需要做出的变革。从宏观总体上看是权力下放和自治： DevOps 技能集中化转变为跨职能团队。 持续交付 发行时间表和流程的权力下放。 自治 决策权力下放。 我们将这种权力下放编","title":"2.4 本章小结"},{"content":"策略即代码涉及编纂所有策略，并作为 CI/CD 管道的一部分运行，使其成为应用程序运行时的一个组成部分。策略类别的例子包括授权策略、网络策略和实现工件策略（例如，容器策略）。典型的 策略即代码软件的策略管理能力可能带有一套预定义的策略类别和策略，也支持通过提供策略模板定义新的策略类别和 相关策略。策略即代码所要求的尽职调查是，它应该提供保护，防止与应用环境（包括基础设施）相关的所有已知威胁，只有当该代码被定期扫描和更新，以应对与应用类别（如网络应用）和托管基础设施相关的威胁，才能确保这一点。下面的表 1 中给出了一些策略类别和相关策略的例子。\n表 1：策略类别和策略实例\n   策略类别 策略示例     网络策略和零信任策略 - 封锁指定端口 - 指定入口主机名称 - 一般来说，所有的网络访问控制策略   实施工件策略（例如，容器策略） - 对服务器进行加固，对基础镜像进行漏洞扫描 - 确保容器不以 root 身份运行 - 阻止容器的权限升级   存储策略 - 设置持久性卷大小 - 设置持久性卷回收策略   访问控制策略 - 确保策略涵盖所有数据对象 - 确保策略涵盖管理和应用访问的所 …","relpermalink":"/service-mesh-devsecops/implement/ci-cd-pipeline-for-policy-as-code/","summary":"策略即代码涉及编纂所有策略，并作为 CI/CD 管道的一部分运行，使其成为应用程序运行时的一个组成部分。策略类别的例子包括授权策略、网络策略和实现工件策略（例如，容器策略）。典型的 策略即代码软件的策略管理能力可能","title":"4.4 策略即代码的 CI/CD 管道"},{"content":"云原生基础架构并不适合所有人。任何架构设计都经过了一系列的权衡。您只有熟悉自己的需求才能决定哪些权衡是有益的，哪些是有害的。\n不要在不了解架构的影响和限制的情况下采用工具或设计。我们相信云原生基础架构有很多好处，但需要意识到不应该盲目的采用。我们不愿意引导大家通过错误的方式来满足需求。\n怎么知道是否应该使用云原生基础架构设计？确定云原生基础架构是否适合您，下面是一些需要了解的问题：\n 您有云原生应用程序吗？(有关可从云原生基础架构中受益的应用程序功能，请参阅第 1 章) 您的工程团队是否愿意且能够编写出体现其作业功能的生产质量代码？ 您在本地或公有云是否拥有基于 API 驱动的基础架构（IaaS）？ 您的业务是否需要更快的开发迭代或非线性人员 / 系统缩放比例？  如果您对所有这些问题都回答 “yes”，那么您可能会从本书其余部分介绍的基础架构中受益。如果您对这些问题中的某个问题回答是 “no”，这并不意味着您无法从某些云原生实践中受益，但是您可能需要做更多工作，然后才能从此类基础架构中充分受益。\n在业务准备好之前武断地采用云原生基础架构效果会很糟糕，因为这会强制使用一个不正确的解决方 …","relpermalink":"/cloud-native-infra/when-to-adopt-cloud-native/","summary":"云原生基础架构并不适合所有人。任何架构设计都经过了一系列的权衡。您只有熟悉自己的需求才能决定哪些权衡是有益的，哪些是有害的。 不要在不了解架构的影响和限制的情况下采用工具或设计。我们相信云原生基础架构有","title":"第 2 章：采纳云原生基础架构的时机"},{"content":"勘误 1\nPDF 原文第 4 页，kubelet 端口，默认应为 10250，而不是 10251。\n","relpermalink":"/kubernetes-hardening-guidance/corrigendum/","summary":"勘误 1 PDF 原文第 4 页，kubelet 端口，默认应为 10250，而不是 10251。","title":"勘误"},{"content":"本节将为你介绍 HTTP 连接管理器中的请求匹配。\n路径匹配 我们只谈了一个使用前缀字段匹配前缀的匹配规则。下面的表格解释了其他支持的匹配规则。\n   规则名称 描述     prefix 前缀必须与path标头的开头相匹配。例如，前缀 /api 将匹配路径 /api 和 /api/v1，而不是 /。   path 路径必须与确切的path标头相匹配（没有查询字符串）。例如，路径 /api 将匹配路径 /api，但不匹配 /api/v1 或 /。   safe_regex 路径必须符合指定的正则表达式。例如，正则表达式 ^/products/\\d+$ 将匹配路径 /products/123 或 /products/321，但不是 /products/hello 或 /api/products/123。   connect_matcher 匹配器只匹配 CONNECT 请求（目前在 Alpha 中）。    默认情况下，前缀和路径匹配是大小写敏感的。要使其不区分大小写，我们可以将 case_sensitive 设置为 false。注意，这个设置不适用于 safe_regex 匹配。 …","relpermalink":"/envoy-handbook/hcm/request-matching/","summary":"本节将为你介绍 HTTP 连接管理器中的请求匹配。 路径匹配 我们只谈了一个使用前缀字段匹配前缀的匹配规则。下面的表格解释了其他支持的匹配规则。 规则名称 描述 prefix 前缀必须与path标头的开头相匹配。例如，前缀 /api 将匹配路","title":"请求匹配"},{"content":"我们在前面看到了通过客户端库来治理服务的架构图，那是我们在改造成服务网格架构前使用微服务架构通常的形式，下图是使用服务网格架构的最终形式。\n   服务网格架构示意图  当然在达到这一最终形态之前我们需要将架构一步步演进，下面给出的是参考的演进路线。\nIngress 或边缘代理 如果你使用的是 Kubernetes 做容器编排调度，那么在进化到服务网格架构之前，通常会使用 Ingress Controller，做集群内外流量的反向代理，如使用 Traefik 或 Nginx Ingress Controller。\n   Ingress 或边缘代理架构示意图  这样只要利用 Kubernetes 的原有能力，当你的应用微服务化并容器化需要开放外部访问且只需要 L7 代理的话这种改造十分简单，但问题是无法管理服务间流量。\n路由器网格 Ingress 或者边缘代理可以处理进出集群的流量，为了应对集群内的服务间流量管理，我们可以在集群内加一个 Router 层，即路由器层，让集群内所有服务间的流量都通过该路由器。\n   路由器网格架构示意图  这个架构无需对原有的单体应用和新的微服务应用做什么改 …","relpermalink":"/cloud-native-handbook/service-mesh/service-mesh-patterns/","summary":"我们在前面看到了通过客户端库来治理服务的架构图，那是我们在改造成服务网格架构前使用微服务架构通常的形式，下图是使用服务网格架构的最终形式。 服务网格架构示意图 当然在达到这一最终形态之前我们需要将架构一步","title":"服务网格的部署模式"},{"content":"自动化开始听起来很神奇，但我们要面对现实：计算机分析不能每天告诉你系统有什么问题或为你修复它。它只能为你节省时间。\n至此，我想停止夸赞，我必须承认自动化的一些局限性。我这样做是因为围绕结合人工智能和可观测性会有相当多的炒作。这种炒作导致了惊人的论断，大意是：“人工智能异常检测和根源分析可以完全诊断问题！” 和 “人工智能操作将完全管理你的系统！”\n谨防炒作 为了摆脱此类炒作，我想明确的是，这类梦幻般的营销主张并不是我所宣称的现代可观测性将提供的。事实上，我预测许多与人工智能问题解决有关的主张大多是夸大其词。\n为什么人工智能不能解决我们的问题？一般来说，机器无法识别软件中的 “问题”，因为定义什么是 “问题” 需要一种主观的分析方式。而我们所讨论的那种现实世界的人工智能不能以任何程度的准确性进行主观决策。机器将始终缺乏足够的背景。\n例如，我们说该版本降低了性能，这里面也隐含了一个期望，就是这个版本包含了每个用户都期望的新功能。对于这个版本，性能退步是一个特征，而不是一个错误。\n虽然它们可能看起来像类似的活动，但在确定相关关系和确定根本原因之间存在着巨大的鸿沟。当你有正确的数据结构时，相关 …","relpermalink":"/opentelemetry-obervability/the-limitations-of-automated-analysis/","summary":"第 3 章：自动分析的局限性","title":"第 3 章：自动分析的局限性"},{"content":"为什么尽快进行 Code Review？ 在Google，我们优化了开发团队共同开发产品的速度。，而不是优化单个开发者编写代码的速度。个人开发的速度很重要，它并不如整个团队的速度那么重要。\n当代码审查很慢时，会发生以下几件事：\n 整个团队的速度降低了。是的，对审查没有快速响应的个人的确完成了其他工作。但是，对于团队其他人来说重要的新功能与缺陷修復将会被延迟数天、数周甚至数月，只因为每个 CL 正在等待审查和重新审查。 开发者开始抗议代码审查流程。如果审查者每隔几天只响应一次，但每次都要求对 CL 进行重大更改，那么开发者可能会变得沮丧。通常，开发者将表达对审查者过于“严格”的抱怨。如果审查者请求相同实质性更改（确实可以改善代码健康状况），但每次开发者进行更新时都会快速响应，则抱怨会逐渐消失。大多数关于代码审查流程的投诉实际上是通过加快流程来解决的。 代码健康状况可能会受到影响。如果审查速度很慢，则造成开发者提交不尽如人意的 CL 的压力会越来越大。审查太慢还会阻止代码清理、重构以及对现有 CL 的进一步改进。  Code Review 应该有多快？ 如果您没有处于重点任务的中，那么您应 …","relpermalink":"/eng-practices/review/reviewer/speed/","summary":"为什么尽快进行 Code Review？ 在Google，我们优化了开发团队共同开发产品的速度。，而不是优化单个开发者编写代码的速度。个人开发的速度很重要，它并不如整个团队的速度那么重要。 当代码审查很慢时，会发生","title":"Code Review 速度"},{"content":"本节介绍 Kubernetes 的网络策略方面。\n命名空间 命名空间 用于在 Kubernetes 中创建虚拟集群。包括 NetworkPolicy 和 CiliumNetworkPolicy 在内的所有 Kubernetes 对象都属于一个特定的命名空间。根据定义和创建策略的方式，会自动考虑 Kubernetes 命名空间：\n 作为 CiliumNetworkPolicy CRD 和 NetworkPolicy 创建和导入的网络策略适用于命名空间内，即该策略仅适用于该命名空间内的 pod。但是，可以授予对其他命名空间中的 pod 的访问权限，如下所述。 通过 API 参考 直接导入的网络策略适用于所有命名空间，除非如下所述指定命名空间选择器。  提示\n 虽然有意支持通过 fromEndpoints 和 toEndpoints 中的 k8s:io.kubernetes.pod.namespace 标签指定命名空间 。禁止在 endpointSelector 中指定命名空间，因为这将违反 Kubernetes 的命名空间隔离原则。endpointSelector …","relpermalink":"/cilium-handbook/policy/kubernetes/","summary":"本节介绍 Kubernetes 的网络策略方面。 命名空间 命名空间 用于在 Kubernetes 中创建虚拟集群。包括 NetworkPolicy 和 CiliumNetworkPolicy 在内的所有 Kubernetes 对象都属于一个特定的命名空间。根据定义和创建策略的方式，会自动考虑 Kubernetes 命名空间： 作为 CiliumNetworkPolicy CRD 和 NetworkPolicy 创建和导入的网络策略适","title":"Kubernetes 网络策略"},{"content":"如果你在 Kubernetes 上运行 Cilium，你可以从 Kubernetes 为你分发的策略中受益。在这种模式下，Kubernetes 负责在所有节点上分发策略，Cilium 会自动应用这些策略。三种格式可用于使用 Kubernetes 本地配置网络策略：\n 在撰写本文时，标准的 NetworkPolicy 资源支持指定三层/四层入口策略，并带有标记为 beta 的有限出口支持。 扩展的 CiliumNetworkPolicy 格式可用作 CustomResourceDefinition 支持入口和出口的三层到七层层策略规范。 CiliumClusterwideNetworkPolicy 格式，它是集群范围的CustomResourceDefinition，用于指定由 Cilium 强制执行的集群范围的策略。规范与 CiliumNetworkPolicy 相同，没有指定命名空间。  Cilium 支持同时运行多个策略类型。但是，在同时使用多种策略类型时应谨慎，因为理解跨多种策略类型的完整允许流量集可能会令人困惑。如果不注意，这可能会导致意外的策略允许行为。\n网络策略 有关详细信 …","relpermalink":"/cilium-handbook/kubernetes/policy/","summary":"如果你在 Kubernetes 上运行 Cilium，你可以从 Kubernetes 为你分发的策略中受益。在这种模式下，Kubernetes 负责在所有节点上分发策略，Cilium 会自动应用这些策略。三种格式可用于使用 Kubernetes 本地配置网络策略： 在撰写本","title":"网络策略"},{"content":"近年来，云原生应用已呈指数级增长。在本章中，我将讨论为什么 eBPF 如此适合于云原生环境。为了更具象化，我将提到 Kubernetes，但同样适用于任何容器平台。\n每台主机一个内核 要理解为什么 eBPF 在云原生世界中如此强大，你需要搞清楚一个概念：每台机器（或虚拟机）只有一个内核，所有运行在该机器上的容器都共享同一个内核 1 如 图 5-1 所示，内核了解主机上运行的所有应用代码。\n  图 5-1. 同一主机上的所有容器共享一个内核  通过对内核的检测，就像我们在使用 eBPF 时做的那样，我们可以同时检测在该机器上运行的所有应用程序代码。当我们将 eBPF 程序加载到内核并将其附加到事件上时，它就会被触发，而不考虑哪个进程与该事件有关。\neBPF 与 sidecar 模式的比较 在 eBPF 之前，Kubernetes 的可观测性和安全工具大多都采用了 sidecar 模式。这种模式允许你在与应用程序相同的 pod 中，单独部署一个工具容器。这种模式的发明是一个进步，因为这意味着不再需要直接在应用程序中编写工具代码。仅仅通过部署 sidecar，工具就获得了同一 pod 中的其 …","relpermalink":"/what-is-ebpf/ebpf-in-cloud-native-environments/","summary":"近年来，云原生应用已呈指数级增长。在本章中，我将讨论为什么 eBPF 如此适合于云原生环境。为了更具象化，我将提到 Kubernetes，但同样适用于任何容器平台。 每台主机一个内核 要理解为什么 eBPF 在云原生世界中如此","title":"第五章：云原生环境中的 eBPF"},{"content":"Etcd 是 Kubernetes 集群中的一个十分重要的组件，用于保存集群所有的网络配置和对象的状态信息。在后面具体的安装环境中，我们安装的 etcd 的版本是 v3.1.5，整个 Kubernetes 系统中一共有两个服务需要用到 etcd 用来协同和存储配置，分别是：\n 网络插件 flannel、对于其它网络插件也需要用到 etcd 存储网络的配置信息 Kubernetes 本身，包括各种对象的状态和元信息配置  注意：flannel 操作 etcd 使用的是 v2 的 API，而 Kubernetes 操作 etcd 使用的 v3 的 API，所以在下面我们执行 etcdctl 的时候需要设置 ETCDCTL_API 环境变量，该变量默认值为 2。\n原理 Etcd 使用的是 raft 一致性算法来实现的，是一款分布式的一致性 KV 存储，主要用于共享配置和服务发现。关于 raft 一致性算法请参考 该动画演示。\n关于 Etcd 的原理解析请参考 Etcd 架构与实现解析。\n使用 Etcd 存储 Flannel 网络信息 我们在安装 Flannel …","relpermalink":"/kubernetes-handbook/architecture/etcd/","summary":"Etcd 是 Kubernetes 集群中的一个十分重要的组件，用于保存集群所有的网络配置和对象的状态信息。在后面具体的安装环境中，我们安装的 etcd 的版本是 v3.1.5，整个 Kubernetes 系统中一共有两个服务需要用到 etcd 用来协同和存储配置，分别是：","title":"Etcd 解析"},{"content":"在联邦政府的各个机构中，有几个 DevSecOps 倡议，其重点和焦点各不相同，这取决于软件和任务需求所带来的流程。尽管并不详尽，但以下是对 这些倡议 的简要概述：\n DevSecOps 管道参与构建、签入和签出一个名为 Iron Bank 的容器镜像仓库，这是一个经过国防部审查的强化容器镜像库。 空军的 Platform One，也就是实现了连续操作授权（C-ATO）概念的 DevSecOps 平台，这又简化了国防部的授权程序，以适应现代连续软件部署的速度和频率。 国家地理空间情报局（NGA）在 \u0026#34;NGA 软件之路\u0026#34; 中概述了其 DevSecOps 战略，其中为其每个软件产品规定了三个关键指标：可用性、准备时间和部署频率，以及用于实现 DevSecOps 管道的七个不同产品系列的规格，包括消息传递和工作流工具。 医疗保险和医疗补助服务中心（CMS）正在采用一种 DevSecOps 方法，其中一个重点是为软件材料清单（SBOM）奠定基 —— 这是一种正式记录，包含用于构建软件的各种组件的细节和供应链关系。制作 SBOM 的目的是为了实现持续诊断和缓解（CDM）计划下的目标。 在海军水面 …","relpermalink":"/service-mesh-devsecops/intro/related-devsecops-initiatives/","summary":"在联邦政府的各个机构中，有几个 DevSecOps 倡议，其重点和焦点各不相同，这取决于软件和任务需求所带来的流程。尽管并不详尽，但以下是对 这些倡议 的简要概述： DevSecOps 管道参与构建、签入和签出一个名为 Iron Bank 的容器镜像仓库，这是一","title":"1.2 相关的 DevSecOps 倡议"},{"content":"可观测性即代码在应用程序的每个服务组件中部署一个监控代理，以收集三种类型的数据（在第 4.1 节中描述），将它们发送到专门的工具，将它们关联起来，进行分析，并在仪表板上显示分析后的综合数据，以呈现整个应用程序级别的情况。这种综合数据的一个例子是日志模式，它提供了一个日志数据的视图，该视图是在使用一些标准（例如，一个服务或一个事件）对日志数据进行过滤后呈现的。数据根据共同的模式（例如，基于时间戳或 IP 地址范围）被分组，以方便解释。不寻常的发生被识别出来，然后这些发现可以被用来指导和加速 进一步的调查。\n","relpermalink":"/service-mesh-devsecops/implement/ci-cd-pipeline-for-observability-as-code/","summary":"可观测性即代码在应用程序的每个服务组件中部署一个监控代理，以收集三种类型的数据（在第 4.1 节中描述），将它们发送到专门的工具，将它们关联起来，进行分析，并在仪表板上显示分析后的综合数据，以呈现整个应用程序","title":"4.5 可观测性即代码的 CI/CD 管道"},{"content":"我们在前一章中讨论了在采用云原生基础架构的前提。在部署之前，需要有由 API 驱动的基础架构（IaaS）供给。\n在本章中，我们将探讨云原生基础架构拓扑的概念，并在云中实现它们。我们将学习可以帮助运维人员控制其基础架构的常用工具和模式。\n部署基础架构的第一步应该是能够将其表述出来。传统上，可以在白板上处理，或者如果幸运的话，可以在公司 wiki 上存储的文档中处理。今天，一切都变得更加程序化，基础架构表述通常以便于应用程序解释的方式记录。无论如何表述，全面的表述基础架构的需求是不变的。\n正如人们所期望的那样，精巧的云基础架构可以从简单的设计到非常复杂的设计。无论复杂性如何，必须对基础架构的表现给予高度的重视，以确保设计的可重复性。能够清晰地传递想法更为重要。因此，明确、准确和易于理解的基础架构级资源表述势在必行。\n我们也将从制作精良的表述中获得很多好处：\n 随着时间的推移，基础架构设计可以共享和版本化。 基础架构设计可以被 fork 和修改以适应特殊情况。 表述隐含的是文档。  随着本章向前推进，我们将看到基础架构表述是如何成为基础架构部署的第一步。我们将以不同的方式探索表述基础架构的能 …","relpermalink":"/cloud-native-infra/evolution-of-cloud-native-developments/","summary":"我们在前一章中讨论了在采用云原生基础架构的前提。在部署之前，需要有由 API 驱动的基础架构（IaaS）供给。 在本章中，我们将探讨云原生基础架构拓扑的概念，并在云中实现它们。我们将学习可以帮助运维人员控制其基","title":"第 3 章：云原生部署的演变"},{"content":"Kubernetes，经常被缩写为 “K8s”，是一个开源的容器或编排系统，用于自动部署、扩展和管理容器化应用程序。它管理着构成集群的所有元素，从应用中的每个微服务到整个集群。与单体软件平台相比，将容器化应用作为微服务使用可以提供更多的灵活性和安全优势，但也可能引入其他复杂因素。\n   图 1：Kubernetes 集群组件的高层视图  本指南重点关注安全挑战，并尽可能提出适用于国家安全系统和关键基础设施管理员的加固策略。尽管本指南是针对国家安全系统和关键基础设施组织的，但也鼓励联邦和州、地方、部落和领土（SLTT）政府网络的管理员实施所提供的建议。Kubernetes 集群的安全问题可能很复杂，而且经常在利用其错误配置的潜在威胁中被滥用。以下指南提供了具体的安全配置，可以帮助建立更安全的 Kubernetes 集群。\n建议 每个部分的主要建议摘要如下：\n Kubernetes Pod 安全  使用构建的容器，以非 root 用户身份运行应用程序 在可能的情况下，用不可变的文件系统运行容器 扫描容器镜像，以发现可能存在的漏洞或错误配置 使用 Pod 安全政策来执行最低水平的安全，包括: …","relpermalink":"/kubernetes-hardening-guidance/introduction/","summary":"Kubernetes，经常被缩写为 “K8s”，是一个开源的容器或编排系统，用于自动部署、扩展和管理容器化应用程序。它管理着构成集群的所有元素，从应用中的每个微服务到整个集群。与","title":"简介"},{"content":"Envoy 支持在同一虚拟主机内将流量分割到不同的路由。我们可以在两个或多个上游集群之间分割流量。\n有两种不同的方法。第一种是使用运行时对象中指定的百分比，第二种是使用加权集群。\n使用运行时的百分比进行流量分割 使用运行时对象的百分比很适合于金丝雀发布或渐进式交付的场景。在这种情况下，我们想把流量从一个上游集群逐渐转移到另一个。\n实现这一目标的方法是提供一个 runtime_fraction 配置。让我们用一个例子来解释使用运行时百分比的流量分割是如何进行的。\nroute_config:virtual_hosts:- name:hello_vhostdomains:[\u0026#34;hello.io\u0026#34;]routes:- match:prefix:\u0026#34;/\u0026#34;runtime_fraction:default_value:numerator:90denominator:HUNDREDroute:cluster:hello_v1- match:prefix:\u0026#34;/\u0026#34;route:cluster:hello_v2上述配置声明了两个版本的 hello 服务：hello_v1 和 hello_v2。\n在第一个匹配中，我们 …","relpermalink":"/envoy-handbook/hcm/traffic-splitting/","summary":"Envoy 支持在同一虚拟主机内将流量分割到不同的路由。我们可以在两个或多个上游集群之间分割流量。 有两种不同的方法。第一种是使用运行时对象中指定的百分比，第二种是使用加权集群。 使用运行时的百分比进行流量分割 使用","title":"流量分割"},{"content":"到目前为止，我们已经从数据的角度讨论了现代可观测性。但是，现代可观测性还有另一个方面，从长远来看，它可能被证明同样重要：如何对待产生数据的仪表（instrumention）。\n大多数软件系统都是用现成的部件构建的：网络框架、数据库、HTTP 客户端、代理服务器、编程语言。在大多数组织中，很少有这种软件基础设施是在内部编写的。相反，这些组件是在许多组织中共享的。最常见的是，这些共享组件是以开放源码（OSS）库的形式出现的，并具有许可权。\n由于这些开放源码软件库几乎囊括了一般系统中的所有关键功能，因此获得这些库的高质量说明对大多数可观测性系统来说至关重要。\n传统上，仪表是 “单独出售” 的。这意味着，软件库不包括产生追踪、日志或度量的仪表。相反，特定解决方案的仪表是在事后添加的，作为部署可观测系统的一部分。\n 什么是特定解决方案仪表？\n在本章中，术语 “特定解决方案仪表” 是指任何旨在与特定的可观测系统一起工作的仪表，使用的是作为该特定系统的数据存储系统的产物而开发的客户端。在这些系统中，客户端和存储系统常常深深地融合在一起。因此，如果一个应用要从一个观测系统切换到另一个观测系统，通常需要 …","relpermalink":"/opentelemetry-obervability/supporting-open-source-and-native-instrumentation/","summary":"第 4 章：支持开源和原生监测","title":"第 4 章：支持开源和原生监测"},{"content":"在这一节中，我们将解释 Envoy 的基本构建模块。\nEnvoy 配置的根被称为引导配置。它包含了一些字段，我们可以在这里提供静态或动态的资源和高级别的 Envoy 配置（例如，Envoy 实例名称、运行时配置、启用管理界面等等）。\n为了开始学习，我们将主要关注静态资源，在课程的后面，我们将介绍如何配置动态资源。\nEnvoy 输出许多统计数据，这取决于启用的组件和它们的配置。我们会在整个课程中提到不同的统计信息，在课程后面的专门模块中，我们会更多地讨论统计信息。\n下图显示了通过这些概念的请求流。\n   Envoy 构建块  这一切都从监听器开始。Envoy 暴露的监听器是命名的网络位置，可以是一个 IP 地址和一个端口，也可以是一个 Unix 域套接字路径。Envoy 通过监听器接收连接和请求。考虑一下下面的 Envoy 配置。\nstatic_resources:listeners:- name:listener_0address:socket_address:address:0.0.0.0port_value:10000filter_chains:[{}]通过上面的 Envoy 配 …","relpermalink":"/cloud-native-handbook/service-mesh/envoy-building-blocks/","summary":"在这一节中，我们将解释 Envoy 的基本构建模块。 Envoy 配置的根被称为引导配置。它包含了一些字段，我们可以在这里提供静态或动态的资源和高级别的 Envoy 配置（例如，Envoy 实例名称、运行时配置、启用管理界面等等）。 为了开","title":"Envoy 的构建模块"},{"content":"总结  保持友善。 解释你的推理。 在给出明确的指示与只指出问题并让开发人员自己决定间做好平衡。 鼓励开发人员简化代码或添加代码注释，而不仅仅是向你解释复杂性。  礼貌 一般而言，对于那些正在被您审查代码的人，除了保持有礼貌且尊重以外，重要的是还要确保您（的评论）是非常清楚且有帮助的。你并不总是必须遵循这种做法，但在说出可能令人不安或有争议的事情时你绝对应该使用它。 例如：\n糟糕的示例：“为什么这里你使用了线程，显然并发并没有带来什么好处？”\n好的示例：“这里的并发模型增加了系统的复杂性，但没有任何实际的性能优势，因为没有性能优势，最好是将这些代码作为单线程处理而不是使用多线程。”\n解释为什么 关于上面的“好”示例，您会注意到的一件事是，它可以帮助开发人员理解您发表评论的原因。 并不总是需要您在审查评论中包含此信息，但有时候提供更多解释，对于表明您的意图，您在遵循的最佳实践，或为您建议如何提高代码健康状况是十分恰当的。\n给予指导 一般来说，修复 CL 是开发人员的责任，而不是审查者。 您无需为开发人员详细设计解决方案或编写代码。\n但这并不意味着审查者应该没有帮助。一般来说，您应该在指出 …","relpermalink":"/eng-practices/review/reviewer/comments/","summary":"总结 保持友善。 解释你的推理。 在给出明确的指示与只指出问题并让开发人员自己决定间做好平衡。 鼓励开发人员简化代码或添加代码注释，而不仅仅是向你解释复杂性。 礼貌 一般而言，对于那些正在被您审查代码的人，除了保","title":"如何撰写 Code Review 评论"},{"content":"在 Kubernetes 中管理 pod 时，Cilium 将创建一个 CiliumEndpoint 的自定义资源定义（CRD）。每个由 Cilium 管理的 pod 都会创建一个 CiliumEndpoint，名称相同且在同一命名空间。CiliumEndpoint 对象包含的信息与 cilium endpoint get 在.status 字段下的 json 输出相同，但可以为集群中的所有 pod 获取。添加 -o json 将导出每个端点的更多信息。这包括端点的标签、安全身份和对其有效的策略。\n例如：\n$ kubectl get ciliumendpoints --all-namespaces NAMESPACE NAME AGE default app1-55d7944bdd-l7c8j 1h default app1-55d7944bdd-sn9xj 1h default app2 1h default app3 1h kube-system cilium-health-minikube 1h kube-system microscope 1h …","relpermalink":"/cilium-handbook/kubernetes/ciliumendpoint/","summary":"在 Kubernetes 中管理 pod 时，Cilium 将创建一个 CiliumEndpoint 的自定义资源定义（CRD）。每个由 Cilium 管理的 pod 都会创建一个 CiliumEndpoint，名称相同且在同一命名空间。CiliumEndpoint 对象包含的信息与 cilium endpoint","title":"端点 CRD"},{"content":"现在你已经了解了什么是 eBPF 以及它是如何工作的，我们再探索一些可能会在生产部署中使用的基于 eBPF 技术的工具。我们将举一些基于 eBPF 的开源项目的例子，这些项目提供了三方面的能力：网络、可观测性和安全。\n网络 eBPF 程序可以连接到网络接口和内核的网络堆栈的各个点。在每个点上，eBPF 程序可以丢弃数据包，将其发送到不同的目的地，甚至修改其内容。这就实现了一些非常强大的功能。让我们来看看通常用 eBPF 实现的几个网络功能。\n负载均衡 Facebook 正在大规模的使用 eBPF 的网络功能，因此你不必对 eBPF 用于网络的可扩展性有任何怀疑。他们是 BPF 的早期采用者，并在 2018 年推出了 Katran，一个开源的四层负载均衡器。\n另一个高度扩展的负载均衡器的例子是来自 Cloudflare 的 Unimog 边缘负载均衡器。通过在内核中运行，eBPF 程序可以操作网络数据包，并将其转发到适当的目的地，而不需要数据包通过网络堆栈和用户空间。\nCilium 项目作为一个 eBPF Kubernetes 网络插件更为人所知（我一会儿会讨论），但作为独立的负载均衡 …","relpermalink":"/what-is-ebpf/ebpf-tools/","summary":"现在你已经了解了什么是 eBPF 以及它是如何工作的，我们再探索一些可能会在生产部署中使用的基于 eBPF 技术的工具。我们将举一些基于 eBPF 的开源项目的例子，这些项目提供了三方面的能力：网络、可观测性和安全。 网络 eBPF 程序可以","title":"第六章：eBPF 工具"},{"content":"由于 DevSecOps 的基本要素跨越了开发（安全的构建和测试、打包）、交付 / 部署和持续监控（以确保运行期间的安全状态），本文建议的目标受众包括软件开发、运维和安全团队。\n","relpermalink":"/service-mesh-devsecops/intro/target-audience/","summary":"由于 DevSecOps 的基本要素跨越了开发（安全的构建和测试、打包）、交付 / 部署和持续监控（以确保运行期间的安全状态），本文建议的目标受众包括软件开发、运维和安全团队。","title":"1.3 目标受众"},{"content":"无论代码类型如何，CI/CD 管道都有一些共同的实施问题需要解决。确保流程安全涉及到为操作构建任务分配角色。自动化工具（例如，Git Secrets）可用于此目的。为保证 CI/CD 管道的安全，以下安全任务应被视为最低限度：\n 强化托管代码和工件库的服务器。 确保用于访问存储库的凭证，如授权令牌和生成拉动请求的凭证。 控制谁可以在容器镜像注册处签入和签出，因为它们是 CI 管道产生的工件的存储处，是 CI 和 CD 管道之间的桥梁。 记录所有的代码和构建更新活动。 如果在 CI 管道中构建或测试失败 —— 向开发人员发送构建报告并停止进一步的管道任务。配置代码库自动阻止来自 CD 管道的所有拉取请求。 如果审计失败，将构建报告发送给安全团队，并停止进一步的管道任务。 确保开发人员只能访问应用程序代码，而不能访问五种管道代码类型中的任何一种。 在构建和发布过程中，在每个需要的 CI/CD 阶段签署发布工件（最好是多方签署）。 在生产发布期间，验证所有需要的签名（用多个阶段的密钥生成），以确保没有人绕过管道。  ","relpermalink":"/service-mesh-devsecops/implement/securing-the-ci-cd-pipeline/","summary":"无论代码类型如何，CI/CD 管道都有一些共同的实施问题需要解决。确保流程安全涉及到为操作构建任务分配角色。自动化工具（例如，Git Secrets）可用于此目的。为保证 CI/CD 管道的安全，以下安全任务应被视为","title":"4.6 确保 CI/CD 管道的安全"},{"content":"HCM 支持在加权集群、路由、虚拟主机和 / 或全局配置层面操纵请求和响应头。\n注意，我们不能直接从配置中修改所有的 Header，使用 Wasm 扩展的情况除外。然后，我们可以修改 :authority  header，例如下面的情况。\n不可变的头是伪头（前缀为:，如:scheme）和host头。此外，诸如 :path 和 :authority 这样的头信息可以通过 prefix_rewrite、regex_rewrite 和 host_rewrite 配置来间接修改。\nEnvoy 按照以下顺序对请求 / 响应应用这些头信息：\n 加权的集群级头信息 路由级 Header 虚拟主机级 Header 全局级 Header  这个顺序意味着 Envoy 可能会用更高层次（路由、虚拟主机或全局）配置的头来覆盖加权集群层次上设置的 Header。\n在每一级，我们可以设置以下字段来添加 / 删除请求 / 响应头。\n response_headers_to_add：要添加到响应中的 Header 信息数组。 response_headers_to_remove：要从响应中移除的 Header 信息 …","relpermalink":"/envoy-handbook/hcm/header-manipulation/","summary":"HCM 支持在加权集群、路由、虚拟主机和 / 或全局配置层面操纵请求和响应头。 注意，我们不能直接从配置中修改所有的 Header，使用 Wasm 扩展的情况除外。然后，我们可以修改 :authority header，例如下面的情况。 不可变的头","title":"Header 操作"},{"content":"在前一章中，我们了解了基础架构的各种表示以及围绕其部署工具的各种方法。在本章中，我们将看看如何设计部署和管理基础架构的应用程序。在上一章中我们重点关注基础架构即软件的开放世界，有时称为基础架构即应用。\n在云原生环境中，传统的基础架构运维人员需要转变为基础架构软件工程师。与过去的其他运维角色不同，这仍然是一种新兴的做法。我们迫切需要开始探索这种模式和制定标准。\n基础架构即软件与基础架构即代码之间的根本区别在于，软件会持续运行，并会根据调节器模式创建或改变基础架构，我们将在本章后面对其进行解释。此外，基础架构即软件的新范例是，软件现在与数据存储具有更传统的关系，并公开用于定义所需状态的API。例如，该软件可能会根据数据存储中的需要改变基础架构的表示形式，并且可以很好地管理数据存储本身！希望进行协调的状态更改通过API发送到软件，而不是通过运行静态代码库中的程序。\n迈向基础架构即软件的第一步是让基础架构的运维人员意识到自己是软件工程师。我们热烈欢迎您来到这个领域！先前的工具（例如配置管理）也有类似的改变基础架构运维人员的工作职能的目标，但是运维人员通常只会在狭窄的应用范围内编写有限的DSL（ …","relpermalink":"/cloud-native-infra/designing-infrastructure-applicaitons/","summary":"在前一章中，我们了解了基础架构的各种表示以及围绕其部署工具的各种方法。在本章中，我们将看看如何设计部署和管理基础架构的应用程序。在上一章中我们重点关注基础架构即软件的开放世界，有时称为基础架构即应用。","title":"第 4 章：设计基础架构应用程序"},{"content":"Kubernetes 可以成为数据和 / 或计算能力盗窃的重要目标。虽然数据盗窃是传统上的主要动机，但寻求计算能力（通常用于加密货币挖掘）的网络行为者也被吸引到 Kubernetes 来利用其底层基础设施。除了资源盗窃，网络行为者还可能针对 Kubernetes 造成拒绝服务。下面的威胁代表了 Kubernetes 集群最可能的破坏源。\n 供应链风险 - 对供应链的攻击载体是多种多样的，并且在减轻风险方面具有挑战性。供应链风险是指对手可能颠覆构成系统的任何元素的风险，包括帮助提供最终产品的产品组件、服务或人员。这可能包括用于创建和管理 Kubernetes 集群的第三方软件和供应商。供应链的潜在威胁会在多个层面上影响 Kubernetes，包括：  容器 / 应用层面 - 在 Kubernetes 中运行的应用及其第三方依赖的安全性，它们依赖于开发者的可信度和开发基础设施的防御能力。来自第三方的恶意容器或应用程序可以为网络行为者在集群中提供一个立足点。 基础设施 - 托管 Kubernetes 的底层系统有其自身的软件和硬件依赖性。系统作为工作节点或控制平面一部分的，任何潜在威胁都可能 …","relpermalink":"/kubernetes-hardening-guidance/threat-model/","summary":"Kubernetes 可以成为数据和 / 或计算能力盗窃的重要目标。虽然数据盗窃是传统上的主要动机，但寻求计算能力（通常用于加密货币挖掘）的网络行为者也被吸引到 Kubernetes 来利用其底层基础设施。除了资源盗窃，网络行为者还可能针对 Kubernetes 造成","title":"威胁建模"},{"content":"第 2 章描述了实现自动分析所需的数据模型，第 4 章描述了支持原生 开源仪表的额外要求，并赋予各角色（应用程序所有者、运维和响应者）自主权。这就是我们对现代可观测性的概念模型。\n在本报告的其余部分，我们描述了这个新模型的一个事实实现，即 OpenTelemetry。这一章描述了构成 OpenTelemetry 遥测管道的所有组件。后面的章节将描述稳定性保证、建议的设置以及 OpenTelemetry 现实中的部署策略。有关该项目的更多细节可以在附录中找到。\n信号 OpenTelemetry 规范被组织成不同类型的遥测，我们称之为信号（signal）。主要的信号是追踪。日志和度量是其他例子。信号是 OpenTelemetry 中最基本的设计单位。\n每一个额外的信号首先是独立开发的，然后与追踪和其他相关信号整合。这种分离允许开发新的、实验性的信号，而不影响已经变得稳定的信号的兼容性保证。\nOpenTelemetry 是一个跨领域的关注点（cross-cutting concern），它在事务通过每个库和服务时追踪其执行。为了达到这个目的，所有的信号都建立在低级别的上下文传播系统之上，该系 …","relpermalink":"/opentelemetry-obervability/architectural-overview/","summary":"第 5 章：OpenTelemetry 架构概述","title":"第 5 章：OpenTelemetry 架构概述"},{"content":"HCM 是一个网络级的过滤器，将原始字节转译成 HTTP 级别的消息和事件（例如，收到的 Header，收到的 Body 数据等）。\nHCM 过滤器还处理标准的 HTTP 功能。它支持访问记录、请求 ID 生成和跟踪、Header 操作、路由表管理和统计等功能。\n从协议的角度来看，HCM 原生支持 HTTP/1.1、WebSockets、HTTP/2 和 HTTP/3（仍在 Alpha 阶段）。\nEnvoy 代理被设计成一个 HTTP/2 复用代理，这体现在描述 Envoy 组件的术语中。\nHTTP/2 术语\n在 HTTP/2 中，流是已建立的连接中的字节的双向流动。每个流可以携带一个或多个消息（message）。消息是一个完整的帧（frame）序列，映射到一个 HTTP 请求或响应消息。最后，帧是 HTTP/2 中最小的通信单位。每个帧都包含一个帧头（frame header），它至少可以识别该帧所属的流。帧可以携带有关 HTTP Header、消息有效载荷等信息。\n无论流来自哪个连接（HTTP/1.1、HTTP/2 或 HTTP/3），Envoy …","relpermalink":"/cloud-native-handbook/service-mesh/http-conneciton-manager/","summary":"HCM 是一个网络级的过滤器，将原始字节转译成 HTTP 级别的消息和事件（例如，收到的 Header，收到的 Body 数据等）。 HCM 过滤器还处理标准的 HTTP 功能。它支持访问记录、请求 ID 生成和跟踪、Header 操作、路由表管理和统计","title":"HTTP 连接管理器介绍"},{"content":"有时开发人员会拖延（Pushback）代码审查。他们要么不同意您的建议，要么抱怨您太严格。\n谁是对的？ 当开发人员不同意您的建议时，请先花点时间考虑一下是否正确。通常，他们比你更接近代码，所以他们可能真的对它的某些方面有更好的洞察力。他们的论点有意义吗？从代码健康的角度来看它是否有意义？如果是这样，让他们知道他们是对的，把问题解决。\n但是，开发人员并不总是对的。在这种情况下，审查人应进一步解释为什么认为他们的建议是正确的。好的解释在描述对开发人员回复的理解的同时，还会解释为什么请求更改。\n特别是，当审查人员认为他们的建议会改善代码健康状况时，他们应该继续提倡更改，如果他们认为最终的代码质量改进能够证明所需的额外工作是合理的。提高代码健康状况往往只需很小的几步。\n有时需要几轮解释一个建议才能才能让对方真正理解你的用意。只要确保始终保持礼貌，让开发人员知道你有听到他们在说什么，只是你不同意该论点而已。\n沮丧的开发者 审查者有时认为，如果审查者人坚持改进，开发人员会感到不安。有时候开发人员会感到很沮丧，但这样的感觉通常只会持续很短的时间，后来他们会非常感谢您在提高代码质量方面给他们的帮助。通 …","relpermalink":"/eng-practices/review/reviewer/pushback/","summary":"有时开发人员会拖延（Pushback）代码审查。他们要么不同意您的建议，要么抱怨您太严格。 谁是对的？ 当开发人员不同意您的建议时，请先花点时间考虑一下是否正确。通常，他们比你更接近代码，所以他们可能真的","title":"处理 Code Review 中的拖延"},{"content":"在 Kubernetes 中管理 pod 时，Cilium 将为 Cilium 管理的每个 pod 创建一个 CiliumEndpoint（CEP）的自定义资源定义（CRD）。如果启用了 enable-cilium-endpoint-slice，那么 Cilium 还会创建一个 CiliumEndpointSlice （CES）类型的 CRD，将一组具有相同安全身份的 CEP 对象分组到一个 CES 对象中，并广播 CES 对象来向其他代理传递身份，而不是通过广播 CEP 来实现。在大多数情况下，这减少了控制平面上的负载，可以使用相同的主资源维持更大规模的集群。\n例如：\n$ kubectl get ciliumendpointslices --all-namespaces NAME AGE ces-548bnpgsf-56q9f 171m ces-dy4d8x6j2-qgc2z 171m ces-f6qfylrxh-84vxm 171m ces-k29rv92f5-qb4sw 171m ces-m9gs68csm-w2qg8 171m ","relpermalink":"/cilium-handbook/kubernetes/ciliumendpointslice/","summary":"在 Kubernetes 中管理 pod 时，Cilium 将为 Cilium 管理的每个 pod 创建一个 CiliumEndpoint（CEP）的自定义资源定义（CRD）。如果启用了 enable-cilium-endpoint-slice，那么 Cilium 还会创","title":"端点切片 CRD"},{"content":"我希望这个简短的报告能让你了解 eBPF 和它的强大之处。我真正希望的是，你已经准备好尝试一些基于 eBPF 的工具！如果你想在技术方面深入研究，可以从 ebpf.io 开始，在那里你会找到更多关于技术和 ebPF 基金会的信息。对于编码实例，可以在 GitHub 上的 ebpf-beginners 仓库里找到。\n为了了解其他人是如何利用 eBPF 工具的，请参加 eBPF Summit 和 Cloud Native eBPF Day 等活动，在这些活动中，用户分享他们的成功和学习经验。还有一个活跃的 Slack 频道 ebpf.io/slack。我希望能在那里见到你！\n","relpermalink":"/what-is-ebpf/conclusion/","summary":"我希望这个简短的报告能让你了解 eBPF 和它的强大之处。我真正希望的是，你已经准备好尝试一些基于 eBPF 的工具！如果你想在技术方面深入研究，可以从 ebpf.io 开始，在那里你会找到更多关于技术和 ebPF 基金会的信息。对于编码实例，可","title":"第七章：结论"},{"content":"容器运行时接口（Container Runtime Interface），简称 CRI。CRI 中定义了 容器 和 镜像 的服务的接口，因为容器运行时与镜像的生命周期是彼此隔离的，因此需要定义两个服务。该接口使用 Protocol Buffer，基于 gRPC，在 Kubernetes v1.10 + 版本中是在 pkg/kubelet/apis/cri/runtime/v1alpha2 的 api.proto 中定义的。\nCRI 架构 Container Runtime 实现了 CRI gRPC Server，包括 RuntimeService 和 ImageService。该 gRPC Server 需要监听本地的 Unix socket，而 kubelet 则作为 gRPC Client 运行。\n   启用 CRI 除非集成了 rktnetes，否则 CRI 都是被默认启用了，从 Kubernetes 1.7 版本开始，旧的预集成的 docker CRI 已经被移除。\n要想启用 CRI 只需要在 kubelet 的启动参数重传入此参 …","relpermalink":"/kubernetes-handbook/architecture/open-interfaces/cri/","summary":"容器运行时接口（Container Runtime Interface），简称 CRI。CRI 中定义了 容器 和 镜像 的服务的接口，因为容器运行时与镜像的生命周期是彼此隔离的，因此需要定义两个服务。该接口使用 Protocol Buffer，","title":"容器运行时接口（CRI）"},{"content":"由于参考平台是由容器编排和资源管理平台以及服务网格软件组成的，以下出版物为确保该平台的安全提供了指导，并为本文件的内容提供了背景信息。\n SP800-204，基于微服务的应用系统的安全策略，讨论了基于微服务的应用的特点和安全要求，以及满足这些要求的总体策略。 SP800-204A，使用服务网格构建基于微服务的安全应用，为基于微服务的应用的各种安全服务（如建立安全会话、安全监控等）提供了部署指导，这些服务使用基于独立于应用代码运行的服务代理的专用基础设施（即服务网格）。 SP800-204B，使用服务网格的基于微服务的应用的基于属性的访问控制，为在服务网格中构建满足安全要求的认证和授权框架提供了部署指导，例如：（1）通过在任何一对服务之间的通信中实现相互认证来实现零信任；（2）基于访问控制模型，如基于属性的访问控制（ABAC）模型的强大访问控制机制，可用于表达广泛的策略集，并在用户群、对象（资源）和部署环境方面可扩展。 SP800-190，应用容器安全指南，解释了与容器技术相关的安全问题，并为在规划、实施和维护容器时解决这些问题提出了实用建议。这些建议是针对容器技术架构中的每个层级提供 …","relpermalink":"/service-mesh-devsecops/intro/relationship-to-other-nist-guidance-documents/","summary":"由于参考平台是由容器编排和资源管理平台以及服务网格软件组成的，以下出版物为确保该平台的安全提供了指导，并为本文件的内容提供了背景信息。 SP800-204，基于微服务的应用系统的安全策略，讨论了基于微服","title":"1.4 与其他 NIST 指导文件的关系"},{"content":"下一个常见问题涉及工作流模型。所有的 CI/CD 管道都可以有两种类型的工作流程模型，这取决于作为管道一部分部署的自动化工具。\n 基于推的模式 基于拉的模式  在支持基于推模式的 CI/CD 工具中，在管道的一个阶段或阶段所做的改变会触发后续阶段或阶段的改变。例如，通过一系列的编码脚本，CI 系统中的新构建会触发管道中 CD 部分的变化，从而改变部署基础设施（如 Kubernetes 集群）。使用 CI 系统作为部署变化的基础，其安全方面的缺点是有可能将凭证暴露在部署环境之外，尽管已尽最大努力确保 CI 脚本的安全，因为 CI 脚本是在部署基础设施的信任域之外运行的。由于 CD 工具拥有生产系统的 key，基于推送的模式就变得不安全了。\n在基于拉的工作流程模型中，与部署环境有关的运维（例如 Kubernetes 运维、Flux、ArgoCD）一旦观察到有新镜像被推送到注册表，就会从环境内部拉动新镜像。新镜像被从注册表中拉出，部署清单被自动更新，新镜像被部署在环境（如集群）中。因此，实际的部署基础设施状态与 Git 部署库中声明性描述的状态实现了衔接。此外，部署环境凭证（例如集群凭证）不 …","relpermalink":"/service-mesh-devsecops/implement/workflow-models-in-ci-cd-pipelines/","summary":"下一个常见问题涉及工作流模型。所有的 CI/CD 管道都可以有两种类型的工作流程模型，这取决于作为管道一部分部署的自动化工具。 基于推的模式 基于拉的模式 在支持基于推模式的 CI/CD 工具中，在管道的一个阶段或阶段所做的改变会","title":"4.7 CI/CD 管道中的工作流模型"},{"content":"Pod 是 Kubernetes 中最小的可部署单元，由一个或多个容器组成。Pod 通常是网络行为者在利用容器时的初始执行环境。出于这个原因，Pod 应该被加固，以使利用更加困难，并限制成功入侵的影响。\n   图3：有 sidecar 代理作为日志容器的 Pod 组件  “非 root” 容器和 “无 root” 容器引擎 默认情况下，许多容器服务以有特权的 root 用户身份运行，应用程序在容器内以 root 用户身份执行，尽管不需要有特权的执行。\n通过使用非 root 容器或无 root 容器引擎来防止 root 执行，可以限制容器受损的影响。这两种方法都会对运行时环境产生重大影响，因此应该对应用程序进行全面测试，以确保兼容性。\n非 root 容器：容器引擎允许容器以非 root 用户和非 root 组成员身份运行应用程序。通常情况下，这种非默认设置是在构建容器镜像的时候配置的。附录 A：非 root 应用的 Dockerfile 示例 显示了一个 Dockerfile 示例，它以非 root 用户身份运行一个应用。\n非 root 用户。另外，Kubernetes …","relpermalink":"/kubernetes-hardening-guidance/kubernetes-pod-security/","summary":"Pod 是 Kubernetes 中最小的可部署单元，由一个或多个容器组成。Pod 通常是网络行为者在利用容器时的初始执行环境。出于这个原因，Pod 应该被加固，以使利用更加困难，并限制成功入侵的影响。 图3：有 sidecar 代理作为日志容器的 Pod","title":"Kubernetes Pod 安全"},{"content":"在构建应用程序以管理基础架构时，我们要将需要公开的 API 与要创建的应用程序等量看待。这些 API 将代表您的基础架构的抽象，而应用程序将使用 API 消费这些这些基础架构。\n务必牢牢掌握两者的重要性，和如何利用它们来创建可扩展的弹性基础架构。\n在本章中，我们将举一个虚构的云原生应用程序和 API 示例，这些应用程序和 API 会经历正常的应用程序周期。如果您想了解更多有关管理云原生应用程序的信息，请参阅第 7 章。\n设计 API  这里的 API 是指处理数据结构中的基础架构表示，而不关心如何暴露或消费这些 API。通常使用 HTTP RESTful 端点来传递数据结构，API 如何实现对本章并不重要。\n 随着基础架构的不断发展，运行在基础架构之上的应用程序也要随之演变。为这些应用程序的功能将随着时间而改变，因此基础架构是也是隐性地演变。随着基础架构的不断发展，管理它的应用程序也必须发展。\n基础架构的功能、需求和发展将永无止境。如果幸运的话，云供应商的 API 将会保持稳定，不会频繁更改。作为基础架构工程师，我们需要做好准备，以适应这些需求。我们需要准备好发展我们的基础架构和运行其 …","relpermalink":"/cloud-native-infra/developing-infrastructure-applications/","summary":"在构建应用程序以管理基础架构时，我们要将需要公开的 API 与要创建的应用程序等量看待。这些 API 将代表您的基础架构的抽象，而应用程序将使用 API 消费这些这些基础架构。 务必牢牢掌握两者的重要性，和如何利用它们来创建可","title":"第 5 章：开发基础架构应用程序"},{"content":"HCM 支持修改和定制由 Envoy 返回的响应。请注意，这对上游返回的响应不起作用。\n本地回复是由 Envoy 生成的响应。本地回复的工作原理是定义一组映射器（mapper），允许过滤和改变响应。例如，如果没有定义任何路由或上游集群，Envoy 会发送一个本地 HTTP 404。\n每个映射器必须定义一个过滤器，将请求属性与指定值进行比较（例如，比较状态代码是否等于 403）。我们可以选择从多个过滤器来匹配状态代码、持续时间、Header、响应标志等。\n除了过滤器字段，映射器还有新的状态代码（status_code）、正文（body 和 body_format_override）和 Header（headers_to_add）字段。例如，我们可以有一个匹配请求状态代码 403 的过滤器，然后将状态代码改为 500，更新正文，或添加 Header。\n下面是一个将 HTTP 503 响应改写为 HTTP 401 的例子。注意，这指的是 Envoy 返回的状态代码。例如，如果上游不存在，Envoy 将返回一个 503。\n...- …","relpermalink":"/envoy-handbook/hcm/reply-modification/","summary":"HCM 支持修改和定制由 Envoy 返回的响应。请注意，这对上游返回的响应不起作用。 本地回复是由 Envoy 生成的响应。本地回复的工作原理是定义一组映射器（mapper），允许过滤和改变响应。例如，如果没有定义任何路由或上游集","title":"修改响应"},{"content":"OpenTelemetry 被设计成允许长期稳定性和不确定性并存的局面。在 OpenTelemetry 中，稳定性保证是在每个信号的基础上提供的。与其看版本号，不如检查你想使用的信号的稳定性等级。\n信号生命周期 图 6-1 显示了新信号是如何被添加到 OpenTelemetry 的。实验性信号仍在开发中。它们可能在任何时候改变并破坏兼容性。实验性信号的开发是以规范提案的形式开始的，它是与一组原型一起开发的。一旦实验性信号准备好在生产中使用，信号的特性就会被冻结，新信号的测试版就会以多种语言创建。测试版可能不是完整的功能，它们可能会有一些突破性的变化，但它们被认为是为早期采用者的产品反馈做好准备。一旦一个信号被认为可以被宣布为稳定版本，就会发布一个候选版本。如果候选版本能够在一段时间内保持稳定，没有问题，那么该信号的规范和测试版都被宣布为稳定。\n一旦一个信号变得稳定，它就属于 OpenTelemetry 的长期支持保障范围。OpenTelemetry 非常重视向后兼容和无缝升级。详情见以下章节。\n如果 OpenTelemetry 信号的某个组件需要退役，该组件将被标记为废弃的。被废弃的组 …","relpermalink":"/opentelemetry-obervability/stability-and-long-term-support/","summary":"第 6 章：稳定和长期支持","title":"第 6 章：稳定和长期支持"},{"content":"有时候紧急 CL 必须尽快通过 code review 过程。\n什么是紧急情况？ 紧急 CL 是这样的小更新：允许主要发布继续而不是回滚，修复显著影响用户生产的错误，处理紧迫的法律问题，关闭主要安全漏洞等。\n在紧急情况下，我们确实关心 Code Review 的整体速度，而不仅仅是响应的速度。仅在这种情况下，审查人员应该更关心审查的速度和代码的正确性（是否解决了紧急情况？）。此外（显然）这类状况的审查应该优先于所有其他 code reivew。\n但是，在紧急情况解决后，您应该再次查看紧急 CL 并进行更彻底的审查。\n什么不是紧急情况？ 需要说明的是，以下情况并非紧急情况：\n 想要在本周而不是下周推出（除非有一些实际硬性截止日期，例如合作伙伴协议）。 开发人员已经在很长一段时间内完成了一项功能想要获得 CL。 审查者都在另一个时区，目前是夜间或他们已离开现场。 现在是星期五，在开发者在过周末之前获得这个 CL 会很棒。 今天因为软（非硬）截止日期，经理表示必须完成此审核并签入 CL。 回滚导致测试失败或构建破坏的 CL。  等等。\n什么是 Hard Deadline？ 硬性截止日 …","relpermalink":"/eng-practices/review/emergencies/","summary":"有时候紧急 CL 必须尽快通过 code review 过程。 什么是紧急情况？ 紧急 CL 是这样的小更新：允许主要发布继续而不是回滚，修复显著影响用户生产的错误，处理紧迫的法律问题，关闭主要安全漏洞等。 在紧急情况下，我们确实关心 Code Review 的","title":"紧急情况"},{"content":"Cilium 与多个 Kubernetes API 组兼容。有些是废弃的或测试版的，可能只在 Kubernetes 的特定版本中可用。\n所有列出的 Kubernetes 版本都经过 e2e 测试，保证与 Cilium 兼容。本表中未列出的旧版 Kubernetes 不支持 Cilium。较新的 Kubernetes 版本，虽然没有列出，但将取决于 Kubernetes 提供的后向兼容性。\n   Kubernetes 版本 Kubernetes NetworkPolicy API CiliumNetworkPolicy     1.16, 1.17, 1.18, 1.19, 1.20, 1.21, 1.22, 1.23 networking.k8s.io/v1 cilium.io/v2 有一个 CRD    Cilium 在 Kubernetes 中使用了一个网络策略的 CRD。这个 CRD 的模式验证可能会有变化，它可以验证 Cilium Clusterwide Network Policy（CCNP）或 Cilium Network Policy（CNP）的正确性。\nCRD 本身有一 …","relpermalink":"/cilium-handbook/kubernetes/compatibility/","summary":"Cilium 与多个 Kubernetes API 组兼容。有些是废弃的或测试版的，可能只在 Kubernetes 的特定版本中可用。 所有列出的 Kubernetes 版本都经过 e2e 测试，保证与 Cilium 兼容。本表中未列出的旧版 Kubernetes 不支持 Cilium。较新的 Kubernetes 版本，虽然没有列出，但将取决于 Kubernetes 提供","title":"Kubernetes 兼容性"},{"content":"容器网络接口（Container Network Interface），简称 CNI，是 CNCF 旗下的一个项目，由一组用于配置 Linux 容器的网络接口的规范和库组成，同时还包含了一些插件。CNI 仅关心容器创建时的网络分配，和当容器被删除时释放网络资源。有关详情请查看 GitHub。\nKubernetes 源码的 vendor/github.com/containernetworking/cni/libcni 目录中已经包含了 CNI 的代码，也就是说 Kubernetes 中已经内置了 CNI。\n接口定义 CNI 的接口中包括以下几个方法：\ntype CNI interface { AddNetworkList (net *NetworkConfigList, rt *RuntimeConf) (types.Result, error) DelNetworkList (net *NetworkConfigList, rt *RuntimeConf) error AddNetwork (net *NetworkConfig, rt *RuntimeConf) …","relpermalink":"/kubernetes-handbook/architecture/open-interfaces/cni/","summary":"容器网络接口（Container Network Interface），简称 CNI，是 CNCF 旗下的一个项目，由一组用于配置 Linux 容器的网络接口的规范和库组成，同时还包含了一些插件。CNI 仅关心容器创建时的网络分配，和当容器被","title":"容器网络接口（CNI）"},{"content":"本文件的结构如下：\n第二章简要介绍了参考平台，为其提供了实施 DevSecOps 原语的指导。\n第三章介绍了 DevSecOps 的基本要素（即管道），设计和执行管道的方法，以及自动化在执行中的作用。\n第四章涵盖了管道的所有方面，包括（a）所有管道需要解决的共同问题，（b）对第 1.1 节中列出的参考平台中五种代码类型的管道的描述，以及（c）DevSecOps 在整个生命周期中对整个应用环境（有五种代码类型的参考平台，因此承载着 DevSecOps 的实施）的安全保证的好处，包括 持续授权操作（C-ATO）。\n第五章提供了摘要和结论。\n 下一章   ","relpermalink":"/service-mesh-devsecops/intro/organization-of-this-document/","summary":"本文件的结构如下： 第二章简要介绍了参考平台，为其提供了实施 DevSecOps 原语的指导。 第三章介绍了 DevSecOps 的基本要素（即管道），设计和执行管道的方法，以及自动化在执行中的作用。 第四章涵盖了管道的所有方面，包括（a）所有管","title":"1.5 本文件的组织"},{"content":"最后一个常见的问题是安全测试。无论代码类型是什么（例如，应用服务、Iac、Pac 或可观测性），基于微服务的基础设施的 DevSecOps 的 CI/CD 管道与服务网格应包括由自动化工具或作为服务提供的应用安全测试（AST）。这些工具会分析和测试应用程序的安全漏洞。根据 Gartner 的说法，有 四种 主要的 AST 技术：\n 静态 AST（SAST）工具：分析应用程序的源码、字节码或二进制代码的安全漏洞，通常在编程和 / 或测试软件生命周期（SLC）阶段。具体来说，这项技术涉及到在提交中查看应用程序并分析其依赖关系的 技术。如果任何依赖关系包含问题或已知的安全漏洞，提交将被标记为不安全的，不允许继续部署。这也可以包括在代码中找到应该被删除的硬编码密码 / 秘密。 动态 AST（DAST）工具：在测试或运行阶段，分析应用程序的动态运行状态。它们模拟针对应用程序（通常是支持网络的应用程序、服务和 API）的攻击，分析应用程序的反应，并确定它是否有漏洞。特别是，DAST 工具比 SAST 更进一步，在 CI 工作中启动生产环境的副本，以扫描所产生的容器和 可执行文件。动态方面有助于系统 …","relpermalink":"/service-mesh-devsecops/implement/security-testing-common-requirement-for-ci-cd-pipelines-for-all-code-types/","summary":"最后一个常见的问题是安全测试。无论代码类型是什么（例如，应用服务、Iac、Pac 或可观测性），基于微服务的基础设施的 DevSecOps 的 CI/CD 管道与服务网格应包括由自动化工具或作为服务提供的应用安全测试（AST）。这些工","title":"4.8 安全测试——所有代码类型的 CI/CD 管道的共同要求"},{"content":"基础架构是用来支撑应用程序的。可信任的软件对于工程成功至关重要。如果每次在终端输入 ls 命令，都会发生随机动作，那么你将永远也不会相信 ls，而是去找另一种方式来列出目录中的文件。\n我们的基础架构必须值得信任。本章旨在建立信任和验证基础架构的意识形态。我们将描述的实践旨在增加对应用程序和基础架构工程的信心。\n软件测试在当今的软件工程领域非常普遍。然而，如何测试基础架构还没有很明确的最佳实践。\n这意味着本书中的所有章节中，这一节应该是最令人兴奋的！在该领域像您这样的工程师有充分的空间发挥出色的影响力。\n软件测试是一种证明软件可以正常工作的有效做法，证明软件在各种特殊情况下软件仍然可以正常运行。因此，如果我们将相同的范例应用于基础架构测试，测试目标如下：\n 证明基础架构按预期运行。 证明基础架构不会失败。 证明这两种情况在各种边缘情况下都是正确的。  衡量基础架构是否有效需要我们先定义什么叫有效。现在，您应该对使用基础架构和工程代表应用程序的想法感到满意。\n定义基础架构 API 的人应该花时间构思一个可以创建有效基础架构的理智的 API。例如，如果创建了一个定义虚拟机的 API 但里面没 …","relpermalink":"/cloud-native-infra/testing-cloud-native-infrastructure/","summary":"基础架构是用来支撑应用程序的。可信任的软件对于工程成功至关重要。如果每次在终端输入 ls 命令，都会发生随机动作，那么你将永远也不会相信 ls，而是去找另一种方式来列出目录中的文件。 我们的基础架构必须值得信任","title":"第 6 章：测试云原生基础架构"},{"content":"唯一的请求 ID 对于通过多个服务追踪请求、可视化请求流和精确定位延迟来源至关重要。\n我们可以通过 request_id_extension 字段配置请求 ID 的生成方式。如果我们不提供任何配置，Envoy 会使用默认的扩展，称为 UuidRequestIdConfig。\n默认扩展会生成一个唯一的标识符（UUID4）并填充到 x-request-id HTTP 头中。Envoy 使用 UUID 的第 14 个位点来确定跟踪的情况。\n如果第 14 个比特位（nibble）被设置为 9，则应该进行追踪采样。如果设置为 a，应该是由于服务器端的覆盖（a）而强制追踪，如果设置为 b，应该是由客户端的请求 ID 加入而强制追踪。\n之所以选择第 14 个位点，是因为它在设计上被固定为 4。因此，4 表示一个默认的 UUID 没有跟踪状态，例如 7b674932-635d-4ceb-b907-12674f8c7267（说明：第 14 比特位实际为第 13 个数字）。\n我们在 UuidRequestIdconfig 中的两个配置选项是 pack_trace_reason …","relpermalink":"/envoy-handbook/hcm/request-id-generation/","summary":"唯一的请求 ID 对于通过多个服务追踪请求、可视化请求流和精确定位延迟来源至关重要。 我们可以通过 request_id_extension 字段配置请求 ID 的生成方式。如果我们不提供任何配置，Envoy 会使用默认的扩展，称为 UuidRequestId","title":"生成请求 ID"},{"content":"集群网络是 Kubernetes 的一个核心概念。容器、Pod、服务和外部服务之间的通信必须被考虑在内。默认情况下，很少有网络策略来隔离资源，防止集群被破坏时的横向移动或升级。资源隔离和加密是限制网络行为者在集群内转移和升级的有效方法。\n关键点\n 使用网络策略和防火墙来隔离资源。 确保控制平面的安全。 对流量和敏感数据（例如 Secret）进行静态加密。  命名空间 Kubernetes 命名空间是在同一集群内的多个个人、团队或应用程序之间划分集群资源的一种方式。默认情况下，命名空间不会被自动隔离。然而，命名空间确实为一个范围分配了一个标签，这可以用来通过 RBAC 和网络策略指定授权规则。除了网络隔离之外，策略可以限制存储和计算资源，以便在命名空间层面上对 Pod 进行更好的控制。\n默认有三个命名空间，它们不能被删除：\n kube-system（用于 Kubernetes 组件） kube-public（用于公共资源） default（针对用户资源）  用户 Pod 不应该放在 kube-system 或 kube-public 中，因为这些都是为集群服务保留的。可以用 YAML 文 …","relpermalink":"/kubernetes-hardening-guidance/network-separation-and-hardening/","summary":"集群网络是 Kubernetes 的一个核心概念。容器、Pod、服务和外部服务之间的通信必须被考虑在内。默认情况下，很少有网络策略来隔离资源，防止集群被破坏时的横向移动或升级。资源隔离和加密是限制网络行为者在集群内转移和升","title":"网络隔离和加固"},{"content":"现在我们了解了组成 OpenTelemetry 的各个构件，我们应该如何将它们组合成一个强大的生产管道？\n答案取决于你的出发点是什么。OpenTelemetry 是模块化的，设计成可以在各种不同的规模下工作。你只需要使用相关的部分。这就是说，我们已经创建了一个建议的路线图供你遵循。\n安装 OpenTelemetry 客户端 可以单独使用 OpenTelemetry 客户端而不部署收集器。这种基本设置通常是绿地部署的充分起点，无论是测试还是初始生产。OpenTelemetry SDK 可以被配置为直接向大多数可观测性服务传输遥测数据。\n挑选一个导出器 默认情况下，OpenTelemetry 使用 OTLP 导出数据。该 SDK 提供了几种常见格式的导出器。Zipkin、Prometheus、StatsD 等。如果你使用的可观测性后端没有原生支持 OTLP，那么这些其他格式中的一种很可能会被支持。安装正确的导出器并将数据直接发送到你的后端系统。\n安装库仪表 除了 SDK，OpenTelemetry 仪表必须安装在所有 HTTP 客户端、Web 框架、数据库和应用程序的消息队列中。如果这些库 …","relpermalink":"/opentelemetry-obervability/suggested-setups-and-telemetry-pipelines/","summary":"第 7 章：建议的设置和遥测管道","title":"第 7 章：建议的设置和遥测管道"},{"content":"验证安装 检查 DaemonSet 的状态并验证所有需要的实例都处于 ready 状态：\n$ kubectl --namespace kube-system get ds NAME DESIRED CURRENT READY NODE-SELECTOR AGE cilium 1 1 0 \u0026lt;none\u0026gt; 3s 在此示例中，我们看到 1 个期望的状态，0 个 ready 状态。这表明有问题。下一步是通过在 k8s-app=cilium 标签上匹配列出所有 cilium pod，并根据每个 pod 的重启次数对列表进行排序，以便轻松识别失败的 pod：\n$ kubectl --namespace kube-system get pods --selector k8s-app=cilium \\  --sort-by=\u0026#39;.status.containerStatuses[0].restartCount\u0026#39; NAME READY STATUS RESTARTS AGE cilium-813gf 0/1 CrashLoopBackOff 2 44s cilium-813gf pod 失败并且已经重新启动 …","relpermalink":"/cilium-handbook/kubernetes/troubleshooting/","summary":"验证安装 检查 DaemonSet 的状态并验证所有需要的实例都处于 ready 状态： $ kubectl --namespace kube-system get ds NAME DESIRED CURRENT READY NODE-SELECTOR AGE cilium 1 1 0 \u003cnone\u003e 3s 在此示例中，我们看到 1 个期望的状态，0 个 ready 状态。这表明有问题。下一步是通过在 k8s-app=cilium 标签上匹配列出所有 cilium pod，并根","title":"故障排除"},{"content":"容器存储接口（Container Storage Interface），简称 CSI，CSI 试图建立一个行业标准接口的规范，借助 CSI 容器编排系统（CO）可以将任意存储系统暴露给自己的容器工作负载。有关详细信息，请查看设计方案。\ncsi 卷类型是一种 out-tree（即跟其它存储插件在同一个代码路径下，随 Kubernetes 的代码同时编译的） 的 CSI 卷插件，用于 Pod 与在同一节点上运行的外部 CSI 卷驱动程序交互。部署 CSI 兼容卷驱动后，用户可以使用 csi 作为卷类型来挂载驱动提供的存储。\nCSI 持久化卷支持是在 Kubernetes v1.9 中引入的，作为一个 alpha 特性，必须由集群管理员明确启用。换句话说，集群管理员需要在 apiserver、controller-manager 和 kubelet 组件的 “--feature-gates =” 标志中加上 “CSIPersistentVolume = true”。\nCSI 持久化卷具有以下字段可供用户指定：\n driver：一个字符串值，指定要使用的卷驱动程序的名称。必须少于 63 个字 …","relpermalink":"/kubernetes-handbook/architecture/open-interfaces/csi/","summary":"容器存储接口（Container Storage Interface），简称 CSI，CSI 试图建立一个行业标准接口的规范，借助 CSI 容器编排系统（CO）可以将任意存储系统暴露给自己的容器工作负载。有关详细信息，请查看设计","title":"容器存储接口（CSI）"},{"content":"DevSecOps 的好处包括：\n 各个 IT 团队之间，特别是开发人员、运维和安全团队以及其他利益相关者之间更好的沟通和协作。导致 更好的生产力。 简化软件开发、交付和部署过程 —— 由于自动化，停机时间减少，发布时间加快，基础设施和运维成本降低，效率提高。 通过实施零信任来减少攻击面，这也限制了横向移动，从而防止攻击升级。具有现代行为预防能力的持续监控进一步促进了这一点。 安全优势。通过对每个请求的验证监控、警报和反馈机制来提高安全性，因为可观测性是代码。这些将在以下段落中详细描述。具体的能力包括：  a. 运行时：杀死恶意容器。 b. 反馈：由于一个错误的程序更新了代码并重新触发了管道，所以反馈到了正确的存储库。 c. 监测新的和终止的服务，并调整相关服务（如服务代理）。 d. 启用安全断言。不可绕过 —— 通过在同一空间执行的代理、安全会话、强大的认证和授权以及安全的状态转换。   启用持续授权操作（C-ATO），在本节末尾详细描述。 对每个请求的验证和上述的反馈机制将在下面进一步描述： 每个请求的验证。来自用户或客户端应用程序（服务）的每个请求都要经过验证和授权（使用 OPA …","relpermalink":"/service-mesh-devsecops/implement/benefits-of-devsecops-primitives-to-application-security-in-the-service-mesh/","summary":"DevSecOps 的好处包括： 各个 IT 团队之间，特别是开发人员、运维和安全团队以及其他利益相关者之间更好的沟通和协作。导致 更好的生产力。 简化软件开发、交付和部署过程 —— 由于自动化，停机时间减少，发布时间加快，基础设施和","title":"4.9 DevSecOps 原语对服务网格中应用安全的好处"},{"content":"Envoy 支持许多可配置的超时，这取决于你使用代理的场景。\n我们将在 HCM 部分看一下不同的可配置超时。请注意，其他过滤器和组件也有各自的超时时间，我们在此不做介绍。\n在配置的较高层次上设置的一些超时 —— 例如在 HCM 层次，可以覆盖较低层次上的配置，例如 HTTP 路由层次。\n最著名的超时可能是请求超时。请求超时（request_timeout）指定了 Envoy 等待接收整个请求的时间（例如 120s）。当请求被启动时，该计时器被激活。当最后一个请求字节被发送到上游时，或者当响应被启动时，定时器将被停用。默认情况下，如果没有提供或设置为 0，则超时被禁用。\n类似的超时称为 idle_timeout，表示如果没有活动流，下游或上游连接何时被终止。默认的空闲超时被设置为 1 小时。空闲超时可以在 HCM 配置的 common_http_protocol_options 中设置，如下所示。\n...filters:- name:envoy.filters.network.http_connection_managertyped_config:\u0026#34;@type\u0026#34;: …","relpermalink":"/envoy-handbook/hcm/timeout/","summary":"Envoy 支持许多可配置的超时，这取决于你使用代理的场景。 我们将在 HCM 部分看一下不同的可配置超时。请注意，其他过滤器和组件也有各自的超时时间，我们在此不做介绍。 在配置的较高层次上设置的一些超时 —— 例如在 HCM 层次，","title":"超时"},{"content":"云原生应用程序依赖基础架构才能运行，反过来说，云原生基础架构也需要云原生应用程序来维持。\n对于传统基础架构，维护和升级基本上都是由人工完成。可能是在单机上手动运行服务或使用自动化工具定义基础结构和应用程序的快照。\n但是，如果基础架构可以由应用程序管理，同时基础架构又可以管理应用程序，那么基础架构工具就会成为另一种应用程序。工程师对于基础架构的责任可以用调解器模式表示，内置于该基础架构上运行的应用程序中。\n我们花了三章来说明如何构建可以管理基础架构的应用程序。本章将介绍如何在基础架构上运行云原生应用或其它任何应用。\n如前所述，保持基础架构和应用程序的简单非常重要。解决应用程序复杂性最常用的方法就是把应用程序分解成小的，易于理解的组件。通常通过创建单一职责的服务来实现，或者将代码分解为一系列事件触发的函数。\n随着小型、可部署单元的扩容成多份即使是最自动化的基础架构也可能被压垮。管理大量应用程序的唯一方法是让它们承担第 1 章中所述的功能性操作。应用程序需要在可以按规模管理之前变成原生云。\n学习完本章不会帮助您建下一个伟大的应用程序，但您将了解一些基础知识，这块可以让您的应用程序在云原生基础 …","relpermalink":"/cloud-native-infra/managing-cloud-native-applications/","summary":"云原生应用程序依赖基础架构才能运行，反过来说，云原生基础架构也需要云原生应用程序来维持。 对于传统基础架构，维护和升级基本上都是由人工完成。可能是在单机上手动运行服务或使用自动化工具定义基础结构和应用程","title":"第 7 章：管理云原生应用程序"},{"content":"认证和授权是限制访问集群资源的主要机制。如果集群配置错误，网络行为者可以扫描知名的 Kubernetes 端口，访问集群的数据库或进行 API 调用，而不需要经过认证。用户认证不是 Kubernetes 的一个内置功能。然而，有几种方法可以让管理员在集群中添加认证。\n认证  管理员必须向集群添加一个认证方法，以实现认证和授权机制。\n Kubernetes 集群有两种类型的用户：服务账户和普通用户账户。服务账户代表 Pod 处理 API 请求。认证通常由 Kubernetes 通过 ServiceAccount Admission Controller 使用承载令牌自动管理。不记名令牌被安装到 Pod 中约定俗成的位置，如果令牌不安全，可能会在集群外使用。正因为如此，对 Pod Secret 的访问应该限制在那些需要使用 Kubernetes RBAC 查看的人身上。对于普通用户和管理员账户，没有自动的用户认证方法。管理员必须在集群中添加一个认证方法，以实现认证和授权机制。\nKubernetes 假设由一个独立于集群的服务来管理用户认证。Kubernetes 文档中列出了几种实现用户认证 …","relpermalink":"/kubernetes-hardening-guidance/authentication-and-authorization/","summary":"认证和授权是限制访问集群资源的主要机制。如果集群配置错误，网络行为者可以扫描知名的 Kubernetes 端口，访问集群的数据库或进行 API 调用，而不需要经过认证。用户认证不是 Kubernetes 的一个内置功能。然而，有几种方法可以让管理员在集","title":"认证和授权"},{"content":"上线一个新的遥测系统可能是一项复杂的工作。它需要整个工程组织的支持，不能一蹴而就。不要低估这可能会产生的问题！\n在大型组织中，通常有许多服务团队负责系统的不同部分。通常情况下，每个团队都需要付出一定的努力来使他们所管理的服务得到充分的工具化。而这些团队都有自己积压的工作，他们当然希望能够优先处理这些工作。\n不幸的是，可观测性计划在开始提供价值和证明其价值之前就会耗尽人的耐心。但通过仔细的计划和协调，这种情况是可以避免的。\n主要目标 在推广 OpenTelemetry 时，重要的是要记住，任何基于分布式追踪的可观测性系统都需要对参与事务的每个服务进行检测，以提供最大价值。如果只有部分服务被检测到，那么追踪就会被分割成小的、不相连的部分。\n这种散乱的仪表的结果是不可取的。这种情况——不一致的仪表和断裂的追踪是你想要避免的主要事情。如果追踪是断开的，运维人员仍然需要在他们的头脑中把所有的东西拼凑起来，以获得他们系统的情况。更糟糕的是，自动分析工具可以使用的数据非常有限。与人类运维人员不同，他们可以运用直觉，跳出框框来思考问题，而分析工具却只能使用他们得到的数据。由于数据有限，他们提供有用的见 …","relpermalink":"/opentelemetry-obervability/roll-out/","summary":"第 8 章：如何在组织中推广 OpenTelemetry","title":"第 8 章：如何在组织中推广 OpenTelemetry"},{"content":"在参考平台中，整个应用系统的运行状态或执行状态是由于基础设施代码（例如，用于服务间通信的网络路由、资源配置代码）、策略代码（例如，指定认证和授权策略的代码）和会话管理代码（例如，建立 mTLS 会话的代码、生成 JWT 令牌的代码）的执行的组合，这些代码由可观测性作为代码的执行所揭示。服务网格的可观测性代码在运行期间将基础设施、策略和会话管理代码的执行输出转发给各种监控工具，这些工具产生适用的指标和日志聚合工具以及追踪工具，这些工具又将其输出转发给集中式仪表板。作为这些工具输出的组成部分的分析，使系统管理员能够获得整个应用系统运行时状态的综合全局视图。正是通过持续监控和零信任设计功能实现的 DevSecOps 平台的运行时性能，为云原生应用提供了所有必要的安全保障。\n在 DevSecOps 管道中，实现持续 ATO 的活动是：\n 检查合规的代码。可以检查以下代码是否符合《风险管理框架》的规定  (a) IaC：生成网络路线，资源配置 (b) 策略即代码：对 AuthN 和 AuthZ 策略进行编码 (c) 会话管理代码：mTLS 会话，JWT 令牌 (d) 可观测性代码\n具体的风险评估 …","relpermalink":"/service-mesh-devsecops/implement/leveraging-devsecops-for-continuous-authorization-to-operate-c-ato/","summary":"在参考平台中，整个应用系统的运行状态或执行状态是由于基础设施代码（例如，用于服务间通信的网络路由、资源配置代码）、策略代码（例如，指定认证和授权策略的代码）和会话管理代码（例如，建立 mTLS 会话的代码、生成","title":"4.10 利用 DevSecOps 进行持续授权操作（C-ATO）"},{"content":"Aeraki 是腾讯云在 2021 年 3 月开源的一个服务网格领域的项目。Aeraki 提供了一个端到端的云原生服务网格协议扩展解决方案，以一种非侵入的方式为 Istio 提供了强大的第三方协议扩展能力，支持在 Istio 中对 Dubbo、Thrift、Redis，以及对私有协议进行流量管理。Aeraki 的架构如下图所示：\n   Aeraki架构图  来源：https://istio.io/latest/blog/2021/aeraki/\n从 Aeraki 架构图中可以看到，Aeraki 协议扩展解决方案包含了两个组件：\n Aeraki：Aeraki 作为一个 Istio 增强组件运行在控制面，通过自定义 CRD 向运维提供了用户友好的流量规则配置。Aeraki 将这些流量规则配置翻译为 Envoy 配置，通过 Istio 下发到数据面的 sidecar 代理上。Aeraki 还作为一个 RDS 服务器为数据面的 MetaProtocol Proxy 提供动态路由。Aeraki 提供的 RDS 和 Envoy 的 RDS 有所不同，Envoy RDS 主要为 HTTP 协议提供动 …","relpermalink":"/istio-handbook/ecosystem/aeraki/","summary":"Aeraki 是腾讯云在 2021 年 3 月开源的一个服务网格领域的项目。Aeraki 提供了一个端到端的云原生服务网格协议扩展解决方案，以一种非侵入的方式为 Istio 提供了强大的第三方协议扩展能力，支持在 Istio 中对 Dubbo、Thrif","title":"Aeraki"},{"content":"AuthorizationPolicy（授权策略）实现了对网格中工作负载的访问控制。\n授权策略支持访问控制的 CUSTOM、DENY 和 ALLOW 操作。当 CUSTOM、DENY 和 ALLOW 动作同时用于一个工作负载时，首先评估 CUSTOM 动作，然后是 DENY 动作，最后是 ALLOW 动作。评估是按以下顺序进行：\n 如果有任何 CUSTOM 策略与请求相匹配，如果评估结果为拒绝，则拒绝该请求。 如果有任何 DENY 策略与请求相匹配，则拒绝该请求。 如果没有适合该工作负载的 ALLOW 策略，允许该请求。 如果有任何 ALLOW 策略与该请求相匹配，允许该请求。 拒绝该请求。  Istio 授权策略还支持 AUDIT 动作，以决定是否记录请求。AUDIT 策略不影响请求是否被允许或拒绝到工作负载。请求将完全基于 CUSTOM、DENY 和 ALLOW 动作被允许或拒绝。\n如果工作负载上有一个与请求相匹配的 AUDIT 策略，则请求将被内部标记为应该被审计。必须配置并启用一个单独的插件，以实际履行审计决策并完成审计行为。如果没有启用这样的支持插件，该请求将不会被审计。目 …","relpermalink":"/istio-handbook/config-security/authorization-policy/","summary":"AuthorizationPolicy（授权策略）实现了对网格中工作负载的访问控制。 授权策略支持访问控制的 CUSTOM、DENY 和 ALLOW 操作。当 CUSTOM、DENY 和 ALLOW 动作同时用于一个工作负载时，首先评","title":"AuthorizationPolicy"},{"content":"为了排除 Istio 的问题，对 Envoy 的工作原理有一个基本的了解是很有帮助的。Envoy 配置是一个 JSON 文件，分为多个部分。我们需要了解 Envoy 的基本概念是监听器、路由、集群和端点。\n这些概念映射到 Istio 和 Kubernetes 资源，如下图所示。\n   Envoy 概念与 Istio 和 Kubernetes 的映射  监听器是命名的网络位置，通常是一个 IP 和端口。Envoy 对这些位置进行监听，这是它接收连接和请求的地方。\n每个 sidecar 都有多个监听器生成。每个 sidecar 都有一个监听器，它被绑定到 0.0.0.0:15006。这是 IP Tables 将所有入站流量发送到 Pod 的地址。第二个监听器被绑定到 0.0.0.0:15001，这是所有从 Pod 中出站的流量地址。\n当一个请求被重定向（使用 IP Tables 配置）到 15001 端口时，监听器会把它交给与请求的原始目的地最匹配的虚拟监听器。如果它找不到目的地，它就根据配置的 OutboundTrafficPolicy 来发送流量。默认情况下， …","relpermalink":"/istio-handbook/troubleshooting/envoy-basic/","summary":"为了排除 Istio 的问题，对 Envoy 的工作原理有一个基本的了解是很有帮助的。Envoy 配置是一个 JSON 文件，分为多个部分。我们需要了解 Envoy 的基本概念是监听器、路由、集群和端点。 这些概念映射到 Istio 和 Kubernetes 资源，如下图所示。 Envoy 概念","title":"Envoy 基础问题"},{"content":"在学习一门新的技术之前，有必要先了解它的基本语境和术语，这样在阅读后续的章节时，可以保证我们的思想一致。在这一节我将为你介绍 Envoy 中常用的术语。\nEnvoy 中常用术语有：\n 主机（Host）：能够进行网络通信的实体（在手机或服务器等上的应用程序）。在 Envoy 中主机是指逻辑网络应用程序。只要每台主机都可以独立寻址，一块物理硬件上就运行多个主机。 下游（Downstream）：下游主机连接到 Envoy，主动向 Envoy 发送请求并期待获得响应。 上游（Upstream）：上游主机收到来自 Envoy 的连接请求同时相应该请求。 集群（Cluster）: 集群是 Envoy 连接到的一组逻辑上相似的上游主机。Envoy 通过服务发现发现集群中的成员。Envoy 可以通过主动运行状况检查来确定集群成员的健康状况。Envoy 如何将请求路由到集群成员由负载均衡策略确定。 网格（Mesh）：网格是一组互相协调以提供一致网络拓扑的主机。Envoy 网格是指一组 Envoy 代理，它们构成了由多种不同服务和应用程序平台组成的分布式系统的消息传递基础。 运行时配置：与 Envoy 一 …","relpermalink":"/istio-handbook/data-plane/envoy-terminology/","summary":"在学习一门新的技术之前，有必要先了解它的基本语境和术语，这样在阅读后续的章节时，可以保证我们的思想一致。在这一节我将为你介绍 Envoy 中常用的术语。 Envoy 中常用术语有： 主机（Host）：能够进行网络通信的实体（在","title":"Envoy 中的基本术语"},{"content":"Gateway 描述了一个在网格边缘运行的负载均衡器，接收传入或传出的 HTTP/TCP 连接。该规范描述了一组应该暴露的端口、要使用的协议类型、负载均衡器的 SNI 配置等。\n示例 例如，下面的 Gateway 配置设置了一个代理，作为负载均衡器，暴露了 80 和 9080 端口（http）、443（https）、9443（https）和 2379 端口（TCP）的入口（ingress）。网关将被应用于运行在标签为 app: my-gateway-controller 的 pod 上的代理。虽然 Istio 将配置代理来监听这些端口，但用户有责任确保这些端口的外部流量被允许进入网格。\napiVersion:networking.istio.io/v1alpha3kind:Gatewaymetadata:name:my-gatewaynamespace:some-config-namespacespec:selector:app:my-gateway-controllerservers:- port:number:80name:httpprotocol:HTTPhosts:- …","relpermalink":"/istio-handbook/config-networking/gateway/","summary":"Gateway 描述了一个在网格边缘运行的负载均衡器，接收传入或传出的 HTTP/TCP 连接。该规范描述了一组应该暴露的端口、要使用的协议类型、负载均衡器的 SNI 配置等。 示例 例如，下面的 Gateway 配置设置了一个代理，作为负载均衡器，暴露了 80 和","title":"Gateway"},{"content":"Istio 1.11 引入了 Telemetry API，并在 1.13 中进行了完善。使用该 API，你可以一站式的灵活地配置指标、访问日志和追踪。\n使用 API 下面将向你介绍如何使用 Telemetry API。\n范围、继承和重写 Telemetry API 资源从 Istio 配置中继承的顺序：\n 根配置命名空间（例如：istio-system） 本地命名空间（命名空间范围内的资源，没有工作负载 selector） 工作负载（具有工作负载 selector 的命名空间范围的资源）  根配置命名空间（通常是 istio-system）中的 Telemetry API 资源提供网格范围内的默认行为。根配置命名空间中的任何特定工作负载选择器将被忽略 / 拒绝。在根配置命名空间中定义多个网格范围的 Telemetry API 资源是无效的。\n要想在某个特定的命名空间内覆盖网格范围的配置，可以通过在该命名空间中应用新的 Telemetry 资源（无需工作负载选择器）来实现。命名空间配置中指定的任何字段将完全覆盖父配置（根配置命名空间）中的字段。\n要想覆盖特定工作负载的遥测配置，可以在其命 …","relpermalink":"/istio-handbook/observability/telemetry-api/","summary":"Istio 1.11 引入了 Telemetry API，并在 1.13 中进行了完善。使用该 API，你可以一站式的灵活地配置指标、访问日志和追踪。 使用 API 下面将向你介绍如何使用 Telemetry API。 范围、继承和重写 Telemetry API 资源从 Istio 配置中继承的顺序： 根配置命名空间（","title":"Telemetry API"},{"content":"注意\n Istio 官方已不再推荐使用 Operator 来安装 Istio，建议你使用 istioctl 命令或者 Helm 来安装。  要安装 Istio，我们需要一个运行中的 Kubernetes 集群实例。所有的云供应商都有一个管理的 Kubernetes 集群提供，我们可以用它来安装 Istio 服务网格。\n我们也可以在你的电脑上使用以下平台之一在本地运行一个 Kubernetes集群。\n要安装 Istio，我们需要一个运行中的 Kubernetes 集群实例。所有的云供应商都有一个管理的 Kubernetes 集群提供，我们可以用它来安装 Istio 服务网格。\n我们也可以在你的电脑上使用以下平台之一在本地运行一个 Kubernetes 集群。\n Minikube Docker Desktop kind MicroK8s  当使用本地 Kubernetes 集群时，确保你的计算机满足 Istio 安装的最低要求（如 16384MB 内存和 4 个 CPU）。另外，确保 Kubernetes 集群的版本是 v1.19.0 或更高。\nMinikube 在这次培训中， …","relpermalink":"/istio-handbook/setup/istio-installation/","summary":"注意 Istio 官方已不再推荐使用 Operator 来安装 Istio，建议你使用 istioctl 命令或者 Helm 来安装。 要安装 Istio，我们需要一个运行中的 Kubernetes 集群实例。所有的云供应商都有一个管理的 Kubernetes 集群提供，我们可以用它来安装 Istio 服务网格。 我们也","title":"安装 Istio"},{"content":"我们将使用谷歌云平台来托管 Kubernetes 集群。打开并登录到你的 GCP 控制台账户，并按照以下步骤创建一个 Kubernetes 集群。\n 从导航中，选择 Kubernetes Engine。 点击创建集群。 将集群命名为 boutique-demo。 选择区域选项，选择最接近你的位置的区域。 点击 default-pool，在节点数中输入 5。 点击 Nodes（节点）。 点击机器配置机器类型下拉，选择e2-medium (2 vCPU, 4 GB memory)。 点击 “Create\u0026#34;来创建集群。  集群的创建将需要几分钟的时间。一旦安装完成，集群将显示在列表中，如下图所示。\n   部署在 GCP 中的 Kubernetes 集群   你也可以将 Online Boutique 应用程序部署到托管在其他云平台上的 Kubernetes 集群，如 Azure 或 AWS。\n 访问集群 我们有两种方式来访问集群。我们可以从浏览器中使用 Cloud Shell。要做到这一点，点击集群旁边的 Connect，然后点击 Run in Cloud Shell 按钮。点击该按钮可以打 …","relpermalink":"/istio-handbook/practice/create-cluster/","summary":"我们将使用谷歌云平台来托管 Kubernetes 集群。打开并登录到你的 GCP 控制台账户，并按照以下步骤创建一个 Kubernetes 集群。 从导航中，选择 Kubernetes Engine。 点击创建集群。 将集群命名为 boutique-demo。 选择区域选项，选择最接","title":"创建集群"},{"content":"我们讨论过只能使用云原生应用程序来创建基础架构。同时基础架构也负责运行这些应用程序。\n运行由应用程序配置和控制的基础架构可以轻松得扩展。我们通过学习如何通过扩展应用的方式来扩展基础架构。我们还通过学习如何保护应用程序来保护基础架构。\n在动态环境中，无法通过增加人手来管理这样的复杂性，同样也不能靠增加人手来处理策略和安全问题。\n这意味着，就像我们必须创建通过协调器模式强制执行基础架构状态的应用程序一样，我们需要创建实施安全策略的应用程序。在创建应用程序以执行的策略之前，我们需要以机器可解析的格式编写策略。\n策略即代码 由于策略没有明确定义的技术实现，所以策略难以纳入代码。它更多地关注业务如何实现而不是谁来实现。\n如何实现和谁来实现都会经常变化，但是实现方式变化更频繁且不容易被抽象化。它也是组织特定的，可能需要了解创建基础架构人员的沟通结构的具体细节。\n策略需要应用于应用程序生命周期的多个阶段。正如我们在第 7 章中所讨论的，应用程序通常有三个阶段：部署、运行和退役。\n部署阶段将在应用程序和基础架构变更发布之前先应用策略。这将包括部署规则和一致性测试。运行阶段将包括持续的遵守和执行访问控制 …","relpermalink":"/cloud-native-infra/securing-applications/","summary":"我们讨论过只能使用云原生应用程序来创建基础架构。同时基础架构也负责运行这些应用程序。 运行由应用程序配置和控制的基础架构可以轻松得扩展。我们通过学习如何通过扩展应用的方式来扩展基础架构。我们还通过学习如","title":"第 8 章：保护应用程序"},{"content":"Envoy 的强大功能之一是支持动态配置。到现在为止，我们一直在使用静态配置。我们使用 static_resources 字段将监听器、集群、路由和其他资源指定为静态资源。\n当使用动态配置时，我们不需要重新启动 Envoy 进程就可以生效。相反，Envoy 通过从磁盘或网络上的文件读取配置，动态地重新加载配置。动态配置使用所谓的发现服务 API，指向配置的特定部分。这些 API 也被统称为 xDS。当使用 xDS 时，Envoy 调用外部基于 gRPC/REST 的配置供应商，这些供应商实现了发现服务 API 来检索配置。\n外部基于 gRPC/REST 的配置提供者也被称为控制平面。当使用磁盘上的文件时，我们不需要控制平面。Envoy 提供了控制平面的 Golang 实现，但是 Java 和其他控制平面的实现也可以使用。\nEnvoy 内部有多个发现服务 API。所有这些在下表中都有描述。\n   发现服务名称 描述     监听器发现服务（LDS） 使用 LDS，Envoy 可以在运行时发现监听器，包括所有的过滤器栈、HTTP 过滤器和对 RDS 的引用。   扩展配置发现服 …","relpermalink":"/envoy-handbook/dynamic-config/dynamic-configuration/","summary":"Envoy 的强大功能之一是支持动态配置。到现在为止，我们一直在使用静态配置。我们使用 static_resources 字段将监听器、集群、路由和其他资源指定为静态资源。 当使用动态配置时，我们不需要重新启动 Envoy 进程就可以生效。相反，Envoy 通","title":"动态配置简介"},{"content":"多集群部署（两个或更多的集群）为我们提供了更大程度的隔离和可用性，但我们付出的代价是增加了复杂性。如果场景要求高可用性（HA），我们将不得不在多个区域和地区部署集群。\n我们需要做出的下一个决定是，决定我们是否要在一个网络内运行集群，或者是否要使用多个网络。\n下图显示了一个多集群方案（集群 A、B 和 C），跨两个网络部署。\n   多集群服务网格  网络部署模式 当涉及到多个网络时，集群内部运行的工作负载必须使用 Istio 网关才能到达其他集群的工作负载。使用多个网络可以实现更好的容错和网络地址的扩展。\n   多网络服务网格  控制平面部署模型 Istio 服务网格使用控制平面来配置网格内工作负载之间的所有通信。工作负载所连接的控制平面取决于其配置。\n在最简单的情况下，我们有一个服务网格，在一个集群中只有一个控制平面。这就是我们在本课程中一直使用的配置。\n共享控制平面模型涉及多个集群，控制平面只在一个集群中运行。该集群被称为主集群，而部署中的其他集群被称为远程集群。这些集群没有自己的控制平面，相反，它们从主集群共享控制平面。\n   共享的控制平面  另一种部署模式是，我们把所有的集群都 …","relpermalink":"/istio-handbook/advanced/multicluster-deployment/","summary":"多集群部署（两个或更多的集群）为我们提供了更大程度的隔离和可用性，但我们付出的代价是增加了复杂性。如果场景要求高可用性（HA），我们将不得不在多个区域和地区部署集群。 我们需要做出的下一个决定是，决定我","title":"多集群部署"},{"content":"本节将为你讲解 Envoy 中的访问日志配置。\n什么是访问日志？ 每当你打开浏览器访问谷歌或其他网站时，另一边的服务器就会收集你的访问信息。具体来说，它在收集和储存你从服务器上请求的网页数据。在大多数情况下，这些数据包括来源（即主机信息）、请求网页的日期和时间、请求属性（方法、路径、Header、正文等）、服务器返回的状态、请求的大小等等。所有这些数据通常被存储在称为访问日志（access log） 的文本文件中。\n通常，来自网络服务器或代理的访问日志条目遵循标准化的通用日志格式。不同的代理和服务器可以使用自己的默认访问日志格式。Envoy 有其默认的日志格式。我们可以自定义默认格式，并配置它，使其以与其他服务器（如 Apache 或 NGINX）有相同的格式写出日志。有了相同的访问日志格式，我们就可以把不同的服务器放在一起使用，用一个工具把数据记录和分析结合起来。\n本模块将解释访问日志在 Envoy 中是如何工作的，以及如何配置和定制。\n捕获和读取访问日志 我们可以配置捕获任何向 Envoy 代理发出的访问请求，并将其写入所谓的访问日志。让我们看看几个访问日志条目的例子。 …","relpermalink":"/envoy-handbook/logging/access-logging/","summary":"本节将为你讲解 Envoy 中的访问日志配置。 什么是访问日志？ 每当你打开浏览器访问谷歌或其他网站时，另一边的服务器就会收集你的访问信息。具体来说，它在收集和储存你从服务器上请求的网页数据。在大多数情况下，这些数据","title":"访问日志"},{"content":"集群可以在配置文件中静态配置，也可以通过集群发现服务（CDS）API 动态配置。每个集群都是一个端点的集合，Envoy 需要解析这些端点来发送流量。\n解析端点的过程被称为服务发现。\n什么是端点？ 集群是一个识别特定主机的端点的集合。每个端点都有以下属性。\n地址 (address)\n该地址代表上游主机地址。地址的形式取决于集群的类型。对于 STATIC 或 EDS 集群类型，地址应该是一个 IP，而对于 LOGICAL 或 STRICT DNS 集群类型，地址应该是一个通过 DNS 解析的主机名。\n主机名 (hostname)\n一个与端点相关的主机名。注意，主机名不用于路由或解析地址。它与端点相关联，可用于任何需要主机名的功能，如自动主机重写。\n健康检查配置（health_check_config）\n可选的健康检查配置用于健康检查器联系健康检查主机。该配置包含主机名和可以联系到主机以执行健康检查的端口。注意，这个配置只适用于启用了主动健康检查的上游集群。\n服务发现类型 有五种支持的服务发现类型，我们将更详细地介绍。\n静态 (STATIC) 静态服务发现类型是最简单的。在配置中，我们为集群 …","relpermalink":"/envoy-handbook/cluster/service-discovery/","summary":"集群可以在配置文件中静态配置，也可以通过集群发现服务（CDS）API 动态配置。每个集群都是一个端点的集合，Envoy 需要解析这些端点来发送流量。 解析端点的过程被称为服务发现。 什么是端点？ 集群是一个识别","title":"服务发现"},{"content":"这一节中，我将为你介绍服务网格的架构，首先我们先来了解下服务网格常用的四种部署模式，然后是其最基本的组成部分——控制平面和数据平面。\n服务网格的部署模式 这四种模式分别是Sidecar 代理、节点共享代理、Service Account/节点共享代理、带有微代理的共享远程代理。\n   服务网格的部署模式  这几种架构模式的主要区别是代理所在的位置不同，模式一在每个应用程序旁运行一个代理，模式二在每个节点上运行一个代理，模式三在每个节点中，再根据 ServiceAccount 的数量，每个 ServiceAccount 运行一个代理，模式四在每个应用旁运行一个微代理，这个代理仅处理 mTLS ，而七层代理位于远程，这几种方式在安全性、隔离及运维开销上的对比如这个表格所示。\n   模式 内存开销 安全性 故障域 运维     Sidecar 代理 因为为每个 pod 都注入一个代理，所以开销最大。 由于 sidecar 必须与工作负载一起部署，工作负载有可能绕过 sidecar。 Pod 级别隔离，如果有代理出现故障，只影响到 Pod 中的工作负载。 …","relpermalink":"/istio-handbook/concepts/service-mesh-architectures/","summary":"这一节中，我将为你介绍服务网格的架构，首先我们先来了解下服务网格常用的四种部署模式，然后是其最基本的组成部分——控制平面和数据平面。 服务网格的部署模式 这四种模式分别是Sidecar 代理、节点共享代理、","title":"服务网格架构"},{"content":"正如在介绍章节中提到的，监听器子系统处理下游或传入的请求处理。监听器子系统负责传入的请求和对客户端的响应路径。除了定义 Envoy 对传入请求进行 “监听” 的地址和端口外，我们还可以选择对每个监听器进行监听过滤器的配置。\n不要把监听器过滤器和我们前面讨论的网络过滤器链和 L3/L4 过滤器混淆起来。Envoy 在处理网络级过滤器之前先处理监听器过滤器，如下图所示。\n   监听器过滤器  请注意，在没有任何监听器过滤器的情况下操作 Envoy 也是常见的。\nEnvoy 会在网络级过滤器之前处理监听器过滤器。我们可以在监听器过滤器中操作连接元数据，通常是为了影响后来的过滤器或集群如何处理连接。\n监听器过滤器对新接受的套接字进行操作，并可以停止或随后继续执行进一步的过滤器。监听器过滤器的顺序很重要，因为 Envoy 在监听器接受套接字后，在创建连接前，会按顺序处理这些过滤器。\n我们可以使用监听器过滤器的结果来进行过滤器匹配，并选择一个合适的网络过滤器链。例如，我们可以使用 HTTP 检查器监听器过滤器来确定 HTTP 协议（HTTP/1.1 或 HTTP/2）。基于这个结果，我们就可以选择 …","relpermalink":"/envoy-handbook/listener/listener-filters/","summary":"正如在介绍章节中提到的，监听器子系统处理下游或传入的请求处理。监听器子系统负责传入的请求和对客户端的响应路径。除了定义 Envoy 对传入请求进行 “监听” 的地址和端口外，我们还可以选择对每","title":"监听器过滤器"},{"content":"扩展 Envoy 的一种方式是实现不同的过滤器，处理或增强请求。这些过滤器可以生成统计数据，翻译协议，修改请求，等等。\nHTTP 过滤器就是一个例子，比如外部的 authz 过滤器和其他内置于 Envoy 二进制的过滤器。\n此外，我们还可以编写我们的过滤器，让 Envoy 动态地加载和运行。我们可以通过正确的顺序声明来决定我们要在过滤器链中的哪个位置运行过滤器。\n我们有几个选择来扩展 Envoy。默认情况下，Envoy 过滤器是用 C++ 编写的。但是，我们可以用 Lua 脚本编写，或者使用 WebAssembly（WASM）来开发其他编程语言的 Envoy 过滤器。\n请注意，与 C++ 过滤器相比，Lua 和 Wasm 过滤器的 API 是有限的。\n1. 原生 C++ API\n第一个选择是编写原生 C++ 过滤器，然后将其与 Envoy 打包。这就需要我们重新编译 Envoy，并维护我们的版本。如果我们试图解决复杂或高性能的用例，采取这种方式是有意义的。\n2. Lua 过滤器\n第二个选择是使用 Lua 脚本。在 Envoy 中有一个 HTTP 过滤器，允许我们定义一个 Lua 脚本， …","relpermalink":"/envoy-handbook/extending-envoy/overview/","summary":"扩展 Envoy 的一种方式是实现不同的过滤器，处理或增强请求。这些过滤器可以生成统计数据，翻译协议，修改请求，等等。 HTTP 过滤器就是一个例子，比如外部的 authz 过滤器和其他内置于 Envoy 二进制的过滤器。 此外，我们还可以编写我们","title":"可扩展性概述"},{"content":"下面将带您了解 Istio 流量管理相关的基础概念与配置示例。\n VirtualService：在 Istio 服务网格中定义路由规则，控制流量路由到服务上的各种行为。 DestinationRule：是 VirtualService 路由生效后，配置应用与请求的策略集。 ServiceEntry：通常用于在 Istio 服务网格之外启用的服务请求。 Gateway：为 HTTP/TCP 流量配置负载均衡器，最常见的是在网格边缘的操作，以启用应用程序的入口流量。 EnvoyFilter：描述了针对代理服务的过滤器，用来定制由 Istio Pilot 生成的代理配置。一定要谨慎使用此功能。错误的配置内容一旦完成传播，可能会令整个服务网格陷入瘫痪状态。这一配置是用于对 Istio 网络系统内部实现进行变更的。  注：本文中的示例引用自 Istio 官方 Bookinfo 示例，见：Istio 代码库，且对于配置的讲解都以在 Kubernetes 中部署的服务为准。\nVirtualService VirtualService 故名思义，就是虚拟服务，在 Istio 1.0 …","relpermalink":"/istio-handbook/traffic-management/basic/","summary":"下面将带您了解 Istio 流量管理相关的基础概念与配置示例。 VirtualService：在 Istio 服务网格中定义路由规则，控制流量路由到服务上的各种行为。 DestinationRule：是 VirtualService 路由生效后，配置应用与请","title":"流量管理基础概念"},{"content":"在整个教程中，我们已经多次提到了管理接口。Envoy 暴露了一个管理接口，允许我们修改 Envoy，并获得一个视图和查询指标和配置。\n管理接口由一个具有多个端点的 REST API 和一个简单的用户界面组成，如下图所示。\n   Envoy 管理接口  管理接口必须使用 admin 字段明确启用。例如：\nadmin:address:socket_address:address:127.0.0.1port_value:9901在启用管理接口时要小心。任何有权限进入管理接口的人都可以进行破坏性的操作，比如关闭服务器（/quitquit 端点）。我们还可能让他们访问私人信息（指标、集群名称、证书信息等）。目前（Envoy 1.20 版本），管理端点是不安全的，也没有办法配置认证或 TLS。有一个工作项目正在进行中，它将限制只有受信任的 IP 和客户端证书才能访问，以确保传输安全。\n在这项工作完成之前，应该只允许通过安全网络访问管理接口，而且只允许从连接到该安全网络的主机访问。我们可以选择只允许通过 localhost 访问管理接口，如上面的配置中所示。另外，如果你决定允许从远程主机访问，那么请 …","relpermalink":"/envoy-handbook/admin-interface/enabling-admin-interface/","summary":"在整个教程中，我们已经多次提到了管理接口。Envoy 暴露了一个管理接口，允许我们修改 Envoy，并获得一个视图和查询指标和配置。 管理接口由一个具有多个端点的 REST API 和一个简单的用户界面组成，如下图所示。 Envoy","title":"启用管理接口"},{"content":"为了解释什么是认证或 authn，我们将从访问控制试图回答的问题开始：一个主体能否对一个对象执行操作？\n如果我们把上述问题翻译成 Istio 和 Kubernetes 的世界，它将是 “服务 X 能否对服务 Y 进行操作？”\n这个问题的三个关键部分是：主体、动作和对象。\n主体和对象都是 Kubernetes 中的服务。动作，假设我们谈论的是 HTTP，是一个 GET 请求，一个 POST，一个 PUT 等等。\n认证是关于主体（或者在我们的例子中是服务的身份）的。认证是验证某种凭证的行为，并确保该凭证是有效和可信的。一旦进行了认证，我们就有了一个经过认证的主体。下次你旅行时，你向海关官员出示你的护照或身份证，他们会对其进行认证，确保你的凭证（护照或身份证）是有效和可信的。\n在 Kubernetes 中，每个工作负载都被分配了一个独特的身份，它用来与其他每个工作负载进行通信 - 该身份以服务账户的形式提供给工作负载。服务账户是运行时中存在的身份 Pods。\nIstio 使用来自服务账户的 X.509 证书，它根据名为 SPIFFE（每个人的安全生产身份框架）的规范创建一个新的身份。\n证书中 …","relpermalink":"/istio-handbook/security/authn/","summary":"为了解释什么是认证或 authn，我们将从访问控制试图回答的问题开始：一个主体能否对一个对象执行操作？ 如果我们把上述问题翻译成 Istio 和 Kubernetes 的世界，它将是 “服务 X 能否对服务 Y 进行操作？”","title":"认证"},{"content":"日志记录了集群中的活动。审计日志是必要的，这不仅是为了确保服务按预期运行和配置，也是为了确保系统的安全。系统性的审计要求对安全设置进行一致和彻底的检查，以帮助识别潜在威胁。Kubernetes 能够捕获集群操作的审计日志，并监控基本的 CPU 和内存使用信息；然而，它并没有提供深入的监控或警报服务。\n关键点\n 在创建时建立 Pod 基线，以便能够识别异常活动。 在主机层面、应用层面和云端（如果适用）进行日志记录。 整合现有的网络安全工具，进行综合扫描、监控、警报和分析。 设置本地日志存储，以防止在通信失败的情况下丢失。  日志 在 Kubernetes 中运行应用程序的系统管理员应该为其环境建立一个有效的日志、监控和警报系统。仅仅记录 Kubernetes 事件还不足以了解系统上发生的行动的全貌。还应在主机级、应用级和云上（如果适用）进行日志记录。而且，这些日志可以与任何外部认证和系统日志相关联，以提供整个环境所采取的行动的完整视图，供安全审计员和事件响应者使用。\n在 Kubernetes 环境中，管理员应监控 / 记录以下内容：\n API 请求历史 性能指标 部署情况 资源消耗 操作 …","relpermalink":"/kubernetes-hardening-guidance/logging/","summary":"日志记录了集群中的活动。审计日志是必要的，这不仅是为了确保服务按预期运行和配置，也是为了确保系统的安全。系统性的审计要求对安全设置进行一致和彻底的检查，以帮助识别潜在威胁。Kubernetes 能够捕获","title":"日志审计"},{"content":"我们可以在虚拟主机和路由层面定义重试策略。在虚拟主机级别设置的重试策略将适用于该虚拟主机的所有路由。如果在路由级别上定义了重试策略，它将优先于虚拟主机策略，并被单独处理——即路由级别的重试策略不会继承虚拟主机级别的重试策略的值。即使 Envoy 将重试策略独立处理，配置也是一样的。\n除了在配置中设置重试策略外，我们还可以通过请求头（即 x-envoy-retry-on 头）进行配置。\n在 Envoy 配置中，我们可以配置以下内容。\n 最大重试次数：Envoy 将重试请求，重试次数最多为配置的最大值。指数退避算法是用于确定重试间隔的默认算法。另一种确定重试间隔的方法是通过 Header（例如 x-envoy-upstream-rq-per-try-timeout-ms）。所有重试也包含在整个请求超时中，即 request_timeout 配置设置。默认情况下，Envoy 将重试次数设置为一次。 重试条件：我们可以根据不同的条件重试请求。例如，我们只能重试 5xx 响应代码，网关失败，4xx 响应代码，等等。 重试预算：重试预算规定了与活动请求数有关的并发请求的限制。这可以帮助防止过大的重 …","relpermalink":"/envoy-handbook/hcm/retries/","summary":"我们可以在虚拟主机和路由层面定义重试策略。在虚拟主机级别设置的重试策略将适用于该虚拟主机的所有路由。如果在路由级别上定义了重试策略，它将优先于虚拟主机策略，并被单独处理——即路由级别的重试策略不会继承","title":"重试"},{"content":"OpenTelemetry 是一个大型项目。OpenTelemetry 项目的工作被划分为特殊兴趣小组（SIG）。虽然所有的项目决策最终都是通过 GitHub issue 和 pull request 做出的，但 SIG 成员经常通过 CNCF 的官方 Slack 保持联系，而且大多数 SIG 每周都会在 Zoom 上会面一次。\n任何人都可以加入一个 SIG。要想了解更多关于当前 SIG、项目成员和项目章程的细节，请查看 GitHub 上的 OpenTelemetry 社区档案库。\n规范 OpenTelemetry 是一个规范驱动的项目。OpenTelemetry 技术委员会负责维护该规范，并通过管理规范的 backlog 来指导项目的发展。\n小的改动可以以 GitHub issue 的方式提出，随后的 pull request 直接提交给规范。但是，对规范的重大修改是通过名为 OpenTelemetry Enhancement Proposals（OTEPs）的征求意见程序进行的。\n任何人都可以提交 OTEP。OTEP 由技术委员会指定的具体审批人进行审查，这些审批人根据其专业领域进 …","relpermalink":"/opentelemetry-obervability/organization/","summary":"附录 A：OpenTelemetry 项目组织","title":"附录 A：OpenTelemetry 项目组织"},{"content":"该节将带领大家了解 Kubernetes 中的基本概念，尤其是作为 Kubernetes 中调度的最基本单位 Pod。\n本节中包括以下内容：\n 了解 Pod 的构成 Pod 的生命周期 Pod 中容器的启动顺序模板定义  Kubernetes 中的基本组件 kube-controller-manager 就是用来控制 Pod 的状态和生命周期的，在了解各种 controller 之前我们有必要先了解下 Pod 本身和其生命周期。\n","relpermalink":"/kubernetes-handbook/architecture/pod-state-and-lifecycle/","summary":"该节将带领大家了解 Kubernetes 中的基本概念，尤其是作为 Kubernetes 中调度的最基本单位 Pod。 本节中包括以下内容： 了解 Pod 的构成 Pod 的生命周期 Pod 中容器的启动顺序模板定义 Kubernetes 中的基本组件 kube-controller-manager 就是用来控制 Pod 的状态和生命周期的，在了解各","title":"Pod 状态与生命周期管理"},{"content":"如果您认为云原生基础架构是可购买的产品或是从云供应商那购买的服务器，我们很抱歉让您失望了。如果不采用这些做法并改变您建设和维护基础架构的方式，您就不会受益。\n它不仅仅影响服务器、网络和存储。它关乎的是工程师如何管理应用程序，就像接受故障一样。\n围绕云原生实践建立的文化与传统技术和工程组织有很大不同。我们并不是解决组织文化或结构问题的专家，但如果您希望改变组织结构，我们建议您从高绩效组织中实施 DevOps 实践的角度来看待价值观和经验教训。\n一些需要探索的地方是 Netflix 的文化套餐，它促进了自由和责任感，还有亚马逊的双比萨团队，这些团队以低开销推广自治团体。云原生应用程序需要与构建它们的团队具有相同的解耦特征。康威定律很好地描述了这一点：“设计系统的架构受制于产生这些设计的组织的沟通结构。”\n在我们结束本书时，我们希望关注哪些领域是您采用云原生实践时最重要的。我们还将讨论一些预测变化的基础架构模式，以便您知道将来要寻找什么。\n关注改变的地方 如果您拥有现有的基础架构或传统数据中心，则过渡到云原生不会在一夜之间发生。如果您有足够大的基础架构或两个以上的人员管理它，试图强制实施基础 …","relpermalink":"/cloud-native-infra/implementing-cloud-native-infrasctructure/","summary":"如果您认为云原生基础架构是可购买的产品或是从云供应商那购买的服务器，我们很抱歉让您失望了。如果不采用这些做法并改变您建设和维护基础架构的方式，您就不会受益。 它不仅仅影响服务器、网络和存储。它关乎的是工","title":"第 9 章：实施云原生基础架构"},{"content":"使用路由级别的请求镜像策略（request_mirroring_policies），我们可以配置 Envoy 将流量从一个集群镜像到另一个集群。\n流量镜像或请求镜像是指当传入的请求以一个集群为目标时，将其复制并发送给第二个集群。镜像的请求是 “发射并遗忘” 的，这意味着 Envoy 在发送主集群的响应之前不会等待影子集群的响应。\n请求镜像模式不会影响发送到主集群的流量，而且因为 Envoy 会收集影子集群的所有统计数据，所以这是一种有用的测试技术。\n除了 “发送并遗忘” 之外，还要确保你所镜像的请求是空闲的。否则，镜像请求会扰乱你的服务与之对话的后端。\n影子请求中的 authority/host 头信息将被添加 -shadow字符串。\n为了配置镜像策略，我们在要镜像流量的路由上使用 request_mirror_policies 字段。我们可以指定一个或多个镜像策略，以及我们想要镜像的流量的部分。\nroute_config:name:my_routevirtual_hosts:- name:httpbindomains:[\u0026#34;*\u0026#34;]routes:- …","relpermalink":"/envoy-handbook/hcm/request-mirroring/","summary":"使用路由级别的请求镜像策略（request_mirroring_policies），我们可以配置 Envoy 将流量从一个集群镜像到另一个集群。 流量镜像或请求镜像是指当传入的请求以一个集群为目标时，将其复制并发送","title":"请求镜像"},{"content":"遵循本文件中概述的加固指南是确保在 Kubernetes 协调容器上运行的应用程序安全的一个步骤。然而，安全是一个持续的过程，跟上补丁、更新和升级是至关重要的。具体的软件组件因个人配置的不同而不同，但整个系统的每一块都应尽可能保持安全。这包括更新：Kubernetes、管理程序、虚拟化软件、插件、环境运行的操作系统、服务器上运行的应用程序，以及 Kubernetes 环境中托管的任何其他软件。\n互联网安全中心（CIS）发布了保护软件安全的基准。管理员应遵守 Kubernetes 和任何其他相关系统组件的 CIS 基准。管理员应定期检查，以确保其系统的安全性符合当前安全专家对最佳实践的共识。应定期对各种系统组件进行漏洞扫描和渗透测试，主动寻找不安全的配置和零日漏洞。任何发现都应在潜在的网络行为者发现和利用它们之前及时补救。\n随着更新的部署，管理员也应该跟上从环境中删除任何不再需要的旧组件。使用托管的 Kubernetes 服务可以帮助自动升级和修补 Kubernetes、操作系统和网络协议。然而，管理员仍然必须为他们的容器化应用程序打补丁和升级。\n","relpermalink":"/kubernetes-hardening-guidance/upgrading-and-application-security-practices/","summary":"遵循本文件中概述的加固指南是确保在 Kubernetes 协调容器上运行的应用程序安全的一个步骤。然而，安全是一个持续的过程，跟上补丁、更新和升级是至关重要的。具体的软件组件因个人配置的不同而不同，但整个系统的每一块都应尽","title":"升级和应用安全实践"},{"content":"路线图很快就会过时。有关最新的路线图，请参见 OpenTelemetry 状态页面。也就是说，以下是截至目前 OpenTelemetry 的状态。\n核心组件 目前，OpenTelemetry 追踪信号已被宣布为稳定的，并且在许多语言中都有稳定的实现。\n度量信号刚刚被宣布稳定，测试版的实施将在 2022 年第一季度广泛使用。\n日志信号预计将在 2022 年第一季度宣布稳定，测试版实现预计将在 2022 年第二季度广泛使用。2021 年，Stanza 项目被捐赠给 OpenTelemetry，为 OpenTelemetry 收集器增加了高效的日志处理能力。\n用于 HTTP/RPC、数据库和消息系统的语义公约预计将在 2022 年第一季度宣布稳定。\n未来 完成上述路线图就完成了对 OpenTelemetry 核心功能的稳定性要求。这是一个巨大的里程碑，因为它打开了进一步采用 OpenTelemetry 的大门，包括与数据库、管理服务和 OSS 库的原生集成。这些集成工作大部分已经以原型和测试版支持的形式在进行。随着 OpenTelemetry 的稳定性在 2022 年上半年完成， …","relpermalink":"/opentelemetry-obervability/roadmap/","summary":"附录 B：OpenTelemetry 项目路线图","title":"附录 B：OpenTelemetry 项目路线图"},{"content":"下面的例子是一个 Dockerfile，它以非 root 用户和非 group 成员身份运行一个应用程序。\nFROMubuntu:latest# 升级和安装 make 工具RUN apt update \u0026amp;\u0026amp; apt install -y make# 从一个名为 code 的文件夹中复制源代码，并使用 make 工具构建应用程序。COPY ./codeRUN make /code# 创建一个新的用户（user1）和新的组（group1）；然后切换到该用户的上下文中。RUN useradd user1 \u0026amp;\u0026amp; groupadd group1USERuser1:group1# 设置容器的默认入口CMD /code/app","relpermalink":"/kubernetes-hardening-guidance/appendix/a/","summary":"下面的例子是一个 Dockerfile，它以非 root 用户和非 group 成员身份运行一个应用程序。 FROMubuntu:latest# 升级和安装 make 工具RUN apt update \u0026\u0026 apt install -y make# 从一个名为 code 的文件夹中复制源代码，并使用 make 工具构建应用程序。COPY ./codeRUN make /code# 创建一","title":"附录 A：非 root 应用的 Dockerfile 示例"},{"content":"在云环境中运行时，应用程序需要具有弹性。网络通信方面特别容易出现故障。添加网络弹性的一种常见模式是创建一个导入到应用程序中的库，该库提供本附录中描述的网络弹性模式。但是，导入的库很难维护以多种语言编写的服务，且当新版本的网络库发布时，会增加应用程序测试和重新部署的负担。\n取代应用程序处理网络弹性逻辑的另一种方式是，可以将代理置于适当的位置，作为应用程序的保护和增强层。代理的优势在于避免应用程序需要额外的复杂代码，尽量减少开发人员的工作量。\n可以在连接层（物理或 SDN），应用程序或透明代理中处理网络弹性逻辑。虽然代理不是传统网络堆栈的一部分，但它们可用于透明地管理应用程序的网络弹性。\n透明代理可以在基础架构中的任何位置运行，但与应用程序的距离越近越有利。代理支持的协议还要尽可能全面，且可以代理的开放系统互连模型（OSI 模型）层。\n通过实施以下模式，代理在基础架构的弹性中扮演着积极的角色：\n 负载均衡 负载切分（Load shedding） 服务发现 重试和 deadline 断路  代理也可以用来为应用程序添加功能。包括：\n 安全和认证 路由（入口和出口） 洞察和监测  负载均衡 应 …","relpermalink":"/cloud-native-infra/appendix-a-patterns-for-network-resiilency/","summary":"在云环境中运行时，应用程序需要具有弹性。网络通信方面特别容易出现故障。添加网络弹性的一种常见模式是创建一个导入到应用程序中的库，该库提供本附录中描述的网络弹性模式。但是，导入的库很难维护以多种语言编写","title":"附录 A：网络弹性模式"},{"content":"速率限制是一种限制传入请求的策略。它规定了一个主机或客户端在一个特定的时间范围内发送多少次请求。一旦达到限制，例如每秒 100 个请求，我们就说发送请求的客户端受到速率限制。任何速率受限的请求都会被拒绝，并且永远不会到达上游服务。稍后，我们还将讨论可以使用的断路器，以及速率限制如何限制上游的负载并防止级联故障。\nEnvoy 支持全局（分布式）和局部（非分布式）的速率限制。\n全局和局部速率限制的区别在于，我们要用全局速率限制来控制对一组在多个 Envoy 实例之间共享的上游的访问。例如，我们想对一个叫做多个 Envoy 代理的数据库的访问进行速率限制。另一方面，局部速率限制适用于每一个 Envoy 实例。\n   全局和局部速率限制  局部和全局速率限制可以一起使用，Envoy 分两个阶段应用它们。首先，应用局部速率限制，然后是全局速率限制。\n我们将在接下来的章节中深入研究全局和局部速率限制，并解释这两种情况如何工作。\n","relpermalink":"/envoy-handbook/hcm/rate-limiting-intro/","summary":"速率限制是一种限制传入请求的策略。它规定了一个主机或客户端在一个特定的时间范围内发送多少次请求。一旦达到限制，例如每秒 100 个请求，我们就说发送请求的客户端受到速率限制。任何速率受限的请求都会被拒绝，并且","title":"速率限制"},{"content":"无论是使用全局还是局部的速率限制，Envoy 都会发出下表中描述的指标。我们可以在配置过滤器时使用 stat_prefix 字段来设置统计信息的前缀。\n当使用局部速率限制器时，每个度量名称的前缀是 \u0026lt;stat_prefix\u0026gt;.http_local_rate_limit.\u0026lt;metric_name\u0026gt;，当使用全局速率限制器时，前缀是 cluster.\u0026lt;route_target_cluster\u0026gt;.ratelimit.\u0026lt;metric_name\u0026gt;。\n   速率限制器 指标名称 描述     局部 enabled 速率限制器被调用的请求总数   局部 / 全局 ok 来自令牌桶的低于限制的响应总数   局部 rate_limited 没有可用令牌的答复总数（但不一定强制执行）   局部 enforced 有速率限制的请求总数（例如，返回 HTTP 429）。   全局 over_limit 速率限制服务的超限答复总数   全局 error 与速率限制服务联系的错误总数   全局 failure_mode_allowed 属于错误但由于 failure_mode_deny …","relpermalink":"/envoy-handbook/hcm/rate-limiting-stats/","summary":"无论是使用全局还是局部的速率限制，Envoy 都会发出下表中描述的指标。我们可以在配置过滤器时使用 stat_prefix 字段来设置统计信息的前缀。 当使用局部速率限制器时，每个度量名称的前缀是 \u003cstat_prefix\u0026","title":"速率限制的统计"},{"content":"本文将为您讲解 Pod 的基础概念。\n理解 Pod Pod 是 kubernetes 中你可以创建和部署的最小也是最简的单位。Pod 代表着集群中运行的进程。\nPod 中封装着应用的容器（有的情况下是好几个容器），存储、独立的网络 IP，管理容器如何运行的策略选项。Pod 代表着部署的一个单位：kubernetes 中应用的一个实例，可能由一个或者多个容器组合在一起共享资源。\n Docker 是 kubernetes 中最常用的容器运行时，但是 Pod 也支持其他容器运行时。\n 在 Kubernetes 集群中 Pod 有如下两种使用方式：\n 一个 Pod 中运行一个容器。“每个 Pod 中一个容器” 的模式是最常见的用法；在这种使用方式中，你可以把 Pod 想象成是单个容器的封装，kuberentes 管理的是 Pod 而不是直接管理容器。 在一个 Pod 中同时运行多个容器。一个 Pod 中也可以同时封装几个需要紧密耦合互相协作的容器，它们之间共享资源。这些在同一个 Pod 中的容器可以互相协作成为一个 service 单位 —— 一个容器共享文件，另一个 “sidecar” 容器 …","relpermalink":"/kubernetes-handbook/objects/pod-overview/","summary":"本文将为您讲解 Pod 的基础概念。 理解 Pod Pod 是 kubernetes 中你可以创建和部署的最小也是最简的单位。Pod 代表着集群中运行的进程。 Pod 中封装着应用的容器（有的情况下是好几个容器），存储、独立的网络 IP，管理容器如何运行的策","title":"Pod 概览"},{"content":"Pod 是 Kubernetes 中可以创建的最小部署单元，也是 Kubernetes REST API 中的顶级资源类型。\n在 Kuberentes V1 core API 版本中的 Pod 的数据结构如下图所示：\n   Pod Cheatsheet  什么是 Pod？ Pod 就像是豌豆荚一样，它由一个或者多个容器组成（例如 Docker 容器），它们共享容器存储、网络和容器运行配置项。Pod 中的容器总是被同时调度，有共同的运行环境。你可以把单个 Pod 想象成是运行独立应用的 “逻辑主机”—— 其中运行着一个或者多个紧密耦合的应用容器 —— 在有容器之前，这些应用都是运行在几个相同的物理机或者虚拟机上。\n尽管 kubernetes 支持多种容器运行时，但是 Docker 依然是最常用的运行时环境，我们可以使用 Docker 的术语和规则来定义 Pod。\nPod 中共享的环境包括 Linux 的 namespace、cgroup 和其他可能的隔绝环境，这一点跟 Docker 容器一致。在 Pod 的环境中，每个容器中可能还有更小的子隔离环境。\nPod 中的容器共享 IP 地址和端 …","relpermalink":"/kubernetes-handbook/objects/pod/","summary":"Pod 是 Kubernetes 中可以创建的最小部署单元，也是 Kubernetes REST API 中的顶级资源类型。 在 Kuberentes V1 core API 版本中的 Pod 的数据结构如下图所示： Pod Cheatsheet 什么是 Pod？ Pod 就像是豌豆荚一样，它由一个或者多个容器组成（例如 Docker 容器），它们共享容器存储、网","title":"Pod 解析"},{"content":"关于使用云提供商和避免供应商锁定存在很多争议。这种辩论充满了意识心态之争。\n锁定通常是工程师和管理层关心的问题。应该将其与选择编程语言或框架一样作为应用程序的风险来权衡。编程语言和云提供商的选择是锁定的形式，工程师有责任了解风险并评估这些风险是否可以接受。\n当您选择供应商或技术时，请记住以下几点：\n 锁定是不可避免的。 锁定是一种风险，但并不总是很高。 不要外包思维。  锁定是不可避免的 在技术上有两种类型的锁定：\n技术锁定\n整个开发技术栈中底层技术\n供应商锁定\n大多数情况下，作为项目的一部分而使用的服务和软件（供应商锁定还可能包括硬件和操作系统，但我们只关注服务）\n技术锁定 开发人员将选择他们熟悉的技术或为正在开发的应用程序提供最大利益的技术。这些技术可以是供应商提供的技术（例如.NET 和 Oracle 数据库）到开源软件（例如 Python 和 PostgreSQL）。\n在此级别提供的锁定通常要求符合 API 或规范，这将影响应用程序的开发。也可以选择一些替代技术，但是这通常有很高的转换成本，因为技术对应用程序的设计有很大影响。\n供应商锁定 供应商，如云提供商，是另一种不同形式的 …","relpermalink":"/cloud-native-infra/appendix-b-lock-in/","summary":"关于使用云提供商和避免供应商锁定存在很多争议。这种辩论充满了意识心态之争。 锁定通常是工程师和管理层关心的问题。应该将其与选择编程语言或框架一样作为应用程序的风险来权衡。编程语言和云提供商的选择是锁定的","title":"附录 B：锁定"},{"content":"下面是一个使用只读根文件系统的 Kubernetes 部署模板的例子。\napiVersion:apps/v1kind:Deploymentmetadata:labels:app:webname:webspec:selector:matchLabels:app:webtemplate:metadata:labels:app:webname:webspec:containers:- command:[\u0026#34;sleep\u0026#34;]args:[\u0026#34;999\u0026#34;]image:ubuntu:latestname:websecurityContext:readOnlyRootFilesystem:true#使容器的文件系统成为只读volumeMounts:- mountPath:/writeable/location/here#创建一个可写卷name:volNamevolumes:- emptyDir:{}name:volName","relpermalink":"/kubernetes-hardening-guidance/appendix/b/","summary":"下面是一个使用只读根文件系统的 Kubernetes 部署模板的例子。 apiVersion:apps/v1kind:Deploymentmetadata:labels:app:webname:webspec:selector:matchLabels:app:webtemplate:metadata:labels:app:webname:webspec:containers:- command:[\"sleep\"]args:[\"999\"]image:ubuntu:latestname:websec","title":"附录 B：只读文件系统的部署模板示例"},{"content":"局部速率限制过滤器对过滤器链处理的传入连接应用一个令牌桶速率限制。\n令牌桶算法的基础是令牌在桶中的类比。桶里的令牌以一个固定的速度被重新填满。每次收到一个请求或连接时，我们都会检查桶里是否还有令牌。如果有，就从桶中取出一个令牌，然后处理该请求。如果没有剩余的令牌，该请求就会被放弃（即速率限制）。\n   令牌桶算法  局部速率限制可以在监听器层面或虚拟主机或路由层面进行全局配置，就像全局速率限制一样。我们还可以在同一配置中结合全局和局部速率限制。\ntoken_bucket 指定了过滤器处理的请求所使用的配置。它包括桶可以容纳的最大令牌数量（max_tokens），每次填充的令牌数量（tokens_per_fill）以及填充间隔（fill_interval）。\n下面是一个最多可以容纳 5000 个令牌的桶的配置实例。每隔 30 秒，向桶中添加 100 个令牌。桶中的令牌容量永远不会超过 5000。\ntoken_bucket:max_tokens:5000tokens_per_fill:100fill_interval:30s为了控制令牌桶是在所有 worker 之间共享（即每个 Envoy …","relpermalink":"/envoy-handbook/hcm/local-rate-limiting/","summary":"局部速率限制过滤器对过滤器链处理的传入连接应用一个令牌桶速率限制。 令牌桶算法的基础是令牌在桶中的类比。桶里的令牌以一个固定的速度被重新填满。每次收到一个请求或连接时，我们都会检查桶里是否还有令牌。如果","title":"局部速率限制"},{"content":"当许多主机向少数上游服务器发送请求，且平均延迟较低时，全局或分布式速率限制很有用。\n   许多主机上少许上游服务器发送请求  由于服务器不能快速处理这些请求，请求就会变得滞后。在这种情况下，众多的下游主机可以压倒少数的上游主机。全局速率限制器有助于防止级联故障。\n   速率限制  Envoy 与任何实现定义的 RPC/IDL 协议的外部速率限制服务集成。该服务的参考实现使用 Go、gRPC 和 Redis 作为其后端，可参考这里。\nEnvoy 调用外部速率限制服务（例如，在 Redis 中存储统计信息并跟踪请求），以得到该请求是否应该被速率限制的响应。\n使用外部速率限制服务，我们可以将限制应用于一组服务，或者，如果我们谈论的是服务网格，则应用于网格中的所有服务。我们可以控制进入网格的请求数量，作为一个整体。\n为了控制单个服务层面的请求率，我们可以使用局部速率限制器。局部速率限制器允许我们对每个服务有单独的速率限制。局部和全局速率限制通常是一起使用的。\n配置全局速率限制 在配置全局速率限制时，我们必须设置两个部分——客户端（Envoy）和服务器端（速率限制服务）。\n我们可以在 Envoy …","relpermalink":"/envoy-handbook/hcm/global-rate-limiting/","summary":"当许多主机向少数上游服务器发送请求，且平均延迟较低时，全局或分布式速率限制很有用。 许多主机上少许上游服务器发送请求 由于服务器不能快速处理这些请求，请求就会变得滞后。在这种情况下，众多的下游主机可以压倒","title":"全局速率限制"},{"content":"在这个实验中，我们将学习如何配置使用不同的方式来匹配请求。我们将使用 direct_response，我们还不会涉及任何 Envoy 集群。\n路径匹配 以下是我们想放入配置中的规则：\n 所有请求都需要来自 hello.io 域名（即在向代理发出请求时，我们将使用 Host: hello.io 标头） 所有向路径 /api 发出的请求将返回字符串 hello - path 所有向根路径（即 /）发出的请求将返回字符串 hello - prefix 所有以 /hello 开头并在后面加上数字的请求（如 /hello/1，/hello/523）都应返回 hello - regex字符串  让我们看一下配置：\nstatic_resources:listeners:- name:listener_0address:socket_address:address:0.0.0.0port_value:10000filter_chains:- filters:- name:envoy.filters.network.http_connection_managertyped_config:\u0026#34;@type\u0026#34;: …","relpermalink":"/envoy-handbook/hcm/lab1/","summary":"在这个实验中，我们将学习如何配置使用不同的方式来匹配请求。我们将使用 direct_response，我们还不会涉及任何 Envoy 集群。 路径匹配 以下是我们想放入配置中的规则： 所有请求都需要来自 hello.io 域名（即在向代理","title":"实验1：请求匹配"},{"content":"该特性在自 Kubernetes 1.6 版本推出 beta 版本。Init 容器可以在 PodSpec 中同应用程序的 containers 数组一起来指定。此前 beta 注解的值仍将保留，并覆盖 PodSpec 字段值。\n本文讲解 Init 容器的基本概念，这是一种专用的容器，在应用程序容器启动之前运行，用来包含一些应用镜像中不存在的实用工具或安装脚本。\n理解 Init 容器 Pod 能够具有多个容器，应用运行在容器里面，但是它也可能有一个或多个先于应用容器启动的 Init 容器。\nInit 容器与普通的容器非常像，除了如下两点：\n Init 容器总是运行到成功完成为止。 每个 Init 容器都必须在下一个 Init 容器启动之前成功完成。  如果 Pod 的 Init 容器失败，Kubernetes 会不断地重启该 Pod，直到 Init 容器成功为止。然而，如果 Pod 对应的 restartPolicy 为 Never，它不会重新启动。\n指定容器为 Init 容器，在 PodSpec 中添加 initContainers 字段，以 v1.Container …","relpermalink":"/kubernetes-handbook/objects/init-containers/","summary":"该特性在自 Kubernetes 1.6 版本推出 beta 版本。Init 容器可以在 PodSpec 中同应用程序的 containers 数组一起来指定。此前 beta 注解的值仍将保留，并覆盖 PodSpec 字段值。 本文讲解 Init 容器的基本概念，这是一种专用的容器，在应用程序容器启动之前运行，用来","title":"Init 容器"},{"content":" 以下内容最初由 CNCF 发布在 Kubernetes.io 上，并且在此获得许可。\n 在 2014 年夏天，Box 对沉淀了十年的硬件和软件基础架构的痛苦，这无法与公司的需求保持一致。\n该平台为超过 5000 万用户（包括政府和大型企业如通用电气公司）管理和共享云中的内容，Box 最初是一个使用 PHP 写的具有数百万行的庞大代码，内置裸机数据中心。它已经开始将单体应用分解成微服务。 “随着我们扩展到全球各地，公有云战争正在升温，我们开始专注于如何在许多不同的环境和许多不同的云基础架构提供商之间运行我们的工作负载”，Box 联合创始人和服务架构师 Sam Ghods 说。 “迄今为止，这是一个巨大的挑战，因为所有这些不同的提供商，特别是裸机，都有非常不同的接口和与合作方式。”\n当 Ghods 参加 DockerCon 时，Box 的云原生之旅加速了。该公司已经认识到，它不能再仅仅使用裸机来运行应用程序，正在研究 Docker 容器化，使用 OpenStack 进行虚拟化以及支持公有云。\n在那次会议上，Google 宣布发布 Kubernetes 容器管理系统，Ghods 成功了。 …","relpermalink":"/cloud-native-infra/appendix-c-box-case-study/","summary":"以下内容最初由 CNCF 发布在 Kubernetes.io 上，并且在此获得许可。 在 2014 年夏天，Box 对沉淀了十年的硬件和软件基础架构的痛苦，这无法与公司的需求保持一致。 该平台为超过 5000 万用户（包括政府和大型企业如通用电气公司）管理和共享云","title":"附录 C Box：案例研究"},{"content":"下面是一个 Kubernetes Pod 安全策略的例子，它为集群中运行的容器执行了强大的安全要求。这个例子是基于官方的 Kubernetes 文档。我们鼓励管理员对该策略进行修改，以满足他们组织的要求。 …","relpermalink":"/kubernetes-hardening-guidance/appendix/c/","summary":"下面是一个 Kubernetes Pod 安全策略的例子，它为集群中运行的容器执行了强大的安全要求。这个例子是基于官方的 Kubernetes 文档。我们鼓励管理员对该策略进行修改，以满足他们组织的要求。 apiVersion:policy/v1beta1kind:PodSecurityPolicymetadata:name:restrictedannotations:seccomp.security.alpha.kubernetes.io/allowedProfileNames:'docker/default,runtime/default'apparmor.security.beta.kubernetes.io/allowedProfileNames:'runtime/default'seccomp.security.alpha.kubernetes.io/defaultProfileName:'runtime/default'apparmor.security.beta.kubernetes.io/defaultProfileName:'runtime/default'spec:privileged:false# 需要防止升级到 rootallowPrivilegeEscalation:falserequiredDropCapabilities:- ALLvolumes:- 'configMap'- 'emptyDir'- 'projected'- 'secret'- 'downwardAPI'- 'persistentVolumeClaim'# 假设管理员设置","title":"附录 C：Pod 安全策略示例"},{"content":"在这个实验中，我们将学习如何使用运行时分数和加权集群来配置 Envoy 的流量分割。\n使用运行时分数 当我们只有两个上游集群时，运行时分数是一种很好的流量分割方法。运行时分数的工作原理是提供一个运行时分数（例如分子和分母），代表我们想要路由到一个特定集群的流量的分数。然后，我们使用相同的条件（即，在我们的例子中相同的前缀）提供第二个匹配，但不同的上游集群。\n让我们创建一个 Envoy 配置，对 70% 的流量返回状态为 201 的直接响应。其余的流量返回状态为 202。\nstatic_resources:listeners:- name:listener_0address:socket_address:address:0.0.0.0port_value:10000filter_chains:- filters:- name:envoy.filters.network.http_connection_managertyped_config:\u0026#34;@type\u0026#34;: …","relpermalink":"/envoy-handbook/hcm/lab2/","summary":"在这个实验中，我们将学习如何使用运行时分数和加权集群来配置 Envoy 的流量分割。 使用运行时分数 当我们只有两个上游集群时，运行时分数是一种很好的流量分割方法。运行时分数的工作原理是提供一个运行时分数（例如分子和","title":"实验2：流量分割"},{"content":"Pause 容器，又叫 Infra 容器，本文将探究该容器的作用与原理。\n我们知道在 kubelet 的配置中有这样一个参数：\nKUBELET_POD_INFRA_CONTAINER=--pod-infra-container-image=registry.access.redhat.com/rhel7/pod-infrastructure:latest 上面是 openshift 中的配置参数，kubernetes 中默认的配置参数是：\nKUBELET_POD_INFRA_CONTAINER=--pod-infra-container-image=gcr.io/google_containers/pause-amd64:3.0 Pause 容器，是可以自己来定义，官方使用的 gcr.io/google_containers/pause-amd64:3.0 容器的代码见 Github，使用 C 语言编写。\nPause 容器特点  镜像非常小，目前在 700KB 左右 永远处于 Pause (暂停) 状态  Pause 容器背景 像 Pod 这样一个东西，本身是一个逻辑概念。那在机器上， …","relpermalink":"/kubernetes-handbook/objects/pause-container/","summary":"Pause 容器，又叫 Infra 容器，本文将探究该容器的作用与原理。 我们知道在 kubelet 的配置中有这样一个参数： KUBELET_POD_INFRA_CONTAINER=--pod-infra-container-image=registry.access.redhat.com/rhel7/pod-infrastructure:latest 上面是 openshift 中的配置参数，kubernetes 中默认的配置参数是： KUBELET_POD_INFRA_CONTAINER=--pod-infra-container-image=gcr.io/google_containers/pause-amd64:3.0 Pause 容器，是可以自己来定义，官方使用的 gcr.io/google_containers/pause-amd64:3.0 容器的代码","title":"Pause 容器"},{"content":"下面的例子是为每个团队或用户组，可以使用 kubectl 命令或 YAML 文件创建一个 Kubernetes 命名空间。应避免使用任何带有 kube 前缀的名称，因为它可能与 Kubernetes 系统保留的命名空间相冲突。\nKubectl 命令来创建一个命名空间。\nkubectl create namespace \u0026lt;insert-namespace-name-here\u0026gt; 要使用 YAML 文件创建命名空间，创建一个名为 my-namespace.yaml 的新文件，内容如下：\napiVersion:v1kind:Namespacemetadata:name:\u0026lt;insert-namespace-name-here\u0026gt;应用命名空间，使用：\nkubectl create –f ./my-namespace.yaml 要在现有的命名空间创建新的 Pod，请切换到所需的命名空间：\nkubectl config use-context \u0026lt;insert-namespace-here\u0026gt; 应用新的 Deployment，使用：\nkubectl apply -f deployment.yaml 另外，也 …","relpermalink":"/kubernetes-hardening-guidance/appendix/d/","summary":"下面的例子是为每个团队或用户组，可以使用 kubectl 命令或 YAML 文件创建一个 Kubernetes 命名空间。应避免使用任何带有 kube 前缀的名称，因为它可能与 Kubernetes 系统保留的命名空间相冲突。 Kubectl 命令来创建一个命名空间。 kubectl create namespace \u003cinsert-namespace-name-here\u003e 要使用 YAML 文件创建命名","title":"附录 D：命名空间示例"},{"content":"在这个实验中，我们将学习如何在不同的配置级别上操作请求和响应头。\n我们将使用一个单一的例子，它的作用如下：\n 对所有的请求添加一个响应头 lab: 3 在虚拟主机上添加一个请求头 vh: one 为 /json 路由匹配添加一个名为 json 响应头。响应头有来自请求头 hello 的值  我们将只有一个名为 single_cluster 的上游集群，监听端口为 3030。让我们运行监听该端口的 httpbin 容器：\ndocker run -d -p 3030:80 kennethreitz/httpbin 让我们创建遵循上述规则的 Envoy 配置：\nstatic_resources:listeners:- name:listener_0address:socket_address:address:0.0.0.0port_value:10000filter_chains:- filters:- name:envoy.filters.network.http_connection_managertyped_config:\u0026#34;@type\u0026#34;: …","relpermalink":"/envoy-handbook/hcm/lab3/","summary":"在这个实验中，我们将学习如何在不同的配置级别上操作请求和响应头。 我们将使用一个单一的例子，它的作用如下： 对所有的请求添加一个响应头 lab: 3 在虚拟主机上添加一个请求头 vh: one 为 /json 路由匹配添加一个名为 json 响应头。响","title":"实验3：Header操作"},{"content":"PodSecurityPolicy 类型的对象能够控制，是否可以向 Pod 发送请求，该 Pod 能够影响被应用到 Pod 和容器的 SecurityContext。\n什么是 Pod 安全策略？ Pod 安全策略 是集群级别的资源，它能够控制 Pod 运行的行为，以及它具有访问什么的能力。 PodSecurityPolicy对象定义了一组条件，指示 Pod 必须按系统所能接受的顺序运行。 它们允许管理员控制如下方面：\n   控制面 字段名称     已授权容器的运行 privileged   为容器添加默认的一组能力 defaultAddCapabilities   为容器去掉某些能力 requiredDropCapabilities   容器能够请求添加某些能力 allowedCapabilities   控制卷类型的使用 volumes   主机网络的使用 hostNetwork   主机端口的使用 hostPorts   主机 PID namespace 的使用 hostPID   主机 IPC namespace 的使用 hostIPC …","relpermalink":"/kubernetes-handbook/objects/pod-security-policy/","summary":"PodSecurityPolicy 类型的对象能够控制，是否可以向 Pod 发送请求，该 Pod 能够影响被应用到 Pod 和容器的 SecurityContext。 什么是 Pod 安全策略？ Pod 安全策略 是集群级别的资源，它能够控制 Pod 运行的行为，以及它具有访问什么的能力","title":"Pod 安全策略"},{"content":"网络策略根据使用的网络插件而不同。下面是一个网络策略的例子，参考 Kubernetes 文档将 nginx 服务的访问限制在带有标签访问的 Pod 上。\napiVersion:networking.k8s.io/v1kind:NetworkPolicymetadata:name:example-access-nginxnamespace:prod#这可以是任何一个命名空间，或者在不使用命名空间的情况下省略。spec:podSelector:matchLabels:app:nginxingress:- from:- podSelector:matchLabels:access:\u0026#34;true\u0026#34;新的 NetworkPolicy 可以通过以下方式应用：\nkubectl apply -f policy.yaml 一个默认的拒绝所有入口的策略：\napiVersion:networking.k8s.io/v1kind:NetworkPolicymetadata:name:deny-all-ingressspec:podSelector:{}policyType:- Ingress一个默认的拒绝所有出口 …","relpermalink":"/kubernetes-hardening-guidance/appendix/e/","summary":"网络策略根据使用的网络插件而不同。下面是一个网络策略的例子，参考 Kubernetes 文档将 nginx 服务的访问限制在带有标签访问的 Pod 上。 apiVersion:networking.k8s.io/v1kind:NetworkPo","title":"附录 E：网络策略示例"},{"content":"在这个实验中，我们将学习如何配置不同的重试策略。我们将使用 httpbin Docker 镜像，因为我们可以向不同的路径（例如 /status/[statuscode]）发送请求，而 httpbin 会以该状态码进行响应。\n确保你有 httpbin 容器在 3030 端口监听。\ndocker run -d -p 3030:80 kennethreitz/httpbin 让我们创建 Envoy 配置，定义一个关于 5xx 响应的简单重试策略。我们还将启用管理接口，这样我们就可以在指标中看到重试的报告。\nstatic_resources:listeners:- name:listener_0address:socket_address:address:0.0.0.0port_value:10000filter_chains:- filters:- name:envoy.filters.network.http_connection_managertyped_config:\u0026#34;@type\u0026#34;: …","relpermalink":"/envoy-handbook/hcm/lab4/","summary":"在这个实验中，我们将学习如何配置不同的重试策略。我们将使用 httpbin Docker 镜像，因为我们可以向不同的路径（例如 /status/[statuscode]）发送请求，而 httpbin 会以该状态码进行响应。 确保你有 httpbin 容器在 3030 端口监","title":"实验4：重试"},{"content":"本文讲解的是 Kubernetes 中 Pod 的生命周期，包括生命周期的不同阶段、存活和就绪探针、重启策略等。\nPod phase Pod 的 status 字段是一个 PodStatus 对象，PodStatus中有一个 phase 字段。\nPod 的相位（phase）是 Pod 在其生命周期中的简单宏观概述。该字段并不是对容器或 Pod 的综合汇总，也不是为了做为综合状态机。\nPod 相位的数量和含义是严格指定的。除了本文档中列举的状态外，不应该再假定 Pod 有其他的 phase 值。\n下面是 phase 可能的值：\n 挂起（Pending）：Pod 已被 Kubernetes 系统接受，但有一个或者多个容器镜像尚未创建。等待时间包括调度 Pod 的时间和通过网络下载镜像的时间，这可能需要花点时间。 运行中（Running）：该 Pod 已经绑定到了一个节点上，Pod 中所有的容器都已被创建。至少有一个容器正在运行，或者正处于启动或重启状态。 成功（Succeeded）：Pod 中的所有容器都被成功终止，并且不会再重启。 失败（Failed）：Pod 中的所有容器都已终止了，并 …","relpermalink":"/kubernetes-handbook/objects/pod-lifecycle/","summary":"本文讲解的是 Kubernetes 中 Pod 的生命周期，包括生命周期的不同阶段、存活和就绪探针、重启策略等。 Pod phase Pod 的 status 字段是一个 PodStatus 对象，PodStatus中有一个 phase 字段。 Pod 的相位（phase）是 Pod 在其生命周期中的简单宏观概述。","title":"Pod 的生命周期"},{"content":"在 Kubernetes 1.10 和更新版本中，LimitRange 支持被默认启用。下面的 YAML 文件为每个容器指定了一个 LimitRange，其中有一个默认的请求和限制，以及最小和最大的请求。\napiVersion:v1kind:LimitRangemetadata:name:cpu-min-max-demo-lrspec:limits- default:cpu:1defaultRequest:cpu:0.5max:cpu:2min:cpu 0.5type:ContainerLimitRange 可以应用于命名空间，使用：\nkubectl apply -f \u0026lt;example-LimitRange\u0026gt;.yaml --namespace=\u0026lt;Enter-Namespace\u0026gt; 在应用了这个 LimitRange 配置的例子后，如果没有指定，命名空间中创建的所有容器都会被分配到默认的 CPU 请求和限制。命名空间中的所有容器的 CPU 请求必须大于或等于最小值，小于或等于最大 CPU 值，否则容器将不会被实例化。\n","relpermalink":"/kubernetes-hardening-guidance/appendix/f/","summary":"在 Kubernetes 1.10 和更新版本中，LimitRange 支持被默认启用。下面的 YAML 文件为每个容器指定了一个 LimitRange，其中有一个默认的请求和限制，以及最小和最大的请求。 apiVersion:v1kind:LimitRangemetadata:name:cpu-min-max-demo-lrspec:limits- default:cpu:1defaultRequest:cpu:0.5max:cpu:2min:cpu 0.5type:ContainerLimitRange 可以应用于命名空间，使用： kubectl apply -f \u003cexample-LimitRange\u003e.yaml","title":"附录 F：LimitRange 示例"},{"content":"在这个实验中，我们将学习如何配置一个局部速率限制器。我们将使用运行在 3030 端口的 httpbin 容器。\ndocker run -d -p 3030:80 kennethreitz/httpbin 让我们创建一个有五个令牌的速率限制器。每隔 30 秒，速率限制器就会向桶里补充 5 个令牌。\nstatic_resources:listeners:- name:listener_0address:socket_address:address:0.0.0.0port_value:10000filter_chains:- filters:- name:envoy.filters.network.http_connection_managertyped_config:\u0026#34;@type\u0026#34;: …","relpermalink":"/envoy-handbook/hcm/lab5/","summary":"在这个实验中，我们将学习如何配置一个局部速率限制器。我们将使用运行在 3030 端口的 httpbin 容器。 docker run -d -p 3030:80 kennethreitz/httpbin 让我们创建一个有五个令牌的速率限制器。每隔 30 秒，速率限制器就会向桶里补充 5 个令牌。 static_resources:listeners:- name:listener_0address:socket_address:address:0.0.0.0port_value:10000filter_chains:- filters:- name:envoy.filters.network.http_connection_managertyped_config:\"@type\": type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManagerstat_prefix:ingress_httproute_config:name:local_routevirtual_hosts:- name:instance_1domains:[\"*\"]routes:- match:prefix:/statusroute:cluster:instance_1- match:prefix:/headersroute:cluster:instance_1typed_per_filter_config:envoy.filters.http.local_ratelimit:\"@type\": type.googleapis.com/envoy.extensions.filters.http.local_ratelimit.v3.LocalRateLimitstat_prefix:headers_routetoken_bucket:max_tokens:5tokens_per_fill:5fill_interval:30sfilter_enabled:default_value:numerator:100denominator:HUNDREDfilter_enforced:default_value:numerator:100denominator:HUNDREDresponse_headers_to_add:- append:falseheader:key:x-rate-limitedvalue:OH_NOhttp_filters:- name:envoy.filters.http.local_ratelimittyped_config:\"@type\":","title":"实验5：局部速率限制"},{"content":"Pod Hook（钩子）是由 Kubernetes 管理的 kubelet 发起的，当容器中的进程启动前或者容器中的进程终止之前运行，这是包含在容器的生命周期之中。可以同时为 Pod 中的所有容器都配置 hook。\nHook 的类型包括两种：\n exec：执行一段命令 HTTP：发送 HTTP 请求。  参考下面的配置：\napiVersion:v1kind:Podmetadata:name:lifecycle-demospec:containers:- name:lifecycle-demo-containerimage:nginxlifecycle:postStart:exec:command:[\u0026#34;/bin/sh\u0026#34;,\u0026#34;-c\u0026#34;,\u0026#34;echo Hello from the postStart handler\u0026gt; /usr/share/message\u0026#34;]preStop:exec:command:[\u0026#34;/usr/sbin/nginx\u0026#34;,\u0026#34;-s\u0026#34;,\u0026#34;quit\u0026#34;]postStart 在容器创建之后（但并不能保证钩子会在容器 ENTRYPOINT 之前）执行，这时候 Pod 已经被调度到某台 node  …","relpermalink":"/kubernetes-handbook/objects/pod-hook/","summary":"Pod Hook（钩子）是由 Kubernetes 管理的 kubelet 发起的，当容器中的进程启动前或者容器中的进程终止之前运行，这是包含在容器的生命周期之中。可以同时为 Pod 中的所有容器都配置 hook。 Hook 的类型包括两种： exec：执行一段命令","title":"Pod Hook"},{"content":"通过将 YAML 文件应用于命名空间或在 Pod 的配置文件中指定要求来创建 ResourceQuota 对象，以限制命名空间内的总体资源使用。下面的例子是基于 Kubernetes 官方文档的一个命名空间的配置文件示例：\napiVersion:v1kind:ResourceQuotametadata:name:example-cpu-mem-resourcequotaspec:hard:requests.cpu:\u0026#34;1\u0026#34;requests.memory:1Gilimits.cpu:\u0026#34;2\u0026#34;limits.memory:2Gi可以这样应用这个 ResourceQuota：\nkubectl apply -f example-cpu-mem-resourcequota.yaml -- namespace=\u0026lt;insert-namespace-here\u0026gt; 这个 ResourceQuota 对所选择的命名空间施加了以下限制：\n 每个容器都必须有一个内存请求、内存限制、CPU 请求和 CPU 限制。 所有容器的总内存请求不应超过 1 GiB 所有容器的总内存限制不应超过 2 GiB 所有容器的 CPU 请 …","relpermalink":"/kubernetes-hardening-guidance/appendix/g/","summary":"通过将 YAML 文件应用于命名空间或在 Pod 的配置文件中指定要求来创建 ResourceQuota 对象，以限制命名空间内的总体资源使用。下面的例子是基于 Kubernetes 官方文档的一个命名空间的配置文件示例： apiVersion:v1kind:Resou","title":"附录 G：ResourceQuota 示例"},{"content":"在这个实验中，我们将学习如何配置一个全局速率限制器。我们将使用速率限制器服务和一个 Redis 实例来跟踪令牌。我们将使用 Docker Compose 来运行 Redis 和速率限制器服务容器。\n让我们首先创建速率限制器服务的配置。\ndomain:my_domaindescriptors:- key:generic_keyvalue:instance_1descriptors:- key:header_matchvalue:get_requestrate_limit:unit:MINUTErequests_per_unit:5我们指定一个通用键（instance_1）和一个名为 header_match 的描述符，速率限制为 5 个请求 / 分钟。\n将上述文件保存到 /config/rl-config.yaml 文件中。\n现在我们可以运行 Docker Compose 文件，它将启动 Redis 和速率限制器服务。\nversion:\u0026#34;3\u0026#34;services:redis:image:redis:alpineexpose:- 6379ports:- 6379:6379networks:- …","relpermalink":"/envoy-handbook/hcm/lab6/","summary":"在这个实验中，我们将学习如何配置一个全局速率限制器。我们将使用速率限制器服务和一个 Redis 实例来跟踪令牌。我们将使用 Docker Compose 来运行 Redis 和速率限制器服务容器。 让我们首先创建速率限制器服务的配置。 domain:my_domaindescriptors:- key:generic_keyvalue:instance_1descriptors:- key:head","title":"实验6：全局速率限制"},{"content":"Preset 就是预设，有时候想要让一批容器在启动的时候就注入一些信息，比如 secret、volume、volume mount 和环境变量，而又不想一个一个的改这些 Pod 的 template，这时候就可以用到 PodPreset 这个资源对象了。\n本页是关于 PodPreset 的概述，该对象用来在 Pod 创建的时候向 Pod 中注入某些特定信息。该信息可以包括 secret、volume、volume mount 和环境变量。\n理解 Pod Preset Pod Preset 是用来在 Pod 被创建的时候向其中注入额外的运行时需求的 API 资源。\n您可以使用 label selector 来指定为哪些 Pod 应用 Pod Preset。\n使用 Pod Preset 使得 pod 模板的作者可以不必为每个 Pod 明确提供所有信息。这样一来，pod 模板的作者就不需要知道关于该服务的所有细节。\n关于该背景的更多信息，请参阅 PodPreset 的设计方案。\n如何工作 Kubernetes 提供了一个准入控制器（PodPreset），当其启用时，Pod Preset 会将 …","relpermalink":"/kubernetes-handbook/objects/pod-preset/","summary":"Preset 就是预设，有时候想要让一批容器在启动的时候就注入一些信息，比如 secret、volume、volume mount 和环境变量，而又不想一个一个的改这些 Pod 的 template，这时候就可以用到 PodPreset 这个资源对象了。 本","title":"Pod Preset"},{"content":"要对秘密数据进行静态加密，下面的加密配置文件提供了一个例子，以指定所需的加密类型和加密密钥。将加密密钥存储在加密文件中只能稍微提高安全性。Secret 将被加密，但密钥将在 EncryptionConfiguration 文件中被访问。这个例子是基于 Kubernetes 的官方文档。\napiVersion:apiserver.config.k8s.io/v1kind:EncryptionConfigurationresources:- resources:- secretsproviders:- aescbc:keys:- name:key1secret:\u0026lt;base 64 encoded secret\u0026gt;- identity:{}要使用该加密文件进行静态加密，请在重启 API 服务器时设置 --encryption-provider-config 标志，并注明配置文件的位置。\n","relpermalink":"/kubernetes-hardening-guidance/appendix/h/","summary":"要对秘密数据进行静态加密，下面的加密配置文件提供了一个例子，以指定所需的加密类型和加密密钥。将加密密钥存储在加密文件中只能稍微提高安全性。Secret 将被加密，但密钥将在 EncryptionConfiguration 文件中被访问。这个例子是基于","title":"附录 H：加密示例"},{"content":"这篇文档适用于要构建高可用应用程序的所有者，因此他们需要了解 Pod 可能发生什么类型的中断。也适用于要执行自动集群操作的集群管理员，如升级和集群自动扩容。\n自愿中断和非自愿中断 Pod 不会消失，直到有人（人类或控制器）将其销毁，或者当出现不可避免的硬件或系统软件错误。\n我们把这些不可避免的情况称为应用的非自愿性中断。例如：\n 后端节点物理机的硬件故障 集群管理员错误地删除虚拟机（实例） 云提供商或管理程序故障使虚拟机消失 内核恐慌（kernel panic） 节点由于集群网络分区而从集群中消失 由于节点资源不足而将容器逐出  除资源不足的情况外，大多数用户应该都熟悉以下这些情况；它们不是特定于 Kubernetes 的。\n我们称这些情况为”自愿中断“。包括由应用程序所有者发起的操作和由集群管理员发起的操作。典型的应用程序所有者操作包括：\n 删除管理该 pod 的 Deployment 或其他控制器 更新了 Deployment 的 pod 模板导致 pod 重启 直接删除 pod（意外删除）  集群管理员操作包括：\n 排空（drain）节点进行修复或升级。 从集群中排空节点以缩小集 …","relpermalink":"/kubernetes-handbook/objects/pod-disruption-budget/","summary":"这篇文档适用于要构建高可用应用程序的所有者，因此他们需要了解 Pod 可能发生什么类型的中断。也适用于要执行自动集群操作的集群管理员，如升级和集群自动扩容。 自愿中断和非自愿中断 Pod 不会消失，直到有人（人类或控制","title":"Pod 中断与 PDB（Pod 中断预算）"},{"content":"Envoy sidecar 代理配置中包含以下四个部分：\n bootstrap：Envoy proxy 启动时候加载的静态配置。 listeners：监听器配置，使用 LDS 下发。 clusters：集群配置，静态配置中包括 xds-grpc 和 zipkin 地址，动态配置使用 CDS 下发。 routes：路由配置，静态配置中包括了本地监听的服务的集群信息，其中引用了 cluster，动态配置使用 RDS 下发。  每个部分中都包含静态配置与动态配置，其中 bootstrap 配置又是在集群启动的时候通过 sidecar 启动参数注入的，配置文件在 /etc/istio/proxy/envoy-rev0.json。\nEnvoy 的配置 dump 出来后的结构如下图所示。\n   Envoy 配置  由于 bootstrap 中的配置是来自 Envoy 启动时加载的静态文件，主要配置了节点信息、tracing、admin 和统计信息收集等信息，这不是本文的重点，大家可以自行研究。\n   bootstrap 配置  上图是 bootstrap 的配置信息。\nBootstrap …","relpermalink":"/istio-handbook/data-plane/envoy-proxy-config-deep-dive/","summary":"Envoy sidecar 代理配置中包含以下四个部分： bootstrap：Envoy proxy 启动时候加载的静态配置。 listeners：监听器配置，使用 LDS 下发。 clusters：集群配置，静态配置中包括 xds-grpc 和 zipkin 地址，动态配置使用 CDS","title":"Envoy 代理配置详解"},{"content":"让我们以 Web 前端和 customer 服务为例，看看 Envoy 如何确定将请求从 Web 前端发送到 customer 服务（customers.default.svc.cluster.local）的位置。使用 istioctl proxy-config 命令，我们可以列出 web 前端 pod 的所有监听器。\n$ istioctl proxy-config listeners web-frontend-64455cd4c6-p6ft2 ADDRESS PORT MATCH DESTINATION 10.124.0.10 53 ALL Cluster: outbound|53||kube-dns.kube-system.svc.cluster.local 0.0.0.0 80 ALL PassthroughCluster 10.124.0.1 443 ALL Cluster: outbound|443||kubernetes.default.svc.cluster.local 10.124.3.113 443 ALL Cluster: …","relpermalink":"/istio-handbook/troubleshooting/envoy-sample/","summary":"让我们以 Web 前端和 customer 服务为例，看看 Envoy 如何确定将请求从 Web 前端发送到 customer 服务（customers.default.svc.cluster.local）的位置。使用 istioctl proxy-config 命令，我们可以列出 web 前端 pod 的所有监听器。 $","title":"Envoy 示例"},{"content":"作为 Istio 安装的一部分，我们安装了 Istio 的入口和出口网关。这两个网关都运行一个 Envoy 代理实例，它们在网格的边缘作为负载均衡器运行。入口网关接收入站连接，而出口网关接收从集群出去的连接。\n使用入口网关，我们可以对进入集群的流量应用路由规则。我们可以有一个指向入口网关的单一外部 IP 地址，并根据主机头将流量路由到集群内的不同服务。\n   入口和出口网关  我们可以使用 Gateway 资源来配置网关。网关资源描述了负载均衡器的暴露端口、协议、SNI（服务器名称指示）配置等。\n下面是一个网关资源的例子：\napiVersion:networking.istio.io/v1alpha3kind:Gatewaymetadata:name:my-gatewaynamespace:defaultspec:selector:istio:ingressgatewayservers:- port:number:80name:httpprotocol:HTTPhosts:- dev.example.com- test.example.com上述网关资源设置了一个代理，作为一个负载均衡 …","relpermalink":"/istio-handbook/traffic-management/gateway/","summary":"作为 Istio 安装的一部分，我们安装了 Istio 的入口和出口网关。这两个网关都运行一个 Envoy 代理实例，它们在网格的边缘作为负载均衡器运行。入口网关接收入站连接，而出口网关接收从集群出去的连接。 使用入口网关，我们可以对进入","title":"Gateway"},{"content":"Istio 是最受欢迎和发展最快的开源项目之一。它的发布时间表对企业的生命周期和变更管理实践来说可能非常激进。GetMesh 通过针对不同的 Kubernetes 分发版测试所有 Istio 版本以确保功能的完整性来解决这一问题。GetMesh 的 Istio 版本在安全补丁和其他错误更新方面得到积极的支持，并拥有比上游 Istio 提供的更长的支持期。\n一些服务网格客户需要支持更高的安全要求。GetMesh 通过提供两种 Istio 发行版来解决合规性问题。\n tetrate 发行版，跟踪上游 Istio 并可能应用额外的补丁。 tetratefips 发行版，是符合 FIPS 标准的 tetrate 版本。  如何开始使用？ 第一步是下载 GetMesh CLI。你可以在 macOS 和 Linux 平台上安装 GetMesh。我们可以使用以下命令来下载最新版本的 GetMesh 和认证的 Istio。\n如何开始？ 第一步是下载 GetMesh CLI。你可以在 macOS 和 Linux 平台上安装 GetMesh。我们可以使用以下命令下载最新版本的 GetMesh …","relpermalink":"/istio-handbook/setup/getmesh/","summary":"Istio 是最受欢迎和发展最快的开源项目之一。它的发布时间表对企业的生命周期和变更管理实践来说可能非常激进。GetMesh 通过针对不同的 Kubernetes 分发版测试所有 Istio 版本以确保功能的完整性来解决这一问题。GetMesh 的","title":"GetMesh"},{"content":"过滤器链匹配允许我们指定为监听器选择特定过滤器链的标准。\n我们可以在配置中定义多个过滤器链，然后根据目标端口、服务器名称、协议和其他属性来选择和执行它们。例如，我们可以检查哪个主机名正在连接，然后选择不同的过滤器链。如果主机名 hello.com 连接，我们可以选择一个过滤器链来呈现该特定主机名的证书。\n在 Envoy 开始过滤器匹配之前，它需要有一些由监听器过滤器从接收的数据包中提取的数据。之后，Envoy 要选择一个特定的过滤器链，必须满足所有的匹配条件。例如，如果我们对主机名和端口进行匹配，这两个值都需要匹配，Envoy 才能选择该过滤器链。\n匹配顺序如下：\n 目的地端口（当使用 use_original_dst 时） 目的地 IP 地址 服务器名称（TLS 协议的 SNI） 传输协议 应用协议（TLS 协议的 ALPN） 直接连接的源 IP 地址（这只在我们使用覆盖源地址的过滤器时与源 IP 地址不同，例如，代理协议监听器过滤器） 来源类型（例如，任何、本地或外部网络） 源 IP 地址 来源端口  具体标准，如服务器名称 / SNI 或 IP 地址，也允许使用范围或通配符。如果 …","relpermalink":"/envoy-handbook/listener/filter-chain-matching/","summary":"过滤器链匹配允许我们指定为监听器选择特定过滤器链的标准。 我们可以在配置中定义多个过滤器链，然后根据目标端口、服务器名称、协议和其他属性来选择和执行它们。例如，我们可以检查哪个主机名正在连接，然后选择不","title":"Header 操作"},{"content":"Envoy 具有一个内置的 HTTP Lua 过滤器，允许在请求和响应流中运行 Lua 脚本。Lua 是一种可嵌入的脚本语言，主要在嵌入式系统和游戏中流行。Envoy 使用 LuaJIT（Lua 的即时编译器）作为运行时。LuaJIT 支持的最高 Lua 脚本版本是 5.1，其中一些功能来自 5.2。\n在运行时，Envoy 为每个工作线程创建一个 Lua 环境。正因为如此，没有真正意义上的全局数据。任何在加载时创建和填充的全局数据都可以从每个独立的工作线程中看到。\nLua 脚本是以同步风格的 coroutines 运行的，即使它们可能执行复杂的异步任务。这使得它更容易编写。Envoy 通过一组 API 执行所有的网络 / 异步处理。当一个异步任务被调用时，Envoy 会暂停脚本的执行，等到异步操作完成就会恢复。\n我们不应该从脚本中执行任何阻塞性操作，因为这会影响 Envoy 的性能。我们应该只使用 Envoy 的 API 来进行所有的 IO 操作。\n我们可以使用 Lua 脚本修改和 / 或检查请求和响应头、正文和 Trailer。我们还可以对上游主机进行出站异步 HTTP 调用，或者执 …","relpermalink":"/envoy-handbook/extending-envoy/lua-filter/","summary":"Envoy 具有一个内置的 HTTP Lua 过滤器，允许在请求和响应流中运行 Lua 脚本。Lua 是一种可嵌入的脚本语言，主要在嵌入式系统和游戏中流行。Envoy 使用 LuaJIT（Lua 的即时编译器）作为运行时。LuaJIT 支持的最","title":"Lua 过滤器"},{"content":"Prometheus 是一个开源的监控系统和时间序列数据库。Istio 使用 Prometheus 来记录指标，跟踪 Istio 和网格中的应用程序的健康状况。\n要安装 Prometheus，我们可以使用 Istio 安装包中 /samples/addons 文件夹中的示例安装。\n$ kubectl apply -f istio-1.9.0/samples/addons/prometheus.yaml serviceaccount/prometheus created configmap/prometheus created clusterrole.rbac.authorization.k8s.io/prometheus created clusterrolebinding.rbac.authorization.k8s.io/prometheus created service/prometheus created deployment.apps/prometheus created 要打开 Prometheus 仪表板，我们可以使用 Istio CLI 中的 dashboard 命 …","relpermalink":"/istio-handbook/observability/prometheus/","summary":"Prometheus 是一个开源的监控系统和时间序列数据库。Istio 使用 Prometheus 来记录指标，跟踪 Istio 和网格中的应用程序的健康状况。 要安装 Prometheus，我们可以使用 Istio 安装包中 /samples/addons 文件夹中的示例安装。 $ kubectl apply -f istio-1.9.0/samples/addons/prometheus.yaml serviceaccount/prometheus created configmap/prometheus created clusterrole.rbac.authorization.k8s.io/prometheus created clusterrolebinding.rbac.authorization.k8s.io/prometheus","title":"Prometheus"},{"content":"RequestAuthentication（请求认证）定义了工作负载支持哪些请求认证方法。如果请求包含无效的认证信息，它将根据配置的认证规则拒绝该请求。不包含任何认证凭证的请求将被接受，但不会有任何认证的身份。\n示例 为了限制只对经过认证的请求进行访问，应该伴随着一个授权规则。\n例如，要求对具有标签 app:httpbin 的工作负载的所有请求使用 JWT 认证。\napiVersion:security.istio.io/v1beta1kind:RequestAuthenticationmetadata:name:httpbinnamespace:foospec:selector:matchLabels:app:httpbinjwtRules:- …","relpermalink":"/istio-handbook/config-security/request-authentication/","summary":"RequestAuthentication（请求认证）定义了工作负载支持哪些请求认证方法。如果请求包含无效的认证信息，它将根据配置的认证规则拒绝该请求。不包含任何认证凭证的请求将被接受，但不会有任何认","title":"RequestAuthentication"},{"content":"Slime 是由网易数帆微服务团队开源的一款基于 Istio 的智能网格管理器。Slime 基于 Kubernetes Operator 实现，可作为 Istio 的 CRD 管理器，无须对 Istio 做任何定制化改造，就可以定义动态的服务治理策略，从而达到自动便捷使用 Istio 和 Envoy 高阶功能的目的。\nSlime 试图解决的问题 Slime 项目的诞生主要为了解决以下问题：\n 网格内所有服务配置全量下到所有 Sidecar Proxy，导致其消耗大量资源使得应用性能变差的问题 如何在 Istio 中实现高阶扩展的问题：比如扩展 HTTP 插件；根据服务的资源使用率做到自适应限流  Slime 解决以上问题的答案是构建 Istio 的控制平面，具体做法是：\n 构建可拔插控制器 数据平面监控 CRD 转换  通过以上方式 Slime 可以实现配置懒加载和插件管理器。\nSlime 架构 Slime 内部分为三大模块，其架构图如下所示。\n   Slime 内部架构图  Slime 内部三大组件为：\n slime-boot：在 Kubernetes 上部署 Slime …","relpermalink":"/istio-handbook/ecosystem/slime/","summary":"Slime 是由网易数帆微服务团队开源的一款基于 Istio 的智能网格管理器。Slime 基于 Kubernetes Operator 实现，可作为 Istio 的 CRD 管理器，无须对 Istio 做任何定制化改造，就可以定义动态的服务治理策略，从而达到自动便捷使用 Istio 和 Envoy 高阶功能的目的。","title":"Slime"},{"content":"VirtualService 主要配置流量路由。以下是在流量路由背景下定义的几个有用的术语。\n Service 是与服务注册表（service registry）中的唯一名称绑定的应用行为单元。服务由多个网络端点（endpoint）组成，这些端点由运行在 pod、容器、虚拟机等的工作负载实例实现。 服务版本，又称子集（subset）：在持续部署方案中，对于一个给定的服务，可能有不同的实例子集，运行应用程序二进制的不同变体。这些变体不一定是不同的 API 版本。它们可能是同一服务的迭代变化，部署在不同的环境（prod、staging、dev 等）。发生这种情况的常见场景包括 A/B 测试、金丝雀发布等。一个特定版本的选择可以根据各种标准（header、URL 等）和 / 或分配给每个版本的权重来决定。每个服务都有一个由其所有实例组成的默认版本。 源（source）：下游客户端调用服务。 Host：客户端在尝试连接到服务时使用的地址。 访问模型（access model）：应用程序只针对目标服务（host），而不了解各个服务版本（子集）。版本的实际选择是由代理/sidecar 决定的，使应 …","relpermalink":"/istio-handbook/config-networking/virtual-service/","summary":"VirtualService 主要配置流量路由。以下是在流量路由背景下定义的几个有用的术语。 Service 是与服务注册表（service registry）中的唯一名称绑定的应用行为单元。服务由多个网络端点（endpoint）组成，这些端点由","title":"VirtualService"},{"content":"在集群和 Istio 准备好后，我们可以克隆在 Online Boutique 应用库。\n1. 克隆仓库\ngit clone https://github.com/GoogleCloudPlatform/microservices-demo.git 2. 前往 microservices-demo 目录\ncd microservices-demo 3. 创建 Kubernetes 资源\nkubectl apply -f release/kubernetes-manifests.yaml 4. 检查所有 Pod 都在运行\n$ kubectl get pods NAME READY STATUS RESTARTS AGE adservice-5c9c7c997f-n627f 2/2 Running 0 2m15s cartservice-6d99678dd6-767fb 2/2 Running 2 2m16s checkoutservice-779cb9bfdf-l2rs9 2/2 Running 0 2m18s currencyservice-5db6c7d559-9drtc 2/2 …","relpermalink":"/istio-handbook/practice/online-boutique/","summary":"在集群和 Istio 准备好后，我们可以克隆在 Online Boutique 应用库。 1. 克隆仓库 git clone https://github.com/GoogleCloudPlatform/microservices-demo.git 2. 前往 microservices-demo 目录 cd microservices-demo 3. 创建 Kubernetes 资源 kubectl apply -f release/kubernetes-manifests.yaml 4. 检查所有 Pod 都在运行 $ kubectl get pods NAME READY STATUS RESTARTS AGE adservice-5c9c7c997f-n627f 2/2 Running 0 2m15s cartservice-6d99678dd6-767fb 2/2 Running 2 2m16s checkoutservice-779cb9bfdf-l2rs9 2/2 Running 0 2m18s currencyservice-5db6c7d559-9drtc 2/2 Running 0 2m16s emailservice-5c47dc87bf-dk7qv 2/2 Running 0 2m18s frontend-5fcb8cdcdc-8c9dk 2/2 Running 0 2m17s loadgenerator-79bff5bd57-q9qkd 2/2","title":"部署 Online Boutique 应用"},{"content":"Istio 提供两种类型的认证：对等认证和请求认证。\n对等认证 对等认证用于服务间的认证，以验证建立连接的客户端。\n当两个服务试图进行通信时，相互 TLS 要求它们都向对方提供证书，因此双方都知道它们在与谁交谈。如果我们想在服务之间启用严格的相互 TLS，我们可以使用 PeerAuthentication 资源，将 mTLS 模式设置为 STRICT。\n使用 PeerAuthentication 资源，我们可以打开整个网状结构的相互 TLS（mTLS），而不需要做任何代码修改。\n然而，Istio 也支持一种优雅的模式，我们可以选择在一个工作负载或命名空间的时间内进入相互 TLS。这种模式被称为许可模式。\n当你安装 Istio 时，允许模式是默认启用的。启用允许模式后，如果客户端试图通过相互 TLS 连接到我，我将提供相互 TLS。如果客户端不使用相互 TLS，我也可以用纯文本响应。我是允许客户端做 mTLS 或不做的。使用这种模式，你可以在你的网状网络中逐渐推广相互 TLS。\n简而言之，PeerAuthentication 谈论的是工作负载或服务的通信方式，它并没有说到最终用户。那么， …","relpermalink":"/istio-handbook/security/peer-and-request-authn/","summary":"Istio 提供两种类型的认证：对等认证和请求认证。 对等认证 对等认证用于服务间的认证，以验证建立连接的客户端。 当两个服务试图进行通信时，相互 TLS 要求它们都向对方提供证书，因此双方都知道它们在与谁交谈。如果我们想在","title":"对等认证和请求认证"},{"content":"我们在前面看到了通过客户端库来治理服务的架构图，那是我们在改造成服务网格架构前使用微服务架构通常的形式，下图是使用服务网格架构的最终形式。\n   服务网格架构示意图  当然在达到这一最终形态之前我们需要将架构一步步演进，下面给出的是参考的演进路线。\nIngress 或边缘代理 如果你使用的是 Kubernetes 做容器编排调度，那么在进化到服务网格架构之前，通常会使用 Ingress Controller，做集群内外流量的反向代理，如使用 Traefik 或 Nginx Ingress Controller。\n   Ingress 或边缘代理架构示意图  这样只要利用 Kubernetes 的原有能力，当你的应用微服务化并容器化需要开放外部访问且只需要 L7 代理的话这种改造十分简单，但问题是无法管理服务间流量。\n路由器网格 Ingress 或者边缘代理可以处理进出集群的流量，为了应对集群内的服务间流量管理，我们可以在集群内加一个 Router 层，即路由器层，让集群内所有服务间的流量都通过该路由器。\n   路由器网格架构示意图  这个架构无需对原有的单体应用和新的微服务应用做什么改 …","relpermalink":"/istio-handbook/concepts/service-mesh-patterns/","summary":"我们在前面看到了通过客户端库来治理服务的架构图，那是我们在改造成服务网格架构前使用微服务架构通常的形式，下图是使用服务网格架构的最终形式。 服务网格架构示意图 当然在达到这一最终形态之前我们需要将架构一步","title":"服务网格的实现模式"},{"content":"要用密钥管理服务（KMS）提供商插件来加密 Secret，可以使用以下加密配置 YAML 文件的例子来为提供商设置属性。这个例子是基于 Kubernetes 的官方文档。\napiVersion:apiserver.config.k8s.io/v1kind:EncryptionConfigurationresources:- resources:- secretsproviders:- kms:name:myKMSPluginendpoint:unix://tmp/socketfile.sockcachesize:100timeout:3s- identity:{}要配置 API 服务器使用 KMS 提供商，请将 --encryption-provider-config 标志与配置文件的位置一起设置，并重新启动 API 服务器。\n要从本地加密提供者切换到 KMS，请将 EncryptionConfiguration 文件中的 KMS 提供者部分添加到当前加密方法之上，如下所示。 …","relpermalink":"/kubernetes-hardening-guidance/appendix/i/","summary":"要用密钥管理服务（KMS）提供商插件来加密 Secret，可以使用以下加密配置 YAML 文件的例子来为提供商设置属性。这个例子是基于 Kubernetes 的官方文档。 apiVersion:apiserver.config.k8s.io/v1kind:EncryptionConfigurationresources:- resources:- secretsproviders:- kms:name:myKMSPluginendpoint:unix://tmp/socketfile.sockcachesize:100timeout:3s- identity:{}要配置 API 服务器使用 KMS 提供商，请将 --encryption-provider-config","title":"附录 I：KMS 配置实例"},{"content":"动态提供配置另一种方式是通过指向文件系统上的文件。为了使动态配置发挥作用，我们需要在 node 字段下提供信息。如果我们可能有多个 Envoy 代理指向相同的配置文件，那么 node 字段是用来识别一个特定的 Envoy 实例。\n   为了指向动态资源，我们可以使用 dynamic_resources 字段来告诉 Envoy 在哪里可以找到特定资源的动态配置。例如：\nnode:cluster:my-clusterid:some-iddynamic_resources:lds_config:path:/etc/envoy/lds.yamlcds_config:path:/etc/envoy/cds.yaml上面的片段是一个有效的 Envoy 配置。如果我们把 LDS 和 CDS 作为静态资源来提供，它们的单独配置将非常相似。唯一不同的是，我们必须指定资源类型和版本信息。下面是 CDS 配置的一个片段。\nversion_info:\u0026#34;0\u0026#34;resources:- \u0026#34;@type\u0026#34;: …","relpermalink":"/envoy-handbook/dynamic-config/filesystem/","summary":"动态提供配置另一种方式是通过指向文件系统上的文件。为了使动态配置发挥作用，我们需要在 node 字段下提供信息。如果我们可能有多个 Envoy 代理指向相同的配置文件，那么 node 字段是用来识别一个特定的 Envoy 实例。 为了指向动态资源","title":"来自文件系统的动态配置"},{"content":"我们可以在 HTTP 或 TCP 过滤器级别和监听器级别上配置访问记录器。我们还可以配置多个具有不同日志格式和日志沉积的访问日志。日志沉积（log sink） 是一个抽象的术语，指的是日志写入的位置，例如，写入控制台（stdout、stderr）、文件或网络服务。\n要配置多个访问日志的情况是，我们想在控制台（标准输出）中看到高级信息，并将完整的请求细节写入磁盘上的文件。用于配置访问记录器的字段被称为 access_log。\n让我们看看在 HTTP 连接管理器（HCM）层面上启用访问日志到标准输出（StdoutAccessLog）的例子。\n- filters:- name:envoy.filters.network.http_connection_managertyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManagerstat_prefix:ingress_httpaccess_log:- …","relpermalink":"/envoy-handbook/logging/confguring-logging/","summary":"我们可以在 HTTP 或 TCP 过滤器级别和监听器级别上配置访问记录器。我们还可以配置多个具有不同日志格式和日志沉积的访问日志。日志沉积（log sink） 是一个抽象的术语，指的是日志写入的位置，例如，写入控制台（st","title":"配置访问记录器"},{"content":"/config_dump 端点是一种快速的方法，可以将当前加载的 Envoy 配置显示为 JSON 序列化的 proto 消息。\nEnvoy 输出以下组件的配置，并按照下面的顺序排列。\n 自举（bootstrap） 集群（clusters） 端点（endpoints） 监听器（listeners） 范围路由（scoped routes） 路由（routes） 秘密（secrets）  包括 EDS 配置 为了输出端点发现服务（EDS）的配置，我们可以在查询中加入 ?include_eds 参数。\n筛选输出 同样，我们可以通过提供我们想要包括的资源和一个掩码来过滤输出，以返回一个字段的子集。\n例如，为了只输出静态集群配置，我们可以在资源查询参数中使用 static_clusters 字段，从 ClustersConfigDump proto 在 resource 查询参数中使用。\n$ curl localhost:9901/config_dump?resource=static_clusters{\u0026#34;configs\u0026#34;: [{\u0026#34;@type\u0026#34;: …","relpermalink":"/envoy-handbook/admin-interface/configuration-dump/","summary":"/config_dump 端点是一种快速的方法，可以将当前加载的 Envoy 配置显示为 JSON 序列化的 proto 消息。 Envoy 输出以下组件的配置，并按照下面的顺序排列。 自举（bootstrap） 集群（clusters） 端点（endpoints） 监听器（l","title":"配置转储"},{"content":"Service Mesh 又译作 “服务网格”，作为服务间通信的基础设施层。Buoyant 公司的 CEO Willian Morgan 在他的这篇文章 WHAT’S A SERVICE MESH? AND WHY DO I NEED ONE? 中解释了什么是 Service Mesh，为什么云原生应用需要 Service Mesh。\n服务网格是用于处理服务间通信的专用基础设施层。它负责通过包含现代云原生应用程序的复杂服务拓扑来可靠地传递请求。实际上，服务网格通常通过一组轻量级网络代理来实现，这些代理与应用程序代码一起部署，而不需要感知应用程序本身。—— Willian Morgan Buoyant CEO\n服务网格（Service Mesh）这个术语通常用于描述构成这些应用程序的微服务网络以及应用之间的交互。随着规模和复杂性的增长，服务网格越来越难以理解和管理。它的需求包括服务发现、负载均衡、故障恢复、指标收集和监控以及通常更加复杂的运维需求，例如 A/B 测试、金丝雀发布、限流、访问控制和端到端认证等。\n服务网格的特点 服务网格有如下几个特点：\n 应用程序间通讯的中间层 轻量级网络 …","relpermalink":"/istio-handbook/intro/what-is-service-mesh/","summary":"Service Mesh 又译作 “服务网格”，作为服务间通信的基础设施层。Buoyant 公司的 CEO Willian Morgan 在他的这篇文章 WHAT’S A SERVICE MESH? AND WHY DO I NEED ONE? 中解释了什么是 Service Mesh，为什么云原生应用需要 Service Mesh。 服务网格是用于处理服","title":"什么是服务网格？"},{"content":"如果我们有在虚拟机上运行的工作负载，我们可以将它们连接到 Istio 服务网格，使其成为网格的一部分。\n带有虚拟机的 Istio 服务网格有两种架构：单网络架构和多网络架构。\n单网络架构 在这种情况下，有一个单一的网络。Kubernetes 集群和在虚拟机上运行的工作负载都在同一个网络中，它们可以直接相互通信。\n   单网络架构  单网络架构\n控制面的流量（配置更新、证书签署）是通过 Gateway 发送的。\n虚拟机被配置了网关地址，所以它们在启动时可以连接到控制平面。\n多网络架构 多网络架构横跨多个网络。Kubernetes 集群在一个网络内，而虚拟机则在另一个网络内。这使得 Kubernetes 集群中的 Pod 和虚拟机上的工作负载无法直接相互通信。\n   多网络架构  多网络架构\n所有的流量，控制面和 pod 到工作服的流量都流经网关，网关作为两个网络之间的桥梁。\nIstio 中如何表示虚拟机工作负载？ 在 Istio 服务网格中，有两种方式来表示虚拟机工作负载。\n工作负载组（WorkloadGroup 资源）类似于 Kubernetes 中的部署（Deployment），它代 …","relpermalink":"/istio-handbook/advanced/vm/","summary":"如果我们有在虚拟机上运行的工作负载，我们可以将它们连接到 Istio 服务网格，使其成为网格的一部分。 带有虚拟机的 Istio 服务网格有两种架构：单网络架构和多网络架构。 单网络架构 在这种情况下，有一个单一的网络。Kuber","title":"虚拟机负载"},{"content":"Envoy 支持端点上不同的主动健康检查方法。HTTP、TCP、gRPC 和 Redis 健康检查。健康检查方法可以为每个集群单独配置。我们可以通过集群配置中的 health_checks 字段来配置健康检查。\n无论选择哪种健康检查方法，都需要定义几个常见的配置设置。\n超时（timeout）表示分配给等待健康检查响应的时间。如果在这个字段中指定的时间值内没有达到响应，健康检查尝试将被视为失败。间隔（internal）指定健康检查之间的时间节奏。例如，5 秒的间隔将每 5 秒触发一次健康检查。\n其他两个必要的设置可用于确定一个特定的端点何时被认为是健康或不健康的。healthy_threshold 指定在一个端点被标记为健康之前所需的 “健康” 检查（例如，HTTP 200 响应）的数量。unhealthy_threshold 的作用与此相同，但是对于 “不健康” 的健康检查，它指定了在一个端点被标记为不健康之前所需的不健康检查的数量。\n1. HTTP 健康检查\nEnvoy 向端点发送一个 HTTP 请求。如果端点回应的是 HTTP 200，Envoy 认为它是健康的。200 响应是默认 …","relpermalink":"/envoy-handbook/cluster/active-health-checking/","summary":"Envoy 支持端点上不同的主动健康检查方法。HTTP、TCP、gRPC 和 Redis 健康检查。健康检查方法可以为每个集群单独配置。我们可以通过集群配置中的 health_checks 字段来配置健康检查。 无论选择哪种健康检查方法，都需要定义几个常","title":"主动健康检查"},{"content":"众所周知，Kubernetes 是 Google 于 2014 年 6 月基于其内部使用的 Borg 系统开源出来的容器编排调度引擎。其实从 2000 年开始，Google 就开始基于容器研发三个容器管理系统，分别是 Borg、Omega 和 Kubernetes。这篇由 Google 工程师 Brendan Burns、Brian Grant、David Oppenheimer、Eric Brewer 和 John Wilkes 几人在 2016 年发表的《Borg, Omega, and Kubernetes》论文里，阐述了 Google 从 Borg 到 Kubernetes 这个旅程中所获得知识和经验教训。\nBorg、Omega 和 Kubernetes Google 从 2000 年初就开始使用容器（Linux 容器）系统，Google 开发出来的第一个统一的容器管理系统在内部称之为 “Borg”，用来管理长时间运行的生产服务和批处理服务。由于 Borg 的规模、功能的广泛性和超高的稳定性，一直到现在 Borg 在 Google 内部依然是主要的容器管理系统。\nGoogle  …","relpermalink":"/cloud-native-handbook/kubernetes/history/","summary":"众所周知，Kubernetes 是 Google 于 2014 年 6 月基于其内部使用的 Borg 系统开源出来的容器编排调度引擎。其实从 2000 年开始，Google 就开始基于容器研发三个容器管理系统，分别是 Borg、Omega 和 Kuberne","title":"Kubernetes 的历史"},{"content":"要创建一个 pod-reader 角色，创建一个 YAML 文件，内容如下：\napiVersion:rbac.authorization.k8s.io/v1kind:Rolemetadata:namespace:your-namespace-namename:pod-readerrules:- apiGroups:[\u0026#34;\u0026#34;]# \u0026#34;\u0026#34; 表示核心 API 组resources:[\u0026#34;pods\u0026#34;]verbs:[\u0026#34;get\u0026#34;,\u0026#34;watch\u0026#34;,\u0026#34;list\u0026#34;]应用角色：\nkubectl apply --f role.yaml 要创建一个全局性的 pod-reader ClusterRole：\napiVersion:rbac.authorization.k8s.io/v1kind:ClusterRolemetadata:default# \u0026#34;namespace\u0026#34; 被省略了，因为 ClusterRoles 没有被绑定到一个命名空间上name:global-pod-readerrules:- apiGroups:[\u0026#34;\u0026#34;]# \u0026#34;\u0026#34; 表示核心 API  …","relpermalink":"/kubernetes-hardening-guidance/appendix/j/","summary":"要创建一个 pod-reader 角色，创建一个 YAML 文件，内容如下： apiVersion:rbac.authorization.k8s.io/v1kind:Rolemetadata:namespace:your-namespace-namename:pod-readerrules:- apiGroups:[\"\"]# \"\" 表示核心 API 组resources:[\"pods\"]verbs:[\"get\",\"watch\",\"l","title":"附录 J：pod-reader RBAC 角色"},{"content":"Node 是 Kubernetes 集群的工作节点，可以是物理机也可以是虚拟机。\nNode 的状态 Node 包括如下状态信息：\n Address  HostName：可以被 kubelet 中的 --hostname-override 参数替代。 ExternalIP：可以被集群外部路由到的 IP 地址。 InternalIP：集群内部使用的 IP，集群外部无法访问。   Condition  OutOfDisk：磁盘空间不足时为 True Ready：Node controller 40 秒内没有收到 node 的状态报告为 Unknown，健康为 True，否则为 False。 MemoryPressure：当 node 有内存压力时为 True，否则为 False。 DiskPressure：当 node 有磁盘压力时为 True，否则为 False。   Capacity  CPU 内存 可运行的最大 Pod 个数   Info：节点的一些版本信息，如 OS、kubernetes、docker 等  Node 管理 禁止 Pod 调度到该节点上。\nkubectl cordon …","relpermalink":"/kubernetes-handbook/cluster/node/","summary":"Node 是 Kubernetes 集群的工作节点，可以是物理机也可以是虚拟机。 Node 的状态 Node 包括如下状态信息： Address HostName：可以被 kubelet 中的 --hostname-override 参数替代。 ExternalIP：可以被集群外部路由到的 IP 地址。 InternalIP：集群","title":"Node"},{"content":"要创建一个 RoleBinding，需创建一个 YAML 文件，内容如下：\napiVersion:rbac.authorization.k8s.io/v1# 这个角色绑定允许 \u0026#34;jane\u0026#34; 读取 \u0026#34;your-namespace-name\u0026#34; 的 Pod 命名空间# 你需要在该命名空间中已经有一个名为 \u0026#34;pod-reader\u0026#34;的角色。kind:RoleBindingmetadata:name:read-podsnamespace:your-namespace-namesubjects:# 你可以指定一个以上的 \u0026#34;subject\u0026#34;- kind:Username:jane# \u0026#34;name\u0026#34; 是大小写敏感的apiGroup:rbac.authorization.k8s.ioroleRef:# \u0026#34;roleRef\u0026#34; 指定绑定到一个 Role/ClusterRolekind:Role# 必须是 Role 或 ClusterRolename:pod-reader# 这必须与你想绑定的 Role 或 ClusterRole 的名字相匹配apiGroup:rbac.authorization.k8s.io …","relpermalink":"/kubernetes-hardening-guidance/appendix/k/","summary":"要创建一个 RoleBinding，需创建一个 YAML 文件，内容如下： apiVersion:rbac.authorization.k8s.io/v1# 这个角色绑定允许 \"jane\" 读取 \"your-namespace-name\" 的 Pod 命名空间# 你需要在该命名空间中已经有一个名为 \"pod-reader\"的角色。kind:Rol","title":"附录K：RBAC RoleBinding 和 ClusterRoleBinding 示例"},{"content":"在一个 Kubernetes 集群中可以使用 namespace 创建多个 “虚拟集群”，这些 namespace 之间可以完全隔离，也可以通过某种方式，让一个 namespace 中的 service 可以访问到其他的 namespace 中的服务。\n哪些情况下适合使用多个 namespace 因为 namespace 可以提供独立的命名空间，因此可以实现部分的环境隔离。当你的项目和人员众多的时候可以考虑根据项目属性，例如生产、测试、开发划分不同的 namespace。\nNamespace 使用 获取集群中有哪些 namespace\nkubectl get ns\n集群中默认会有 default 和 kube-system 这两个 namespace。\n在执行 kubectl 命令时可以使用 -n 指定操作的 namespace。\n用户的普通应用默认是在 default 下，与集群管理相关的为整个集群提供服务的应用一般部署在 kube-system 的 namespace 下，例如我们在安装 kubernetes 集群时部署的 kubedns、heapseter、EFK …","relpermalink":"/kubernetes-handbook/cluster/namespace/","summary":"在一个 Kubernetes 集群中可以使用 namespace 创建多个 “虚拟集群”，这些 namespace 之间可以完全隔离，也可以通过某种方式，让一个 namespace 中的 service 可以访问到其他的 namespace 中的服务。 哪些情况下适合使用多个 namespace 因为 namespace 可以提供独立的命名空间，因此可以实现部","title":"Namespace"},{"content":"下面是一个审计策略，它以最高级别记录所有审计事件：\napiVersion:audit.k8s.io/v1kind:Policyrules:- level:RequestResponse# 这个审计策略记录了 RequestResponse 级别的所有审计事件这种审计策略在最高级别上记录所有事件。如果一个组织有可用的资源来存储、解析和检查大量的日志，那么在最高级别上记录所有事件是一个很好的方法，可以确保当事件发生时，所有必要的背景信息都出现在日志中。如果资源消耗和可用性是一个问题，那么可以建立更多的日志规则来降低非关键组件和常规非特权操作的日志级别，只要满足系统的审计要求。如何建立这些规则的例子可以在 Kubernetes 官方文档中找到。\n","relpermalink":"/kubernetes-hardening-guidance/appendix/l/","summary":"下面是一个审计策略，它以最高级别记录所有审计事件： apiVersion:audit.k8s.io/v1kind:Policyrules:- level:RequestResponse# 这个审计策略记录了 RequestResponse 级别的所有审计事件这种审计策略在最高级别上记录所有事件。如果一个组织有可用的资源来存储、解析和检查大量的日志，那么在最高级别上","title":"附录 L：审计策略"},{"content":"Label 是附着到 object 上（例如 Pod）的键值对。可以在创建 object 的时候指定，也可以在 object 创建后随时指定。Labels 的值对系统本身并没有什么含义，只是对用户才有意义。\n\u0026#34;labels\u0026#34;: { \u0026#34;key1\u0026#34; : \u0026#34;value1\u0026#34;, \u0026#34;key2\u0026#34; : \u0026#34;value2\u0026#34; } Kubernetes 最终将对 labels 最终索引和反向索引用来优化查询和 watch，在 UI 和命令行中会对它们排序。不要在 label 中使用大型、非标识的结构化数据，记录这样的数据应该用 annotation。\n动机 Label 能够将组织架构映射到系统架构上（就像是康威定律），这样能够更便于微服务的管理，你可以给 object 打上如下类型的 label：\n \u0026#34;release\u0026#34; : \u0026#34;stable\u0026#34;, \u0026#34;release\u0026#34; : \u0026#34;canary\u0026#34; \u0026#34;environment\u0026#34; : \u0026#34;dev\u0026#34;, \u0026#34;environment\u0026#34; : \u0026#34;qa\u0026#34;, \u0026#34;environment\u0026#34; : \u0026#34;production\u0026#34; \u0026#34;tier\u0026#34; : \u0026#34;frontend\u0026#34;, \u0026#34;tier\u0026#34; : \u0026#34;backend\u0026#34;, …","relpermalink":"/kubernetes-handbook/cluster/label/","summary":"Label 是附着到 object 上（例如 Pod）的键值对。可以在创建 object 的时候指定，也可以在 object 创建后随时指定。Labels 的值对系统本身并没有什么含义，只是对用户才有意义。 \"labels\": { \"key1\" : \"value1\", \"key2\" : \"value2\" } Kubernetes 最终将对 labels 最终索引和反向索引用","title":"Label"},{"content":"在控制平面，用文本编辑器打开 kube-apiserver.yaml 文件。编辑 kube-apiserver 配置需要管理员权限。\nsudo vi /etc/kubernetes/manifests/kube-apiserver.yaml 在 kube-apiserver.yaml 文件中添加以下文字：\n--audit-policy-file=/etc/kubernetes/policy/audit-policy.yaml --audit-log-path=/var/log/audit.log --audit-log-maxage=1825 audit-policy-file 标志应该设置为审计策略的路径，而 audit-log-path 标志应该设置为所需的审计日志写入的安全位置。还有一些其他的标志，比如这里显示的 audit-log-maxage 标志，它规定了日志应该被保存的最大天数，还有一些标志用于指定要保留的最大审计日志文件的数量，最大的日志文件大小（兆字节）等等。启用日志记录的唯一必要标志是 audit-policy-file 和 audit-log-path 标志。其他 …","relpermalink":"/kubernetes-hardening-guidance/appendix/m/","summary":"在控制平面，用文本编辑器打开 kube-apiserver.yaml 文件。编辑 kube-apiserver 配置需要管理员权限。 sudo vi /etc/kubernetes/manifests/kube-apiserver.yaml 在 kube-apiserver.yaml 文件中添加以下文字： --audit-policy-file=/etc/kubernetes/policy/audit-policy.yaml --audit-log-path=/var/log/audit.log --audit-log-maxage=1825 audit-policy-file 标志应该设置为审计策略的路径，而 audit-log-path 标志应该设置为所需的审计日志写入的安全位置。还有一些其他的标志，比","title":"附录 M：向 kube-apiserver 提交审计策略文件的标志示例"},{"content":"Annotation，顾名思义，就是注解。Annotation 可以将 Kubernetes 资源对象关联到任意的非标识性元数据。使用客户端（如工具和库）可以检索到这些元数据。\n关联元数据到对象 Label 和 Annotation 都可以将元数据关联到 Kubernetes 资源对象。Label 主要用于选择对象，可以挑选出满足特定条件的对象。相比之下，annotation 不能用于标识及选择对象。annotation 中的元数据可多可少，可以是结构化的或非结构化的，也可以包含 label 中不允许出现的字符。\nAnnotation 和 label 一样都是 key/value 键值对映射结构：\n\u0026#34;annotations\u0026#34;: {\u0026#34;key1\u0026#34;:\u0026#34;value1\u0026#34;,\u0026#34;key2\u0026#34;:\u0026#34;value2\u0026#34;} 以下列出了一些可以记录在 annotation 中的对象信息：\n  声明配置层管理的字段。使用 annotation 关联这类字段可以用于区分以下几种配置来源：客户端或服务器设置的默认值，自动生成的字段或自动生成的 auto-scaling 和 auto-sizing 系统配置的字段。\n  创建 …","relpermalink":"/kubernetes-handbook/cluster/annotation/","summary":"Annotation，顾名思义，就是注解。Annotation 可以将 Kubernetes 资源对象关联到任意的非标识性元数据。使用客户端（如工具和库）可以检索到这些元数据。 关联元数据到对象 Label 和 Annotation 都可以将元数据关联到 Kubernetes 资源","title":"Annotation"},{"content":"YAML 文件示例：\napiVersion:v1kind:Configpreferences:{}clusters:- name:example-clustercluster:server:http://127.0.0.1:8080#web endpoint address for the log files to be sent toname:audit-webhook-serviceusers:- name:example-usersuser:username:example-userpassword:example-passwordcontexts:- name:example-contextcontext:cluster:example-clusteruser:example-usercurrent-context:example-context#source: https://dev.bitolog.com/implement-audits-webhook/由 webhook 发送的审计事件是以 HTTP POST 请求的形式发送的，请求体中包含 JSON 审计事件。指定的地 …","relpermalink":"/kubernetes-hardening-guidance/appendix/n/","summary":"YAML 文件示例： apiVersion:v1kind:Configpreferences:{}clusters:- name:example-clustercluster:server:http://127.0.0.1:8080#web endpoint address for the log files to be sent toname:audit-webhook-serviceusers:- name:example-usersuser:username:example-userpassword:example-passwordcontexts:- name:example-contextcontext:cluster:example-clusteruser:example-usercurrent-context:example-context#source: https://dev.bitolog.com/implement-audits-webhook/由 webhook 发送的审计事件是以 HTTP POST 请求的形式发送的，请求体中包含 JSON 审计","title":"附录 N：webhook 配置"},{"content":"Taint（污点）和 Toleration（容忍）可以作用于 node 和 pod 上，其目的是优化 pod 在集群间的调度，这跟节点亲和性类似，只不过它们作用的方式相反，具有 taint 的 node 和 pod 是互斥关系，而具有节点亲和性关系的 node 和 pod 是相吸的。另外还有可以给 node 节点设置 label，通过给 pod 设置 nodeSelector 将 pod 调度到具有匹配标签的节点上。\nTaint 和 toleration 相互配合，可以用来避免 pod 被分配到不合适的节点上。每个节点上都可以应用一个或多个 taint ，这表示对于那些不能容忍这些 taint 的 pod，是不会被该节点接受的。如果将 toleration 应用于 pod 上，则表示这些 pod 可以（但不要求）被调度到具有相应 taint 的节点上。\n示例 以下分别以为 node 设置 taint 和为 pod 设置 toleration 为例。\n为 node 设置 taint 为 node1 设置 taint：\nkubectl taint nodes node1 …","relpermalink":"/kubernetes-handbook/cluster/taint-and-toleration/","summary":"Taint（污点）和 Toleration（容忍）可以作用于 node 和 pod 上，其目的是优化 pod 在集群间的调度，这跟节点亲和性类似，只不过它们作用的方式相反，具有 taint 的 node 和 pod 是互斥关系，而具有节点亲和性关系的 node 和 pod 是","title":"Taint 和 Toleration（污点和容忍）"},{"content":"Kubernetes 垃圾收集器的角色是删除指定的对象，这些对象曾经有但以后不再拥有 Owner 了。\nOwner 和 Dependent 一些 Kubernetes 对象是其它一些的 Owner。例如，一个 ReplicaSet 是一组 Pod 的 Owner。具有 Owner 的对象被称为是 Owner 的 Dependent。每个 Dependent 对象具有一个指向其所属对象的 metadata.ownerReferences 字段。\n有时，Kubernetes 会自动设置 ownerReference 的值。例如，当创建一个 ReplicaSet 时，Kubernetes 自动设置 ReplicaSet 中每个 Pod 的 ownerReference 字段值。在 1.6 版本，Kubernetes 会自动为一些对象设置 ownerReference 的值，这些对象是由 ReplicationController、ReplicaSet、StatefulSet、DaemonSet 和 Deployment 所创建或管理。\n也可以通过手动设置 ownerReference 的值， …","relpermalink":"/kubernetes-handbook/cluster/garbage-collection/","summary":"Kubernetes 垃圾收集器的角色是删除指定的对象，这些对象曾经有但以后不再拥有 Owner 了。 Owner 和 Dependent 一些 Kubernetes 对象是其它一些的 Owner。例如，一个 ReplicaSet 是一组 Pod 的 Owner。具有 Owner 的对象被称为是 Owner 的 Dependent。每个 Dependent 对象具","title":"垃圾收集"},{"content":"Deployment 为 Pod 和 ReplicaSet 提供了一个声明式定义（declarative）方法，用来替代以前的 ReplicationController 来方便的管理应用。典型的应用场景包括：\n 定义 Deployment 来创建 Pod 和 ReplicaSet 滚动升级和回滚应用 扩容和缩容 暂停和继续 Deployment  比如一个简单的 nginx 应用可以定义为：\napiVersion:extensions/v1beta1kind:Deploymentmetadata:name:nginx-deploymentspec:replicas:3template:metadata:labels:app:nginxspec:containers:- name:nginximage:nginx:1.7.9ports:- containerPort:80扩容：\nkubectl scale deployment nginx-deployment --replicas 10 如果集群支持 horizontal pod autoscaling 的话， …","relpermalink":"/kubernetes-handbook/controllers/deployment/","summary":"Deployment 为 Pod 和 ReplicaSet 提供了一个声明式定义（declarative）方法，用来替代以前的 ReplicationController 来方便的管理应用。典型的应用场景包括： 定义 Deployment 来创建 Pod 和 ReplicaSet 滚动升级和回滚应用 扩容和缩容 暂停和继续 Deployment 比如一个简单的 nginx 应用可以定","title":"Deployment"},{"content":"StatefulSet 作为 Controller 为 Pod 提供唯一的标识。它可以保证部署和 scale 的顺序。\n使用案例参考：kubernetes contrib - statefulsets，其中包含zookeeper和kakfa的statefulset设置和使用说明。\nStatefulSet是为了解决有状态服务的问题（对应Deployments和ReplicaSets是为无状态服务而设计），其应用场景包括：\n 稳定的持久化存储，即Pod重新调度后还是能访问到相同的持久化数据，基于PVC来实现 稳定的网络标志，即Pod重新调度后其PodName和HostName不变，基于Headless Service（即没有Cluster IP的Service）来实现 有序部署，有序扩展，即Pod是有顺序的，在部署或者扩展的时候要依据定义的顺序依次依次进行（即从0到N-1，在下一个Pod运行之前所有之前的Pod必须都是Running和Ready状态），基于init containers来实现 有序收缩，有序删除（即从N-1到0）  从上面的应用场景可以发现，StatefulSet由以下几个 …","relpermalink":"/kubernetes-handbook/controllers/statefulset/","summary":"StatefulSet 作为 Controller 为 Pod 提供唯一的标识。它可以保证部署和 scale 的顺序。 使用案例参考：kubernetes contrib - statefulsets，其中包含zookeeper和kakfa的statefulset设置和使用说明。 St","title":"StatefulSet"},{"content":"DestinationRule 定义了在路由发生后适用于服务流量的策略。这些规则指定了负载均衡的配置、来自 sidecar 的连接池大小，以及用于检测和驱逐负载均衡池中不健康主机的异常检测设置。\n示例 例如，ratings 服务的一个简单的负载均衡策略看起来如下。\napiVersion:networking.istio.io/v1alpha3kind:DestinationRulemetadata:name:bookinfo-ratingsspec:host:ratings.prod.svc.cluster.localtrafficPolicy:loadBalancer:simple:LEAST_CONN可以通过定义一个 subset 并覆盖在服务级来配置特定版本的策略。下面的规则对前往由带有标签（version:v3）的端点（如 pod）组成的名为 testversion 的子集的所有流量使用轮询负载均衡策略。 …","relpermalink":"/istio-handbook/config-networking/destination-rule/","summary":"DestinationRule 定义了在路由发生后适用于服务流量的策略。这些规则指定了负载均衡的配置、来自 sidecar 的连接池大小，以及用于检测和驱逐负载均衡池中不健康主机的异常检测设置。 示例 例如，ratings 服务的一个简单的负载均衡策略","title":"DestinationRule"},{"content":"Grafana 是一个用于分析和监控的开放平台。Grafana 可以连接到各种数据源，并使用图形、表格、热图等将数据可视化。通过强大的查询语言，你可以定制现有的仪表盘并创建更高级的可视化。\n通过 Grafana，我们可以监控 Istio 安装和服务网格中运行的应用程序的健康状况。\n我们可以使用 grafana.yaml 来部署带有预配置仪表盘的 Grafana 示例安装。该 YAML 文件在 Istio 安装包的 /samples/addons 下。\n确保在部署 Grafana 之前部署 Promeheus 插件，因为 Grafana 使用 Prometheus 作为其数据源。\n运行下面的命令来部署 Grafana 和预配置的仪表盘：\n$ kubectl apply -f istio-1.9.0/samples/addons/grafana.yaml serviceaccount/grafana created configmap/grafana created service/grafana created deployment.apps/grafana created …","relpermalink":"/istio-handbook/observability/grafana/","summary":"Grafana 是一个用于分析和监控的开放平台。Grafana 可以连接到各种数据源，并使用图形、表格、热图等将数据可视化。通过强大的查询语言，你可以定制现有的仪表盘并创建更高级的可视化。 通过 Grafana，我们可以","title":"Grafana"},{"content":"HTTP 检查器监听器过滤器（envoy.filters.listener.http_inspector）允许我们检测应用协议是否是 HTTP。如果协议不是 HTTP，监听器过滤器将通过该数据包。\n如果应用协议被确定为 HTTP，它也会检测相应的 HTTP 协议（如 HTTP/1.x 或 HTTP/2）。\n我们可以使用过滤器链匹配中的 application_protocols 字段来检查 HTTP 检查过滤器的结果。\n让我们考虑下面的片段。\n...listener_filters:- name:envoy.filters.listener.http_inspectortyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.listener.http_inspector.v3.HttpInspectorfilter_chains:- filter_chain_match:application_protocols:[\u0026#34;h2\u0026#34;]filters:- name:my_http2_filter... - …","relpermalink":"/envoy-handbook/listener/http-inspector-listener-filter/","summary":"HTTP 检查器监听器过滤器（envoy.filters.listener.http_inspector）允许我们检测应用协议是否是 HTTP。如果协议不是 HTTP，监听器过滤器将通过该数据包。 如果应用协议被确","title":"HTTP 检查器监听器过滤器"},{"content":"Istio 是一个服务网格的开源实现。从宏观上来看 Istio 支持以下功能。\n流量管理\n利用配置，我们可以控制服务间的流量。设置断路器、超时或重试都可以通过简单的配置改变来完成。\n可观测性\nIstio 通过跟踪、监控和记录让我们更好地了解你的服务，它让我们能够快速发现和修复问题。\n安全性\nIstio 可以在代理层面上管理认证、授权和通信的加密。我们可以通过快速的配置变更在各个服务中执行政策。\nIstio 组件 Istio 服务网格有两个部分：数据平面和控制平面。\n在构建分布式系统时，将组件分离成控制平面和数据平面是一种常见的模式。数据平面的组件在请求路径上，而控制平面的组件则帮助数据平面完成其工作。\nIstio 中的数据平面由 Envoy 代理组成，控制服务之间的通信。网格的控制平面部分负责管理和配置代理。\n   Istio 架构  Envoy（数据平面） Envoy 是一个用 C++ 开发的高性能代理。Istio 服务网格将 Envoy 代理作为一个 sidecar 容器注入到你的应用容器旁边。然后该代理拦截该服务的所有入站和出站流量。注入的代理一起构成了服务网格的数据平面。 …","relpermalink":"/istio-handbook/concepts/istio-intro/","summary":"Istio 是一个服务网格的开源实现。从宏观上来看 Istio 支持以下功能。 流量管理 利用配置，我们可以控制服务间的流量。设置断路器、超时或重试都可以通过简单的配置改变来完成。 可观测性 Istio 通过跟踪、监控和记录让我们更好地了解","title":"Istio 简介"},{"content":"Istio 中有个 issue #9066 要求将 Istio 中默认使用的 Service Graph 替换成 Kiali。Kiali 最初是由 Red Hat 开源的，用于解决 Service Mesh 中可观测性即微服务的可视性问题。目前已获得 Istio 社区的官方支持。\n关于 Kiali 单体应用使用微服务架构拆分成了许多微服务的组合。服务的数量显著增加，就对需要了解服务之间的通信模式，例如容错（通过超时、重试、断路等）以及分布式跟踪，以便能够看到服务调用的去向。服务网格可以在平台级别上提供这些服务，并使应用程序编写者从以上繁重的通信模式中解放出来。路由决策在网格级别完成。Kiali 与Istio 合作，可视化服务网格拓扑、断路器和请求率等功能。Kiali还包括 Jaeger Tracing，可以提供开箱即用的分布式跟踪功能。\nKiali 提供的功能 Kiali 提供以下功能：\n 服务拓扑图 分布式跟踪 指标度量收集和图标 配置校验 健康检查和显示 服务发现  下图展示了 kiali 中显示的 Bookinfo 示例的服务拓扑图。\n   Kiali …","relpermalink":"/istio-handbook/setup/istio-observability-tool-kiali/","summary":"Istio 中有个 issue #9066 要求将 Istio 中默认使用的 Service Graph 替换成 Kiali。Kiali 最初是由 Red Hat 开源的，用于解决 Service Mesh 中可观测性即微服务的可视性问题。目前已获得 Istio 社区的官方支持。 关于 Kiali 单体应用使用微服务架构拆分成了许多微服","title":"安装可观测性工具 Kiali"},{"content":"Merbridge 是由 DaoCloud 在 2022 年初开源的的一款利用 eBPF 加速 Istio 服务网格的插件。使用 Merbridge 可以在一定程度上优化数据平面的网络性能。\n使用条件 要想使用 Merbridge，你的系统必须满足以下条件：\n Istio 网格中的主机使用 Linux 5.7 及以上版本内核  原理 在 Istio 中的透明流量劫持详解中，我们谈到 Istio 默认使用 IPtables 拦截数据平面中的流量到 Envoy 代理，这种拦截方式通用性最强。因为 Pod 中所有的 inbound 和 outbound 流量都会先通过 Envoy 代理，尤其是 Pod 接收的流量，都要先通过 IPtables 将流量劫持到 Envoy 代理后再发往 Pod 中的应用容器的端口。\n   使用 IPtables 劫持流量发到当前 Pod 的应用端口  利用 eBPF 的 sockops 和 redir 能力，可以直接将数据包从 inbound socket 传输到 outbound socket。eBPF 提供了 bpf_msg_redirect_hash 函数 …","relpermalink":"/istio-handbook/ecosystem/merbridge/","summary":"Merbridge 是由 DaoCloud 在 2022 年初开源的的一款利用 eBPF 加速 Istio 服务网格的插件。使用 Merbridge 可以在一定程度上优化数据平面的网络性能。 使用条件 要想使用 Merbridge，你的系统必须满足以下条件： Istio 网格中的主机使用 Linux 5.7 及以上版本内核","title":"Merbridge"},{"content":"服务中的工作负载之间的通信是通过 Envoy 代理进行的。当一个工作负载使用 mTLS 向另一个工作负载发送请求时，Istio 会将流量重新路由到 sidecar 代理（Envoy）。\n然后，sidecar Envoy 开始与服务器端的 Envoy 进行 mTLS 握手。在握手过程中，调用者会进行安全命名检查，以验证服务器证书中的服务账户是否被授权运行目标服务。一旦 mTLS 连接建立，Istio 就会将请求从客户端的 Envoy 代理转发到服务器端的 Envoy 代理。在服务器端的授权后，sidecar 将流量转发到工作负载。\n我们可以在服务的目标规则中改变 mTLS 行为。支持的 TLS 模式有：DISABLE（无 TLS 连接）、SIMPLE（向上游端点发起 TLS 连接）、MUTUAL（通过出示客户端证书进行认证来使用 mTLS）和 ISTIO_MUTUAL（与 MUTUAL 类似，但使用 Istio 自动生成的证书进行 mTLS）。\n什么是 mTLS？ 在一个典型的 Kubernetes 集群中，加密的流量进入集群，经过一个负载均衡器终止 TLS 连接，从而产生解密的流量。然 …","relpermalink":"/istio-handbook/security/mtls/","summary":"服务中的工作负载之间的通信是通过 Envoy 代理进行的。当一个工作负载使用 mTLS 向另一个工作负载发送请求时，Istio 会将流量重新路由到 sidecar 代理（Envoy）。 然后，sidecar Envoy 开始与服务器端的 Envoy 进行 mTLS 握手。在握","title":"mTLS"},{"content":"PeerAuthentication（对等认证）定义了流量将如何被隧道化（或不被隧道化）到 sidecar。\n示例 策略允许命名空间 foo 下所有工作负载的 mTLS 流量。\napiVersion:security.istio.io/v1beta1kind:PeerAuthenticationmetadata:name:defaultnamespace:foospec:mtls:mode:STRICT对于网格级别，根据你的 Istio 安装，将策略放在根命名空间。\n策略允许命名空间 foo 下的所有工作负载的 mTLS 和明文流量，但 finance 的工作负载需要 mTLS。 …","relpermalink":"/istio-handbook/config-security/peer-authentication/","summary":"PeerAuthentication（对等认证）定义了流量将如何被隧道化（或不被隧道化）到 sidecar。 示例 策略允许命名空间 foo 下所有工作负载的 mTLS 流量。 apiVersion:security.isti","title":"PeerAuthentication"},{"content":"Wasm 是一种可执行代码的可移植二进制格式，依赖于一个开放的标准。它允许开发人员用自己喜欢的编程语言编写，然后将代码编译成 Wasm 模块。\n   代码编译成 wasm  Wasm 模块与主机环境隔离，并在一个称为虚拟机（VM） 的内存安全沙盒中执行。Wasm 模块使用一个 API 与主机环境进行通信。\nWasm 的主要目标是在网页上实现高性能应用。例如，假设我们要用 Javascript 构建一个网页应用程序。我们可以用 Go（或其他语言）写一些，并将其编译成一个二进制文件，即 Wasm 模块。然后，我们可以在与 JavaScript 网页应用程序相同的沙盒中运行已编译的 Wasm 模块。\n最初，Wasm 被设计为在网络浏览器中运行。然而，我们可以将虚拟机嵌入到其他主机应用程序中，并执行它们。这就是 Envoy 的作用！\nEnvoy 嵌入了 V8 虚拟机的一个子集。V8 是一个用 C++ 编写的高性能 JavaScript 和 WebAssembly 引擎，它被用于 Chrome 和 Node.js 等。\n我们在本教程的前面提到，Envoy 使用多线程模式运行。这意味着有一个主线 …","relpermalink":"/envoy-handbook/extending-envoy/wasm/","summary":"Wasm 是一种可执行代码的可移植二进制格式，依赖于一个开放的标准。它允许开发人员用自己喜欢的编程语言编写，然后将代码编译成 Wasm 模块。 代码编译成 wasm Wasm 模块与主机环境隔离，并在一个称为虚拟机（VM） 的内存安全沙盒中","title":"WebAssembly（Wasm）"},{"content":"Envoy 通过查询文件或管理服务器来动态发现资源。概括地讲，对应的发现服务及其相应的 API 被称作 xDS。Envoy 通过订阅（subscription）方式来获取资源，如监控指定路径下的文件、启动 gRPC 流或轮询 REST-JSON URL。后两种方式会发送 DiscoveryRequest 请求消息，发现的对应资源则包含在响应消息 DiscoveryResponse 中。下面，我们将具体讨论每种订阅类型。\n文件订阅 发现动态资源的最简单方式就是将其保存于文件，并将路径配置在 ConfigSource 中的 path 参数中。Envoy 使用 inotify（Mac OS X 上为 kqueue）来监控文件的变化，在文件被更新时，Envoy 读取保存的 DiscoveryResponse 数据进行解析，数据格式可以为二进制 protobuf、JSON、YAML 和协议文本等。\n 译者注：core.ConfigSource 配置格式如下：\n { \u0026#34;path\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;api_config_source\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;ads\u0026#34;: \u0026#34;{...}\u0026#34; } 文件订阅 …","relpermalink":"/istio-handbook/data-plane/envoy-xds-protocol/","summary":"Envoy 通过查询文件或管理服务器来动态发现资源。概括地讲，对应的发现服务及其相应的 API 被称作 xDS。Envoy 通过订阅（subscription）方式来获取资源，如监控指定路径下的文件、启动 gRPC 流或轮询 REST-JSON URL","title":"xDS 协议解析"},{"content":"接下来，我们将部署可观测性、分布式追踪、数据可视化工具：\nkubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.10/samples/addons/prometheus.yaml kubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.10/samples/addons/grafana.yaml kubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.10/samples/addons/kiali.yaml kubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.10/samples/addons/extras/zipkin.yaml  如果你在安装 Kiali 的时候发现以下错误 No matches for kind …","relpermalink":"/istio-handbook/practice/observability/","summary":"接下来，我们将部署可观测性、分布式追踪、数据可视化工具： kubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.10/samples/addons/prometheus.yaml kubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.10/samples/addons/grafana.yaml kubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.10/samples/addons/kiali.yaml kubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.10/samples/addons/extras/zipkin.yaml 如果你在安装 Kiali 的时候发现以下错误 No matches for kind \"MonitoringDashboard\" in version \"monitoring.kiali.io/v1alpha1\" 请重新运行以上命令。 要从 Google Cloud Shell 打开仪表盘，我们可以运行 getmesh istioctl dash kiali 命","title":"部署可观测性工具"},{"content":"每当你遇到配置问题时，你可以使用这组步骤来浏览和解决问题。在第一部分，我们要检查配置是否有效。如果配置是有效的，下一步就是看看运行时是如何处理配置的，为此，你需要对 Envoy 配置有基本的了解。\n配置 1. 配置是否有效？\nIstio CLI 有一个叫 validate 的命令，我们可以用它来验证 YAML 配置。YAML 最常见的问题是缩进和数组符号相关的问题。 要验证一个配置，请将 YAML 文件传递给 validate 命令，像这样：\n$ istioctl validate -f myresource.yaml validation succeed 如果资源是无效的，CLI 会给我们一个详细的错误。例如，如果我们拼错了一个字段名：\nunknown field \u0026#34;worloadSelector\u0026#34; in v1alpha3.ServiceEntry 我们可以使用另一个命令istioctl analyze。使用这个命令，我们可以检测 Istio 配置的潜在问题。我们可以针对本地的一组配置文件或实时集群运行它。同时，寻找来自 istiod 的任何警告或错误。\n下面是该命令的一个输出样本， …","relpermalink":"/istio-handbook/troubleshooting/checklist/","summary":"每当你遇到配置问题时，你可以使用这组步骤来浏览和解决问题。在第一部分，我们要检查配置是否有效。如果配置是有效的，下一步就是看看运行时是如何处理配置的，为此，你需要对 Envoy 配置有基本的了解。 配置 1. 配置是否有","title":"调试清单"},{"content":"Envoy 中访问日志的另一个特点是可以指定过滤器，决定是否需要写入访问日志。例如，我们可以有一个访问日志过滤器，只记录 500 状态代码，只记录超过 5 秒的请求，等等。下表显示了支持的访问日志过滤器。\n   访问日志过滤器名称 描述     status_code_filter 对状态代码值进行过滤。   duration_filter 对总的请求持续时间进行过滤，单位为毫秒。   not_health_check_filter 对非健康检查请求的过滤。   traceable_filter 对可追踪的请求进行过滤。   runtime_filter 对请求进行随机抽样的过滤器。   and_filter 对过滤器列表中每个过滤器的结果进行逻辑 “和” 运算。过滤器是按顺序进行评估的。   or_filter 对过滤器列表中每个过滤器的结果进行逻辑 “或” 运算。过滤器是按顺序进行评估的。   header_filter 根据请求头的存在或值来过滤请求。   response_flag_filter 过滤那些收到设置了 Envoy 响应标志的响应的请求。 …","relpermalink":"/envoy-handbook/logging/access-log-filtering/","summary":"Envoy 中访问日志的另一个特点是可以指定过滤器，决定是否需要写入访问日志。例如，我们可以有一个访问日志过滤器，只记录 500 状态代码，只记录超过 5 秒的请求，等等。下表显示了支持的访问日志过滤器。 访问日志过滤器名称","title":"访问日志过滤"},{"content":"我们可以使用 VirtualService 资源在 Istio 服务网格中进行流量路由。通过 VirtualService，我们可以定义流量路由规则，并在客户端试图连接到服务时应用这些规则。例如向 dev.example.com 发送一个请求，最终到达目标服务。\n让我们看一下在集群中运行 customers 应用程序的两个版本（v1 和 v2）的例子。我们有两个 Kubernetes 部署，customers-v1 和 customers-v2。属于这些部署的 Pod 有一个标签 version：v1 或一个标签 version：v2 的设置。\n   路由到 Customers  我们想把 VirtualService 配置为将流量路由到应用程序的 V1 版本。70% 的传入流量应该被路由到 V1 版本。30% 的请求应该被发送到应用程序的 V2 版本。\n下面是上述情况下 VirtualService 资源的样子： …","relpermalink":"/istio-handbook/traffic-management/basic-routing/","summary":"我们可以使用 VirtualService 资源在 Istio 服务网格中进行流量路由。通过 VirtualService，我们可以定义流量路由规则，并在客户端试图连接到服务时应用这些规则。例如向 dev.example.com 发送一个请求，最终到达目标服务。 让我们看一下在","title":"基本路由"},{"content":"使用控制平面来更新 Envoy 比使用文件系统的配置更复杂。我们必须创建自己的控制平面，实现发现服务接口。这里有一个 xDS 服务器实现的简单例子。这个例子显示了如何实现不同的发现服务，并运行 Envoy 连接的 gRPC 服务器的实例来检索配置。\n   Envoy 的动态配置  Envoy 方面的动态配置与文件系统的配置类似。这一次，不同的是，我们提供了实现发现服务的 gRPC 服务器的位置。我们通过静态资源指定一个集群来做到这一点。\n...dynamic_resources:lds_config:resource_api_version:V3api_config_source:api_type:GRPCtransport_api_version:V3grpc_services:- envoy_grpc:cluster_name:xds_clustercds_config:resource_api_version:V3api_config_source:api_type:GRPCtransport_api_version:V3grpc_services:- …","relpermalink":"/envoy-handbook/dynamic-config/control-plane/","summary":"使用控制平面来更新 Envoy 比使用文件系统的配置更复杂。我们必须创建自己的控制平面，实现发现服务接口。这里有一个 xDS 服务器实现的简单例子。这个例子显示了如何实现不同的发现服务，并运行 Envoy 连接的 gRPC 服务器的实例来检索","title":"来自控制平面的动态配置"},{"content":"授权是对访问控制问题中访问控制部分的响应。一个（经过认证的）主体是否被允许对一个对象执行动作？用户 A 能否向服务 A 的路径 /hello 发送一个 GET 请求？\n请注意，尽管主体可以被认证，但它可能不被允许执行一个动作。你的公司 ID 卡可能是有效的、真实的，但我不能用它来进入另一家公司的办公室。如果我们继续之前的海关官员的比喻，我们可以说授权类似于你护照上的签证章。\n这就引出了下一个问题 —— 有认证而无授权（反之亦然）对我们没有什么好处。对于适当的访问控制，我们需要两者。让我给你举个例子：如果我们只认证主体而不授权他们，他们就可以做任何他们想做的事，对任何对象执行任何操作。相反，如果我们授权了一个请求，但我们没有认证它，我们就可以假装成其他人，再次对任何对象执行任何操作。\nIstio 允许我们使用 AuthorizationPolicy 资源在网格、命名空间和工作负载层面定义访问控制。AuthorizationPolicy 支持 DENY、ALLOW、AUDIT 和 CUSTOM 操作。\n每个 Envoy 代理实例都运行一个授权引擎，在运行时对请求进行授权。当请求到达代理时， …","relpermalink":"/istio-handbook/security/authz/","summary":"授权是对访问控制问题中访问控制部分的响应。一个（经过认证的）主体是否被允许对一个对象执行动作？用户 A 能否向服务 A 的路径 /hello 发送一个 GET 请求？ 请注意，尽管主体可以被认证，但它可能不被允许执行一个动作。你的公","title":"授权"},{"content":"管理接口的统计输出的主要端点是通过 /stats 端点访问的。这个输入通常是用来调试的。我们可以通过向 /stats 端点发送请求或从管理接口访问同一路径来访问该端点。\n该端点支持使用 filter 查询参数和正则表达式来过滤返回的统计资料。\n另一个过滤输出的维度是使用 usedonly 查询参数。当使用时，它将只输出 Envoy 更新过的统计数据。例如，至少增加过一次的计数器，至少改变过一次的仪表，以及至少增加过一次的直方图。\n默认情况下，统计信息是以 StatsD 格式写入的。每条统计信息都写在单独的一行中，统计信息的名称（例如，cluster_manager.active_clusters）后面是统计信息的值（例如，15）。\n例如：\n... cluster_manager.active_clusters。15 cluster_manager.cluster_added: 3 cluster_manager.cluster_modified:4 ... format 查询参数控制输出格式。设置为 json 将以 JSON 格式输出统计信息。如果我们想以编程方式访问和解析统计信息，通 …","relpermalink":"/envoy-handbook/admin-interface/statistics/","summary":"管理接口的统计输出的主要端点是通过 /stats 端点访问的。这个输入通常是用来调试的。我们可以通过向 /stats 端点发送请求或从管理接口访问同一路径来访问该端点。 该端点支持使用 filter 查询参数和正则表达式来过滤返回的统计资料。 另","title":"统计"},{"content":"要想说明为什么要使用服务网格，那就要从微服务架构说起，可以说服务网格很大程度上是一种新一代的微服务架构，它解决了微服务中网络层操控性、弹性、可视性的问题。\n微服务架构 开发人员经常将云原生应用程序分解为多个执行特定动作的服务。你可能有一个只处理客户的服务和另一个处理订单或付款的服务。所有这些服务都通过网络相互沟通。如果一个新的付款需要被处理，请求会被发送到付款服务。如果客户数据需要更新，请求会被发送到客户服务，等等。\n   微服务架构  这种类型的架构被称为微服务架构。这种架构有几个好处。你可以有多个较小的团队从事个别服务。这些团队可以灵活地选择他们的技术栈和语言，并且通常有独立部署和发布服务的自主权。这种机制得以运作得益于在其背后通信的网络。随着服务数量的增加，它们之间的通信和网络通信也在增加。服务和团队的数量使得监控和管理通信逻辑变得相当复杂。由于我们也知道网络是不可靠的，它们会失败，所有这些的结合使得微服务的管理和监控相当复杂。\n服务网格概述 服务网格被定义为一个专门的基础设施层，用于管理服务与服务之间的通信，使其可管理、可见、可控制。在某些版本的定义中，你可能还会听到服务网格如 …","relpermalink":"/istio-handbook/intro/why-service-mesh/","summary":"要想说明为什么要使用服务网格，那就要从微服务架构说起，可以说服务网格很大程度上是一种新一代的微服务架构，它解决了微服务中网络层操控性、弹性、可视性的问题。 微服务架构 开发人员经常将云原生应用程序分解为多","title":"为什么要使用服务网格？"},{"content":"第二种类型的健康检查被称为被动健康检查。异常点检测（outlier detection） 是一种被动的健康检查形式。说它 “被动” 是因为 Envoy 没有 “主动” 发送任何请求来确定端点的健康状况。相反，Envoy 观察不同端点的性能，以确定它们是否健康。如果端点被认为是不健康的，它们就会被移除或从健康负载均衡池中弹出。\n端点的性能是通过连续失败、时间成功率、延迟等来确定的。\n为了使异常点检测发挥作用，我们需要过滤器来报告错误、超时和重置。目前，有四个过滤器支持异常点检测。HTTP 路由器、TCP 代理、Redis 代理和 Thrift 代理。\n检测到的错误根据起源点分为两类。\n1. 来自外部的错误\n这些错误是针对事务的，发生在上游服务器上，是对收到的请求的回应。这些错误是在 Envoy 成功连接到上游主机后产生的。例如，端点响应的是 HTTP 500。\n2. 本地产生的错误\nEnvoy 产生这些错误是为了应对中断或阻止与上游主机通信的事件，例如超时、TCP 重置、无法连接到指定端口等。\n这些错误也取决于过滤器的类型。例如，HTTP 路由器过滤器可以检测两种错误。相反，TCP 代理 …","relpermalink":"/envoy-handbook/cluster/outlier-detection/","summary":"第二种类型的健康检查被称为被动健康检查。异常点检测（outlier detection） 是一种被动的健康检查形式。说它 “被动” 是因为 Envoy 没有 “主动” 发送任何","title":"异常点检测"},{"content":"如果你听说过服务网格，并尝试过 Istio，你可能有以下问题。\n 为什么 Istio 要在 Kubernetes 上运行？ Kubernetes 和服务网格在云原生应用架构中分别扮演什么角色？ Istio 扩展了 Kubernetes 的哪些方面？它解决了哪些问题？ Kubernetes、Envoy 和 Istio 之间是什么关系？  本文将带大家了解 Kubernetes 和 Istio 的内部工作原理。此外，我会介绍 Kubernetes 中的负载均衡方法，并解释为什么有了 Kubernetes 后还需要 Istio。\nKubernetes 本质上是通过声明式配置来实现应用生命周期管理，而服务网格本质上是提供应用间的流量、安全管理和可观测性。如果你已经使用 Kubernetes 搭建了一个稳定的应用平台，那么如何设置服务间调用的负载均衡和流量控制？是否有这样一个通用的工具或者说平台（非 SDK），可以实现？这就需要用到服务网格了。\nEnvoy 引入了 xDS 协议，这个协议得到了各种开源软件的支持，比如 Istio、MOSN 等。Envoy 将 xDS 贡献给服务网格或云原生基础设 …","relpermalink":"/istio-handbook/intro/cloud-native-application-network/","summary":"如果你听说过服务网格，并尝试过 Istio，你可能有以下问题。 为什么 Istio 要在 Kubernetes 上运行？ Kubernetes 和服务网格在云原生应用架构中分别扮演什么角色？ Istio 扩展了 Kubernetes 的哪些方面？它解决了哪些问题？ Kubernetes、Envo","title":"云原生应用网络"},{"content":"将应用程序的功能划分为单独的进程运行在同一个最小调度单元中（例如 Kubernetes 中的 Pod）可以被视为 sidecar 模式。如下图所示，sidecar 模式允许您在应用程序旁边添加更多功能，而无需额外第三方组件配置或修改应用程序代码。\n   Sidecar 模式示意图  就像连接了 Sidecar 的三轮摩托车一样，在软件架构中， Sidecar 连接到父应用并且为其添加扩展或者增强功能。Sidecar 应用与主应用程序松散耦合。它可以屏蔽不同编程语言的差异，统一实现微服务的可观测性、监控、日志记录、配置、断路器等功能。\n使用 Sidecar 模式的优势 使用 sidecar 模式部署服务网格时，无需在节点上运行代理，但是集群中将运行多个相同的 sidecar 副本。在 sidecar 部署方式中，每个应用的容器旁都会部署一个伴生容器，这个容器称之为 sidecar 容器。Sidecar 接管进出应用容器的所有流量。在 Kubernetes 的 Pod 中，在原有的应用容器旁边注入一个 Sidecar 容器，两个容器共享存储、网络等资源， …","relpermalink":"/cloud-native-handbook/kubernetes/sidecar-pattern/","summary":"将应用程序的功能划分为单独的进程运行在同一个最小调度单元中（例如 Kubernetes 中的 Pod）可以被视为 sidecar 模式。如下图所示，sidecar 模式允许您在应用程序旁边添加更多功能，而无需额外第三方组件配置或修改应用程序代","title":"Sidecar 模式"},{"content":"本文将为您介绍 DaemonSet 的基本概念。\n什么是 DaemonSet？ DaemonSet 确保全部（或者一些）Node 上运行一个 Pod 的副本。当有 Node 加入集群时，也会为他们新增一个 Pod 。当有 Node 从集群移除时，这些 Pod 也会被回收。删除 DaemonSet 将会删除它创建的所有 Pod。\n使用 DaemonSet 的一些典型用法：\n 运行集群存储 daemon，例如在每个 Node 上运行 glusterd、ceph。 在每个 Node 上运行日志收集 daemon，例如fluentd、logstash。 在每个 Node 上运行监控 daemon，例如 Prometheus Node Exporter、collectd、Datadog 代理、New Relic 代理，或 Ganglia gmond。  一个简单的用法是，在所有的 Node 上都存在一个 DaemonSet，将被作为每种类型的 daemon 使用。 一个稍微复杂的用法可能是，对单独的每种类型的 daemon 使用多个 DaemonSet，但具有不同的标志，和/或对不同硬件类型具有 …","relpermalink":"/kubernetes-handbook/controllers/daemonset/","summary":"本文将为您介绍 DaemonSet 的基本概念。 什么是 DaemonSet？ DaemonSet 确保全部（或者一些）Node 上运行一个 Pod 的副本。当有 Node 加入集群时，也会为他们新增一个 Pod 。当有 Node 从集群移除时，这些 Pod 也会被回收。删除 DaemonSet 将会删除它创","title":"DaemonSet"},{"content":"ReplicationController 用来确保容器应用的副本数始终保持在用户定义的副本数，即如果有容器异常退出，会自动创建新的 Pod 来替代；而如果异常多出来的容器也会自动回收。\n在新版本的 Kubernetes 中建议使用 ReplicaSet 来取代 ReplicationController。ReplicaSet 跟 ReplicationController 没有本质的不同，只是名字不一样，并且 ReplicaSet 支持集合式的 selector。\n虽然 ReplicaSet 可以独立使用，但一般还是建议使用 Deployment 来自动管理 ReplicaSet，这样就无需担心跟其他机制的不兼容问题（比如 ReplicaSet 不支持 rolling-update 但 Deployment 支持）。\nReplicaSet 示例：\napiVersion:extensions/v1beta1kind:ReplicaSetmetadata:name:frontend# these labels can be applied automatically# from the …","relpermalink":"/kubernetes-handbook/controllers/replicaset/","summary":"ReplicationController 用来确保容器应用的副本数始终保持在用户定义的副本数，即如果有容器异常退出，会自动创建新的 Pod 来替代；而如果异常多出来的容器也会自动回收。 在新版本的 Kubernetes 中建议使用 ReplicaSet 来取代 ReplicationContro","title":"ReplicationController 和 ReplicaSet"},{"content":"Job 负责批处理任务，即仅执行一次的任务，它保证批处理任务的一个或多个 Pod 成功结束。\nJob Spec 格式  spec.template 格式同 Pod RestartPolicy 仅支持 Never 或 OnFailure 单个 Pod 时，默认 Pod 成功运行后 Job 即结束 .spec.completions 标志 Job 结束需要成功运行的 Pod 个数，默认为 1 .spec.parallelism 标志并行运行的 Pod 的个数，默认为 1 spec.activeDeadlineSeconds 标志失败 Pod 的重试最大时间，超过这个时间不会继续重试  一个简单的例子：\napiVersion:batch/v1kind:Jobmetadata:name:pispec:template:metadata:name:pispec:containers:- name:piimage:perlcommand:[\u0026#34;perl\u0026#34;,\u0026#34;-Mbignum=bpi\u0026#34;,\u0026#34;-wle\u0026#34;,\u0026#34;print bpi(2000)\u0026#34;]restartPolicy:Never$ kubectl …","relpermalink":"/kubernetes-handbook/controllers/job/","summary":"Job 负责批处理任务，即仅执行一次的任务，它保证批处理任务的一个或多个 Pod 成功结束。 Job Spec 格式 spec.template 格式同 Pod RestartPolicy 仅支持 Never 或 OnFailure 单个 Pod 时，默认 Pod 成功运行后 Job 即结束 .spec.completions 标志 Job 结束需要成功运行的 Pod 个数，默认为 1 .spec.parallelism 标志并行运行","title":"Job"},{"content":"Cron Job 管理基于时间的 Job，即：\n 在给定时间点只运行一次 周期性地在给定时间点运行  一个 CronJob 对象类似于 crontab （cron table）文件中的一行。它根据指定的预定计划周期性地运行一个 Job，格式可以参考 Cron 。\n前提条件 当前使用的 Kubernetes 集群，版本 \u0026gt;= 1.8（对 CronJob）。对于先前版本的集群，版本 \u0026lt; 1.8，启动 API Server（参考 为集群开启或关闭 API 版本 获取更多信息）时，通过传递选项 --runtime-config=batch/v2alpha1=true 可以开启 batch/v2alpha1 API。\n典型的用法如下所示：\n 在给定的时间点调度 Job 运行 创建周期性运行的 Job，例如：数据库备份、发送邮件。  CronJob Spec   .spec.schedule：调度，必需字段，指定任务运行周期，格式同 Cron\n  .spec.jobTemplate：Job 模板，必需字段，指定需要运行的任务，格式同 Job …","relpermalink":"/kubernetes-handbook/controllers/cronjob/","summary":"Cron Job 管理基于时间的 Job，即： 在给定时间点只运行一次 周期性地在给定时间点运行 一个 CronJob 对象类似于 crontab （cron table）文件中的一行。它根据指定的预定计划周期性地运行一个 Job，格式可以参考 Cron 。 前提条件 当","title":"CronJob"},{"content":"为了使 Ingress 正常工作，集群中必须运行 Ingress controller。 这与其他类型的控制器不同，其他类型的控制器通常作为 kube-controller-manager 二进制文件的一部分运行，在集群启动时自动启动。 你需要选择最适合自己集群的 Ingress controller 或者自己实现一个。\nKubernetes 社区和众多厂商开发了大量的 Ingress Controller，你可以在 这里 找到。\n使用多个 Ingress 控制器 你可以使用 IngressClass 在集群中部署任意数量的 Ingress 控制器。 请注意你的 Ingress 类资源的 .metadata.name 字段。 当你创建 Ingress 时，你需要用此字段的值来设置 Ingress 对象的 ingressClassName 字段（请参考 IngressSpec v1 reference）。 ingressClassName 是之前的注解做法的替代。\n如果你不为 Ingress 指定 IngressClass，并且你的集群中只有一个 IngressClass 被标记为了集群 …","relpermalink":"/kubernetes-handbook/controllers/ingress-controller/","summary":"为了使 Ingress 正常工作，集群中必须运行 Ingress controller。 这与其他类型的控制器不同，其他类型的控制器通常作为 kube-controller-manager 二进制文件的一部分运行，在集群启动时自动启动。 你需要选择最适合自己集群的 Ingress controller 或者自己实现一个","title":"Ingress 控制器"},{"content":"Kubernetes 中不仅支持 CPU、内存为指标的 HPA，还支持自定义指标的 HPA，例如 QPS。\n设置自定义指标 Kubernetes 1.6\n 在 Kubernetes1.6 集群中配置自定义指标的 HPA 的说明已废弃。\n 在设置定义指标 HPA 之前需要先进行如下配置：\n  将 heapster 的启动参数 --api-server 设置为 true\n  启用 custom metric API\n  将 kube-controller-manager 的启动参数中 --horizontal-pod-autoscaler-use-rest-clients 设置为 true，并指定 --master 为 API server 地址，如 --master=http://172.20.0.113:8080\n  在 Kubernetes1.5 以前很容易设置，参考 1.6 以前版本的 kubernetes 中开启自定义 HPA，而在 1.6 中因为取消了原来的 annotation 方式设置 custom metric，只能通过 API server …","relpermalink":"/kubernetes-handbook/controllers/hpa/custom-metrics-hpa/","summary":"Kubernetes 中不仅支持 CPU、内存为指标的 HPA，还支持自定义指标的 HPA，例如 QPS。 设置自定义指标 Kubernetes 1.6 在 Kubernetes1.6 集群中配置自定义指标的 HPA 的说明已废弃。 在设置定义指标 HPA 之前需要先进行如下配置： 将 heapster 的启动参数 --api-server 设置为","title":"自定义指标 HPA"},{"content":"准入控制器（Admission Controller）位于 API Server 中，在对象被持久化之前，准入控制器拦截对 API Server 的请求，一般用来做身份验证和授权。其中包含两个特殊的控制器：MutatingAdmissionWebhook 和 ValidatingAdmissionWebhook。分别作为配置的变异和验证准入控制 webhook。\n准入控制器包括以下两种：\n 变更（Mutating）准入控制：修改请求的对象 验证（Validating）准入控制：验证请求的对象  准入控制器是在 API Server 的启动参数重配置的。一个准入控制器可能属于以上两者中的一种，也可能两者都属于。当请求到达 API Server 的时候首先执行变更准入控制，然后再执行验证准入控制。\n我们在部署 Kubernetes 集群的时候都会默认开启一系列准入控制器，如果没有设置这些准入控制器的话可以说你的 Kubernetes 集群就是在裸奔，应该只有集群管理员可以修改集群的准入控制器。\n例如我会默认开启如下的准入控制器。 …","relpermalink":"/kubernetes-handbook/controllers/admission-controller/","summary":"准入控制器（Admission Controller）位于 API Server 中，在对象被持久化之前，准入控制器拦截对 API Server 的请求，一般用来做身份验证和授权。其中包含两个特殊的控制器：MutatingAdmissionW","title":"准入控制器（Admission Controller）"},{"content":"Kubernetes Pod 是有生命周期的，它们可以被创建，也可以被销毁，然而一旦被销毁生命就永远结束。通过 ReplicationController 能够动态地创建和销毁 Pod。 每个 Pod 都会获取它自己的 IP 地址，即使这些 IP 地址不总是稳定可依赖的。这会导致一个问题：在 Kubernetes 集群中，如果一组 Pod（称为 backend）为其它 Pod （称为 frontend）提供服务，那么 frontend Pod 该如何发现和连接哪些 backend Pod 呢？\n关于 Service Kubernetes Service 定义了这样一种抽象：Pod 的逻辑分组，一种可以访问它们的策略 —— 通常称为微服务。这一组 Pod 能够被 Service 访问到，通常是通过 Label Selector（查看下面了解，为什么可能需要没有 selector 的 Service）实现的。\n举个例子，假设有一个用于图片处理的运行了三个副本的 backend。这些副本是可互换的 —— frontend 不需要关心它们调用了哪个 backend 副本。 …","relpermalink":"/kubernetes-handbook/service-discovery/service/","summary":"Kubernetes Pod 是有生命周期的，它们可以被创建，也可以被销毁，然而一旦被销毁生命就永远结束。通过 ReplicationController 能够动态地创建和销毁 Pod。 每个 Pod 都会获取它自己的 IP 地址，即使这些 IP 地址不总是稳定可依赖的。这会导致一个问题：在 Kubernetes","title":"Service"},{"content":"拓扑感知路由指的是客户端对一个服务的访问流量，可以根据这个服务的端点拓扑，优先路由到与该客户端在同一个节点或者可用区的端点上的路由行为。\n先决条件 为了开启服务感知路由，你需要：\n 开启 TopologyAwareHints 智能感知提示门控 开启 EndpointSlice 控制器 安装 kube-proxy  端点切片 我们知道 Endpoint 通常情况下是由 Service 资源自动创建和管理的，但是随着 Kubernetes 集群的规模越来越大和管理的服务越来越多，Endpoint API 的局限性变得越来越明显。 端点切片（EndpointSlices）提供了一种简单的方法来跟踪 Kubernetes 集群中的网络端点。它们为 Endpoint 提供了一种可伸缩和可拓展的替代方案，同时还可以被用到拓扑感知路由中。\nEndpointSlices 示例如下： …","relpermalink":"/kubernetes-handbook/service-discovery/topology-aware-routing/","summary":"拓扑感知路由指的是客户端对一个服务的访问流量，可以根据这个服务的端点拓扑，优先路由到与该客户端在同一个节点或者可用区的端点上的路由行为。 先决条件 为了开启服务感知路由，你需要： 开启 TopologyAwareHints 智能感知提示门控 开启","title":"拓扑感知路由"},{"content":"Bookinfo 示例是 Istio 官方为了演示 Istio 功能而开发的一个示例应用，该应用有如下特点：\n 使用微服务方式开发，共有四个微服务 多语言应用，使用了 Java、Python、Ruby 和 NodeJs 语言 为了演示流量管理的高级功能，有的服务同时推出了多个版本  Bookinfo 应用部署架构 以下为 Istio 官方提供的该应用的架构图。\n   Istio 的 Bookinfo 示例应用架构图  Bookinfo 应用分为四个单独的微服务，其中每个微服务的部署的结构中都注入了一个 Sidecar：\n productpage ：productpage 微服务会调用 details 和 reviews 两个微服务，用来生成页面。 details ：这个微服务包含了书籍的信息。 reviews ：这个微服务包含了书籍相关的评论。它还会调用 ratings 微服务。 ratings ：ratings 微服务中包含了由书籍评价组成的评级信息。  reviews 微服务有 3 个版本：\n v1 版本不会调用 ratings 服务。 v2 版本会调用 ratings 服务，并使 …","relpermalink":"/istio-handbook/setup/bookinfo-sample/","summary":"Bookinfo 示例是 Istio 官方为了演示 Istio 功能而开发的一个示例应用，该应用有如下特点： 使用微服务方式开发，共有四个微服务 多语言应用，使用了 Java、Python、Ruby 和 NodeJs 语言 为了演示流量管理的高级功能，有的服务同时","title":"Bookinfo 示例"},{"content":"到目前为止，我们已经谈到了向 Envoy 发送请求时产生的日志。然而，Envoy 也会在启动时和执行过程中产生日志。\n我们可以在每次运行 Envoy 时看到 Envoy 组件的日志。\n... [2021-11-03 17:22:43.361][1678][info][main] [source/server/server.cc:368] initializing epoch 0 (base id=0, hot restart version=11.104) [2021-11-03 17:22:43.361][1678][info][main] [source/server/server.cc:370] statically linked extensions: [2021-11-03 17:22:43.361][1678][info][main] [source/server/server.cc:372] envoy.filters.network: envoy.client_ssl_auth, envoy.echo, envoy.ext_authz, …","relpermalink":"/envoy-handbook/logging/envoy-component-logs/","summary":"到目前为止，我们已经谈到了向 Envoy 发送请求时产生的日志。然而，Envoy 也会在启动时和执行过程中产生日志。 我们可以在每次运行 Envoy 时看到 Envoy 组件的日志。 ... [2021-11-03 17:22:43.361][1678][info][main] [source/server/server.cc:368] initializing epoch 0 (base id=0, hot restart version=11.104) [2021-11-03 17:22:43.361][1678][info][main] [source/server/server.cc:370] statically linked extensions: [2021-11-03 17:22:43.361][1678][info][main] [source/server/server.cc:372] envoy.filters.network: envoy.client_ssl_auth, envoy.echo, envoy.ext_authz, envoy.filters.network.client_ssl_auth ... 组件","title":"Envoy 组件日志"},{"content":"Istio 是目前最流行的服务网格的开源实现，它的架构目前来说有两种，一种是 Sidecar 模式，这也是 Istio 最传统的部署架构，另一种是 Proxyless 模式。\n   Istio 的部署架构  Sidecar 模式 Sidecar 模式是 Istio 开源之初就在使用的模式，这种模式将应用程序的功能划分为单独的进程运行在同一个最小调度单元中，比如 Kubernetes 的 Pod 中。这种架构分为两个部分：控制平面和数据平面，控制平面是一个单体应用 Istiod，数据平面是由注入在每个 Pod 中的 Envoy 代理组成。你可以在 Sidecar 中添加更多功能，而不需要修改应用程序代码。这也是服务网格最大的一个卖点之一，将原先的应用程序 SDK 中的功能转移到了 Sidecar 中，这样开发者就可以专注于业务逻辑，而 sidecar 就交由运维来处理。\n   Sidecar 模式  我们在看下应用程序 Pod 中的结构。Pod 中包含应用容器和 Sidecar 容器，sidecar 容器与控制平面通信，获取该 Pod 上的所有代理配置，其中还有个 Init 容器，它是在 …","relpermalink":"/istio-handbook/concepts/istio-architecture/","summary":"Istio 是目前最流行的服务网格的开源实现，它的架构目前来说有两种，一种是 Sidecar 模式，这也是 Istio 最传统的部署架构，另一种是 Proxyless 模式。 Istio 的部署架构 Sidecar 模式 Sidecar 模式是 Istio 开源之初就在使用的模式，这种模式将应用程序的功能划分为单","title":"Istio 架构解析"},{"content":"本文将概述如何配置 Istio 的开发环境及编译和生成二进制文件和 Kubernetes 的 YAML 文件，更高级的测试、格式规范、原型和参考文档编写等请参考 Istio Dev Guide。\n依赖环境 Istio 开发环境依赖以下软件：\n Docker：测试和运行时 Go 1.11：程序开发 fpm 包构建工具：用来打包 Kubernetes 1.7.3+  设置环境变量 在编译过程中需要依赖以下环境变量，请根据你自己的\nexport ISTIO=$GOPATH/src/istio.io # DockerHub 的用户名 USER=jimmysong export HUB=\u0026#34;docker.io/$USER\u0026#34; # Docker 镜像的 tag，这里为了方便指定成了固定值，也可以使用 install/updateVersion.sh 来生成 tag export TAG=$USER # GitHub 的用户名 export GITHUB_USER=rootsongjc # 指定 Kubernetes 集群的配置文件地址 export …","relpermalink":"/istio-handbook/troubleshooting/istio-dev-env/","summary":"本文将概述如何配置 Istio 的开发环境及编译和生成二进制文件和 Kubernetes 的 YAML 文件，更高级的测试、格式规范、原型和参考文档编写等请参考 Istio Dev Guide。 依赖环境 Istio 开发环境依赖以下软件： Docker：测试和运行时 Go 1.11","title":"Istio 开发环境配置"},{"content":"用于认证的 JSON Web Token（JWT）令牌格式，由 RFC 7519 定义。参见 OAuth 2.0 和 OIDC 1.0，了解在整个认证流程中如何使用。\n示例 JWT 的规格是由 https://example.com 签发，受众要求必须是 bookstore_android.apps.example.com 或 bookstore_web.apps.example.com。该令牌应呈现在 Authorization header（默认）。Json 网络密钥集（JWKS）将按照 OpenID Connect 协议被发现。\nissuer:https://example.comaudiences:- bookstore_android.apps.example.combookstore_web.apps.example.com这个例子在非默认位置（x-goog-iap-jwt-assertion header）指定了令牌。它还定义了 URI 来明确获取 JWKS。 …","relpermalink":"/istio-handbook/config-security/jwt/","summary":"用于认证的 JSON Web Token（JWT）令牌格式，由 RFC 7519 定义。参见 OAuth 2.0 和 OIDC 1.0，了解在整个认证流程中如何使用。 示例 JWT 的规格是由 https://example.com 签发，受众要求必须是 bookstore_android.apps.example.com 或 bookstore_web.apps.example","title":"JWTRule"},{"content":"Listener 发现服务（LDS）是一个可选的 API，Envoy 将调用它来动态获取 Listener。Envoy 将协调 API 响应，并根据需要添加、修改或删除已知的 Listener。\nListener 更新的语义如下：\n 每个 Listener 必须有一个独特的名字。如果没有提供名称，Envoy 将创建一个 UUID。要动态更新的 Listener ，管理服务必须提供 Listener 的唯一名称。 当一个 Listener 被添加，在参与连接处理之前，会先进入“预热”阶段。例如，如果 Listener 引用 RDS 配置，那么在 Listener 迁移到 “active” 之前，将会解析并提取该配置。 Listener 一旦创建，实际上就会保持不变。因此，更新 Listener 时，会创建一个全新的 Listener （使用相同的监听套接字）。新增加的 Listener 都会通过上面所描述的相同“预热”过程。 当更新或删除 Listener 时，旧的 Listener 将被置于 “draining（逐出）” 状态，就像整个服务重新启动时一样。Listener 移除之后， …","relpermalink":"/istio-handbook/data-plane/envoy-lds/","summary":"Listener 发现服务（LDS）是一个可选的 API，Envoy 将调用它来动态获取 Listener。Envoy 将协调 API 响应，并根据需要添加、修改或删除已知的 Listener。 Listener 更新的语义如下： 每个 Listener 必须有一个独特的","title":"LDS（监听器发现服务）"},{"content":"目的地指的是不同的子集（subset）或服务版本。通过子集，我们可以识别应用程序的不同变体。在我们的例子中，我们有两个子集，v1 和 v2，它们对应于我们 customer 服务的两个不同版本。每个子集都使用键/值对（标签）的组合来确定哪些 Pod 要包含在子集中。我们可以在一个名为 DestinationRule 的资源类型中声明子集。\n下面是定义了两个子集的 DestinationRule 资源的样子。\napiVersion:networking.istio.io/v1alpha3kind:DestinationRulemetadata:name:customers-destinationspec:host:customers.default.svc.cluster.localsubsets:- name:v1labels:version:v1- name:v2labels:version:v2让我们看看我们可以在 DestinationRule 中设置的流量策略。\nDestinationRule 中的流量策略 通过 DestinationRule，我们可以定义设置，如负载均衡配 …","relpermalink":"/istio-handbook/traffic-management/subset-and-destinationrule/","summary":"目的地指的是不同的子集（subset）或服务版本。通过子集，我们可以识别应用程序的不同变体。在我们的例子中，我们有两个子集，v1 和 v2，它们对应于我们 customer 服务的两个不同版本。每个子集都使用键/值对（标签","title":"Subset 和 DestinationRule"},{"content":"WorkloadEntry 使运维能够描述单个非 Kubernetes 工作负载的属性，如虚拟机或裸机服务器，以加入网格。WorkloadEntry 必须同时配置 Istio ServiceEntry，通过适当的标签选择工作负载，并提供 MESH_INTERNAL 服务的服务定义（主机名、端口属性等）。ServiceEntry 对象可以根据服务条目中指定的标签选择器来选择多个工作负载条目以及 Kubernetes pod。\n当工作负载连接到 istiod 时，自定义资源中的状态字段将被更新，以表明工作负载的健康状况以及其他细节，类似于 Kubernetes 更新 pod 状态的方式。\n示例 下面的例子声明了一个 WorkloadEntry 条目，代表 details.bookinfo.com 服务的一个虚拟机。这个虚拟机安装了 sidecar，并使用 details-legacy 服务账户进行引导。该服务通过 80 端口暴露给网格中的应用程序。通往该服务的 HTTP 流量被 Istio mTLS 封装，并被发送到目标端口 8080 的虚拟机上的 sidecar，后者又将其转发到同一端口 …","relpermalink":"/istio-handbook/config-networking/workload-entry/","summary":"WorkloadEntry 使运维能够描述单个非 Kubernetes 工作负载的属性，如虚拟机或裸机服务器，以加入网格。WorkloadEntry 必须同时配置 Istio ServiceEntry，通过适当的标签选择工作负载，并提供 MESH_INTERNAL 服务的服务定义（主机名、","title":"WorkloadEntry"},{"content":"分布式追踪是一种监测微服务应用程序的方法。使用分布式追踪，我们可以在请求通过被监控系统的不同部分时追踪它们。\n每当一个请求进入服务网格时，Envoy 都会生成一个唯一的请求 ID 和追踪信息，并将其作为 HTTP 头的一部分来存储。任何应用程序都可以将这些头信息转发给它所调用的其他服务，以便在系统中创建一个完整的追踪。\n分布式追踪是一个跨度（span）的集合。当请求流经不同的系统组件时，每个组件都会贡献一个跨度。每个跨度都有一个名称，开始和结束的时间戳，一组称为标签（tag）和日志（log）的键值对，以及一个跨度上下文。\n标签被应用于整个跨度，并用于查询和过滤。下面是我们在使用 Zipkin 时将看到的几个标签的例子。注意，其中有些是通用的，有些是 Istio 特有的。\n istio.mesh_id istio.canonical_service upstream_cluster http.url http.status_code zone  单个跨度与识别跨度、父跨度、追踪 ID 的上下文头一起被发送到一个叫做采集器的组件。采集器对数据进行验证、索引和存储。\n当请求流经 Envoy  …","relpermalink":"/istio-handbook/observability/zipkin/","summary":"分布式追踪是一种监测微服务应用程序的方法。使用分布式追踪，我们可以在请求通过被监控系统的不同部分时追踪它们。 每当一个请求进入服务网格时，Envoy 都会生成一个唯一的请求 ID 和追踪信息，并将其作为 HTTP 头的一","title":"Zipkin"},{"content":"断路是一种重要的模式，可以帮助服务的弹性。断路模式通过控制和管理对故障服务的访问来防止额外的故障。它允许我们快速失败，并尽快向下游反馈。\n让我们看一个定义断路的片段。\n...clusters:- name:my_cluster_name...circuit_breakers:thresholds:- priority:DEFAULTmax_connections:1000- priority:HIGHmax_requests:2000...我们可以为每条路由的优先级分别配置断路器的阈值。例如，较高优先级的路由应该有比默认优先级更高的阈值。如果超过了任何阈值，断路器就会断开，下游主机就会收到 HTTP 503 响应。\n我们可以用多种选项来配置断路器。\n1. 最大连接数（max_connections）\n指定 Envoy 与集群中所有端点的最大连接数。如果超过这个数字，断路器会断开，并增加集群的 upstream_cx_overflow 指标。默认值是 1024。\n2. 最大的排队请求（max_pending_requests）\n指定在等待就绪的连接池连接时被排队的最大请求数。当超过该阈值 …","relpermalink":"/envoy-handbook/cluster/circuit-breakers/","summary":"断路是一种重要的模式，可以帮助服务的弹性。断路模式通过控制和管理对故障服务的访问来防止额外的故障。它允许我们快速失败，并尽快向下游反馈。 让我们看一个定义断路的片段。 ...clusters:- name:my_cluster_name...circuit_breakers:thresholds:- priority:DEFAULTmax_connections:1000- priority:HIGHma","title":"断路器"},{"content":"我们已经建立了一个新的 Docker 镜像，它使用了与当前运行的前端服务不同的标头。让我们看看如何部署所需的资源并将一定比例的流量路由到不同的前端服务版本。\n在我们创建任何资源之前，让我们删除现有的前端部署（kubectl delete deploy frontend），并创建一个版本标签设置为 original 。\napiVersion:apps/v1kind:Deploymentmetadata:name:frontendspec:selector:matchLabels:app:frontendversion:originaltemplate:metadata:labels:app:frontendversion:originalannotations:sidecar.istio.io/rewriteAppHTTPProbers:\u0026#34;true\u0026#34;spec:containers:- name:serverimage:gcr.io/google-samples/microservices-demo/frontend:v0.2.1ports:- …","relpermalink":"/istio-handbook/practice/traffic-routing/","summary":"我们已经建立了一个新的 Docker 镜像，它使用了与当前运行的前端服务不同的标头。让我们看看如何部署所需的资源并将一定比例的流量路由到不同的前端服务版本。 在我们创建任何资源之前，让我们删除现有的前端部署（kube","title":"路由流量"},{"content":"/logging 端点启用或禁用特定组件或所有记录器的不同日志级别。\n要列出所有的记录器，我们可以向 /logging 端点发送一个 POST 请求。\n$ curl -X POST localhost:9901/logging active loggers: admin: info alternate_protocols_cache: info aws: info assert: info backtrace: info cache_filter: info client: info config: info ... 输出将包含记录器的名称和每个记录器的日志级别。要改变所有活动日志记录器的日志级别，我们可以使用 level 参数。例如，我们可以运行下面的程序，将所有日志记录器的日志记录级别改为 debug。\n$ curl -X POST localhost:9901/logging?level=debug active loggers: admin: debug alternate_protocols_cache: debug aws: debug assert: debug …","relpermalink":"/envoy-handbook/admin-interface/logging/","summary":"/logging 端点启用或禁用特定组件或所有记录器的不同日志级别。 要列出所有的记录器，我们可以向 /logging 端点发送一个 POST 请求。 $ curl -X POST localhost:9901/logging active loggers: admin: info alternate_protocols_cache: info aws: info assert: info backtrace: info cache_filter: info client: info config: info ... 输出将包含记录器的名称和每个记录器的日志级别。要","title":"日志"},{"content":"我们将在这个实验中创建一个动态的 Envoy 配置，并通过单独的配置文件配置监听器、集群、路由和端点。\n让我们从最小的 Envoy 配置开始。\nnode:cluster:cluster-1id:envoy-instance-1dynamic_resources:lds_config:path:./lds.yamlcds_config:path:./cds.yamladmin:address:socket_address:address:127.0.0.1port_value:9901access_log:- name:envoy.access_loggers.filetyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.access_loggers.stream.v3.StdoutAccessLog将上述 YAML 保存到 envoy-proxy-1.yaml 文件。我们还需要创建空的（暂时的）cds.yaml 和 lds.yaml 文件。\ntouch {cds,lds}.yaml 我们现在可以用这个配置来运行 Envoy …","relpermalink":"/envoy-handbook/dynamic-config/lab8/","summary":"我们将在这个实验中创建一个动态的 Envoy 配置，并通过单独的配置文件配置监听器、集群、路由和端点。 让我们从最小的 Envoy 配置开始。 node:cluster:cluster-1id:envoy-instance-1dynamic_resources:lds_config:path:./lds.yamlcds_config:path:./cds.yamladmin:address:socket_address:address:127.0.0.1port_value:9901access_log:- name:envoy.access_loggers.filetyped_config:\"@type\": type.googleapis.com/envoy.extensions.acc","title":"实验 8：来自文件系统的动态配置"},{"content":"在这个实验中，我们将编写一个 Lua 脚本，为响应头添加一个头，并使用一个文件中定义的全局脚本。\n我们将创建一个 Envoy 配置和一个 Lua 脚本，在响应句柄上添加一个头。由于我们不会使用请求路径，所以我们不需要定义 envoy_on_request 函数。响应函数看起来像这样。\nfunction envoy_on_response(response_handle) response_handle:headers():add(\u0026#34;hello\u0026#34;, \u0026#34;world\u0026#34;) end 我们在从 headers() 函数返回的 header 对象上调用 add(\u0026lt;header-name\u0026gt;, \u0026lt;header-value\u0026gt;) 函数。\n让我们在 Envoy 配置中内联定义这个脚本。为了简化配置，我们将使用 direct_response，而不是集群。\nstatic_resources:listeners:- name:mainaddress:socket_address:address:0.0.0.0port_value:10000filter_chains:- filters:- …","relpermalink":"/envoy-handbook/extending-envoy/lab16/","summary":"在这个实验中，我们将编写一个 Lua 脚本，为响应头添加一个头，并使用一个文件中定义的全局脚本。 我们将创建一个 Envoy 配置和一个 Lua 脚本，在响应句柄上添加一个头。由于我们不会使用请求路径，所以我们不需要定义 envoy_on_request 函数。响","title":"实验16：使用 Lua 脚本扩展 Envoy"},{"content":"原始目的地过滤器（envoy.filters.listener.original_dst）会读取 SO_ORIGINAL_DST 套接字选项。当一个连接被 iptables REDIRECT 或 TPROXY 目标（如果transparent选项被设置）重定向时，这个选项被设置。该过滤器可用于与 ORIGINAL_DST 类型的集群连接。\n当使用 ORIGINAL_DST 集群类型时，请求会被转发到由重定向元数据寻址的上游主机，而不做任何主机发现。因此，在集群中定义任何端点都是没有意义的，因为端点是从原始数据包中提取的，并不是由负载均衡器选择。\n我们可以将 Envoy 作为一个通用代理，使用这种集群类型将所有请求转发到原始目的地。\n要使用 ORIGINAL_DST 集群，流量需要通过 iptables REDIRECT 或 TPROXY 目标到达 Envoy。\n...listener_filters:- name:envoy.filters.listener.original_dsttyped_config:\u0026#34;@type\u0026#34;: …","relpermalink":"/envoy-handbook/listener/original-destination-listener-filter/","summary":"原始目的地过滤器（envoy.filters.listener.original_dst）会读取 SO_ORIGINAL_DST 套接字选项。当一个连接被 iptables REDIRECT 或 TPROXY 目标（如果transparent选项被设置）重定向时，这个选项被设置。","title":"原始目的地监听器过滤器"},{"content":"Ingress 是从 Kubernetes 集群外部访问集群内部服务的入口，是将 Kubernetes 集群内部服务暴露到外界的几种方式之一。本文将为你详细介绍 Ingress 资源对象。\n术语\n在本篇文章中你将会看到一些在其他地方被交叉使用的术语，为了防止产生歧义，我们首先来澄清下。\n 节点：Kubernetes 集群中的一台物理机或者虚拟机。 集群：位于 Internet 防火墙后的节点，这是 Kubernetes 管理的主要计算资源。 边界路由器：为集群强制执行防火墙策略的路由器。 这可能是由云提供商或物理硬件管理的网关。 集群网络：一组逻辑或物理链接，可根据 Kubernetes 网络模型 实现群集内的通信。 集群网络的实现包括 Overlay 模型的 flannel 和基于 SDN 的 OVS。 服务：使用标签选择器标识一组 pod 成为的 Kubernetes 服务。 除非另有说明，否则服务假定在集群网络内仅可通过虚拟 IP 访问。  什么是 Ingress？ 通常情况下，service 和 pod 仅可在集群内部网络中通过 IP 地址访问。所有到达边界路由器的流量或被丢弃 …","relpermalink":"/kubernetes-handbook/service-discovery/ingress/","summary":"Ingress 是从 Kubernetes 集群外部访问集群内部服务的入口，是将 Kubernetes 集群内部服务暴露到外界的几种方式之一。本文将为你详细介绍 Ingress 资源对象。 术语 在本篇文章中你将会看到一些在其他地方被交叉使用的术语，为了防止产生歧义，我们首先来","title":"Ingress"},{"content":"除了直接使用 Service 和 Ingress 之外，Kubernetes 社区还发起了 Gateway API 项目，这是一个 CRD，可以帮助我们将 Kubernetes 中的服务暴露到集群外。\nGateway API 是一个由 SIG-NETWORK 管理的开源项目。该项目的目标是在 Kubernetes 生态系统中发展服务网络 API。Gateway API 提供了暴露 Kubernetes 应用的接口 ——Service、Ingress 等。\n该 API 在 Istio 中也被应用，用于将 Kubernetes 中的服务暴露到服务网格之外。\n目标 Gateway API 旨在通过提供表现性的、可扩展的、面向角色的接口来改善服务网络，这些接口由许多厂商实现，并得到了业界的广泛支持。\nGateway API 是一个 API 资源的集合 —— Service、GatewayClass、Gateway、HTTPRoute、TCPRoute 等。使用这些资源共同为各种网络用例建模。\n下图中展示的是 Kubernetes 集群中四层和七层的网络配置。从图中可以看到通过将这些资源对象分 …","relpermalink":"/kubernetes-handbook/service-discovery/gateway/","summary":"除了直接使用 Service 和 Ingress 之外，Kubernetes 社区还发起了 Gateway API 项目，这是一个 CRD，可以帮助我们将 Kubernetes 中的服务暴露到集群外。 Gateway API 是一个由 SIG-NETWORK 管理的开源项目。该项目的目标是在 Kubernetes 生态系统中发展服务网络 API。G","title":"Gateway"},{"content":"ServiceAccount 为 Pod 中的进程提供身份信息。\n注意\n 本文档描述的关于 ServiceAccount 的行为只有当你按照 Kubernetes 项目建议的方式搭建集群的情况下才有效。集群管理员可能在你的集群中进行了自定义配置，这种情况下该文档可能并不适用。  当你（真人用户）访问集群（例如使用 kubectl 命令）时，API 服务器会将你认证为一个特定的 User Account（目前通常是 admin，除非你的系统管理员自定义了集群配置）。Pod 容器中的进程也可以与 API 服务器联系。 当它们在联系 API 服务器的时候，它们会被认证为一个特定的 ServiceAccount（例如default）。\n使用默认的 ServiceAccount 访问 API 服务器 当你创建 pod 的时候，如果你没有指定一个 ServiceAccount，系统会自动得在与该 pod 相同的 namespace 下为其指派一个 default ServiceAccount。如果你获取刚创建的 pod 的原始 json 或 yaml 信息（例如使用kubectl get …","relpermalink":"/kubernetes-handbook/auth/serviceaccount/","summary":"ServiceAccount 为 Pod 中的进程提供身份信息。","title":"ServiceAccount"},{"content":"注意：本文基于 Kubernetes 1.6 撰写，当时 RBAC 模式处于 beta 版本。\n基于角色的访问控制（Role-Based Access Control，即”RBAC”）使用 rbac.authorization.k8s.io API Group 实现授权决策，允许管理员通过 Kubernetes API 动态配置策略。\n要启用 RBAC，请使用 --authorization-mode=RBAC 启动 API Server。\nAPI 概述 本节将介绍 RBAC API 所定义的四种顶级类型。用户可以像使用其他 Kubernetes API 资源一样 （例如通过 kubectl、API 调用等）与这些资源进行交互。例如，命令 kubectl create -f (resource).yml 可以被用于以下所有的例子，当然，读者在尝试前可能需要先阅读以下相关章节的内容。\nRole 与 ClusterRole 在 RBAC API 中，一个角色包含了一套表示一组权限的规则。 权限以纯粹的累加形式累积（没有” 否定” 的规则）。 角色可以由命名空间（namespace） …","relpermalink":"/kubernetes-handbook/auth/rbac/","summary":"注意：本文基于 Kubernetes 1.6 撰写，当时 RBAC 模式处于 beta 版本。 基于角色的访问控制（Role-Based Access Control，即”RBAC”）使用 rbac.authorization.k8s.io API Group 实现授权决策，允许管理员通过 Kubernetes API 动态配置策略。 要启用 RBAC，请使用 --authorization-mode=RBAC","title":"基于角色的访问控制（RBAC）"},{"content":"网络策略说明一组 Pod 之间是如何被允许互相通信，以及如何与其它网络 Endpoint 进行通信。 NetworkPolicy 资源使用标签来选择 Pod，并定义了一些规则，这些规则指明允许什么流量进入到选中的 Pod 上。关于 Network Policy 的详细用法请参考 Kubernetes 官网。\nNetwork Policy 的作用对象是 Pod，也可以应用到 Namespace 和集群的 Ingress、Egress 流量。Network Policy 是作用在 L3/4 层的，即限制的是对 IP 地址和端口的访问，如果需要对应用层做访问限制需要使用如 Istio 这类 Service Mesh。\n前提条件 网络策略通过网络插件来实现，所以必须使用一种支持 NetworkPolicy 的网络方案（如 calico）—— 非 Controller 创建的资源，是不起作用的。\n隔离的与未隔离的 Pod 默认 Pod 是未隔离的，它们可以从任何的源接收请求。 具有一个可以选择 Pod 的网络策略后，Pod 就会变成隔离的。 一旦 Namespace 中配置的网络策略能够选择一个 …","relpermalink":"/kubernetes-handbook/auth/network-policy/","summary":"网络策略说明一组 Pod 之间是如何被允许互相通信，以及如何与其它网络 Endpoint 进行通信。 NetworkPolicy 资源使用标签来选择 Pod，并定义了一些规则，这些规则指明允许什么流量进入到选中的 Pod 上。关于 Network Policy 的详细用法请参考 Kubernetes 官网。 Network Policy 的","title":"NetworkPolicy"},{"content":"SPIFFE，即每个人的安全生产身份框架（Secure Production Identity Framework for Everyone），是一套开源标准，用于在动态和异构环境中安全地进行身份识别。采用 SPIFFE 的系统无论在哪里运行，都可以轻松可靠地相互认证。\nSPIFFE 开源规范的核心是——通过简单 API 定义了一个短期的加密身份文件 SVID。然后，工作负载进行认证时可以使用该身份文件，例如建立 TLS 连接或签署和验证 JWT 令牌等。\nSPIFFE 已经在云原生应用中得到了大量的应用，尤其是在 Istio 和 Envoy 中。下面将向你介绍 SPIFFE 的一些基本概念。\n工作负载 工作负载是个单一的软件，以特定的配置部署，用于单一目的；它可能包括软件的多个运行实例，所有这些实例执行相同的任务。工作负载这个术语可以包含一系列不同的软件系统定义，包括：\n 一个运行 Python 网络应用程序的网络服务器，在一个虚拟机集群上运行，前面有一个负载均衡器。 一个 MySQL 数据库的实例。 一个处理队列中项目的 worker 程序。 独立部署的系统的集合，它们一起工作，例 …","relpermalink":"/kubernetes-handbook/auth/spiffe/","summary":"SPIFFE，即每个人的安全生产身份框架（Secure Production Identity Framework for Everyone），是一套开源标准，用于在动态和异构环境中安全地进行身份识别。采用 SPIFFE 的系统无论在哪里运行，都可以轻松可靠地相互认证。 SPIFFE 开源","title":"SPIFFE"},{"content":"这篇文章将向你介绍 SPIRE 的架构、基本概念及原理。\nSPIRE 是 SPIFFE API 的一个生产就绪的实现，它执行节点和工作负载认证，以便根据一组预先定义的条件，安全地向工作负载发出 SVID，并验证其他工作负载的 SVID。\nSPIRE 架构和组件 SPIRE 部署由一个 SPIRE 服务器和一个或多个 SPIRE 代理组成。服务器充当通过代理向一组工作负载发放身份的签名机构。它还维护一个工作负载身份的注册表，以及为签发这些身份而必须验证的条件。代理在本地向工作负载公开 SPIFFE 工作负载 API，必须安装在工作负载运行的每个节点上。\n  SPIRE 架构图  服务器 SPIRE 服务器负责管理和发布其配置的 SPIFFE 信任域中的所有身份。它存储注册条目（指定决定特定 SPIFFE ID 应被签发的条件的选择器）和签名密钥，使用节点证明来自动验证代理的身份，并在被验证的代理请求时为工作负载创建 SVID。\n   SPIRE 服务器  服务器的行为是通过一系列的插件决定的。SPIRE 包含几个插件，你可以建立额外的插件来扩展 SPIRE 以满足特定的使用情况。插件的类 …","relpermalink":"/kubernetes-handbook/auth/spire/","summary":"这篇文章将向你介绍 SPIRE 的架构、基本概念及原理。","title":"SPIRE"},{"content":"SPIRE Kubernetes 工作负载注册器实现了一个 Kubernetes ValidatingAdmissionWebhook，便于在 Kubernetes 内自动注册工作负载。\n配置 命令行配置 注册器有以下命令行选项：\n   标志 描述 默认值     -config 磁盘上 HCL 配置文件的路径 k8s-workload-registrar.conf    HCL 配置 配置文件是注册器所必需的。它包含 HCL 编码的配置项。\n   键 类型 必需的？ 描述 默认     log_level string 必需的 日志级别（panic、 fatal、 error、 warn、 warning、 info、 debug、trace 之一） info   log_path string 可选的 写入日志的磁盘路径    trust_domain string 必需的 SPIRE 服务器的信任域    agent_socket_path string 可选的 SPIRE 代理的 Unix 域套接字的路径。如果 server_address 不是 unix 域套接字地址，则为必 …","relpermalink":"/kubernetes-handbook/auth/spire-k8s-workload-registar/","summary":"本文介绍了如何在 Kubernetes 中使用 SPIRE 工作负载注册器，包括工作负载注册器部署的方式，注册模式等。","title":"SPIRE Kubernetes 工作负载注册器"},{"content":"本文介绍 SPIRE 如何向工作负载颁发身份的详细步骤，叙述了从代理在节点上启动到同一节点上的工作负载收到 X.509 SVID 形式的有效身份的全过程。请注意，JWT 格式的 SVID 的处理方式是不同的。为了简单演示的目的，工作负载在 AWS EC2 上运行。\n SPIRE 服务器启动。 除非用户配置了 UpstreamAuthority 插件，否则服务器会生成自签名证书（使用自己的私钥签名的证书）；服务器将使用此证书为该服务器信任域中的所有工作负载签署 SVID。 如果是第一次启动，服务器会自动生成一个信任包（trust bundle），其内容存储在你指定的 sql 数据存储中 —— 在服务器插件：DataStore sql 的 “内置插件” 部分中描述。 服务器打开其注册 API，以允许你注册工作负载。 工作负载节点上的 SPIRE 代理启动。 代理执行节点证明，向服务器证明它正在运行的节点的身份。例如，在 AWS EC2 实例上运行时，它通常会通过向服务器提供 AWS 实例身份文档来执行节点证明。 代理通过 TLS 连接向服务器提供此身份证明，该 TLS 连接通过代理配置的引 …","relpermalink":"/kubernetes-handbook/auth/svid/","summary":"本文介绍了为工作负载颁发 X.509 SVID 身份的详细步骤。","title":"SVID 身份颁发过程"},{"content":"如果你安装了拥有三个节点的 Kubernetes 集群，节点的状态如下所述。\n[root@node1 ~]# kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME node1 Ready \u0026lt;none\u0026gt; 2d v1.9.1 \u0026lt;none\u0026gt; CentOS Linux 7 (Core) 3.10.0-693.11.6.el7.x86_64 docker://1.12.6 node2 Ready \u0026lt;none\u0026gt; 2d v1.9.1 \u0026lt;none\u0026gt; CentOS Linux 7 (Core) 3.10.0-693.11.6.el7.x86_64 docker://1.12.6 node3 Ready \u0026lt;none\u0026gt; 2d v1.9.1 \u0026lt;none\u0026gt; CentOS Linux 7 (Core) 3.10.0-693.11.6.el7.x86_64 docker://1.12.6 当前 Kubernetes 集群中运行的所有 Pod 信息： …","relpermalink":"/kubernetes-handbook/networking/flannel/","summary":"如果你安装了拥有三个节点的 Kubernetes 集群，节点的状态如下所述。 [root@node1 ~]# kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME node1 Ready \u003cnone\u003e 2d v1.9.1 \u003cnone\u003e CentOS Linux 7 (Core) 3.10.0-693.11.6.el7.x86_64 docker://1.12.6 node2 Ready \u003cnone\u003e 2d v1.9.1 \u003cnone\u003e CentOS Linux 7 (Core) 3.10.0-693.11.6.el7.x86_64 docker://1.12.6 node3 Ready \u003cnone\u003e 2d v1.9.1 \u003cnone\u003e CentOS Linux 7 (Core) 3.10.0-693.11.6.el7.x86_64 docker://1.12.6 当前 Kubernetes 集群中运行的所有 Pod 信息： [root@node1 ~]# kubectl get pods --all-namespaces","title":"扁平网络 Flannel"},{"content":"Kiali 是一个基于 Istio 的服务网格的管理控制台。它提供了仪表盘、可观测性，并让我们通过强大的配置和验证能力来操作网格。它通过推断流量拓扑来显示服务网格，并显示网格的健康状况。Kiali 提供了详细的指标，强大的验证，Grafana 访问，以及与 Jaeger 的分布式跟踪的强大集成。\n要安装 Kiali，请使用 addons 文件夹中的 kiali.yaml 文件：\n$ kubectl apply -f istio-1.9.0/samples/addons/kiali.yaml customresourcedefinition.apiextensions.k8s.io/monitoringdashboards.monito ring.kiali.io created serviceaccount/kiali created configmap/kiali created clusterrole.rbac.authorization.k8s.io/kiali-viewer created clusterrole.rbac.authorization.k8s.io/kiali …","relpermalink":"/istio-handbook/observability/kiali/","summary":"Kiali 是一个基于 Istio 的服务网格的管理控制台。它提供了仪表盘、可观测性，并让我们通过强大的配置和验证能力来操作网格。它通过推断流量拓扑来显示服务网格，并显示网格的健康状况。Kiali 提供了详细的指标，强大的验","title":"Kiali"},{"content":"路由发现服务（RDS）是 Envoy 里面的一个可选 API，用于动态获取路由配置。路由配置包括 HTTP header 修改、虚拟主机以及每个虚拟主机中包含的单个路由规则配置。每个 HTTP 连接管理器都可以通过 API 独立地获取自身的路由配置。\n注意：Envoy 从 1.9 版本开始已不再支持 v1 API。\n统计 RDS 的统计树以 http.\u0026lt;stat_prefix\u0026gt;.rds.\u0026lt;route_config_name\u0026gt;.*.为根，route_config_name名称中的任何:字符在统计树中被替换为_。统计树包含以下统计信息：\n   名字 类型 描述     config_reload Counter 因配置不同而导致配置重新加载的总次数   update_attempt Counter 尝试调用配置加载 API 的总次数   update_success Counter 调用配置加载 API 成功的总次数   update_failure Counter 调用配置加载 API 因网络错误的失败总数   update_rejected Counter 调用配置加载 API …","relpermalink":"/istio-handbook/data-plane/envoy-rds/","summary":"路由发现服务（RDS）是 Envoy 里面的一个可选 API，用于动态获取路由配置。路由配置包括 HTTP header 修改、虚拟主机以及每个虚拟主机中包含的单个路由规则配置。每个 HTTP 连接管理器都可以通过 API 独立地获取自身的路由配置。 注意","title":"RDS（路由发现服务）"},{"content":"Sidecar 模式是 Istio 服务网格采用的模式，在服务网格出现之前该模式就一直存在，尤其是当微服务出现后开始盛行，本文讲解 Sidecar 模式。\n什么是 Sidecar 模式 将应用程序的功能划分为单独的进程可以被视为 Sidecar 模式。Sidecar 设计模式允许你为应用程序添加许多功能，而无需额外第三方组件的配置和代码。\n就如 Sidecar 连接着摩托车一样，类似地在软件架构中， Sidecar 应用是连接到父应用并且为其扩展或者增强功能。Sidecar 应用与主应用程序松散耦合。\n让我用一个例子解释一下。想象一下假如你有6个微服务相互通信以确定一个包裹的成本。\n每个微服务都需要具有可观测性、监控、日志记录、配置、断路器等功能。所有这些功能都是根据一些行业标准的第三方库在每个微服务中实现的。\n但再想一想，这不是多余吗？它不会增加应用程序的整体复杂性吗？如果你的应用程序是用不同的语言编写时会发生什么——如何合并那些特定用于 .Net、Java、Python 等语言的第三方库。\n使用 Sidecar 模式的优势  通过抽象出与功能相关的共同基础设施到一个不同层降低了微服 …","relpermalink":"/istio-handbook/concepts/sidecar-pattern/","summary":"Sidecar 模式是 Istio 服务网格采用的模式，在服务网格出现之前该模式就一直存在，尤其是当微服务出现后开始盛行，本文讲解 Sidecar 模式。 什么是 Sidecar 模式 将应用程序的功能划分为单独的进程可以被视为 Sidecar 模式。Sidecar 设计模式允许","title":"Sidecar 模式"},{"content":"WorkloadGroup 描述了工作负载实例的集合。它提供了一个规范，工作负载实例可用于启动其代理，包括元数据和身份。它只适用于虚拟机等非 Kubernetes 工作负载，旨在模仿现有的用于 Kubernetes 工作负载的 sidecar 注入和部署规范模型，以引导 Istio 代理。\n示例 下面的例子声明了一个代表工作负载集合的工作负载组，这些工作负载将在 bookinfo 命名空间的 reviews 下注册。在引导过程中，这组标签将与每个工作负载实例相关联，端口 3550 和 8080 将与工作负载组相关联，并使用 default 服务账户。app.kubernetes.io/version 只是一个标签的例子。 …","relpermalink":"/istio-handbook/config-networking/workload-group/","summary":"WorkloadGroup 描述了工作负载实例的集合。它提供了一个规范，工作负载实例可用于启动其代理，包括元数据和身份。它只适用于虚拟机等非 Kubernetes 工作负载，旨在模仿现有的用于 Kubernetes 工作负载的 sidecar 注入和部署规范模型，以引导 Istio 代理。 示例 下面","title":"WorkloadGroup"},{"content":"在本节中，我们将为推荐服务引入 5 秒的延迟。Envoy 将为 50% 的请求注入延迟。\napiVersion:networking.istio.io/v1alpha3kind:VirtualServicemetadata:name:recommendationservicespec:hosts:- recommendationservicehttp:- route:- destination:host:recommendationservicefault:delay:percentage:value:50fixedDelay:5s将上述 YAML 保存为 recommendation-delay.yaml，然后用 kubectl apply -f recommendation-delay.yaml 创建 VirtualService。\n我们可以在浏览器中打开 INGRESS_HOST，然后点击其中一个产品。推荐服务的结果显示在屏幕底部的”Other Products You Might Light“部分。如果我们刷新几次页面，我们会注意到，该页面要么立即加载，要么有一个延迟加载页面。 …","relpermalink":"/istio-handbook/practice/fault-injection/","summary":"在本节中，我们将为推荐服务引入 5 秒的延迟。Envoy 将为 50% 的请求注入延迟。 apiVersion:networking.istio.io/v1alpha3kind:VirtualServicemetadata:name:recommendationservicespec:hosts:- recommendationservicehttp:- route:- destination:host:recommendationservicefault:delay:percentage","title":"错误注入"},{"content":"弹性（Resiliency）是指在面对故障和对正常运行的挑战时，提供和保持可接受的服务水平的能力。这不是为了避免故障。它是以一种没有停机或数据丢失的方式来应对它们。弹性的目标是在故障发生后将服务恢复到一个完全正常的状态。\n使服务可用的一个关键因素是在提出服务请求时使用超时（timeout）和重试（retry）策略。我们可以在 Istio 的 VirtualService 上配置这两者。\n使用超时字段，我们可以为 HTTP 请求定义一个超时。如果请求的时间超过了超时字段中指定的值，Envoy 代理将放弃请求，并将其标记为超时（向应用程序返回一个 HTTP 408）。连接将保持开放，除非触发了异常点检测。下面是一个为路由设置超时的例子：\n...- route:- destination:host:customers.default.svc.cluster.localsubset:v1timeout:10s...除了超时之外，我们还可以配置更细化的重试策略。我们可以控制一个给定请求的重试次数，每次尝试的超时时间，以及我们想要重试的具体条件。\n例如，我们可以只在上游服务器返回任何 5xx 响应 …","relpermalink":"/istio-handbook/traffic-management/resiliency/","summary":"弹性（Resiliency）是指在面对故障和对正常运行的挑战时，提供和保持可接受的服务水平的能力。这不是为了避免故障。它是以一种没有停机或数据丢失的方式来应对它们。弹性的目标是在故障发生后将服务恢复到","title":"弹性"},{"content":"负载均衡是一种在单个上游集群的多个端点之间分配流量的方式。在众多端点之间分配流量的原因是为了最好地利用可用资源。\n为了实现资源的最有效利用，Envoy 提供了不同的负载均衡策略，可以分为两组：全局负载均衡和分布式负载均衡。不同的是，在全局负载均衡中，我们使用单一的控制平面来决定端点之间的流量分配。Envoy 决定负载如何分配（例如，使用主动健康检查、分区感知路由、负载均衡策略）。\n在多个端点之间分配负载的技术之一是一致性哈希。服务器使用请求的一部分来创建一个哈希值来选择一个端点。在模数散列中，哈希值被认为是一个巨大的数字。为了得到发送请求的端点索引，我们将哈希值与可用端点的数量取余数（index=hash % endpointCount）。如果端点的数量是稳定的，这种方法效果很好。然而，如果端点被添加或删除（即它们不健康，我们扩大或缩小它们的规模，等等），大多数请求将在一个与以前不同的端点上结束。\n一致性哈希是一种方法，每个端点根据某些属性被分配多个有价值的值。然后，每个请求被分配到具有最接近哈希值的端点。这种方法的价值在于，当我们添加或删除端点时，大多数请求最终会被分配到与之前相同的 …","relpermalink":"/envoy-handbook/cluster/load-balancing/","summary":"负载均衡是一种在单个上游集群的多个端点之间分配流量的方式。在众多端点之间分配流量的原因是为了最好地利用可用资源。 为了实现资源的最有效利用，Envoy 提供了不同的负载均衡策略，可以分为两组：全局负载均衡","title":"负载均衡"},{"content":"集群端点（/clusters）将显示配置的集群列表，并包括以下信息：\n 每个主机的统计数据 每个主机的健康状态 断路器设置 每个主机的权重和位置信息   这里的主机指的是每个被发现的属于上游集群的主机。\n 下面的片段显示了信息的模样（注意，输出是经过裁剪的）。\n{ \u0026#34;cluster_statuses\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;api_google_com\u0026#34;, \u0026#34;host_statuses\u0026#34;: [ { \u0026#34;address\u0026#34;: { \u0026#34;socket_address\u0026#34;: { \u0026#34;address\u0026#34;: \u0026#34;10.0.0.1\u0026#34;, \u0026#34;port_value\u0026#34;: 8080 } }, \u0026#34;stats\u0026#34;: [ { \u0026#34;value\u0026#34;: \u0026#34;23\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;cx_total\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;rq_error\u0026#34; }, { \u0026#34;value\u0026#34;: \u0026#34;51\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;rq_success\u0026#34; }, ... ], \u0026#34;health_status\u0026#34;: { \u0026#34;eds_health_status\u0026#34;: \u0026#34;HEALTHY\u0026#34; }, \u0026#34;weight\u0026#34;: 1, \u0026#34;locality\u0026#34;: {} } ], …","relpermalink":"/envoy-handbook/admin-interface/clusters/","summary":"集群端点（/clusters）将显示配置的集群列表，并包括以下信息： 每个主机的统计数据 每个主机的健康状态 断路器设置 每个主机的权重和位置信息 这里的主机指的是每个被发现的属于上游集群的主机。 下面的片段显示","title":"集群"},{"content":"在这个实验中，我们将学习使用日志过滤器，根据一些请求属性，只记录某些请求。\n让我们想出几个我们想用过滤器来实现的日志要求。\n 将所有带有 HTTP 404 状态码的请求记录到一个名为 not_found.log 的日志文件中。 将所有带有 env=debug 头值的请求记录到一个名为 debug.log 的日志文件中。 将所有 POST 请求记录到标准输出  基于这些要求，我们将有两个访问记录器记录到文件（not_found.log 和 debug.log）和一个写到 stdout 的访问记录器。\naccess_log 字段是一个数组，我们可以在它下面定义多个记录器。在各个日志记录器里面，我们可以使用 filter 字段来指定何时将字符串写到日志中。\n对于第一个要求，我们将使用状态码过滤器，而对于第二个要求，我们将使用 Header 过滤器。下面是配置的样子。\nstatic_resources:listeners:- name:listener_0address:socket_address:address:0.0.0.0port_value:10000filter_chains:- …","relpermalink":"/envoy-handbook/logging/lab12/","summary":"在这个实验中，我们将学习使用日志过滤器，根据一些请求属性，只记录某些请求。 让我们想出几个我们想用过滤器来实现的日志要求。 将所有带有 HTTP 404 状态码的请求记录到一个名为 not_found.log 的日志文件中。 将所有带有 env=debug 头值的请求记","title":"实验 12：使用日志过滤器"},{"content":"在这个实验中，我们将使用 TinyGo、proxy-wasm-go-sdk 和 func-e CLI 来构建和测试一个 Envoy Wasm 扩展。\n我们将写一个简单的 Wasm 模块，为响应头添加一个头。稍后，我们将展示如何读取配置和添加自定义指标。我们将使用 Golang 并使用 TinyGo 编译器进行编译。\n安装 TinyGo 让我们下载并安装 TinyGo。\nwget https://github.com/tinygo-org/tinygo/releases/download/v0.21.0/tinygo_0.21.0_amd64.deb sudo dpkg -i tinygo_0.21.0_amd64.deb 你可以运行 tinygo version 来检查安装是否成功。\n$ tinygo version tinygo version 0.21.0 linux/amd64 (using go version go1.17.2 and LLVM version 11.0.0) 为 Wasm 模块搭建脚手架 我们将首先为我们的扩展创建一个新的文件夹，初始化 Go 模块， …","relpermalink":"/envoy-handbook/extending-envoy/lab17/","summary":"在这个实验中，我们将使用 TinyGo、proxy-wasm-go-sdk 和 func-e CLI 来构建和测试一个 Envoy Wasm 扩展。 我们将写一个简单的 Wasm 模块，为响应头添加一个头。稍后，我们将展示如何读取配置和添加自定义指标。我们","title":"实验17：使用 Wasm 和 Go 扩展 Envoy"},{"content":"原始源过滤器（envoy.filters.listener.original_src）在 Envoy 的上游（接收 Envoy 请求的主机）一侧复制了连接的下游（连接到 Envoy 的主机）的远程地址。\n例如，如果我们用 10.0.0.1 发送请求给 Envoy 连接到上游，源 IP 是 10.0.0.1 。这个地址是由代理协议过滤器决定的（接下来解释），或者它可以来自于可信的 HTTP Header。\n- name:envoy.filters.listener.original_srctyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions. filters.listener.original_src.v3.OriginalSrcmark:100该过滤器还允许我们在上游连接的套接字上设置 SO_MARK 选项。SO_MARK 选项用于标记通过套接字发送的每个数据包，并允许我们做基于标记的路由（我们可以在以后匹配标记）。\n上面的片段将该标记设置为 100。使用这个标记， …","relpermalink":"/envoy-handbook/listener/original-source-listener-filter/","summary":"原始源过滤器（envoy.filters.listener.original_src）在 Envoy 的上游（接收 Envoy 请求的主机）一侧复制了连接的下游（连接到 Envoy 的主机）的远程地址。 例如，如果我们用 10.0.0.1 发送请求给 Envoy 连接","title":"原始源监听器过滤器"},{"content":"Calico 原意为”有斑点的“，如果说一只猫为 calico cat 的话，就是说这是只花猫，也叫三色猫，所以 calico 的 logo 是只三色猫。\n   Calico  概念 Calico 创建和管理一个扁平的三层网络（不需要 overlay），每个容器会分配一个可路由的 IP。由于通信时不需要解包和封包，网络性能损耗小，易于排查，且易于水平扩展。\n小规模部署时可以通过 BGP client 直接互联，大规模下可通过指定的 BGP Route Reflector 来完成，这样保证所有的数据流量都是通过 IP 路由的方式完成互联的。\nCalico 基于 iptables 还提供了丰富而灵活的网络 Policy，保证通过各个节点上的 ACL 来提供 Workload 的多租户隔离、安全组以及其他可达性限制等功能。\nCalico 架构 Calico 由以下组件组成，在部署 Calico 的时候部分组件是可选的。\n Calico API server Felix BIRD confd Dikastes CNI 插件 数据存储插件 IPAM 插件 kube-controllers …","relpermalink":"/kubernetes-handbook/networking/calico/","summary":"Calico 原意为”有斑点的“，如果说一只猫为 calico cat 的话，就是说这是只花猫，也叫三色猫，所以 calico 的 logo 是只三色猫。 Calico 概念 Calico 创建和管理一个扁平的三层网络（不需要 overlay），每个容器会分配一个可路由的 IP。由于通信","title":"非 Overlay 扁平网络 Calico"},{"content":"Cilium 是一款开源软件，也是 CNCF 的孵化项目，目前已有公司提供商业化支持，还有基于 Cilium 实现的服务网格解决方案。最初它仅是作为一个 Kubernetes 网络组件。Cilium 在 1.7 版本后推出并开源了 Hubble，它是专门为网络可视化设计，能够利用 Cilium 提供的 eBPF 数据路径，获得对 Kubernetes 应用和服务的网络流量的深度可视性。这些网络流量信息可以对接 Hubble CLI、UI 工具，可以通过交互式的方式快速进行问题诊断。除了 Hubble 自身的监控工具，还可以对接主流的云原生监控体系——Prometheus 和 Grafana，实现可扩展的监控策略。\n   Cilium  本节将带你了解什么是 Cilium 及选择它的原因。\nCilium 是什么？ Cilium 为基于 Kubernetes 的 Linux 容器管理平台上部署的服务，透明地提供服务间的网络和 API 连接及安全。\nCilium 底层是基于 Linux 内核的新技术 eBPF，可以在 Linux 系统中动态注入强大的安全性、可视性和网络控制逻辑。 …","relpermalink":"/kubernetes-handbook/networking/cilium/","summary":"Cilium 是一款开源软件，也是 CNCF 的孵化项目，目前已有公司提供商业化支持，还有基于 Cilium 实现的服务网格解决方案。最初它仅是作为一个 Kubernetes 网络组件。Cilium 在 1.7 版本后推出并开源了 Hubble，它是专门为网络可视化设计","title":"基于 eBPF 的网络 Cilium"},{"content":"Secret 解决了密码、token、密钥等敏感数据的配置问题，而不需要把这些敏感数据暴露到镜像或者 Pod Spec 中。Secret 可以以 Volume 或者环境变量的方式使用。\nSecret 有三种类型：\n Service Account ：用来访问 Kubernetes API，由 Kubernetes 自动创建，并且会自动挂载到 Pod 的 /run/secrets/kubernetes.io/serviceaccount 目录中； Opaque ：base64 编码格式的 Secret，用来存储密码、密钥等； kubernetes.io/dockerconfigjson ：用来存储私有 docker registry 的认证信息。  Opaque Secret Opaque 类型的数据是一个 map 类型，要求 value 是 base64 编码格式：\n$ echo -n \u0026#34;admin\u0026#34; | base64 YWRtaW4= $ echo -n \u0026#34;1f2d1e2e67df\u0026#34; | base64 MWYyZDFlMmU2N2Rm secrets.yml …","relpermalink":"/kubernetes-handbook/storage/secret/","summary":"Secret 解决了密码、token、密钥等敏感数据的配置问题，而不需要把这些敏感数据暴露到镜像或者 Pod Spec 中。Secret 可以以 Volume 或者环境变量的方式使用。 Secret 有三种类型： Service Account ：用来访问 Kubernetes API，由 Kubernetes 自动创建，并且会自","title":"Secret"},{"content":"其实 ConfigMap 功能在 Kubernetes1.2 版本的时候就有了，许多应用程序会从配置文件、命令行参数或环境变量中读取配置信息。这些配置信息需要与 docker image 解耦，你总不能每修改一个配置就重做一个 image 吧？ConfigMap API 给我们提供了向容器中注入配置信息的机制，ConfigMap 可以被用来保存单个属性，也可以用来保存整个配置文件或者 JSON 二进制大对象。\nConfigMap 概览 ConfigMap API 资源用来保存 key-value pair 配置数据，这个数据可以在 pods 里使用，或者被用来为像 controller 一样的系统组件存储配置数据。虽然 ConfigMap 跟 Secrets 类似，但是 ConfigMap 更方便的处理不含敏感信息的字符串。 注意：ConfigMaps 不是属性配置文件的替代品。ConfigMaps 只是作为多个 properties 文件的引用。你可以把它理解为 Linux 系统中的 /etc 目录，专门用来存储配置文件的目录。下面举个例子，使用 ConfigMap …","relpermalink":"/kubernetes-handbook/storage/configmap/","summary":"其实 ConfigMap 功能在 Kubernetes1.2 版本的时候就有了，许多应用程序会从配置文件、命令行参数或环境变量中读取配置信息。这些配置信息需要与 docker image 解耦，你总不能每修改一个配置就重做一个 image 吧？ConfigMap API 给我们提供了向容器中","title":"ConfigMap"},{"content":"ConfigMap 是用来存储配置文件的 kubernetes 资源对象，所有的配置内容都存储在 etcd 中，下文主要是探究 ConfigMap 的创建和更新流程，以及对 ConfigMap 更新后容器内挂载的内容是否同步更新的测试。\n测试示例 假设我们在 default namespace 下有一个名为 nginx-config 的 ConfigMap，可以使用 kubectl 命令来获取：\n$ kubectl get configmap nginx-config NAME DATA AGE nginx-config 1 99d 获取该 ConfigMap 的内容。\nkubectl get configmap nginx-config -o yaml apiVersion: v1 data: nginx.conf: |- worker_processes 1; events { worker_connections 1024; } http { sendfile on; server { listen 80; # a test endpoint that returns http …","relpermalink":"/kubernetes-handbook/storage/configmap-hot-update/","summary":"ConfigMap 是用来存储配置文件的 kubernetes 资源对象，所有的配置内容都存储在 etcd 中，下文主要是探究 ConfigMap 的创建和更新流程，以及对 ConfigMap 更新后容器内挂载的内容是否同步更新的测试。 测试示例 假设我们在 default namespace 下有一个名为 nginx-config 的 ConfigMa","title":"ConfigMap 的热更新"},{"content":"Volume 容器磁盘上的文件的生命周期是短暂的，这就使得在容器中运行重要应用时会出现一些问题。首先，当容器崩溃时，kubelet 会重启它，但是容器中的文件将丢失——容器以干净的状态（镜像最初的状态）重新启动。其次，在 Pod 中同时运行多个容器时，这些容器之间通常需要共享文件。Kubernetes 中的 Volume 抽象就很好的解决了这些问题。\n建议先熟悉 pod。\n背景 Docker 中也有一个 volume 的概念，尽管它稍微宽松一些，管理也很少。在 Docker 中，卷就像是磁盘或是另一个容器中的一个目录。它的生命周期不受管理，直到最近才有了 local-disk-backed 卷。Docker 现在提供了卷驱动程序，但是功能还非常有限（例如Docker1.7只允许每个容器使用一个卷驱动，并且无法给卷传递参数）。\n另一方面，Kubernetes 中的卷有明确的寿命——与封装它的 Pod 相同。所以，卷的生命比 Pod 中的所有容器都长，当这个容器重启时数据仍然得以保存。当然，当 Pod 不再存在时，卷也将不复存在。也许更重要的是，Kubernetes 支持多种类型的 …","relpermalink":"/kubernetes-handbook/storage/volume/","summary":"Volume 容器磁盘上的文件的生命周期是短暂的，这就使得在容器中运行重要应用时会出现一些问题。首先，当容器崩溃时，kubelet 会重启它，但是容器中的文件将丢失——容器以干净的状态（镜像最初的状态）重新启动。其","title":"Volume"},{"content":"本文档介绍了 Kubernetes 中 PersistentVolume 的当前状态。建议您在阅读本文档前先熟悉 volume。\n介绍 对于管理计算资源来说，管理存储资源明显是另一个问题。PersistentVolume 子系统为用户和管理员提供了一个 API，该 API 将如何提供存储的细节抽象了出来。为此，我们引入两个新的 API 资源：PersistentVolume 和 PersistentVolumeClaim。\nPersistentVolume（PV）是由管理员设置的存储，它是群集的一部分。就像节点是集群中的资源一样，PV 也是集群中的资源。 PV 是 Volume 之类的卷插件，但具有独立于使用 PV 的 Pod 的生命周期。此 API 对象包含存储实现的细节，即 NFS、iSCSI 或特定于云供应商的存储系统。\nPersistentVolumeClaim（PVC）是用户存储的请求。它与 Pod 相似。Pod 消耗节点资源，PVC 消耗 PV 资源。Pod 可以请求特定级别的资源（CPU 和内存）。声明可以请求特定的大小和访问模式（例如，可以以读/写一次或 只读多次模式挂 …","relpermalink":"/kubernetes-handbook/storage/persistent-volume/","summary":"本文档介绍了 Kubernetes 中 PersistentVolume 的当前状态。建议您在阅读本文档前先熟悉 volume。 介绍 对于管理计算资源来说，管理存储资源明显是另一个问题。PersistentVolume 子系统为用户和管理员提供了一个 API，该 API","title":"持久化卷（Persistent Volume）"},{"content":"本文介绍了 Kubernetes 中 StorageClass 的概念。在阅读本文之前建议先熟悉 卷 和 Persistent Volume（持久卷）。\n介绍 StorageClass 为管理员提供了描述存储 “class（类）” 的方法。 不同的 class 可能会映射到不同的服务质量等级或备份策略，或由群集管理员确定的任意策略。 Kubernetes 本身不清楚各种 class 代表的什么。这个概念在其他存储系统中有时被称为“配置文件”。\nStorageClass 资源 StorageClass 中包含 provisioner、parameters 和 reclaimPolicy 字段，当 class 需要动态分配 PersistentVolume 时会使用到。\nStorageClass 对象的名称很重要，用户使用该类来请求一个特定的方法。 当创建 StorageClass 对象时，管理员设置名称和其他参数，一旦创建了对象就不能再对其更新。\n管理员可以为没有申请绑定到特定 class 的 PVC 指定一个默认的 StorageClass ： …","relpermalink":"/kubernetes-handbook/storage/storageclass/","summary":"本文介绍了 Kubernetes 中 StorageClass 的概念。在阅读本文之前建议先熟悉 卷 和 Persistent Volume（持久卷）。 介绍 StorageClass 为管理员提供了描述存储 “class（类）” 的方法。 不同的 class 可能会映射到不同的服务质量等级或备","title":"Storage Class"},{"content":"集群发现服务（CDS）是一个可选的 API，Envoy 将调用该 API 来动态获取集群管理器的成员。Envoy 还将根据 API 响应协调集群管理，根据需要完成添加、修改或删除已知的集群。\n关于 Envoy 是如何通过 CDS 从 pilot-discovery 服务中获取的 cluster 配置，请参考 Service Mesh深度学习系列part3—istio源码分析之pilot-discovery模块分析（续）一文中的 CDS 服务部分。\n注意\n 在 Envoy 配置中静态定义的 cluster 不能通过 CDS API 进行修改或删除。 Envoy 从 1.9 版本开始已不再支持 v1 API。  统计 CDS 的统计树以 cluster_manager.cds. 为根，统计如下：\n   名字 类型 描述     config_reload Counter 因配置不同而导致配置重新加载的总次数   update_attempt Counter 尝试调用配置加载 API 的总次数   update_success Counter 调用配置加载 API …","relpermalink":"/istio-handbook/data-plane/envoy-cds/","summary":"集群发现服务（CDS）是一个可选的 API，Envoy 将调用该 API 来动态获取集群管理器的成员。Envoy 还将根据 API 响应协调集群管理，根据需要完成添加、修改或删除已知的集群。 关于 Envoy 是如何通过 CDS 从 pilot-discovery 服务中获取","title":"CDS（集群发现服务）"},{"content":"ServiceEntry 可以在 Istio 的内部服务注册表中添加额外的条目，这样网格中自动发现的服务就可以访问 / 路由到这些手动指定的服务。服务条目描述了服务的属性（DNS 名称、VIP、端口、协议、端点）。这些服务可以是网格的外部服务（如 Web API），也可以是不属于平台服务注册表的网格内部服务（如与 Kubernetes 中的服务通信的一组虚拟机）。此外，服务条目的端点也可以通过使用 workloadSelector 字段动态选择。这些端点可以是使用 WorkloadEntry 对象声明的虚拟机工作负载或 Kubernetes pod。在单一服务下同时选择 pod 和 VM 的能力允许将服务从 VM 迁移到 Kubernetes，而不必改变与服务相关的现有 DNS 名称。\n示例 下面的例子声明了一些内部应用程序通过 HTTPS 访问的外部 API。Sidecar 检查了 ClientHello 消息中的 SNI 值，以路由到适当的外部服务。 …","relpermalink":"/istio-handbook/config-networking/service-entry/","summary":"ServiceEntry 可以在 Istio 的内部服务注册表中添加额外的条目，这样网格中自动发现的服务就可以访问 / 路由到这些手动指定的服务。服务条目描述了服务的属性（DNS 名称、VIP、端口、协议、端点）。这些服务可以是网格的外部服务","title":"ServiceEntry"},{"content":"这一节将带大家了解 Istio 为数据平面自动注入 Sidecar 的详细过程。\nSidecar 注入过程概览 如 Istio 官方文档中对 Istio sidecar 注入的描述，你可以使用 istioctl 命令手动注入 Sidecar，也可以为 Kubernetes 集群自动开启 sidecar 注入，这主要用到了 Kubernetes 的准入控制器中的 webhook，参考 Istio 官网中对 Istio sidecar 注入的描述。\n   Sidecar 注入流程图  手动注入 sidecar 与自动注入 sidecar 的区别 不论是手动注入还是自动注入，sidecar 的注入过程有需要遵循如下步骤：\n Kubernetes 需要了解待注入的 sidecar 所连接的 Istio 集群及其配置； Kubernetes 需要了解待注入的 sidecar 容器本身的配置，如镜像地址、启动参数等； Kubernetes 根据 sidecar 注入模板和以上配置填充 sidecar 的配置参数，将以上配置注入到应用容器的一侧；  Istio 和 sidecar …","relpermalink":"/istio-handbook/concepts/istio-sidecar-injector/","summary":"这一节将带大家了解 Istio 为数据平面自动注入 Sidecar 的详细过程。 Sidecar 注入过程概览 如 Istio 官方文档中对 Istio sidecar 注入的描述，你可以使用 istioctl 命令手动注入 Sidecar，也可以为 Kubernetes 集群自动开启 sidecar 注入，这主要用到了 Kubernetes 的准入控制器中的 w","title":"Sidecar 自动注入过程详解"},{"content":"为了帮助我们提高服务的弹性，我们可以使用故障注入功能。我们可以在 HTTP 流量上应用故障注入策略，在转发目的地的请求时指定一个或多个故障注入。\n有两种类型的故障注入。我们可以在转发前延迟（delay）请求，模拟缓慢的网络或过载的服务，我们可以中止（abort） HTTP 请求，并返回一个特定的 HTTP 错误代码给调用者。通过中止，我们可以模拟一个有故障的上游服务。\n下面是一个中止 HTTP 请求并返回 HTTP 404 的例子，针对 30% 的传入请求。\n- route:- destination:host:customers.default.svc.cluster.localsubset:v1fault:abort:percentage:value:30httpStatus:404如果我们不指定百分比，所有的请求将被中止。请注意，故障注入会影响使用该 VirtualService 的服务。它并不影响该服务的所有消费者。\n同样地，我们可以使用 fixedDelay 字段对请求应用一个可选的延迟。\n- route:- …","relpermalink":"/istio-handbook/traffic-management/fault-injection/","summary":"为了帮助我们提高服务的弹性，我们可以使用故障注入功能。我们可以在 HTTP 流量上应用故障注入策略，在转发目的地的请求时指定一个或多个故障注入。 有两种类型的故障注入。我们可以在转发前延迟（delay）请求，模拟","title":"错误注入"},{"content":"代理协议监听器过滤器（envoy.filters.listener.proxy_protocol）增加了对 HAProxy 代理协议的支持。\n代理使用其 IP 堆栈连接到远程服务器，并丢失初始连接的源和目的地信息。PROXY 协议允许我们在不丢失客户端信息的情况下链接代理。该协议定义了一种在主 TCP 流之前通过 TCP 通信连接的元数据的方式。元数据包括源 IP 地址。\n使用这个过滤器，Envoy 可以从 PROXY 协议中获取元数据，并将其传播到 x-forwarded-for 头中。\n","relpermalink":"/envoy-handbook/listener/proxy-protocol-listener-filter/","summary":"代理协议监听器过滤器（envoy.filters.listener.proxy_protocol）增加了对 HAProxy 代理协议的支持。 代理使用其 IP 堆栈连接到远程服务器，并丢失初始连接的源和目的地信息。PROXY","title":"代理协议监听器过滤器"},{"content":"为了演示弹性功能，我们将在产品目录服务部署中添加一个名为 EXTRA_LATENCY 的环境变量。这个变量会在每次调用服务时注入一个额外的休眠。\n通过运行 kubectl edit deploy productcatalogservice 来编辑产品目录服务部署。这将打开一个编辑器。滚动到有环境变量的部分，添加 EXTRA_LATENCY 环境变量。\n...spec:containers:- env:- name:EXTRA_LATENCYvalue:6s...保存并推出编辑器。\n如果我们刷新页面，我们会发现页面需要 6 秒的时间来加载）——那是由于我们注入的延迟。\n让我们给产品目录服务添加一个 2 秒的超时。\napiVersion:networking.istio.io/v1alpha3kind:VirtualServicemetadata:name:productcatalogservicespec:hosts:- productcatalogservicehttp:- route:- destination:host:productcatalogservicetimeout:2s …","relpermalink":"/istio-handbook/practice/resiliency/","summary":"为了演示弹性功能，我们将在产品目录服务部署中添加一个名为 EXTRA_LATENCY 的环境变量。这个变量会在每次调用服务时注入一个额外的休眠。 通过运行 kubectl edit deploy productcatalogservice 来编辑产品目录服务部署。这将打开一个编辑器。滚动到有环境变量的部分，","title":"弹性"},{"content":"/listeners 端点列出了所有配置的监听器。这包括名称以及每个监听器的地址和监听的端口。\n例如：\n$ curl localhost:9901/listeners http_8080::0.0.0.0:8080 http_hello_world_9090::0.0.0.0:9090 对于 JSON 输出，我们可以在 URL 上附加 ?format=json。\n$ curl localhost:9901/listeners?format=json { \u0026#34;listener_statuses\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;http_8080\u0026#34;, \u0026#34;local_address\u0026#34;: { \u0026#34;socket_address\u0026#34;: { \u0026#34;address\u0026#34;: \u0026#34;0.0.0.0\u0026#34;, \u0026#34;port_value\u0026#34;: 8080 } } }, { \u0026#34;name\u0026#34;: \u0026#34;http_hello_world_9090\u0026#34;, \u0026#34;local_address\u0026#34;: { \u0026#34;socket_address\u0026#34;: { \u0026#34;address\u0026#34;: \u0026#34;0.0.0.0\u0026#34;, \u0026#34;port_value\u0026#34;: 9090 } } } ] } 监听器排空 发生排 …","relpermalink":"/envoy-handbook/admin-interface/listeners-and-draining/","summary":"/listeners 端点列出了所有配置的监听器。这包括名称以及每个监听器的地址和监听的端口。 例如： $ curl localhost:9901/listeners http_8080::0.0.0.0:8080 http_hello_world_9090::0.0.0.0:9090 对于 JSON 输出，我们可以在 URL 上附加 ?format=json。 $ curl localhost:9901/listeners?format=json { \"listener_statuses\": [ { \"name\": \"http_8080\", \"local_address\": { \"socket_address\": { \"address\": \"0.0.0.0\", \"port_value\": 8080 } } }, { \"name\": \"http_hello_world_9090\", \"local_address\": { \"socket_address\": {","title":"监听器和监听器的排空"},{"content":"本实验涵盖了如何配置 Envoy 使用独立的 gRPC 访问日志服务（ALS）。我们将使用一个基本的 gRPC 服务器，它实现了 StreamAccessLogs 函数，并将收到的日志从 Envoy 输出到标准输出。\n让我们先把 ALS 服务器作为一个 Docker 容器来运行。\ndocker run -dit -p 5000:5000 gcr.io/tetratelabs/envoy-als:0.1.0 ALS 服务器默认监听端口为 5000，所以如果我们看一下 Docker 容器的日志，它们应该类似于下面的内容。\n$ docker logs [container-id] Creating new ALS server 2021/11/05 20:24:03 Listening on :5000 输出告诉我们，ALS 正在监听 5000 端口。\n在 Envoy 配置中，有两个要求需要配置。首先是使用 access_log 字段的访问日志和一个名为 HttpGrpcAccessLogConfig 的日志类型。其次，在访问日志配置中，我们必须引用 gRPC 服务器。 …","relpermalink":"/envoy-handbook/logging/lab13/","summary":"本实验涵盖了如何配置 Envoy 使用独立的 gRPC 访问日志服务（ALS）。我们将使用一个基本的 gRPC 服务器，它实现了 StreamAccessLogs 函数，并将收到的日志从 Envoy 输出到标准输出。 让我们先把 ALS 服务器作为一个 Docker 容器来运行。 docker run -dit -p 5000:5000 gcr.io/tetratelabs/envoy-als:0.1.0 ALS 服务器默","title":"实验 13：使用 gRPC 访问日志服务（ALS）记录日志"},{"content":"在这个实验中，我们将演示如何使用断路器。我们将运行一个 Python HTTP 服务器和一个 Envoy 代理在它前面。要启动在 8000 端口监听的 Python 服务器，请运行：\npython3 -m http.server 8000 接下来，我们将用下面的断路器创建 Envoy 配置：\n...circuit_breakers:thresholds:max_connections:20max_requests:100max_pending_requests:20因此，如果我们超过了 20 个连接或 100 个请求或 20 个待处理请求，断路器就会断开。下面是完整的 Envoy 配置：\nstatic_resources:listeners:- name:listener_0address:socket_address:address:0.0.0.0port_value:10000filter_chains:- filters:- name:envoy.filters.network.http_connection_managertyped_config:\u0026#34;@type\u0026#34;: …","relpermalink":"/envoy-handbook/cluster/lab7/","summary":"在这个实验中，我们将演示如何使用断路器。我们将运行一个 Python HTTP 服务器和一个 Envoy 代理在它前面。要启动在 8000 端口监听的 Python 服务器，请运行： python3 -m http.server 8000 接下来，我们将用下面的断路器创建 Envoy 配置： ...circuit_bre","title":"实验7：断路器"},{"content":"本地持久化卷允许用户通过标准 PVC 接口以简单便携的方式访问本地存储。PV 中包含系统用于将 Pod 安排到正确节点的节点亲和性信息。\n一旦配置了本地卷，外部静态配置器（provisioner）可用于帮助简化本地存储管理。请注意，本地存储配置器与大多数配置器不同，并且尚不支持动态配置。相反，它要求管理员预先配置每个节点上的本地卷，并且这些卷应该是：\n Filesystem volumeMode（默认）PV—— 将它们挂载到发现目录下。 Block volumeMode PV——在发现目录下为节点上的块设备创建一个符号链接。  配置器将通过为每个卷创建和清除 PersistentVolumes 来管理发现目录下的卷。\n配置要求  本地卷插件希望路径稳定，包括在重新启动时和添加或删除磁盘时。 静态配置器仅发现挂载点（对于文件系统模式卷）或符号链接（对于块模式卷）。对于基于目录的本地卷必须绑定到发现目录中。  版本兼容性 推荐配置器版本与Kubernetes版本\n   Provisioner version K8s version Reason     2.1.0 1.10 Beta …","relpermalink":"/kubernetes-handbook/storage/local-persistent-storage/","summary":"本地持久化卷允许用户通过标准 PVC 接口以简单便携的方式访问本地存储。PV 中包含系统用于将 Pod 安排到正确节点的节点亲和性信息。 一旦配置了本地卷，外部静态配置器（provisioner）可用于帮助简化本地存储管","title":"本地持久化存储"},{"content":"自定义资源是对 Kubernetes API 的扩展，Kubernetes 中的每个资源都是一个 API 对象的集合，例如我们在 YAML 文件里定义的那些 spec 都是对 Kubernetes 中的资源对象的定义，所有的自定义资源可以跟 Kubernetes 中内建的资源一样使用 kubectl 操作。\n自定义资源 Kubernetes 从 1.6 版本开始包含一个内建的资源叫做 TPR（ThirdPartyResource），可以用它来创建自定义资源，但该资源在 Kubernetes 1.7 版本开始已被 CRD（CustomResourceDefinition）取代。\n扩展 API 自定义资源实际上是为了扩展 Kubernetes 的 API，向 Kubernetes API 中增加新类型，可以使用以下三种方式：\n 修改 Kubernetes 的源码，显然难度比较高，也不太合适 创建自定义 API server 并聚合到 API 中  编写自定义资源是扩展 Kubernetes API 的最简单的方式，是否编写自定义资源来扩展 API 请参考 Should I add a …","relpermalink":"/kubernetes-handbook/extend/custom-resource/","summary":"自定义资源是对 Kubernetes API 的扩展，Kubernetes 中的每个资源都是一个 API 对象的集合，例如我们在 YAML 文件里定义的那些 spec 都是对 Kubernetes 中的资源对象的定义，所有的自定义资源可以跟 Kubernetes 中内建的资源一样使用 kubectl 操作。 自定义资源","title":"使用自定义资源扩展 API"},{"content":"本文是如何创建 CRD 来扩展 Kubernetes API 的教程。CRD 是用来扩展 Kubernetes 最常用的方式，在 Service Mesh 和 Operator 中也被大量使用。因此读者如果想在 Kubernetes 上做扩展和开发的话，是十分有必要了解 CRD 的。\n在阅读本文前您需要先了解使用自定义资源扩展 API， 以下内容译自 Kubernetes 官方文档，有删改，推荐阅读如何从零开始编写一个 Kubernetes CRD。\n创建 CRD（CustomResourceDefinition） 创建新的 CustomResourceDefinition（CRD）时，Kubernetes API Server 会为您指定的每个版本创建新的 RESTful 资源路径。CRD 可以是命名空间的，也可以是集群范围的，可以在 CRD scope 字段中所指定。与现有的内置对象一样，删除命名空间会删除该命名空间中的所有自定义对象。CustomResourceDefinition 本身是非命名空间的，可供所有命名空间使用。\n参考下面的 CRD， …","relpermalink":"/kubernetes-handbook/extend/crd/","summary":"本文是如何创建 CRD 来扩展 Kubernetes API 的教程。CRD 是用来扩展 Kubernetes 最常用的方式，在 Service Mesh 和 Operator 中也被大量使用。因此读者如果想在 Kubernetes 上做扩展和开发的话，是十分有必要了解 CRD 的。 在阅读本文前您需要先了解使用自定义资源扩展 API","title":"使用 CRD 扩展 Kubernetes API"},{"content":"Aggregated（聚合的）API server 是为了将原来的 API server 这个巨石（monolithic）应用给拆分成，为了方便用户开发自己的 API server 集成进来，而不用直接修改 kubernetes 官方仓库的代码，这样一来也能将 API server 解耦，方便用户使用实验特性。这些 API server 可以跟 core API server 无缝衔接，使用 kubectl 也可以管理它们。\n架构 我们需要创建一个新的组件，名为 kube-aggregator，它需要负责以下几件事：\n 提供用于注册 API server 的 API 汇总所有的 API server 信息 代理所有的客户端到 API server 的请求  注意：这里说的 API server 是一组 “API Server”，而不是说我们安装集群时候的那个 API server，而且这组 API server 是可以横向扩展的。\n安装配置聚合的 API server 有两种方式来启用 kube-aggregator：\n 使用 test mode/single-user mode，作为 …","relpermalink":"/kubernetes-handbook/extend/aggregated-api-server/","summary":"Aggregated（聚合的）API server 是为了将原来的 API server 这个巨石（monolithic）应用给拆分成，为了方便用户开发自己的 API server 集成进来，而不用直接修改 kubernetes 官方仓库的代码，这样一来也能将 API server 解耦，方便用","title":"Aggregated API Server"},{"content":"APIService 是用来表示一个特定的 GroupVersion 的中的 server，它的结构定义位于代码 staging/src/k8s.io/kube-aggregator/pkg/apis/apiregistration/types.go 中。\n下面是一个 APIService 的示例配置：\napiVersion:apiregistration.k8s.io/v1beta1kind:APIServicemetadata:name:v1alpha1.custom-metrics.metrics.k8s.iospec:insecureSkipTLSVerify:truegroup:custom-metrics.metrics.k8s.iogroupPriorityMinimum:1000versionPriority:5service:name:apinamespace:custom-metricsversion:v1alpha1APIService 详解 使用 apiregistration.k8s.io/v1beta1 版本的 APIService， …","relpermalink":"/kubernetes-handbook/extend/apiservice/","summary":"APIService 是用来表示一个特定的 GroupVersion 的中的 server，它的结构定义位于代码 staging/src/k8s.io/kube-aggregator/pkg/apis/apiregistration/types.go 中。 下面是一个 APIService 的示例配置： apiVersion:apiregistration.k8s.io/v1beta1kind:APIServicemetadata:name:v1alpha1.custom-metrics.metrics.k8s.iospec:insecureSkipTLSVerify:truegroup:custom-metrics.metrics.k8s.iogroupPriorityMinimum:1000versionPriority:5service:name:apinamespace:custom-metricsversion:v1alpha1APIService 详解 使用 apiregistration.k8s.io/v1beta1 版本的 APIService，在 metadata.name 中定义该 API 的名字。 使用上面的 yaml 的创建 v1alpha1.custom-metrics.metrics.k8s.io APIService。 i","title":"APIService"},{"content":"服务目录（Service Catalog）是 Kubernetes 的扩展 API，它使运行在 Kubernetes 集群中的应用程序可以轻松使用外部托管软件产品，例如由云提供商提供的数据存储服务。\n它提供列表清单、提供 (provision) 和绑定 (binding) 来自服务代理（Service Brokers）的外部托管服务，而不需要关心如何创建或管理这些服务的详细情况。\n由 Open Service Broker API 规范定义的 Service broker 是由第三方提供和维护的一组托管服务的端点 (endpoint)，该第三方可以是 AWS，GCP 或 Azure 等云提供商。\n托管服务可以是 Microsoft Azure Cloud Queue，Amazon Simple Queue Service 和 Google Cloud Pub/Sub 等，它们可以是应用可以使用的提供的各种软件。\n通过 Service Catalog，集群运营者可以浏览由 Service Broker 提供的托管服务列表，提供的托管服务实例，并与其绑定，使其可被 Kubernetes 集 …","relpermalink":"/kubernetes-handbook/extend/service-catalog/","summary":"服务目录（Service Catalog）是 Kubernetes 的扩展 API，它使运行在 Kubernetes 集群中的应用程序可以轻松使用外部托管软件产品，例如由云提供商提供的数据存储服务。 它提供列表清单、提供 (provision) 和绑定 (binding) 来自服务代理（Ser","title":"服务目录（Service Catalog）"},{"content":"2020 年初，Kubernetes 社区提议 Multi-Cluster Services API，旨在解决长久以来就存在的 Kubernetes 多集群服务管理问题。\nKubernetes 用户可能希望将他们的部署分成多个集群，但仍然保留在这些集群中运行的工作负载之间的相互依赖关系，这有很多原因。今天，集群是一个硬边界，一个服务对远程的 Kubernetes 消费者来说是不透明的，否则就可以利用元数据（如端点拓扑结构）来更好地引导流量。为了支持故障转移或在迁移过程中的临时性，用户可能希望消费分布在各集群中的服务，但今天这需要非复杂的定制解决方案。\n多集群服务 API 旨在解决这些问题。\n参考  KEP-1645: Multi-Cluster Services API - github.com  ","relpermalink":"/kubernetes-handbook/multi-cluster/multi-cluster-services-api/","summary":"2020 年初，Kubernetes 社区提议 Multi-Cluster Services API，旨在解决长久以来就存在的 Kubernetes 多集群服务管理问题。 Kubernetes 用户可能希望将他们的部署分成多个集群，但仍然保留在这些集群中运行的工作负载之间的相互依赖关系，这有很多原","title":"多集群服务 API（Multi-Cluster Services API）"},{"content":"Kubernetes 从 1.8 版本起就声称单集群最多可支持 5000 个节点和 15 万个 Pod，我相信很少有公司会部署如此庞大的一个单集群，总有很多情况下因为各种各样的原因我们可能会部署多个集群，但是有时候有想将他们统一起来管理，这时候就需要用到集群联邦（Federation）。\n为什么要使用集群联邦 Federation 使管理多个集群变得简单。它通过提供两个主要构建模块来实现：\n 跨集群同步资源：Federation 提供了在多个集群中保持资源同步的能力。例如，可以保证同一个 deployment 在多个集群中存在。 跨集群服务发现：Federation 提供了自动配置 DNS 服务以及在所有集群后端上进行负载均衡的能力。例如，可以提供一个全局 VIP 或者 DNS 记录，通过它可以访问多个集群后端。  Federation 还可以提供一些其它用例：\n 高可用：通过在集群间分布负载并自动配置 DNS 服务和负载均衡，federation 最大限度地减少集群故障的影响。 避免厂商锁定：通过更简单的跨集群应用迁移方式，federation 可以防止集群厂商锁定。 …","relpermalink":"/kubernetes-handbook/multi-cluster/federation/","summary":"Kubernetes 从 1.8 版本起就声称单集群最多可支持 5000 个节点和 15 万个 Pod，我相信很少有公司会部署如此庞大的一个单集群，总有很多情况下因为各种各样的原因我们可能会部署多个集群，但是有时候有想将他们统一起来管理，这时候就","title":"集群联邦（Cluster Federation）"},{"content":"EDS 只是 Envoy 中众多的服务发现方式的一种。要想了解 EDS 首先我们需要先知道什么是 Endpoint。\nEndpoint\nEndpoint 即上游主机标识。它的数据结构如下：\n{ \u0026#34;address\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;health_check_config\u0026#34;: \u0026#34;{...}\u0026#34; } 其中包括端点的地址和健康检查配置。详情请参考 Endpoints。\n终端发现服务（EDS）是一个基于 gRPC 或 REST-JSON API 服务器的 xDS 管理服务，在 Envoy 中用来获取集群成员。集群成员在 Envoy 的术语中被称为“终端”。对于每个集群，Envoy 都会通过发现服务来获取成员的终端。由于以下几个原因，EDS 是首选的服务发现机制：\n Envoy 对每个上游主机都有明确的了解（与通过 DNS 解析的负载均衡进行路由相比而言），并可以做出更智能的负载均衡决策。 在每个主机的发现 API 响应中携带的额外属性通知 Envoy 负载均衡权重、金丝雀状态、区域等。这些附加属性在负载均衡、统计信息收集等过程中会被 Envoy 网格全局使用。  Envoy 提供了 Java …","relpermalink":"/istio-handbook/data-plane/envoy-eds/","summary":"EDS 只是 Envoy 中众多的服务发现方式的一种。要想了解 EDS 首先我们需要先知道什么是 Endpoint。 Endpoint Endpoint 即上游主机标识。它的数据结构如下： { \"address\": \"{...}\", \"health_check_config\": \"{...}\" } 其中包括端点的地址和健康检查配置。详情请参考 Endpoints","title":"EDS（端点发现服务）"},{"content":"在 Pod 中的 init 容器启动之后，就向 Pod 中增加了 iptables 规则，本文将向你介绍 Istio 中的基于 iptables 的透明流量劫持的全过程。\n查看 iptables nat 表中注入的规则 Init 容器通过向 iptables nat 表中注入转发规则来劫持流量的，下图显示的是 productpage 服务中的 iptables 流量劫持的详细过程。\n   Envoy sidecar 流量劫持流程示意图  Init 容器启动时命令行参数中指定了 REDIRECT 模式，因此只创建了 NAT 表规则，接下来我们查看下 NAT 表中创建的规则，这是全文中的重点部分，前面讲了那么多都是为它做铺垫的。\nproductpage 访问 reviews Pod，入站流量处理过程对应于图示上的步骤：1、2、3、4、Envoy Inbound Handler、5、6、7、8、应用容器。\nreviews Pod 访问 rating 服务的出站流量处理过程对应于图示上的步骤是：9、10、11、12、Envoy Outbound Handler、13、14、15。\n上图中关于流 …","relpermalink":"/istio-handbook/concepts/transparent-traffic-hijacking/","summary":"在 Pod 中的 init 容器启动之后，就向 Pod 中增加了 iptables 规则，本文将向你介绍 Istio 中的基于 iptables 的透明流量劫持的全过程。 查看 iptables nat 表中注入的规则 Init 容器通过向 iptables nat 表中注入转发规则来劫持流量的，下图显示的是 productpage 服务中的 iptables 流量劫持的详","title":"Istio 中的透明流量劫持过程详解"},{"content":"Sidecar 描述了 sidecar 代理的配置，该代理负责协调与它所连接的工作负载实例的 inbound 和 outbound 通信。默认情况下，Istio 会对网格中的所有 sidecar 代理进行必要的配置，以达到网格中的每个工作负载实例，并接受与工作负载相关的所有端口的流量。Sidecar 配置提供了一种方法来微调代理在转发工作负载的流量时将接受的一组端口和协议。此外，还可以限制代理在转发工作负载实例的 outbound 流量时可以到达的服务集。\n网格中的服务和配置被组织到一个或多个命名空间（例如，Kubernetes 命名空间）。命名空间中的 Sidecar 配置将适用于同一命名空间中的一个或多个工作负载实例，并通过 workloadSelector 字段进行选择。在没有 workloadSelector 的情况下，它将适用于同一命名空间的所有工作负载实例。在确定应用于工作负载实例的 Sidecar 配置时，将优先考虑具有选择该工作负载实例的 workloadSelector 的资源，而不是没有任何 workloadSelector 的 Sidecar 配置。\n注意事项 在 …","relpermalink":"/istio-handbook/config-networking/sidecar/","summary":"Sidecar 描述了 sidecar 代理的配置，该代理负责协调与它所连接的工作负载实例的 inbound 和 outbound 通信。默认情况下，Istio 会对网格中的所有 sidecar 代理进行必要的配置，以达到网格中的每个工作负载实例，并接受与工作负载相关的所有端口的流","title":"Sidecar"},{"content":"TLS 监听器过滤器让我们可以检测到传输的是 TLS 还是明文。 如果传输是 TLS，它会检测服务器名称指示（SNI）和 / 或客户端的应用层协议协商（ALPN）。\n什么是 SNI？\nSNI 或服务器名称指示（Server Name Indication）是对 TLS 协议的扩展，它告诉我们在 TLS 握手过程的开始，哪个主机名正在连接。我们可以使用 SNI 在同一个 IP 地址和端口上提供多个 HTTPS 服务（使用不同的证书）。如果客户端以主机名 “hello.com” 进行连接，服务器可以出示该主机名的证书。同样地，如果客户以 “example.com” 连接，服务器就会提供该证书。\n什么是 ALPN？\nALPN 或应用层协议协商是对 TLS 协议的扩展，它允许应用层协商应该在安全连接上执行哪种协议，而无需进行额外的往返请求。使用 ALPN，我们可以确定客户端使用的是 HTTP/1.1 还是 HTTP/2。\n我们可以使用 SNI 和 ALPN 值来匹配过滤器链，使用 server_names（对于 SNI）和 / 或 application_protocols（对于 ALPN）字 …","relpermalink":"/envoy-handbook/listener/tls-inspector-listener-filter/","summary":"TLS 监听器过滤器让我们可以检测到传输的是 TLS 还是明文。 如果传输是 TLS，它会检测服务器名称指示（SNI）和 / 或客户端的应用层协议协商（ALPN）。 什么是 SNI？ SNI 或服务器名称指示（Server Name Indic","title":"TLS 检查器监听器过滤器"},{"content":"分接式过滤器（Tap Filter）的目的是根据一些匹配的属性来记录 HTTP 流量。有两种方法来配置分接式过滤器。\n  使用 Envoy 配置里面的 static_config 字段\n  使用 admin_config 字段并指定配置 ID。\n  不同的是，我们在静态配置（static_config）中一次性提供所有东西——匹配配置和输出配置。当使用管理配置（admin_config）时，我们只提供配置 ID，然后在运行时使用 /tap 管理端点来配置过滤器。\n正如我们所提到的，过滤器的配置被分成两部分：匹配配置和输出配置。\n我们可以用匹配配置指定匹配谓词，告诉分接式过滤器要分接哪些请求并写入配置的输出。\n例如，下面的片段显示了如何使用 any_match 来匹配所有的请求，无论其属性如何。\ncommon_config:static_config:match:any_match:true...我们也有一个选项，可以在请求和响应 Header、Trailer 和正文上进行匹配。\nHeader/Trailer 匹配 Header/Trailer …","relpermalink":"/envoy-handbook/admin-interface/tap-filter/","summary":"分接式过滤器（Tap Filter）的目的是根据一些匹配的属性来记录 HTTP 流量。有两种方法来配置分接式过滤器。 使用 Envoy 配置里面的 static_config 字段 使用 admin_config 字段并指定配置 ID。 不同的是，我们在静态配置（static_conf","title":"分接式过滤器"},{"content":"在这个实验中，我们将学习如何将 Envoy 应用日志发送到 Google Cloud Logging。我们将在 GCP 中运行的虚拟机（VM）实例上运行 Envoy，配置 Envoy 实例，将应用日志发送到 GCP 中的云日志。配置 Envoy 实例将允许我们在日志资源管理器中查看 Envoy 的日志，获得日志分析，并使用其他谷歌云功能。\n为了使用谷歌云的日志收集、分析和其他工具，我们需要安装云日志代理（Ops Agent）。\n在这个演示中，我们将在一个单独的虚拟机上安装 Ops 代理。另外，请注意，其他云供应商可能使用不同的日志工具和服务。\n安装 Ops 代理 在 Google Cloud，在你的地区创建一个新的虚拟机实例。一旦创建了虚拟机，我们就可以通过 SSH 进入该实例并安装 Ops 代理。\n在虚拟机实例中，运行以下命令来安装 Ops 代理。\ncurl -sSO https://dl.google.com/cloudagents/add-google-cloud-ops-agent-repo.sh sudo bash …","relpermalink":"/envoy-handbook/logging/lab14/","summary":"在这个实验中，我们将学习如何将 Envoy 应用日志发送到 Google Cloud Logging。我们将在 GCP 中运行的虚拟机（VM）实例上运行 Envoy，配置 Envoy 实例，将应用日志发送到 GCP 中的云日志。配置 Envoy 实例将允许我们在日志资源管理器中","title":"实验 14：将 Envoy 的日志发送到 Google Cloud Logging"},{"content":"Kubernetes 作为一个容器编排调度引擎，资源调度是它的最基本也是最重要的功能，这一节中我们将着重讲解 Kubernetes 中是如何做资源调度的。\nKubernetes 中有一个叫做 kube-scheduler 的组件，该组件就是专门监听 kube-apiserver 中是否有还未调度到 node 上的 pod，再通过特定的算法为 pod 指定分派 node 运行。\nKubernetes 中的众多资源类型，例如 Deployment、DaemonSet、StatefulSet 等都已经定义了 Pod 运行的一些默认调度策略，但是如果我们细心的根据 node 或者 pod 的不同属性，分别为它们打上标签之后，我们将发现 Kubernetes 中的高级调度策略是多么强大。当然如果要实现动态的资源调度，即 pod 已经调度到某些节点上后，因为一些其它原因，想要让 pod 重新调度到其它节点。\n考虑以下两种情况：\n 集群中有新增节点，想要让集群中的节点的资源利用率比较均衡一些，想要将一些高负载的节点上的 pod 驱逐到新增节点上，这是 kuberentes 的 scheduler 所 …","relpermalink":"/kubernetes-handbook/cluster/scheduling/","summary":"Kubernetes 作为一个容器编排调度引擎，资源调度是它的最基本也是最重要的功能，这一节中我们将着重讲解 Kubernetes 中是如何做资源调度的。 Kubernetes 中有一个叫做 kube-scheduler 的组件，该组件就是专门监听 kube-apiserver 中是否有还未调度到 node 上的 pod，再通过特定的","title":"资源调度"},{"content":"QoS（Quality of Service），大部分译为“服务质量等级”，又译作“服务质量保证”，是作用在 Pod 上的一个配置，当 Kubernetes 创建一个 Pod 时，它就会给这个 Pod 分配一个 QoS 等级，可以是以下等级之一：\n Guaranteed：Pod 里的每个容器都必须有内存/CPU 限制和请求，而且值必须相等。 Burstable：Pod 里至少有一个容器有内存或者 CPU 请求且不满足 Guarantee 等级的要求，即内存/CPU 的值设置的不同。 BestEffort：容器必须没有任何内存或者 CPU 的限制或请求。  该配置不是通过一个配置项来配置的，而是通过配置 CPU/内存的 limits 与 requests 值的大小来确认服务质量等级的。使用 kubectl get pod -o yaml 可以看到 pod 的配置输出中有 qosClass 一项。该配置的作用是为了给资源调度提供策略支持，调度算法根据不同的服务质量等级可以确定将 pod 调度到哪些节点上。\n例如，下面这个 YAML 配置中的 Pod …","relpermalink":"/kubernetes-handbook/cluster/qos/","summary":"QoS（Quality of Service），大部分译为“服务质量等级”，又译作“服务质量保证”，是作用在 Pod 上的一个配置，当 Kubernetes 创建一个 Pod 时，它就会给这个 Pod 分配一个 QoS 等级，可以是以下等级之一： Guarant","title":"服务质量等级（QoS）"},{"content":"当你使用 Kubernetes 的时候，有没有遇到过 Pod 在启动后一会就挂掉然后又重新启动这样的恶性循环？你有没有想过 Kubernetes 是如何检测 pod 是否还存活？虽然容器已经启动，但是 Kubernetes 如何知道容器的进程是否准备好对外提供服务了呢？让我们通过 Kubernetes 官网的这篇文章 Configure Liveness and Readiness Probes，来一探究竟。\n本文将展示如何配置容器的存活和可读性探针。\nKubelet 使用 liveness probe（存活探针）来确定何时重启容器。例如，当应用程序处于运行状态但无法做进一步操作，liveness 探针将捕获到 deadlock，重启处于该状态下的容器，使应用程序在存在 bug 的情况下依然能够继续运行下去（谁的程序还没几个 bug 呢）。\nKubelet 使用 readiness probe（就绪探针）来确定容器是否已经就绪可以接受流量。只有当 Pod 中的容器都处于就绪状态时 kubelet 才会认定该 Pod 处于就绪状态。该信号的作用是控制哪些 Pod 应该作为 service …","relpermalink":"/kubernetes-handbook/config/liveness-readiness-probes/","summary":"当你使用 Kubernetes 的时候，有没有遇到过 Pod 在启动后一会就挂掉然后又重新启动这样的恶性循环？你有没有想过 Kubernetes 是如何检测 pod 是否还存活？虽然容器已经启动，但是 Kubernetes 如何知道容器的进程是否准备好对外提供服务了呢？让我们通过 Kubernetes","title":"配置 Pod 的 liveness 和 readiness 探针"},{"content":"Service account 为 Pod 中的进程提供身份信息。\n本文是关于 Service Account 的用户指南，管理指南另见 Service Account 的集群管理指南 。\n注意：本文档描述的关于 Service Account 的行为只有当您按照 Kubernetes 项目建议的方式搭建起集群的情况下才有效。您的集群管理员可能在您的集群中有自定义配置，这种情况下该文档可能并不适用。\n当您（真人用户）访问集群（例如使用kubectl命令）时，apiserver 会将您认证为一个特定的 User Account（目前通常是admin，除非您的系统管理员自定义了集群配置）。Pod 容器中的进程也可以与 apiserver 联系。 当它们在联系 apiserver 的时候，它们会被认证为一个特定的 Service Account（例如default）。\n使用默认的 Service Account 访问 API server 当您创建 pod 的时候，如果您没有指定一个 service account，系统会自动得在与该pod 相同的 namespace 下为其指派一 …","relpermalink":"/kubernetes-handbook/config/service-account/","summary":"Service account 为 Pod 中的进程提供身份信息。 本文是关于 Service Account 的用户指南，管理指南另见 Service Account 的集群管理指南 。 注意：本文档描述的关于 Service Account 的行为只有当您按照 Kubernetes 项目建议的方式搭建起集群的情况下才有效。您的集群管理员可能在您的集","title":"配置 Pod 的 Service Account"},{"content":"Secret 对象类型用来保存敏感信息，例如密码、OAuth 令牌和 ssh key。将这些信息放在 secret 中比放在 pod 的定义中或者 docker 镜像中来说更加安全和灵活。\nSecret 概览 Secret 是一种包含少量敏感信息例如密码、token 或 key 的对象。这样的信息可能会被放在 Pod spec 中或者镜像中；将其放在一个 secret 对象中可以更好地控制它的用途，并降低意外暴露的风险。\n用户可以创建 secret，同时系统也创建了一些 secret。\n要使用 secret，pod 需要引用 secret。Pod 可以用两种方式使用 secret：作为 volume 中的文件被挂载到 pod 中的一个或者多个容器里，或者当 kubelet 为 pod 拉取镜像时使用。\n内置 secret Service Account 使用 API 凭证自动创建和附加 secret Kubernetes 自动创建包含访问 API 凭据的 secret，并自动修改您的 pod 以使用此类型的 secret。\n如果需要，可以禁用或覆盖自动创建和使用API凭据。但是，如果您 …","relpermalink":"/kubernetes-handbook/config/secret/","summary":"Secret 对象类型用来保存敏感信息，例如密码、OAuth 令牌和 ssh key。将这些信息放在 secret 中比放在 pod 的定义中或者 docker 镜像中来说更加安全和灵活。 Secret 概览 Secret 是一种包含少量敏感信息例如密码、token 或 key 的对象。这样的信","title":"Secret 配置"},{"content":"当用多个团队或者用户共用同一个集群的时候难免会有资源竞争的情况发生，这时候就需要对不同团队或用户的资源使用配额做出限制。\n开启资源配额限制功能 目前有两种资源分配管理相关的控制策略插件 ResourceQuota 和 LimitRange。\n要启用它们只要 API Server 的启动配置的 KUBE_ADMISSION_CONTROL 参数中加入了 ResourceQuota 的设置，这样就给集群开启了资源配额限制功能，加入 LimitRange 可以用来限制一个资源申请的范围限制，参考 为 namesapce 配置默认的内存请求与限额 和 在 namespace 中配置默认的CPU请求与限额。\n两种控制策略的作用范围都是对于某一 namespace，ResourceQuota 用来限制 namespace 中所有的 Pod 占用的总的资源 request 和 limit，而 LimitRange 是用来设置 namespace 中 Pod 的默认的资源 request 和 limit 值。\n资源配额分为三种类型：\n 计算资源配额 存储资源配额 对象数量配额  关于资源配额的详细信息 …","relpermalink":"/kubernetes-handbook/config/quota/","summary":"当用多个团队或者用户共用同一个集群的时候难免会有资源竞争的情况发生，这时候就需要对不同团队或用户的资源使用配额做出限制。 开启资源配额限制功能 目前有两种资源分配管理相关的控制策略插件 ResourceQuota 和 LimitRan","title":"管理 namespace 中的资源配额"},{"content":"对于没有使用过 Kubernetes 的 Docker 用户，如何快速掌握 kubectl 命令？\n在本文中，我们将向 docker-cli 用户介绍 Kubernetes 命令行如何与 api 进行交互。该命令行工具——kubectl，被设计成 docker-cli 用户所熟悉的样子，但是它们之间又存在一些必要的差异。该文档将向您展示每个 docker 子命令和 kubectl 与其等效的命令。\n在使用 kubernetes 集群的时候，docker 命令通常情况是不需要用到的，只有在调试程序或者容器的时候用到，我们基本上使用 kubectl 命令即可，所以在操作 kubernetes 的时候我们抛弃原先使用 docker 时的一些观念。\ndocker run 如何运行一个 nginx Deployment 并将其暴露出来？ 查看 kubectl run 。\n使用 docker 命令：\n$ docker run -d --restart=always -e DOMAIN=cluster --name nginx-app -p 80:80 nginx …","relpermalink":"/kubernetes-handbook/cli/docker-cli-to-kubectl/","summary":"对于没有使用过 Kubernetes 的 Docker 用户，如何快速掌握 kubectl 命令？ 在本文中，我们将向 docker-cli 用户介绍 Kubernetes 命令行如何与 api 进行交互。该命令行工具——kubectl，被设计成 docker-cli 用户所熟悉的样子，但是它们之间又存在一些必要的差异。该文档","title":"Docker 用户过渡到 kubectl 命令行指南"},{"content":"Kubernetes 提供的 kubectl 命令是与集群交互最直接的方式，v1.6 版本的 kubectl 命令参考图如下：\n   kubectl cheatsheet  Kubectl 的子命令主要分为 8 个类别：\n 基础命令（初学者都会使用的） 基础命令（中级） 部署命令 集群管理命令 故障排查和调试命令 高级命令 设置命令 其他命令  熟悉这些命令有助于大家来操作和管理 kubernetes 集群。\n命令行提示 为了使用 kubectl 命令更加高效，我们可以选择安装一下开源软件来增加操作 kubectl 命令的快捷方式，同时为 kubectl 命令增加命令提示。\n   增加kubeclt命令的工具（图片来自网络）   kubectx：用于切换 Kubernetes context kube-ps1：为命令行终端增加$PROMPT字段 kube-shell：交互式带命令提示的 kubectl 终端  全部配置完成后的 kubectl 终端如下图所示：\n   增强的kubectl命令  开源项目 kube-shell 可以为 kubectl 提供自动的命令提示和补全，使用起来特 …","relpermalink":"/kubernetes-handbook/cli/using-kubectl/","summary":"Kubernetes 提供的 kubectl 命令是与集群交互最直接的方式，v1.6 版本的 kubectl 命令参考图如下： kubectl cheatsheet Kubectl 的子命令主要分为 8 个类别： 基础命令（初学者都会使用的） 基础命令（中级） 部署命令 集群管理命令 故障排查和调试命令 高级命令 设置命","title":"Kubectl 命令概览"},{"content":"EnvoyFilter 提供了一种机制来定制 Istio Pilot 生成的 Envoy 配置。使用 EnvoyFilter 来修改某些字段的值，添加特定的过滤器，甚至添加全新的 listener、cluster 等。这个功能必须谨慎使用，因为不正确的配置可能破坏整个网格的稳定性。与其他 Istio 网络对象不同，EnvoyFilter 是累加应用。对于特定命名空间中的特定工作负载，可以存在任意数量的 EnvoyFilter。这些 EnvoyFilter 的应用顺序如下：配置根命名空间中的所有 EnvoyFilter，其次是工作负载命名空间中的所有匹配 EnvoyFilter。\n注意一 该 API 的某些方面与 Istio 网络子系统的内部实现以及 Envoy 的 xDS API 有很深的关系。虽然 EnvoyFilter API 本身将保持向后兼容，但通过该机制提供的任何 Envoy 配置应在 Istio 代理版本升级时仔细审查，以确保废弃的字段被适当地删除和替换。\n注意二 当多个 EnvoyFilter 被绑定到特定命名空间的同一个工作负载时，所有补丁将按照创建顺序处理。 …","relpermalink":"/istio-handbook/config-networking/envoy-filter/","summary":"EnvoyFilter 提供了一种机制来定制 Istio Pilot 生成的 Envoy 配置。使用 EnvoyFilter 来修改某些字段的值，添加特定的过滤器，甚至添加全新的 listener、cluster 等。这个功能必须谨慎使用，因为不正确的配置可能破坏整个网格的稳定性。与","title":"EnvoyFilter"},{"content":"本文以 Istio 官方的 bookinfo 示例来讲解在进入 Pod 的流量被 iptables 转交给 Envoy sidecar 后，Envoy 是如何做路由转发的，详述了 Inbound 和 Outbound 处理过程。\n下面是 Istio 官方提供的 bookinfo 的请求流程图，假设 bookinfo 应用的所有服务中没有配置 DestinationRule。\n   Bookinfo 示例  我们将要解析的是 reviews-v1 这个 Pod 中的 Inbound 和 Outbound 流量。\n理解 Inbound Handler Inbound Handler 的作用是将 iptables 拦截到的 downstream 的流量转发给 Pod 内的应用程序容器。在我们的实例中，假设其中一个 Pod 的名字是 reviews-v1-545db77b95-jkgv2，运行 istioctl proxy-config listener reviews-v1-545db77b95-jkgv2 --port 15006 查看该 Pod 中 15006 端口上的监听器情况 ，你将 …","relpermalink":"/istio-handbook/concepts/sidecar-traffic-routing-deep-dive/","summary":"本文以 Istio 官方的 bookinfo 示例来讲解在进入 Pod 的流量被 iptables 转交给 Envoy sidecar 后，Envoy 是如何做路由转发的，详述了 Inbound 和 Outbound 处理过程。 下面是 Istio 官方提供的 bookinfo 的请求流程图，假设 bookinfo 应用的所有服务中没有配置 DestinationRu","title":"Istio 中的 Sidecar 的流量路由详解"},{"content":"SDS（秘钥发现服务）是 Envoy 1.8.0 版本起开始引入的服务。可以在 bootstrap.static_resource 的 secret 配置中为 Envoy 指定 TLS 证书（secret）。也可以通过秘钥发现服务（SDS）远程获取。Istio 预计将在 1.1 版本中支持 SDS。\nSDS 带来的最大的好处就是简化证书管理。要是没有该功能的话，我们就必须使用 Kubernetes 中的 secret 资源创建证书，然后把证书挂载到代理容器中。如果证书过期，还需要更新 secret 和需要重新部署代理容器。使用 SDS，中央 SDS 服务器将证书推送到所有 Envoy 实例上。如果证书过期，服务器只需将新证书推送到 Envoy 实例，Envoy 可以立即使用新证书而无需重新部署。\n如果 listener server 需要从远程获取证书，则 listener server 不会被标记为 active 状态，在获取证书之前不会打开其端口。如果 Envoy 由于连接失败或错误的响应数据而无法获取证书，则 listener server 将被标记为 active，并且打开端 …","relpermalink":"/istio-handbook/data-plane/envoy-sds/","summary":"SDS（秘钥发现服务）是 Envoy 1.8.0 版本起开始引入的服务。可以在 bootstrap.static_resource 的 secret 配置中为 Envoy 指定 TLS 证书（secret）。也可以通过秘钥发现服务（SDS）远程获取。Istio 预计将在 1.1 版本中支持 SDS。 SDS 带来的最大的好处就","title":"SDS（秘钥发现服务）"},{"content":"在前面，我们了解了如何利用流量的比例（weight 字段）在多个子集之间进行流量路由。在某些情况下，纯粹的基于权重的流量路由或分割已经足够了。然而，在有些场景和情况下，我们可能需要对流量如何被分割和转发到目标服务进行更细化的控制。\nIstio 允许我们使用传入请求的一部分，并将其与定义的值相匹配。例如，我们可以匹配传入请求的 URI前缀，并基于此路由流量。\n   属性 描述     uri 将请求 URI 与指定值相匹配   schema 匹配请求的 schema（HTTP、HTTPS…）   method 匹配请求的 method（GET、POST…）   authority 匹配请求 authority 头   headers 匹配请求头。头信息必须是小写的，并以连字符分隔（例如：x-my-request-id）。注意，如果我们使用头信息进行匹配，其他属性将被忽略（uri、schema、method、authority）。    上述每个属性都可以用这些方法中的一种进行匹配：\n  精确匹配：例如，exact: \u0026#34;value\u0026#34; 匹配精确的字符串\n  前缀匹配：例如，prefix: …","relpermalink":"/istio-handbook/traffic-management/advanced-routing/","summary":"在前面，我们了解了如何利用流量的比例（weight 字段）在多个子集之间进行流量路由。在某些情况下，纯粹的基于权重的流量路由或分割已经足够了。然而，在有些场景和情况下，我们可能需要对流量如何被分割和转发","title":"高级路由"},{"content":"/healthcheck/fail 可用于失败的入站健康检查。端点 /healthcheck/ok 用于恢复失败的端点。\n这两个端点都需要使用 HTTP 健康检查过滤器。我们可能会在关闭服务器前或进行完全重启时使用它来排空服务器。当调用失败的健康检查选项时，所有的健康检查都将失败，无论其配置如何。\n","relpermalink":"/envoy-handbook/admin-interface/healthchecks/","summary":"/healthcheck/fail 可用于失败的入站健康检查。端点 /healthcheck/ok 用于恢复失败的端点。 这两个端点都需要使用 HTTP 健康检查过滤器。我们可能会在关闭服务器前或进行完全重启时使用它来排空服务器。当调用失败的健康检查选项时，所有的健康检查都将失","title":"健康检查"},{"content":"在这个实验中，我们将学习如何配置原始目的地过滤器。要做到这一点，我们需要启用 IP 转发，然后更新 iptables 规则，以捕获所有流量并将其重定向到 Envoy 正在监听的端口。\n我们将使用一个 Linux 虚拟机，而不是 Google Cloud Shell。\n让我们从启用 IP 转发开始。\n# 启用IP转发功能 sudo sysctl -w net.ipv4.ip_forward=1 接下来，我们需要配置 iptables 来捕获所有发送到 80 端口的流量，并将其重定向到 10000 端口。Envoy 代理将在 10000 端口进行监听。\n首先，我们需要确定我们将在 iptables 命令中使用的网络接口名称。我们可以使用 ip link show 命令列出网络接口。例如：\njimmy@instance-1:~$ ip link show 1: lo: \u0026lt; LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 link/loopback …","relpermalink":"/envoy-handbook/listener/lab9/","summary":"在这个实验中，我们将学习如何配置原始目的地过滤器。要做到这一点，我们需要启用 IP 转发，然后更新 iptables 规则，以捕获所有流量并将其重定向到 Envoy 正在监听的端口。 我们将使用一个 Linux 虚拟机，而不是 Google Cloud Shell。 让我们从","title":"实验 9：原始目的地过滤器"},{"content":"kubectl 命令是操作 Kubernetes 集群的最直接和最高效的途径，这个60多 MB 大小的二进制文件，到底有啥能耐呢？\nKubectl 自动补全 $ source \u0026lt;(kubectl completion bash) # setup autocomplete in bash, bash-completion package should be installed first. $ source \u0026lt;(kubectl completion zsh) # setup autocomplete in zsh Kubectl 上下文和配置 设置 kubectl 命令交互的 kubernetes 集群并修改配置信息。参阅 使用 kubeconfig 文件进行跨集群验证 获取关于配置文件的详细信息。\n$ kubectl config view # 显示合并后的 kubeconfig 配置 # 同时使用多个 kubeconfig 文件并查看合并后的配置 $ KUBECONFIG=~/.kube/config:~/.kube/kubconfig2 kubectl config view #  …","relpermalink":"/kubernetes-handbook/cli/kubectl-cheatsheet/","summary":"kubectl 命令是操作 Kubernetes 集群的最直接和最高效的途径，这个60多 MB 大小的二进制文件，到底有啥能耐呢？ Kubectl 自动补全 $ source \u003c(kubectl completion bash) # setup autocomplete in bash, bash-completion package should be installed first. $ source \u003c(kubectl completion zsh) # setup autocomplete in zsh Kubectl 上下文和配置 设置 kubectl 命令交互的 kubernetes 集群并修改配置信息","title":"Kubectl 命令技巧大全"},{"content":"Kubenretes1.6 中使用 etcd V3 版本的 API，使用 etcdctl 直接 ls 的话只能看到 /kube-centos 一个路径。需要在命令前加上 ETCDCTL_API=3 这个环境变量才能看到 kuberentes 在 etcd 中保存的数据。\nETCDCTL_API=3 etcdctl get /registry/namespaces/default -w=json|python -m json.tool 如果是使用 kubeadm 创建的集群，在 Kubenretes 1.11 中，etcd 默认使用 tls ，这时你可以在 master 节点上使用以下命令来访问 etcd ：\nETCDCTL_API=3 etcdctl --cacert=/etc/kubernetes/pki/etcd/ca.crt \\ --cert=/etc/kubernetes/pki/etcd/peer.crt \\ --key=/etc/kubernetes/pki/etcd/peer.key \\ get /registry/namespaces/default -w=json | …","relpermalink":"/kubernetes-handbook/cli/etcdctl/","summary":"Kubenretes1.6 中使用 etcd V3 版本的 API，使用 etcdctl 直接 ls 的话只能看到 /kube-centos 一个路径。需要在命令前加上 ETCDCTL_API=3 这个环境变量才能看到 kuberentes 在 etcd 中保存的数据。 ETCDCTL_API=3 etcdctl get /registry/namespaces/default -w=json|python -m json.tool 如果是使用 kubeadm 创建的集群，在 Kubenretes 1.11 中，etcd 默认使用 tls ，这时你可以在","title":"使用 etcdctl 访问 Kubernetes 数据"},{"content":"在使用二进制文件部署 Kubernetes 集群的时候，很多人在进行到部署证书时遇到各种各样千奇百怪的问题，这一步是创建集群的基础，我们有必要详细了解一下其背后的流程和原理。\n概览 每个 Kubernetes 集群都有一个集群根证书颁发机构（CA）。 集群中的组件通常使用 CA 来验证 API server 的证书，由 API 服务器验证 kubelet 客户端证书等。为了支持这一点，CA 证书包被分发到集群中的每个节点，并作为一个 secret 附加分发到默认 service account 上。 或者，你的 workload 可以使用此 CA 建立信任。 你的应用程序可以使用类似于 ACME 草案的协议，使用 certificates.k8s.io API 请求证书签名。\n集群中的 TLS 信任 让 Pod 中运行的应用程序信任集群根 CA 通常需要一些额外的应用程序配置。 您将需要将 CA 证书包添加到 TLS 客户端或服务器信任的 CA 证书列表中。 例如，您可以使用 golang TLS 配置通过解析证书链并将解析的证书添加到 tls.Config …","relpermalink":"/kubernetes-handbook/security/managing-tls-in-a-cluster/","summary":"在使用二进制文件部署 Kubernetes 集群的时候，很多人在进行到部署证书时遇到各种各样千奇百怪的问题，这一步是创建集群的基础，我们有必要详细了解一下其背后的流程和原理。 概览 每个 Kubernetes 集群都有一个集群根证书颁发机构（CA）","title":"管理集群中的 TLS"},{"content":"Kubelet 的 HTTPS 端点对外暴露了用于访问不同敏感程度数据的 API，并允许您在节点或者容器内执行不同权限级别的操作。\n本文档向您描述如何通过认证授权来访问 kubelet 的 HTTPS 端点。\nKubelet 认证 默认情况下，所有未被配置的其他身份验证方法拒绝的，对 kubelet 的 HTTPS 端点的请求将被视为匿名请求，并被授予 system:anonymous 用户名和 system:unauthenticated 组。\n如果要禁用匿名访问并发送 401 Unauthorized 的未经身份验证的请求的响应：\n 启动 kubelet 时指定 --anonymous-auth=false 标志  如果要对 kubelet 的 HTTPS 端点启用 X509 客户端证书身份验证：\n 启动 kubelet 时指定 --client-ca-file 标志，提供 CA bundle 以验证客户端证书 启动 apiserver 时指定 --kubelet-client-certificate 和 --kubelet-client-key 标志 参阅 apiserver 认 …","relpermalink":"/kubernetes-handbook/security/kubelet-authentication-authorization/","summary":"Kubelet 的 HTTPS 端点对外暴露了用于访问不同敏感程度数据的 API，并允许您在节点或者容器内执行不同权限级别的操作。 本文档向您描述如何通过认证授权来访问 kubelet 的 HTTPS 端点。 Kubelet 认证 默认情况下，所有未被配置的其他身份验证方法拒","title":"Kublet 的认证授权"},{"content":"本文档介绍如何为 kubelet 设置 TLS 客户端证书引导（bootstrap）。\nKubernetes 1.4 引入了一个用于从集群级证书颁发机构（CA）请求证书的 API。此 API 的原始目的是为 kubelet 提供 TLS 客户端证书。可以在 这里 找到该提议，在 feature #43 追踪该功能的进度。\nkube-apiserver 配置 您必须提供一个 token 文件，该文件中指定了至少一个分配给 kubelet 特定 bootstrap 组的 “bootstrap token”。\n该组将作为 controller manager 配置中的默认批准控制器而用于审批。随着此功能的成熟，您应该确保 token 被绑定到基于角色的访问控制（RBAC）策略上，该策略严格限制了与证书配置相关的客户端请求（使用 bootstrap token）。使用 RBAC，将 token 范围划分为组可以带来很大的灵活性（例如，当您配置完成节点后，您可以禁用特定引导组的访问）。\nToken 认证文件 Token 可以是任意的，但应该可以表示为从安全随机数生成器（ …","relpermalink":"/kubernetes-handbook/security/tls-bootstrapping/","summary":"本文档介绍如何为 kubelet 设置 TLS 客户端证书引导（bootstrap）。 Kubernetes 1.4 引入了一个用于从集群级证书颁发机构（CA）请求证书的 API。此 API 的原始目的是为 kubelet 提供 TLS 客户端证书。可以在 这里 找到该提议，在 feature #43 追踪该功","title":"TLS Bootstrap"},{"content":"本文将讲述如何配置和启用 ip-masq-agent。\n创建 ip-masq-agent 要创建 ip-masq-agent，运行下面的 kubectl 命令：\nkubectl create -f https://raw.githubusercontent.com/kubernetes-incubator/ip-masq-agent/master/ip-masq-agent.yaml 关于 ip-masq-agent 的更多信息请参考 该文档。\n在大多数情况下，默认的一套规则应该是足够的；但是，如果内置的规则不适用于您的集群，您可以创建并应用 ConfigMap 来自定义受影响的 IP 范围。例如，为了仅允许 ip-masq-agent 考虑 10.0.0.0/8，您可以在名为 “config” 的文件中创建以下 ConfigMap。\nnonMasqueradeCIDRs:- 10.0.0.0/8resyncInterval:60s注意：重要的是，该文件被命名为 config，因为默认情况下，该文件将被用作 ip-masq-agent 查找的关键字。\n运行下列命令将 ConfigMap …","relpermalink":"/kubernetes-handbook/security/ip-masq-agent/","summary":"本文将讲述如何配置和启用 ip-masq-agent。 创建 ip-masq-agent 要创建 ip-masq-agent，运行下面的 kubectl 命令： kubectl create -f https://raw.githubusercontent.com/kubernetes-incubator/ip-masq-agent/master/ip-masq-agent.yaml 关于 ip-masq-agent 的更多信息请参考 该文档。 在大多数情况下，默认的一套规则应该是足够的；但是，如","title":"IP 伪装代理"},{"content":"当我们安装好集群后，如果想要把 kubectl 命令交给用户使用，就不得不对用户的身份进行认证和对其权限做出限制。\n下面以创建一个 devuser 用户并将其绑定到 dev 和 test 两个 namespace 为例说明。\n创建 CA 证书和秘钥 创建 devuser-csr.json 文件\n{ \u0026#34;CN\u0026#34;: \u0026#34;devuser\u0026#34;, \u0026#34;hosts\u0026#34;: [], \u0026#34;key\u0026#34;: { \u0026#34;algo\u0026#34;: \u0026#34;rsa\u0026#34;, \u0026#34;size\u0026#34;: 2048 }, \u0026#34;names\u0026#34;: [ { \u0026#34;C\u0026#34;: \u0026#34;CN\u0026#34;, \u0026#34;ST\u0026#34;: \u0026#34;BeiJing\u0026#34;, \u0026#34;L\u0026#34;: \u0026#34;BeiJing\u0026#34;, \u0026#34;O\u0026#34;: \u0026#34;k8s\u0026#34;, \u0026#34;OU\u0026#34;: \u0026#34;System\u0026#34; } ] } 生成 CA 证书和私钥\n下面我们在 master 节点上为 devuser 创建证书和秘钥，在 /etc/kubernetes/ssl 目录下执行以下命令：\n执行该命令前请先确保该目录下已经包含如下文件：\nca-key.pem ca.pem ca-config.json devuser-csr.json $ cfssl gencert -ca=ca.pem …","relpermalink":"/kubernetes-handbook/security/kubectl-user-authentication-authorization/","summary":"当我们安装好集群后，如果想要把 kubectl 命令交给用户使用，就不得不对用户的身份进行认证和对其权限做出限制。 下面以创建一个 devuser 用户并将其绑定到 dev 和 test 两个 namespace 为例说明。 创建 CA 证书和秘钥 创建 devuser-csr.json 文件 { \"CN\": \"devuser\", \"hosts\": [], \"key\": { \"algo\": \"rsa\", \"size\": 2048","title":"创建用户认证授权的 kubeconfig 文件"},{"content":"在开启了 TLS 的集群中，每当与集群交互的时候少不了的是身份认证，使用 kubeconfig（即证书） 和 token 两种认证方式是最简单也最通用的认证方式，在 dashboard 的登录功能就可以使用这两种登录功能。\n下文分两块以示例的方式来讲解两种登陆认证方式：\n 为 brand 命名空间下的 brand 用户创建 kubeconfig 文件 为集群的管理员（拥有所有命名空间的 amdin 权限）创建 token  使用 kubeconfig 如何生成kubeconfig文件请参考创建用户认证授权的kubeconfig文件。\n 注意我们生成的 kubeconfig 文件中没有 token 字段，需要手动添加该字段。\n 比如我们为 brand namespace 下的 brand 用户生成了名为 brand.kubeconfig 的 kubeconfig 文件，还要再该文件中追加一行 token 的配置（如何生成 token 将在下文介绍），如下所示：\n   kubeconfig文件  对于访问 dashboard 时候的使用 kubeconfig 文件 …","relpermalink":"/kubernetes-handbook/security/auth-with-kubeconfig-or-token/","summary":"在开启了 TLS 的集群中，每当与集群交互的时候少不了的是身份认证，使用 kubeconfig（即证书） 和 token 两种认证方式是最简单也最通用的认证方式，在 dashboard 的登录功能就可以使用这两种登录功能。 下文分两块以示例的方式","title":"使用 kubeconfig 或 token 进行用户身份认证"},{"content":"在安装集群的时候我们在 master 节点上生成了一堆证书、token，还在 kubelet 的配置中用到了 bootstrap token，安装各种应用时，为了能够与 API server 通信创建了各种 service account，在 Dashboard 中使用了 kubeconfig 或 token 登陆，那么这些都属于什么认证方式？如何区分用户的？我特地翻译了下这篇官方文档，想你看了之后你将找到答案。\n重点查看 bearer token 和 HTTP 认证中的 token 使用，我们已经有所应用，如 使用kubeconfig或token进行用户身份认证。\n认识 Kubernetes 中的用户 Kubernetes 集群中包含两类用户：一类是由 Kubernetes 管理的 service account，另一类是普通用户。\n普通用户被假定为由外部独立服务管理。管理员分发私钥，用户存储（如 Keystone 或 Google 帐户），甚至包含用户名和密码列表的文件。在这方面，Kubernetes 没有代表普通用户帐户的对象。无法通过 API 调用的方式向集群中添加普通用户。\n …","relpermalink":"/kubernetes-handbook/security/authentication/","summary":"在安装集群的时候我们在 master 节点上生成了一堆证书、token，还在 kubelet 的配置中用到了 bootstrap token，安装各种应用时，为了能够与 API server 通信创建了各种 service account，在 Dashboard 中使用了 kubeconfig 或 token 登陆，那么这些都属于什么认证","title":"Kubernetes 中的用户与身份认证授权"},{"content":"本文是对 Kubernetes 集群安全性管理的最佳实践。\n端口 请注意管理好以下端口。\n   端口 进程 描述     4149/TCP kubelet 用于查询容器监控指标的 cAdvisor 端口   10250/TCP kubelet 访问节点的 API 端口   10255/TCP kubelet 未认证的只读端口，允许访问节点状态   10256/TCP kube-proxy kube-proxy 的健康检查服务端口   9099/TCP calico-felix calico 的健康检查服务端口（如果使用 calico/canal）   6443/TCP kube-apiserver Kubernetes API 端口    Kubernetes 安全扫描工具 kube-bench kube-bench 可以消除大约 kubernetes 集群中 95％的配置缺陷。通过应用 CIS Kubernetes Benchmark 来检查 master 节点、node 节点及其控制平面组件，从而确保集群设置了特定安全准则。在经历特定的 Kubernetes 安全问题或安全增强功能 …","relpermalink":"/kubernetes-handbook/security/kubernetes-security-best-practice/","summary":"本文是对 Kubernetes 集群安全性管理的最佳实践。 端口 请注意管理好以下端口。 端口 进程 描述 4149/TCP kubelet 用于查询容器监控指标的 cAdvisor 端口 10250/TCP kubelet 访问节点的 API 端口 10255/TCP kubelet 未认证的只读端口，允许访问节点状态 10256/TCP kube-proxy kube-proxy 的健康检查服务端口 9099/TCP calico-felix calico 的健康","title":"Kubernetes 集群安全性配置最佳实践"},{"content":"虽然 Envoy 本质上采用了最终一致性模型，但 ADS 提供了对 API 更新推送进行排序的机会，并确保单个管理服务器对 Envoy 节点的 API 更新具有亲和力。ADS 允许管理服务器在单个双向 gRPC 流上传递一个或多个 API 及其资源。否则，一些 API（如 RDS 和 EDS）可能需要管理多个流并连接到不同的管理服务器。\nADS 通过适当得排序 xDS 可以无中断的更新 Envoy 的配置。例如，假设 foo.com 已映射到集群 X。我们希望将路由表中将该映射更改为在集群 Y。为此，必须首先提供 X、Y 这两个集群的 CDS/EDS 更新。\n如果没有 ADS，CDS/EDS/RDS 流可能指向不同的管理服务器，或者位于需要协调的不同 gRPC流连接的同一管理服务器上。EDS 资源请求可以跨两个不同的流分开，一个用于 X，一个用于 Y。ADS 将这些流合并到单个流和单个管理服务器，从而无需分布式同步就可以正确地对更新进行排序。使用 ADS，管理服务器将在单个流上提供 CDS、EDS 和 RDS 更新。\n 下一章   ","relpermalink":"/istio-handbook/data-plane/envoy-ads/","summary":"虽然 Envoy 本质上采用了最终一致性模型，但 ADS 提供了对 API 更新推送进行排序的机会，并确保单个管理服务器对 Envoy 节点的 API 更新具有亲和力。ADS 允许管理服务器在单个双向 gRPC 流上传递一个或多个 API 及其资源。否则，一些 API（","title":"ADS（聚合发现服务）"},{"content":"ProxyConfig 暴露了代理级别的配置选项。ProxyConfig 可以在每个工作负载、命名空间或整个网格的基础上进行配置。ProxyConfig 不是一个必要的资源；有默认值。\n注意：ProxyConfig 中的字段不是动态配置的——更改需要重新启动工作负载才能生效。\n对于任何命名空间，包括根配置命名空间，只有一个无工作负载选择器的 ProxyConfig 资源是有效的。\n对于带有工作负载选择器的资源，只有一个选择任何特定工作负载的资源才有效。\n对于网格级别的配置，请将资源放在你的 Istio 安装的根配置命名空间中，而不使用工作负载选择器：\napiVersion:networking.istio.io/v1beta1kind:ProxyConfigmetadata:name:my-proxyconfignamespace:istio-systemspec:concurrency:0image:type:distroless对于命名空间级别的配置，将资源放在所需的命名空间中，不需要工作负载选择器： …","relpermalink":"/istio-handbook/config-networking/proxy-config/","summary":"ProxyConfig 暴露了代理级别的配置选项。ProxyConfig 可以在每个工作负载、命名空间或整个网格的基础上进行配置。ProxyConfig 不是一个必要的资源；有默认值。 注意：ProxyConfig 中的字段不是动","title":"ProxyConfig"},{"content":"通过 ServiceEntry 资源，我们可以向 Istio 的内部服务注册表添加额外的条目，并使不属于我们网格的外部服务或内部服务看起来像是我们服务网格的一部分。\n当一个服务在服务注册表中时，我们就可以使用流量路由、故障注入和其他网格功能，就像我们对其他服务一样。\n下面是一个 ServiceEntry 资源的例子，它声明了一个可以通过 HTTPS 访问的外部 API（api.external-svc.com）。\napiVersion:networking.istio.io/v1alpha3kind:ServiceEntrymetadata:name:external-svcspec:hosts:- api.external-svc.comports:- number:443name:httpsprotocol:TLSresolution:DNSlocation:MESH_EXTERNALhosts 字段可以包含多个外部 API，在这种情况下，Envoy sidecar 会根据下面的层次结构来进行检查。如果任何一项不能被检查，Envoy 就会转到层次结构中的下一项。\n HTTP …","relpermalink":"/istio-handbook/traffic-management/seviceentry/","summary":"通过 ServiceEntry 资源，我们可以向 Istio 的内部服务注册表添加额外的条目，并使不属于我们网格的外部服务或内部服务看起来像是我们服务网格的一部分。 当一个服务在服务注册表中时，我们就可以使用流量路由、故障注入和其他网格功能","title":"ServiceEntry"},{"content":"在这个实验中，我们将看一个例子，说明我们如何设置 Envoy，以便在一个 IP 地址上用不同的证书为多个网站服务。\n我们将使用自签名的证书进行测试，但如果使用真实签名的证书，其过程是相同的。\n使用 openssl，我们将在 certs 文件夹中为 www.hello.com 和 www.example.com 创建自签名证书。\n$ mkdir certs \u0026amp;\u0026amp; cd certs $ openssl req -nodes -new -x509 -keyout www_hello_com.key -out www_hello_com.cert -subj \u0026#34;/C=US/ST=Washington/L=Seattle/O=Hello LLC/OU=Org/CN=www.hello.com\u0026#34; $ openssl req -nodes -new -x509 -keyout www_example_com.key -out www_example_com.cert -subj \u0026#34;/C=US/ST=Washington/L=Seattle/O=Example …","relpermalink":"/envoy-handbook/listener/lab10/","summary":"在这个实验中，我们将看一个例子，说明我们如何设置 Envoy，以便在一个 IP 地址上用不同的证书为多个网站服务。 我们将使用自签名的证书进行测试，但如果使用真实签名的证书，其过程是相同的。 使用 openssl，","title":"实验 10：TLS 检查器过滤器"},{"content":"在这个实验中，我们将展示如何使用和配置 HTTP 分接式过滤器。我们将配置一个 /error 路由，它返回一个直接的响应，其主体为 error 值。然后，在分接式过滤器中，我们将配置匹配器，以匹配任何响应主体包含 error 字符串和请求头 debug: true 的请求。如果这两个条件都为真，那么我们就分接该请求并将输出写入一个前缀为 tap_debug 的文件。\n让我们从创建匹配配置开始。我们将使用两个匹配器，一个用来匹配请求头（http_request_headers_match），另一个用来匹配响应体（http_response_generic_body_match）。我们将用逻辑上的 AND 来组合这两个条件。\n下面是匹配配置的样子。\n- name:envoy.filters.http.taptyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.http.tap.v3.Tapcommon_config:static_config:match:and_match:rules:- …","relpermalink":"/envoy-handbook/admin-interface/lab15/","summary":"在这个实验中，我们将展示如何使用和配置 HTTP 分接式过滤器。我们将配置一个 /error 路由，它返回一个直接的响应，其主体为 error 值。然后，在分接式过滤器中，我们将配置匹配器，以匹配任何响应主体包含 error 字符串和请求头 debug: true 的请","title":"实验15：使用 HTTP 分接式过滤器"},{"content":"本文列举了集中访问 Kubernetes 集群的方式。\n第一次使用 kubectl 访问 如果您是第一次访问 Kubernetes API 的话，我们建议您使用 Kubernetes 命令行工具：kubectl。\n为了访问集群，您需要知道集群的地址，并且需要有访问它的凭证。通常，如果您完成了入门指南那么这些将会自动设置，或者其他人为您部署的集群提供并给您凭证和集群地址。\n使用下面的命令检查 kubectl 已知的集群的地址和凭证：\n$ kubectl config view 直接访问 REST API Kubectl 处理对 apiserver 的定位和认证。如果您想直接访问 REST API，可以使用像 curl、wget 或浏览器这样的 http 客户端，有以下几种方式来定位和认证：\n 以 proxy 模式运行 kubectl。  推荐方法。 使用已保存的 apiserver 位置信息。 使用自签名证书验证 apiserver 的身份。 没有 MITM（中间人攻击）的可能。 认证到 apiserver。 将来，可能会做智能的客户端负载均衡和故障转移。   直接向 http 客户端提 …","relpermalink":"/kubernetes-handbook/access/methods/","summary":"本文列举了集中访问 Kubernetes 集群的方式。 第一次使用 kubectl 访问 如果您是第一次访问 Kubernetes API 的话，我们建议您使用 Kubernetes 命令行工具：kubectl。 为了访问集群，您需要知道集群的地址，并且需要有访问它的凭证。通常，如果您完成了入","title":"访问集群"},{"content":"Kubernetes 的认证方式对于不同的人来说可能有所不同。\n 运行 kubelet 可能有一种认证方式（即证书）。 用户可能有不同的认证方式（即令牌）。 管理员可能具有他们为个人用户提供的证书列表。 我们可能有多个集群，并希望在同一个地方将其全部定义——这样用户就能使用自己的证书并重用相同的全局配置。  所以为了能够让用户轻松地在多个集群之间切换，对于多个用户的情况下，我们将其定义在了一个 kubeconfig 文件中。\n此文件包含一系列与昵称相关联的身份验证机制和集群连接信息。它还引入了一个（用户）认证信息元组和一个被称为上下文的与昵称相关联的集群连接信息的概念。\n如果明确指定，则允许使用多个 kubeconfig 文件。在运行时，它们与命令行中指定的覆盖选项一起加载并合并（参见下面的 规则）。\n相关讨论 http://issue.k8s.io/1755\nKubeconfig 文件的组成 Kubeconifg 文件示例 current-context:federal-contextapiVersion:v1clusters:- …","relpermalink":"/kubernetes-handbook/access/authenticate-across-clusters-kubeconfig/","summary":"Kubernetes 的认证方式对于不同的人来说可能有所不同。 运行 kubelet 可能有一种认证方式（即证书）。 用户可能有不同的认证方式（即令牌）。 管理员可能具有他们为个人用户提供的证书列表。 我们可能有多个集群，并希望在同一个地方将其","title":"使用 kubeconfig 文件配置跨集群认证"},{"content":"本页向您展示如何使用 kubectl port-forward 命令连接到运行在 Kubernetes 集群中的 Redis 服务器。这种类型的连接对于数据库调试很有帮助。\n创建一个 Pod 来运行 Redis 服务器   创建一个 Pod：\nkubectl create -f https://k8s.io/docs/tasks/access-application-cluster/redis-master.yaml 命令运行成功后将有以下输出验证该 Pod 是否已经创建：\npod \u0026#34;redis-master\u0026#34; created   检查 Pod 是否正在运行且处于就绪状态：\nkubectl get pods 当 Pod 就绪，输出显示 Running 的状态：\nNAME READY STATUS RESTARTS AGE redis-master 2/2 Running 0 41s   验证 Redis 服务器是否已在 Pod 中运行，并监听 6379 端口：\n{% raw %} kubectl get pods redis-master --template=\u0026#39;{{(index …","relpermalink":"/kubernetes-handbook/access/connecting-to-applications-port-forward/","summary":"本页向您展示如何使用 kubectl port-forward 命令连接到运行在 Kubernetes 集群中的 Redis 服务器。这种类型的连接对于数据库调试很有帮助。 创建一个 Pod 来运行 Redis 服务器 创建一个 Pod： kubectl create -f https://k8s.io/docs/tasks/access-application-cluster/redis-master.yaml 命令运行成功后将有以下输出验证该 Pod 是否已经创建： pod \"redis-master\" created","title":"通过端口转发访问集群中的应用程序"},{"content":"本文向您展示如何创建 Kubernetes Service 对象，外部客户端可以使用它来访问集群中运行的应用程序。该 Service 可以为具有两个运行实例的应用程序提供负载均衡。\n目的  运行 Hello World 应用程序的两个实例。 创建一个暴露 node 节点端口的 Service 对象。 使用 Service 对象访问正在运行的应用程序。  为在两个 pod 中运行的应用程序创建 service   在集群中运行 Hello World 应用程序：\nkubectl run hello-world --replicas=2 --labels=\u0026#34;run=load-balancer-example\u0026#34; --image=gcr.io/google-samples/node-hello:1.0 --port=8080 上述命令创建一个 Deployment 对象和一个相关联的 ReplicaSet 对象。该 ReplicaSet 有两个 Pod，每个 Pod 中都运行一个 Hello World 应用程序。\n  显示关于该 Deployment 的信息：\nkubectl get …","relpermalink":"/kubernetes-handbook/access/service-access-application-cluster/","summary":"本文向您展示如何创建 Kubernetes Service 对象，外部客户端可以使用它来访问集群中运行的应用程序。该 Service 可以为具有两个运行实例的应用程序提供负载均衡。 目的 运行 Hello World 应用程序的两个实例。 创建一个暴露 node 节点端口的 Service 对象。 使用 Service 对","title":"使用 service 访问群集中的应用程序"},{"content":"前面几节讲到如何访问 Kubernetes 集群，本文主要讲解访问 Kubernetes 中的 Pod 和 Serivce 的几种方式，包括如下几种：\n hostNetwork hostPort NodePort LoadBalancer Ingress  说是暴露 Pod 其实跟暴露 Service 是一回事，因为 Pod 就是 Service 的后端。\nhostNetwork: true 这是一种直接定义 Pod 网络的方式。\n如果在 Pod 中使用 hostNotwork:true 配置的话，在这种 pod 中运行的应用程序可以直接看到 pod 启动的主机的网络接口。在主机的所有网络接口上都可以访问到该应用程序。以下是使用主机网络的 pod 的示例定义：\napiVersion:v1kind:Podmetadata:name:influxdbspec:hostNetwork:truecontainers:- name:influxdbimage:influxdb部署该 Pod：\n$ kubectl create -f influxdb-hostnetwork.yml 访问该 pod …","relpermalink":"/kubernetes-handbook/access/accessing-kubernetes-pods-from-outside-of-the-cluster/","summary":"前面几节讲到如何访问 Kubernetes 集群，本文主要讲解访问 Kubernetes 中的 Pod 和 Serivce 的几种方式，包括如下几种： hostNetwork hostPort NodePort LoadBalancer Ingress 说是暴露 Pod 其实跟暴露 Service 是一回事，因为 Pod 就是 Service 的后端。 hostNetwork: true 这是一种直接定义 Pod 网络的方式。 如果在 Pod 中使用 hostNotwork:true 配置的","title":"从外部访问 Kubernetes 中的 Pod"},{"content":"Lens 是一款开源的 Kubenretes IDE，也可以作为桌面客户端，官方网站 https://k8slens.dev，具有以下特性：\n 完全开源，GitHub 地址 https://github.com/lensapp/lens 实时展示集群状态 内置 Prometheus 监控 多集群，多个 namespace 管理 原生 Kubernetes 支持 支持使用 chart 安装应用 使用 kubeconfig 登陆认证 支持多平台，Windows、Mac、Linux Visual Studio Code 友好的风格设计  Lens 界面图下图所示。\n   Lens Kubernetes IDE 界面  参考  Lens, Kubernetes IDE - k8slens.dev  ","relpermalink":"/kubernetes-handbook/access/lens/","summary":"Lens 是一款开源的 Kubenretes IDE，也可以作为桌面客户端，官方网站 https://k8slens.dev，具有以下特性： 完全开源，GitHub 地址 https://github.com/lensapp/lens 实时展示集群状态 内置 Prometheus 监控 多集群，多个 namespace 管理 原生 Kubernetes 支持 支持使用 chart","title":"Lens - Kubernetes IDE"},{"content":"Kubernator 相较于 Kubernetes Dashboard 来说，是一个更底层的 Kubernetes UI，Dashboard 操作的都是 Kubernetes 的底层对象，而 Kubernator 是直接操作 Kubernetes 各个对象的 YAML 文件。\nKubernator 提供了一种基于目录树和关系拓扑图的方式来管理 Kubernetes 的对象的方法，用户可以在 Web 上像通过 GitHub 的网页版一样操作 Kubernetes 的对象，执行修改、拷贝等操作，详细的使用方式见 https://github.com/smpio/kubernator。\n安装 Kubernator Kubernator 的安装十分简单，可以直接使用 kubectl 命令来运行，它不依赖任何其它组件。\nkubectl create ns kubernator kubectl -n kubernator run --image=smpio/kubernator --port=80 kubernator kubectl -n kubernator expose deploy …","relpermalink":"/kubernetes-handbook/access/kubernator-kubernetes-ui/","summary":"Kubernator 相较于 Kubernetes Dashboard 来说，是一个更底层的 Kubernetes UI，Dashboard 操作的都是 Kubernetes 的底层对象，而 Kubernator 是直接操作 Kubernetes 各个对象的 YAML 文件。 Kubernator 提供了一种基于目录树和关系拓扑图的方式来管理 Kubernetes 的对象的方法，用户可以在 Web 上像通过 GitHub","title":"Kubernator - 更底层的 Kubernetes UI"},{"content":"本文讲解了如何开发容器化应用，并使用 Wercker 持续集成工具构建 docker 镜像上传到 docker 镜像仓库中，然后在本地使用 docker-compose 测试后，再使用 kompose 自动生成 kubernetes 的 yaml 文件，再将注入 Envoy sidecar 容器，集成 Istio 服务网格中的详细过程。\n整个过程如下图所示。\n   流程图  为了讲解详细流程，我特意写了用 Go 语言开发的示例程序放在 GitHub 中，模拟监控流程：\n k8s-app-monitor-test：生成模拟的监控数据，在接收到 http 请求返回 json 格式的 metrics 信息 K8s-app-monitor-agent：获取监控 metrics 信息并绘图，访问该服务将获得监控图表  API 文档见 k8s-app-monitor-test 中的 api.html 文件，该文档在 API blueprint 中定义，使用 aglio 生成，打开后如图所示：\n   API  关于服务发现 K8s-app-monitor-agent …","relpermalink":"/kubernetes-handbook/devops/deploy-applications-in-kubernetes/","summary":"本文讲解了如何开发容器化应用，并使用 Wercker 持续集成工具构建 docker 镜像上传到 docker 镜像仓库中，然后在本地使用 docker-compose 测试后，再使用 kompose 自动生成 kubernetes 的 yaml 文件，再将注入 Envoy sidecar 容器，集成 Istio 服务网格中的详细过程。 整个过程如下图所示。 流","title":"适用于 Kubernetes 的应用开发部署流程"},{"content":"默认情况下，注入的 sidecar 代理的配置方式是，它们接受所有端口的流量，并且在转发流量时可以到达网格中的任何服务。\n在某些情况下，你可能想改变这种配置，配置代理，所以它只能使用特定的端口和访问某些服务。要做到这一点，你可以在 Istio 中使用 Sidecar 资源。\nSidecar 资源可以被部署到 Kubernetes 集群内的一个或多个命名空间，但如果没有定义工作负载选择器，每个命名空间只能有一个 sidecar 资源。\nSidecar 资源由三部分组成，一个工作负载选择器、一个入口（ingress）监听器和一个出口（egress）监听器。\n工作负载选择器 工作负载选择器决定了哪些工作负载会受到 sidecar 配置的影响。你可以决定控制一个命名空间中的所有 sidecar，而不考虑工作负载，或者提供一个工作负载选择器，将配置只应用于特定的工作负载。\n例如，这个 YAML 适用于默认命名空间内的所有代理，因为没有定义选择器。 …","relpermalink":"/istio-handbook/traffic-management/sidecar/","summary":"默认情况下，注入的 sidecar 代理的配置方式是，它们接受所有端口的流量，并且在转发流量时可以到达网格中的任何服务。 在某些情况下，你可能想改变这种配置，配置代理，所以它只能使用特定的端口和访问某些服务。要做到这一","title":"Sidecar"},{"content":"在这个实验中，我们将学习如何使用 TLS 检查器过滤器来选择一个特定的过滤器链。单个过滤器链将根据 transport_protocol 和 application_protocol将流量分配到不同的上游集群。\n我们将为我们的上游主机使用 mendhak/http-https-echo Docker 镜像。这些容器可以被配置为监听 HTTP/HTTPS 并将响应回传。\n我们将运行镜像的三个实例来代表非 TLS HTTP、TLS HTTP/1.1 和 TLS HTTP/2 协议。\n# non-TLS HTTP docker run -dit -p 8080:8080 -t mendhak/http-https-echo:18 # TLS HTTP1.1 docker run -dit -e HTTPS_PORT=443 -p 443:443 -t mendhak/http-https-echo:18 # TLS HTTP2 docker run -dit -e HTTPS_PORT=8443 -p 8443:8443 -t mendhak/http-https-echo:18 为了确保这 …","relpermalink":"/envoy-handbook/listener/lab11/","summary":"在这个实验中，我们将学习如何使用 TLS 检查器过滤器来选择一个特定的过滤器链。单个过滤器链将根据 transport_protocol 和 application_protocol将流量分配到不同的上游集群。 我们将为我们的上游主机使用 mendhak/http-https-echo Docker 镜像。这","title":"实验 11：匹配传输和应用协议"},{"content":"本文档不是说明如何在 kubernetes 中开发和部署应用程序，如果您想要直接开发应用程序在 kubernetes 中运行可以参考 适用于kubernetes的应用开发部署流程。\n本文旨在说明如何将已有的应用程序尤其是传统的分布式应用程序迁移到 kubernetes 中。如果该类应用程序符合云原生应用规范（如12因素法则）的话，那么迁移会比较顺利，否则会遇到一些麻烦甚至是阻碍。具体请参考 迁移至云原生应用架构。\n下图是将单体应用迁移到云原生的步骤。\n   将单体应用迁移到云原生(图片来自 DevOpsDay Toronto)  接下来我们将以 Spark on YARN with kubernetes 为例来说明，该例子足够复杂也很有典型性，了解了这个例子可以帮助大家将自己的应用迁移到 kubernetes 集群上去。\n下图即整个架构的示意图，所有的进程管理和容器扩容直接使用 Makefile。\n   spark on yarn with kubernetes  注意： 该例子仅用来说明具体的步骤划分和复杂性，在生产环境应用还有待验证，请谨慎使用。 …","relpermalink":"/kubernetes-handbook/devops/migrating-hadoop-yarn-to-kubernetes/","summary":"本文档不是说明如何在 kubernetes 中开发和部署应用程序，如果您想要直接开发应用程序在 kubernetes 中运行可以参考 适用于kubernetes的应用开发部署流程。 本文旨在说明如何将已有的应用程序尤其是传统的分布式应用程序迁移到 kubernetes","title":"迁移传统应用到 Kubernetes 步骤详解——以 Hadoop YARN 为例"},{"content":"StatefulSet 这个对象是专门用来部署用状态应用的，可以为 Pod 提供稳定的身份标识，包括 hostname、启动顺序、DNS 名称等。\n下面以在 Kubernetes1.6 版本中部署 zookeeper 和 kafka 为例讲解 StatefulSet 的使用，其中 kafka 依赖于 zookeeper。\nDockerfile 和配置文件见 zookeeper 和 kafka。\n注：所有的镜像基于 CentOS 系统的 JDK 制作，为我的私人镜像，外部无法访问，yaml 中没有配置持久化存储。\n部署 Zookeeper Dockerfile 中从远程获取 zookeeper 的安装文件，然后在定义了三个脚本：\n zkGenConfig.sh：生成 zookeeper 配置文件 zkMetrics.sh：获取 zookeeper 的 metrics zkOk.sh：用来做 ReadinessProb  我们在来看下这三个脚本的执行结果。\nzkMetrics.sh 脚本实际上执行的是下面的命令：\n$ echo mntr | nc localhost …","relpermalink":"/kubernetes-handbook/devops/using-statefulset/","summary":"StatefulSet 这个对象是专门用来部署用状态应用的，可以为 Pod 提供稳定的身份标识，包括 hostname、启动顺序、DNS 名称等。 下面以在 Kubernetes1.6 版本中部署 zookeeper 和 kafka 为例讲解 StatefulSet 的使用，其中 kafka 依赖于 zookeeper。 Dockerfile 和配置文件","title":"使用StatefulSet部署有状态应用"},{"content":"Kubernetes 的社区是以 SIG（Special Interest Group 特别兴趣小组）和工作组的形式组织起来的，每个工作组都会定期召开视频会议。\n所有的 SIG 和工作组都使用 slack 和邮件列表沟通。\n   Kubernetes SIG  主要 SIG 列表  api-machinery：所有 API 级别的功能，包括了 API server、API 注册和发现、通用的 API CRUD 语义，准入控制，编码 / 解码，转换，默认值，持久化层（etcd），OpenAPI，第三方资源，垃圾回收（gc）和客户端库的方方面面。 aws：如何在 AWS 上支持和使用 kubernetes。 apps：在 kubernetes 上部署和运维应用程序。关注开发者和 DevOps 在 kubernetes 上运行应用程序的体验。 architecture：维持 kubernetes 在架构设计上的一致性和原则。 auth：kubernetes 的认证授权、权限管理和安全性策略。 autoscaling：集群的自动缩放，pod 的水平和垂直自动缩放，pod 的资源初始化，pod  …","relpermalink":"/kubernetes-handbook/develop/sigs-and-working-group/","summary":"Kubernetes 的社区是以 SIG（Special Interest Group 特别兴趣小组）和工作组的形式组织起来的，每个工作组都会定期召开视频会议。 所有的 SIG 和工作组都使用 slack 和邮件列表沟通。 Kubernetes SIG 主要 SIG 列表 api-machinery：所有 API 级","title":"SIG 和工作组"},{"content":"我们将在 Mac 上使用 docker 环境编译 kuberentes。\n安装依赖 brew install gnu-tar Docker 环境，至少需要给容器分配 4G 内存，在低于 3G 内存的时候可能会编译失败。\n执行编译 切换目录到 kuberentes 源码的根目录下执行：\n./build/run.sh make 可以在 docker 中执行跨平台编译出二进制文件。\n需要用的的 docker 镜像：\ngcr.io/google_containers/kube-cross:v1.7.5-2 该镜像基于 Ubuntu 构建，大小 2.15G，编译环境中包含以下软件：\n Go1.7.5 etcd protobuf g++ 其他 golang 依赖包  在我自己的电脑上的整个编译过程大概要半个小时。\n编译完成的二进制文件在 /_output/local/go/bin/ 目录下。\n","relpermalink":"/kubernetes-handbook/develop/developing-environment/","summary":"我们将在 Mac 上使用 docker 环境编译 kuberentes。 安装依赖 brew install gnu-tar Docker 环境，至少需要给容器分配 4G 内存，在低于 3G 内存的时候可能会编译失败。 执行编译 切换目录到 kuberentes 源码的根目录下执行： ./build/run.sh make 可以在 docker 中执行跨平台编译出","title":"配置 Kubernetes 开发环境"},{"content":"这篇文章将指导你如何测试 Kubernetes。\n单元测试 单元测试仅依赖于源代码，是测试代码逻辑是否符合预期的最简单方法。\n运行所有的单元测试\nmake test 仅测试指定的 package\n# 单个package make test WHAT=./pkg/api # 多个packages make test WHAT=./pkg/{api,kubelet} 或者，也可以直接用 go test\ngo test -v k8s.io/kubernetes/pkg/kubelet 仅测试指定 package 的某个测试 case\n# Runs TestValidatePod in pkg/api/validation with the verbose flag set make test WHAT=./pkg/api/validation KUBE_GOFLAGS=\u0026#34;-v\u0026#34; KUBE_TEST_ARGS=\u0026#39;-run ^TestValidatePod$\u0026#39; # Runs tests that match the regex ValidatePod|ValidateConfigMap in …","relpermalink":"/kubernetes-handbook/develop/testing/","summary":"这篇文章将指导你如何测试 Kubernetes。 单元测试 单元测试仅依赖于源代码，是测试代码逻辑是否符合预期的最简单方法。 运行所有的单元测试 make test 仅测试指定的 package # 单个package make test WHAT=./pkg/api # 多个package","title":"测试 Kubernetes"},{"content":"访问 kubernetes 集群有以下几种方式：\n   方式 特点 支持者     Kubernetes dashboard 直接通过 Web UI 进行操作，简单直接，可定制化程度低 官方支持   kubectl 命令行操作，功能最全，但是比较复杂，适合对其进行进一步的分装，定制功能，版本适配最好 官方支持   client-go 从 kubernetes 的代码中抽离出来的客户端包，简单易用，但需要小心区分 kubernetes 的 API 版本 官方支持   client-python python 客户端，kubernetes-incubator 官方支持   Java client fabric8 中的一部分，kubernetes 的 java 客户端 Red Hat    下面，我们基于 client-go，对 Deployment 升级镜像的步骤进行了定制，通过命令行传递一个 Deployment 的名字、应用容器名和新 image 名字的方式来升级。\nkubernetes-client-go-sample 项目的 main.go 代码如下：\npackage main …","relpermalink":"/kubernetes-handbook/develop/client-go-sample/","summary":"访问 kubernetes 集群有以下几种方式： 方式 特点 支持者 Kubernetes dashboard 直接通过 Web UI 进行操作，简单直接，可定制化程度低 官方支持 kubectl 命令行操作，功能最全，但是比较复杂，适合对其进行进一步的分装，定制功能，版本适配最好 官方支持 client-go 从 kubernetes 的","title":"client-go 示例"},{"content":"Operator 是由 CoreOS 开发的，用来扩展 Kubernetes API，特定的应用程序控制器，它用来创建、配置和管理复杂的有状态应用，如数据库、缓存和监控系统。Operator 基于 Kubernetes 的资源和控制器概念之上构建，但同时又包含了应用程序特定的领域知识。创建 Operator 的关键是 CRD（自定义资源）的设计。awesome-operators 中罗列了目前已知的 Operator。\n工作原理 Operator 是将运维人员对软件操作的知识给代码化，同时利用 Kubernetes 强大的抽象来管理大规模的软件应用。\nOperator 使用了 Kubernetes 的自定义资源扩展 API 机制，如使用 CRD（CustomResourceDefinition）来创建。Operator 通过这种机制来创建、配置和管理应用程序。\nOperator 基于 Kubernetes 的以下两个概念构建：\n 资源：对象的状态定义 控制器：观测、分析和行动，以调节资源的分布  Operator 用途 若您有以下需求，可能会需要用到 Operator：\n 按需部署一个 …","relpermalink":"/kubernetes-handbook/develop/operator/","summary":"Operator 是由 CoreOS 开发的，用来扩展 Kubernetes API，特定的应用程序控制器，它用来创建、配置和管理复杂的有状态应用，如数据库、缓存和监控系统。Operator 基于 Kubernetes 的资源和控制器概念之上构建，但同时又包含了应用程序特定的","title":"Operator"},{"content":"Operator SDK 由 CoreOS 开源，它是用于构建 Kubernetes 原生应用的 SDK，它提供更高级别的 API、抽象和项目脚手架。在阅读本文前请先确认您已经了解 Operator是什么。\n使用 Kubernetes 中原生的对象来部署和管理复杂的应用程序不是那么容易，尤其是要管理整个应用的生命周期、组件的扩缩容，我们之前通常是编写各种脚本，通过调用 Kubernetes 的命令行工具来管理 Kubernetes 上的应用。现在可以通过 CRD（CustomResourceDefinition）来自定义这些复杂操作，通过将运维的知识封装在自定义 API 里来减轻运维人员的负担。同时我们还可以像操作 Kubernetes 的原生资源对象一样，使用 kubectl 来操作 CRD。\n下面我们将安装和试用一下 Operator SDK。\n安装 Operator SDK $ mkdir -p $GOPATH/src/github.com/operator-framework $ cd …","relpermalink":"/kubernetes-handbook/develop/operator-sdk/","summary":"Operator SDK 由 CoreOS 开源，它是用于构建 Kubernetes 原生应用的 SDK，它提供更高级别的 API、抽象和项目脚手架。在阅读本文前请先确认您已经了解 Operator是什么。 使用 Kubernetes 中原生的对象来部署和管理复杂的应用程序不是那么容易，","title":"Operator SDK"},{"content":"Kubebuilder 是一个基于 CRD 来构建 Kubernetes API 的框架，可以使用 CRD 来构建 API、Controller 和 Admission Webhook。\n动机 目前扩展 Kubernetes 的 API 的方式有创建 CRD、使用 Operator SDK 等方式，都需要写很多的样本文件（boilerplate），使用起来十分麻烦。为了能够更方便构建 Kubernetes API 和工具，就需要一款能够事半功倍的工具，与其他 Kubernetes API 扩展方案相比，kubebuilder 更加简单易用，并获得了社区的广泛支持。\n工作流程 Kubebuilder 的工作流程如下：\n 创建一个新的工程目录 创建一个或多个资源 API CRD 然后将字段添加到资源 在控制器中实现协调循环（reconcile loop），watch 额外的资源 在集群中运行测试（自动安装 CRD 并自动启动控制器） 更新引导集成测试测试新字段和业务逻辑 使用用户提供的 Dockerfile 构建和发布容器  设计哲学 Kubebuilder …","relpermalink":"/kubernetes-handbook/develop/kubebuilder/","summary":"Kubebuilder 是一个基于 CRD 来构建 Kubernetes API 的框架，可以使用 CRD 来构建 API、Controller 和 Admission Webhook。 动机 目前扩展 Kubernetes 的 API 的方式有创建 CRD、使用 Operator SDK 等方式，都需要写很多的样本文件（boilerplate），","title":"Kubebuilder"},{"content":"EnvoyFilter 资源允许你定制由 Istio Pilot 生成的 Envoy 配置。使用该资源，你可以更新数值，添加特定的过滤器，甚至添加新的监听器、集群等等。小心使用这个功能，因为不正确的定制可能会破坏整个网格的稳定性。\n过滤器是叠加应用的，这意味着对于特定命名空间中的特定工作负载，可以有任何数量的过滤器。根命名空间（例如 istio-system）中的过滤器首先被应用，然后是工作负载命名空间中的所有匹配过滤器。\n下面是一个 EnvoyFilter 的例子，它在请求中添加了一个名为 api-version 的头。\napiVersion:networking.istio.io/v1alpha3kind:EnvoyFiltermetadata:name:api-header-filternamespace:defaultspec:workloadSelector:labels:app:web-frontendconfigPatches:- …","relpermalink":"/istio-handbook/traffic-management/envoyfilter/","summary":"EnvoyFilter 资源允许你定制由 Istio Pilot 生成的 Envoy 配置。使用该资源，你可以更新数值，添加特定的过滤器，甚至添加新的监听器、集群等等。小心使用这个功能，因为不正确的定制可能会破坏整个网格的稳定性。 过滤器是叠加应用的，这意味","title":"EnvoyFilter"},{"content":"本页假定您已经熟悉 Kubernetes 的核心概念并可以轻松的部署自己的应用程序。在浏览了本页面及其链接的内容后，您将会更好的理解如下部分：\n 可以在应用程序中使用的高级功能 扩展 Kubernetes API 的各种方法  使用高级功能部署应用 现在您知道了 Kubernetes 中提供的一组 API 对象。理解了 daemonset 和 deployment 之间的区别对于应用程序部署通常是足够的。也就是说，熟悉 Kubernetes 中其它的鲜为人知的功能也是值得的。因为这些功能有时候对于特别的用例是非常强大的。\n容器级功能 如您所知，将整个应用程序（例如容器化的 Rails 应用程序，MySQL 数据库以及所有应用程序）迁移到单个 Pod 中是一种反模式。这就是说，有一些非常有用的模式超出了容器和 Pod 之间的 1:1 的对应关系：\n Sidecar 容器：虽然 Pod 中依然需要有一个主容器，你还可以添加一个副容器作为辅助（见 日志示例)。单个 Pod 中的两个容器可以通过共享卷进行通信。 Init 容器：Init 容器在 Pod 的应用容器（如主容器和 sidecar  …","relpermalink":"/kubernetes-handbook/develop/advance-developer/","summary":"本页假定您已经熟悉 Kubernetes 的核心概念并可以轻松的部署自己的应用程序。在浏览了本页面及其链接的内容后，您将会更好的理解如下部分： 可以在应用程序中使用的高级功能 扩展 Kubernetes API 的各种方法 使用高级功能部署应用 现在您知道了","title":"高级开发指南"},{"content":"如果您想参与 Kubernetes 社区，请先阅读下Kubernetes Community这个 GitHub Repo中的文档，该文档中包括社区的治理形式、社区成员资格申请、提交 Issue、查找问题和提交 PR 的指导等。\n参考  Kubernetes Community Kubernetes Developer Guide Enhencement Tracking and Backlog Kubernetes 官方网站项目  ","relpermalink":"/kubernetes-handbook/develop/contribute/","summary":"如果您想参与 Kubernetes 社区，请先阅读下Kubernetes Community这个 GitHub Repo中的文档，该文档中包括社区的治理形式、社区成员资格申请、提交 Issue、查找问题和提交 PR 的指导等。 参考 Kubernetes Community Kubernetes Developer Guide Enhencement Tracking and","title":"参与 Kubernetes 社区贡献"},{"content":"Minikube 用于在本地运行 kubernetes 环境，用来开发和测试。\n安装 Minikube 到 GitHub 下载 minikube，我安装的是 minikube v1.11.0。\n下载完成后修改文件名为 minikube，然后 chmod +x minikube，移动到 $PATH 目录下：\nsudo mv ~/Download/minikube-darwin-adm64 /usr/local/bin/ sudo chmod +x /usr/local/bin/minikube 安装 kubectl 方式一\n参考 Install and Set Up kubectl，直接使用二进制文件安装即可。\ncurl -LO https://storage.googleapis.com/kubernetes-release/release/`curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt`/bin/darwin/amd64/kubectl …","relpermalink":"/kubernetes-handbook/develop/minikube/","summary":"Minikube 用于在本地运行 kubernetes 环境，用来开发和测试。 安装 Minikube 到 GitHub 下载 minikube，我安装的是 minikube v1.11.0。 下载完成后修改文件名为 minikube，然后 chmod +x minikube，移动到 $PATH 目录下： sudo mv ~/Download/minikube-darwin-adm64 /usr/local/bin/ sudo chmod +x /usr/local/bin/minikube 安","title":"Minikube"},{"content":"背景介绍 Apache SkyWalking 观察部署在服务网格中的服务的度量、日志、追踪和事件。在进行故障排除时，SkyWalking 错误分析是一个宝贵的工具，可以帮助确定错误发生的位置。然而，确定性能问题更加困难：利用预先存在的观察数据往往不可能找到性能问题的根本原因。为此，动态调试和故障排除在进行服务性能剖析时就必不可少。在这篇文章中，我们将讨论如何使用 eBPF 技术来改进 SkyWalking 中的剖析功能，并用于分析服务网格中的性能影响。\nSkyWalking 中的追踪剖析 自 SkyWalking 7.0.0 以来，Trace Profiling 通过定期对线程堆栈进行采样，让开发者知道运行哪行代码花费更多时间，从而帮助开发者发现性能问题。然而，Trace Profiling 不适合以下情况：\n 线程模型：Trace Profiling 对于剖析在单线程中执行的代码最有用。它对严重依赖异步执行模式的中间件不太有用。例如，Go 中的 Goroutines 或 Kotlin Coroutines。 语言：目前，Trace Profiling 只支持 Java …","relpermalink":"/blog/pinpoint-service-mesh-critical-performance-impact-by-using-ebpf/","summary":"在这篇文章中，我们将讨论如何使用 eBPF 技术来改进 SkyWalking 中的剖析功能，并用于分析服务网格中的性能影响。","title":"使用 eBPF 准确定位服务网格的关键性能问题"},{"content":"编者的话 本文是来自 Tetrate 工程师的分享，Tetrate 的拳头产品是 Tetrate Service Bridge（下文简称 TSB），它是在开源的 Istio 和 Envoy 基础上构建的，但为其增加了管理平面。\n简介 Tetrate 的应用连接平台 Tetrate Service Bridge（TSB）提供两种网关类型，分别为一级网关（Tier-1）和二级网关（Tier-2），它们都基于 Envoy 构建，但是目的有所不同。本文将探讨这两种类型网关的功能，以及何时选用哪种网关。\n关于两级网关的简要介绍：\n 一级网关（下文简称 T1）位于应用边缘，用于多集群环境。同一应用会同时托管在不同的集群上，T1网关将对该应用的请求流量在这些集群之间路由。 二级网关（下文简称 T2）位于一个的集群边缘，用于将流量路由到该集群内由服务网格管理的服务。  两级网关释义 托管在 TSB 管理的集群中的应用部署的设计与开源的 Istio 模型非常相似。它们的结构相同，使用入口网关来路由传入的流量。T2 网关相当于 Istio 的入口网关（Ingress Gateway）， …","relpermalink":"/blog/designing-traffic-flow-via-tier1-and-tier2-ingress-gateways/","summary":"在 Kubernetes 中我们不能仅依靠网络层加密，还需要 mTLS 来对客户端和服务端进行双向的传输层认证。本文将聚焦于 TLS 的真实性，以及证书管理的难题，说明服务网格对于在 Kubernetes 中开启 mTLS 带来的便利。","title":"通过两级网关设计来路由服务网格流量"},{"content":"编者的话 本文翻译节选自 A Kubernetes engineer’s guide to mTLS，为了便于读者理解，笔者对原文做了一点修改 1。因为笔者最近在研究 Istio 中的身份认证及 SPIFFE，对如何在 Kubernetes 中应用 mTLS 以及为什么要使用 mTLS 产生了浓厚的兴趣，再回想起五年前手动安装 Kubernetes 时，因为给集群开启 TLS 问题而导致安装停滞不前。\n本文的主要观点是：在 Kubernetes 中我们不能仅依靠网络层加密，还需要 mTLS 来对客户端和服务端进行双向的传输层认证。本文将聚焦于 TLS 的真实性，以及证书管理的难题，说明服务网格对于在 Kubernetes 中开启 mTLS 带来的便利。\n 简介 Mutual TLS（双向 TLS），或称 mTLS，是 Kubernetes 中的一个热门话题，尤其是对于那些负责为应用程序提供传输层加密的人来说。但是，你有没有考虑过，什么是 mTLS，它提供什么样的安全，为什么需要 mTLS？\n本指南我将介绍什么是 mTLS，它与常规 TLS 的关系，以及为什么它与 Kubernetes  …","relpermalink":"/blog/mtls-guide/","summary":"在 Kubernetes 中我们不能仅依靠网络层加密，还需要 mTLS 来对客户端和服务端进行双向的传输层认证。本文将聚焦于 TLS 的真实性，以及证书管理的难题，说明服务网格对于在 Kubernetes 中开启 mTLS 带来的便利。","title":"写给 Kubernetes 工程师的 mTLS 指南"},{"content":"编者的话 Istio 1.14 版本增加了对 SPIRE 集成的支持，这篇文章将指导你如何在 Istio 中集成 SPIRE。\nSPIRE 是 SPIFFE 规范的一个生产就绪的实现，它可以执行节点和工作负载证明，以便安全地将加密身份发给在异构环境中运行的工作负载。通过与 Envoy 的 SDS API 集成，SPIRE 可以被配置为 Istio 工作负载的加密身份来源。Istio 可以检测到一个 UNIX 域套接字的存在，该套接字在定义的套接字路径上实现了 Envoy SDS API，允许 Envoy 直接从它那里进行通信和获取身份。\n这种与 SPIRE 的集成提供了灵活的认证选项，这是默认的 Istio 身份管理所不具备的，同时利用了 Istio 强大的服务管理。例如，SPIRE 的插件架构能够提供多样化的工作负载认证选项，超越 Istio 提供的 Kubernetes 命名空间和服务账户认证。SPIRE 的节点认证将认证扩展到工作负载运行的物理或虚拟硬件上。\n关于这种 SPIRE 与 Istio 集成的快速演示，请参阅通过 Envoy 的 SDS API 将 SPIRE …","relpermalink":"/blog/istio-spire-integration/","summary":"Istio 1.14 版本增加了对 SPIRE 集成的支持，这篇文章将指导你如何在 Istio 中集成 SPIRE。","title":"如何在 Istio 中集成 SPIRE"},{"content":"前言 OpenTelemetry 追踪包含了理解分布式系统和排除故障的信息宝库 —— 但你的服务必须首先被指标化，以发射 OpenTelemetry 追踪来实现这一价值。然后，这些追踪信息需要被发送到一个可观察的后端，使你能够获得关于这些数据的任意问题的答案。可观测性是一个分析问题。\n本周早些时候，我们部分解决了这个问题，宣布在 Promscale 中普遍提供 OpenTelemetry 追踪支持，将由 SQL 驱动的可观测性带给所有开发者。随着对分析语言 ——SQL 的全面支持，我们解决了分析的问题。但我们仍然需要解决第一部分的问题：测量。\n为了让你的服务发出追踪数据，你必须手动添加 OpenTelemetry 测量工具到代码中。而且你必须针对所有服务和你使用的所有框架来做，否则你将无法看到每个请求的执行情况。你还需要部署 OpenTelemetry 收集器来接收所有新的追踪，处理它们，批处理它们，并最终将它们发送到你的可观测性后端。这需要花费大量的时间和精力。\n如果你不需要做所有这些手工工作，并且可以在几分钟内而不是几小时甚至几天内启动和运行呢？如果你还能建立一个完整的可观测性技术 …","relpermalink":"/blog/generate-and-store-opentelemetry-traces-automatically/","summary":"首先，我们将解释一下如何在 Kubernetes 自动生成和存储 OpenTelemetry 追踪，剖析 OpenTelemetry Operator 在内部的真正作用。接下来，我们将通过一个例子演示如何将其直接付诸实践。","title":"一键开启 Kubernetes 可观测性——如何自动生成和存储 OpenTelemetry 追踪"},{"content":"主要收获   对于现代软件系统来说，可观测性不是关于数学方程。它是关于人类如何与复杂的系统互动并试图理解它们。\n  混沌工程利用了可观测性，因为它可以检测到系统稳定状态的偏差。混沌工程借助可观测性可以发现和克服系统的弱点。\n  可观测性依赖于系统所发出的信号，这些信号提供了关于系统行为的原始数据。然而，可观测性不仅受限于这些信号的质量，还受限于这些信号的可视化和解释的方式。\n  考虑到混沌工程、可观测性和可视化涉及到人类自我的解释，仪表盘的设计者可能会对这些解释产生偏差，这是一个事实。在这个意义上，视觉隐喻并不能保证我们以正确的方式解释这些数据。\n  基于视觉隐喻的仪表盘可以提供比经典的可视化更有用的数据。然而，这两种策略都很容易产生偏差；例如，在一项研究中，大多数参与者都注意到，由于显示了糟糕的柱状图和线状图，没有在图中显示出重要的分界点，因此整体结果是有偏差的。\n  自从 Netflix、Slack 和 Linkedin 等领先的技术公司采用混沌工程来抵御生产中的意外中断后，这门学科在近来已经成为主流。在这条道路上，可观测性发挥了关键作用，为工程师们带来了数据和监控的力量，他们现在 …","relpermalink":"/blog/chaos-engineering-observability-visual-metaphors/","summary":"本文为可观测性引入了一个新的角色：视觉隐喻，并进行了一项研究，对比了视觉隐喻与传统表示的结果。","title":"混沌工程和视觉隐喻的可观测性"},{"content":"前言 今天，Envoy 社区宣布了一个令人兴奋的新项目：Envoy Gateway。该项目将行业领导者联合起来，精简由 Envoy 驱动的应用网关的好处。这种方法使 Envoy Gateway 能够立即为快速创新打下坚实的基础。该项目将提供一套服务来管理 Envoy 代理机群，通过易用性来推动采用，并通过定义明确的扩展机制来支持众多的用例。\n我们为什么要这样做？ Tetrate 是 Envoy Proxy 的第一贡献者（按提交量计算），也是 Envoy Gateway 指导小组的成员，其贡献者涵盖技术和管理领域。我们相信，我们强大的伙伴关系和在开源软件方面的深厚经验将有助于确保 Envoy Gateway 的成功。Tetrate 推动了 EG 计划，因为我们致力于上游项目，因为我们相信这将降低 Envoy Proxy 用户的进入门槛，也因为这与我们开发服务网格作为零信任架构基础的使命相一致。Tetrate 将大力投资建设 Envoy Gateway 的安全功能，包括支持 OAuth2 和 Let’s Encrypt 集成等 API 功能。\n对上游项目的承诺 Tetrate 从第一天起就 …","relpermalink":"/blog/the-gateway-to-a-new-frontier/","summary":"在我们看来，由于Envoy的设计、功能设置、安装基础和社区，它是业内最好的API网关。有了Envoy Gateway，企业可以在将Envoy嵌入其API管理策略方面增加信心。","title":"Envoy API Gateway——推动网关的进一步发展"},{"content":"前言 今天，我们很高兴地宣布 Envoy Gateway 成为 Envoy 代理家族的新成员，该项目旨在大幅降低将 Envoy 作为 API 网关的使用门槛。\n历史 Envoy 在 2016 年秋天开源，令我们惊讶的是，它很快就引领了整个行业。用户被这个项目的许多不同方面所吸引，包括它的包容性社区、可扩展性、API 驱动的配置模型、强大的可观测性输出和越来越广泛的功能集。\n尽管在其早期历史中，Envoy 成为了服务网格的代名词，但它在 Lyft 的首次使用实际上是作为 API 网关 / 边缘代理，提供深入的可观测性输出，帮助 Lyft 从单体架构迁移到微服务架构。\n在过去的 5 年多时间里，我们看到 Envoy 被大量的终端用户采用，既可以作为 API 网关，也可以作为服务网格中的 sidecar 代理。同时，我们看到围绕 Envoy 出现了一个庞大的供应商生态系统，在开源和专有领域提供了大量的解决方案。Envoy 的供应商生态系统对项目的成功至关重要；如果没有对所有在 Envoy 上兼职或全职工作的员工的资助，这个项目肯定不会有今天的成就。\nEnvoy 作为许多不同的架构类型和供应商 …","relpermalink":"/blog/introducing-envoy-gateway/","summary":"今天，我们很高兴地宣布 Envoy Gateway 成为 Envoy 代理家族的新成员，该项目旨在大幅降低将 Envoy 作为 API 网关的使用门槛。","title":"开源项目 Envoy Gateway 简介"},{"content":"什么是 Wasm 插件？ 你可以使用 Wasm 插件在数据路径上添加自定义代码，轻松地扩展服务网格的功能。可以用你选择的语言编写插件。目前，有 AssemblyScript（TypeScript-ish）、C++、Rust、Zig 和 Go 语言的 Proxy-Wasm SDK。\n在这篇博文中，我们描述了如何使用 Wasm 插件来验证一个请求的有效载荷。这是 Wasm 与 Istio 的一个重要用例，也是你可以使用 Wasm 扩展 Istio 的许多方法的一个例子。您可能有兴趣阅读我们关于在 Istio 中使用 Wasm 的博文，并观看我们关于在 Istio 和 Envoy 中使用 Wasm 的免费研讨会的录音。\n何时使用 Wasm 插件？ 当你需要添加 Envoy 或 Istio 不支持的自定义功能时，你应该使用 Wasm 插件。使用 Wasm 插件来添加自定义验证、认证、日志或管理配额。\n在这个例子中，我们将构建和运行一个 Wasm 插件，验证请求 body 是 JSON，并包含两个必要的键 ——id 和 token。\n编写 Wasm 插件 这个示例使用 tinygo …","relpermalink":"/blog/validating-a-request-payload-with-wasm/","summary":"本文是一个使用 Go 语言开发 Wasm 插件验证请求负载，并将其部署到 Istio 或 Envoy 上的教程。","title":"使用 WebAssembly 验证请求负载"},{"content":"前言 Slack 有一个全球客户群，在高峰期有数百万同时连接的用户。用户之间的大部分通信涉及到向对方发送大量的微小信息。在 Slack 的大部分历史中，我们一直使用 HAProxy 作为所有传入流量的负载均衡器。今天，我们将讨论我们在使用 HAProxy 时所面临的问题，我们如何用 Envoy Proxy 来解决这些问题，迁移所涉及的步骤，以及结果是什么。让我们开始吧！\nSlack 的 Websockets 为了即时传递信息，我们使用 websocket 连接，这是一种双向的通信链接，负责让你看到 “有几个人在打字……\u0026#34;，然后是他们打的东西，速度几乎是光速的。websocket 连接被摄取到一个叫做 “wss”（WebSocket 服务）的系统中，可以通过 wss-primary.slack.com 和 wss-backup.slack.com（这不是网站，如果去访问，只会得到一个 HTTP 404）从互联网上访问。\n   显示websockets工作原理的图表  Websocket 连接一开始是普通的 HTTPS 连接，然后客户端发出协议切换请求，将连接升级为 Websocket。 …","relpermalink":"/blog/migrating-millions-of-concurrent-websockets-to-envoy/","summary":"本文是 Slack 花半年时间从 HAProxy 迁移到 Envoy 上的经验分享。","title":"Slack 将数百万个并发的 Websockets 迁移到 Envoy 上经验分享"},{"content":"编者的话 本文作者是 Isovalent 联合创始人\u0026amp;CTO，原文标题 How eBPF will solve Service Mesh - Goodbye Sidecars，作者回顾了Linux 内核的连接性，实现服务网格的几种模式，以及如何使用 eBPF 实现无 Sidecar 的服务网格。\n什么是服务网格？ 随着分布式应用的引入，额外的可视性、连接性和安全性要求也浮出水面。应用程序组件通过不受信任的网络跨越云和集群边界进行通信，负载均衡、弹性变得至关重要，安全必须发展到发送者和接收者都可以验证彼此的身份的模式。在分布式应用的早期，这些要求是通过直接将所需的逻辑嵌入到应用中来解决的。服务网格将这些功能从应用程序中提取出来，作为基础设施的一部分提供给所有应用程序使用，因此不再需要修改每个应用程序。\n   服务网格示意图  纵观今天服务网格的功能设置，可以总结为以下几点：\n 弹性连接：服务与服务之间的通信必须能够跨越边界，如云、集群和场所。通信必须是有弹性的和容错的。 L7 流量管理：负载均衡、速率限制和弹性必须是 L7 感知的（HTTP、REST、gRPC、WebSocket 等）。 …","relpermalink":"/blog/ebpf-solve-service-mesh-sidecar/","summary":"本文回顾了Linux 内核的连接性，实现服务网格的几种模式，以及如何使用 eBPF 实现无 Sidecar 的服务网格。","title":"告别 Sidecar——使用 eBPF 解锁内核级服务网格"},{"content":"前言 Istio 1.12 中新的 WebAssembly 基础设施使其能够轻松地将额外的功能注入网格部署中。\n经过三年的努力，Istio 现在有了一个强大的扩展机制，可以将自定义和第三方 Wasm 模块添加到网格中的 sidecar。Tetrate 工程师米田武（Takeshi Yoneda）和周礼赞（Lizan Zhou）在实现这一目标方面发挥了重要作用。这篇文章将介绍 Istio 中 Wasm 的基础知识，以及为什么它很重要，然后是关于建立自己的 Wasm 插件并将其部署到网格的简短教程。\n为什么 Istio 中的 Wasm 很重要 使用 Wasm，开发人员可以更容易的扩展网格和网关。在 Tetrate，我们相信这项技术正在迅速成熟，因此我们一直在投资上游的 Istio，使配置 API、分发机制和从 Go 开始的可扩展性体验更加容易。我们认为这将使 Istio 有一个全新的方向。\n有何期待：新的插件配置 API，可靠的获取和安装机制 有一个新的顶级 API，叫做 WasmPlugin，可以让你配置要安装哪些插件，从哪里获取它们（OCI 镜像、容器本地文件或远程 HTTP 资源）， …","relpermalink":"/blog/istio-wasm-extensions-and-ecosystem/","summary":"Istio 1.12 中新的 WebAssembly 基础设施使其能够轻松地将额外的功能注入网格部署中。","title":"Istio 1.12 引入 Wasm 插件配置 API 用于扩展 Istio 生态"},{"content":"编者的话 本文译自 Istio 官方博客，博客原标题 gRPC Proxyless Service Mesh，其实是 Istio 1.11 版本中支持的实验特性，可以直接将 gRPC 服务添加到 Istio 中，而不需要再向 Pod 中注入 Envoy 代理。本文中还给出了一个 Demo 性能测试数据，这种做法可以极大的提升应用性能，降低网络延迟。\nIstio 使用一组发现 API（统称为 xDS API 来动态配置其 Envoy sidecar 代理。这些 API 的目标是成为一个 通用的数据平面 API。gRPC 项目对 xDS API 有很好的支持，也就是说你可以管理 gRPC 工作负载，而不需要同时部署 Envoy sidecar。你可以在 Megan Yahya 的 KubeCon EU 2021 演讲中了解更多关于该集成的信息。关于 gRPC 支持的最新情况，可以在他们的提案中找到，还有实现状态。\nIstio 1.11 增加了实验性支持，可以直接将 gRPC 服务添加到网格中。我们支持基本的服务发现，一些基于 VirtualService 的流量策略，以及双向 TLS。\n支 …","relpermalink":"/blog/grpc-proxyless-service-mesh/","summary":"编者的话 本文译自 Istio 官方博客，博客原标题 gRPC Proxyless Service Mesh，其实是 Istio 1.11 版本中支持的实验特性，可以直接将 gRPC 服务添加到 Istio 中，而不需要再向 Pod 中注入 Envoy 代理。本文中还给出了一个 Demo 性能测试数据，这种做法可以极大的提升应","title":"基于 gRPC 和 Istio 的无 sidecar 代理的服务网格"},{"content":"前言 今天有几个服务网格的产品和项目，承诺简化应用微服务之间的连接，同时提供额外的功能，如安全连接、可观测性和流量管理。但正如我们在过去几年中反复看到的那样，对服务网格的兴奋已经被对额外的复杂性和开销的实际担忧所抑制。让我们来探讨一下 eBPF 是如何让我们精简服务网格，使服务网格的数据平面更有效率，更容易部署。\nSidecar 问题 今天的 Kubernetes 服务网格解决方案要求你在每一个应用 pod 上添加一个代理 sidecar 容器，如 Envoy 或 Linkerd-proxy。这是正确的：即使在一个非常小的环境中，比如说有 20 个服务，每个服务运行五个 pod，分布在三个节点上，你也有 100 个代理容器。无论代理的实现多么小和有效，这种纯粹的重复都会耗费资源。\n每个代理使用的内存与它需要能够通信的服务数量有关。Pranay Singhal 写了他配置 Istio 的经验，将每个代理的消耗从 1GB 左右减少到更合理的 60-70MB。但是，即使在我们的小环境中，在三个节点上有 100 个代理，这种优化配置仍然需要每个节点 2GB 左右。\n   来自\u0026lt;a …","relpermalink":"/blog/how-ebpf-streamlines-the-service-mesh/","summary":"本文探讨一下eBPF是如何让我们精简服务网格，使服务网格的数据平面更有效率，更容易部署。","title":"eBPF 如何简化服务网格"},{"content":"讲师分享  云原生社区 meetup 第八期上海站开场，郭旭东 云原生 2.0 华为云赋能 “新云原生企业”，张凯豪 蚂蚁万级规模 K8s 集群 etcd 架构优化实践 —ETCD on OceanBase，宣超 新一代开源 HCI 底层原理剖析，胡凯 攀登规模化的高峰 —— 蚂蚁集团大规模 Sigma 集群 ApiServer 优化实践，唐博，谭崇康 云原生分布式存储 Rook 及其在企业中应用的未来，林文炜  ","relpermalink":"/event/cloud-native-meetup-shanghai-08/","summary":"本次活动聚焦于 OceanBase、Rook、Sigma 等。","title":"云原生社区 meetup 第八期上海站"},{"content":"主要收获  了解采用服务网格技术的新兴架构趋势，特别是多云、多集群和多租户模式，如何在异构基础设施（裸机、虚拟机和 Kubernetes）中部署服务网格解决方案，以及从边缘计算层到网格的应用 / 服务连接。 了解服务网格生态系统中的一些新模式，如多集群服务网格、媒体服务网格（Media Service Mesh）和混沌网格，以及经典的微服务反模式，如 “死星（Death Star） “架构。 获取最新的关于在部署领域使用服务网格的创新总结，在 Pod（K8s 集群）和 VM（非 K8s 集群）之间进行快速实验、混乱工程和金丝雀部署。 探索服务网格扩展领域的创新，包括：增强身份管理，以确保微服务连接的安全性，包括自定义证书授权插件，自适应路由功能，以提高服务的可用性和可扩展性，以及增强 sidecar 代理。 了解操作方面即将出现的情况，如配置多集群功能和将 Kubernetes 工作负载连接到托管在虚拟机基础设施上的服务器，以及管理多集群服务网格中所有功能和 API 的开发者门户。  在过去的几年里，服务网格技术有了长足的发展。服务网格在各组织采用云原生技术方面发挥着重要作用。通过提供 …","relpermalink":"/blog/service-mesh-ultimate-guide-e2/","summary":"本文是 InfoQ 自 2020 年 2 月发表的服务网格终极指南后的第二版，发布于 2021 年 9 月。","title":"服务网格终极指南第二版——下一代微服务开发"},{"content":"讲师分享  云原生社区 meetup 第七期深圳站开场致辞 - 宋净超 使用 IAST 构建高效的 DevSecOps 流程 - 董志勇 云原生场景下的开发和调试-汪晟杰，黄金浩 Envoy 在腾讯游戏云原生平台应用 - 田甜 使用 KubeVela 构建混合云应用管理平台 - 邓洪超  ","relpermalink":"/event/cloud-native-meetup-shenzhen-07/","summary":"本次活动关注于 DevSecOps、Envoy、KubeVela、Nocalhost 等。","title":"云原生社区 meetup 第七期深圳站"},{"content":"编者的话 本文译自 Envoy 代理的创始人 Matt Klein 于昨晚在个人博客上发布的文章 5 year of Envoy OSS。他在 Twitter 因为自己的程序 bug 造成重大事故而离职，后加入 Lyft，在开源 Envoy 之前几乎没有贡献和管理开源项目的经验，这篇文章分享了他个人及 Envoy 开源的心路历程，在投身开源 Envoy 还是为雇主 Lyft 效命，该如何抉择？看完本文，相信对于开源项目的维护者、创业者及投资人都会大有收获。\n前言 今天是 Envoy Proxy 开源的 5 周年。毫不夸张地说，在专业方面，过去的 5 年是一个史诗般的过山车，我的情绪介于兴奋、自豪、焦虑、尴尬、无聊、倦怠之间。我想分享一下这个项目的前传和历史，以及我在发展大型开源软件项目的过程中所学到的一些经验教训。\n前传和历史 前传 除了一些小的弯路，我在技术行业二十年的职业生涯一直专注于底层系统：嵌入式系统，操作系统，虚拟化，文件系统，以及最近的分布式系统网络。我的分布式系统网络之旅始于 2010 年初在亚马逊，我有幸帮助开发了第一批高性能计算（HPC）EC2 实例类型。我学到了大量 …","relpermalink":"/blog/envoy-oss-5-year/","summary":"开源网络代理 Envoy 的创始人 Matt Klein，在 Twitter 因为自己的程序 bug 造成重大事故而离职，后加入 Lyft，在开源 Envoy 之前几乎没有贡献和管理开源项目的经验，这篇文章分享了他个人及 Envoy 开源的心路历程，在投身开源 Envoy 还是为雇主 Lyft 效命，该如何抉择？","title":"网络代理 Envoy 开源五周年，创始人 Matt Klein 亲述开源心路历程及经验教训"},{"content":"话题  欢迎来到云原生社区大连站 Connecting, Controlling and Observing Dubbo Microservices with Flomesh，林杨 基于云原生技术的服务最大化可用性，白西原（乐天创研） Jutopia 一站式云原生机器学习平台，何昌钦 分布式系统的发展与趋势分析，张卫滨  ","relpermalink":"/event/cloud-native-meetup-dalian-06/","summary":"本次活动关注服务网格、机器学习。","title":"云原生社区 meetup 第六期大连站"},{"content":"编者的话 本文译自 Istio 官方博客 Security Best Practices。\nIstio 的安全功能提供了强大的身份、策略、透明的 TLS 加密以及认证、授权和审计（AAA）工具来保护你的服务和数据。然而，为了充分安全地利用这些功能，必须注意遵循最佳实践。建议在继续阅读之前，先回顾一下安全概述。\n双向 TLS Istio 将尽可能使用双向 TLS 对流量进行自动加密。然而，代理在默认情况下被配置为许可模式（Permissive Mode），这意味着他们将接受双向 TLS 和明文流量。\n虽然这是为了增量采用或允许来自没有 Istio sidecar 的客户端的流量的需要，但它也削弱了安全立场。建议在可能的情况下迁移到严格模式（Strict Mode），以强制使用双向 TLS。\n然而，仅靠双向 TLS 并不足以保证流量的安全，因为它只提供认证，而不是授权。这意味着，任何拥有有效证书的人仍然可以访问一个服务。\n为了完全锁定流量，建议配置授权策略。这允许创建细粒度的策略来允许或拒绝流量。例如，你可以只允许来自 app 命名空间的请求访问 hello-world 服务。 …","relpermalink":"/blog/istio-security-best-practices/","summary":"本文列举了 Istio 安全的最佳实践。","title":"Istio 安全最佳实践"},{"content":"话题  开场演讲：欢迎来到云原生社区成都站，宋净超，云原生社区创始人、Tetrate 布道师 Amazon EKS Distro 开源项目解析 \u0026amp; 演示，粟伟，亚马逊云科技资深解决方案架构师 面向量化投资的 AI 平台 ——AI 赋能投资：打造以大数据 + AI 为核心的下一代投资平台，梁举，宽邦科技 CEO 基于 TiDB 的云原生数据库实践，王天宜，TiDB 社区部门架构师 Layotto: 开启服务网格 + 应用运行时新篇章，石建伟（卓与），蚂蚁集团高级技术专家  ","relpermalink":"/event/cloud-native-meetup-chengdu-05/","summary":"本次活动关注 EKS Distro、TiDB、AI 和服务网格。","title":"云原生社区 meetup 第五期成都站"},{"content":"开场致辞 讲师：宋净超（Tetrate 布道师、云原生社区创始人）\n讲师介绍：Tetrate 云原生布道师，云原生社区创始人，CNCF Ambassador。\n有了 Nginx 和 Kong，为什么还需要 Apache APISIX？ 讲师：王院生\n个人介绍：支流科技联合创始人 CTO\n演讲概要\n在云原生时代，k8s 和微服务已经成为主流，在带来巨大生产力提升的同时，也增加了系统的复杂度。如何发布、管理和可视化服务，成为了一个重要的问题。每次修改配置都要 reload 的 Nginx、依赖 postgres 才能工作的 Kong，都不是云原生时代的理想之选。这正是我们创造 Apache APISIX 的原因：没有 reload、毫秒内全集群生效、不依赖数据库、极致性能、支持 Java 和 Go 开发插件。\n听众收益\n更好的理解 API 网关、服务网格，以及各个开源项目的优劣势\n云原生时代的研发效能 讲师：黄国峰\n个人介绍：腾讯 PCG 工程效能专家。10 多年的软件和互联网从业经验；现任腾讯工程效能部，负责持续集成、研发流程和构建系统等平台；曾任职唯品会高级经理，负责架构团队。在云原生 …","relpermalink":"/event/cloud-native-meetup-guangzhou-04/","summary":"本次活动关注于 Dapr 和 APISIX 等。","title":"云原生社区 meetup 第四期广州站"},{"content":"开场致辞 讲师：宋净超（Tetrate 布道师、云原生社区创始人）\n讲师介绍：Tetrate 云原生布道师，云原生社区创始人，CNCF Ambassador。\n使用 Chaos Mesh 来保障云原生系统的健壮性 讲师：周强\n公司：PingCAP\n讲师介绍：周强，PingCAP 工程效率负责人，Chaos Mesh 负责人，专注稳定性和性能测试平台。在混沌工程领域有 4 年的从业经验，领导开发云原生混沌测试平台 Chaos Mesh。\n演讲概要:\n在云原生的世界中，错误无处不在，混沌工程在提高系统稳定性方面起着至关重要的作用。通过执行混沌工程实验，我们可以了解系统的弱点并主动解决。我们开发了云原生混沌工程平台 Chaos Mesh，并在内部使用 Chaos Mesh 来提升云原生分布式数据库 TiDB 的健壮性。目前 Chaos Mesh 已加入 CNCF Sandbox 项目，该平台依托于 k8s 基础设施，通过对 pod/container 进行诸如杀节点、IO 错误和延时注入、时间回退、内核分配内存失败等等来进行混沌测试。主题大纲:\n 在分布式领域会遇到的质量和稳定性问题 混沌工 …","relpermalink":"/event/cloud-native-meetup-hangzhou-03/","summary":"本次活动关注于 ChaosMesh、Envoy 和 KubeVela。","title":"云原生社区 meetup 第三期杭州站"},{"content":"Istio 1.8——还是从前那个少年 讲师：宋净超（Tetrate 布道师、云原生社区创始人）\n个人介绍：Tetrate 布道师、CNCF Ambassador、云原生社区 创始人、电子工业出版社优秀译者、出品人。Kubernetes、Istio 等技术的早期使用及推广者。曾就职于科大讯飞、TalkingData 和蚂蚁集团。\n议题简介：带你回顾 Istio 的发展历程，看他是否还是从前那个少年，“没有一丝丝改变”，能够经历时间的考验。带你一起来了解 Istio 1.8 的新特性，看它是如何作为传统和现代应用的桥接器，成为云原生应用中的中流砥柱。同时也会为你分享云原生社区的规划，为了推行云原生，我们在行动。\n百度服务网格在金融行业的大规模落地实践 讲师：孙召昌（百度高级研发工程师）\n个人介绍：百度高级研发工程师，现就职于百度基础架构部云原生团队，参与了服务网格产品的研发工作和大规模落地实践，对云原生、微服务、Service Mesh等方向有深入的研究和实践经验。\n议题简介：百度服务网格技术在金融行业大规模落地过程的实践经验和思考，主要包括：\n 支持传统微服务应用的平滑迁移，兼 …","relpermalink":"/event/cloud-native-meetup-beijing-02/","summary":"本次活动关注 Istio、云原生存储、可观测性、DubboGo 等。","title":"云原生社区 meetup 第二期北京站"},{"content":"\n 演讲幻灯片及视频回放请点击页面上方的按钮。  开场演讲 讲师：宋净超\n公司：Tetrate\n讲师介绍：Tetrate 布道师，云原生社区创始人，CNCF Ambassador。\nKubernetes 在 UCloud 内部的应用 讲师：高鹏\n公司：UCloud\n讲师介绍：高鹏 UCloud 后台研发工程师，负责内部云原生平台的建设。\n演讲概要：在 Kubernetes 的实际应用中，我们会碰到各种各样的问题，比如复杂的网络结构、持久化存储的实现、多租户的权限模型、集群的升级、Istio 的使用、镜像仓库的高可用、CI/CD、监控告警、日志、Operator 等等等等。这次分享将介绍 UCloud 内部对 Kubernetes 以及云原生生态的应用实践，和大家分析每个选择背后的原因，碰到的问题以及解决方案。\n使用 Apache SkyWalking Adapter 实现 K8s HPA 讲师：高洪涛\n公司：Tetrate\n讲师介绍：高洪涛 美国 servicemesh 服务商 Tetrate 创始工程师。原华为软件开发云技术专家，对云 APM 有深入的理解，并有丰富的 APM 产品设 …","relpermalink":"/event/cloud-native-meetup-shanghai-01/","summary":"云原生社区举办的第一届城市站 meetup。","title":"云原生社区 meetup 第一期上海站"},{"content":"讲师与演讲话题 蚂蚁集团 API Gateway Mesh 的思考与实践 主讲人: 贾岛\n在 Service Mesh 微服务架构中，我们常常会听到东西流量和南北流量两个术语。蚂蚁集团开源的Service Mesh Sidecar MOSN 已经多次与大家见面交流了，以往的议题重点在东西流量的服务发现与路由，那么蚂蚁集团在南北流量上的思考是怎样的？本次分享，将从蚂蚁集团 API 网关发展历程来看，Mesh 化的网关架构是怎样的，解决了什么问题，双十一的实践表现，以及我们对未来的思考。\n酷家乐的 Istio 与 Knative 踩坑实录 主讲人: 付铖\n酷家乐在部分业务模块，自2018年使用了 Istio 进行服务治理，自2019年使用了 Knative 作为 FaaS 基础设施，在实践过程中解决了大量问题，也积累了不少第一手经验。本次分享，将重点讨论服务网格的性能损耗，存量业务迁移难题，函数计算的冷启动时间问题以及解决方案等。\n云原生开放智能网络代理 MOSN 金融级云原生架构助推器-蚂蚁集团 主讲人：肖涵（涵畅）\n圆桌环节：Service Mesh 落地的务实与创新 主讲人: …","relpermalink":"/event/service-mesh-meetup-09/","summary":"这是第七届 Service Mesh Meetup。","title":"Service Mesh Meetup #9 杭州站"},{"content":"本期为 Service Mesh Meetup#8 特别场，联合 CNCF、阿里巴巴及蚂蚁集团共同举办。\n不是任何一朵云都撑得住双 11。\n成交 2684 亿，阿里巴巴核心系统 100% 上云。\n蚂蚁集团的核心交易链路大规模上线 Service Mesh。\n这次，让双 11 狂欢继续，让云原生经得起双 11 大考，也让云原生走到开发者身边。\n你将收获 3 大经验加持：\n 双 11 洗礼下的阿里巴巴 K8s 超大规模实践经验 蚂蚁集团首次 Service Mesh 大规模落地经验 阿里巴巴超大规模神龙裸金属 K8s 集群运维实践经验  讲师与演讲话题 释放云原生价值，双 11 洗礼下的阿里巴巴 K8s 超大规模实践 主讲人：曾凡松（逐灵） 、汪萌海（木苏）\n2019 双 11 点燃了全球人民的购物热情，而阿里经济体核心系统全面上云则刷爆了国内的技术圈子，引起了众多热爱云计算、云原生技术专家的热议。阿里巴巴是首个在超大规模体量公司内大规模使用 K8s 的公司，借此机会将为大家带来阿里巴巴在生产场景中大规模应用 K8s 的实践经验，包括在大规模应用管理上的经验教训；当前如何通过云原生方式高效 …","relpermalink":"/event/service-mesh-meetup-08/","summary":"这是第八届 Service Mesh Meetup。","title":"Service Mesh Meetup #8 北京站"},{"content":"本期 Meetup 邀请社区大咖，从服务网格下微服务架构设计、在 5G 时代的应用、如何使用开源的 Traefik 构建云原生边缘路由及蚂蚁集团的服务网格代理演进角度给大家带来精彩分享。\n讲师与演讲话题 服务网格技术在5G网络管理平台中的落地实践 赵化冰（中兴通讯网管软件资深专家）\n在通信网络向5G演进的过程中，电信行业借鉴了IT行业的微服务架构和云原生相关技术对5G网络功能进行重构，以提供敏捷、灵活、易于扩展的业务能力。 本演讲主题将介绍在5G网络管理平台的微服务架构中落地微服务网格的产品实践，包括多网络平面支持、API网关和网格Ingress的定位、Consul Registry的性能增强等等。\n蚂蚁集团网络代理的演进之路 肖涵（蚂蚁集团高级技术专家）\n从网络硬件设备到自研平台，从传统服务治理到 Service Mesh，本次分享将介绍蚂蚁集团网络代理在接入层以及 Service Mesh 化道路上是如何一步步支撑秒级百万支付，千万红包请求的。\n进击的Traefik——云原生边缘路由探秘 杨川胡（ 知群后台负责人）\nTraefik 是一个云原生的边缘路由器，开源的反向代理和负载均衡 …","relpermalink":"/event/service-mesh-meetup-07/","summary":"这是第七届 Service Mesh Meetup。","title":"Service Mesh Meetup #7 成都站"},{"content":"讲师与演讲话题 虎牙直播在微服务改造方面的实践 张波 虎牙基础保障部中间件团队负责人\n本次主要分享虎牙注册中心、名字服务、DNS 的改造实践，以及如何通过 Nacos 实现与 istio 打通实现，使微服务平滑过渡到 service mesh。\nService Mesh 在蚂蚁集团的生产级安全实践 彭泽文 蚂蚁集团高级开发工程师\n介绍通过 Envoy SDS（Secret Discovery Service）实现 Sidecar 证书管理的落地方案；分享如何为可信身份服务构建敏感信息数据下发通道，以及 Service Mesh Sidecar 的 TLS 生产级落地实践。\n基于 Kubernetes 的微服务实践 涂小刚 慧择网运维经理\n介绍如何跟据现有业务环境情况制定容器化整体解决方案，导入业务进入 K8S 平台，容器和原有业务环境互通。制订接入规范、配置中心对接 K8S 服务、网络互通方案、DNS 互通方案、jenkins-pipeline 流水线构建方案、日志采集方案、监控方案等。\nService Mesh 发展趋势（续）： …","relpermalink":"/event/service-mesh-meetup-06/","summary":"这是第六届 Service Mesh Meetup。","title":"Service Mesh Meetup #6 广州站"},{"content":"讲师与演讲话题 唯品会 Service Mesh 的实践分享 郑德惠 唯品会Java资深开发工程师，内部Service Mesh框架负责人，唯品会开源项目vjtools重要开发者，10年电信与互联网后台开发经验。\nSOFAMosn 持续演进路径及实践案例 陈逸凡 花名无钩，蚂蚁集团资深开发工程师。专注于网络接入层，高性能服务器研发，SOFAMosn团队核心成员\n在网格的边缘试探——企业 Istio 试水指南 崔秀龙 HPE 软件分析师，Kubernetes 权威指南作者之一，Kubernetes、Istio 项目成员\nRoundtable：回顾2018，Service Mesh 蓄势待发 主持人：宋净超，ServiceMesher 社区联合创始人\n","relpermalink":"/event/service-mesh-meetup-05/","summary":"这是第五届 Service Mesh Meetup。","title":"Service Mesh Meetup #5 广州站"},{"content":"讲师与演讲话题 Observability and Istio telemetry 吴晟 Apache SkyWalking创始人、Apache Sharding-Sphere原型作者、比特大陆资深技术专家、CNCF OpenTracing标准化委员会成员\n蚂蚁集团 Service Mesh 渐进式迁移方案 敖小剑 蚂蚁集团高级技术专家，十六年软件开发经验，微服务专家，Service Mesh布道师，Servicemesher社区联合创始人\n张瑜标 阿里巴巴技术专家、前京东Hadoop负责人、Hadoop代码贡献者、现负责UC 基于Kubernetes自研的PaaS平台整体的稳定性\n探讨和实践基于Isito的微服务治理事件监控 徐运元 谐云科技云平台架构师，致力于容器 PaaS 平台、企业级容器云平台的方案设计和技术落地\nEnvoy、Contour与Kubernetes实践 冯玮 七牛容器云平台产品架构师，曾在百度和华为从事公有云领域高性能分布式计算和存储平台的架构设计和产品研发\n","relpermalink":"/event/service-mesh-meetup-04/","summary":"这是第四届 Service Mesh Meetup。","title":"Service Mesh Meetup #4 上海站"},{"content":"讲师与演讲话题 张超盟（华为）——Kubernetes容器应用基于Istio的灰度发布实践\nTopic摘要：随着1.0版本在上月底的发布，标志着Istio作为最火热的ServcieMesh框架已经逐渐成熟。本次议题中将以典型的灰度发布为例，分享华为云容器服务在Istio的实践，以及Istio和Kubernetes的完美结合释放云原生应用的核心优势，加速企业微服务技术转型。\n讲师简介：华为云微服务平台架构师，现负责华为云容器服务Istio产品化工作。参与华为PaaS平台产品设计研发，在Kubernetes容器服务、微服务架构、云服务目录、大数据、APM、DevOpS工具等多个领域有深入研究与实践。曾供职于趋势科技。\n朱经惠 （联邦车网）——Istio控制平面组件原理解析\nTopic摘要：网上有很多关于Istio的介绍，但主要的关注是数据平面。所以这次独辟蹊径，给大家解密Istio里强大的控制平面：管理生命周期的Pilot-Agent，配置中心Pilot-Discovery， 生成遥测报告的Mixer以及安全证书管理的Istio_Ca。通过本次分享您将了解其工作原理和现存的问题。\n讲师简 …","relpermalink":"/event/service-mesh-meetup-03/","summary":"这是第三届 Service Mesh Meetup。","title":"Service Mesh Meetup #3 深圳站"},{"content":"讲师与演讲话题 张亮（京东金融数据研发负责人）：Service Mesh的延伸 —— 论道Database Mesh\n个人简介：张亮，京东金融数据研发负责人。热爱开源，目前主导两个开源项目Elastic-Job和Sharding-Sphere(Sharding-JDBC)。擅长以java为主分布式架构以及以Kubernetes和Mesos为主的云平台方向，推崇优雅代码，对如何写出具有展现力的代码有较多研究。2018年初加入京东金融，现担任数据研发负责人。目前主要精力投入在将Sharding-Sphere打造为业界一流的金融级数据解决方案之上。\n随着Service Mesh概念的推广与普及，云原生、低接入成本以及分布式组件下移等理念，已逐渐被认可。在Service Mesh依旧处于高速迭代的发展期的同时，以它的理念为参考，其他的Mesh思想也在崭露萌芽。 Database Mesh即是Service Mesh的其中一种延伸，虽然理念与Service Mesh相近，但数据库与无状态的服务却有着巨大的差别。Database Mesh与分布式数据库（如NoSQL和NewSQL）的功能范畴并非重 …","relpermalink":"/event/service-mesh-meetup-02/","summary":"这是第二届 Service Mesh Meetup。","title":"Service Mesh Meetup #2 北京站"},{"content":"讲师分享  云原生社区 meetup 第七期深圳站开场致辞 - 宋净超 使用 IAST 构建高效的 DevSecOps 流程 - 董志勇 云原生场景下的开发和调试-汪晟杰，黄金浩 Envoy 在腾讯游戏云原生平台应用 - 田甜 使用 KubeVela 构建混合云应用管理平台 - 邓洪超  ","relpermalink":"/event/service-mesh-meetup-01/","summary":"这是第一届 Service Mesh Meetup。","title":"Service Mesh Meetup #1 杭州站"},{"content":"SOFAStack（Scalable Open Financial Architecture Stack）是蚂蚁集团自主研发并开源的金融级分布式架构，包含了构建金融级云原生架构所需的各个组件，是在金融场景里锤炼出来的最佳实践。SOFAStack 官方网站：https://www.sofastack.tech/\n参加此次 Meetup 您将获得：\n 基于 SOFAStack 快速构建微服务 金融场景下的分布式事务最佳实践 基于 Kubernetes 的云原生部署体验 云上的 Service Mesh 基本使用场景体验 基于 Serverless 轻松构建云上应用  如何注册：此活动须提前注册。请将 SOFAStack Cloud Native Workshop 添加到您 KubeCon + CloudNativeCon + Open Source Summit 的注册表里。您可以使用 KCCN19COMATF 折扣码获取 KubeCon 半价门票！\n如果对此活动有任何疑问，请发送邮件至 jingchao.sjc@antfin.com。\n活动详情 9:00 - 9:20 …","relpermalink":"/event/sofastack-cloud-native-workshop/","summary":"这是 KubeCon 第一次在中国举办，蚂蚁集团参与了同场活动，进行了 SOFAStack 云原生动手实验。","title":"SOFAStack Cloud Native Workshop"},{"content":"Kubernetes Hardening Guidance National Security Agency Cybersecurity and Infrastructure Security Agency\nCybersecurity Technical Report\nNotices and history Document change history\nAugust 2021 1.0 Initial release\nDisclaimer of warranties and endorsement\nThe information and opinions contained in this document are provided “as is” and without any warranties or guarantees. Reference herein to any specific commercial products, process, or service by trade name, trademark, manufacturer, or otherwise, …","relpermalink":"/kubernetes-hardening-guidance/kubernetes-hardening-guidance-english/","summary":"Kubernetes Hardening Guidance National Security Agency Cybersecurity and Infrastructure Security Agency\nCybersecurity Technical Report\nNotices and history Document change history\nAugust 2021 1.0 Initial release\nDisclaimer of warranties and endorsement\nThe information and opinions contained in this document are provided “as is” and without any warranties or guarantees. Reference herein to any specific commercial products, process, or service by trade name, trademark, manufacturer, or otherwise, does not necessarily constitute or imply its endorsement, recommendation, or favoring by the United States Government, and this guidance shall not be used for advertising or product endorsement purposes.\nTrademark recognition\nKubernetes is a registered trademark of The Linux Foundation.","title":""},{"content":"","relpermalink":"/service-mesh-devsecops/intro/reference-platform-for-the-implementation-of-devsecops-primitives/","summary":"","title":""}]