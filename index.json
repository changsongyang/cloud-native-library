[{"content":"本指南介绍云原生安全与身份解决方案 SPIFFE 和 SPIRE。\n本书大纲 概念\n规范\n架构\n安装\n配置\nSPIRE 集成示例\n开始阅读 ","relpermalink":"/spiffe-and-spire/","summary":"介绍 SPIFFE 和 SPIRE。","title":"SPIFFE 和 SPIRE：实现零信任安全身份"},{"content":"本章介绍 SPIFFE 和 SPIRE 中的基本概念。\nSPIFFE\nSPIRE\n","relpermalink":"/spiffe-and-spire/concept/","summary":"本章介绍 SPIFFE 和 SPIRE 中的基本概念。 SPIFFE SPIRE","title":"概念"},{"content":"本指南将帮助你实际开始使用“工作负载载入”。\n作为本指南的一部分，你将：\n在你的 Kubernetes 集群中部署 Istio Bookinfo 示例 在 AWS EC2 实例上部署 ratings 应用程序并将其载入到服务网格 验证 Kubernetes Pod(s) 与 AWS EC2 实例之间的流量 在 AWS Auto Scaling Group 上部署 ratings 应用程序并将其载入到服务网格 本指南旨在演示工作负载载入功能，易于跟随。\n为了保持简单，你无需配置基础设施，就像在生产部署的情况下所需的那样。\n具体来说：\n你无需设置可路由的 DNS 记录 你无需使用受信任的 CA 授权（如 Let’s Encrypt） 你无需将 Kubernetes 集群和 AWS EC2 实例放在同一网络或对等网络上 在继续之前，请确保完成以下先决条件：\n创建一个 Kubernetes 集群，以安装 TSB 和示例应用程序 按照 TSB 演示 安装说明操作 创建一个 AWS 帐户以启动 EC2 实例，在那里部署工作负载，并将其载入到服务网格。 安装 Bookinfo …","relpermalink":"/tsb/setup/workload-onboarding/quickstart/aws-ec2/","summary":"本指南将帮助你实际开始使用“工作负载载入”。 作为本指南的一部分，你将： 在你的 Kubernetes 集群中部署 Istio Bookinfo 示例 在 AWS EC2 实例上部署 ratings 应用程序并将其载入到服务网格 验证 Kubernetes Pod(s) 与 AWS EC2 实例之间的流量 在 AWS Auto Scaling Group 上部署 ratings 应用程序并将","title":"在 AWS EC2 上快速载入工作负载"},{"content":" Access Bindings\nAPI\nAPI Access Bindings\nApplication\nApplication Access Bindings\nApplication Service\nApprovals Service\nAuth\nAuth\nCluster Service\nClusters\nCommon Object Types\nEast/West Gateway\nEgress Gateway\nGateway\nGateway Access Bindings\nGateway Common Configuration Messages\nGateway Group\nGateway Service\nInfo\nIngress Gateway\nIstio Direct Mode Gateway Service\nIstio Direct Mode Security Service\nIstio Direct Mode Traffic Service\nIstio Internal Access Bindings\nIstio Internal Direct Mode Service …","relpermalink":"/tsb/refs/tsb/","summary":"Access Bindings\nAPI\nAPI Access Bindings\nApplication\nApplication Access Bindings\nApplication Service\nApprovals Service\nAuth\nAuth\nCluster Service\nClusters\nCommon Object Types\nEast/West Gateway\nEgress Gateway\nGateway\nGateway Access Bindings\nGateway Common Configuration Messages\nGateway Group\nGateway Service\nInfo\nIngress Gateway\nIstio Direct Mode Gateway Service\nIstio Direct Mode Security Service\nIstio Direct Mode Traffic Service\nIstio Internal Access Bindings\nIstio Internal Direct Mode Service\nIstio Internal Direct Mode Service\nIstio internal Group\nMetric\nMetric Service\nOpenAPI Extensions\nOrganization\nOrganization Access Bindings\nOrganization Setting\nOrganizations Service\nPermissions\nPermissions Service\nPolicy Bindings\nPolicy Service\nRegistered Service\nRole\nRole Service\nSecurity Access Bindings","title":"tsb"},{"content":"Tetrate Servcie Bridge（TSB）基于开源的 Istio、Envoy 和 SkyWalking 建立，是 Tetrate 的旗舰产品。本手册将帮助你全面地了解 TSB。无论你是应用程序开发人员、平台运维者，我们都会定制内容来满足你的需求。如果你遇到任何障碍，请放心，我们随时提供支持。\n对于应用程序开发人员 作为使用 TSB 将应用程序部署到环境中的应用程序开发人员，你将体验到简化的过程。首先使用 Sidecar 代理部署你的应用程序。然后，深入研究高级配置，例如将流量路由到应用程序、实施速率限制或在虚拟机和 Kubernetes 应用程序之间划分流量以实现逐步现代化。\n理解关键概念 掌握服务网格架构 探索 TSB 的架构 高效的交通管理 TSB 的全局可观测性 部署和配置应用程序 使用 Sidecar 部署应用程序 - 如果需要，请熟悉 Istio 的故障排除资源。 为外部流量配置 TSB 使用 OpenAPI 注释 高效的应用管理 监控指标和跟踪 解决常见用例 将流量引导至应用程序 实施速率限制 逐步金丝雀发布 将虚拟机流量迁移到 Kubernetes 跨集群故障 …","relpermalink":"/tsb/","summary":"Tetrate Service Bridge（TSB）中文文档：安装、使用和升级。","title":"Tetrate Service Bridge 手册"},{"content":"工作负载载入是 TSB 的一个功能，它自动化了在服务网格中将在 Kubernetes 之外部署的工作负载载入的过程。\n例如，你可以使用它来将部署在虚拟机上（或者可能是自动缩放组中的虚拟机）的工作负载载入，这些工作负载并不属于你的 Kubernetes 集群的一部分。\n注意\n工作负载载入功能目前是一个 alpha 版本。\n有可能它尚不支持所有可能的部署场景。尤其值得注意的是，它尚不支持使用 Iptables 进行流量重定向。你应该根据需要配置 Istio Sidecar 和你的应用程序。\n目前，此功能支持从以下环境载入工作负载：\n部署在 AWS EC2 实例上的工作负载 部署在 AWS Auto-Scaling Groups 上的工作负载 作为 AWS ECS 任务部署的工作负载 部署在本地环境的工作负载 概览\n设置工作负载载入\n载入虚拟机\n管理已载入的工作负载\n故障排除指南\n在本地载入工作负载\n载入 AWS ECS 工作负载\n","relpermalink":"/tsb/setup/workload-onboarding/guides/","summary":"工作负载载入是 TSB 的一个功能，它自动化了在服务网格中将在 Kubernetes 之外部署的工作负载载入的过程。 例如，你可以使用它来将部署在虚拟机上（或者可能是自动缩放组中的虚拟机）的工作负载载入，这些工作负载并不属于你的 Kubernetes 集","title":"工作负载载入指南"},{"content":"本指南描述了 Tetrate 管理平面的灾难恢复方案及其影响。适用于 Tetrate Service Bridge（TSB）和 Tetrate Service Express（TSE）。\nTetrate 管理平面的设计（以及分布式控制平面架构）具有以下特点：\n架构松散耦合：Tetrate 的架构设计松散耦合且具有自我修复能力，这意味着故障的“影响范围”有限，当组件恢复时，平台很快会达到良好的配置状态。 所有 Tetrate 组件都是无状态的，可以从故障中恢复：唯一的例外是 Postgres 数据库（配置和审计日志）和 ElasticSearch 数据库（指标），以及 K8s 集群中的秘密。 应用和服务不受影响：管理或控制平面组件的任何故障都不会影响在工作负载集群中运行的应用程序和服务的正确操作或安全性。 高可用性：我们建议以冗余的高可用方式运行工作负载。冗余的高可用管理平面是可能的，但在 Tetrate 的松散耦合架构中，带来的好处有限，代价是资源使用和额外复杂性。 使用 Tetrate 实现高可用性和灾难恢复 工作负载和数据平面的高可用性 Tetrate 可以帮助你管理和运营跨区域和 …","relpermalink":"/tsb/design-guides/ha-dr-mp/","summary":"本指南描述了 Tetrate 管理平面的灾难恢复方案及其影响。适用于 Tetrate Service Bridge（TSB）和 Tetrate Service Express（TSE）。 Tetrate 管理平面的设计（以及分布式控制平面架构）具有以下特点： 架构松散耦合：Tetrate 的架构","title":"理解高可用性"},{"content":" 在 Kubernetes 和虚拟机之间切分流量\n将虚拟机上的单体应用迁移到你的集群\n客户端负载均衡\n将流量发送到使用 HTTPS 的外部主机\n配置（多端口、多协议）服务的 ServiceRoute\n金丝雀发布\n","relpermalink":"/tsb/howto/traffic/","summary":"在 Kubernetes 和虚拟机之间切分流量 将虚拟机上的单体应用迁移到你的集群 客户端负载均衡 将流量发送到使用 HTTPS 的外部主机 配置（多端口、多协议）服务的 ServiceRoute 金丝雀发布","title":"流量管理和迁移"},{"content":" 在应用集群中启用 Tier1 网关\n流式服务日志\nIstio CNI\n网关删除保持 Webhook\n配置保护\nEdge 处的 DNS 解析\nGitOps\n配置集群外部地址\n","relpermalink":"/tsb/operations/features/","summary":"在应用集群中启用 Tier1 网关 流式服务日志 Istio CNI 网关删除保持 Webhook 配置保护 Edge 处的 DNS 解析 GitOps 配置集群外部地址","title":"特性"},{"content":"本指南描述了为应用载入准备 Tetrate 管理的平台的基本建议。适用于 Tetrate Service Express (TSE) 和 Tetrate Service Bridge (TSB)。\n为简单起见，本文档仅考虑在 Kubernetes 集群上部署的服务。\n用户类型 本文档假设存在以下两种用户类型：\n平台所有者：用户可以直接访问 TSE 或 TSB，并希望预先配置平台以接收和托管服务。平台所有者需要定义适当的默认值和防护措施，并准备附加服务，如 DNS 或仪表板。\n应用所有者：用户无法访问 TSE 或 TSB，但可以访问 Kubernetes 集群中的一个或多个命名空间。每个应用所有者希望使用标准的 Kubernetes API 和工具（如 CD 流水线）将生产服务部署到集群中。\nTSB 用户和角色层次结构 TSB 提供了一个非常丰富的 用户和角色层次结构，允许平台所有者将有限的 TSB 功能委托给其他用户类型，包括多个应用所有者用户和团队。本文档不涵盖这些更复杂的情况。\n相反，本文档适用于由 TSE 和 TSB 支持的更简单的 “一个平台所有者团队，多个应用所有者，高信任” …","relpermalink":"/tsb/design-guides/app-onboarding/","summary":"本指南描述了为应用载入准备 Tetrate 管理的平台的基本建议。适用于 Tetrate Service Express (TSE) 和 Tetrate Service Bridge (TSB)。 为简单起见，本文档仅考虑在 Kubernetes 集群上部署的服务。 用户类型 本文档假设存在以下两种用户类型： 平台所有者：用户可以直接访问 TSE","title":"应用载入最佳实践"},{"content":"Argo CD是一种开源的持续交付工具，用于自动化和管理应用程序的部署、更新和回滚。它是一个声明式的工具，专为在 Kubernetes 集群中进行应用程序部署而设计。\n🔔 注意：本文档根据 Argo CD v2.8 Commit 4d2cd06f86（北京时间 2023 年 6 月 30 日 19 时）翻译。\nArgo CD 的主要功能包括：\n持续交付：Argo CD 允许用户将应用程序的配置和清单文件定义为 Git 存储库中的声明式资源，从而实现持续交付。它能够自动检测 Git 存储库中的更改，并将这些更改应用于目标 Kubernetes 集群。\n健康监测和回滚：Argo CD 能够监测应用程序的健康状态，并在检测到问题时触发回滚操作。这有助于确保应用程序在部署期间和运行时保持稳定和可靠。\n多环境管理：Argo CD 支持多个环境（例如开发、测试、生产）的管理。它可以帮助用户在不同环境中进行应用程序的部署和配置管理，并确保这些环境之间的一致性。\n基于 GitOps 的操作：Argo CD 采用了 GitOps 的操作模式，即将应用程序的状态和配置定义为 Git 存储库中的声明式资源。 …","relpermalink":"/argo-cd/","summary":"Argo CD 中文文档（非官方）","title":"Argo CD 中文文档"},{"content":"Argo Rollouts 是一个 Kubernetes 控制器，它提供了在应用程序部署过程中执行渐进式发布和蓝绿部署等高级部署策略的能力。它是基于 Kubernetes 原生的 Deployment 资源构建的，通过引入新的 Rollout 资源来扩展和增强部署控制。\n🔔 注意：本文档根据 Argo Rollouts v1.5 Commit 1d53b25（北京时间 2023 年 6 月 21 日 3 时）翻译。\nArgo Rollouts 具有以下主要功能：\n渐进式发布（Progressive Delivery）：Argo Rollouts 允许你逐步增加新版本的流量并监控其性能，以确保新版本稳定可靠。你可以通过配置渐进式发布的步骤和条件来控制流量的切换和回滚。\n蓝绿部署（Blue-Green Deployment）：Argo Rollouts 支持蓝绿部署模式，其中在新旧版本之间进行无缝切换。通过在新版本上运行一些或全部流量，并根据用户的反馈和性能指标来验证新版本的稳定性，可以确保零停机时间的部署。\n金丝雀部署（Canary Deployment）：Argo Rollouts 支 …","relpermalink":"/argo-rollouts/","summary":"Argo Rollouts 中文文档（非官方）","title":"Argo Rollouts 中文文档"},{"content":" 蓝绿部署\n金丝雀部署\n","relpermalink":"/argo-rollouts/rollout/deployment-strategies/","summary":"蓝绿部署 金丝雀部署","title":"部署策略"},{"content":"本书译自 Solving the Bottom Turtle — a SPIFFE Way to Establish Trust in Your Infrastructure via Universal Identity，译者 Jimmy Song。\n《零信任的基石》封面 Copyright Solving the Bottom Turtle — a SPIFFE Way to Establish Trust in Your Infrastructure via Universal Identity\nby Daniel Feldman, Emily Fox, Evan Gilman, Ian Haken, Frederick Kautz, Umair Khan, Max Lambrecht, Brandon Lum, Agustín Martínez Fayó, Eli Nesterov, Andres Vega, Michael Wardrop. 2020.\nThis work is licensed under the Creative Commons Attribution …","relpermalink":"/spiffe/","summary":"本书系统的讲解了 SPFFE 来解决零信任的身份问题。","title":"零信任的基石：使用 SPIFFE 为基础设施创建通用身份"},{"content":" 注意\n本书正在编写中。 ","relpermalink":"/hugo-in-action/","summary":"静态网站","title":"Hugo 实战"},{"content":" 注意\n《Cilium 中文指南》除特殊说明，默认基于 Cilium 1.11 稳定版本。 《Cilium 中文指南》当前内容译自 Cilium 官方文档，节选了以下章节：\n概念：描述了 Cilium 的组件以及部署 Cilium 的不同模式。提供高层次的 运行一个完整的 Cilium 部署并理解其行为所需的高层次理解。 开始：快速开始使用 Cilium。 策略：详细介绍了策略语言结构和支持的格式。 内部原理：介绍了一些组件的内部细节。 其他未翻译部分主要涉及命令、API 及运维，本指南未来会加入笔者个人观点及其他内容。\n大纲 Cilium 和 Hubble 简介\n多集群（集群网格）\nCilium 概念\n网络\n网络安全\neBPF 数据路径\n网络策略\nKubernetes 集成\n开始阅读 ","relpermalink":"/cilium-handbook/","summary":"Cilium 中文指南","title":"Cilium 中文指南"},{"content":"《什么是 eBPF —— 新一代网络、安全和可观测性工具介绍》译自 O’Reilly 发布的报告“What is eBPF”，作者是 Liz Rice，由 JImmy Song 翻译，英文原版可以在 O’Reilly 网站上获取。\n《什么是 eBPF》中文版封面 译者序 最近两年来关于 eBPF 的讨论在云原生社区里越来越多，尤其是当谈到 Cilium 的商业化，使用 eBPF 来优化 Istio 服务网格，甚至扬言干掉 Sidecar 时，eBPF 更是赚足了眼球。\n这本报告是由基于 Cilium 的创业公司 Isovalent 的 Liz Rice 撰写，由 O’Reilly 发布，相信可以为你揭开 eBPF 技术的神秘面纱，带你了解什么是 eBPF 还有它的强大之处。更重要的是它在云原生环境中，在服务网格、可观测性和安全中的应用。\n关于作者 Liz Rice 是云原生网络和安全专家，Isovalent 的首席开源官，是基于 eBPF 的 Cilium 网络项目的创建者。她在 2019-2022 年担任 CNCF 的技术监督委员会（TOC）主席，并在 2018 …","relpermalink":"/what-is-ebpf/","summary":"新一代网络、安全和可观测性工具简介。","title":"什么是 eBPF？"},{"content":" 关于本教程\n本教程迁移自《Kubernetes 中文指南——云原生应用架构实战手册》，原手册使用 Gitbook 发布，内容涵盖 容器、Kubernetes、服务网格、Serverless 等云元生的多个领域，因内容过于宽泛，且 Gitbook 项目已停止维护，现将其中的 Kubernetes 教程部分独立成书，并使用 Hugo 重新构建。 云原生是一种行为方式和设计理念，究其本质，凡是能够提高云上资源利用率和应用交付效率的行为或方式都是云原生的。云计算的发展史就是一部云原生化的历史。Kubernetes 开启了云原生的序幕，服务网格 Istio 的出现，引领了后 Kubernetes 时代的微服务，Serverless 的兴起，使得云原生从基础设施层不断向应用架构层挺进，我们正处于一个云原生的新时代。\n《Kubernetes 基础教程》封面 Kubernetes 是 Google 于 2014 年 6 月基于其内部使用的 Borg 系统开源出来的容器编排调度引擎，Google 将其作为初始和核心项目贡献给 CNCF（云原生计算基金会），近年来逐渐发展出了云原生生态。 …","relpermalink":"/kubernetes-handbook/","summary":"云原生应用架构实战手册","title":"Kubernetes 基础教程"},{"content":"Kubernetes Hardening Guidance（查看英文原版 PDF）是由美国国家安全局（NSA）于 2021 年 8 月发布的，其中文版《Kubernetes 加固指南》（或译作《Kubernetes 强化指南》），译者 Jimmy Song。\n《Kubernetes 加固指南》封面 许可证 您可以使用署名 - 非商业性使用 - 相同方式共享 4.0 (CC BY-NC-SA 4.0) 协议共享。\n交流群 欢迎加入云原生社区可观测性讨论组（微信群）参与讨论交流，加入前请先填写入群申请问卷后联系 Jimmy Song 入群。\n开始阅读 ","relpermalink":"/kubernetes-hardening-guidance/","summary":"本指南译自美国国家安全局（NSA）于 2021 年 8 月发布的的 Kubernetes Hardening Guidance。","title":"Kubernetes 加固指南"},{"content":" 软件正在吞噬世界。\n——Mark Andreessen\n近些年来，在一些长期由领导者支配的行业中，这些领导者的领先地位已经岌岌可危，这都是由以这些行业为核心业务的软件公司造成的。像 Square、Uber、Netflix、Airbnb 和特斯拉这样的公司能够持续快速增长，并且拥有傲人的市场估值，成为它们所在行业的新领导者。这些创新公司有什么共同点？\n快速创新\n持续可用的服务\n弹性可扩展的 Web\n以移动为核心的用户体验\n将软件迁移到云上是一种自演化，使用了云原生应用架构是这些公司能够如此具有破坏性的核心原因。对于云，我们指的是一个任何能够按需、自助弹性提供和释放计算、网络和存储资源的计算环境。云的定义包括公有云（例如 Amazon Web Services、Google Cloud 和 Microsoft Azure）和私有云（例如 VMware vSphere 和 OpenStack）。\n本章中我们将探讨云原生应用架构的创新性，然后验证云原生应用架构的主要特性。\n开始阅读 ","relpermalink":"/migrating-to-cloud-native-application-architectures/the-rise-of-cloud-native/","summary":"软件正在吞噬世界。 ——Mark Andreessen 近些年来，在一些长期由领导者支配的行业中，这些领导者的领先地位已经岌岌可危，这都是由以这些行业为核心业务的软件公司造成的。像 Square、Uber、Netflix、Ai","title":"第一章：云原生的崛起"},{"content":"Google 有许多通用工程实践，几乎涵盖所有语言和项目。此文档为长期积累的最佳实践，是集体经验的结晶。我们尽可能地将其公之于众，您的组织和开源项目也会从中受益。\n《谷歌工程实践》封面 当前包含以下文档：\nGoogle 代码审查指南，实则两套指南：\n代码审查者指南 代码开发者指南 译者序 此仓库翻译自 google/eng-practices，目前为止的主要内容为 Google 总结的如何进行 Code Review（代码审查） 指南，根据原 Github 仓库的标题判断以后会追加更多 Google 工程实践的内容。\n许可证 您可以使用署名 - 非商业性使用 - 相同方式共享 4.0 (CC BY-NC-SA 4.0) 协议共享。\n交流群 欢迎加入云原生社区微信讨论群，加入前请先填写入群申请问卷后联系 Jimmy Song 入群。\n开始阅读 ","relpermalink":"/eng-practices/","summary":"谷歌代码审查实践指南","title":"谷歌工程实践"},{"content":"本书译自美国国家标准标准与技术研究院（NIST）Special Publication 800-204C。\n《利用服务网格为基于微服务的应用程序实施 DevSecOps》封面 本书大纲 声明\n执行摘要\n第一章：简介\n第二章：实施 DevSecOps 原语的参考平台\n第三章：DevSecOps 组织准备、关键基本要素和实施\n第四章：为参考平台实施 DevSecOps 原语\n第五章：摘要和结论\n关于本书 作者：Ramaswamy Chandramouli\n计算机安全司信息技术实验室\n美国商务部\nGina M. Raimondo，秘书\n国家标准和技术研究所\nJames K. Olthoff，履行负责标准和技术的商务部副部长兼国家标准和技术研究所所长的非专属职能和职责\n本出版物可在：https://doi.org/10.6028/NIST.SP.800-204C 免费获取。\n开始阅读 ","relpermalink":"/service-mesh-devsecops/","summary":"本书译自美国国家标准标准与技术研究院（NIST）Special Publication 800-204C。","title":"利用服务网格为基于微服务的应用程序实施 DevSecOps"},{"content":"本书是 Migrating to Cloud Native Application Architectures 的中文版，本书英文版发布于 2015 年 2 月，中文版由 Jimmy Song 翻译，发布于 2017 年 7 月。\n《迁移到云原生应用架构》图书封面 译者序 云时代的云原生应用大势已来，将传统的单体架构应用迁移到云原生架构，你准备好了吗？\n俗话说“意识决定行动”，在迁移到云原生应用之前，我们大家需要先对 Cloud Native（云原生）的概念、组织形式并对实现它的技术有一个大概的了解，这样才能指导我们的云原生架构实践。\n英文版作于 2015 年，其中的示例主要针对 Java 应用，实际上也适用于任何应用类型，云原生应用架构适用于异构语言的程序开发，不仅仅是针对 Java 语言的程序开发。截止到本人翻译本书时，云原生应用生态系统已经初具规模，CNCF 成员不断发展壮大，基于 Cloud Native 的创业公司不断涌现，Kubernetes 引领容器编排潮流，和 Service Mesh 技术（如 Linkerd 和 Istio）的出现，Go 语言的兴起等为我们将应用迁移 …","relpermalink":"/migrating-to-cloud-native-application-architectures/","summary":"本书是 Migrating to Cloud Native Application Architectures 的中文版。","title":"迁移到云原生应用架构"},{"content":"本书为 Cloud Native Infrastructure 中文版，作者 Justin Garrison 和 Kris Nova，英文版发行于 2017 年 11 月，已可以在网上免费获得，本书是关于创建和管理基础架构，以适用于云原生应用全生命周期管理的模式和实践。\n《云原生基础架构》封面 阅读完这本书后，您将会有如下收获：\n理解为什么说云原生基础架构是高效运行云原生应用所必须的 根据准则来决定您的业务何时以及是否应该采用云原生 了解部署和管理基础架构和应用程序的模式 设计测试以证明您的基础架构可以按预期工作，即使在各种边缘情况下也是如此 了解如何以策略即代码的方式保护基础架构 本书大纲 前言\n介绍\n第 1 章：什么是云原生基础架构？\n第 2 章：采纳云原生基础架构的时机\n第 3 章：云原生部署的演变\n第 4 章：设计基础架构应用程序\n第 5 章：开发基础架构应用程序\n第 6 章：测试云原生基础架构\n第 7 章：管理云原生应用程序\n第 8 章：保护应用程序\n第 9 章：实施云原生基础架构\n附录 A：网络弹性模式\n附录 B：锁定\n附录 C Box：案例研究\n免责声明 本书英文版版权属 …","relpermalink":"/cloud-native-infra/","summary":"《云原生基础架构》，Cloud Native Infrastructure 中文版。","title":"云原生基础架构"},{"content":"本报告译自 O’Reilly 出品的 The Future of Observablity with OpeTelemetry，作者 Ted Young，译者 Jimmy Song。\n《OpenTelemetry 可观测性的未来》封面 关于本书 本书内容包括：\nOpenTelemetry 如何满足库作者、应用程序拥有者、运维和响应者的需求 应用程序的不同角色如何围绕 OpenTelemetry 来协同和独立工作 关于在组织中采用和管理 OpenTelemetry 的实用建议 关于作者 Ted Young 是 OpenTelemetry 项目的联合创始人之一。在过去的二十年里，他设计并建立了各种大规模的分布式系统，包括可视化 FX 管道和容器调度系统。他目前在 Lightstep 公司担任开发者教育总监，住在俄勒冈州波特兰的一个小农场里。\n许可证 您可以使用署名 - 非商业性使用 - 相同方式共享 4.0 (CC BY-NC-SA 4.0) 协议共享。\n交流群 欢迎加入云原生社区可观测性讨论组（微信群）参与讨论交流，加入前请先填写入群申请问卷后联系 Jimmy Song 入群。 …","relpermalink":"/opentelemetry-obervability/","summary":"本报告译自 O'Reilly 出品的 The Future of Observablity with OpenTelemetry，作者 Ted Young，译者 Jimmy Song。","title":"OpenTelemetry 可观测性的未来"},{"content":"欢迎来到云原生导览——一站式了解云原生技术体系。\n导览图 关于云原生\nKubernetes\n服务网格\n社区\n开始阅读 ","relpermalink":"/cloud-native-handbook/","summary":"云原生技术体系导览","title":"云原生导览"},{"content":"本章将介绍什么是云原生、云原生应用。\n本章大纲 什么是云原生？\n云原生的设计哲学\n什么是云原生应用？\n云原生快速入门\n阅读本章 ","relpermalink":"/cloud-native-handbook/intro/","summary":"本章介绍什么是云原生。","title":"关于云原生"},{"content":"简介 代码审查是除了代码作者之外，其他人检查代码的过程。\nGoogle 通过 Code Review 来维护代码和产品质量。\n此文档是 Google Code Review 流程和政策的规范说明。\n此页面是我们进行 Code Review 流程的概述。本指南还有另外两套文档：\n如何进行 Code Review：针对代码审查者的详细指南。 代码开发者指南：针对 CL 开发者的的详细指南。 代码审查者应该关注哪些方面？ 代码审查时应该关注以下方面：\n设计：代码是否经过精心设计并适合您的系统？ 功能：代码的行为是否与作者的意图相同？代码是否可以正常响应用户的行为？ 复杂度：代码能更简单吗？将来其他开发人员能轻松理解并使用此代码吗？ 测试：代码是否具有正确且设计良好的自动化测试？ 命名：开发人员是否为变量、类、方法等选择了明确的名称？ 注释：注释是否清晰有用？ 风格：代码是否遵守了风格指南？ 文档：开发人员是否同时更新了相关文档？ 参阅 如何进行 Code Review 获取更多资料。\n选择最合适审查者 一般而言，您希望找到能在合理的时间内回复您的评论的最合适的审查者。\n最合适的审查者应该是能 …","relpermalink":"/eng-practices/review/","summary":"简介 代码审查是除了代码作者之外，其他人检查代码的过程。 Google 通过 Code Review 来维护代码和产品质量。 此文档是 Google Code Review 流程和政策的规范说明。 此页面是我们进行 Code Review 流程的概述。本指南还有另外两套文档： 如何进行 Code Review","title":"代码审查指南"},{"content":" SPIFFE ID 和 SVID\nX.509 SVID\nJWT SVID\nSPIFFE 工作负载 API\nSPIFFE 工作负载端点\nSPIFFE 信任域和 Bundle\nSPIFFE 联邦\n","relpermalink":"/spiffe-and-spire/standard/","summary":"SPIFFE ID 和 SVID X.509 SVID JWT SVID SPIFFE 工作负载 API SPIFFE 工作负载端点 SPIFFE 信任域和 Bundle SPIFFE 联邦","title":"规范"},{"content":"本指南将帮助你快速入门实际中的“工作负载载入”。\n在本指南的一部分中，你将会：\n将 Istio Bookinfo 示例部署到 Elastic Kubernetes Service (EKS) 集群中。 将 ratings 应用程序部署为 AWS ECS 任务，并将其加入服务网格。 验证 Kubernetes Pod(s) 与 AWS ECS 任务之间的流量。 本指南旨在演示工作负载加入功能的易于跟随的演示。\n为了保持简单，你无需像在生产部署的情况下那样配置基础设施。\n具体而言：\n你无需设置可路由的 DNS 记录。 你无需使用受信任的 CA 机构（例如 Let’s Encrypt）。 在继续之前，请确保完成以下先决条件：\n创建一个 EKS 集群，以便安装 TSB 和示例应用程序。 按照 TSB 演示 安装说明进行安装。 按照 安装 Bookinfo 示例 中的说明进行操作。 按照 启用工作负载加入 中的说明进行操作。 配置工作负载的 WorkloadGroup 和 Sidecar\n加入 AWS ECS 任务\n","relpermalink":"/tsb/setup/workload-onboarding/quickstart/aws-ecs/","summary":"本指南将帮助你快速入门实际中的“工作负载载入”。 在本指南的一部分中，你将会： 将 Istio Bookinfo 示例部署到 Elastic Kubernetes Service (EKS) 集群中。 将 ratings 应用程序部署为 AWS ECS 任务，并将其加入服务网格。 验证 Kubernetes Pod(s) 与 AWS ECS 任务之间的流量。 本指南旨在演示","title":"在 AWS ECS 上使用工作负载快速入门"},{"content":" Agent Configuration\nAWS Identity\nAWS Identity Matcher\nCondition\nCore types\nHost Info\nJWT Identity\nJWT Identity Matcher\nJWT Issuer\nOnboarding Configuration\nOnboarding Policy\nTransport layer security config\nWorkload Auto Registration\nWorkload Configuration\nWorkload Identity\nWorkload Registration\n","relpermalink":"/tsb/refs/onboarding/","summary":"Agent Configuration\nAWS Identity\nAWS Identity Matcher\nCondition\nCore types\nHost Info\nJWT Identity\nJWT Identity Matcher\nJWT Issuer\nOnboarding Configuration\nOnboarding Policy\nTransport layer security config\nWorkload Auto Registration\nWorkload Configuration\nWorkload Identity\nWorkload Registration","title":"onboard"},{"content":"欢迎来到 Tetrate Service Bridge (TSB) 的概念部分。本节向你介绍 TSB 背后的基本思想、其架构以及它如何在你的环境中工作。\nTSB 如何运作？ Tetrate Service Bridge 是一个在基础设施之上运行的服务网格管理平面，提供一个集中平台来管理和配置整个网格管理环境的网络、安全性和可观测性。\n以下是 TSB 工作原理的概述：\n逻辑视图：TSB 通过将资源分组为 services 、 workspaces 和 groups 来组织你的环境，使它们更易于管理。 基于租户的方法：TSB 鼓励你在组织内创建 tenants 。它将用户帐户和团队与你的公司目录同步，从而简化访问管理。 访问控制：TSB 允许你定义细粒度的访问控制，提供编辑权限并实施零信任方法。这增强了安全性并使你能够监控环境中的活动。 审计跟踪：TSB 跟踪服务和共享资源的更改，确保对所有操作（无论是批准还是拒绝）进行审计跟踪。 配置管理：TSB 允许你编写配置更改并将其分组到 services 中，从而使你能够高效地进行更改并集中管理它们。 隔离故障域：TSB …","relpermalink":"/tsb/concepts/","summary":"欢迎来到 Tetrate Service Bridge (TSB) 的概念部分。本节向你介绍 TSB 背后的基本思想、其架构以及它如何在你的环境中工作。 TSB 如何运作？ Tetrate Service Bridge 是一个在基础设施之上运行的服务网格管理平面，提供一个集中平台来管理和配置整个网格管理环境的","title":"概念"},{"content":" AWS EC2\nAWS ECS\n本地工作负载\n","relpermalink":"/tsb/setup/workload-onboarding/quickstart/","summary":"AWS EC2 AWS ECS 本地工作负载","title":"工作负载快速载入"},{"content":" 统一网关\n使用 IngressGateway 和 ServiceRoute 基于子集的流量路由\n共享入口网关\n多集群访问控制和身份传播\n使用东西网关实现多集群流量故障转移\n使用 Tier-2 网关进行多集群流量路由\n使用 Tier-1 网关进行多集群流量切换\n配置 Authz 以支持代理协议\n使用 Keycloak 进行终端用户身份验证\n控制对外部服务的访问\n分布式入口网关\n配置和路由 TSB 中的 HTTP、非 HTTP（多协议）和多端口服务流量\n使用 OpenAPI 注解配置应用程序网关\n应用程序入口\n","relpermalink":"/tsb/howto/gateway/","summary":"统一网关 使用 IngressGateway 和 ServiceRoute 基于子集的流量路由 共享入口网关 多集群访问控制和身份传播 使用东西网关实现多集群流量故障转移 使用 Tier-2 网关进行多集群流量路由 使用 Tier-1 网关进行多集群流量切换 配置 Authz 以支持代理协议 使用 Keycloak 进行终端用户","title":"配置网关"},{"content":"在整个安装过程中，你可能会发现参考“安装与升级”部分中介绍的主题很有用。请务必查看下载 tctl 和使用 tctl 连接到 TSB 等主题，以增强你的理解。\n如需快速概览，你还可以浏览我们的演示安装指南。\n通过遵循这些指南，你将能够使用 tctl 无缝安装 TSB，无论是在本地基础设施上还是在云服务器上。\n演示安装\n管理平面安装\n载入集群\nTSB 卸载\nTSB 升级\n","relpermalink":"/tsb/setup/self-managed/","summary":"在整个安装过程中，你可能会发现参考“安装与升级”部分中介绍的主题很有用。请务必查看下载 tctl 和使用 tctl 连接到 TSB 等主题，以增强你的理解。 如需快速概览，你还可以浏览我们的演示安装指南。 通过遵循这些指南，你将能够","title":"使用 tctl 安装"},{"content":" 更改管理员密码\n将 LDAP 配置为身份提供者\nAzure AD 作为身份提供者\n用户同步\n角色和权限\n","relpermalink":"/tsb/operations/users/","summary":"更改管理员密码 将 LDAP 配置为身份提供者 Azure AD 作为身份提供者 用户同步 角色和权限","title":"用户权限"},{"content":"本章将介绍什么是云原生、云原生应用。\n本章大纲 什么是 Kubernetes?\nKubernetes 的历史\nSidecar 模式\n阅读本章 ","relpermalink":"/cloud-native-handbook/kubernetes/","summary":"关于 Kubernetes。","title":"Kubernetes"},{"content":"本章将介绍服务网格。\n阅读本章 ","relpermalink":"/cloud-native-handbook/service-mesh/","summary":"本章介绍服务网格。","title":"服务网格"},{"content":" 扩展 SPIRE 部署\nSPIRE 嵌套架构\nSPIRE 联邦架构\n","relpermalink":"/spiffe-and-spire/architecture/","summary":"扩展 SPIRE 部署 SPIRE 嵌套架构 SPIRE 联邦架构","title":"架构"},{"content":" 入门指南\ntctl\ntctl apply\ntctl collect\ntctl completion\ntctl config\ntctl delete\ntctl edit\ntctl experimental\ntctl get\ntctl install\ntctl login\ntctl ui\ntctl validate\ntctl version\ntctl whoami\nWorkloadEntry Annotations\n","relpermalink":"/tsb/reference/cli/","summary":"入门指南\ntctl\ntctl apply\ntctl collect\ntctl completion\ntctl config\ntctl delete\ntctl edit\ntctl experimental\ntctl get\ntctl install\ntctl login\ntctl ui\ntctl validate\ntctl version\ntctl whoami\nWorkloadEntry Annotations","title":"CLI"},{"content":" TSB 中的 GitOps\n配置 Flux CD 进行 GitOps\n使用 Argo Rollout 和 SkyWalking 进行金丝雀分析和渐进式交付\n","relpermalink":"/tsb/howto/gitops/","summary":"TSB 中的 GitOps\n配置 Flux CD 进行 GitOps\n使用 Argo Rollout 和 SkyWalking 进行金丝雀分析和渐进式交付","title":"GitOps"},{"content":" IstioOperator Options\n","relpermalink":"/tsb/refs/istio.io/","summary":"IstioOperator Options","title":"istio.io"},{"content":" 先决条件和下载\n使用 tctl 安装\n使用 Helm 安装\n载入工作负载\nAWS\n证书安装\n使用 tctl 连接到 TSB\n资源消耗与容量规划\nTSB 组件\n防火墙信息\nIstio 隔离边界\n仓库机密\n控制平面升级\n从 tctl 迁移到 Helm\n","relpermalink":"/tsb/setup/","summary":"先决条件和下载 使用 tctl 安装 使用 Helm 安装 载入工作负载 AWS 证书安装 使用 tctl 连接到 TSB 资源消耗与容量规划 TSB 组件 防火墙信息 Istio 隔离边界 仓库机密 控制平面升级 从 tctl 迁移到 Helm","title":"安装与升级"},{"content":"本指南将帮助你在实践中开始使用“工作负载载入”。\n作为这个指南的一部分，你将会：\n将 Istio Bookinfo 示例部署到你的 Kubernetes 集群中。 在本地虚拟机上部署 ratings 应用程序，并将其加入到服务网格中。 验证 Kubernetes Pod(s) 和本地虚拟机之间的流量。 本指南旨在演示工作负载载入功能的易于跟随的示例。\n为了保持简单，你无需像在生产部署的情况下那样配置基础设施。\n具体来说：\n你无需设置可路由的 DNS 记录。 你无需使用受信任的 CA 机构（例如 Let’s Encrypt）。 在继续之前，请确保完成以下先决条件：\n创建一个 Kubernetes 集群，以安装 TSB 和示例应用程序。 按照 TSB 演示 安装的说明进行操作。 按照 安装示例 Bookinfo 的说明进行操作。 按照 启用工作负载载入 的说明进行操作。 确保本地虚拟机和 Kubernetes 集群位于相同的网络或对等网络上。 配置本地 WorkloadGroup 和 Sidecar\n配置本地虚拟机\n从本地虚拟机上进行工作负载载入\n","relpermalink":"/tsb/setup/workload-onboarding/quickstart/on-premise/","summary":"本指南将帮助你在实践中开始使用“工作负载载入”。 作为这个指南的一部分，你将会： 将 Istio Bookinfo 示例部署到你的 Kubernetes 集群中。 在本地虚拟机上部署 ratings 应用程序，并将其加入到服务网格中。 验证 Kubernetes Pod(s) 和本地虚拟机之间的流量。 本指南","title":"在本地快速入门工作负载"},{"content":"本设计指南解释了如何配置一个跨多个云区域的多个工作负载集群的高可用部署。\n对于关键业务和大规模服务，可能需要在两个或多个区域部署，以实现可伸缩性和高可用性。本设计指南解释了如何使用 Tetrate 的 Edge Gateway 解决方案来实现这一目标。Edge Gateway 提供了一个高可用负载均衡代理的前端层，接收流量并将其转发到工作负载集群。\n介绍 Tetrate Edge Gateway 解决方案 管理员可以通过多种方式配置其生产环境，以在两个或多个工作负载集群之间分发流量。\n常见的高可用模式 对于简单的小规模部署，通常会看到几种模式：\n全局服务器负载均衡（GSLB）。使用 GSLB 系统，每个工作负载集群都会被分配一个公共端点（IP 地址），并使用 DNS 控制将用户定向到哪个端点。GSLB 系统对每个端点执行健康检查，并将失败的集群排除在轮换之外，它还可以执行其他负载均衡措施，如接近路由（将客户端发送到最近的集群）或基于负载的路由（将客户端发送到性能最佳的集群）。 GSLB 解决方案通常使用 DNS 控制客户端分配到数据中心的方式。基于 DNS 的控制的缺点是客户端和中间服 …","relpermalink":"/tsb/design-guides/ha-multicluster/","summary":"本设计指南解释了如何配置一个跨多个云区域的多个工作负载集群的高可用部署。 对于关键业务和大规模服务，可能需要在两个或多个区域部署，以实现可伸缩性和高可用性。本设计指南解释了如何使用 Tetrate 的 Edge Gateway 解决方案来实现","title":"使用 Edge Gateway 实现高可用集群"},{"content":"本文档解释了如何利用 Helm Charts 来安装 TSB 的不同组件。\nTSB Helm Chart\n管理平面安装\n控制平面安装\n数据平面安装\nTSB Helm 升级\nHelm TSB 卸载\n","relpermalink":"/tsb/setup/helm/","summary":"本文档解释了如何利用 Helm Charts 来安装 TSB 的不同组件。 TSB Helm Chart 管理平面安装 控制平面安装 数据平面安装 TSB Helm 升级 Helm TSB 卸载","title":"使用 Helm 安装"},{"content":" 遥测架构\nSidecar RED 指标\n关键指标\n警报指南\n分布式跟踪集成\nNew Relic 集成\n","relpermalink":"/tsb/operations/telemetry/","summary":"遥测架构 Sidecar RED 指标 关键指标 警报指南 分布式跟踪集成 New Relic 集成","title":"遥测和审计"},{"content":"概念一章对 Cilium 和 Hubble 的所有方面进行了更深入的介绍。如果你想了解 Cilium 和 Hubble 的概要介绍，请参阅 Cilium 和 Hubble 简介。\n本章大纲 组件概览\nCilium 术语说明\n可观测性\n阅读本章 ","relpermalink":"/cilium-handbook/concepts/","summary":"概念一章对 Cilium 和 Hubble 的所有方面进行了更深入的介绍。如果你想了解 Cilium 和 Hubble 的概要介绍，请参阅 Cilium 和 Hubble 简介。 本章大纲 组件概览 Cilium 术语说明 可观测性 阅读本章","title":"Cilium 概念"},{"content":"Kubernetes 最初源于谷歌内部的 Borg，提供了面向应用的容器集群部署和管理系统。Kubernetes 的目标旨在消除编排物理 / 虚拟计算，网络和存储基础设施的负担，并使应用程序运营商和开发人员完全将重点放在以容器为中心的原语上进行自助运营。Kubernetes 也提供稳定、兼容的基础（平台），用于构建定制化的 workflows 和更高级的自动化任务。\nKubernetes 具备完善的集群管理能力，包括多层次的安全防护和准入机制、多租户应用支撑能力、透明的服务注册和服务发现机制、内建负载均衡器、故障发现和自我修复能力、服务滚动升级和在线扩容、可扩展的资源自动调度机制、多粒度的资源配额管理能力。Kubernetes 还提供完善的管理工具，涵盖开发、部署测试、运维监控等各个环节。\nBorg 简介 Borg 是谷歌内部的大规模集群管理系统，负责对谷歌内部很多核心服务的调度和管理。Borg 的目的是让用户能够不必操心资源管理的问题，让他们专注于自己的核心业务，并且做到跨多个数据中心的资源利用率最大化。\nBorg 主要由 BorgMaster、Borglet、borgcfg …","relpermalink":"/kubernetes-handbook/architecture/","summary":"Kubernetes 最初源于谷歌内部的 Borg，提供了面向应用的容器集群部署和管理系统。Kubernetes 的目标旨在消除编排物理 / 虚拟计算，网络和存储基础设施的负担，并使应用程序运营商和开发人员完全将重点放在以容器为","title":"Kubernetes 架构"},{"content":" 从客户给我们下达订单开始，一直到我们收到现金为止，我们一直都关注时间线。而且我们正在通过删除非附加值的废物来减少这个时间表。\n—— Taichi Ohno\nTaichi Ohno 被公认为精益制造之父。虽然精益制造的实践无法完全适用于软件开发领域，但它们的原则是一致的。这些原则可以指导我们很好地寻求典型的企业 IT 组织采用云原生应用架构所需的变革，并且接受作为这一转变所带来的部分的文化和组织转型。\n开始阅读 ","relpermalink":"/migrating-to-cloud-native-application-architectures/changes-needed/","summary":"从客户给我们下达订单开始，一直到我们收到现金为止，我们一直都关注时间线。而且我们正在通过删除非附加值的废物来减少这个时间表。 —— Taichi Ohno Taichi Ohno 被公认为精益制造之父。虽然精益制造的实践无法完全适用于软件开发领","title":"第二章：在变革中前行"},{"content":"现在我们已经定义了云原生应用架构，并简要介绍了企业在采用它们时必须考虑做出的变化，现在是深入研究技术细节的时候了。对每个技术细节的深入讲解已经超出了本报告的范围。本章中仅是对采用云原生应用架构后，需要做的特定工作和采用的模式的一系列简短的介绍，文中还给出了一些进一步深入了解这些方法的链接。\n开始阅读 ","relpermalink":"/migrating-to-cloud-native-application-architectures/migration-cookbook/","summary":"现在我们已经定义了云原生应用架构，并简要介绍了企业在采用它们时必须考虑做出的变化，现在是深入研究技术细节的时候了。对每个技术细节的深入讲解已经超出了本报告的范围。本章中仅是对采用云原生应用架构后，需要","title":"第三章：迁移指南"},{"content":"云原生应用由多个松散耦合的组件（称为微服务，通常以容器形式实现）组成，在需要零信任概念的无边界网络环境中运行（企业内部或云），并由来自不同地点的用户访问（例如，校园、家庭办公室等）。云原生应用不只是指在云中运行的应用。它们还指具有设计和运行时架构的一类应用，如微服务，以及用于提供所有应用服务（包括安全）的专用基础设施。将 零信任原则 纳入这类应用提供了一些技术，其中对所有受保护资源的访问是通过基于身份的保护和基于网络的保护（如微分）来强制执行的。\n由于业务原因，云原生应用程序需要敏捷和安全的更新和部署技术，以及应对网络安全事件的必要弹性。因此，它们需要一种与传统的单层或多层应用不同的应用开发、部署和运行时监控范式（统称为软件生命周期范式）。DevSecOps（开发、安全和运维）是这类应用的促进范式，因为它通过（a）持续集成、持续交付 / 持续部署（CI/CD）管道（在第 3 节中解释）等基本要素促进了敏捷和安全的开发、交付、部署和运维；（b）整个生命周期的安全测试；以及（c）运行时的持续监控，所有这些都由自动化工具支持。事实上，满足上述目标的范式最初被赋予了 DevOps 这个术语，以 …","relpermalink":"/service-mesh-devsecops/intro/","summary":"云原生应用由多个松散耦合的组件（称为微服务，通常以容器形式实现）组成，在需要零信任概念的无边界网络环境中运行（企业内部或云），并由来自不同地点的用户访问（例如，校园、家庭办公室等）。云原生应用不只是指","title":"第一章：简介"},{"content":"本节页面的内容为开发人员进行代码审查的最佳实践。这些指南可帮助您更快地完成审核并获得更高质量的结果。您不必全部阅读它们，但它们适用于每个 Google 开发人员，并且许阅读全文通常会很有帮助。\n写好 CL 描述 小型 CL 如何处理审查者的评论 另请参阅代码审查者指南，它为代码审阅者提供了详细的指导。\n","relpermalink":"/eng-practices/review/developer/","summary":"本节页面的内容为开发人员进行代码审查的最佳实践。这些指南可帮助您更快地完成审核并获得更高质量的结果。您不必全部阅读它们，但它们适用于每个 Google 开发人员，并且许阅读全文通常会很有帮助。 写好 CL 描述 小型 CL 如何处","title":"开发者指南"},{"content":"本节是基于过往经验编写的 Code Review 最佳方式建议。其中分为了很多独立的部分，共同组成完整的文档。虽然您不必阅读文档，但通读一遍会对您自己和团队很有帮助。\nCode Review 标准 Code Review 要点 查看 CL 的步骤 Code Review 速度 如何撰写 Code Review 评论 处理 Code Review 中的抵触 另请参阅代码开发者指南，该指南为正在进行 Code Review 的开发开发者提供详细指导。\n","relpermalink":"/eng-practices/review/reviewer/","summary":"本节是基于过往经验编写的 Code Review 最佳方式建议。其中分为了很多独立的部分，共同组成完整的文档。虽然您不必阅读文档，但通读一遍会对您自己和团队很有帮助。 Code Review 标准 Code Review 要点 查看 CL 的步骤 Code Review 速度 如何撰写 Code Review 评论 处理","title":"审查者指南"},{"content":" 获取 SPIRE\n安装 SPIRE 服务器\n安装 SPIRE 代理\n","relpermalink":"/spiffe-and-spire/installation/","summary":"获取 SPIRE 安装 SPIRE 服务器 安装 SPIRE 代理","title":"安装"},{"content":" Elasticsearch 权限\nElasticsearch 清理流程\n","relpermalink":"/tsb/operations/elasticsearch/","summary":"Elasticsearch 权限\nElasticsearch 清理流程","title":"ElasticSearch"},{"content":" Common Configuration Objects\nControl Plane\nData Plane\ninstall/helm/common/v1alpha1/common.proto\ninstall/helm/controlplane/v1alpha1/values.proto\nKubernetes\nManagement Plane\n","relpermalink":"/tsb/refs/install/","summary":"Common Configuration Objects\nControl Plane\nData Plane\ninstall/helm/common/v1alpha1/common.proto\ninstall/helm/controlplane/v1alpha1/values.proto\nKubernetes\nManagement Plane","title":"install"},{"content":" REST API 指南\n","relpermalink":"/tsb/reference/rest-api/","summary":"REST API 指南","title":"REST API"},{"content":" 简介\n部署应用程序\n创建租户\n创建工作区\n创建配置组\n配置权限\n入口网关\n拓扑和指标\n流量转移\n安全\n创建应用程序和 API\n","relpermalink":"/tsb/quickstart/","summary":"简介 部署应用程序 创建租户 创建工作区 创建配置组 配置权限 入口网关 拓扑和指标 流量转移 安全 创建应用程序和 API","title":"快速入门"},{"content":"流量限制允许你根据诸如源 IP 地址和 HTTP 头部等流量属性，将通过 TSB 的流量限制在预定限额内。\n如果你担心以下任何一点，你可能需要考虑流量限制：\n防止恶意活动，如 DDoS 攻击。 防止你的应用程序及其资源（如数据库）过载。 实现某种业务逻辑，如为不同用户组设置不同的 API 限制。 TSB 支持两种流量限制模式：内部和外部。\n内部流量限制 此模式在一个集群内或跨多个集群实现全局流量限制。 在此模式中，你可以使用 API 根据各种流量属性配置限制。\n在幕后，TSB 的全局控制平面在每个集群部署一个速率限制服务器，该服务器作为一个全局服务，从多个 Envoy 代理接收元数据，并根据配置作出流量限制决策。\n此模式要求用户设置一个 Redis 服务器，用作存储后端，以持久化速率限制元数据计数。\n如果你希望利用流量限制功能而无需实现自己的流量限制服务，我们建议使用此模式。\n有关如何启用内部流量限制的详细信息，请阅读此文档。\n外部流量限制 在此模式中，你将部署一个实现 Envoy 速率限制服务接口 的速率限制服务器，并配置 API 根据指定的标准将速率限制元数据发送到你的服务器。\n外 …","relpermalink":"/tsb/howto/rate-limiting/","summary":"流量限制允许你根据诸如源 IP 地址和 HTTP 头部等流量属性，将通过 TSB 的流量限制在预定限额内。 如果你担心以下任何一点，你可能需要考虑流量限制： 防止恶意活动，如 DDoS 攻击。 防止你的应用程序及其资源（如数据库）过载。 实现","title":"限制流量速率"},{"content":" 工作负载载入指南\n快速载入\n使用 tctl 将虚拟机（VM）接入 TSB 服务网格\n","relpermalink":"/tsb/setup/workload-onboarding/","summary":"工作负载载入指南 快速载入 使用 tctl 将虚拟机（VM）接入 TSB 服务网格","title":"载入工作负载"},{"content":"本章大纲 路由\nIP 地址管理（IPAM）\nIP 地址伪装\nIPv4 分片处理\n阅读本章 ","relpermalink":"/cilium-handbook/networking/","summary":"本章大纲 路由 IP 地址管理（IPAM） IP 地址伪装 IPv4 分片处理 阅读本章","title":"网络"},{"content":"本章大纲 介绍\n基于身份\n策略执行\n代理注入\n阅读本章 ","relpermalink":"/cilium-handbook/security/","summary":"本章大纲 介绍 基于身份 策略执行 代理注入 阅读本章","title":"网络安全"},{"content":"如第 1.1 节所述，参考平台是一个容器编排和管理平台。在现代应用环境中，平台是物理（裸机）或虚拟化（如虚拟机、容器）基础设施上的一个抽象层。在实施 DevSecOps 原语之前，平台只是包含了应用代码，其中包含了应用逻辑和服务网状代码，而服务网状代码又提供应用服务。本节将考虑以下内容：\n一个容器编排和资源管理平台，容纳了应用程序代码和大部分的服务网格代码 服务网格的软件架构 本章大纲 2.1 容器编排和资源管理平台\n2.2 服务网格架构\n开始阅读 ","relpermalink":"/service-mesh-devsecops/reference-platform/","summary":"如第 1.1 节所述，参考平台是一个容器编排和管理平台。在现代应用环境中，平台是物理（裸机）或虚拟化（如虚拟机、容器）基础设施上的一个抽象层。在实施 DevSecOps 原语之前，平台只是包含了应用代码，其中包含了应用逻辑和服务","title":"第二章：实施 DevSecOps 原语的参考平台"},{"content":"本章将介绍什么是云原生、云原生应用。\n阅读本章 ","relpermalink":"/cloud-native-handbook/community/","summary":"社区","title":"社区"},{"content":" 配置 SPIRE\n注册工作负载\n使用 SVID\nSPIRE Agent\nSPIRE Server\n","relpermalink":"/spiffe-and-spire/configuration/","summary":"配置 SPIRE 注册工作负载 使用 SVID SPIRE Agent SPIRE Server","title":"配置"},{"content":" 在 AWS 安装 TSB\n从 AWS 容器市场安装 TSB\n","relpermalink":"/tsb/setup/aws/","summary":"在 AWS 安装 TSB\n从 AWS 容器市场安装 TSB","title":"AWS"},{"content":" gRPC API 指南\n","relpermalink":"/tsb/reference/grpc-api/","summary":"gRPC API 指南","title":"gRPC API"},{"content":" IAM (OAuth)\nIAM (OIDC)\n","relpermalink":"/tsb/refs/iam/","summary":"IAM (OAuth)\nIAM (OIDC)","title":"iam"},{"content":" PostgreSQL 凭据\nElasticsearch 凭据\nIstio CA\n","relpermalink":"/tsb/operations/vault/","summary":"PostgreSQL 凭据 Elasticsearch 凭据 Istio CA","title":"Vault 客户端集成"},{"content":" 流量管理和迁移\n配置网关\nGitOps\n限制流量速率\n外部授权\n使用 WASM 扩展\n使用 WAF 功能\n使用 SkyWalking 进行 HPA\n利用 TSB 服务账号\n创建安全域\n","relpermalink":"/tsb/howto/","summary":"流量管理和迁移 配置网关 GitOps 限制流量速率 外部授权 使用 WASM 扩展 使用 WAF 功能 使用 SkyWalking 进行 HPA 利用 TSB 服务账号 创建安全域","title":"操作任务"},{"content":"Tetrate Service Bridge (TSB) 提供了授权功能，用于授权传入的所有 HTTP 请求，包括传入 Gateways 和 Workloads 的请求。TSB 支持本地授权，使用 JWT 声明，以及外部授权 (ext-authz)，后者使用在外部运行的服务来确定是否应允许或拒绝请求。\n如果你有一个独立的内部系统，希望使用不同于 JWT 的身份验证方案，或者希望集成第三方授权解决方案，比如 Open Policy Agent (OPA) 或 PlainID，你可以决定使用外部授权系统。\nExt-authz 可以在不同的上下文中进行配置，例如 Tier-1 Gateways、Ingress Gateways 以及 Traffic Settings。以下表格显示了在 TSB 中如何使用外部授权的一些可能方式：\n上下文 示例用途 Tier-1 网关 可以配置 Tier-1 网关，仅接受带有有效 JWT 和经过身份验证 API 声明的请求，以及带有适当基本授权的请求等 Ingress 网关 Ingress 网关 / Tier-2 网关 / 应用程序网关可以配置以实施基于用户权益 …","relpermalink":"/tsb/howto/authorization/","summary":"Tetrate Service Bridge (TSB) 提供了授权功能，用于授权传入的所有 HTTP 请求，包括传入 Gateways 和 Workloads 的请求。TSB 支持本地授权，使用 JWT 声明，以及外部授权 (ext-authz)，后者使用在外部运行的服务来确定是否应允许或拒绝请求。 如果你有","title":"外部授权"},{"content":"开始使用 Argo Rollouts：\n基础使用\nAmbassador\nAWS ALB\nAWS AppMesh\nIstio\nNginx\n快速开始 ","relpermalink":"/argo-rollouts/getting-started/","summary":"开始使用 Argo Rollouts： 基础使用 Ambassador AWS ALB AWS AppMesh Istio Nginx 快速开始","title":"入门"},{"content":"本章大纲 介绍\n数据包流程\neBPF Map\nIptables 用法\n阅读本章 ","relpermalink":"/cilium-handbook/ebpf/","summary":"本章大纲 介绍 数据包流程 eBPF Map Iptables 用法 阅读本章","title":"eBPF 数据路径"},{"content":"DevSecOps 在早期就将安全纳入了软件工程流程。它将安全流程和工具集成到 DevOps 的所有开发工作流程（或后面解释的管道）中，并使之自动化，从而实现无缝和连续。换句话说，它可以被看作是三个过程的组合。开发 + 安全 + 运维。\n本节讨论了 DevSecOps 的以下方面：\n组织对 DevSecOps 的准备情况 开发安全运维平台 开发安全运维的基本构件或关键原语 本章大纲 3.1 组织对 DevSecOps 的准备情况\n3.2 DevSecOps 平台\n3.3 DevSecOps 关键原语和实施任务\n开始阅读 ","relpermalink":"/service-mesh-devsecops/devsecops/","summary":"DevSecOps 在早期就将安全纳入了软件工程流程。它将安全流程和工具集成到 DevOps 的所有开发工作流程（或后面解释的管道）中，并使之自动化，从而实现无缝和连续。换句话说，它可以被看作是三个过程的组合。开发 + 安全 + 运维。 本节","title":"第三章：DevSecOps 组织准备、关键基本要素和实施"},{"content":" Envoy\nEnvoy + X.509\nEnvoy + JWT-SVID\n使用 Envoy 和 X.509-SVID 进行 OPA 授权\n使用 Envoy 和 JWT-SVIDs 进行 OPA 授权\n","relpermalink":"/spiffe-and-spire/examples/","summary":"Envoy Envoy + X.509 Envoy + JWT-SVID 使用 Envoy 和 X.509-SVID 进行 OPA 授权 使用 Envoy 和 JWT-SVIDs 进行 OPA 授权","title":"SPIRE 集成示例"},{"content":" Audit\n","relpermalink":"/tsb/refs/audit/","summary":"Audit","title":"audit"},{"content":" YAML API 指南\n","relpermalink":"/tsb/reference/yaml-api/","summary":"YAML API 指南","title":"YAML API"},{"content":" WASM 扩展概述\nTSB 配置\nWasm 扩展示例\n","relpermalink":"/tsb/howto/wasm/","summary":"WASM 扩展概述 TSB 配置 Wasm 扩展示例","title":"使用 WASM 扩展"},{"content":" 特性\n用户权限\n遥测和审计\nElasticSearch\nVault 客户端集成\n配置推广\n配置日志级别\n配置多个 IAM 令牌验证密钥\n备份和恢复 PostgreSQL\n将数据迁移到新的组织\n降低 Istio 资源消耗\n定制 TSB Kubernetes 组件\n优雅关闭 istio-proxy 连接\n","relpermalink":"/tsb/operations/","summary":"特性 用户权限 遥测和审计 ElasticSearch Vault 客户端集成 配置推广 配置日志级别 配置多个 IAM 令牌验证密钥 备份和恢复 PostgreSQL 将数据迁移到新的组织 降低 Istio 资源消耗 定制 TSB Kubernetes 组件 优雅关闭 istio-proxy 连接","title":"运维指南"},{"content":" 证书类型\n内部证书要求\n自动证书管理\n","relpermalink":"/tsb/setup/certificate/","summary":"证书类型 内部证书要求 自动证书管理","title":"证书安装"},{"content":"本指南适用于想要为其他开发人员安装和配置 Argo CD 的管理员和操作员。\n🔔 注意：请确保你已完成 入门指南。\n架构概述\nApplicationSet 控制器\n","relpermalink":"/argo-cd/operator-manual/","summary":"本指南适用于想要为其他开发人员安装和配置 Argo CD 的管理员和操作员。 🔔 注意：请确保你已完成 入门指南。 架构概述 ApplicationSet 控制器","title":"操作手册"},{"content":"Kubernetes 作为云原生应用的基础调度平台，相当于云原生的操作系统，为了便于系统的扩展，Kubernetes 中开放的以下接口，可以分别对接不同的后端，来实现自己的业务逻辑：\n容器运行时接口（CRI）：提供计算资源 容器网络接口（CNI）：提供网络资源 容器存储接口（CSI），提供存储资源 以上三种资源相当于一个分布式操作系统的最基础的几种资源类型，而 Kuberentes 是将他们粘合在一起的纽带。\n本节大纲 容器运行时接口（CRI）\n容器网络接口（CNI）\n容器存储接口（CSI）\n","relpermalink":"/kubernetes-handbook/architecture/open-interfaces/","summary":"Kubernetes 作为云原生应用的基础调度平台，相当于云原生的操作系统，为了便于系统的扩展，Kubernetes 中开放的以下接口，可以分别对接不同的后端，来实现自己的业务逻辑： 容器运行时接口（CRI）：提供计算资源 容","title":"开放接口"},{"content":"各种 CI/CD 管道都涉及到参考平台（即基于微服务的应用，有提供基础设施服务的服务网格）。虽然参考应用是基于微服务的应用，但 DevSecOps 的原语可以应用于单体应用以及既在企业内部又基于云的应用（如混合云、单一公有云和多云）。\n在第 2.1 节中，我们提到了我们参考应用环境中的五种代码类型。我们还提到，也可以为这五种代码类型中的每一种创建单独的 CI/CD 管道。这五种代码类型在参考平台组件中的位置将被讨论，然后是描述相关 CI/CD 管道的单独章节：\n参考平台中的代码类型和相关的 CI/CD 管道（4.1 节） 应用程序代码和应用服务代码的 CI/CD 管道（4.2 节） 基础设施即代码（IaC）的 CI/CD 管道（4.3 节） 策略即代码的 CI/CD 管道（4.4 节） 可观测性即代码的 CI/CD 管道（4.5 节） 所有 CI/CD 管道的实施问题，无论代码类型如何，都将在以下章节中讨论：\n确保 CI/CD 管道的安全（4.6 节） CI/CD 管道中的工作流模型（4.7 节） CI/CD 管道中的安全测试（4.8 节） 本节还将考虑 DevSecOps 的整体优 …","relpermalink":"/service-mesh-devsecops/implement/","summary":"各种 CI/CD 管道都涉及到参考平台（即基于微服务的应用，有提供基础设施服务的服务网格）。虽然参考应用是基于微服务的应用，但 DevSecOps 的原语可以应用于单体应用以及既在企业内部又基于云的应用（如混合云、单一公有云和多云）","title":"第四章：为参考平台实施 DevSecOps 原语"},{"content":" API 参考\nTSB CRD 参考\n","relpermalink":"/tsb/reference/k8s-api/","summary":"API 参考\nTSB CRD 参考","title":"Kubernetes API"},{"content":"本指南适用于已安装 Argo CD 并正在管理应用程序的开发人员。\n🔔 注意：请确保你已完成 入门指南。\n工具\n自动同步策略\n对比选项\n同步选项 环境变量\n选择性同步\n","relpermalink":"/argo-cd/user-guide/","summary":"本指南适用于已安装 Argo CD 并正在管理应用程序的开发人员。 🔔 注意：请确保你已完成 入门指南。 工具 自动同步策略 对比选项 同步选项 环境变量 选择性同步","title":"用户手册"},{"content":" 部署策略\nRollout 规范\nHPA\nVPA\n短暂元数据\n重启 Rollouts\n缩小失败的 Rollout\n回滚窗口\n反亲和性\nHelm\nKustomize\n控制器指标\n","relpermalink":"/argo-rollouts/rollout/","summary":"部署策略\nRollout 规范\nHPA\nVPA\n短暂元数据\n重启 Rollouts\n缩小失败的 Rollout\n回滚窗口\n反亲和性\nHelm\nKustomize\n控制器指标","title":"Rollout"},{"content":"本文为托管云原生应用的参考平台实施 DevSecOps 原语提供全面指导。它包括对参考平台的概述，并描述了基本的 DevSecOps 原语（即 CI/CD 管道）、其构建模块、管道的设计和执行，以及自动化在 CI/CD 管道中有效执行工作流程的作用。\n参考平台的架构除了应用代码和提供应用服务的代码外还包括用于基础设施、运行时策略和持续监测应用健康状况的功能元素，可以通过具有独立 CI/CD 管道类型的声明性代码来部署。还介绍了这些代码的运行时行为、实现高安全性的好处，以及使用风险管理工具和仪表盘指标的管道内的工件来提供持续授权操作（C-ATO）。\n","relpermalink":"/service-mesh-devsecops/summary-and-conclusion/","summary":"本文为托管云原生应用的参考平台实施 DevSecOps 原语提供全面指导。它包括对参考平台的概述，并描述了基本的 DevSecOps 原语（即 CI/CD 管道）、其构建模块、管道的设计和执行，以及自动化在 CI/CD 管道中有效执行工作流程的作用。 参考平台的架构","title":"第五章：摘要和结论"},{"content":"该设计指南同时适用于 TSB 和 TSE。\n理解高可用性\n应用载入最佳实践\n使用 Edge Gateway 实现高可用集群\n","relpermalink":"/tsb/design-guides/","summary":"该设计指南同时适用于 TSB 和 TSE。 理解高可用性 应用载入最佳实践 使用 Edge Gateway 实现高可用集群","title":"设计指南"},{"content":" 安装 httpbin\n安装 Open Policy Agent\n安装 sleep\n","relpermalink":"/tsb/reference/samples/","summary":"安装 httpbin 安装 Open Policy Agent 安装 sleep","title":"示例应用"},{"content":" 基本故障排除\nIngress Gateway 故障排除\n配置状态故障排除\n使用调试容器\nUI 指标故障排除\n请求头大小超限\n多重传输编码块处理\n识别性能不佳的服务\n集群载入故障排除\n","relpermalink":"/tsb/troubleshooting/","summary":"基本故障排除 Ingress Gateway 故障排除 配置状态故障排除 使用调试容器 UI 指标故障排除 请求头大小超限 多重传输编码块处理 识别性能不佳的服务 集群载入故障排除","title":"问题排查"},{"content":"🔔 警告：你可能不想阅读这部分文档。本手册的这一部分面向想要开发与 Argo CD 交互的第三方应用程序的人们，例如：\n聊天机器人 Slack 集成 🔔 注意：请确保你已完成 入门指南。\n","relpermalink":"/argo-cd/developer-guide/","summary":"🔔 警告：你可能不想阅读这部分文档。本手册的这一部分面向想要开发与 Argo CD 交互的第三方应用程序的人们，例如： 聊天机器人 Slack 集成 🔔 注意：请确保你已完成 入门指南。","title":"开发手册"},{"content":" 概览\nAmbassador\nAPISIX\nAWS ALB\nIstio\nNginx\n插件\nTraefik\n多提供方\n","relpermalink":"/argo-rollouts/traffic-management/","summary":"概览 Ambassador APISIX AWS ALB Istio Nginx 插件 Traefik 多提供方","title":"流量管理"},{"content":" tsb\nonboard\nistio.io\ninstall\niam\naudit\n","relpermalink":"/tsb/refs/","summary":"tsb\nonboard\nistio.io\ninstall\niam\naudit","title":"API"},{"content":" CLI\nREST API\ngRPC API\nYAML API\nKubernetes API\n示例应用\n","relpermalink":"/tsb/reference/","summary":"CLI REST API gRPC API YAML API Kubernetes API 示例应用","title":"参考"},{"content":" 概览\n插件\n指标\n","relpermalink":"/argo-rollouts/analysis/","summary":"概览 插件 指标","title":"分析"},{"content":" 服务网格与 GitOps\nTSB 常见问题解答\n","relpermalink":"/tsb/knowledge-base/","summary":"服务网格与 GitOps TSB 常见问题解答","title":"知识库和 FAQ"},{"content":"以下列举的内容都是 Kubernetes 中的对象（Object），这些对象都可以在 YAML 文件中作为一种 API 类型来配置。\nPod Node Namespace Service Volume PersistentVolume Deployment Secret StatefulSet DaemonSet ServiceAccount ReplicationController ReplicaSet Job CronJob SecurityContext ResourceQuota LimitRange HorizontalPodAutoscaling Ingress ConfigMap Label CustomResourceDefinition Role ClusterRole 我将它们简单的分类为以下几种资源对象： …","relpermalink":"/kubernetes-handbook/objects/","summary":"以下列举的内容都是 Kubernetes 中的对象（Object），这些对象都可以在 YAML 文件中作为一种 API 类型来配置。 Pod Node Namespace Service Volume PersistentVolume Deployment Secret StatefulSet DaemonSet ServiceAccount ReplicationController ReplicaSet Job CronJob SecurityContext ResourceQuota LimitRange HorizontalPodAutoscaling Ingress ConfigMap Label CustomResourceDefinition Role ClusterRole 我将它们简单的分类为以下几种资源对象： 类别 名称 资源对象 Po","title":"Kubernetes 中的资源对象"},{"content":" 概览\n服务\n","relpermalink":"/argo-rollouts/notifications/","summary":"概览 服务","title":"通知"},{"content":"欢迎使用 Tetrate Service Bridge (TSB) 版本 1.6 的发行说明。此版本引入了多项新功能，可增强可用性、安全性和可见性。TSB 继续提供统一的方法来连接和保护不同环境中的服务，包括 Kubernetes 集群、虚拟机和裸机工作负载。\n主要亮点 跨集群高可用性和安全性 TSB 1.6 专注于通过将远程集群更紧密地结合在一起以简化管理和可扩展性来提高可用性和安全性：\n所有服务的跨集群高可用性：引入 EastWestGateway 功能，实现集群之间的自动服务故障转移，无需外部网关。最大限度地提高服务可用性、简化故障转移并增强安全性。 跨集群身份传播和安全域：创建跨集群的可扩展安全策略，确保本地、远程和故障转移服务的访问控制规则一致。 增强可见性和故障排除 高级可见性和跟踪工具：使应用程序开发人员能够解决跨集群的分布式应用程序中的性能问题。利用 tctl collect 导出运行时数据以进行离线分析，并使用 tctl troubleshoot 进行深入调查。 附加功能和灵活性 WASM 扩展支持：使用 WebAssembly (WASM) 扩展通过自定义功能扩展代 …","relpermalink":"/tsb/release-notes-announcements/","summary":"欢迎使用 Tetrate Service Bridge (TSB) 版本 1.6 的发行说明。此版本引入了多项新功能，可增强可用性、安全性和可见性。TSB 继续提供统一的方法来连接和保护不同环境中的服务，包括 Kubernetes 集群、虚拟机和裸机工作负载。 主要亮点 跨集群高可用性和","title":"TSB 1.6 发行说明"},{"content":" 概览\n命令\n","relpermalink":"/argo-rollouts/kubectl-plugin/","summary":"概览\n命令","title":"Kubectl plugin"},{"content":" 非修订版到修订版的升级\n已修订版本间的升级\n网关升级\n修订的 Istio CNI 和升级\n","relpermalink":"/tsb/setup/upgrades/","summary":"非修订版到修订版的升级 已修订版本间的升级 网关升级 修订的 Istio CNI 和升级","title":"控制平面升级"},{"content":"本章记录了用于在 Cilium 中配置网络策略的策略语言。安全策略可以通过以下机制指定和导入：\n使用 Kubernetes NetworkPolicy、CiliumNetworkPolicy 和 CiliumClusterwideNetworkPolicy 资源。更多细节请参见网络策略一节。在这种模式下，Kubernetes 将自动向所有代理分发策略。 通过代理的 CLI 或 API 参考直接导入到代理中。这种方法不会自动向所有代理分发策略。用户有责任在所有需要的代理中导入策略。 本章内容包括：\n策略执行模式 规则基础 三层示例 四层示例 七层示例 拒绝政策 主机策略 七层协议可视性 在策略中使用 Kubernetes 构造 端点生命周期 故障排除 阅读本章 ","relpermalink":"/cilium-handbook/policy/","summary":"本章记录了用于在 Cilium 中配置网络策略的策略语言。安全策略可以通过以下机制指定和导入： 使用 Kubernetes NetworkPolicy、CiliumNetworkPolicy 和 CiliumClusterwideNetworkPolicy 资源。更多细节请参见网络策略一节。在这种模式","title":"网络策略"},{"content":"为了管理异构和不同配置的主机，为了便于 Pod 的运维管理，Kubernetes 中提供了很多集群管理的配置和管理功能，通过 namespace 划分的空间，通过为 node 节点创建 label 和 taint 用于 pod 的调度等。\n本节大纲 Node\nNamespace\nLabel\nAnnotation\nTaint 和 Toleration（污点和容忍）\n垃圾收集\n资源调度\n服务质量等级（QoS）\n","relpermalink":"/kubernetes-handbook/cluster/","summary":"为了管理异构和不同配置的主机，为了便于 Pod 的运维管理，Kubernetes 中提供了很多集群管理的配置和管理功能，通过 namespace 划分的空间，通过为 node 节点创建 label 和 taint 用于 pod 的调度等。 本节大纲 Node Namespace Label Annotation Taint 和 Tolerat","title":"集群资源管理"},{"content":" 介绍\n入门\n用例\n安全\n","relpermalink":"/argo-cd/operator-manual/applicationset/","summary":"介绍 入门 用例 安全","title":"ApplicationSet 控制器"},{"content":"Kubernetes 中内建了很多 controller（控制器），这些相当于一个状态机，用来控制 Pod 的具体状态和行为。\n本节大纲 Deployment\nStatefulSet\nDaemonSet\nReplicationController 和 ReplicaSet\nJob\nCronJob\nIngress 控制器\nHorizontal Pod Autoscaling\n准入控制器（Admission Controller）\n","relpermalink":"/kubernetes-handbook/controllers/","summary":"Kubernetes 中内建了很多 controller（控制器），这些相当于一个状态机，用来控制 Pod 的具体状态和行为。 本节大纲 Deployment StatefulSet DaemonSet ReplicationController 和 ReplicaSet Job CronJob Ingress 控制器 Horizontal Pod Autoscaling 准入控制器（Admission Controller）","title":"控制器"},{"content":"本章大纲 介绍\n概念\n要求\n网络策略\n端点 CRD\n端点切片 CRD\nKubernetes 兼容性\n故障排除\n阅读本章 ","relpermalink":"/cilium-handbook/kubernetes/","summary":"本章大纲 介绍 概念 要求 网络策略 端点 CRD 端点切片 CRD Kubernetes 兼容性 故障排除 阅读本章","title":"Kubernetes 集成"},{"content":"应用的资源使用率通常都有高峰和低谷的时候，如何削峰填谷，提高集群的整体资源利用率，让 service 中的 Pod 个数自动调整呢？这就有赖于 Horizontal Pod Autoscaling 了，顾名思义，使 Pod 水平自动缩放。这个 Object（跟 Pod、Deployment 一样都是 API resource）也是最能体现 kubernetes 之于传统运维价值的地方，不再需要手动扩容了，终于实现自动化了，还可以自定义指标，没准未来还可以通过人工智能自动进化呢！\nHPA 属于 Kubernetes 中的 autoscaling SIG（Special Interest Group），其下有两个 feature：\nArbitrary/Custom Metrics in the Horizontal Pod Autoscaler#117 Monitoring Pipeline Metrics HPA API #118 Kubernetes 自 1.2 版本引入 HPA 机制，到 1.6 版本之前一直是通过 kubelet 来获取监控指标来判断是否需要扩缩容，1.6 版本之后 …","relpermalink":"/kubernetes-handbook/controllers/hpa/","summary":"应用的资源使用率通常都有高峰和低谷的时候，如何削峰填谷，提高集群的整体资源利用率，让 service 中的 Pod 个数自动调整呢？这就有赖于 Horizontal Pod Autoscaling 了，顾名思义，使 Pod 水平自动缩放。这个 Object（跟 Pod、Deployme","title":"Horizontal Pod Autoscaling"},{"content":"Kubernetes 中为了实现服务实例间的负载均衡和不同服务间的服务发现，创造了 Serivce 对象，同时又为从集群外部访问集群创建了 Ingress 对象。\n本节大纲 Service\n拓扑感知路由\nIngress\nGateway API\n","relpermalink":"/kubernetes-handbook/service-discovery/","summary":"Kubernetes 中为了实现服务实例间的负载均衡和不同服务间的服务发现，创造了 Serivce 对象，同时又为从集群外部访问集群创建了 Ingress 对象。 本节大纲 Service 拓扑感知路由 Ingress Gateway API","title":"服务发现与路由"},{"content":"Kubernetes 中提供了良好的多租户认证管理机制，如 RBAC、ServiceAccount 还有各种策略等。\n另外还有一个 CNCF 孵化项目 SPIFFE 旨在为所有的分布式应用提供身份支持，目前已应用在了 Envoy、Istio 等应用中。\n本节大纲 ServiceAccount\n基于角色的访问控制（RBAC）\nNetworkPolicy\nSPIFFE\nSPIRE\nSPIRE Kubernetes 工作负载注册器\nSVID 身份颁发过程\n","relpermalink":"/kubernetes-handbook/auth/","summary":"Kubernetes 中提供了良好的多租户认证管理机制，如 RBAC、ServiceAccount 还有各种策略等。 另外还有一个 CNCF 孵化项目 SPIFFE 旨在为所有的分布式应用提供身份支持，目前已应用在了 Envoy、Istio 等应用中。 本","title":"身份与权限认证"},{"content":"Kubernetes 中的网络可以说对初次接触 Kubernetes 或者没有网络方面经验的人来说可能是其中最难的部分。Kubernetes 本身并不提供网络功能，只是把网络接口开放出来，通过插件的形式实现。\n网络要解决的问题 既然 Kubernetes 中将容器的联网通过插件的方式来实现，那么该如何解决容器的联网问题呢？\n如果您在本地单台机器上运行 docker 容器的话会注意到所有容器都会处在 docker0 网桥自动分配的一个网络 IP 段内（172.17.0.1/16）。该值可以通过 docker 启动参数 --bip 来设置。这样所有本地的所有的容器都拥有了一个 IP 地址，而且还是在一个网段内彼此就可以互相通信了。\n但是 Kubernetes 管理的是集群，Kubernetes 中的网络要解决的核心问题就是每台主机的 IP 地址网段划分，以及单个容器的 IP 地址分配。概括为：\n保证每个 Pod 拥有一个集群内唯一的 IP 地址 保证不同节点的 IP 地址划分不会重复 保证跨节点的 Pod 可以互相通信 保证不同节点的 Pod 可以与跨节点的主机互相通信 为了解决该问题，出 …","relpermalink":"/kubernetes-handbook/networking/","summary":"Kubernetes 中的网络可以说对初次接触 Kubernetes 或者没有网络方面经验的人来说可能是其中最难的部分。Kubernetes 本身并不提供网络功能，只是把网络接口开放出来，通过插件的形式实现。 网络要解决的问题 既然 Kubernetes 中将容器的联网","title":"网络"},{"content":"为了管理存储，Kubernetes 提供了以下资源对象：\nSecret：用于管理敏感信息 ConfigMap：存储配置 Volume、PV、PVC、StorageClass 等：用来管理存储卷 本节将为你讲解 Kubernetes 中的存储对象。\n本节大纲 Secret\nConfigMap\nConfigMap 的热更新\nVolume\n持久化卷（Persistent Volume）\nStorage Class\n本地持久化存储\n","relpermalink":"/kubernetes-handbook/storage/","summary":"为了管理存储，Kubernetes 提供了以下资源对象： Secret：用于管理敏感信息 ConfigMap：存储配置 Volume、PV、PVC、StorageClass 等：用来管理存储卷 本节将为你讲解 Kubernetes 中","title":"存储"},{"content":"Kubernetes 是一个高度开放可扩展的架构，可以通过自定义资源类型（CRD）来定义自己的类型，还可以自己来扩展 API 服务，用户的使用方式跟 Kubernetes 的原生对象无异。\n本节大纲 使用自定义资源扩展 API\n使用 CRD 扩展 Kubernetes API\nAggregated API Server\nAPIService\n服务目录（Service Catalog）\n","relpermalink":"/kubernetes-handbook/extend/","summary":"Kubernetes 是一个高度开放可扩展的架构，可以通过自定义资源类型（CRD）来定义自己的类型，还可以自己来扩展 API 服务，用户的使用方式跟 Kubernetes 的原生对象无异。 本节大纲 使用自定义资源扩展 API 使用 CRD 扩展 Kubernetes API Aggregated API Server APIService 服务目录（S","title":"扩展集群"},{"content":"组织需要部署多个 Kubernetes 集群来为不同的业务提供隔离，增强可用性和可扩展性。\n什么是多集群？ 多集群是一种在多个 Kubernetes 集群上或跨集群部署应用的策略，目的是提高可用性、隔离性和可扩展性。多集群对于确保遵守不同的和相互冲突的法规非常重要，因为单个集群可以进行调整，以遵守特定地域或认证的法规。软件交付的速度和安全性也可以提高，单个开发团队将应用程序部署到隔离的集群中，并有选择地暴露哪些服务可用于测试和发布。\n配置多集群访问 你可以使用 kubectl config 命令配置要访问的集群，详见配置对多集群的访问。\n集群联邦 集群联邦（Federation）是指通过 Federation API 资源来统一管理多个集群的资源，如定义 Deployment 如何部署到不同集群上，及其所需的副本数等。这些集群可能位于不同的可用区、地区或者供应商。实施集群联邦一般是为了达到以下目的：\n简化管理多个集群的 Kubernetes 组件 (如 Deployment、Service 等）； 在多个集群之间分散工作负载（Pod），以提升应用（服务）的可靠性； 跨集群的资源编排，依 …","relpermalink":"/kubernetes-handbook/multi-cluster/","summary":"组织需要部署多个 Kubernetes 集群来为不同的业务提供隔离，增强可用性和可扩展性。 什么是多集群？ 多集群是一种在多个 Kubernetes 集群上或跨集群部署应用的策略，目的是提高可用性、隔离性和可扩展性。多集群对于确保遵守不同的和相互冲","title":"多集群管理"},{"content":"Kubernetes 中的各个资源对象的配置指南。\n本节大纲 配置 Pod 的 liveness 和 readiness 探针\n配置 Pod 的 Service Account\nSecret 配置\n管理 namespace 中的资源配额\n","relpermalink":"/kubernetes-handbook/config/","summary":"Kubernetes 中的各个资源对象的配置指南。 本节大纲 配置 Pod 的 liveness 和 readiness 探针 配置 Pod 的 Service Account Secret 配置 管理 namespace 中的资源配额","title":"资源对象配置"},{"content":"Kubernetes 中的 kubectl 及其他管理命令使用。\n本节大纲 Docker 用户过渡到 kubectl 命令行指南\nKubectl 命令概览\nKubectl 命令技巧大全\n使用 etcdctl 访问 Kubernetes 数据\n","relpermalink":"/kubernetes-handbook/cli/","summary":"Kubernetes 中的 kubectl 及其他管理命令使用。 本节大纲 Docker 用户过渡到 kubectl 命令行指南 Kubectl 命令概览 Kubectl 命令技巧大全 使用 etcdctl 访问 Kubernetes 数据","title":"命令使用"},{"content":"Kubernetes 支持多租户，这就需要对集群的安全性进行管理。\n本节大纲 管理集群中的 TLS\nKublet 的认证授权\nTLS Bootstrap\nIP 伪装代理\n创建用户认证授权的 kubeconfig 文件\n使用 kubeconfig 或 token 进行用户身份认证\nKubernetes 中的用户与身份认证授权\nKubernetes 集群安全性配置最佳实践\n","relpermalink":"/kubernetes-handbook/security/","summary":"Kubernetes 支持多租户，这就需要对集群的安全性进行管理。 本节大纲 管理集群中的 TLS Kublet 的认证授权 TLS Bootstrap IP 伪装代理 创建用户认证授权的 kubeconfig 文件 使用 kubeconfig 或 token 进行用户身份认证 Kubernetes 中的用户与身份认证授权 Kubernetes 集群安全性配置最佳实践","title":"集群安全性管理"},{"content":"根据用户部署和暴露服务的方式不同，有很多种方式可以用来访问 Kubernetes 集群。\n最简单也是最直接的方式是使用 kubectl 命令。 其次可以使用 kubeconfig 文件来认证授权访问 API server。 通过各种 proxy 经过端口转发访问 Kubernetes 集群中的服务 使用 Ingress，在集群外访问 Kubernetes 集群内的 service 本节大纲 访问集群\n使用 kubeconfig 文件配置跨集群认证\n通过端口转发访问集群中的应用程序\n使用 service 访问群集中的应用程序\n从外部访问 Kubernetes 中的 Pod\nLens - Kubernetes IDE\nKubernator - 更底层的 Kubernetes UI\n","relpermalink":"/kubernetes-handbook/access/","summary":"根据用户部署和暴露服务的方式不同，有很多种方式可以用来访问 Kubernetes 集群。 最简单也是最直接的方式是使用 kubectl 命令。 其次可以使用 kubeconfig 文件来认证授权访问 API server。 通过各种 proxy 经过端口转发访问 Kubernetes 集群中的服务 使用 Ing","title":"访问 Kubernetes 集群"},{"content":"理论上只要可以使用主机名做服务注册的应用都可以迁移到 Kubernetes 集群上。看到这里你可能不禁要问，为什么使用 IP 地址做服务注册发现的应用不适合迁移到 kubernetes 集群？因为这样的应用不适合自动故障恢复，因为目前 Kubernetes 中不支持固定 Pod 的 IP 地址，当 Pod 故障后自动转移到其他节点的时候该 Pod 的 IP 地址也随之变化。\n将传统应用迁移到 Kubernetes 中可能还有很长的路要走，但是直接开发云原生应用，Kubernetes 就是最佳运行时环境了。\n本节大纲 适用于 Kubernetes 的应用开发部署流程\n迁移传统应用到 Kubernetes 步骤详解——以 Hadoop YARN 为例\n使用 StatefulSet 部署有状态应用\n持续集成与交付（CI/CD）\n使用 Kustomize 配置 Kubernetes 应用\n","relpermalink":"/kubernetes-handbook/devops/","summary":"理论上只要可以使用主机名做服务注册的应用都可以迁移到 Kubernetes 集群上。看到这里你可能不禁要问，为什么使用 IP 地址做服务注册发现的应用不适合迁移到 kubernetes 集群？因为这样的应用不适合自动故障恢复，因为目前 Kubernetes 中不支持固定 Pod","title":"在 Kubernetes 中开发部署应用"},{"content":"讲解如何在原生 Kubernetes 的基础上做定制开发。\n本节大纲 SIG 和工作组\n配置 Kubernetes 开发环境\n测试 Kubernetes\nclient-go 示例\nOperator\nOperator SDK\nKubebuilder\n高级开发指南\n参与 Kubernetes 社区贡献\nMinikube\n阅读本章 ","relpermalink":"/kubernetes-handbook/develop/","summary":"讲解如何在原生 Kubernetes 的基础上做定制开发。 本节大纲 SIG 和工作组 配置 Kubernetes 开发环境 测试 Kubernetes client-go 示例 Operator Operator SDK Kubebuilder 高级开发指南 参与 Kubernetes 社区贡献 Minikube 阅读本章","title":"开发指南"},{"content":"本节向你介绍 TSB Operator 的基本概念。你将深入了解 TSB Operator 如何管理 TSB 的整个生命周期，包括跨各个平面的安装、升级和运行时行为。\nKubernetes 知识\n如果你不熟悉 Kubernetes 命名空间、Operator、清单和自定义资源，建议你熟悉这些概念。此背景将极大地增强你对 TSB Operator 的理解以及维护 TSB 服务网格的能力。\n你可以查阅 Kubernetes 文档来了解有关 Operator 模式的更多信息。\nTSB Operator 在控制 TSB 管理、控制和数据平面组件的安装、升级和运行时行为方面发挥着关键作用。为了确保兼容性并提供平滑的升级体验，基于 Kubernetes 的 TSB 组件清单已集成到 TSB Operator 中。因此，管理、控制和数据平面组件的版本与管理它们的 TSB Operator 部署的版本相关联。TSB Operator 利用用户创建的自定义资源 (CR) 来配置和实例化这些组件。\n为了有效管理 TSB 生命周期，TSB Operator 与 tctl CLI 工具密切协作。使用 tctl …","relpermalink":"/tsb/concepts/operators/","summary":"本节向你介绍 TSB Operator 的基本概念。你将深入了解 TSB Operator 如何管理 TSB 的整个生命周期，包括跨各个平面的安装、升级和运行时行为。 Kubernetes 知识 如果你不熟悉 Kubernetes 命名空间、Operator、清单和自定义资源，建议你熟悉这些概念。此背","title":"TSB Operator"},{"content":"本文指导你如何配置 Envoy 代理与 SPIFFE 和 SPIRE 配合使用。\nEnvoy 是一种流行的开源服务代理，广泛用于提供抽象、安全、经过身份验证和加密的服务间通信。Envoy 拥有丰富的配置系统，允许灵活地与第三方进行交互。\n该配置系统的一个组成部分是 Secret Discovery Service 协议或 SDS。Envoy 使用 SDS 从 SDS 提供者检索和维护更新的“密钥”。在 TLS 身份验证的上下文中，这些密钥是 TLS 证书、私钥和可信 CA 证书。SPIRE 代理可以配置为 Envoy 的 SDS 提供者，使其能够直接向 Envoy 提供所需的密钥材料以进行 TLS 身份验证。SPIRE 代理还会根据需要重新生成短期密钥和证书。\n有关如何将 SPIRE 与 Envoy 集成的基于 Kubernetes 的示例，请参阅使用 X.509 证书集成 Envoy 和使用 JWT 集成 Envoy。\n工作原理 当 Envoy 连接到 SPIRE 代理提供的 SDS 服务器时，代理会对 Envoy 进行验证，并确定应向 Envoy 公开哪些服务标识和 CA 证书，以 …","relpermalink":"/spiffe-and-spire/examples/envoy/","summary":"本文指导你如何配置 Envoy 代理与 SPIFFE 和 SPIRE 配合使用。 Envoy 是一种流行的开源服务代理，广泛用于提供抽象、安全、经过身份验证和加密的服务间通信。Envoy 拥有丰富的配置系统，允许灵活地与第三方进行交互。 该配置系统的一个","title":"在 Envoy 中集成 SPIRE"},{"content":"SPIFFE，即普适安全生产身份框架（Secure Production Identity Framework for Everyone），是一套开源标准，用于在动态和异构环境中安全地进行身份识别。采用 SPIFFE 的系统无论在哪里运行，都可以轻松可靠地相互认证。\nSPIFFE 开源规范的核心是——通过简单 API 定义了一个短期的加密身份文件 SVID。然后，工作负载进行认证时可以使用该身份文件，例如建立 TLS 连接或签署和验证 JWT 令牌等。\nSPIFFE 已经在云原生应用中得到了大量的应用，尤其是在 Istio 和 Envoy 中。下面将向你介绍 SPIFFE 的一些基本概念。\n工作负载 工作负载是一个单一的软件实体，通过特定配置部署，用于单一目的；它可能包括多个运行中的软件实例，所有这些实例执行相同的任务。术语“工作负载”可能涵盖软件系统的各种不同定义，包括：\n运行 Python Web 应用程序的 Web 服务器，部署在一组虚拟机上，前面有一个负载均衡器。 一个 MySQL 数据库的实例。 处理队列中条目的工作程序。 一组独立部署的系统共同工作， …","relpermalink":"/spiffe-and-spire/concept/spiffe/","summary":"SPIFFE，即普适安全生产身份框架（Secure Production Identity Framework for Everyone），是一套开源标准，用于在动态和异构环境中安全地进行身份识别。采用 SPIFFE 的系统无论在哪里运行，都可以轻松可靠地相互认证。 SPIFFE 开源规范","title":"SPIFFE 基本概念"},{"content":"SPIFFE 标准提供了一个规范，用于在异构环境和组织边界中引导和颁发服务的身份。它包括各种规范，每个规定了 SPIFFE 功能的特定子集的操作。\n特别是本文档作为 SPIFFE 标准的核心规范。虽然在 SPIFFE 范围内还有其他规范，但符合本文档就足以实现 SPIFFE 合规性，并获得 SPIFFE 标准本身的互操作性好处。\n引言 本文档提出了正式的 SPIFFE 规范。它定义了 SPIFFE 标准的两个最基本组件：SPIFFE 身份和 SPIFFE 可验证身份文档。\n第 2 节概述了 SPIFFE 身份（SPIFFE ID）及其命名空间。SPIFFE ID 被定义为符合RFC 3986标准的 URI，包括“信任域名”和相关路径。信任域名作为 URI 的授权组件，用于识别发放给定身份的系统。以下示例演示了如何构造 SPIFFE ID：\nspiffe://trust-domain-name/path\n有效的 SPIFFE ID 必须将方案设置为spiffe，包含非零的信任域名，并且不能包含查询或片段组件。换句话说，SPIFFE ID 由spiffe方案和一个特定站点 …","relpermalink":"/spiffe-and-spire/standard/spiffe-id/","summary":"SPIFFE 标准提供了一个规范，用于在异构环境和组织边界中引导和颁发服务的身份。它包括各种规范，每个规定了 SPIFFE 功能的特定子集的操作。 特别是本文档作为 SPIFFE 标准的核心规范。虽然在 SPIFFE 范围内还有其他规范，但符合本文档就足以","title":"SPIFFE ID 和 SVID"},{"content":"本页面描述了一些开始使用 SPIRE 的选项。\nDocker Compose SPIRE 101 是一个在 Docker Compose 上运行的 SPIRE 入门介绍 spire-tutorials 存储库中提供了其他 Docker Compose 演示 Kubernetes SPIRE 没有官方的 Helm chart、Kustomize 文件或自定义资源操作器，但 Kubernetes 快速入门 包括一套用于测试 SPIRE Server 和 Agent 的基本 Kubernetes YAML 文件 spire-tutorials 存储库中提供了其他 Kubernetes 演示 Linux SPIRE GitHub releases 页面提供了每个 SPIRE 版本的下载链接和变更日志 spiffe.io 的获取 SPIRE 页面提供了其他下载选项和构建 SPIRE 的说明 Linux 和 MacOS X 快速入门 介绍了如何下载和测试 SPIRE Server 和 Agent 的简单单节点安装 MacOS 没有预编译的 MacOS 可执行文件可用，但 Linux 和 MacOS …","relpermalink":"/spiffe-and-spire/installation/getting-spire/","summary":"本页面描述了一些开始使用 SPIRE 的选项。 Docker Compose SPIRE 101 是一个在 Docker Compose 上运行的 SPIRE 入门介绍 spire-tutorials 存储库中提供了其他 Docker Compose 演示 Kubernetes SPIRE 没有官方的 Helm chart、Kustomize 文件或自定义资源操作器，但 Kubernetes 快速入门 包括一套用于测试 SPIRE","title":"获取 SPIRE"},{"content":"扩展 SPIRE 可以通过嵌套拓扑和联合拓扑来实现。嵌套拓扑允许将多个 SPIRE 服务器链接在一起，以发放属于同一信任域的身份。联合拓扑用于在不同信任域之间建立信任，使工作负载能够在不同信任域中进行身份验证。SPIRE 还可以与其他 SPIFFE 兼容系统和 OIDC 提供者系统进行联合，以实现安全的身份验证和通信。在部署规模时，需要考虑 SVID 和根证书的生存时间、工作负载数量和分布、JWT-SVID 的使用等因素，并注意数据存储的设计和规划。\nSPIRE 部署可以根据工作负载的增长来调整大小或规模。一个 SPIRE 部署由一个或多个共享复制数据存储的 SPIRE 服务器组成，或者相反，由在同一信任域中的一组 SPIRE 服务器和至少一个 SPIRE 代理（通常是一个以上）组成。\n部署的大小范围广泛。单个 SPIRE 服务器可以容纳多个代理和工作负载注册条目。一个规模大小的考虑是，由于涉及到管理和发放与这些条目相对应的身份所涉及的操作数量，SPIRE 服务器实例的内存和 CPU 消耗往往与部署中的工作负载注册条目数量成比例增长。单个 SPIRE 服务器实例也代表了一个单点故障。\n …","relpermalink":"/spiffe-and-spire/architecture/scaling-spire/","summary":"扩展 SPIRE 可以通过嵌套拓扑和联合拓扑来实现。嵌套拓扑允许将多个 SPIRE 服务器链接在一起，以发放属于同一信任域的身份。联合拓扑用于在不同信任域之间建立信任，使工作负载能够在不同信任域中进行身份验证。SPIRE 还可","title":"扩展 SPIRE 部署：支持的 SPIRE 拓扑结构、身份联合和规模考虑"},{"content":"要根据你的应用程序需求自定义 SPIRE 服务器和 SPIRE 代理的行为，你需要编辑服务器和代理的配置文件。\n如何配置 SPIRE SPIRE 服务器和代理的配置文件分别为 server.conf 和 agent.conf。\n默认情况下，服务器期望配置文件位于 conf/server/server.conf，但是服务器可以通过 --config 标志配置为使用不同位置的配置文件。有关更多信息，请参阅 SPIRE 服务器参考。\n同样，代理期望配置文件位于 conf/agent/agent.conf，但是代理可以通过 --config 标志配置为使用不同位置的配置文件。有关更多信息，请参阅 SPIRE 代理参考。\n配置文件在启动服务器或代理时加载一次。如果更改了服务器或代理的配置文件，则必须重新启动服务器或代理以使配置生效。\n在 Kubernetes 中运行 SPIRE 时，通常将配置文件存储在 ConfigMap 对象中，然后将其作为文件挂载到运行代理或服务器进程的容器中。\nSPIRE 代理支持使用 HCL 或 JSON 作为配置文件结构语法。下面的示例将假定使用 HCL。 …","relpermalink":"/spiffe-and-spire/configuration/configuring/","summary":"要根据你的应用程序需求自定义 SPIRE 服务器和 SPIRE 代理的行为，你需要编辑服务器和代理的配置文件。 如何配置 SPIRE SPIRE 服务器和代理的配置文件分别为 server.conf 和 agent.conf。 默认情况下，服务器期望配置文件位于 conf/se","title":"配置 SPIRE"},{"content":"请见 TSB 文档：https://docs.tetrate.io/service-bridge/reference/k8s-api/tsb-crds-gen\n包：\ntsb.tetrate.io/v2 application.tsb.tetrate.io/v2 extension.tsb.tetrate.io/v2 gateway.tsb.tetrate.io/v2 istiointernal.tsb.tetrate.io/v2 rbac.tsb.tetrate.io/v2 security.tsb.tetrate.io/v2 traffic.tsb.tetrate.io/v2 ","relpermalink":"/tsb/reference/k8s-api/tsb-crds-gen/","summary":"请见 TSB 文档：https://docs.tetrate.io/service-bridge/reference/k8s-api/tsb-crds-gen 包： tsb.tetrate.io/v2 application.tsb.tetrate.io/v2 extension.tsb.tetrate.io/v2 gateway.tsb.tetrate.io/v2 istiointernal.tsb.tetrate.io/v2 rbac.tsb.tetrate.io/v2 security.tsb.tetrate.io/v2 traffic.tsb.tetrate.io/v2","title":"API 参考"},{"content":"如果你的 Elasticsearch 访问受角色限制，你需要确保为 TSB 组件存在正确的角色。\nOAP 对于 OAP，必要的角色权限在下面的 JSON 中描述如下。\n{ \u0026#34;cluster\u0026#34;: [\u0026#34;manage_index_templates\u0026#34;, \u0026#34;monitor\u0026#34;], \u0026#34;indices\u0026#34;: [ { \u0026#34;names\u0026#34;: [\u0026#34;skywalking_*\u0026#34;], \u0026#34;privileges\u0026#34;: [\u0026#34;manage\u0026#34;, \u0026#34;read\u0026#34;, \u0026#34;write\u0026#34;], \u0026#34;allow_restricted_indices\u0026#34;: false } ], \u0026#34;applications\u0026#34;: [], \u0026#34;run_as\u0026#34;: [], \u0026#34;metadata\u0026#34;: {}, \u0026#34;transient_metadata\u0026#34;: { \u0026#34;enabled\u0026#34;: true } } 你可以使用 cURL、Kibana 控制台或任何其他工具将此信息发布到 Elasticsearch 服务器以创建角色，然后你可以将该角色分配给将使用的 OAP 用户。\n","relpermalink":"/tsb/operations/elasticsearch/elasticsearch-role/","summary":"如果你的 Elasticsearch 访问受角色限制，你需要确保为 TSB 组件存在正确的角色。 OAP 对于 OAP，必要的角色权限在下面的 JSON 中描述如下。 { \"cluster\": [\"manage_index_templates\", \"monitor\"], \"indices\": [ { \"names\": [\"skywalking_*\"], \"privileges\": [\"manage\", \"read\", \"write\"], \"allow_restricted_indices\": false } ], \"applications\": [], \"run_as\": [], \"metadata\": {}, \"transient_metadata\": { \"enabled\": true } } 你可以使用 cURL、Kibana","title":"Elasticsearch 权限"},{"content":" Protobuf files\n要通过 gRPC 连接与 TSB 通信，您需要 protobuf 和 gRPC 服务定义文件。如果您希望获取它们，请联系您的 Tetrate 账户经理。 在本指南中，您将看到如何使用 TSB gRPC API 执行常见操作。本指南中的示例使用 Go 绑定，因为这是我们默认提供的绑定，但为其他语言生成的 gRPC 客户端也可以工作。\n初始化 gRPC 客户端 传输配置 在创建 gRPC 客户端时，首先要配置的是与 TSB 的连接。根据 TSB 的公开方式，您可以将连接配置为纯文本连接或 TLS 连接。\n配置纯文本连接 要配置纯文本连接，请使用以下 gRPC 拨号选项：\nopts := []grpc.DialOption{ grpc.WithBlock(), // 在调用 Dial 时阻塞，直到连接真正建立 grpc.WithInsecure(), } tsbAddress := \u0026#34;\u0026lt;tsb 主机\u0026gt;:\u0026lt;tsb 端口\u0026gt;\u0026#34; cc, err := grpc.DialContext(ctx, tsbAddress, opts...) 第一个选项将指导 gRPC 客户 …","relpermalink":"/tsb/reference/grpc-api/guide/","summary":"Protobuf files 要通过 gRPC 连接与 TSB 通信，您需要 protobuf 和 gRPC 服务定义文件。如果您希望获取它们，请联系您的 Tetrate 账户经理。 在本指南中，您将看到如何使用 TSB gRPC API 执行常见操作。本指南中的示例使用 Go 绑定，因为这是我们默认提供的绑定，但为","title":"gRPC API 指南"},{"content":"在开始之前，你必须具备以下条件：\nVault 1.3.1 或更新版本 Vault 注入器 0.3.0 或更新版本 设置 Vault 安装 Vault（不需要在 Kubernetes 集群中安装，但应该能够从 Kubernetes 集群内部访问）。Vault 注入器（agent-injector）必须安装到集群中，并配置以注入 sidecar。这可以通过自动完成 Helm 图表 v0.5.0+ 来实现，该图表安装了 Vault 0.12+ 和 Vault 注入器 0.3.0+。下面的示例假设 Vault 安装在 tsb 命名空间中。\n有关详细信息，请查看 Vault 文档。\nhelm install --name=vault --set=\u0026#39;server.dev.enabled=true\u0026#39; ./vault-helm 为 PostgreSQL 设置数据库秘密引擎 在 Vault 中启用数据库秘密引擎。\nvault secrets enable database 预期输出：\nSuccess! Enabled the database secrets engine at: database/ 默 …","relpermalink":"/tsb/operations/vault/postgresql/","summary":"在开始之前，你必须具备以下条件： Vault 1.3.1 或更新版本 Vault 注入器 0.3.0 或更新版本 设置 Vault 安装 Vault（不需要在 Kubernetes 集群中安装，但应该能够从 Kubernetes 集群内部访问）。Vault 注入器（agent-injector）必须安装到集","title":"PostgreSQL 凭据"},{"content":"在本指南中，您将学习如何使用 TSB REST API 执行常见操作。本指南中的示例使用 curl，因为它是用于执行 HTTP 请求的常用命令，但任何可以执行 HTTP 请求的工具都可以使用。\n身份验证 TSB 有两种主要的身份验证机制：基本身份验证和 JWT 令牌身份验证。\n基本身份验证 基本 HTTP 身份验证 通过在 HTTP Authorization 头中发送编码在头值中的凭据来完成。头的基本格式如下：\nAuthorization: Basic base64(username:password) 例如：\nAuthorization: Basic dGVzdDoxMjPCow== JWT 令牌身份验证 JWT 令牌身份验证是基于头的，并通过在 x-tetrate-token 头中设置 JWT 令牌来配置。例如：\nx-tetrate-token: …","relpermalink":"/tsb/reference/rest-api/guide/","summary":"在本指南中，您将学习如何使用 TSB REST API 执行常见操作。本指南中的示例使用 curl，因为它是用于执行 HTTP 请求的常用命令，但任何可以执行 HTTP 请求的工具都可以使用。 身份验证 TSB 有两种主要的身份验证机制：基本身份验证和 JWT","title":"REST API 指南"},{"content":"概述 本文介绍如何使用 Helm Charts 来安装 Tetrate Service Bridge (TSB) 的不同组件。假设你的系统上已经安装了 Helm。\nTSB 为其每一个平面 都提供了 Chart：\n管理平面：安装 TSB 管理平面 Operator（可选择安装 MP CR 和/或密钥）。 控制平面：安装 TSB 控制平面 Operator（可选择安装 MP CR 和/或密钥）。 数据平面：安装 TSB 数据平面 Operator。 每个 Chart 都安装了相应平面的 Operator。管理平面和控制平面都允许创建触发 Operator 的相应资源（使用 spec 属性）以部署所有 TSB 组件和/或必需的密钥（使用 secrets 属性）以使其正常运行。\n这种行为让你选择完全配置 TSB 并与 CD 流水线集成的方式。你可以使用 Helm 来：\n仅安装 Operator 安装/升级平面资源（管理平面或控制平面 CR）以及 Operator 安装/升级Operator和密钥 一次安装/升级它们（Operator、资源、密钥） 关于密钥，要牢记 helm …","relpermalink":"/tsb/setup/helm/helm/","summary":"概述 本文介绍如何使用 Helm Charts 来安装 Tetrate Service Bridge (TSB) 的不同组件。假设你的系统上已经安装了 Helm。 TSB 为其每一个平面 都提供了 Chart： 管理平面：安装 TSB 管理平面 Operator（可选择安装 MP CR 和/或密钥）。 控制平面：","title":"TSB Helm Chart"},{"content":"本文档解释了如何在 TSB 中使用 GitOps 工作流。该文档假设已在管理平面集群和/或应用程序集群中启用了 GitOps。\nTSB 中 GitOps 支持的主要思想是允许：\n管理员团队可以直接在管理平面集群中创建 TSB 配置资源。 应用程序团队可以直接在应用程序集群中创建 TSB 配置资源。 应用程序团队可以像推送应用程序本身的更改一样推送应用程序配置的更改，并允许将应用程序部署资源和 TSB 配置打包在一起，例如在同一个 Helm 图中。\n为了实现这一点，所有 TSB 配置对象都存在于 Kubernetes 自定义资源定义（CRD）中，以便可以轻松应用于集群。如下图所示，一旦资源应用到集群中，它们将被自动协调并转发到管理平面。\nTSB 中基于 Flux 的 GitOps 示意图 TSB Kubernetes 自定义资源 用于 TSB 配置的 Kubernetes 自定义资源与任何其他 Kubernetes 资源一样。以下示例显示了一个 Workspace 定义：\napiVersion: tsb.tetrate.io/v2 kind: Workspace metadata: …","relpermalink":"/tsb/howto/gitops/gitops/","summary":"本文档解释了如何在 TSB 中使用 GitOps 工作流。该文档假设已在管理平面集群和/或应用程序集群中启用了 GitOps。 TSB 中 GitOps 支持的主要思想是允许： 管理员团队可以直接在管理平面集群中创建 TSB 配置资源。 应用程序团队可以直接","title":"TSB 中的 GitOps"},{"content":"本文将描述什么是 WASM 扩展以及其好处。\n什么是 WASM 扩展？ WASM 扩展是WebAssembly的软件插件，可用于扩展 Istio 代理（Envoy）。 这些 WASM 扩展在一个沙盒环境中执行，对外部系统的访问受到限制，并且可以使用不同的编程语言及其 SDK 创建。 这个沙盒环境提供了隔离，以防止一个插件中的编程错误或崩溃影响其他插件，并提供了安全性，以防止一个插件从系统获取信息。\nWASM 扩展的好处是什么？ Envoy 可以使用过滤器进行扩展，有各种内置的过滤器用于不同的协议，可以配置为在网络流量的一部分执行。 通过这些过滤器（网络、HTTP）的组合，你可以增强传入请求、转换协议、收集统计信息、修改响应、执行身份验证等等。\n为了拥有自定义过滤器，有几种选择：\n使用 C++编写自己的过滤器并将其与 Envoy 打包。 这意味着重新编译 Envoy 并维护不同版本。 使用依赖于 HTTP Lua 过滤器的 Lua 脚本。 适用于简单的脚本和更复杂的部署过程。 使用基于 WASM 的扩展 允许使用不同的编程语言编写复杂的脚本，并自动化部署过程。 一些 WASM 扩展的好处 …","relpermalink":"/tsb/howto/wasm/wasm-overview/","summary":"本文将描述什么是 WASM 扩展以及其好处。 什么是 WASM 扩展？ WASM 扩展是WebAssembly的软件插件，可用于扩展 Istio 代理（Envoy）。 这些 WASM 扩展在一个沙盒环境中执行，对外部系统的访问受到限制，并且可以使用不同的编","title":"WASM 扩展概述"},{"content":"在本指南中，你将学习如何使用 TSB CLI（tctl）执行常见操作。你将学习如何配置 CLI 以访问你的 TSB 安装，并如何从命令行管理 TSB 资源。\n入门 要使用 YAML API，你需要安装并配置 TSB CLI。一旦你安装并配置了 CLI 以与你的 TSB 安装进行通信，你将需要使用 tctl login 命令配置对 TSB 平台的访问权限：\ntctl login 你将被要求提供 TSB 组织名称，这在 TSB 安装过程中已设置或提供给你，租户名称以及凭据。\n平台管理员应该已经为你分配了一个租户。如果没有，或者你是执行初始设置的管理员，可以将租户留空。你始终可以稍后编辑已配置的用户并在需要时设置租户。\nOrganization: tetrate Tenant: Username: admin Password: Login Successful! Configured user: demo-admin User \u0026#34;demo-admin\u0026#34; enabled in profile: demo 重要提示\n用户设置中预先配置的组织和租户仅用于 tctl get 和 tctl …","relpermalink":"/tsb/reference/yaml-api/guide/","summary":"在本指南中，你将学习如何使用 TSB CLI（tctl）执行常见操作。你将学习如何配置 CLI 以访问你的 TSB 安装，并如何从命令行管理 TSB 资源。 入门 要使用 YAML API，你需要安装并配置 TSB CLI。一旦你安装并配置了 CLI 以与你的 TSB","title":"YAML API 指南"},{"content":"为了演示在 Kubernetes 之外部署的工作负载如何与网格的其余部分集成，我们需要有其他应用程序可以与之通信。\n在本指南中，你需要部署 Istio Bookinfo 示例到你的 Kubernetes 集群中。\n部署 Bookinfo 示例 创建命名空间 bookinfo，并添加正确的标签：\nkubectl create namespace bookinfo kubectl label namespace bookinfo istio-injection=enabled 部署 bookinfo 应用程序：\ncat \u0026lt;\u0026lt;EOF | kubectl apply -n bookinfo -f - apiVersion: security.istio.io/v1beta1 kind: PeerAuthentication metadata: name: default spec: mtls: mode: STRICT EOF kubectl apply -n bookinfo -f …","relpermalink":"/tsb/setup/workload-onboarding/quickstart/aws-ec2/bookinfo/","summary":"为了演示在 Kubernetes 之外部署的工作负载如何与网格的其余部分集成，我们需要有其他应用程序可以与之通信。 在本指南中，你需要部署 Istio Bookinfo 示例到你的 Kubernetes 集群中。 部署 Bookinfo 示例 创建命名空间 bookinfo，并添加正确的标签： kubectl create","title":"安装 Bookinfo 示例"},{"content":"httpbin 是一个简单的 HTTP 请求和响应服务，用于测试。\n在 TSB 文档中的许多示例中都使用了 httpbin 服务。本文档提供了该服务的基本安装过程。\n请确保参考每个 TSB 文档，以了解特定的注意事项或所需的自定义，以使示例正常工作，因为本文档描述了最通用的安装步骤。\n以下示例假设你已经设置了 TSB，并且已经将 Kubernetes 集群注册到要安装 httpbin 工作负载的 TSB 上。\n除非另有说明，使用 kubectl 命令的示例必须指向相同的集群。在运行这些命令之前，请确保你的 kubeconfig 指向所需的集群。\n命名空间 除非另有说明，假定 httpbin 服务已安装在 httpbin 命名空间中。如果尚未存在，请在目标集群中创建此命名空间。\n运行以下命令以创建命名空间（如果尚不存在）：\nkubectl create namespace httpbin 此命名空间中的 httpbin pod 必须运行 Istio sidecar 代理。要自动启用此 sidecar 对所有 pod 的注入，请执行以下操作：\nkubectl label namespace …","relpermalink":"/tsb/reference/samples/httpbin/","summary":"httpbin 是一个简单的 HTTP 请求和响应服务，用于测试。 在 TSB 文档中的许多示例中都使用了 httpbin 服务。本文档提供了该服务的基本安装过程。 请确保参考每个 TSB 文档，以了解特定的注意事项或所需的自定义，以使示例正常工作，因为本文档","title":"安装 httpbin"},{"content":"在继续之前，请确保你熟悉 Istio 隔离边界 功能。\n升级前 从非修订版升级到版本控制平面设置涉及启用 Istio 隔离边界功能。 启用后，可以在隔离边界内配置版本，控制平面必须升级到该版本。 按照 隔离边界安装 中提到的步骤部署具有启用隔离边界功能的控制平面。\n启用 Istio 隔离边界功能后，你需要在添加隔离边界到 ControlPlane CR 之前，将 TSB 数据平面 Operator 的规模缩小。这是为了避免 TSB 数据平面 Operator 和 TSB 控制平面 Operator 在协调相同的 TSB Ingress/Egress/Tier1Gateway 资源时发生竞争条件。\nkubectl scale --replicas=0 deployment tsb-operator-data-plane -n istio-gateway 出于同样的原因，我们还必须将 istio-operator 在 istio-gateway 命名空间中的规模缩小。\nkubectl scale --replicas=0 deployment istio-operator -n …","relpermalink":"/tsb/setup/upgrades/non-revisioned-to-revisioned/","summary":"在继续之前，请确保你熟悉 Istio 隔离边界 功能。 升级前 从非修订版升级到版本控制平面设置涉及启用 Istio 隔离边界功能。 启用后，可以在隔离边界内配置版本，控制平面必须升级到该版本。 按照 隔离边界安装 中提到的步骤部署具有启","title":"非修订版到修订版的升级"},{"content":"Service Mesh 架构已得到广泛采用，Tetrate 的团队由一些最早开发支持该架构的技术的工程师组成。在本节中，我们将介绍该架构、其术语、功能、特性，并重点介绍 Istio，这是为 Tetrate Service Bridge 提供支持的领先网格实现。\n什么是服务网格？ 服务网格是通过代理位于应用程序组件和网络之间的基础设施层。虽然这些组件通常是微服务，但任何工作负载（从无服务器容器到虚拟机或裸机上的传统 n 层应用程序）都可以参与网格。代理不是通过网络在组件之间进行直接通信，而是拦截并管理该通信。\n服务网格架构：控制平面和数据平面 数据平面 这些代理被称为“sidecar 代理”，因为它们与每个应用程序实例一起部署，构成了服务网格的数据平面。它们在运行时处理应用程序流量。Tetrate Service Bridge 采用 Envoy 作为数据平面实现。Envoy 提供了大量的安全、流量策略和遥测功能，包括：\n服务发现 弹性机制（重试、熔断、异常值检测） 客户端负载均衡 细粒度的 L7 流量控制 根据请求实施安全策略 基于 L7 元数据的身份验证、速率限制、策略 具有强 L7 …","relpermalink":"/tsb/concepts/service-mesh/","summary":"Service Mesh 架构已得到广泛采用，Tetrate 的团队由一些最早开发支持该架构的技术的工程师组成。在本节中，我们将介绍该架构、其术语、功能、特性，并重点介绍 Istio，这是为 Tetrate Service Bridge 提供支持的领先网格实现。 什么是","title":"服务网格简介"},{"content":"当你在 Kubernetes 上部署工作负载时，以下操作会在背后自动进行：\nIstio Sidecar 会部署在你的工作负载旁边。 该 Sidecar 会配置工作负载的位置和其他所需元数据。 然而，当你将工作负载部署在独立的虚拟机之外时， 你必须自己处理这些事情。\n工作负载载入功能为你解决了这个问题。 使用此功能，你只需执行以下步骤，即可将部署在虚拟机上的工作负载引入到网格中：\n在目标虚拟机上安装 Istio Sidecar（通过 DEB/RPM 软件包）。 在目标虚拟机上安装 Workload Onboarding Agent（同样通过 DEB/RPM 软件包）。 提供一个最小的、声明性的配置，描述在哪里引入工作负载，例如： apiVersion: config.agent.onboarding.tetrate.io/v1alpha1 kind: OnboardingConfiguration onboardingEndpoint: # 连接至 host: onboarding-endpoint.your-company.corp workloadGroup: # …","relpermalink":"/tsb/setup/workload-onboarding/guides/overview/","summary":"当你在 Kubernetes 上部署工作负载时，以下操作会在背后自动进行： Istio Sidecar 会部署在你的工作负载旁边。 该 Sidecar 会配置工作负载的位置和其他所需元数据。 然而，当你将工作负载部署在独立的虚拟机之外时， 你必须自己处理这些事情。 工作负","title":"概览"},{"content":"本文描述如何更改 TSB 管理员的密码。\nTSB 管理员在每个 TSB 实例中都是本地配置的，不属于企业身份提供者（IdP）。这允许超级用户在连接到身份提供者出现问题以进行故障排除和平台修复时能够登录 TSB。\n更新密钥 管理员凭据存储在管理平面命名空间中的 admin-credentials Kubernetes 密钥中（默认为 tsb）。它以 SHA-256 哈希的形式安全存储，因此无法被反向解析，可以通过直接更新带有所需密码的密钥来修改。\n以下示例显示了如何生成一个稍后可以应用的更新密钥：\nnew_password=\u0026#34;Tetrate1\u0026#34; new_password_shasum=$(echo -n $new_password | shasum -a 256 | awk \u0026#39;{print $1}\u0026#39;) kubectl -n tsb create secret generic admin-credentials --from-literal=admin=$new_password_shasum --dry-run=client -o yaml 这将输出包含更新密码的密钥的 YAML，并可以 …","relpermalink":"/tsb/operations/users/admin-password/","summary":"本文描述如何更改 TSB 管理员的密码。 TSB 管理员在每个 TSB 实例中都是本地配置的，不属于企业身份提供者（IdP）。这允许超级用户在连接到身份提供者出现问题以进行故障排除和平台修复时能够登录 TSB。 更新密钥 管理员凭","title":"更改管理员密码"},{"content":"本页深入介绍了 TSB Operator 如何配置管理平面组件，并概述了 TSB Operator 管理的各种组件。\nTSB Operator 配置为监督管理平面组件的生命周期，主动监视部署的同一命名空间内的 ManagementPlane 自定义资源 (CR)。默认情况下，管理平面驻留在 tsb 命名空间中。你可以参阅管理平面安装 API 参考文档，了解有关自定义资源 API 的全面详细信息。\n组件 管理平面组件 以下是你可以使用管理平面 Operator 配置和管理的各种类型的自定义组件：\n组件 Service Deployment Cronjobs apiServer tsb tsb teamsync iamServer iam iam webUI web web frontEnvoy envoy envoy oap oap oap collector otel-collector otel-collector xcpOperator xcp-operator-central xcp-operator-central xcpCentral xcp-central central …","relpermalink":"/tsb/concepts/operators/management-plane/","summary":"本页深入介绍了 TSB Operator 如何配置管理平面组件，并概述了 TSB Operator 管理的各种组件。 TSB Operator 配置为监督管理平面组件的生命周期，主动监视部署的同一命名空间内的 ManagementPlane 自定义资源 (CR)。默认情况下，管理平面驻留在 tsb 命名空间中。你","title":"管理平面"},{"content":"如果 Tetrate 管理平面失败，您需要恢复管理平面以恢复正常操作状态。本指南提供了一个流程概述，您应该在进行此过程时与 Tetrate 技术支持 协商。\n为了应对管理组件的意外故障，我们建议考虑以下建议：\n要么在可靠的冗余集群中维护 Postgres 数据库，要么（在 TSE 的情况下）利用定期的 Postgres 备份。 保留 iam-signing-key 的备份。 如果保留指标很重要，请在可靠的冗余集群中维护 ElasticSearch 数据库，或定期备份，以便在必要时进行恢复。 概述 如果管理平面失败或托管管理平面的集群停止运行，您需要恢复管理平面以恢复正常运行状态。恢复是使用 helm 基础安装完成的。 本方案将演示如何在新的管理集群上从失败的管理集群中恢复配置的任务。\n先决条件 本指南做出以下假设：\nPostgreSQL 数据库（配置）可用。要么数据库位于失败的集群之外，要么可以从备份中恢复（仅适用于 TSE）。 ElasticSearch 数据库（指标）可用。要么数据库位于失败的集群之外，要么可以从备份中恢复，或者可以使用全新的（空的）ElasticSearch 数据 …","relpermalink":"/tsb/design-guides/ha-dr-mp/dr-managementplane/","summary":"如果 Tetrate 管理平面失败，您需要恢复管理平面以恢复正常操作状态。本指南提供了一个流程概述，您应该在进行此过程时与 Tetrate 技术支持 协商。 为了应对管理组件的意外故障，我们建议考虑以下建议： 要么在可靠的冗余集群中维护 Postgres","title":"恢复失败的管理平面组件"},{"content":"本文档介绍了在 TSB 中进行基本故障排除的一些可能方法，以便查找特定路由的错误配置问题或 50x 错误的常见原因。\n系统架构 在本文档中，采用了以下具有 Tier1-Tier2 设置的系统架构：\n有两个不同的集群，training-mp 包含管理平面和配置为 tier1 的控制平面，training-cp 配置为 tier2，包含 bookinfo 和 httpbin 应用程序。\nTier1 网关故障排除 当检测到 50x 错误时，重要的是要理解错误消息，因为它会指向不同的信息源。\n例如，假设你使用 curl 发出了一个 HTTP 请求到由 TSB 控制的服务之一，并且观察到类似以下的错误：\nFailed to connect to \u0026lt;hostname\u0026gt; port \u0026lt;port\u0026gt;: Connection refused 这通常意味着没有配置监听器。这又意味着我们要么：\n缺少网关对象 访问了错误的端口 网关没有正确配置，或者 Tier1 网关的 Pod 没有运行。 要检查监听器是否存在，你可以使用 istioctl：\n$ istioctl pc listener …","relpermalink":"/tsb/troubleshooting/troubleshooting/","summary":"本文档介绍了在 TSB 中进行基本故障排除的一些可能方法，以便查找特定路由的错误配置问题或 50x 错误的常见原因。 系统架构 在本文档中，采用了以下具有 Tier1-Tier2 设置的系统架构： 有两个不同的集群，training-mp 包含管理","title":"基本故障排除"},{"content":"平台所有者（“平台”）将通过以下步骤准备一个集群：\n部署 TSE/TSB\n首先部署 TSE 或 TSB，并启动预期的工作负载集群。\n启用严格（零信任）安全\n配置平台以遵循 ‘require-mTLS’ 和 ‘deny-all’ 的零信任安全策略。\n创建 Kubernetes 命名空间\n在每个将由应用所有者用于托管服务和应用程序的集群中创建并标记命名空间。\n创建 Tetrate 工作区\n创建将用于管理命名空间内服务行为的 Tetrate 工作区和相关配置。\n部署入口网关\n如有需要，在将托管应该提供给外部访问的服务的工作区中部署入口网关。\n启用 GitOps 集成\n启用 GitOps 集成，以便应用所有者用户可以在不需要 Tetrate 特权访问的情况下与平台交互。\n启用其他集成\n启用其他集成，以便应用所有者用户可以在不需要 Tetrate 特权访问的情况下与平台交互。\n平台：部署 TSE/TSB 按照产品说明部署 TSE 或 TSB 管理平面，然后启动预期的工作负载集群。\n请确保安装所需的附加组件并满足必要的先决条件。\n平台：启用严格安全 你应该使用 TSE/TSB 配置平台以零信任方式 …","relpermalink":"/tsb/design-guides/app-onboarding/prepare/","summary":"平台所有者（“平台”）将通过以下步骤准备一个集群： 部署 TSE/TSB 首先部署 TSE 或 TSB，并启动预期的工作负载集群。 启用严格（零信任）安全 配置平台以遵循 ‘require-mTLS’ 和 ‘deny-all’ 的零信任安全策略。 创建 Kubernetes 命名空间","title":"集群准备"},{"content":"欢迎使用 TSB 快速入门指南！本指南旨在引导你完成在 TSB 上加入和配置应用程序的过程。通过遵循本快速入门，你将了解如何针对各种基本场景部署应用程序并配置 TSB 及其组件。\n在本快速入门指南中，你将探索以下场景：\n部署 Istio bookinfo 示例应用程序 创建租户并连接集群 创建工作区 建立对工作区的 tctl 访问权限 创建配置组 配置权限 设置入口网关 检查服务拓扑和指标 使用 TSB 进行流量转移 在 TSB 内启用安全设置 创建应用程序并使用 OpenAPI 规范配置 API 在开始使用快速入门指南之前，请确保你：\n熟悉 TSB 概念 安装 TSB 演示环境 本指南中的每个示例将演示如何使用 tctl 命令行工具和 TSB UI 进行更改。\n在这些示例中，你将使用超级管理员权限，授予你访问所有 TSB 功能的权限。但是，请记住，对于生产用途，并非每个人都可以被授予管理员权限。出于安全考虑，不建议为每个人提供管理员访问权限。\n","relpermalink":"/tsb/quickstart/introduction/","summary":"欢迎使用 TSB 快速入门指南！本指南旨在引导你完成在 TSB 上加入和配置应用程序的过程。通过遵循本快速入门，你将了解如何针对各种基本场景部署应用程序并配置 TSB 及其组件。 在本快速入门指南中，你将探索以下场景： 部署 Istio bookinfo","title":"快速开始简介"},{"content":"你将在本地虚拟机上部署 ratings 应用程序并将其加入服务网格。\n创建工作负载组 执行以下命令以创建一个 WorkloadGroup：\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: networking.istio.io/v1alpha3 kind: WorkloadGroup metadata: name: ratings namespace: bookinfo labels: app: ratings spec: template: labels: app: ratings class: vm serviceAccount: bookinfo-ratings EOF 字段 spec.template.network 被省略，以指示 Istio 控制平面虚拟机在本地具有直接连接到 Kubernetes Pod 的能力。\n字段 spec.template.serviceAccount 声明工作负载具有 Kubernetes 集群内服务账号 bookinfo-ratings 的身份。此服务账号是在之前的 Istio bookinfo 示例部署期 …","relpermalink":"/tsb/setup/workload-onboarding/quickstart/on-premise/configure-workload-onboarding/","summary":"你将在本地虚拟机上部署 ratings 应用程序并将其加入服务网格。 创建工作负载组 执行以下命令以创建一个 WorkloadGroup： cat \u003c\u003cEOF | kubectl apply -f - apiVersion: networking.istio.io/v1alpha3 kind: WorkloadGroup metadata: name: ratings namespace: bookinfo labels: app: ratings spec: template: labels: app: ratings class: vm serviceAccount: bookinfo-ratings EOF 字段 spec.template.network 被省略，以指示 Istio 控制","title":"配置本地 WorkloadGroup 和 Sidecar"},{"content":"你将部署 ratings 应用程序作为 AWS ECS 任务，并将其加入服务网格。\n创建 WorkloadGroup 执行以下命令创建一个 WorkloadGroup：\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: networking.istio.io/v1alpha3 kind: WorkloadGroup metadata: name: ratings namespace: bookinfo labels: app: ratings spec: template: labels: app: ratings class: ecs cloud: aws serviceAccount: bookinfo-ratings EOF 字段 spec.template.serviceAccount 声明了工作负载将具有 Kubernetes 集群内的服务账号 bookinfo-ratings 的身份。服务账号 bookinfo-ratings 是在之前部署 Istio bookinfo 示例时创建的。\n创建 Sidecar …","relpermalink":"/tsb/setup/workload-onboarding/quickstart/aws-ecs/configure-workload-onboarding/","summary":"你将部署 ratings 应用程序作为 AWS ECS 任务，并将其加入服务网格。 创建 WorkloadGroup 执行以下命令创建一个 WorkloadGroup： cat \u003c\u003cEOF | kubectl apply -f - apiVersion: networking.istio.io/v1alpha3 kind: WorkloadGroup metadata: name: ratings namespace: bookinfo labels: app: ratings spec: template: labels: app: ratings class: ecs cloud: aws serviceAccount: bookinfo-ratings EOF 字段 spec.template.serviceAccount 声明了工作负载将具有 Kubernetes 集群","title":"配置 AWS ECS 工作负载的 WorkloadGroup 和 Sidecar"},{"content":"TSB 带有每个控制平面集群的速率限制服务器组件。默认情况下，此功能已禁用。\n此部分仅讨论内部模式的安装过程，不涉及外部服务器的安装。\n配置 可以通过在 ControlPlane Operator API 或 Helm 值 中明确指定 rateLimitServer 组件的配置并将其应用于相关的控制平面集群来启用速率限制服务器。rateLimitServer 需要一个 Redis 后端来跟踪速率限制属性计数，并且其详细信息需要包含在配置中。\n你的 Control Plane Operator 配置可能如下所示：\nspec: ... components: rateLimitServer: domain: \u0026lt;domain\u0026gt; backend: redis: uri: \u0026lt;redis-uri\u0026gt; 注意在 components 对象中引入了 rateLimitServer。\ndomain 的值用于对速率限制的存储元数据进行分组。对所有 Control Planes 指定相同的 domain 将有效允许你配置跨所有集群的全局速率限制。如果使用不同的值为 domain，那么速率限制效果将仅局限于查看相 …","relpermalink":"/tsb/howto/rate-limiting/internal-rate-limiting/","summary":"TSB 带有每个控制平面集群的速率限制服务器组件。默认情况下，此功能已禁用。 此部分仅讨论内部模式的安装过程，不涉及外部服务器的安装。 配置 可以通过在 ControlPlane Operator API 或 Helm 值 中明确指定 rateLimitServer 组件的配置并将其应用于相关的控制平面","title":"启用内部速率限制服务器"},{"content":"Tetrate Service Bridge 的命令行界面（CLI）允许你与 TSB API 交互，以便以编程或交互的方式轻松操作对象和配置。CLI 通过提交 TSB 或 Istio 对象的 YAML 表示来工作。\n安装 请参阅 TSB 文档。\n配置 CLI 配置支持多个配置文件，用于轻松管理来自同一 CLI 的不同环境。CLI 中的配置由集群和凭据的一对一配对定义。\n凭据 CLI 中的凭据被称为 user。有关 user 子命令的完整参考信息可以在 CLI 参考 页面中找到。下面是创建名为 admin-user 用户的示例：\ntctl config users set admin-user --username admin --password \u0026#39;MySuperSecret!\u0026#39; --org tetrate --tenant tenant1 每当在配置文件中使用 admin-user 时，CLI 将提交 admin 用户和 MySuperSecret! 密码，以及 tetrate 组织和 tenant1 租户。\n密码中的特殊字符\n在终端中使用可能被视为特殊字符的字符时要小心。例如，如果 …","relpermalink":"/tsb/reference/cli/guide/","summary":"Tetrate Service Bridge 的命令行界面（CLI）允许你与 TSB API 交互，以便以编程或交互的方式轻松操作对象和配置。CLI 通过提交 TSB 或 Istio 对象的 YAML 表示来工作。 安装 请参阅 TSB 文档。 配置 CLI 配置支持多个配置文件，用于轻松管理来自同一 CLI 的不","title":"入门指南"},{"content":"Tetrate Service Bridge (TSB) 提供了授权功能，用于授权来自另一个服务的每个 HTTP 请求（“服务到服务\u0026#34;请求）。\nTSB 支持本地授权，使用 JWT 声明，以及外部授权，后者使用在外部运行的服务来确定是否应允许或拒绝请求。外部授权可以用于网关和工作负载（通过它们的 Sidecar）。\n如果你有一个独立的内部系统，或者希望与第三方授权解决方案（如 Open Policy Agent (OPA) 或 PlainID）集成，你可以决定使用外部授权系统。\n本文描述了如何使用 OPA 作为示例配置服务到服务的授权。OPA 是一个开源的通用策略引擎，提供高级声明性语言，让你可以将策略规定为代码。\nOPA 支持\nTetrate 不提供对 OPA 的支持。如果你需要针对你的用例支持，请查找其他支持。 在开始之前，请确保你已经完成以下步骤：\n熟悉 TSB 概念 安装 TSB 环境。你可以使用 TSB 演示 进行快速安装 完成了 TSB 使用快速入门。本文假设你已经创建了一个租户，并熟悉 工作空间 和配置组。还需要将 tctl 配置到你的 TSB 环境。 概述 下图显示了在使 …","relpermalink":"/tsb/howto/authorization/sidecar/","summary":"Tetrate Service Bridge (TSB) 提供了授权功能，用于授权来自另一个服务的每个 HTTP 请求（“服务到服务\"请求）。 TSB 支持本地授权，使用 JWT 声明，以及外部授权，后者使用在外部运行的服务来确定是否应允许或拒绝请求。外","title":"使用外部授权进行服务到服务的授权"},{"content":"本文介绍了 Tetrate Service Bridge（TSB）生态系统中统一网关的概念，解释了其重要性，并提供了详细的使用场景。\n简介 统一网关是在 TSB 1.7.0 中引入的关键功能，它将Tier1Gateway和IngressGateway的功能合并到一个称为Gateway的公共资源中。这种统一简化了网关管理过程，并提供了更一致的体验。\n从 TSB 1.7.0 开始，Tier1Gateway 和 IngressGateway 资源将被弃用，我们强烈建议使用 Gateway 资源满足你的所有网关需求。前 Tier1 Gateway 现在将被统称为Edge Gateway。\n统一网关选项卡无缝集成到 TSB UI 中，使得任何网关的配置都变得容易，不管它是作为 Tier 1 还是 Tier 2 网关工作。\nTSB UI 中的 Unified Gateway 为什么需要统一网关？ 在我们的旅程早期，我们认识到我们的客户对集群特定（Tier 2）和跨云供应商（Tier1）网关有不同的需求。因此，我们开发了不同的网关解决方案来满足这些不同的需求。然而，随着我们的 Gateway API …","relpermalink":"/tsb/howto/gateway/unified-gateway/","summary":"本文介绍了 Tetrate Service Bridge（TSB）生态系统中统一网关的概念，解释了其重要性，并提供了详细的使用场景。 简介 统一网关是在 TSB 1.7.0 中引入的关键功能，它将Tier1Gateway和IngressGateway的","title":"统一网关"},{"content":"本页面提供了开始使用 Tetrate Service Bridge（TSB）安装所需的先决条件和下载说明的全面概述。\n要有效地管理 TSB 服务网格，需要对 Kubernetes 和 Docker 仓库操作有深入的了解。我们建议咨询它们各自的支持文档以获取额外的指导。\n先决条件 你可以安装用于生产的 TSB，也可以安装用于演示配置文件以快速了解 TSB。请查看以下表格中的每个要求：\n项目 生产 TSB 演示/快速入门TSB Kubernetes 集群：\nEKS 1.21 - 1.24\nGKE 1.21 - 1.24\nAKS 1.21 - 1.24（包括 Azure Stack HCI）\nOpenShift 4.7 - 4.11\nDocker UCP 3.2.5 或更高版本 ✓ ✓ Docker UCP 3.2.5 或更高版本 ✓ ✓ 私有 Docker 注册表（HTTPS） ✓ ✓ Tetrate 存储库帐户和 API 密钥（如果你尚未拥有此内容，请联系 Tetrate） ✓ ✓ Docker 引擎 18.03.01 或更高版本，具有对私有 Docker 注册表的推送访问权限 ✓ ✓ …","relpermalink":"/tsb/setup/requirements-and-download/","summary":"本页面提供了开始使用 Tetrate Service Bridge（TSB）安装所需的先决条件和下载说明的全面概述。 要有效地管理 TSB 服务网格，需要对 Kubernetes 和 Docker 仓库操作有深入的了解。我们建议咨询它们各自的支持文档以获取额外的指导。 先决条件","title":"先决条件和下载"},{"content":"本指南将引导你完成 TSB 演示配置文件的安装，该配置文件旨在快速概述 TSB 的功能。演示配置文件包括 PostgreSQL、Elasticsearch 和 LDAP，所有这些都在 Kubernetes 集群上进行编排。为了确保无缝体验，你的集群应包含 3-6 个节点，每个节点至少配备 4 个 vCPU 和 16 GB 内存。集群还必须建立默认存储类，并能够为 Elasticsearch 和 PostgreSQL 创建最小容量为 100 GB 的持久卷声明。\n在继续之前，请参阅 TSB 支持政策来验证与你的 Kubernetes 版本的兼容性。\n先决条件 要安装演示配置文件，请确保你已完成以下步骤：\n1. 获取 tctl 并同步镜像 首先按照下载部分中概述的步骤下载 tctl 。此外，按照同步容器镜像中所述同步所需的容器镜像。\n2. 设置 Kubernetes 集群 准备一个要安装演示配置文件的 Kubernetes 集群。创建集群的具体步骤取决于你的环境。有关创建 Kubernetes 集群的具体说明，请参阅你的环境手册。\n使用 kind 如果你使用 kind 集群进行安装，请按照 …","relpermalink":"/tsb/setup/self-managed/demo-installation/","summary":"本指南将引导你完成 TSB 演示配置文件的安装，该配置文件旨在快速概述 TSB 的功能。演示配置文件包括 PostgreSQL、Elasticsearch 和 LDAP，所有这些都在 Kubernetes 集群上进行编排。为了确保无缝体验，你的","title":"演示安装"},{"content":"创建一个简单的示例，包括两个工作负载集群和一个边缘网关集群。\n在这个示例中，我们将配置三个 Kubernetes 集群：\n集群 cluster-1 和 cluster-2 将作为工作负载集群，每个集群都有一个 bookinfo 应用程序实例和一个 Ingress Gateway 用于公开应用程序 集群 cluster-edge 将托管前端边缘（“Tier-1”）网关，该网关将接收流量并分发到工作负载集群中的 Ingress Gateway 边缘和工作负载负载均衡 开始之前 在配置中有一些移动部分，因此在继续之前，识别并命名每个部分会很有帮助：\ncluster-1 cluster-2 cluster-edge AWS 区域： eu-west-1 eu-west-2 eu-west-1 命名空间： bookinfo bookinfo edge 工作区： bookinfo-ws bookinfo-ws edge-ws 网络： app-network app-network edge-network 网关组： bookinfo-gwgroup-1 bookinfo-gwgroup-2 …","relpermalink":"/tsb/design-guides/ha-multicluster/demo-1/","summary":"创建一个简单的示例，包括两个工作负载集群和一个边缘网关集群。 在这个示例中，我们将配置三个 Kubernetes 集群： 集群 cluster-1 和 cluster-2 将作为工作负载集群，每个集群都有一个 bookinfo 应用程序实例和一个 Ingress Gateway 用于公开应用程序 集群 cluster-edge 将托管前端边","title":"演示环境"},{"content":" 注意\n本页面详细介绍了如何收集 Tetrate Service Bridge 运营所需的遥测数据，而不是由 Tetrate Service Bridge 管理的应用程序。 Tetrate Service Bridge 使用 Open Telemetry Collector 来简化指标收集。标准部署包括管理平面中的一个 Collector，以及每个已接入的控制平面旁边都有一个 Collector。使用 Collector 使 Tetrate Service Bridge 能够通过只需 Operator 抓取一个组件而不是所有组件，从而简化每个集群的遥测数据收集。\n管理平面 在管理平面中有一个名为 collector 的组件。它是一个聚合器，通过 Prometheus 公开了一个用于抓取所有管理平面组件的端点。\n要查看此端点的输出，可以使用以下方式查询：\nkubectl port-forward -n \u0026lt;managementplane-namespace\u0026gt; svc/otel-collector 9090:9090 \u0026amp; curl localhost:9090/metrics 示例输出： …","relpermalink":"/tsb/operations/telemetry/telemetry-architecture/","summary":"注意 本页面详细介绍了如何收集 Tetrate Service Bridge 运营所需的遥测数据，而不是由 Tetrate Service Bridge 管理的应用程序。 Tetrate Service Bridge 使用 Open Telemetry Collector 来简化指标收集。标准部署包括管理平面中的一个 Collector，以及每个已接入的控制平面旁边都有一个","title":"遥测架构"},{"content":"本文档描述了如何在 AWS 单一 VPC 中安装 TSB。\n在开始之前，请确保你已经：\n熟悉 TSB 概念 安装 tctl 并 同步你的 tctl 镜像 安装 EKS CLI 安装 AWS CLI 使用单一 VPC 安装 TSB 在这种情况下，你将需要在你的 AWS 帐户中运行 3 个 EKS 集群，以及运行 Elasticsearch 和 Postgres。\n请按照相应的 AWS 指南进行更详细的设置：\n创建 EKS 集群 开始使用 Amazon OpenSearch 服务（CLI 参考） 创建 PostgreSQL DB 实例（CLI 参考） 首先，使用以下命令模板创建管理平面集群。 由于命令中没有明确定义 VPC，将为你创建一个新的 VPC。\n$ eksctl create cluster \\ --name \u0026lt;NAME\u0026gt; \\ --version \u0026lt;VERSION\u0026gt; \\ --region \u0026lt;REGION\u0026gt; \\ --nodegroup-name \u0026lt;POOLNAME\u0026gt; \\ --nodes \u0026lt;NUM\u0026gt; \\ --node-type \u0026lt;TYPE\u0026gt; \\ --managed 一旦管理平面集群、节 …","relpermalink":"/tsb/setup/aws/vpc/","summary":"本文档描述了如何在 AWS 单一 VPC 中安装 TSB。 在开始之前，请确保你已经： 熟悉 TSB 概念 安装 tctl 并 同步你的 tctl 镜像 安装 EKS CLI 安装 AWS CLI 使用单一 VPC 安装 TSB 在这种情况下，你将需要在你的 AWS 帐户中运行 3 个 EKS 集群，以及运行 Elasticsearch 和 Pos","title":"在 AWS 安装 TSB"},{"content":"本文将教你如何设置在虚拟机和 Kubernetes 集群上运行的服务之间的流量路由。\n在本指南中，你将：\n在集群中安装 Istio 演示bookinfo应用程序 在虚拟机上安装bookinfo应用程序的ratings服务 将流量在虚拟机和集群中的ratings应用程序之间进行80/20的分流 在开始之前，请确保你已经：\n安装了 TSB 管理平面 对一个集群进行了载入 安装了数据面 Operator 首先，从在你的集群中安装 bookinfo 开始。\nkubectl create ns bookinfo kubectl apply -f \\ https://raw.githubusercontent.com/istio/istio/master/samples/bookinfo/platform/kube/bookinfo.yaml \\ -n bookinfo 遵循VM 载入文档。在载入过程中，将 Istio 演示的ratings应用程序作为你的工作负载运行。\nsudo docker run -d \\ --name ratings \\ -p 127.0.0.1:9080:9080 \\ …","relpermalink":"/tsb/howto/traffic/splitting-service-traffic-between-k8s-vms/","summary":"本文将教你如何设置在虚拟机和 Kubernetes 集群上运行的服务之间的流量路由。 在本指南中，你将： 在集群中安装 Istio 演示bookinfo应用程序 在虚拟机上安装bookinfo应用程序的ratings服务 将流量在虚拟机和集群","title":"在 Kubernetes 和虚拟机之间切分流量"},{"content":"Tier1 网关 用于使用 Istio mTLS 在其他集群中跨一个或多个入口网关（或 Tier2 网关）分发流量。在 1.6 版本之前，Tier1 网关需要一个专用集群，并且不能与其他网关（例如入口网关）或应用工作负载一起使用。\n从 TSB 1.6 版本开始，你无需为运行 Tier1 网关而提供一个专用的集群。你可以在任何应用程序集群中部署 Tier1 网关。目前此功能默认处于禁用状态；在将来的版本中将默认启用。\n在应用集群中启用 Tier1 网关 为了在应用集群中部署 Tier1 网关，你首先需要编辑 ControlPlane CR 或 Helm 值中的 xcp 组件，并添加一个名为 DISABLE_TIER1_TIER2_SEPARATION 的环境变量，其值为 true。\nspec: components: xcp: ... kubeSpec: overlays: - apiVersion: install.xcp.tetrate.io/v1alpha1 kind: EdgeXcp name: edge-xcp patches: ... - path: …","relpermalink":"/tsb/operations/features/tier1-in-app-cluster/","summary":"Tier1 网关 用于使用 Istio mTLS 在其他集群中跨一个或多个入口网关（或 Tier2 网关）分发流量。在 1.6 版本之前，Tier1 网关需要一个专用集群，并且不能与其他网关（例如入口网关）或应用工作负载一起使用。 从 TSB 1.6 版本开始，你无需为","title":"在应用集群中启用 Tier1 网关"},{"content":" 注意\n自 1.7 版本以来，TSB 支持用于 TSB 管理平面 TLS 证书、内部证书和中间 Istio CA 证书的自动证书管理。详细信息请参阅 自动证书管理。 有 4 种 TSB 运算符需要了解的证书类型：\nTSB 内部证书：用于 TSB 内部组件相互信任的证书。 应用 TLS 证书：提供给应用程序用户的证书，用于 Web 浏览器或工具。 中间 Istio CA 证书：用于签发 Istio 工作负载叶子证书的中间 CA 证书。 工作负载叶子证书：针对每个代理和网关签发的证书。 下面的图片显示了这些证书及其与 TSB 组件和你的应用程序的关系。\nTSB 组件中的证书 TSB 内部证书 TSB 的全局控制平面 (XCP) 从管理平面分发配置到控制平面集群。XCP 由 XCP central 和 XCP edge 组成。XCP central 部署在管理平面，TSB 服务器通过名为 MPC 的组件与其交互。TSB 内部证书（图片中突出显示为绿色）用于保护 XCP central、XCP edge、MPC 组件之间的通信。TSB 使用带 TLS 的 JWT 来确保通信的安全性。 …","relpermalink":"/tsb/setup/certificate/certificate-setup/","summary":"注意 自 1.7 版本以来，TSB 支持用于 TSB 管理平面 TLS 证书、内部证书和中间 Istio CA 证书的自动证书管理。详细信息请参阅 自动证书管理。 有 4 种 TSB 运算符需要了解的证书类型： TSB 内部证书：用于 TSB 内部组件相互信任的证书。 应用 TLS 证","title":"证书类型"},{"content":"生产 Argo CD 支持多种不同的 Kubernetes 清单定义方式：\nKustomize应用程序 Helm Chart YAML/JSON/Jsonnet 清单的目录，包括Jsonnet。 任何配置为配置管理插件的自定义配置管理工具 开发 Argo CD 还支持直接上传本地清单。由于这是 GitOps 范式的反模式，因此只能出于开发目的而这样做。override需要具有权限的用户（通常是管理员）才能在本地上传清单。支持上述所有不同的 Kubernetes 部署工具。上传本地应用程序：\n$ argocd app sync APPNAME --local /path/to/dir/ ","relpermalink":"/argo-cd/user-guide/application-sources/","summary":"生产 Argo CD 支持多种不同的 Kubernetes 清单定义方式： Kustomize应用程序 Helm Chart YAML/JSON/Jsonnet 清单的目录，包括Jsonnet。 任何配置为配置管理插件的自定义配置管理工具 开发 Argo CD 还支持直接上传本地清单。由于这是 GitOps 范式的反模式，","title":"工具"},{"content":" Argo CD 架构 组件 API 服务器 API 服务器是一个 gRPC/REST 服务器，用于公开 Web UI、CLI 和 CI/CD 系统使用的 API。它具有以下职责：\n应用程序管理和状态报告 调用应用程序操作（例如同步、回滚、用户定义的操作） 存储为 K8s 机密的存储库和集群凭据管理 身份验证和身份验证委派到外部身份提供者 RBAC 执行 Git webhook 事件的侦听器/转发器 存储库服务器 存储库服务器是一个内部服务，它维护 Git 存储库的本地缓存，其中包含应用程序清单。它负责在提供以下输入时生成并返回 Kubernetes 清单：\n存储库 URL 修订版（提交、标记、分支） 应用程序路径 模板特定设置：参数、helm values.yaml 应用程序控制器 应用程序控制器是一个 Kubernetes 控制器，它不断监视运行中的应用程序，并将当前的实时状态与期望的目标状态（如 repo 中指定的）进行比较。它检测 OutOfSync 应用程序状态，并可选择采取纠正措施。它负责调用任何用户定义的生命周期事件钩子（PreSync、Sync、PostSync）\n","relpermalink":"/argo-cd/operator-manual/architecture/","summary":"Argo CD 架构 组件 API 服务器 API 服务器是一个 gRPC/REST 服务器，用于公开 Web UI、CLI 和 CI/CD 系统使用的 API。它具有以下职责： 应用程序管理和状态报告 调用应用程序操作（例如同步、回滚、用户定义的操作） 存储为 K8s 机密的存储库和集","title":"架构概述"},{"content":"什么是 Argo CD？ Argo CD 是一个基于声明式 GitOps 的 Kubernetes 应用程序交付工具。\nArgo CD UI 为什么选择 Argo CD？ 应用程序定义、配置和环境应该是声明式的，并进行版本控制。应用程序部署和生命周期管理应该是自动化的、可审计的和易于理解的。\n入门指南 快速入门 kubectl create namespace argocd kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml 请参阅我们的入门指南。我们还为其他功能提供了面向用户的文档。如果你想升级 ArgoCD，请参阅升级指南。我们还为有兴趣构建第三方集成的开发人员提供面向开发者的文档。\n工作原理 Argo CD 遵循使用 Git 存储库作为定义期望应用程序状态的真实来源的 GitOps 模式。Kubernetes 清单可以通过以下几种方式指定：\nkustomize 应用程序 helm chart jsonnet …","relpermalink":"/argo-cd/overview/","summary":"什么是 Argo CD？ Argo CD 是一个基于声明式 GitOps 的 Kubernetes 应用程序交付工具。 Argo CD UI 为什么选择 Argo CD？ 应用程序定义、配置和环境应该是声明式的，并进行版本控制。应用程序部署和生命周期管理应该是自动化的、可审计的和易于理解的","title":"Argo CD 简介"},{"content":"介绍 应用程序集控制器是一个Kubernetes 控制器，它添加了对ApplicationSet自定义资源定义 (CRD) 的支持。这个控制器/CRD 使得自动化和更大的灵活性在管理 Argo CD 应用程序跨大量的集群和在 monorepos 内成为可能，同时它使多租户 Kubernetes 集群上的自助式使用成为可能。\n应用程序集控制器与现有的 Argo CD 安装一起工作。Argo CD 是一个声明性的、GitOps 持续交付工具，允许开发人员从他们现有的 Git 工作流程中定义和控制 Kubernetes 应用程序资源的部署。\n从 Argo CD v2.3 开始，应用程序集控制器与 Argo CD 捆绑在一起。\n应用程序集控制器通过添加支持面向集群管理员的附加功能来补充 Argo CD。ApplicationSet控制器提供：\n使用单个 Kubernetes 清单定位多个 Kubernetes 集群的能力，使用 Argo CD 部署多个应用程序的能力 使用单个 Kubernetes 清单从一个或多个 Git 存储库中部署多个应用程序的能力 改进了对 monorepos 的支持： …","relpermalink":"/argo-cd/operator-manual/applicationset/overview/","summary":"介绍 应用程序集控制器是一个Kubernetes 控制器，它添加了对ApplicationSet自定义资源定义 (CRD) 的支持。这个控制器/CRD 使得自动化和更大的灵活性在管理 Argo CD 应用程序跨大量的集群和在 monorepos 内成为","title":"ApplicationSet 控制器介绍"},{"content":"Argo Rollouts 提供了多种形式的分析方法来驱动渐进式交付。本文档描述了如何实现不同形式的渐进式交付，包括分析执行的时间点、频率和发生次数。\n自定义资源定义 CRD 描述 Rollout Rollout 作为 Deployment 资源的替代品，提供了额外的蓝绿和金丝雀更新策略。这些策略可以在更新过程中创建 AnalysisRuns 和 Experiments，这些 AnalysisRuns 和 Experiments 可以推进更新，或者中止更新。 AnalysisTemplate AnalysisTemplate 是一个模板规范，定义了如何执行金丝雀分析，例如应该执行的指标、其频率以及被视为成功或失败的值。AnalysisTemplates 可以使用输入值进行参数化。 ClusterAnalysisTemplate ClusterAnalysisTemplate 类似于 AnalysisTemplate，但它不限于其命名空间。它可以被任何 Rollout 在整个集群中使用。 AnalysisRun AnalysisRun 是 AnalysisTemplate 的一个实例 …","relpermalink":"/argo-rollouts/analysis/overview/","summary":"Argo Rollouts 提供了多种形式的分析方法来驱动渐进式交付。本文档描述了如何实现不同形式的渐进式交付，包括分析执行的时间点、频率和发生次数。 自定义资源定义 CRD 描述 Rollout Rollout 作为 Deployment 资源的替代品，提供了额外的蓝绿和金丝雀更新策","title":"分析和渐进式交付"},{"content":"Kubectl 插件是一种扩展 kubectl 命令提供额外行为的方式。通常，它们用于添加新功能到 kubectl 并自动化可脚本化的工作流程来操作集群。官方文档可在 此处 找到。\nArgo Rollouts 提供了一个 Kubectl 插件来丰富 Rollouts、Experiments 和 Analysis 的体验。它提供了可视化 Argo Rollouts 资源的能力并从命令行上运行常规操作，例如 promote 或 retry。\n安装 请参阅 安装指南 了解安装插件的说明。\n用法 获取有关可用的 Argo Rollouts kubectl 插件命令的信息的最佳方法是运行 kubectl argo rollouts。插件列出了该工具可以执行的所有可用命令以及每个命令的描述。所有插件命令与 Kubernetes API 服务器交互，并使用 KubeConfig 凭据进行身份验证。由于插件利用运行命令的用户的 KubeConfig，因此插件具有这些配置的权限。\n与 kubectl 类似，该插件使用许多与 kubectl 相同的标志。例如，kubectl argo rollouts …","relpermalink":"/argo-rollouts/kubectl-plugin/overview/","summary":"Kubectl 插件是一种扩展 kubectl 命令提供额外行为的方式。通常，它们用于添加新功能到 kubectl 并自动化可脚本化的工作流程来操作集群。官方文档可在 此处 找到。 Argo Rollouts 提供了一个 Kubectl 插件来丰富 Rollouts、Experiments 和 Analysis","title":"Kubectl 插件"},{"content":"🔔 重要提示：自版本 1.1 起可用。\nArgo Rollouts 提供通知功能，由Notifications Engine支持。控制器管理员可以利用灵活的触发器和模板系统来配置终端用户请求的通知。终端用户可以通过在 Rollout 对象中添加注释来订阅配置的触发器。\n配置 触发器定义了通知应该在何时发送以及通知内容模板。默认情况下，Argo Rollouts 附带了一系列内置触发器，涵盖了 Argo Rollout 生命周期的最重要事件。触发器和模板都在argo-rollouts-notification-configmap ConfigMap 中配置。为了快速入门，你可以使用在notifications-install.yaml中定义的预配置通知模板。\n如果你正在利用 Kustomize，则建议将notifications-install.yaml作为远程资源包含在你的kustomization.yaml文件中：\napiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - …","relpermalink":"/argo-rollouts/notifications/overview/","summary":"🔔 重要提示：自版本 1.1 起可用。 Argo Rollouts 提供通知功能，由Notifications Engine支持。控制器管理员可以利用灵活的触发器和模板系统来配置终端用户请求的通知。终端用户可以通过在 Rollout 对象中添加注释来订阅","title":"通知"},{"content":"Argo Rollouts 是一个 Kubernetes 自定义资源定义 (Custom Resource Definition, CRD)，扩展了 Kubernetes Deployment Controller，它添加了渐进式交付（Progressive Delivery）和蓝绿部署（Blue Green Deployment）等交付策略。Argo Rollouts 可以与 Istio、Linkerd 等服务网格和其他流量管理工具集成。\n流量管理 流量管理是通过控制数据平面，为应用程序创建智能的路由规则。这些路由规则可以操作流量，将其引导到应用程序的不同版本，从而实现渐进式交付。这些控制规则通过确保只有一小部分用户接收新版本，从而限制了新版本的波及范围。\n实现流量管理有各种技术：\n原始百分比（例如，5% 的流量应该流向新版本，而其余的流向稳定版本） 基于头的路由（例如，将带有特定标头的请求发送到新版本） 交叉流量，其中所有流量都被复制并并行发送到新版本（但响应被忽略） Kubernetes 中的流量管理工具 核心 Kubernetes 对象没有细粒度的工具来满足所有流量管理要求。在 …","relpermalink":"/argo-rollouts/traffic-management/overview/","summary":"Argo Rollouts 是一个 Kubernetes 自定义资源定义 (Custom Resource Definition, CRD)，扩展了 Kubernetes Deployment Controller，它添加了渐进式交付（Progressive Delivery）和蓝绿部署（Blue Green Deployment）等交付策略。Argo Rollouts","title":"流量管理概览"},{"content":"本指南通过演示部署、升级、推广和终止 Rollout 来演示 Argo Rollouts 的各种概念和特性。\n要求 安装了 argo-rollouts 控制器的 Kubernetes 集群（请参阅 安装指南） 安装了带有 argo-rollouts 插件的 kubectl（请参阅 安装指南） 1. 部署 Rollout 首先，我们部署一个 Rollout 资源和一个针对该 Rollout 的 Kubernetes Service。本指南中的示例 Rollout 利用了金丝雀升级策略，该策略将 20％的流量发送到金丝雀，然后进行手动推广，最后对其余升级进行逐渐自动化的流量增加。此行为在 Rollout spec 的以下部分中描述：\nspec: replicas: 5 strategy: canary: steps: - setWeight: 20 - pause: {} - setWeight: 40 - pause: {duration: 10} - setWeight: 60 - pause: {duration: 10} - setWeight: 80 - pause: …","relpermalink":"/argo-rollouts/getting-started/basic-usage/","summary":"本指南通过演示部署、升级、推广和终止 Rollout 来演示 Argo Rollouts 的各种概念和特性。 要求 安装了 argo-rollouts 控制器的 Kubernetes 集群（请参阅 安装指南） 安装了带有 argo-rollouts 插件的 kubectl（请参阅 安装指南） 1. 部署 Rollout 首先，我们部署一个 Rollout 资源和一个针","title":"基础使用"},{"content":"什么是 Argo Rollouts？ Argo Rollouts 是一组 Kubernetes 控制器和自定义资源（CRD），为 Kubernetes 提供高级部署功能，例如蓝绿、金丝雀、金丝雀分析、实验和渐进式交付等功能。\nArgo Rollouts 可选地与 Ingress 控制器和服务网格集成，利用它们的流量整形能力在更新期间逐渐将流量转移到新版本。此外，Rollouts 可以查询和解释来自各种提供商的度量标准，以验证关键 KPI 并在更新期间驱动自动升级或回滚。\n这是一个演示视频（点击在 Youtube 上观看）：\n为什么选择 Argo Rollouts？ Kubernetes Deployment 对象支持滚动更新策略，该策略提供了一组基本的安全性保证（就绪探针）来保证更新期间的安全性。但是，滚动更新策略面临许多限制：\n对滚动更新速度的控制很少 无法控制流量流向新版本 就绪探针不适用于更深入的、压力或一次性检查 没有查询外部度量标准以验证更新的能力 可以停止进程，但无法自动中止并回滚更新 因此，在大型高容量生产环境中，滚动更新往往被认为是过于冒险的更新过程，因为它无法控制爆炸 …","relpermalink":"/argo-rollouts/overview/","summary":"什么是 Argo Rollouts？ Argo Rollouts 是一组 Kubernetes 控制器和自定义资源（CRD），为 Kubernetes 提供高级部署功能，例如蓝绿、金丝雀、金丝雀分析、实验和渐进式交付等功能。 Argo Rollouts 可选地与 Ingress 控制器和服务网格集成，利用它们的流量整形能","title":"Argo Rollouts 简介"},{"content":"蓝绿部署允许用户减少同时运行多个版本的时间。\n概述 除了管理 ReplicaSet 外，在 BlueGreenUpdate 策略期间，Rollout 控制器还将修改 Service 资源。Rollout 规范要求用户在同一命名空间中指定对活动服务的引用以及可选的预览服务。活动服务用于将常规应用程序流量发送到旧版本，而预览服务用于将流量漏斗到新版本。Rollout 控制器通过向这些服务的选择器注入 ReplicaSet 的唯一哈希来确保正确的流量路由。这允许 Rollout 定义一个活动和预览堆栈以及从预览到活动的过程。\n当 Rollout 的 .spec.template 字段发生更改时，控制器将创建新的 ReplicaSet。如果活动服务没有将流量发送到 ReplicaSet，则控制器将立即开始将流量发送到 ReplicaSet。否则，活动服务将指向旧 ReplicaSet，而 ReplicaSet 变得可用。一旦新的 ReplicaSet 变得可用，控制器将修改活动服务以指向新的 ReplicaSet。 …","relpermalink":"/argo-rollouts/rollout/deployment-strategies/bluegreen/","summary":"蓝绿部署允许用户减少同时运行多个版本的时间。 概述 除了管理 ReplicaSet 外，在 BlueGreenUpdate 策略期间，Rollout 控制器还将修改 Service 资源。Rollout 规范要求用户在同一命名空间中指定对活动服务的引用以及可选的预览服务。活动服","title":"蓝绿部署策略"},{"content":"本书介绍了服务身份的 SPIFFE 标准，以及 SPIFFE 的参考实现 SPIRE。这些项目为现代异构基础设施提供了一个统一的身份控制平面。这两个项目都是开源的，隶属于云原生计算基金会（CNCF）。\n随着企业发展他们的应用架构以充分利用新的基础设施技术，他们的安全模式也必须不断发展。软件已经从一个单片机上的单个应用发展到了几十甚至几百个紧密联系的微服务，这些微服务可能分布在公共云或私人数据中心的数千个虚拟机上。在这个新的基础设施世界里，SPIFFE 和 SPIRE 帮助保障系统的安全。\n本书努力提炼了 SPIFFE 和 SPIRE 的最重要的专家经验，以提供对身份问题的深刻理解，并协助你解决这些问题。通过这些项目，开发和运维可以利用新的基础设施技术构建软件，同时让安全团队摆脱昂贵和耗时的人工安全流程。\n关于零号乌龟 访问控制、秘密管理和身份是相互依赖的。大规模地管理秘密需要有效的访问控制；实施访问控制需要身份；证明身份需要拥有一个秘密。保护一个秘密需要想出一些办法来保护另一个秘密，这就需要保护那个秘密，以此类推。\n这让人想起一个著名的轶事：一个女人打断了一位哲学家的讲座，告诉他世界是 …","relpermalink":"/spiffe/preface/","summary":"本书介绍了服务身份的 SPIFFE 标准，以及 SPIFFE 的参考实现 SPIRE。这些项目为现代异构基础设施提供了一个统一的身份控制平面。这两个项目都是开源的，隶属于云原生计算基金会（CNCF）。 随着企业发展他们的应用架构以充","title":"关于本书"},{"content":"Linux 内核在网络堆栈中支持一组 BPF 钩子（hook），可用于运行 BPF 程序。Cilium 数据路径使用这些钩子来加载 BPF 程序，这些程序一起使用时会创建更高级别的网络结构。\n以下是 Cilium 使用的钩子列表和简要说明。有关每个钩子细节的更详尽的文档，请参阅 BPF 和 XDP 参考指南。\nXDP：XDP BPF 钩子位于网络驱动程序中的最早可能点，并在数据包接收时触发 BPF 程序的运行。这实现了可能的最佳数据包处理性能，因为程序在任何其他处理发生之前直接在数据包数据上运行。此钩子非常适合运行丢弃恶意或意外流量的过滤程序以及其他常见的 DDOS 保护机制。\n流量控制入口/出口：附加到流量控制（traffic control，简称 TC）入口钩子的 BPF 程序附加到网络接口，与 XDP 相同，但将在网络堆栈完成数据包的初始处理后运行。该钩子在三层网络之前运行，但可以访问与数据包关联的大部分元数据。这非常适合进行本地节点处理，例如应用三层/四层端点策略并将流量重定向到端点。对于面向网络的设备，TC 入口钩子可以与上面的 XDP 钩子耦合。完成此操作后，可以合理地假设 …","relpermalink":"/cilium-handbook/ebpf/intro/","summary":"Linux 内核在网络堆栈中支持一组 BPF 钩子（hook），可用于运行 BPF 程序。Cilium 数据路径使用这些钩子来加载 BPF 程序，这些程序一起使用时会创建更高级别的网络结构。 以下是 Cilium 使用的钩子列表和简要说明。有关每个钩子","title":"eBPF 数据路径介绍"},{"content":"Cilium 能为 Kubernetes 集群提供什么？ 在 Kubernetes 集群中运行 Cilium 时提供以下功能：\nCNI 插件支持，为 pod 连接 提供 联网。 NetworkPolicy 资源的基于身份的实现，用于隔离三层和四层网络 pod 的连接。 以 CustomResourceDefinition 形式对 NetworkPolicy 的扩展，扩展策略控制以添加： 针对以下应用协议的入口和出口执行七层策略： HTTP Kafka 对 CIDR 的出口支持以保护对外部服务的访问 强制外部无头服务自动限制为服务配置的 Kubernetes 端点集 ClusterIP 实现为 pod 到 pod 的流量提供分布式负载平衡 完全兼容现有的 kube-proxy 模型 Pod 间连接 在 Kubernetes 中，容器部署在称为 pod 的单元中，其中包括一个或多个可通过单个 IP 地址访问的容器。使用 Cilium，每个 pod 从运行 pod 的 Linux 节点的节点前缀中获取一个 IP 地址。有关其他详细信息，请参阅 IP 地址管理（IPAM）。在没有任何网络安全策 …","relpermalink":"/cilium-handbook/kubernetes/intro/","summary":"Cilium 能为 Kubernetes 集群提供什么？ 在 Kubernetes 集群中运行 Cilium 时提供以下功能： CNI 插件支持，为 pod 连接 提供 联网。 NetworkPolicy 资源的基于身份的实现，用于隔离三层和四层网络 pod 的连接。 以 CustomResourceDefinition 形式对 NetworkPolicy 的扩展，扩展策略控制以添加： 针对以下应用协议的入","title":"Kubernetes 集成介绍"},{"content":"Cilium 在多个层面上提供安全性。可以单独使用或组合使用。\n基于身份：端点之间的连接策略（三层），例如任何带有标签的端点 role=frontend 都可以连接到任何带有标签的端点 role=backend。 限制传入和传出连接的可访问端口（四层），例如带标签的端点 role=frontend只能在端口 443（https）上进行传出连接，端点role=backend 只能接受端口 443（https）上的连接。 应用程序协议级别的细粒度访问控制，以保护 HTTP 和远程过程调用（RPC）协议，例如带有标签的端点 role=frontend 只能执行 REST API 调用 GET /userdata/[0-9]+，所有其他与 role=backend API 的交互都受到限制。 ","relpermalink":"/cilium-handbook/security/intro/","summary":"Cilium 在多个层面上提供安全性。可以单独使用或组合使用。 基于身份：端点之间的连接策略（三层），例如任何带有标签的端点 role=frontend 都可以连接到任何带有标签的端点 role=backend。 限制传入和传出连接的可访问端口（","title":"介绍"},{"content":"封装 当没有提供配置时，Cilium 会自动在此模式下运行，因为它是对底层网络基础设施要求最低的模式。\n在这种模式下，所有集群节点使用基于 UDP 的封装协议 VXLAN 或 Geneve。Cilium 节点之间的所有流量都被封装。\n对网络的要求 封装依赖于正常的节点到节点的连接。这意味着如果 Cilium 节点已经可以互相到达，那么所有的路由要求都已经满足了。\n底层网络和防火墙必须允许封装数据包：\n封装方式 端口范围 / 协议 VXLAN（默认） 8472/UDP Geneve 6081/UDP 封装模式的优点 封装模式具有以下优点：\n简单\n连接集群节点的网络不需要知道 PodCIDR。集群节点可以产生多个路由或链路层域。只要集群节点可以使用 IP/UDP 相互访问，底层网络的拓扑就无关紧要。\n寻址空间\n由于不依赖于任何底层网络限制，如果相应地配置了 PodCIDR 大小，可用的寻址空间可能会更大，并且允许每个节点运行任意数量的 Pod。\n自动配置\n当与 Kubernetes 等编排系统一起运行时，集群中所有节点的列表（包括它们关联的分配前缀节点）会自动提供给每个代理。加入集群的新节 …","relpermalink":"/cilium-handbook/networking/routing/","summary":"封装 当没有提供配置时，Cilium 会自动在此模式下运行，因为它是对底层网络基础设施要求最低的模式。 在这种模式下，所有集群节点使用基于 UDP 的封装协议 VXLAN 或 Geneve。Cilium 节点之间的所有流量都被封装","title":"路由"},{"content":"本文将为你简要介绍 Cilium 和 Hubble。\n什么是 Cilium？ Cilium 是开源软件，用于透明地保护使用 Docker 和 Kubernetes 等 Linux 容器管理平台部署的应用服务之间的网络连接。\nCilium 的基础是一种新的 Linux 内核技术，称为 eBPF，它使强大的安全可视性和控制逻辑动态插入 Linux 本身。由于 eBPF 在 Linux 内核内运行，Cilium 安全策略的应用和更新无需对应用程序代码或容器配置进行任何改动。\n什么是 Hubble？ Hubble 是一个完全分布式的网络和安全可观测性平台。它建立在 Cilium 和 eBPF 之上，以完全透明的方式实现对服务的通信行为以及网络基础设施的深度可视性。\n通过建立在 Cilium 之上，Hubble 可以利用 eBPF 实现可视性。依靠 eBPF，所有的可视性都是可编程的，并允许采用一种动态的方法，最大限度地减少开销，同时按照用户的要求提供深入和详细的可视性。Hubble 的创建和专门设计是为了最好地利用这些新的 eBPF 力量。\nHubble 可以回答诸如以下问题。\n服务依赖和拓扑 …","relpermalink":"/cilium-handbook/intro/","summary":"本文将为你简要介绍 Cilium 和 Hubble。 什么是 Cilium？ Cilium 是开源软件，用于透明地保护使用 Docker 和 Kubernetes 等 Linux 容器管理平台部署的应用服务之间的网络连接。 Cilium 的基础是一种新的 Linux 内核技术，称为 eBPF，它使强大的安全可","title":"Cilium 和 Hubble 简介"},{"content":"Cilium 代理（agent）和 Cilium 网络策略的配置决定了一个端点（Endpoint）是否接受来自某个来源的流量。代理可以进入以下三种策略执行模式：\ndefault\n如果任何规则选择了一个 Endpoint 并且该规则有一个入口部分，那么该端点就会在入口处进入默认拒绝状态。如果任何规则选择了一个 Endpoint 并且该规则有一个出口部分，那么该端点就会在出口处进入默认拒绝状态。这意味着端点开始时没有任何限制，一旦有规则限制其在入口处接收流量或在出口处传输流量的能力，那么端点就会进入白名单模式，所有流量都必须明确允许。\nalways\n在 always 模式下，即使没有规则选择特定的端点，也会在所有端点上启用策略执行。如果你想配置健康实体，在启动 cilium-agent 时用 enable-policy=always 检查整个集群的连接性，你很可能想启用与健康端点的通信。\nnever\n在“never\u0026#34; 模式下，即使规则选择了特定的端点，所有端点上的策略执行也被禁用。换句话说，所有流量都允许来自任何来源（入口处）或目的地（出口处）。\n要在运行时为 Cilium 代理管理的所有 …","relpermalink":"/cilium-handbook/policy/intro/","summary":"Cilium 代理（agent）和 Cilium 网络策略的配置决定了一个端点（Endpoint）是否接受来自某个来源的流量。代理可以进入以下三种策略执行模式： default 如果任何规则选择了一个 Endpoint 并且该规则有一个入口部分，那么该端点就会","title":"网络策略模式"},{"content":"本文将为你介绍 Cilium 和 Hubble 部署中包含的组件。\nCilium 包含以下组件：\n代理 客户端 Operator CNI 插件 Hubble 包含以下组件：\n服务器 中继器 客户端 图形用户界面 eBPF 另外你还需要一个数据库来存储代理的状态。\n下图展示的 Cilium 部署的组件。\nCilium 组件示意图 Cilium 代理\nCilium 代理（cilium-agent）在集群的每个节点上运行。在高层次上，代理接受通过 Kubernetes 或 API 的配置，描述网络、服务负载均衡、网络策略、可视性和监控要求。\nCilium 代理监听来自编排系统（如 Kubernetes）的事件，以了解容器或工作负载的启动和停止时间。它管理 eBPF 程序，Linux 内核用它来控制这些容器的所有网络访问。\n客户端（CLI）\nCilium CLI 客户端（cilium）是一个命令行工具，与 Cilium 代理一起安装。它与运行在同一节点上的 Cilium 代理的 REST API 互动。CLI 允许检查本地代理的状态。它还提供工具，直接访问 eBPF map 以验证其状态。 …","relpermalink":"/cilium-handbook/concepts/overview/","summary":"本文将为你介绍 Cilium 和 Hubble 部署中包含的组件。 Cilium 包含以下组件： 代理 客户端 Operator CNI 插件 Hubble 包含以下组件： 服务器 中继器 客户端 图形用户界面 eBPF 另外你还需要一个数据库来存储代理的状态。 下图展示的 Cilium 部署的组件。 Cilium 组件示意图 Cilium 代","title":"组件概览"},{"content":"在过去的几年里，eBPF 已经从相对默默无闻变成了现代基础设施建设中最热门的技术领域之一。就我个人而言，自从看到 Thomas Graf 在 DockerCon 17 的黑带会议（Black Blet）1 上谈到 eBPF 时，我就对它的可能性感到兴奋。在云原生计算基金会（CNCF），我在技术监督委员会（TOC）的同事把 eBPF 作为我们预测 2021 年将会起飞的重点技术之一来关注。超过 2500 人报名参加了当年的 eBPF 峰会线上会议，世界上最先进的几家软件工程公司共同创建了 eBPF 基金会。显然，人们对这项技术有很大的兴趣。\n在这个简短的报告中，我希望能给你一些启示，为什么人们对 eBPF 如此兴奋，以及它在现代计算环境中提供的工具能力。你会了解到 eBPF 是什么以及为什么它如此强大。还有一些代码实例，以使这种感觉更加具象化（但如果你愿意，你可以跳过这些）。\n你将了解到在建立支持 eBPF 的工具时涉及的内容，以及为什么 eBPF 在如此短的时间内变得如普遍。\n在这份简短的报告中，难免无法了解所有的细节，但如果你想更深入地了解，我将给你一些参考信息。\n扩展的伯克利数据包 …","relpermalink":"/what-is-ebpf/introduction/","summary":"在过去的几年里，eBPF 已经从相对默默无闻变成了现代基础设施建设中最热门的技术领域之一。就我个人而言，自从看到 Thomas Graf 在 DockerCon 17 的黑带会议（Black Blet）1 上谈到 eBPF 时，我就对它的可能性感到兴奋。在云原生","title":"第一章：eBPF 简介"},{"content":"Kubernetes 是云原生时代的 POSIX 在单机时代，POSIX 是类 UNIX 系统的通用 API，而在云原生时代，Kubernetes 是云操作系统的的 POSIX，它定义了基于云的分布式系统的 API。下表将 Kubernetes 与 POSIX 进行了对比。\n对比项 Linux Kubernetes 隔离单元 进程 Pod 硬件 单机 数据中心 并发 线程 容器 资源管理 进程内存\u0026amp;CPU 内存、CPU Limit/Request 存储 文件 ConfigMap、Secret、Volume 网络 端口绑定 Service 终端 tty、pty、shell kubectl exec 网络安全 IPtables NetworkPolicy 权限 用户、文件权限 ServiceAccount、RBAC 注意\n我们不能说 Linux 就是 POSIX，只能说 Linux 是 UNIX 兼容的。 参考 Kubernetes is the POSIX of the cloud - home.robusta.dev ","relpermalink":"/cloud-native-handbook/kubernetes/what-is-kubernetes/","summary":"Kubernetes 是云原生时代的 POSIX 在单机时代，POSIX 是类 UNIX 系统的通用 API，而在云原生时代，Kubernetes 是云操作系统的的 POSIX，它定义了基于云的分布式系统的 API。下表将 Kubernetes 与 POSIX 进行了对比。 对比项 Linux Kubernetes 隔","title":"什么是 Kubernetes?"},{"content":"首先我们来阐述下将应用迁移到云原生架构的动机。\n速度 天下武功，唯快不破，市场竞争亦是如此。想象一下，能够快速创新、实验并交付软件的企业，与使用传统软件交付模式的企业，谁将在市场竞争中胜出呢？\n在传统企业中，为应用提供环境和部署新版本花费的时间通常以天、周或月来计算。这种速度严重限制了每个发行版可以承担的风险，因为修复这些错误往往跟发行一个新版本有差不多的耗时。\n互联网公司经常提到它们每天几百次发布的实践。为什么频繁发布如此重要？如果你可以每天实现几百次发布，你们就可以几乎立即从错误的版本恢复过来。如果你可以立即从错误中恢复过来，你就能够承受更多的风险。如果你可以承受更多的风险，你就可以做更疯狂的试验 —— 这些试验结果可能会成为你接下来的竞争优势。\n基于云基础设置的弹性和自服务的特性天生就适应于这种工作方式。通过调用云服务 API 来提供新的应用程序环境比基于表单的手动过程要快几个数量级。然后通过另一个 API 调用将代码部署到新的环境中。将自服务和 hook 添加到团队的 CI/CD 服务器环境中进一步加快了速度。现在，我们可以回答精益大师 Mary Poppendick 提出的问 …","relpermalink":"/migrating-to-cloud-native-application-architectures/the-rise-of-cloud-native/why-cloud-native-application-architectures/","summary":"首先我们来阐述下将应用迁移到云原生架构的动机。 速度 天下武功，唯快不破，市场竞争亦是如此。想象一下，能够快速创新、实验并交付软件的企业，与使用传统软件交付模式的企业，谁将在市场竞争中胜出呢？ 在传统企业中","title":"1.1 为何使用云原生应用架构"},{"content":"由于微服务通常是以容器的形式实现的，因此容器编排和资源管理平台被用于服务的部署、运维和维护。\n一个典型的协调和资源管理平台由各种逻辑（形成抽象层）和物理工件组成，用于部署容器。例如，在 Kubernetes 中，容器在最小的部署单元内运行，称为 Pod。一个 Pod 理论上可以承载一组容器，但通常情况下，一个 Pod 内只运行一个容器。一组 Pod 被定义在所谓的节点内，节点可以是物理机或虚拟机（VM）。一组节点构成了一个集群。通常情况下，需要单个微服务的多个实例来分配工作负载，以达到预期的性能水平。集群是一个资源池（节点），用于分配微服务的工作负载。使用的技术之一是横向扩展，即访问频率较高的微服务被分配更多的实例或分配到具有更多资源（如 CPU 和 / 或内存）的节点。\n","relpermalink":"/service-mesh-devsecops/reference-platform/container-orchestration-and-resource-management-platform/","summary":"由于微服务通常是以容器的形式实现的，因此容器编排和资源管理平台被用于服务的部署、运维和维护。 一个典型的协调和资源管理平台由各种逻辑（形成抽象层）和物理工件组成，用于部署容器。例如，在 Kubernetes 中，容器在最小的","title":"2.1 容器编排和资源管理平台"},{"content":"企业 IT 采用云原生架构所需的变革根本不是技术性的，而是企业文化和组织的变革，围绕消除造成浪费的结构、流程和活动。在本节中，我们将研究必要的文化转变。\n从信息孤岛到 DevOps 企业 IT 通常被组织成以下许多孤岛：\n软件开发 质量保证 数据库管理 系统管理 IT 运营 发布管理 项目管理 创建这些孤岛是为了让那些了解特定领域的人员来管理和指导那些执行该专业领域工作的人员。这些孤岛通常具有不同的管理层次，工具集、沟通风格、词汇表和激励结构。这些差异启发了企业 IT 目标的不同范式，以及如何实现这一目标。\n但这里面存在很多矛盾，例如开发和运维分别对软件变更持有的观念就是个经常被提起的例子。开发的任务通常被视为通过开发软件功能为组织提供额外的价值。这些功能本身就是向 IT 生态系统引入变更。所以开发的使命可以被描述为“交付变化”，而且经常根据有多少次变更来进行激励。\n相反，IT 运营的使命可以被描述为“防止变更”。IT 运营通常负责维护 IT 系统所需的可用性、弹性、性能和耐用性。因此，他们经常以维持关键绩效指标（KPI）来进行激励，例如平均故障间隔时间（MTBF）和平均恢复时 …","relpermalink":"/migrating-to-cloud-native-application-architectures/changes-needed/cultural-change/","summary":"企业 IT 采用云原生架构所需的变革根本不是技术性的，而是企业文化和组织的变革，围绕消除造成浪费的结构、流程和活动。在本节中，我们将研究必要的文化转变。 从信息孤岛到 DevOps 企业 IT 通常被组织成以下许多孤岛： 软件开发","title":"2.1 文化变革"},{"content":"在和客户讨论分解数据、服务和团队后，客户经常向我提出这样的问题，“太棒了！但是我们要怎样实现呢？”这是个好问题。如何拆分已有的单体应用并把他们迁移上云呢？\n事实证明，我已经看到了很多成功的例子，使用增量迁移这种相当可复制的模式，我现在向我所有的客户推荐这种模式。SoundCloud 和 Karma 就是公开的例子。\n本节中，我们将讲解如何一步步地将单体服务分解并将它们迁移到云上。\n新功能使用微服务形式 您可能感到很惊奇，第一步不是分解单体应用。我们假设您依然要在单体应用中构建服务。事实上，如果您没有任何新的功能来构建，那么您甚至不应该考虑这个分解。（鉴于我们的主要动机是速度，您如何维持原状还能获取速度呢？）\n团队决定，处理架构变化的最佳方法不是立即分解 Mothership 架构，而是不添加任何新的东西。我们所有的新功能以微服务形式构建…\n——Phil Calcado, SoundCloud\n所以不要继续再向单体应用中增加代码，将所有的新功能以微服务的形式构建。这是第一步就要考虑好的，因为从头开始构架一个服务比分解一个单体应用并提出服务出来容易和快速的多。\n然而有一点不可避免，就是新构 …","relpermalink":"/migrating-to-cloud-native-application-architectures/migration-cookbook/decomposition-recipes/","summary":"在和客户讨论分解数据、服务和团队后，客户经常向我提出这样的问题，“太棒了！但是我们要怎样实现呢？”这是个好问题。如何拆分已有的单体应用并把他们迁移上云呢？ 事实证明，我已经看到了很多成功的例子，使用增量","title":"3.1 分解架构"},{"content":"DevSecOps 是一种软件开发、部署和生命周期管理方法，它涉及到从整个应用程序或平台的一次大型发布转变为持续集成、持续交付和持续部署（CI/CD）方法。这种转变又要求公司的 IT 部门的结构和工作流程发生变化。最明显的变化是组织一个 DevSecOps 小组，由软件开发人员、安全专家和 IT 运维专家组成，负责应用程序（即微服务）的每一部分。这个较小的团队不仅能促进最初的敏捷开发和部署的效率和效果，还能促进后续的生命周期管理活动，如监控应用行为、开发补丁、修复错误或扩展应用。这种具有三个领域专业知识的跨职能团队的组成，构成了在组织中引入 DevSecOps 的关键成功因素。\n","relpermalink":"/service-mesh-devsecops/devsecops/organizational-preparedness-for-devsecops/","summary":"DevSecOps 是一种软件开发、部署和生命周期管理方法，它涉及到从整个应用程序或平台的一次大型发布转变为持续集成、持续交付和持续部署（CI/CD）方法。这种转变又要求公司的 IT 部门的结构和工作流程发生变化。最明显的变","title":"3.1 组织对 DevSecOps 的准备情况"},{"content":"对上述五类代码（即应用、应用服务、基础设施、策略和监控）的简要描述如下：\n应用程序代码和应用服务代码：前者包含一组特定业务事务的数据和应用逻辑，而后者包含所有服务的代码，如网络连接、负载均衡和网络弹性。 基础设施即代码（IaC）：用于提供和配置基础设施资源的代码，它以可重复和一致的方式承载 应用程序的部署。这种代码是用一种声明性语言编写的，当执行时，为正在部署的应用程序提供和配置基础设施。这种类型的代码就像在应用程序的微服务中发现的任何其他代码，只是它提供的是基础设施服务（例如，配置服务器）而不是事务服务（例如，在线零售应用程序的支付处理）。 策略即代码：描述了许多策略，包括安全策略，作为 可执行模块。一个例子是授权策略，它的代码包含了策略（如允许、拒绝等）和适用领域（如 RESTAPI 的方法，GET、PUT 等，路径等动词或工件）这段代码可以用特殊用途的策略语言（如 Rego）或常规应用中使用的语言（如 Go）编写。这段代码可能与 IaC 的配置代码有一些重合。然而，对于实施与特定于应用领域的关键安全服务相关的策略，需要一个单独的策略作为代码，驻留在参考平台的策略执行点（PEP） …","relpermalink":"/service-mesh-devsecops/implement/description-of-code-types-and-reference-platform-components/","summary":"对上述五类代码（即应用、应用服务、基础设施、策略和监控）的简要描述如下： 应用程序代码和应用服务代码：前者包含一组特定业务事务的数据和应用逻辑，而后者包含所有服务的代码，如网络连接、负载均衡和网络弹性。","title":"4.1 代码类型和参考平台组件的描述"},{"content":"还记得有一次告警电话半夜把我吵醒，发现是生产环境离线了。原来是系统瘫痪了，我们不得不要赔钱，这是我的错。\n从那一刻起，我就一直痴迷于构建坚如磐石的基础架构和基础架构管理系统，这样我就不会重蹈覆辙了。在我的职业生涯中，我为 Terraform、Kubernetes，一些编程语言和 Kops 做出过贡献，并创建了 Kubicorn。我不仅见证了系统基础架构的发展，而且我也帮助它完善。随着基础架构行业的发展，我们发现企业基础架构现在正以新的、令人兴奋的方式通过应用层管理。到目前为止，Kubernetes 是这种管理基础架构的新范例的最成熟的例子。\n我与人合著了这本书，部分地介绍了将基础架构作为云原生软件的新范例。此外，我希望鼓励基础架构工程师开始编写云原生应用程序。在这本书中，我们探讨了管理基础架构的丰富历史，并为云原生技术的未来定义了管理基础架构的模式。我们解释了基础架构由软件化 API 驱动的重要性。我们还探索了创建复杂系统的第一个基础架构组件的引导问题，并教授了扩展和测试基础架构的重要性。\n我于 2017 年加入 Heptio，担任资深布道师，并且很高兴能与行业中最聪明的系统工程师密切 …","relpermalink":"/cloud-native-infra/foreword/","summary":"还记得有一次告警电话半夜把我吵醒，发现是生产环境离线了。原来是系统瘫痪了，我们不得不要赔钱，这是我的错。 从那一刻起，我就一直痴迷于构建坚如磐石的基础架构和基础架构管理系统，这样我就不会重蹈覆辙了。在我","title":"前言"},{"content":"以下是关于本书的声明。\n许可 本出版物由 NIST 根据 2014 年《联邦信息安全现代化法案》（FISMA）（44 U.S.C. §3551 etseq）规定的法定职责编写，公共法律（P.L.）113-283。NIST 负责制定信息安全标准和准则，包括联邦信息系统的最低要求，但这些标准和准则在未经对国家安全系统行使策略权力的适当联邦官员明确批准的情况下，不得适用于这些系统。本准则与管理和预算办公室（OMB）A-130 号通知的要求一致。\n本出版物中的任何内容都不应被视为与商务部长根据法定授权对联邦机构的强制性和约束性标准和准则相抵触。这些准则也不应被解释为改变或取代商务部长、OMB 主任或任何其他联邦官员的现有权力。本出版物可由非政府组织在自愿的基础上使用，在美国不受版权限制。但是，请注明出处，NIST 将对此表示感谢。\n国家标准和技术研究所特别出版物 800-204C Natl.Inst. Stand.Technol.Spec.800-204C, 45 pages (March 2022) CODEN: NSPUE2\n本出版物可从以下网站免费获取。 …","relpermalink":"/service-mesh-devsecops/preface/","summary":"以下是关于本书的声明。 许可 本出版物由 NIST 根据 2014 年《联邦信息安全现代化法案》（FISMA）（44 U.S.C. §3551 etseq）规定的法定职责编写，公共法律（P.L.）113-283。NIST 负责制定信息安全标准","title":"声明"},{"content":"文件变更历史\n英文版\n日期 版本 描述 2021 年 8 月 1.0 首次发布 中文版\n日期 版本 描述 2021 年 8 月 8 日 1.0 首次发布 担保和认可的免责声明\n本文件中的信息和意见是 “按原样” 提供的，没有任何保证或担保。本文件以商品名称、商标、制造商或其他方式提及任何具体的商业产品、程序或服务，并不一定构成或暗示美国政府对其的认可、推荐或青睐，而且本指南不得用于广告或产品代言的目的。\n关于中文版\n中文版为 Jimmy Song 个人翻译，翻译过程中完全遵照原版，未做任何删减。其本人与本书的原作者没有任何组织或利益上的联系，翻译本书仅为交流学习之用。\n商标认可\nKubernetes 是 Linux 基金会的注册商标。 SELinux 是美国国家安全局的注册商标。 AppArmor 是 SUSE LLC 的注册商标。 Windows 和 Hyper-V 是微软公司的注册商标。 ETCD 是 CoreOS, Inc. 的注册商标。 Syslog-ng 是 One Identity Software International Designated Activity 公司的 …","relpermalink":"/kubernetes-hardening-guidance/notices-and-hitory/","summary":"文件变更历史 英文版 日期 版本 描述 2021 年 8 月 1.0 首次发布 中文版 日期 版本 描述 2021 年 8 月 8 日 1.0 首次发布 担保和认可的免责声明 本文件中的信息和意见是 “按原样” 提供的，没有任何保证或担保。本文件以","title":"通知和历史"},{"content":"Service Mesh 又译作“服务网格”，作为服务间通信的基础设施层。Buoyant 公司的 CEO Willian Morgan 在他的这篇文章 WHAT’S A SERVICE MESH? AND WHY DO I NEED ONE? 中解释了什么是 Service Mesh，为什么云原生应用需要 Service Mesh。\n服务网格是用于处理服务间通信的专用基础设施层。它负责通过包含现代云原生应用程序的复杂服务拓扑来可靠地传递请求。实际上，服务网格通常通过一组轻量级网络代理来实现，这些代理与应用程序代码一起部署，而不需要感知应用程序本身。—— Willian Morgan Buoyant CEO\n服务网格（Service Mesh）这个术语通常用于描述构成这些应用程序的微服务网络以及应用之间的交互。随着规模和复杂性的增长，服务网格越来越难以理解和管理。它的需求包括服务发现、负载均衡、故障恢复、指标收集和监控以及通常更加复杂的运维需求，例如 A/B 测试、金丝雀发布、限流、访问控制和端到端认证等。\n服务网格的特点 服务网格有如下几个特点： …","relpermalink":"/cloud-native-handbook/service-mesh/what-is-service-mesh/","summary":"Service Mesh 又译作“服务网格”，作为服务间通信的基础设施层。Buoyant 公司的 CEO Willian Morgan 在他的这篇文章 WHAT’S A SERVICE MESH? AND WHY DO I NEED ONE? 中解释了什么是 Service Mesh，为什么云原生应用需要 Service Mesh。 服务网格是用于处理服","title":"什么是服务网格？"},{"content":"云原生（Cloud Native）这个词汇由来已久，以致于何时出现已无据可考。云原生开始大规模出现在受众视线中，与 Pivotal 提出的云原生应用的理念有着莫大的关系。我们现在谈到云原生，更多的指的是一种文化，而不具象为哪些技术体系。\nPivotal 推出过 Pivotal Cloud Foundry 云原生应用平台和 Spring 开源 Java 开发框架，成为云原生应用架构中先驱者和探路者。Pivotal 是云原生应用平台第一股，2018 年在纽交所上市，2019 年底被 VMWare 以 27 亿美元收购，加入到 VMware 新的产品线 Tanzu。\nPivotal 最初的定义 早在 2015 年 Pivotal 公司的 Matt Stine 写了一本叫做 迁移到云原生应用架构 的小册子，其中探讨了云原生应用架构的几个主要特征：\n符合 12 因素应用 面向微服务架构 自服务敏捷架构 基于 API 的协作 抗脆弱性 笔者已于 2017 年翻译了本书，详见 迁移到云原生应用架构。\nCNCF 最初的定义 到了 2015 年 Google 主导成立了云原生计算基金会（CNCF）， …","relpermalink":"/cloud-native-handbook/intro/what-is-cloud-native/","summary":"云原生（Cloud Native）这个词汇由来已久，以致于何时出现已无据可考。云原生开始大规模出现在受众视线中，与 Pivotal 提出的云原生应用的理念有着莫大的关系。我们现在谈到云原生，更多的指的是一种文化，而不具","title":"什么是云原生？"},{"content":"CNCF，全称 Cloud Native Computing Foundation（云原生计算基金会），成立于 2015 年 7 月 21 日（于美国波特兰 OSCON 2015 上宣布），其最初的口号是坚持和整合开源技术来让编排容器作为微服务架构的一部分，其作为致力于云原生应用推广和普及的一支重要力量，不论您是云原生应用的开发者、管理者还是研究人员都有必要了解。\nCNCF 作为一个厂商中立的基金会，致力于 Github 上的快速成长的开源技术的推广，如 Kubernetes、Prometheus、Envoy 等，帮助开发人员更快更好的构建出色的产品。CNCF 维护了一个全景图项目，详见 GitHub。\n关于 CNCF 的使命与组织方式请参考CNCF 章程，概括的讲 CNCF 的使命包括以下三点：\n容器化包装。 通过中心编排系统的动态资源管理。 面向微服务。 CNCF 这个角色的作用是推广技术，形成社区，开源项目管理与推进生态系统健康发展。\n另外 CNCF 组织由以下部分组成：\n会员：白金、金牌、银牌、最终用户、学术和非赢利成员，不同级别的会员在治理委员会中的投票权不同。 理事会：负责 …","relpermalink":"/cloud-native-handbook/community/cncf/","summary":"CNCF，全称 Cloud Native Computing Foundation（云原生计算基金会），成立于 2015 年 7 月 21 日（于美国波特兰 OSCON 2015 上宣布），其最初的口号是坚持和整合开源技术来让编排容器作为微服务架构的一部分，其作为致力于云原生应用推","title":"云原生计算基金会（CNCF）"},{"content":"软件开发的模式又一次改变了。开源软件和公有云供应商已经从根本上改变了我们构建和部署软件的方式。有了开源软件，我们的应用不再需要从头开始编码。而通过使用公有云供应商，我们不再需要配置服务器或连接网络设备。所有这些都意味着，你可以在短短几天甚至几小时内从头开始构建和部署一个应用程序。\n但是，仅仅因为部署新的应用程序很容易，并不意味着操作和维护它们也变得更容易。随着应用程序变得更加复杂，更加异质，最重要的是，更加分布式，看清大局，以及准确地指出问题发生的地方，变得更加困难。\n但有些事情并没有改变：作为开发者和运维，我们仍然需要能够从用户的角度理解应用程序的性能，我们仍然需要对用户的事务有一个端到端的看法。我们也仍然需要衡量和说明在处理这些事务的过程中资源是如何被消耗的。也就是说，我们仍然需要可观测性。\n但是，尽管对可观测性的需求没有改变，我们实施可观测性解决方案的方式必须改变。本报告详细介绍了关于可观测性的传统思维方式对现代应用的不足：继续将可观测性作为一系列工具（尤其是作为 “三大支柱”）来实施，几乎不可能以可靠的方式运行现代应用。\nOpenTelemetry 通过提供一种综合的方法来收集 …","relpermalink":"/opentelemetry-obervability/foreword/","summary":"前言","title":"前言"},{"content":"代码审查的主要目的是确保逐步改善 Google 代码库的整体健康状况。代码审查的所有工具和流程都是为此而设计的。\n为了实现此目标，必须做出一系列权衡。\n首先，开发人员必须能够对任务进行改进。如果开发者从未向代码库提交过代码，那么代码库的改进也就无从谈起。此外，如果审核人员对代码吹毛求疵，那么开发人员以后也很难再做出改进。\n另外，审查者有责任确保随着时间的推移，CL 的质量不会使代码库的整体健康状况下降。这可能很棘手，因为通常情况下，代码库健康状况会随着时间的而下降，特别是在对团队有严格的时间要求时，团队往往会采取捷径来达成他们的目标。\n此外，审查者应对正在审核的代码负责并拥有所有权。审查者希望确保代码库保持一致、可维护及 Code Review 要点中所提及的所有其他内容。\n因此，我们将以下规则作为 Code Review 中期望的标准：\n一般来说，审核人员应该倾向于批准 CL，只要 CL 确实可以提高系统的整体代码健康状态，即使 CL 并不完美。\n这是所有 Code Review 指南中的高级原则。\n当然，也有一些限制。例如，如果 CL 添加了审查者认为系统中不需要的功能，那么即使代 …","relpermalink":"/eng-practices/review/reviewer/standard/","summary":"代码审查的主要目的是确保逐步改善 Google 代码库的整体健康状况。代码审查的所有工具和流程都是为此而设计的。 为了实现此目标，必须做出一系列权衡。 首先，开发人员必须能够对任务进行改进。如果开发者从未向代码库提交过","title":"Code Review 标准"},{"content":"CL 描述是进行了哪些更改以及为何更改的公开记录。CL 将作为版本控制系统中的永久记录，可能会在长时期内被除审查者之外的数百人阅读。\n开发者将来会根据描述搜索您的 CL。有人可能会仅凭有关联性的微弱印象，但没有更多具体细节的情况下，来查找你的改动。如果所有重要信息都在代码而不是描述中，那么会让他们更加难以找到你的 CL。\n首行 正在做什么的简短摘要。 完整的句子，使用祈使句。 后面跟一个空行。 CL 描述的第一行应该是关于这个 CL 是做什么的简短摘要，后面跟一个空白行。这是将来大多数的代码搜索者在浏览代码的版本控制历史时，最常被看到的内容，因此第一行应该提供足够的信息，以便他们不必阅读 CL 的整个描述就可以获得这个 CL 实际上是做了什么的信息。\n按照传统，CL 描述的第一行应该是一个完整的句子，就好像是一个命令（一个命令句）。例如，“Delete the FizzBuzz RPC and replace it with the new system.”而不是“Deleting the FizzBuzz RPC and replacing it with the new …","relpermalink":"/eng-practices/review/developer/cl-descriptions/","summary":"CL 描述是进行了哪些更改以及为何更改的公开记录。CL 将作为版本控制系统中的永久记录，可能会在长时期内被除审查者之外的数百人阅读。 开发者将来会根据描述搜索您的 CL。有人可能会仅凭有关联性的微弱印象，但没有","title":"写好 CL 描述"},{"content":"本教程在 Kubernetes 快速入门教程的基础上，演示了如何配置 SPIRE 以提供动态的 X.509 证书形式的服务身份，并由 Envoy 秘密发现服务（SDS）使用。本教程中展示了实现 X.509 SVID 身份验证所需的更改，因此你应该首先运行或至少阅读 Kubernetes 快速入门教程。\n为了演示 X.509 身份验证，我们创建了一个简单的场景，包含三个服务。其中一个服务是后端服务，是一个简单的 nginx 实例，用于提供静态数据。我们另外运行两个 Symbank 演示银行应用作为前端服务。Symbank 前端服务向 nginx 后端发送 HTTP 请求以获取用户账户详细信息。\n如图所示，前端服务通过 Envoy 实例建立的 mTLS 连接与后端服务连接，并且 Envoy 实例会为每个工作负载执行 X.509 SVID 身份验证。\n在本教程中，你将学习如何：\n配置 SPIRE 以支持 SDS 配置 Envoy SDS 以使用 SPIRE 提供的 X.509 证书 在 SPIRE 服务器上为 Envoy 实例创建注册条目 使用 SPIRE 测试成功的 X.509 身份验证  …","relpermalink":"/spiffe-and-spire/examples/envoy-x509/","summary":"本教程在 Kubernetes 快速入门教程的基础上，演示了如何配置 SPIRE 以提供动态的 X.509 证书形式的服务身份，并由 Envoy 秘密发现服务（SDS）使用。本教程中展示了实现 X.509 SVID 身份验证所需的更改，因此你应该首先运行或至少阅读 Kubernetes 快速入门教程","title":"使用 Envoy 和 X.509-SVID"},{"content":"SPIRE 是 SPIFFE API 的一个生产就绪的实现，它执行节点和工作负载认证，以便根据一组预先定义的条件，安全地向工作负载发出 SVID，并验证其他工作负载的 SVID。\nSPIRE 架构和组件 SPIRE 部署由一个 SPIRE 服务器和一个或多个 SPIRE 代理组成。服务器充当通过代理向一组工作负载发放身份的签名机构。它还维护一个工作负载身份的注册表，以及为签发这些身份而必须验证的条件。代理在本地向工作负载公开 SPIFFE 工作负载 API，必须安装在工作负载运行的每个节点上。\nSPIRE 架构图 服务器 SPIRE 服务器负责管理和发布其配置的 SPIFFE 信任域中的所有身份。它存储注册条目（指定决定特定 SPIFFE ID 应被签发的条件的选择器）和签名密钥，使用节点证明来自动验证代理的身份，并在被验证的代理请求时为工作负载创建 SVID。\nSPIRE 服务器 服务器的行为是通过一系列的插件决定的。SPIRE 包含几个插件，你可以建立额外的插件来扩展 SPIRE 以满足特定的使用情况。插件的类型包括：\n节点证明器插件：与代理节点证明器一起，验证代理运行的节点的身 …","relpermalink":"/spiffe-and-spire/concept/spire/","summary":"SPIRE 是 SPIFFE API 的一个生产就绪的实现，它执行节点和工作负载认证，以便根据一组预先定义的条件，安全地向工作负载发出 SVID，并验证其他工作负载的 SVID。 SPIRE 架构和组件 SPIRE 部署由一个 SPIRE 服务器和一个或多个 SPIRE 代理组成。","title":"SPIRE 基本概念"},{"content":"嵌套 SPIRE 允许将 SPIRE 服务器“链接”在一起，并且所有 SPIRE 服务器都可以在同一信任域中发放身份，这意味着在同一信任域中标识的所有工作负载都可以使用根密钥验证其身份文档。\n嵌套拓扑结构通过将一个 SPIRE 代理与每个下游 SPIRE 服务器“链接”在一起来实现。下游 SPIRE 服务器通过 Workload API 获得凭证，然后直接与上游 SPIRE 服务器进行身份验证，以获取一个中间 CA。\n为了演示嵌套拓扑中的 SPIRE 部署，我们使用 Docker Compose 创建了一个场景，其中包括一个根 SPIRE 部署和两个嵌套的 SPIRE 部署。\n嵌套拓扑结构非常适合多云部署。由于可以混合匹配节点验证者，下游 SPIRE 服务器可以位于不同的云提供商环境中，并为工作负载和 SPIRE 代理提供身份。\n在本教程中，你将学习以下内容：\n在嵌套拓扑中配置 SPIRE 配置 UpstreamAuthority 插件 为嵌套 SPIRE 服务器创建注册条目 测试在整个信任域中创建的 SVID …","relpermalink":"/spiffe-and-spire/architecture/nested/","summary":"嵌套 SPIRE 允许将 SPIRE 服务器“链接”在一起，并且所有 SPIRE 服务器都可以在同一信任域中发放身份，这意味着在同一信任域中标识的所有工作负载都可以使用根密钥验证其身份文档。 嵌套拓扑结构通过将一个 SPIRE 代理与每个下游 SPIRE 服务器","title":"SPIRE 嵌套架构：将 SPIRE 服务器链接为同一信任域"},{"content":"SPIFFE 标准提供了一种框架的规范，能够在异构环境和组织边界中引导和发放服务的身份。它定义了一种称为 SPIFFE 可验证身份文档（SVID）的身份文档。\nSVID 本身并不代表一种新的文档类型。相反，我们提出了一个规范，定义了如何将 SVID 信息编码到现有文档类型中。\n本文档定义了一种标准，其中将 X.509 证书用作 SVID。假设读者对 X.509 有基本的了解。关于 X.509 的具体信息，请参考RFC 5280。\n引言 SPIFFE 的最重要的功能之一是保护进程间通信。核心标准允许进行身份验证，但利用加密身份来构建安全的通信通道也是非常有益的。由于 TLS 被广泛采用，并且使用基于 X.509 的身份验证，将 X.509 用作 SPIFFE SVID 显然是有优势的。\n本规范讨论了将 SVID 信息编码到 X.509 证书中的约束条件，以及如何验证 X.509 SVID。\nSPIFFE ID 在 X.509 SVID 中，对应的 SPIFFE ID 被设置为主题备用名称扩展（SAN 扩展，参见RFC 5280 第 4.2.1.6 节）。一个 X.509 SVID 必须恰 …","relpermalink":"/spiffe-and-spire/standard/x509-svid/","summary":"SPIFFE 标准提供了一种框架的规范，能够在异构环境和组织边界中引导和发放服务的身份。它定义了一种称为 SPIFFE 可验证身份文档（SVID）的身份文档。 SVID 本身并不代表一种新的文档类型。相反，我们提出了一个规范，定义了如何","title":"X.509 SPIFFE 可验证身份文档"},{"content":"本文指导你如何在 Linux 和 Kubernetes 上安装 SPIRE 服务器。\n步骤 1：获取 SPIRE 二进制文件 预构建的 SPIRE 发行版可在 SPIRE 下载页面找到。tarball 包含服务器和代理二进制文件。\n如果需要，你也可以从源代码构建 SPIRE。\n步骤 2：安装服务器和代理 本入门指南描述了如何在同一节点上安装服务器和代理。在典型的生产部署中，服务器将安装在一个节点上，而一个或多个代理将安装在不同的节点上。\n要安装服务器和代理，请执行以下操作：\n从 SPIRE 下载页面获取最新的 tarball，然后使用以下命令将其解压缩到 /opt/spire 目录中：\nwget https://github.com/spiffe/spire/releases/download/v1.8.2/spire-1.8.2-linux-amd64-musl.tar.gz tar zvxf spire-1.8.2-linux-amd64-musl.tar.gz sudo cp -r spire-1.8.2/. /opt/spire/ 为了方便起见，将 spire-server …","relpermalink":"/spiffe-and-spire/installation/install-server/","summary":"本文指导你如何在 Linux 和 Kubernetes 上安装 SPIRE 服务器。 步骤 1：获取 SPIRE 二进制文件 预构建的 SPIRE 发行版可在 SPIRE 下载页面找到。tarball 包含服务器和代理二进制文件。 如果需要，你也可以从源代码构建 SPIRE。 步骤 2：安装服务器","title":"安装 SPIRE 服务器"},{"content":"本文将指导你在 SPIRE 服务器中使用 SPIFFE ID 注册工作负载。\n如何创建注册条目 注册条目包含以下内容：\nSPIFFE ID 一个或多个选择器集合 父级 ID 服务器将向代理发送所有有权在该节点上运行的工作负载的注册条目列表。代理缓存这些注册条目并保持其更新。\n在工作负载认证期间，代理会发现选择器并将其与缓存的注册条目中的选择器进行比较，以确定应该为工作负载分配哪些 SVID。\n你可以通过在命令行中发出 spire-server entry create 命令或直接调用 Entry API 来注册工作负载，具体方法请参阅 Entry API 文档。可以使用 spire-server entry update 命令修改现有条目。\n在 Kubernetes 上运行时，调用 SPIRE 服务器的常见方法是通过在运行 SPIRE 服务器的 Pod 上使用kubectl exec命令。例如：\nkubectl exec -n spire spire-server-0 -- \\ /opt/spire/bin/spire-server entry create \\ -spiffeID …","relpermalink":"/spiffe-and-spire/configuration/registering/","summary":"本文将指导你在 SPIRE 服务器中使用 SPIFFE ID 注册工作负载。 如何创建注册条目 注册条目包含以下内容： SPIFFE ID 一个或多个选择器集合 父级 ID 服务器将向代理发送所有有权在该节点上运行的工作负载的注册条目列表。代理缓存这些注册条目","title":"注册工作负载"},{"content":"在开始之前，你必须具备以下条件：\nVault 1.3.1 或更新版本 Vault Injector 0.3.0 或更新版本 Elasticsearch 6.x 或 7.x，带有基本许可证或更高版本 设置 Vault 安装 Vault（不需要在 Kubernetes 集群中安装，但应该能够从 Kubernetes 集群内部访问）。必须将 Vault Injector（agent-injector）安装到集群中，并配置以注入 sidecar。Helm Chart v0.5.0+ 会自动完成此操作，它安装了 Vault 0.12+ 和 Vault-Injector 0.3.0+。下面的示例假设 Vault 安装在 tsb 命名空间中。\n有关详细信息，请查看 Vault 文档。\nhelm install --name=vault --set=\u0026#39;server.dev.enabled=true\u0026#39; ./vault-helm 启动启用安全性的 Elasticsearch 如果你管理自己的 Elasticsearch 实例，你需要启用安全性并设置超级用户密码。\n为 Vault 创建角色 首先，使用超级用 …","relpermalink":"/tsb/operations/vault/elasticsearch/","summary":"在开始之前，你必须具备以下条件： Vault 1.3.1 或更新版本 Vault Injector 0.3.0 或更新版本 Elasticsearch 6.x 或 7.x，带有基本许可证或更高版本 设置 Vault 安装 Vault（不需要在 Kubernetes 集群中安装，但应该能够从 Kubernetes 集群内部访问）。必须将 Vault Injector（","title":"Elasticsearch 凭据"},{"content":"在某些情况下，由于 Elasticsearch 索引中的数据模型更改，需要清除现有的索引和模板，以便新版本的 OAP 可以正常运行。\n下面的步骤描述了如何从 Elasticsearch 中清除此类数据并确保 OAP 组件将正确启动。\n减少副本数\n确保在继续之前按照步骤 1 和步骤 2 进行操作 1. 将控制平面命名空间中的 oap-deployment 部署的副本数减少为 0。\n注意\n这需要在 TSB 中登记的所有 CP 集群中执行。 kubectl -n ${CONTROL_NAMESPACE} scale deployment oap-deployment --replicas=0 2. 将管理命名空间中的 oap 部署的副本数减少为 0。\nkubectl -n ${MANAGEMENT_NAMESPACE} scale deployment oap --replicas=0 3. 执行以下命令以删除 Elasticsearch 中的模板和索引。\nes_host=localhost es_port=9200 es_user=\u0026lt;USER\u0026gt; es_pass=\u0026lt;PASS\u0026gt; for …","relpermalink":"/tsb/operations/elasticsearch/wipe-elastic/","summary":"在某些情况下，由于 Elasticsearch 索引中的数据模型更改，需要清除现有的索引和模板，以便新版本的 OAP 可以正常运行。 下面的步骤描述了如何从 Elasticsearch 中清除此类数据并确保 OAP 组件将正确启动。 减少副本数 确保在继续之前按照步骤 1 和步骤 2","title":"Elasticsearch 清理流程"},{"content":"无论我们是使用 TSB 的IngressGateway还是 Istio 的Gateway和VirtualService资源将外部流量路由到我们的服务，都可能会遇到我们暴露的路由问题。在本文档中，我们将展示一些最常见的故障场景以及如何进行故障排查。\n缺少配置 首先要检查的一件事是我们在 TSB 中创建的配置是否存在于目标集群中。例如，在这种情况下：\n$ curl -vk http://helloworld.tetrate.io/hello [ ... ] \u0026gt; GET /hello HTTP/1.1 \u0026gt; Host: helloworld.tetrate.io \u0026gt; User-Agent: curl/7.81.0 \u0026gt; Accept: */* \u0026gt; * Mark bundle as not supporting multiuse \u0026lt; HTTP/1.1 404 Not Found \u0026lt; date: Wed, 27 Apr 2022 14:46:41 GMT \u0026lt; server: istio-envoy \u0026lt; content-length: 0 \u0026lt; 我们得到了一个 404 的 HTTP 响应（未找到），对于 …","relpermalink":"/tsb/troubleshooting/gateway-troubleshooting/","summary":"无论我们是使用 TSB 的IngressGateway还是 Istio 的Gateway和VirtualService资源将外部流量路由到我们的服务，都可能会遇到我们暴露的路由问题。在本文档中，我们将展示一些最常见的故障","title":"Ingress Gateway 故障排除"},{"content":" 注意\n默认情况下，控制平面中的 OAP 不会公开 RED 指标。 要公开 RED 遥测数据，请在启动 OAP 时设置环境变量 SW_EXPORTER_ENABLE_OC=true。 TSB 提供一个与 Prometheus 兼容的单一端点，通过 OAP 服务公开来自 sidecar 的 RED 应用程序指标。每个控制平面集群都公开一个供 Prometheus 抓取的端点，可以使用以下命令查询：\nkubectl port-forward -n \u0026lt;controlplane-namespace\u0026gt; svc/oap 1234:1234 \u0026amp; curl localhost:1234/metrics 导出的 RED 指标包括：\n请求状态码 # HELP tsb_oap_service_status_code 状态码的数量 # TYPE tsb_oap_service_status_code 计数器 tsb_oap_service_status_code{status=\u0026#34;\u0026lt;STATUS|ALL\u0026gt;\u0026#34;,svc=\u0026#34;SERVICE_NAME\u0026#34;,} COUNT 请求延迟 # HELP …","relpermalink":"/tsb/operations/telemetry/red-metrics/","summary":"注意 默认情况下，控制平面中的 OAP 不会公开 RED 指标。 要公开 RED 遥测数据，请在启动 OAP 时设置环境变量 SW_EXPORTER_ENABLE_OC=true。 TSB 提供一个与 Prometheus 兼容的单一端点，通过 OAP 服务公开来自 sidecar 的 RED 应用","title":"Sidecar RED 指标"},{"content":"TSB 主控制平面提供了 Kubernetes CRD（自定义资源定义），您可以使用它们来配置您的应用程序。请参阅以下链接以获取更多详细信息：\n如何启用 GitOps 功能 如何使用 GitOps 功能 完整的 TSB Kubernetes CRD 可以在此处下载。\n","relpermalink":"/tsb/reference/k8s-api/guide/","summary":"TSB 主控制平面提供了 Kubernetes CRD（自定义资源定义），您可以使用它们来配置您的应用程序。请参阅以下链接以获取更多详细信息： 如何启用 GitOps 功能 如何使用 GitOps 功能 完整的 TSB Kubernetes CRD 可以在此处下载。","title":"TSB CRD 参考"},{"content":"本节重点介绍组成 TSB 的架构及其含义。你将获得以下方面的知识：\n我们的可靠部署理念是 TSB 架构背后的驱动力 数据平面，由 Envoy 提供支持 由 Istio 提供支持的本地控制平面 Tetrate Service Bridge 的全局控制平面 Tetrate Service Bridge 的管理平面 拥有管理平面的重要性 Tetrate Service Bridge 中的 Envoy 扩展 读完本节后，你应该清楚地了解 TSB 架构的每个元素以及它们如何协同工作以帮助你管理环境。\n部署理念 TSB 的架构基于以故障域为中心的强大部署理念。此方法涉及识别和隔离关键系统发生故障时受影响的基础设施部分。这些故障域分为三类：\n物理故障：包括主机故障、机架故障、硬盘驱动器故障和资源短缺。 逻辑故障：逻辑故障包括错误配置、安全漏洞以及与依赖项和数据相关的问题。 数据故障：这些涉及数据库问题、错误更新、复制失败和备份问题。 为了创建可靠的系统，这些故障域被分组到孤岛中并作为独立实例进行复制。所得系统的可靠性取决于最小化副本之间的相互依赖性。\n物理故障域 在现代云环境中，需要考虑的一组物理故 …","relpermalink":"/tsb/concepts/architecture/","summary":"本节重点介绍组成 TSB 的架构及其含义。你将获得以下方面的知识： 我们的可靠部署理念是 TSB 架构背后的驱动力 数据平面，由 Envoy 提供支持 由 Istio 提供支持的本地控制平面 Tetrate Service Bridge 的全局控制平面 Tetrate Service Bridge 的管理平面 拥有管理平面的重要性","title":"TSB 架构"},{"content":"本文将描述 WASM 扩展在 TSB 中是如何定义的，以及它们如何分配给层次结构中的组件。\nTSB 中的 WASM 为了控制网格中允许的扩展，避免安全泄漏并简化扩展升级过程，TSB 拥有一个WASM 扩展目录， 管理员将在其中注册所有可用于不同组件中使用的扩展。 此目录将包含每个扩展的描述、镜像和执行属性。 当扩展的新版本可用时，更改 WASM 扩展目录记录的内容将将更新传播到该扩展的所有分配。\nUI 这些扩展被打包为 OCI 镜像，包含 WASM 文件，并部署在容器镜像仓库中，Istio 将从中拉取并提取内容。 使用 OCI 镜像交付 WASM 扩展的好处在于，安全性已经实现并标准化，与其他工作负载镜像一样。\n扩展可以允许全局使用，也可以在一组租户中受到限制，这将影响扩展可以附加的位置。\n在扩展在目录中创建后，它们将启用并可用于与同一组织层次结构中的 TSB 组件的附件中使用。它们的属性将成为附件的配置。 可以配置 WASM 扩展的组件包括：组织、租户、工作区、安全组、入口网关、出口网关和第一层网关。\n在 TSB 资源中使用 WASM 扩展 WASM 扩展可以在组织设置、租户设置、工 …","relpermalink":"/tsb/howto/wasm/wasm-extension/","summary":"本文将描述 WASM 扩展在 TSB 中是如何定义的，以及它们如何分配给层次结构中的组件。 TSB 中的 WASM 为了控制网格中允许的扩展，避免安全泄漏并简化扩展升级过程，TSB 拥有一个WASM 扩展目录， 管理员将在其中注册所有可用于不","title":"TSB 配置"},{"content":"在本文档中，我们将启用入口网关中的速率限制，并展示如何基于 HTTP 请求中的 user-agent 字符串进行速率限制。\n在开始之前，请确保你已经完成以下步骤：\n熟悉 TSB 概念 安装 TSB 环境。你可以使用 TSB 演示 进行快速安装 完成了 TSB 使用快速入门。本文档假定你已经创建了租户，熟悉工作区和配置组，并配置了 tctl 到你的 TSB 环境。 启用速率限制服务器 请阅读并按照 启用速率限制服务器文档 中的说明操作。\n演示安装\n如果你使用 TSB 演示 安装，你已经有一个正在运行并且可以使用的速率限制服务，可以跳过这一部分。 部署 httpbin 服务 请按照 本文档中的说明 创建 httpbin 服务。你可以跳过 “创建证书” 和 “载入 httpbin 应用程序” 部分。\n基于 User-agent 进行速率限制 创建一个名为 rate-limiting-ingress-config.yaml 的文件，用于编辑现有的入口网关，以便对每个 User-agent 头的值进行速率限制，限制为每分钟 5 次请求。将 organization 和 tenant 替换为适当的 …","relpermalink":"/tsb/howto/rate-limiting/ingress-gateway/","summary":"在本文档中，我们将启用入口网关中的速率限制，并展示如何基于 HTTP 请求中的 user-agent 字符串进行速率限制。 在开始之前，请确保你已经完成以下步骤： 熟悉 TSB 概念 安装 TSB 环境。你可以使用 TSB 演示 进行快速安装 完成了 TSB 使用快速入门。","title":"TSB 入口网关中的速率限制"},{"content":"本文档提供有关 TSB 的发布节奏、不同版本的支持期以及组件版本矩阵的信息。\nTSB 发布说明 TSB 遵循基于 semver.org 的语义版本控制模型。每个版本号代表的含义如下：\nMAJOR ：因不兼容的 API 更改而增加。 MINOR ：针对新功能增加。 PATCH ：针对错误和安全修复而增加。 TSB 发布节奏和支持 TSB 定期且至少每季度发布 PATCH 更新 (1.x.y)。 每个新的 MINOR 版本都属于长期支持 (LTS) 政策。 Tetrate 为 LTS 版本提供从一般可用性 (GA) 到一般支持终止 (EoGS) 的支持，通常在 GA 后 12 个月设置。 在支持窗口期间通过 PATCH 版本提供错误和安全修复。 新功能不会向后移植到 LTS 版本。 TSB 候选版本 候选版本提供对新功能的早期访问以进行测试，但不建议用于生产使用。\n发布候选版本\n候选版本可能包含已知或未知的错误，并且不适合生产使用。 支持的版本 Tetrate 根据以下时间表提供支持和补丁：\nTSB 版本 一般可用性 一般支持结束 Kubernetes 版本 OpenShift …","relpermalink":"/tsb/release-notes-announcements/support-policy/","summary":"本文档提供有关 TSB 的发布节奏、不同版本的支持期以及组件版本矩阵的信息。 TSB 发布说明 TSB 遵循基于 semver.org 的语义版本控制模型。每个版本号代表的含义如下： MAJOR ：因不兼容的 API 更改而增加。 MINOR ：针对新功能增加。 PATCH ：针对错误和安","title":"TSB 支持策略"},{"content":"Open Policy Agent (OPA) 是一个开源的通用策略引擎，提供了一种高级声明性语言，允许您将策略定义为代码。OPA 还提供了简单的 API，用于从您的软件中卸载策略决策。\n本文档描述了在 TSB 中配置 OPA 的简化版本，以配合使用它作为外部授权 (ext-authz) 服务的部分，您的实际应用程序可能存在需要进行调整的差异。\nOPA 支持\nTetrate 不提供对 OPA 的支持。如果您需要支持，请在其他地方寻找。 有关下面所述的配置的更详细解释，请参考官方文档。\n准备策略 OPA 需要使用OPA的策略语言编写策略文件以决定是否应授权请求。由于实际策略将因示例而异，因此本文档不会涵盖如何编写此文件的详细信息。请参考OPA网站上的文档以获取详细信息。\n需要注意的一点是策略文件中指定的包名称。如果您的策略文件具有以下包声明，您将在稍后的容器配置中使用值 helloworld.authz。\npackage helloworld.authz 示例：具有基本身份验证的策略 此示例显示了一个策略，仅允许用户 alice 和 bob 通过基本身份验证进行身份验证。如果用户被授权， …","relpermalink":"/tsb/reference/samples/opa/","summary":"Open Policy Agent (OPA) 是一个开源的通用策略引擎，提供了一种高级声明性语言，允许您将策略定义为代码。OPA 还提供了简单的 API，用于从您的软件中卸载策略决策。 本文档描述了在 TSB 中配置 OPA 的简化版本，以配合使用它作为外部授","title":"安装 Open Policy Agent"},{"content":"在本部分中，你将在演示 TSB 环境中部署示例应用程序 (bookinfo)。将使用 TSB UI 和 tctl 命令验证部署。\n先决条件 在继续阅读本指南之前，请确保你已完成以下步骤：\n熟悉 TSB 概念，包括工作区和组 安装 TSB 演示 TSB 演示安装负责载入集群、安装所需的 Operator 并为你提供必要的访问凭据。\n部署 Bookinfo 应用程序 你将使用经典的 Istio bookinfo 应用程序来测试 TSB 的功能。\n创建命名空间并部署应用程序 # Create namespace and label it for Istio injection kubectl create namespace bookinfo kubectl label namespace bookinfo istio-injection=enabled # Deploy the bookinfo application kubectl apply -n bookinfo -f …","relpermalink":"/tsb/quickstart/deploy-sample-app/","summary":"在本部分中，你将在演示 TSB 环境中部署示例应用程序 (bookinfo)。将使用 TSB UI 和 tctl 命令验证部署。 先决条件 在继续阅读本指南之前，请确保你已完成以下步骤： 熟悉 TSB 概念，包括工作区和组 安装 TSB 演示 TSB 演示安装负责","title":"部署应用程序"},{"content":"本文档介绍了如何通过 AWS 容器市场在你的 Amazon Kubernetes (EKS) 集群中安装 Tetrate Service Bridge (TSB)。\n注意\n本文档适用于已经购买了 Tetrate 的 AWS 容器市场提供的用户。 如果你没有订阅 Tetrate 容器市场提供的服务，本文档将不起作用。 如果你对 AWS 市场的私有报价感兴趣，请联系 Tetrate。 Tetrate Operator 概述 Tetrate Operator 是 Tetrate 提供的 Kubernetes Operator，它使安装、部署和升级 TSB 更加简单。Tetrate Service Bridge 的 AWS 容器市场提供安装了 Tetrate Operator 的一个版本到 EKS 集群中。之后，TSB 可以安装在你 EKS 集群中的任何命名空间中。 在本文档中，假定 TSB 将安装在 tsb 命名空间中。\n使用 Tetrate Operator 的先决条件 要使用市场上的 Tetrate 提供，确保满足以下要求：\n你可以访问配置了服务帐户的 EKS 集群（Kubernetes …","relpermalink":"/tsb/setup/aws/container-marketplace/","summary":"本文档介绍了如何通过 AWS 容器市场在你的 Amazon Kubernetes (EKS) 集群中安装 Tetrate Service Bridge (TSB)。 注意 本文档适用于已经购买了 Tetrate 的 AWS 容器市场提供的用户。 如果你没有订阅 Tetrate 容器市场提供的服务，本文档将不起作用。 如果你对 AWS 市场的私有报价","title":"从 AWS 容器市场安装 TSB"},{"content":"GitOps 是一种实践，它使用 Git 存储库作为应用程序/系统状态的真实来源。对状态的更改通过拉取请求（PR）和批准工作流程执行，并将由持续交付（CD）流程自动应用于系统。以下图像对此进行了说明。\nGitOps 有三个核心实践：\n基础设施即代码 这描述了将所有基础设施和应用程序配置存储为 Git 中的代码的实践。\n使用拉取请求进行更改 更改是在分支上提出的，然后创建 PR 将更改合并到主分支中。使用 PR 允许运维工程师与开发团队、安全团队和其他利益相关者进行协作进行对等审查。\nCI（持续集成）和 CD（持续部署）自动化 在理想情况下，不会对 GitOps 管理的环境进行手动更改。相反，CI 和 CD 充当一种协调循环。每次进行更改时，自动化工具会将环境的状态与 Git 存储库中定义的真实来源进行比较。\nGitOps 好处 通过使用 GitOps 实践，你可以获得以下几个好处：\n带有历史记录的版本\n由于每个更改都存储在 Git 中，因此很容易发现进行了哪些更改。在大多数情况下，更改可以追溯到特定的事件或更改请求。\n所有权\n由于更改可以在 Git 历史记录中查找，因此可以找出谁拥有相 …","relpermalink":"/tsb/knowledge-base/gitops/","summary":"GitOps 是一种实践，它使用 Git 存储库作为应用程序/系统状态的真实来源。对状态的更改通过拉取请求（PR）和批准工作流程执行，并将由持续交付（CD）流程自动应用于系统。以下图像对此进行了说明。 GitOps 有三个核心实践： 基","title":"服务网格与 GitOps"},{"content":"本指南使用在 演示环境 中描述的环境，即：\n一个位于 region-1 的 Edge 集群 两个工作负载集群，分别位于 region-1 和 region-2 在工作负载集群中运行的 BookInfo 应用程序 一个 Edge Gateway 用于负载均衡流量到工作负载集群 Edge 和工作负载负载均衡 一个简单的 HTTP 请求被路由到 Edge Gateway，然后转发到其中一个工作负载集群，并生成成功的响应：\ncurl http://bookinfo.tse.tetratelabs.io/productpage 在本指南中，我们将查看系统在工作负载集群发生故障时的故障转移行为。\n生成和观察测试流量 生成一定数量的测试流量对于系统非常有帮助。适用于基准测试的工具，如 wrk，非常适合这个任务：\nwhile sleep 1; do \\ wrk -c 1 -t 1 -d 30 http://bookinfo.tse.tetratelabs.io/productpage ; \\ done 观察 Edge Gateway 上的流量，方法如下：\nkubectl logs -f -n edge …","relpermalink":"/tsb/design-guides/ha-multicluster/cluster-failover/","summary":"本指南使用在 演示环境 中描述的环境，即： 一个位于 region-1 的 Edge 集群 两个工作负载集群，分别位于 region-1 和 region-2 在工作负载集群中运行的 BookInfo 应用程序 一个 Edge Gateway 用于负载均衡流量到工作负载集群 Edge 和工作负载负载均衡 一个简单的 HTTP 请求被路由","title":"工作负载集群故障转移"},{"content":"下表显示了现有的 TSB 功能及其当前阶段。该表将在每个主要或次要版本中更新。\n特征阶段定义 下表定义了 TSB 功能的成熟阶段。\n阶段 Alpha（技术预览） Beta Stable 特征 可能不包含最终版本计划的所有功能。 功能完整，但可能包含许多已知或未知的错误。 功能完整，没有已知错误。 生产用途 不应该在生产中使用。 可用于生产。 可靠，生产经过强化。 API 不保证向后兼容性。 API 是有版本的。 可靠，值得生产。API 具有版本控制功能，并具有自动版本转换功能以实现向后兼容性。 性能 未量化或保证。 未量化或保证。 性能（延迟/规模）被量化、记录，并保证不会出现回归。 文档 缺乏文档。 Documented. 记录用例。 环境 在单一环境（仅限 EKS 或 GKE）上进行测试。 至少在两个环境上进行了测试。 （EKS、GKE、OpenShift） 在多种环境下经过良好测试。 （AKS、EKS、GKE、MKE、OpenShift） 监控 并非所有重要指标都可用。 大多数重要指标都可用。 所有重要指标均可用。 功能状态表 领域 描述 状态 API tctl UI …","relpermalink":"/tsb/release-notes-announcements/feature-status/","summary":"下表显示了现有的 TSB 功能及其当前阶段。该表将在每个主要或次要版本中更新。 特征阶段定义 下表定义了 TSB 功能的成熟阶段。 阶段 Alpha（技术预览） Beta Stable 特征 可能不包含最终版本计划的所有功能。 功能完整，但可能包含许","title":"功能状态"},{"content":"我们将考虑以下故障场景：\n工作负载集群 边缘控制平面 从管理到工作负载的连接丢失 中央控制平面 管理平面 管理集群 并评估故障对以下操作的影响：\n运行生产负载 - 生产负载的可用性、安全性和正确运行 本地集群操作 - 包括直接修改集群配置（如 kubectl 操作）和间接修改（即由本地边缘控制平面进行的更改以应用 TSB 策略或更新服务发现终端点） 指标收集 - 来自远程工作负载集群的指标的集中收集和存储 管理操作 - 由 GitOps、API 或管理 UI 执行的 TSB 配置更改 我们将研究典型的恢复情况，当失败组件恢复或被恢复时。\n架构和术语 在本指南中，我们将使用以下架构描述：\n简化架构图，显示主要配置和指标流 工作负载集群：工作负载集群是托管生产工作负载的 Kubernetes 集群。 生产工作负载：生产工作负载是在工作负载集群中运行的应用程序或服务。为了避免疑虑，‘生产工作负载’ 还包括非生产工作负载。 数据平面：数据平面是部署在工作负载集群中的本地 Istio 实例。 边缘控制平面：边缘控制平面是部署在工作负载集群中的 Tetrate 软件组件（ …","relpermalink":"/tsb/design-guides/ha-dr-mp/scenarios/","summary":"我们将考虑以下故障场景： 工作负载集群 边缘控制平面 从管理到工作负载的连接丢失 中央控制平面 管理平面 管理集群 并评估故障对以下操作的影响： 运行生产负载 - 生产负载的可用性、安全性和正确运行 本地集群操作 - 包括直接","title":"故障场景"},{"content":"此 Chart 安装 TSB 管理平面 Operator，还允许你使用 TSB ManagementPlane CR 安装 TSB 管理平面组件以及使其完全运行所需的所有秘密。\n在开始之前，请确保你已经查看了 Helm 安装过程。\n安装概述 创建一个 values.yaml 文件并使用所需的配置进行编辑。你可以在下面的配置部分中找到有关可用 Helm 配置的更多详细信息。有关 spec 部分的完整参考，请参阅 TSB ManagementPlane CR。\n使用 helm install 命令安装 TSB 管理平面。确保将 image.registry 和 version 设置为正确的注册表位置和 TSB 版本。\n等待所有 TSB 管理平面组件成功部署。你可以尝试登录 TSB UI 或使用 tctl 来验证你的安装。\n安装 要安装 TSB 管理平面，请创建一个 values.yaml 文件，包含以下内容，并根据你的需求进行编辑。\nspec: # 设置组织名称。组织名称必须小写以符合 RFC 标准。 organization: \u0026lt;organization-name\u0026gt; dataStore: …","relpermalink":"/tsb/setup/helm/managementplane/","summary":"此 Chart 安装 TSB 管理平面 Operator，还允许你使用 TSB ManagementPlane CR 安装 TSB 管理平面组件以及使其完全运行所需的所有秘密。 在开始之前，请确保你已经查看了 Helm 安装过程。 安装概述 创建一个 values.yaml 文件并使用所需的配置进行编辑。你可以","title":"管理平面安装"},{"content":"本页面将向你展示如何在生产环境中安装 Tetrate Service Bridge 管理平面。\n在开始之前，请确保你已经：\n检查了要求\n检查了TSB 管理平面组件\n检查了证书类型和内部证书要求\n检查了防火墙信息\n如果你正在升级以前的版本，请还要检查PostgreSQL 备份和还原\n下载了 Tetrate Service Bridge CLI（tctl）\n同步了 Tetrate Service Bridge 镜像\n管理平面 Operator 为了保持安装简单，但仍允许许多自定义配置选项，我们创建了一个管理平面 Operator。该 Operator 将在集群中运行，并根据 ManagementPlane 自定义资源中描述的内容引导管理平面的启动。它会监视更改并执行它们。为了帮助创建正确的自定义资源文档（CRD），我们已经添加了能力到我们的tctl客户端，用于创建基本清单，然后你可以根据你的要求进行修改。之后，你可以将清单直接应用于适当的集群，或在你的源控制操作的集群中使用。\n关于 Operator\n如果你想了解有关 Operator 的内部工作原理以及 Operator 模式的更多信息， …","relpermalink":"/tsb/setup/self-managed/management-plane-installation/","summary":"本页面将向你展示如何在生产环境中安装 Tetrate Service Bridge 管理平面。 在开始之前，请确保你已经： 检查了要求 检查了TSB 管理平面组件 检查了证书类型和内部证书要求 检查了防火墙信息 如果你正在升级以前的版本，请还要检查Post","title":"管理平面安装"},{"content":"概述 要将 AWS 弹性容器服务（ECS）任务加入，你需要按照以下步骤操作：\n创建 AWS ECS 集群 创建任务的 IAM 角色 创建任务执行 IAM 角色 创建一个包含 Workload Onboarding Agent 作为 sidecar 容器的 AWS ECS 任务定义 创建任务的子网 创建安全组 使用此任务定义创建 AWS ECS 服务 创建 AWS ECS 集群 使用 FARGATE 作为容量提供程序创建名为 bookinfo 的 AWS ECS 集群。\naws ecs create-cluster --cluster-name bookinfo --capacity-providers FARGATE 为任务创建 IAM 角色 创建任务的 IAM 角色，并使用以下信任策略。\ncat \u0026lt;\u0026lt; EOF \u0026gt; task-role-trust-policy.json { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: …","relpermalink":"/tsb/setup/workload-onboarding/quickstart/aws-ecs/onboard-ecs/","summary":"概述 要将 AWS 弹性容器服务（ECS）任务加入，你需要按照以下步骤操作： 创建 AWS ECS 集群 创建任务的 IAM 角色 创建任务执行 IAM 角色 创建一个包含 Workload Onboarding Agent 作为 sidecar 容器的 AWS ECS 任务定义 创建任务的子网 创建安全组 使用此任务定义创建 AWS ECS","title":"加入 AWS ECS 任务"},{"content":"本文描述了如何配置 Tetrate Service Bridge（TSB）的 LDAP 集成。在 TSB 中，LDAP 集成允许你将 LDAP 用作用户登录 TSB 的身份提供者，并自动将用户和组从 LDAP 同步到 TSB。\n本文假定你已经具备配置 LDAP 服务以及如何使用它进行身份验证的工作知识。\n配置 LDAP 可以通过ManagementPlane CR 或 Helm 值进行配置。以下是一个使用 LDAP 作为 TSB 身份提供者的自定义资源 YAML 的示例。你需要编辑ManagementPlane CR 或 Helm 值，并配置相关部分。请参阅LDAPSettings了解更多详细信息。\n以下各节将更详细地解释 YAML 文件的每个部分的含义。\nspec: hub: \u0026lt;registry-location\u0026gt; organization: \u0026lt;organization-name\u0026gt; ... identityProvider: ldap: host: \u0026lt;ldap-hostname-or-ip\u0026gt; port: \u0026lt;ldap-port\u0026gt; search: baseDN: …","relpermalink":"/tsb/operations/users/configuring-ldap/","summary":"本文描述了如何配置 Tetrate Service Bridge（TSB）的 LDAP 集成。在 TSB 中，LDAP 集成允许你将 LDAP 用作用户登录 TSB 的身份提供者，并自动将用户和组从 LDAP 同步到 TSB。 本文假定你已经具备配置 LDAP 服务以及如何使用它进行身份验证","title":"将 LDAP 配置为身份提供者"},{"content":"本文将教你如何将虚拟机上的部分\u0026#34;单体\u0026#34;工作负载迁移到集群，并在虚拟机和集群之间分流流量。\n在这个示例中，在虚拟机上运行的服务器将被视为\u0026#34;单体\u0026#34;应用程序。如果虚拟机正在调用其他虚拟机，可以按照相同的步骤进行操作。只需确保可以从你的集群解析并访问被调用的虚拟机。\n在开始之前：\n安装 TSB 管理平面 载入了一个集群 安装数据面 Operator 配置一个虚拟机以运行在 TSB 工作负载中（本指南假定使用 Ubuntu 20.04）。 第一步是安装 Docker。\nsudo apt-get update sudo apt-get -y install docker.io 然后，运行 httpbin 服务器并测试其是否正常工作。\nsudo docker run -d \\ --name httpbin \\ -p 127.0.0.1:80:80 \\ kennethreitz/httpbin curl localhost/headers { \u0026#34;headers\u0026#34;: { \u0026#34;Accept\u0026#34;: \u0026#34;*/*\u0026#34;, \u0026#34;Host\u0026#34;: \u0026#34;localhost\u0026#34;, \u0026#34;User-Agent\u0026#34;: \u0026#34;curl/7.68.0\u0026#34; } …","relpermalink":"/tsb/howto/traffic/migrating-vm-monoliths/","summary":"本文将教你如何将虚拟机上的部分\"单体\"工作负载迁移到集群，并在虚拟机和集群之间分流流量。 在这个示例中，在虚拟机上运行的服务器将被视为\"单体\"应用程序。如果虚拟机","title":"将虚拟机上的单体应用迁移到你的集群"},{"content":"本页面深入介绍了 TSB Operator 如何管理控制平面组件的生命周期，并概述了你可以通过 TSB Operator 配置和管理的自定义资源。\nTSB Operator 配置为监督控制平面组件的生命周期，主动监控部署的同一命名空间内的 ControlPlane 自定义资源 (CR)。默认情况下，控制平面位于 istio-system 命名空间中。有关自定义资源 API 的详细信息，你可以参考控制平面安装 API 参考文档。\n控制平面 Operator 组件 以下是你可以使用控制平面 Operator 配置和管理的各种类型的自定义组件：\n组件 Service Deployment istio Istio-operator-metrics (istiod, vmgateway) Istio-operator (istiod vmgateway) (istio-cni-node daemonset in kube-system namespace) oap oap oap-deployment collector otel-collector otel-collector …","relpermalink":"/tsb/concepts/operators/control-plane/","summary":"本页面深入介绍了 TSB Operator 如何管理控制平面组件的生命周期，并概述了你可以通过 TSB Operator 配置和管理的自定义资源。 TSB Operator 配置为监督控制平面组件的生命周期，主动监控部署的同一命名空间内的 ControlPlane 自定义资源 (CR)。默认情况下，","title":"控制平面"},{"content":" Alpha 功能\n流式服务日志是一个 Alpha 功能，不建议在生产环境中使用。 TSB 具有直接从 TSB UI 查看服务日志的功能。使用此功能，你将能够查看应用程序和 sidecar 的几乎实时日志，以进行故障排除。\n日志存储\nTSB 不会将任何日志存储在存储系统中。日志直接从集群流式传输到管理平面。 管理平面 要在管理平面中启用服务日志流式传输，请在 ManagementPlane CR 或 Helm 值中的 oap 组件下添加 streamingLogEnabled: true ，然后应用。\nspec: hub: \u0026lt;registry_location\u0026gt; organization: \u0026lt;organization\u0026gt; ... components: ... oap: streamingLogEnabled: true 控制平面 对于每个注册的集群，请在 ControlPlane CR 或 Helm 值中的 oap 组件下添加 streamingLogEnabled: true ，然后应用。\nspec: hub: \u0026lt;registry_location\u0026gt; managementPlane: …","relpermalink":"/tsb/operations/features/streaming-log/","summary":"Alpha 功能 流式服务日志是一个 Alpha 功能，不建议在生产环境中使用。 TSB 具有直接从 TSB UI 查看服务日志的功能。使用此功能，你将能够查看应用程序和 sidecar 的几乎实时日志，以进行故障排除。 日志存储 TSB 不会将任何日志存储在存储系统中","title":"流式服务日志"},{"content":"在继续之前，请确保你了解 TSB 中的 4 种证书类型，特别是内部证书。\n注意\n请注意，此处描述的证书仅用于 TSB 组件之间的通信，因此不属于通常由 Istio 或应用程序 TLS 证书管理的工作负载证书。 提醒\n如果你在管理平面集群中安装了 cert-manager，你可以使用 tctl 自动在管理平面中安装所需的发行者和证书，并创建控制平面证书。有关更多详细信息，请参阅 管理平面安装 和 载入集群 文档。 要使用常规（非相互）TLS 进行 JWT 身份验证，XCP central 证书必须在其主体备用名称（SANs）中包含其地址。这将是 DNS 名称或 IP 地址。\n与上述 mTLS 类似，管理平面中的 XCP central 使用存储在名为 xcp-central-cert 的管理平面命名空间（默认为 tsb）中的密钥中的证书。密钥必须包含标准的 tls.crt、tls.key 和 ca.crt 字段的数据。\n以下是如果你使用 IP 地址作为 XCP central 证书的 cert-manager 资源示例。\napiVersion: cert-manager.io/v1 …","relpermalink":"/tsb/setup/certificate/certificate-requirements/","summary":"在继续之前，请确保你了解 TSB 中的 4 种证书类型，特别是内部证书。 注意 请注意，此处描述的证书仅用于 TSB 组件之间的通信，因此不属于通常由 Istio 或应用程序 TLS 证书管理的工作负载证书。 提醒 如果你在管理平面集群中安装了 ce","title":"内部证书要求"},{"content":"本文档解释了如何配置 Flux CD 与 Helm 和 GitHub 集成，以将 TSB 应用程序部署到目标集群。\n注意\n本文档假设以下情况：\n已安装 Flux 版本 2 的 CLI。 已安装 Helm 的 CLI。 TSB 正在运行，并且已为目标集群启用了 GitOps 配置。 集群设置 首先，在目标集群上使用 GitHub 集成 安装 Flux。要执行此操作，请在目标集群 Kubernetes 上下文下使用以下命令。\n注意\n你将需要一个 GitHub 个人访问令牌（PAT）输入以下命令。 $ flux bootstrap github \\ --owner=your-org \\ --repository=git-ops \\ --path=./clusters/cluster-01 注意\n如果你使用个人 GitHub 帐户进行测试，可以添加 --personal --private 标志。 这将为名为 cluster-01 的集群在名为 git-ops 的 GitHub 存储库的 clusters/cluster-01/ 目录下设置 Flux 所需的配置。\n注意\n为了调试目的， …","relpermalink":"/tsb/howto/gitops/flux/","summary":"本文档解释了如何配置 Flux CD 与 Helm 和 GitHub 集成，以将 TSB 应用程序部署到目标集群。 注意 本文档假设以下情况： 已安装 Flux 版本 2 的 CLI。 已安装 Helm 的 CLI。 TSB 正在运行，并且已为目标集群启用了 GitOps 配置。 集群设置 首先，在目标集群","title":"配置 Flux CD 进行 GitOps"},{"content":"安装 Bookinfo Ratings 应用程序 SSH 进入本地虚拟机并安装 ratings 应用程序。执行以下命令：\n# 安装最新版本的可信 CA 证书 sudo apt-get update -y sudo apt-get install -y ca-certificates # 添加带有 Node.js 的 DEB 仓库 curl --fail --silent --location https://deb.nodesource.com/setup_14.x | sudo bash - # 安装 Node.js sudo apt-get install -y nodejs # 下载 Bookinfo Ratings 应用程序的 DEB 包 curl -fLO https://dl.cloudsmith.io/public/tetrate/onboarding-examples/raw/files/bookinfo-ratings.deb # 安装 DEB 包 sudo apt-get install -y ./bookinfo-ratings.deb # 删除下载的文件 rm …","relpermalink":"/tsb/setup/workload-onboarding/quickstart/on-premise/configure-vm/","summary":"安装 Bookinfo Ratings 应用程序 SSH 进入本地虚拟机并安装 ratings 应用程序。执行以下命令： # 安装最新版本的可信 CA 证书 sudo apt-get update -y sudo apt-get install -y ca-certificates # 添加带有 Node.js 的 DEB 仓库 curl --fail --silent --location https://deb.nodesource.com/setup_14.x | sudo bash - # 安装 Node.js sudo apt-get install -y nodejs # 下载 Bookinfo Ratings 应用程序的 DEB 包 curl -fLO https://dl.cloudsmith.io/public/tetrate/onboarding-examples/raw/files/bookinfo-ratings.deb # 安装","title":"配置本地虚拟机"},{"content":"为了启用工作负载载入，你需要以下信息：\n用于分配工作负载载入端点的 DNS 名称 该 DNS 名称的 TLS 证书 在本示例中，你将使用 DNS 名称 onboarding-endpoint.example，因为我们不希望你使用可路由的 DNS 名称。\n准备证书 出于生产目的，你需要使用由可信任的证书颁发机构（CA）签名的 TLS 证书，例如 Let’s Encrypt 或内部 CA（如 Vault）。\n在本示例中，你将设置一个示例 CA，它将在本指南的其余部分中使用。\n通过执行以下命令创建一个自签名证书（example-ca.crt.pem）和 CA 私钥（example-ca.key.pem）：\nopenssl req \\ -x509 \\ -subj \u0026#39;/CN=Example CA\u0026#39; \\ -days 3650 \\ -sha256 \\ -newkey rsa:2048 \\ -nodes \\ -keyout example-ca.key.pem \\ -out example-ca.crt.pem \\ -config \u0026lt;(cat \u0026lt;\u0026lt;EOF # \u0026#34;openssl req\u0026#34; …","relpermalink":"/tsb/setup/workload-onboarding/quickstart/aws-ec2/enable-workload-onboarding/","summary":"为了启用工作负载载入，你需要以下信息： 用于分配工作负载载入端点的 DNS 名称 该 DNS 名称的 TLS 证书 在本示例中，你将使用 DNS 名称 onboarding-endpoint.example，因为我们不希望你使用可路由的 DNS 名","title":"启用工作负载载入"},{"content":"步骤 启用工作负载载入 创建 WorkloadGroup 允许工作负载加入 WorkloadGroup 创建 Sidecar 配置 在 VM 上安装工作负载载入代理 启用工作负载载入 要在给定的 Kubernetes 集群中启用工作负载载入，你需要编辑 TSB ControlPlane 资源或 Helm 配置，如下所示：\nspec: ... meshExpansion: onboarding: # (1) REQUIRED endpoint: hosts: - \u0026lt;onboarding-endpoint-dns-name\u0026gt; # (2) REQUIRED secretName: \u0026lt;onboarding-endpoint-tls-cert\u0026gt; # (3) REQUIRED tokenIssuer: jwt: expiration: \u0026lt;onboarding-token-expiration-time\u0026gt; # (4) OPTIONAL localRepository: {} # (5) OPTIONAL 然后：\n要在给定的 Kubernetes 集群中启用工作负载载入， …","relpermalink":"/tsb/setup/workload-onboarding/guides/setup/","summary":"步骤 启用工作负载载入 创建 WorkloadGroup 允许工作负载加入 WorkloadGroup 创建 Sidecar 配置 在 VM 上安装工作负载载入代理 启用工作负载载入 要在给定的 Kubernetes 集群中启用工作负载载入，你需要编辑 TSB ControlPlane 资源或 Helm 配置，如下所示： spec: ... meshExpansion: onboarding: # (1) REQUIRED endpoint: hosts: - \u003conboarding-endpoint-dns-name\u003e # (2) REQUIRED secretName:","title":"设置工作负载载入"},{"content":"在本操作指南中，你将了解如何通过基于 URI 端点、标头和端口匹配流量并将其路由到目标服务的主机:端口来设置基于子集的流量路由。\n先决条件 在继续之前，请确保你已完成以下任务：\n熟悉 TSB 概念。 安装 TSB 演示环境。 创建一个租户。 创建工作区和配置组 首先，使用以下 YAML 配置创建工作区和配置组：\nhelloworld-ws-groups.yaml apiversion: api.tsb.tetrate.io/v2 kind: Workspace metadata: organization: tetrate tenant: tetrate name: helloworld-ws spec: namespaceSelector: names: - \u0026#39;*/helloworld\u0026#39; --- apiVersion: gateway.tsb.tetrate.io/v2 kind: Group metadata: organization: tetrate tenant: tetrate workspace: helloworld-ws name: helloworld-gw …","relpermalink":"/tsb/howto/gateway/subset-based-routing-using-igw-and-service-route/","summary":"在本操作指南中，你将了解如何通过基于 URI 端点、标头和端口匹配流量并将其路由到目标服务的主机:端口来设置基于子集的流量路由。 先决条件 在继续之前，请确保你已完成以下任务： 熟悉 TSB 概念。 安装 TSB 演示环境。 创建一个","title":"使用 IngressGateway 和 ServiceRoute 基于子集的流量路由"},{"content":"在继续之前，请确保你熟悉Istio 隔离边界功能。\n注意\n可以在单个隔离边界内执行已修订到已修订的控制平面升级。 升级之前 一旦启用了 Istio 隔离边界功能，边界可以用于保持服务发现隔离，并在隔离边界内升级 Istio 控制平面。对于包含单个隔离边界的ControlPlane CR 或 Helm 值：\nspec: ... components: xcp: isolationBoundaries: - name: global revisions: - name: stable istio: tsbVersion: 1.6.0 你将升级stable修订版本中的所有工作负载以使用tsbVersion: 1.6.1。\n控制平面升级策略\nTSB 支持修订到修订升级的原地和金丝雀控制平面升级。 控制平面原地升级 对于原地升级，你可以直接更新tsbVersion字段 - 保留修订name不变。\nspec: ... components: xcp: isolationBoundaries: - name: global revisions: - name: stable istio: …","relpermalink":"/tsb/setup/upgrades/revisioned-to-revisioned/","summary":"在继续之前，请确保你熟悉Istio 隔离边界功能。 注意 可以在单个隔离边界内执行已修订到已修订的控制平面升级。 升级之前 一旦启用了 Istio 隔离边界功能，边界可以用于保持服务发现隔离，并在隔离边界内升级 Istio 控制平面。","title":"已修订版本间的升级"},{"content":"在本文中，将使用 httpbin 作为工作负载。传入 Ingress GW 的请求将由 OPA 检查。如果请求被视为未经授权，那么将以 403（Forbidden）响应拒绝请求。\n以下图像显示了在使用外部授权系统时的请求和响应流程，你将部署 OPA 作为独立服务。\n部署 httpbin 服务 按照此文档中的所有说明创建httpbin服务。\n部署 OPA 服务 参考\u0026#34;安装 Open Policy Agent“文档，创建具有基本身份验证的策略并部署 OPA 作为独立服务。\n配置 Ingress Gateway 你需要为httpbin再次配置 Ingress Gateway 以使用 OPA。创建名为 httpbin-ingress.yaml 的文件，其中包含以下内容：\napiVersion: gateway.tsb.tetrate.io/v2 kind: IngressGateway metadata: organization: tetrate name: httpbin-ingress-gateway group: httpbin workspace: httpbin tenant: …","relpermalink":"/tsb/howto/authorization/ingress-gateway/","summary":"在本文中，将使用 httpbin 作为工作负载。传入 Ingress GW 的请求将由 OPA 检查。如果请求被视为未经授权，那么将以 403（Forbidden）响应拒绝请求。 以下图像显示了在使用外部授权系统时的请求和响应流程，你将部署 OPA 作为独","title":"在 Ingress Gateways 中配置外部授权"},{"content":"在有效使用 Argo CD 之前，有必要了解该平台构建的底层技术。还有必要了解向你提供的功能以及如何使用它们。以下部分提供了一些有用的链接来建立这种理解。\n学习基础知识 浏览在线 Docker 和 Kubernetes 教程： 适合初学者的容器、虚拟机和 Docker 简介 Kubernetes 简介 教程 根据你计划如何模板化你的应用程序： Kustomize Helm 如果你要与 CI 工具集成： GitHub Actions 文档 Jenkins 用户指南 ","relpermalink":"/argo-cd/understand-the-basics/","summary":"在有效使用 Argo CD 之前，有必要了解该平台构建的底层技术。还有必要了解向你提供的功能以及如何使用它们。以下部分提供了一些有用的链接来建立这种理解。 学习基础知识 浏览在线 Docker 和 Kubernetes 教程： 适合初学者的容器、虚拟机和 Docker","title":"理解基础"},{"content":"本指南假定你已经熟悉 Argo CD 及其基本概念。有关更多信息，请参阅 Argo CD 文档。\n要求 安装 kubectl 命令行工具 有一个 kubeconfig 文件（默认位置为 ~/.kube/config）。 安装 有几种安装 ApplicationSet 控制器的选项。\nA) 将 ApplicationSet 作为 Argo CD 的一部分安装 从 Argo CD v2.3 开始，ApplicationSet 控制器已捆绑在 Argo CD 中。无需从 Argo CD 单独安装 ApplicationSet 控制器。\n有关更多信息，请参阅 Argo CD 入门指南。\nB) 将 ApplicationSet 安装到现有的 Argo CD 安装中（Argeo CD v2.3 之前） 注意: 以下说明仅适用于 Argo CD 版本 v2.3.0 之前。\nApplicationSet 控制器 必须 安装到与其所针对的 Argo CD 相同的命名空间中。\n假设 Argo CD 安装在 argocd 命名空间中，请运行以下命令：\nkubectl apply -n argocd -f …","relpermalink":"/argo-cd/operator-manual/applicationset/getting-started/","summary":"本指南假定你已经熟悉 Argo CD 及其基本概念。有关更多信息，请参阅 Argo CD 文档。 要求 安装 kubectl 命令行工具 有一个 kubeconfig 文件（默认位置为 ~/.kube/config）。 安装 有几种安装 ApplicationSet 控制器的选项。 A) 将 ApplicationSet 作为 Argo CD 的一部分安装","title":"ApplicationSet 入门"},{"content":"本教程将指导你如何配置 Argo Rollouts 与 Ambassador 配合以实现金丝雀发布。本指南中使用的所有文件都可在此存储库的 examples 目录中找到。\n要求 Kubernetes 集群 在集群中安装 Argo-Rollouts 注意\n如果使用 Ambassador Edge Stack 或 Emissary-ingress 2.0+，则需要安装 Argo-Rollouts 版本 v1.1+，并需要向 argo-rollouts 部署提供 --ambassador-api-version getambassador.io/v3alpha1。\n1. 安装和配置 Ambassador Edge Stack 如果你的集群中没有 Ambassador，可以按照 Edge Stack 文档 进行安装。\n默认情况下，Edge Stack 通过 Kubernetes 服务路由。为了获得更好的金丝雀性能，我们建议你使用端点路由。通过将以下配置保存在名为 resolver.yaml 的文件中，启用集群上的端点路由：\napiVersion: getambassador.io/v2 …","relpermalink":"/argo-rollouts/getting-started/ambassador/","summary":"本教程将指导你如何配置 Argo Rollouts 与 Ambassador 配合以实现金丝雀发布。本指南中使用的所有文件都可在此存储库的 examples 目录中找到。 要求 Kubernetes 集群 在集群中安装 Argo-Rollouts 注意 如果使用 Ambassador Edge Stack 或 Emissary-ingress 2.0+，则需要安装 Argo-Rollouts 版本 v1.1+，并需要向 argo-rollouts 部","title":"Argo Rollouts 和 Ambassador 快速开始"},{"content":"Ambassador Edge Stack 提供了你在 Kubernetes 集群边缘所需的功能（因此称为“边缘堆栈”）。这包括 API 网关、入口控制器、负载均衡器、开发人员门户、金丝雀流量路由等。它提供了一组 CRD，用户可以配置以启用不同的功能。\nArgo-Rollouts 提供了一个集成，利用了 Ambassador 的 金丝雀路由功能。这允许你的应用程序的流量在部署新版本时逐步增加。\n工作原理 Ambassador Edge Stack 提供了一个名为“Mapping”的资源，用于配置如何将流量路由到服务。通过创建具有相同 URL 前缀并指向不同服务的 2 个映射，可以实现 Ambassador 金丝雀部署。考虑以下示例：\napiVersion: getambassador.io/v2 kind: Mapping metadata: name: stable-mapping spec: prefix: /someapp rewrite: / service: someapp-stable:80 --- apiVersion: getambassador.io/v2 kind: …","relpermalink":"/argo-rollouts/traffic-management/ambassador/","summary":"Ambassador Edge Stack 提供了你在 Kubernetes 集群边缘所需的功能（因此称为“边缘堆栈”）。这包括 API 网关、入口控制器、负载均衡器、开发人员门户、金丝雀流量路由等。它提供了一组 CRD，用户可以配置以启用不同的功能。 Argo-Rollouts 提供了一个集成，","title":"Ambassador Edge Stack"},{"content":"以下描述了 Rollout 的所有可用字段：\napiVersion: argoproj.io/v1alpha1 kind: Rollout metadata: name: example-rollout-canary spec: # 期望的 Pod 数量。 # 默认为 1。 replicas: 5 analysis: # 限制存储历史上成功的分析运行和实验的数量 # 默认为 5。 successfulRunHistoryLimit: 10 # 限制存储历史上不成功的分析运行和实验的数量。 # 不成功的阶段有：\u0026#34;Error\u0026#34;，\u0026#34;Failed\u0026#34;，\u0026#34;Inconclusive\u0026#34; # 默认为 5。 unsuccessfulRunHistoryLimit: 10 # Pod 的标签选择器。被选择的 Pod 的现有副本集将受到此 Rollout 的影响。它必须与 Pod 模板的标签匹配。 selector: matchLabels: app: guestbook # WorkloadRef 包含对提供 Pod 模板（例如 Deployment）的工作负载的引用。如果使用，则不使用 Rollout 模 …","relpermalink":"/argo-rollouts/rollout/specification/","summary":"以下描述了 Rollout 的所有可用字段： apiVersion: argoproj.io/v1alpha1 kind: Rollout metadata: name: example-rollout-canary spec: # 期望的 Pod 数量。 # 默认为 1。 replicas: 5 analysis: # 限制存储历史上成功的分析运行和实验的数量 # 默认为 5。 successfulRunHistoryLimit: 10 # 限制存储历史上不成功的分析运行和实验的数量。 # 不成功的阶段有：","title":"Rollout 规范"},{"content":"控制器安装 两种安装方式：\ninstall.yaml - 标准安装方法。 kubectl create namespace argo-rollouts kubectl apply -n argo-rollouts -f https://github.com/argoproj/argo-rollouts/releases/latest/download/install.yaml 这将创建一个新的命名空间 argo-rollouts，在其中运行 Argo Rollouts 控制器。\n🔔 提示：如果你使用的是其他命名空间名称，请更新 install.yaml 集群角色绑定的服务账户命名空间名称。\n🔔 提示：在 Kubernetes v1.14 或更低版本上安装 Argo Rollouts 时，CRD 清单必须使用 --validate = false 选项进行 kubectl apply。这是由于在 v1.15 中引入的新 CRD 字段的使用，在较低的 API 服务器中默认被拒绝。\n🔔 提示：在 GKE 上，你需要授予你的账户创建新集群角色的权限：\nkubectl create …","relpermalink":"/argo-rollouts/installation/","summary":"控制器安装 两种安装方式： install.yaml - 标准安装方法。 kubectl create namespace argo-rollouts kubectl apply -n argo-rollouts -f https://github.com/argoproj/argo-rollouts/releases/latest/download/install.yaml 这将创建一个新的命名空间 argo-rollouts，在其中运行 Argo Rollouts 控制器。 🔔 提示：如果你使用的是其他命名空间名称，请更新 install.yaml 集群角色绑定的服","title":"安装 Argo Rollouts"},{"content":"🔔 重要提醒：从 v1.5 开始可用 - 状态：Alpha\nArgo Rollouts 通过第三方插件系统支持获取分析指标。这使得用户可以扩展 Rollouts 的功能，以支持不受本地支持的度量提供者。Rollouts 使用一个名为 go-plugin 的插件库来实现。你可以在此处找到示例插件：rollouts-plugin-metric-sample-prometheus\n使用指标插件 安装和使用 argo rollouts 插件有两种方法。第一种方法是将插件可执行文件挂载到 rollouts 控制器容器中。第二种方法是使用 HTTP(S) 服务器托管插件可执行文件。\n将插件可执行文件挂载到 rollouts 控制器容器中 有几种方法可以将插件可执行文件挂载到 rollouts 控制器容器中。其中一些方法将取决于你的特定基础架构。这里有几种方法：\n使用 init 容器下载插件可执行文件 使用 Kubernetes 卷挂载共享卷，例如 NFS、EBS 等。 将插件构建到 rollouts 控制器容器中 然后，你可以使用 configmap 将插件可执行文件位置指向插件。示例： …","relpermalink":"/argo-rollouts/analysis/plugins/","summary":"🔔 重要提醒：从 v1.5 开始可用 - 状态：Alpha Argo Rollouts 通过第三方插件系统支持获取分析指标。这使得用户可以扩展 Rollouts 的功能，以支持不受本地支持的度量提供者。Rollouts 使用一个名为 go-plugin 的插件库来实现。你可以在此处","title":"指标插件"},{"content":"Argo Rollouts 支持以下通知服务：\nAlertmanager Email GitHub Google Chat Grafana Mattermost NewRelic Opsgenie Overview Pagerduty Pushover Rocket.Chat Slack Teams Telegram Webex Teams Webhook ","relpermalink":"/argo-rollouts/notifications/services/","summary":"Argo Rollouts 支持以下通知服务： Alertmanager Email GitHub Google Chat Grafana Mattermost NewRelic Opsgenie Overview Pagerduty Pushover Rocket.Chat Slack Teams Telegram Webex Teams Webhook","title":"通知服务列表"},{"content":"金丝雀发布是一种部署策略，操作员将新版本的应用程序释放到生产流量的一小部分中。\n概述 由于没有关于金丝雀部署的共识标准，因此 Rollouts Controller 允许用户概述他们想要运行其金丝雀部署的方式。用户可以定义一个控制器使用的步骤列表，以在.spec.template发生更改时操作 ReplicaSets。在新的 ReplicaSet 被提升为稳定版本并且旧版本被完全缩减之前，每个步骤将被评估。\n每个步骤可以有两个字段。setWeight字段指定应发送到金丝雀的流量百分比，pause结构指示 Rollout 暂停。当控制器到达 Rollout 的pause步骤时，它将向.status.PauseConditions字段添加一个PauseCondition结构。如果pause结构中的duration字段设置，Rollout 将在等待duration字段的值之前不会进入下一个步骤。否则，Rollout 将无限期等待该 Pause 条件被删除。通过使用setWeight和pause字段，用户可以描述他们想要如何进入新版本。以下是金丝雀策略的示例。\n🔔 重要： …","relpermalink":"/argo-rollouts/rollout/deployment-strategies/canary/","summary":"金丝雀发布是一种部署策略，操作员将新版本的应用程序释放到生产流量的一小部分中。 概述 由于没有关于金丝雀部署的共识标准，因此 Rollouts Controller 允许用户概述他们想要运行其金丝雀部署的方式。用户可以定义一个控制器使用的步骤","title":"金丝雀部署"},{"content":" rollouts rollouts abort rollouts completion rollouts create rollouts create analysisrun rollouts dashboard rollouts get rollouts get experiment rollouts get rollout rollouts lint rollouts list rollouts list experiments rollouts list rollouts rollouts notifications rollouts notifications template rollouts notifications template get rollouts notifications template notify rollouts notifications trigger rollouts notifications trigger get rollouts notifications trigger run rollouts pause rollouts …","relpermalink":"/argo-rollouts/kubectl-plugin/command/","summary":"rollouts rollouts abort rollouts completion rollouts create rollouts create analysisrun rollouts dashboard rollouts get rollouts get experiment rollouts get rollout rollouts lint rollouts list rollouts list experiments rollouts list rollouts rollouts notifications rollouts notifications template rollouts notifications template get rollouts notifications template notify rollouts notifications trigger rollouts notifications trigger get rollouts notifications trigger run rollouts pause rollouts promote rollouts restart rollouts retry rollouts retry experiment rollouts retry rollout rollouts set rollouts set image rollouts status rollouts terminate rollouts terminate analysisrun rollouts terminate experiment rollouts undo rollouts version","title":"kubectl argo rollouts 命令用法"},{"content":"本章介绍了 SPIFFE 的动机和它是如何诞生的。\n压倒性的动机和需要 我们到达今天的位置，首先要经历一些成长的痛苦。\n当互联网在 1981 年首次广泛使用时，它只有 213 个不同的服务器，而安全问题几乎没有被考虑到。随着互联计算机数量的增加，安全问题仍然是一个弱点：容易被利用的漏洞导致了大规模的攻击，如莫里斯蠕虫病毒，它在 1988 年占领了互联网上的大多数 Unix 服务器，或 Slammer 蠕虫病毒，它在 2003 年在数十万台 Windows 服务器上传播。\n随着时间的推移，过去的传统周边防御模式已经不能很好地适应不断发展的计算架构和现代技术的边界。各种解决方案和技术层出不穷，以掩盖基础网络安全概念未能跟上现代化趋势而出现的越来越大的裂缝。\n那么，为什么周边模式如此普遍，怎样才能解决这些缺陷？\n多年来，我们观察到三大的趋势，突出了传统的周边模式对网络未来的限制：\n软件不再在组织控制的单个服务器上运行。自 2015 年以来，新的软件通常被构建为微服务的集合，可以单独扩展或转移到云主机供应商。如果你不能在需要安全的服务周围画出一条精确的线，就不可能在它们周围筑起一道墙。 你不能 …","relpermalink":"/spiffe/history-and-motivation-for-spiffe/","summary":"本章介绍了 SPIFFE 的动机和它是如何诞生的。 压倒性的动机和需要 我们到达今天的位置，首先要经历一些成长的痛苦。 当互联网在 1981 年首次广泛使用时，它只有 213 个不同的服务器，而安全问题几乎没有被考虑到。随着互联计算机数量","title":"SPIFFE 的历史和动机"},{"content":"标签 标签（Label）是一种通用的、灵活的和高度可扩展的方式，可以用来处理大量资源，因为我们可以用它来对事物任意分组和创建集合。每当需要描述、解决或选择某物时，它都是基于标签完成的：\n端点 被分配了从容器运行时、编排系统或其他来源派生的标签。 网络策略 根据标签选择允许通信的 端点对，策略本身也由标签标识。 什么是标签？ 标签是一对由 key 和 value 组成的字符串。可以将标签格式化为具有 key=value 的单个字符串。key 部分是强制性的，并且必须是唯一的。这通常是通过使用反向域名概念来实现的，例如 io.cilium.mykey=myvalue。value 部分是可选的，可以省略，例如 io.cilium.mykey.\n键名通常应由字符集组成 [a-z0-9-.]。\n当使用标签选择资源时，键和值都必须匹配，例如，当一个策略应该应用于所有带有标签 my.corp.foo 的端点时，标签 my.corp.foo=bar 不会与该选择器匹配。\n标签来源 标签可以来自各种来源。例如，端点 将通过本地容器运行时派生与容器关联的标签，以及与 Kubernetes 提供的 pod  …","relpermalink":"/cilium-handbook/concepts/terminology/","summary":"标签 标签（Label）是一种通用的、灵活的和高度可扩展的方式，可以用来处理大量资源，因为我们可以用它来对事物任意分组和创建集合。每当需要描述、解决或选择某物时，它都是基于标签完成的： 端点 被分配了从容器","title":"Cilium 术语说明"},{"content":"IP 地址管理（IPAM）负责分配和管理由 Cilium 管理的网络端点（容器和其他）使用的 IP 地址。Cilium 支持以下各种 IPAM 模式，以满足不同用户的需求。\n集群范围（默认） Kubernetes 主机范围 Azure IPAM AWS ENI Google Kubernetes Engine CRD 支持 集群范围 集群范围 IPAM 模式将 PodCIDR 分配给每个节点，并使用每个节点上的主机范围分配器分配 IP。因此它类似于 Kubernetes 主机范围模式。区别在于 Kubernetes 不是通过 Kubernetes v1.Node资源分配每个节点的 PodCIDR，而是 Cilium Operator 通过 v2.CiliumNode 资源管理每个节点的 PodCIDR。这种模式的优点是它不依赖于 Kubernetes 被配置为分发每个节点的 PodCIDR。\n架构 架构图 如果无法将 Kubernetes 配置为分发 PodCIDR 或需要更多控制，这将非常有用。\n在这种模式下，Cilium 代理将在启动时等待，直到 PodCIDRs …","relpermalink":"/cilium-handbook/networking/ipam/","summary":"IP 地址管理（IPAM）负责分配和管理由 Cilium 管理的网络端点（容器和其他）使用的 IP 地址。Cilium 支持以下各种 IPAM 模式，以满足不同用户的需求。 集群范围（默认） Kubernetes 主机范围 Azure IPAM AWS ENI Google Kubernetes Engine CRD 支持 集群范围 集群范围 IPAM","title":"IP 地址管理（IPAM）"},{"content":"部署 标准 Cilium Kubernetes 部署的配置包括几个 Kubernetes 资源：\nDaemonSet 资源：描述部署到每个 Kubernetes 节点的 Cilium pod。这个 pod 运行 cilium-agent 和相关的守护进程。这个 DaemonSet 的配置包括指示 Cilium docker 容器的确切版本（例如 v1.0.0）的镜像标签和传递给 cilium-agent 的命令行选项。 资源：描述传递给 cilium-agent 的ConfigMap 常用配置值，例如 kvstore 端点和凭据、启用/禁用调试模式等。 ServiceAccount、ClusterRole 和 ClusterRoleBindings 资源：当启用 Kubernetes RBAC 时，cilium-agent` 用于访问 Kubernetes API 服务器的身份和权限。 资源：如果 Secret 需要，描述用于访问 etcd kvstore 的凭据。 现有 Pod 的联网 如果在部署 Cilium 之前 pod 已经在运行 DaemonSet，这些 pod …","relpermalink":"/cilium-handbook/kubernetes/concepts/","summary":"部署 标准 Cilium Kubernetes 部署的配置包括几个 Kubernetes 资源： DaemonSet 资源：描述部署到每个 Kubernetes 节点的 Cilium pod。这个 pod 运行 cilium-agent 和相关的守护进程。这个 DaemonSet 的配置包括指示 Cilium docker 容器的确切版本（例如 v1.0.0）的镜像标签和传递给 cilium-agent 的命令行选项。","title":"概念"},{"content":"Kubernetes 等容器管理系统部署了一个网络模型，该模型为每个 pod（容器组）分配一个单独的 IP 地址。这确保了架构的简单性，避免了不必要的网络地址转换（NAT），并为每个单独的容器提供了全范围的端口号以供使用。这种模型的逻辑结果是，根据集群的大小和 pod 的总数，网络层必须管理大量的 IP 地址。\n在传统上，安全策略基于 IP 地址过滤器。下面是一个简单的例子。如果所有具有标签 role=frontend 的 pod 应该被允许发起与所有具有标签 role=backend 的 pod 的连接，那么每个运行至少一个具有标签 role=backend 的 pod 的集群节点必须安装一个相应的过滤器，允许所有 role=frontend pod 的所有 IP 地址发起与所有本地 role=backend pod 的 IP 地址的连接。所有其他的连接请求都应该被拒绝。这可能看起来像这样。如果目标地址是 10.1.1.2，那么只有当源地址是下列之一时才允许连接 [10.1.2.2,10.1.2.3,20.4.9.1]。\n每次启动或停止带有 role=frontend …","relpermalink":"/cilium-handbook/security/identity/","summary":"Kubernetes 等容器管理系统部署了一个网络模型，该模型为每个 pod（容器组）分配一个单独的 IP 地址。这确保了架构的简单性，避免了不必要的网络地址转换（NAT），并为每个单独的容器提供了全范围的端口号以供使用。这种模","title":"基于身份"},{"content":"虽然监控数据路径状态提供对数据路径状态的自省，但默认情况下它只会提供对三层/四层数据包事件的可视性。如果配置了 七层示例，则可以查看七层协议，但这需要编写每个选定端点的完整策略。为了在不配置完整策略的情况下获得对应用程序的更多可视性，Cilium 提供了一种在与 Kubernetes 一起运行时通过注解来规定可视性的方法。\n可视性信息由注解中以逗号分隔的元组列表表示：\n\u0026lt;{Traffic Direction}/{L4 Port}/{L4 Protocol}/{L7 Protocol}\u0026gt;\n例如：\n\u0026lt;Egress/53/UDP/DNS\u0026gt;,\u0026lt;Egress/80/TCP/HTTP\u0026gt; 为此，你可以在 Kubernetes YAML 中或通过命令行提供注释，例如：\nkubectl annotate pod foo -n bar io.cilium.proxy-visibility=\u0026#34;\u0026lt;Egress/53/UDP/DNS\u0026gt;,\u0026lt;Egress/80/TCP/HTTP\u0026gt;\u0026#34; Cilium 将拾取 pod 已收到这些注释，并将透明地将流量重定向到代理，以便显示 cilium monitor 流量的输出被重定向 …","relpermalink":"/cilium-handbook/policy/visibility/","summary":"虽然监控数据路径状态提供对数据路径状态的自省，但默认情况下它只会提供对三层/四层数据包事件的可视性。如果配置了 七层示例，则可以查看七层协议，但这需要编写每个选定端点的完整策略。为了在不配置完整策略的情","title":"七层可视性"},{"content":"端点到端点 首先，我们使用可选的七层出口和入口策略显示本地端点到端点的流程。随后是启用了套接字层强制的同一端点到端点流。为 TCP 流量启用套接字层实施后，启动连接的握手将遍历端点策略对象，直到 TCP 状态为 ESTABLISHED。然后在建立连接后，只需要七层策略对象。\n端点到端点的流程 端点到出口 接下来，我们使用可选的 overlay 网络显示本地端点到出口。在可选的覆盖网络中，网络流量被转发到与 overlay 网络对应的 Linux 网络接口。在默认情况下，overlay 接口名为 cilium_vxlan。与上面类似，当启用套接字层强制并使用七层代理时，我们可以避免在端点和 TCP 流量的七层策略之间运行端点策略块。如果启用，可选的 L3 加密块将加密数据包。\n端点到出口的流程 入口到端点 最后，我们还使用可选的 overlay 网络显示到本地端点的入口。与上述套接字层强制类似，可用于避免代理和端点套接字之间的一组策略遍历。如果数据包在接收时被加密，则首先将其解密，然后通过正常流程进行处理。\n入口到端点流程 这样就完成了数据路径概述。更多 BPF 细节可以在 BPF …","relpermalink":"/cilium-handbook/ebpf/lifeofapacket/","summary":"端点到端点 首先，我们使用可选的七层出口和入口策略显示本地端点到端点的流程。随后是启用了套接字层强制的同一端点到端点流。为 TCP 流量启用套接字层实施后，启动连接的握手将遍历端点策略对象，直到 TCP 状态为 ESTA","title":"数据包流程"},{"content":"现在我们将探索云原生应用架构的几个主要特征，和这些特征是如何解决我们前面提到的使用云原生应用架构的动机。\n12 因素应用 12 因素应用是一系列云原生应用架构的模式集合，最初由 Heroku 提出。这些模式可以用来说明什么样的应用才是云原生应用。它们关注速度、安全、通过声明式配置扩展、可横向扩展的无状态 / 无共享进程以及部署环境的整体松耦合。如 Cloud Foundry、Heroku 和 Amazon ElasticBeanstalk 都对部署 12 因素应用进行了专门的优化。\n在 12 因素的背景下，应用（或者叫 app）指的是独立可部署单元。组织中经常把一些互相协作的可部署单元称作一个应用。\n12 因素应用遵循以下模式：\n代码库\n每个可部署 app 在版本控制系统中都有一个独立的代码库，可以在不同的环境中部署多个实例。\n依赖\nApp 应该使用适当的工具（如 Maven、Bundler、NPM）来对依赖进行显式的声明，而不该在部署环境中隐式的实现依赖。\n配置\n配置或其他随发布环境（如部署、staging、生产）而变更的部分应当作为操作系统级的环境变量注入。\n后端服务\n后端服务，例 …","relpermalink":"/migrating-to-cloud-native-application-architectures/the-rise-of-cloud-native/defining-cloud-native-architectures/","summary":"现在我们将探索云原生应用架构的几个主要特征，和这些特征是如何解决我们前面提到的使用云原生应用架构的动机。 12 因素应用 12 因素应用是一系列云原生应用架构的模式集合，最初由 Heroku 提出。这些模式可以用来说明什么样的","title":"1.2 云原生架构的定义"},{"content":"在看了基于微服务的应用所需的各种应用服务后，考虑一下提供这些服务的服务网格的架构。服务网格由两个主要部分组成：控制平面和数据平面。\n2.2.1 控制平面 控制平面有几个组件。虽然服务网格的数据面主要由作为容器运行在与应用容器相同的 Pod 中的代理组成，但控制面组件在它们自己的 Pod、节点和相关集群中运行。以下是控制平面的 各种功能：\nEnvoy sidecar 代理的服务发现和配置 自动化的密钥和证书管理 用于策略定义和收集遥测数据的 API 服务网格组件的配置摄取 管理一个到服务网格的入站连接（入站网关） 管理来自服务网格的出站连接（出口网关） 将 sidecar 代理注入那些托管应用程序微服务容器的 Pod、节点或命名空间中 总的来说，控制平面帮助管理员用配置数据填充数据平面组件，这些数据是由控制平面的策略产生的。上述功能 3 的策略可能包括网络路由策略、负载均衡策略、蓝绿部署的策略、金丝雀部署、超时、重试和断路能力。这后三项被统称为网络基础设施服务的弹性能力的特殊名称。最后要说的是与安全相关的策略（例如，认证和授权策略、TLS 建立策略等）。这些策略规则由一个模块解析，该模块 …","relpermalink":"/service-mesh-devsecops/reference-platform/service-mesh-software-architecture/","summary":"在看了基于微服务的应用所需的各种应用服务后，考虑一下提供这些服务的服务网格的架构。服务网格由两个主要部分组成：控制平面和数据平面。 2.2.1 控制平面 控制平面有几个组件。虽然服务网格的数据面主要由作为容器运行在","title":"2.2 服务网格架构"},{"content":"在本节中，我们将探讨采用云原生应用架构的组织在创建团队时需要进行的变革。这个重组背后的理论是著名的康威定律。我们的解决方案是在长周期的产品开发中，创建一个包含了各方面专业员工的团队，而不是将他们分离在单一的团队中，例如测试人员。\n业务能力团队 设计系统的组织，最终产生的设计等同于组织之内、之间的沟通结构。\n——Melvyn Conway\n我们已经讨论了“从孤岛到 DevOps”将 IT 组织成专门的信息孤岛的做法。我们很自然地创造了这些孤岛，并把个体放到与这些孤岛一致的团队中。但是当我们需要构建一个新的软件的时候会怎么样？\n一个很常见的做法是成立一个项目团队。该团队向项目经理汇报，然后项目经理与各种孤岛合作，为项目所需的各个专业领域寻求“资源”。正如康威定律所言，这些团队将很自然地在系统中构筑起各种孤岛，我们最终得到的是联合各种孤岛相对应的孤立模块的架构：\n数据访问层 服务层 Web MVC 层 消息层 等等 这些层次中的每一层都跨越了多个业务能力领域，使得在其之上的创新和部署独立于其他业务能力的新功能变的非常困难。 …","relpermalink":"/migrating-to-cloud-native-application-architectures/changes-needed/organizational-change/","summary":"在本节中，我们将探讨采用云原生应用架构的组织在创建团队时需要进行的变革。这个重组背后的理论是著名的康威定律。我们的解决方案是在长周期的产品开发中，创建一个包含了各方面专业员工的团队，而不是将他们分离在","title":"2.2 组织变革"},{"content":"DevSecOps 是一个敏捷的、自动化的开发和部署过程，它使用称为 CI/CD 管道的原语，在自动化工具的帮助下，将软件从构建阶段带到部署阶段，最后到运行时间 / 操作阶段。这些管道是将开发者的源代码带过各个阶段的工作流程，如构建、测试、打包、交付，以及在各个阶段由测试工具支持的部署。\nDevSecOps 平台是指各种 CI/CD 管道（针对每种代码类型）运行的资源集合。至少，这个平台由以下部分组成。\n(a) 管道软件\nCI 软件 —— 从代码库中提取代码，调用构建软件，调用测试工具，并将测试后的工件存储到图像注册表中。 CD 软件 —— 拉出工件、软件包，并根据 IaC 中的计算、网络和存储资源描述，部署软件包。 (b) SDLC 软件\n构建工具（例如，IDE） 测试工具（SAST、DAST、SCA） (c) 存储库\n源代码库（如 GitHub） 容器镜像存储库或注册表 (d) 可观测性或监测工具\n日志和日志聚合工具 产生指标的工具 追踪工具（应用程序的调用顺序） 可视化工具（结合上述数据生成仪表盘 / 警报）。 在 DevSecOps 平台中，通过内置的设计功能（如零信任）和使用 …","relpermalink":"/service-mesh-devsecops/devsecops/devsecops-platform/","summary":"DevSecOps 是一个敏捷的、自动化的开发和部署过程，它使用称为 CI/CD 管道的原语，在自动化工具的帮助下，将软件从构建阶段带到部署阶段，最后到运行时间 / 操作阶段。这些管道是将开发者的源代码带过各个阶段的工作流程，如构建、","title":"3.2 DevSecOps 平台"},{"content":"当我们开始构建由微服务组成的分布式系统时，我们还会遇到在开发单体应用时通常不会遇到的非功能性要求。有时，使用物理定律就可以解决这些问题，例如一致性、延迟和网络分区问题。然而，脆弱性和易控性的问题通常可以使用相当通用的模式来解决。在本节中，我们将介绍帮助我们解决这些问题的方法。\n这些方法来自于 Spring Cloud 项目和 Netflix OSS 系列项目的组合。\n版本化和分布式配置 在“12 因素应用“中我们讨论过通过操作系统级环境变量为应用注入对应的配置，强调了这种配置管理方式的重要性。这种方式特别适合简单的系统，但是，当系统扩大后，有时我们还需要附加的配置能力：\n为调试一个生产上的问题而变更运行的应用程序日志级别 更改 message broker 中接收消息的线程数 报告所有对生产系统配置所做的更改以支持审计监管 运行中的应用切换功能开关 保护配置中的机密信息（如密码） 为了支持这些特性，我们需要配置具有以下特性的配置管理方法：\n版本控制 可审计 加密 在线刷新 Spring Cloud 项目中包含的一个可提供这些功能的配置服务器。此配置服务器通过 Git …","relpermalink":"/migrating-to-cloud-native-application-architectures/migration-cookbook/distributed-systems-recipes/","summary":"当我们开始构建由微服务组成的分布式系统时，我们还会遇到在开发单体应用时通常不会遇到的非功能性要求。有时，使用物理定律就可以解决这些问题，例如一致性、延迟和网络分区问题。然而，脆弱性和易控性的问题通常可","title":"3.2 使用分布式系统"},{"content":"应用程序代码和应用服务代码驻留在容器编排和资源管理平台中，而实现与之相关的工作流程的 CI/CD 软件通常驻留在同一平台中。应使用第 4.6 节所述的步骤对该管道进行保护，该管道控制下的应用程序代码应接受第 4.8 节所述的安全测试。此外，应用程序所在的调度平台本身应使用运行时安全工具（如 Falco）进行保护，该工具可以实时读取操作系统内核日志、容器日志和平台日志，并根据威胁检测规则引擎对其进行处理，以提醒用户注意恶意行为（例如，创建有特权的容器、未经授权的用户读取敏感文件等）。它们通常有一套默认（预定义）的规则，可以在上面添加自定义规则。在平台上安装它们，可以为集群中的每个节点启动代理，这些代理可以监控在该节点的各个 Pod 中运行的容器。这种类型的工具的优点是，它补充了现有平台的本地安全措施，如访问控制模型和 Pod 安全策略，通过实际检测它们的发生来 防止漏洞。\n","relpermalink":"/service-mesh-devsecops/implement/ci-cd-pipeline-for-application-code-and-application-services-code/","summary":"应用程序代码和应用服务代码驻留在容器编排和资源管理平台中，而实现与之相关的工作流程的 CI/CD 软件通常驻留在同一平台中。应使用第 4.6 节所述的步骤对该管道进行保护，该管道控制下的应用程序代码应接受第 4.8 节所述的安全","title":"4.2 应用程序代码和应用服务代码的 CI/CD 管道"},{"content":"作者\nCybersecurity and Infrastructure Security Agency (CISA)\nNational Security Agency (NSA) Cybersecurity Directorate Endpoint Security\n联系信息\n客户要求 / 一般网络安全问题。\n网络安全需求中心，410-854-4200，Cybersecurity_Requests@nsa.gov。\n媒体咨询 / 新闻台\n媒体关系，443-634-0721，MediaRelations@nsa.gov。\n关于事件响应资源，请联系 CISA：CISAServiceDesk@cisa.dhs.gov。\n中文版\n关于本书中文版的信息请联系 Jimmy Song：jimmysong@jimmysong.io。\n宗旨\n国家安全局和 CISA 制定本文件是为了促进其各自的网络安全，包括其制定和发布网络安全规范和缓解措施的责任。这一信息可以被广泛分享，以触达所有适当的利益相关者。\n","relpermalink":"/kubernetes-hardening-guidance/publication-information/","summary":"作者 Cybersecurity and Infrastructure Security Agency (CISA) National Security Agency (NSA) Cybersecurity Directorate Endpoint Security 联系信息 客户要求 / 一般网络安全问题。 网络安全需求中心，410-854-4200，Cybersecurity_Requests@nsa.gov。 媒体咨询 / 新闻台 媒体关系，","title":"出版信息"},{"content":"基础设施技术的历史向来引人入胜。由于基础设施的规模巨大，它已经经历了一次快速的颠覆性变革。除了计算机和互联网的早期，基础设施可谓日新月异。这些创新使基础架构更快，更可靠，更有价值。\n有些公司的人将基础设施推到了极限，他们已经找到了自动化和抽象的方法，提取基础设施更多商业价值。通过提供灵活的可用资源，他们将曾经是昂贵的成本中心转变为所需的商业公用事业。\n然而，公共事业公司很少为企业提供财务价值，这意味着基础设施往往被忽略并被视为不必要的成本。这使得投入创新或改进的时间和金钱很少。\n这样一个如此简单且令人着迷的业务栈怎么可以被轻易忽略？当基础设施出现故障时，业务显然会受到重视，那么为什么基础设施很难改善呢？\n基础设施已经达到了使消费者都感到无聊的成熟度。然而，它的潜力和新挑战又激发了实施者和工程师们新的激情。\n扩展基础设施并使用新的业务方式让来自不同行业的工程师都能找到解决方案。开源软件（OSS）和社区间的互相协作，这股力量又促使了创新的激增。\n如果管理得当，今天基础设施和应用方面的挑战将会不一样。这使得基础设施建设者和维护人员可以取得进展并开展新的有意义的工作。\n有些公司克服了诸如可扩展 …","relpermalink":"/cloud-native-infra/introduction/","summary":"基础设施技术的历史向来引人入胜。由于基础设施的规模巨大，它已经经历了一次快速的颠覆性变革。除了计算机和互联网的早期，基础设施可谓日新月异。这些创新使基础架构更快，更可靠，更有价值。 有些公司的人将基础设","title":"介绍"},{"content":"云原生应用已经发展成为一个标准化的架构，由以下部分组成。\n多个松散耦合的组件被称为微服务（通常或典型地以容器形式实现）。 一个应用服务基础设施，为用户、服务和设备提供安全通信、认证和授权等服务（例如，服务网格）。 由于安全、商业竞争力和其固有的结构（松散耦合的应用组件），这类应用需要一个不同的应用、部署和运行时监控范式 —— 统称为软件生命周期范式。DevSecOps（分别由开发、安全和运维的首字母缩写组成）是这些应用的开发、部署和运维的促进范式之一，其基本要素包括持续集成、持续交付和持续部署（CI/CD）管道。\nCI/CD 管道是将开发人员的源代码通过各个阶段的工作流程，如构建、功能测试、安全扫描漏洞、打包和部署，由带有反馈机制的自动化工具支持。在本文中，应用环境中涉及的整个源代码集被分为五种代码类型：\n应用代码，它体现了执行一个或多个业务功能的应用逻辑。 应用服务代码，用于服务，如会话建立、网络连接等。 基础设施即代码，它是以声明性代码的形式存在的计算、网络和存储资源。 策略即代码，这是运行时策略（例如，零信任），以声明性代码的形式表达。 可观测性即代码，用于持续监测应用程序的健康 …","relpermalink":"/service-mesh-devsecops/executive-summary/","summary":"云原生应用已经发展成为一个标准化的架构，由以下部分组成。 多个松散耦合的组件被称为微服务（通常或典型地以容器形式实现）。 一个应用服务基础设施，为用户、服务和设备提供安全通信、认证和授权等服务（例如，服务","title":"执行摘要"},{"content":"云原生一词已经被过度的采用，很多软件都号称是云原生，很多打着云原生旗号的会议也如雨后春笋般涌现。\n云原生本身甚至不能称为是一种架构，它首先是一种基础设施，运行在其上的应用称作云原生应用，只有符合云原生设计哲学的应用架构才叫云原生应用架构。\n云原生的设计理念 云原生系统的设计理念如下：\n面向分布式设计（Distribution）：容器、微服务、API 驱动的开发； 面向配置设计（Configuration）：一个镜像，多个环境配置； 面向韧性设计（Resistancy）：故障容忍和自愈； 面向弹性设计（Elasticity）：弹性扩展和对环境变化（负载）做出响应； 面向交付设计（Delivery）：自动拉起，缩短交付时间； 面向性能设计（Performance）：响应式，并发和资源高效利用； 面向自动化设计（Automation）：自动化的 DevOps； 面向诊断性设计（Diagnosability）：集群级别的日志、metric 和追踪； 面向安全性设计（Security）：安全端点、API Gateway、端到端加密； 以上的设计理念很多都是继承自分布式应用的设计理念。虽然有如此多 …","relpermalink":"/cloud-native-handbook/intro/cloud-native-philosophy/","summary":"云原生一词已经被过度的采用，很多软件都号称是云原生，很多打着云原生旗号的会议也如雨后春笋般涌现。 云原生本身甚至不能称为是一种架构，它首先是一种基础设施，运行在其上的应用称作云原生应用，只有符合云原生设","title":"云原生的设计哲学"},{"content":"云原生社区是由 宋净超（Jimmy Song） 于 2020 年 5 月发起的，企业中立的云原生终端用户社区。社区秉持“共识、共治、共建、共享”的原则。社区的宗旨是：连接、中立、开源。立足中国，面向世界，企业中立，关注开源，回馈开源。了解更多请访问云原生社区官网：https://cloudnative.to。\n成立背景 Software is eating the world. —— Marc Andreessen\n“软件正在吞噬这个世界”已被大家多次引用，随着云原生（Cloud Native）的崛起，我们想说的是“Cloud Native is eating the software”。随着越来越多的企业将服务迁移上云，企业原有的开发模式以及技术架构已无法适应云的应用场景，其正在被重塑，向着云原生的方向演进。\n那么什么是云原生？云原生是一系列架构、研发流程、团队文化的最佳实践组合，以此支撑更快的创新速度、极致的用户体验、稳定可靠的用户服务、高效的研发效率。开源社区与云原生的关系密不可分，正是开源社区尤其是终端用户社区的存在，极大地促进了以容器、服务网格、微服务等为代表的云原生技术的持 …","relpermalink":"/cloud-native-handbook/community/cnc/","summary":"云原生社区是由 宋净超（Jimmy Song） 于 2020 年 5 月发起的，企业中立的云原生终端用户社区。社区秉持“共识、共治、共建、共享”的原则。社区的宗旨是：连接、中立、开源。立足中国，面向世界，企业中立，关注开","title":"云原生社区（中国）"},{"content":"Istio 是一个服务网格的开源实现。Istio 支持以下功能。\n流量管理\n利用配置，我们可以控制服务间的流量。设置断路器、超时或重试都可以通过简单的配置改变来完成。\n可观测性\nIstio 通过跟踪、监控和记录让我们更好地了解你的服务，它让我们能够快速发现和修复问题。\n安全性\nIstio 可以在代理层面上管理认证、授权和通信的加密。我们可以通过快速的配置变更在各个服务中执行政策。\nIstio 组件 Istio 服务网格有两个部分：数据平面和控制平面。\n在构建分布式系统时，将组件分离成控制平面和数据平面是一种常见的模式。数据平面的组件在请求路径上，而控制平面的组件则帮助数据平面完成其工作。\nIstio 中的数据平面由 Envoy 代理组成，控制服务之间的通信。网格的控制平面部分负责管理和配置代理。\nIstio 架构 Envoy（数据平面） Envoy 是一个用 C++ 开发的高性能代理。Istio 服务网格将 Envoy 代理作为一个 sidecar 容器注入到你的应用容器旁边。然后该代理拦截该服务的所有入站和出站流量。注入的代理一起构成了服务网格的数据平面。\nEnvoy 代理也是唯一与 …","relpermalink":"/cloud-native-handbook/service-mesh/what-is-istio/","summary":"Istio 是一个服务网格的开源实现。Istio 支持以下功能。 流量管理 利用配置，我们可以控制服务间的流量。设置断路器、超时或重试都可以通过简单的配置改变来完成。 可观测性 Istio 通过跟踪、监控和记录让我们更好地了解你的","title":"什么是 Istio?"},{"content":"可观测性行业正处在巨变中。\n传统上，我们观察系统时使用的是一套筒仓式的、独立的工具，其中大部分包含结构不良（或完全非结构化）的数据。这些独立的工具也是垂直整合的。工具、协议和数据格式都属于一个特定的后端或服务，不能互换。这意味着更换或采用新的工具需要耗费时间来更换整个工具链，而不仅仅是更换后端。\n这种孤立的技术格局通常被称为可观测性的 “三大支柱”：日志、度量和（几乎没有）追踪（见图 1-1）。\n日志\n记录构成事务的各个事件。\n度量\n记录构成一个事务的事件的集合。\n追踪\n测量操作的延迟和识别事务中的性能瓶颈，或者类似的东西。传统上，许多组织并不使用分布式追踪，许多开发人员也不熟悉它。\n图 1-1：可观测性的 \u0026amp;ldquo;三大支柱\u0026amp;rdquo;。 我们用这种方法工作了很久，以致于我们不常质疑它。但正如我们将看到的，“三大支柱” 并不是一种正确的结构化的可观测性方法。事实上，这个术语只是描述了某些技术碰巧被实现的方式，它掩盖了关于如何实际使用我们的工具的几个基本事实。\n什么是事务和资源？ 在我们深入探讨不同可观测性范式的利弊之前，重要的是要定义我们所观察的是什么。我们最感兴趣的分布式系统 …","relpermalink":"/opentelemetry-obervability/history/","summary":"第 1 章：可观测性的历史","title":"第 1 章：可观测性的历史"},{"content":"注意：在考虑这些要点时，请谨记“Code Review 标准”。\n设计 审查中最重要的是 CL 的整体设计。CL 中各种代码的交互是否有意义？此变更是属于您的代码库（codebase）还是属于库（library）？它是否与您系统的其他部分很好地集成？现在是添加此功能的好时机吗？\n功能 这个 CL 是否符合开发者的意图？开发者的意图对代码的用户是否是好的？ “用户”通常都是最终用户（当他们受到变更影响时）和开发者（将来必须“使用”此代码）。\n大多数情况下，我们希望开发者能够很好地测试 CL，以便在审查时代码能够正常工作。但是，作为审查者，仍然应该考虑边缘情况，寻找并发问题，尝试像用户一样思考，并确保您单纯透过阅读方式审查时，代码没有包含任何 bug。\n当要检查 CL 的行为会对用户有重大影响时，验证 CL 的变化将变得十分重要。例如 UI 变更。当您只是阅读代码时，很难理解某些变更会如何影响用户。如果在 CL 中打 patch 或自行尝试这样的变更太不方便，您可以让开发人员为您提供功能演示。\n另一个在代码审查期间特别需要考虑功能的时机，就是如果 CL 中存在某种并行编程，理论上可能导致死 …","relpermalink":"/eng-practices/review/reviewer/looking-for/","summary":"注意：在考虑这些要点时，请谨记“Code Review 标准”。 设计 审查中最重要的是 CL 的整体设计。CL 中各种代码的交互是否有意义？此变更是属于您的代码库（codebase）还是属于库（library）？它是否与您系","title":"Code Review 要点"},{"content":"由于 eBPF 允许在 Linux 内核中运行自定义代码，在解释 eBPF 之前我需要确保你对内核的作用有所了解。然后我们将讨论为什么在修改内核行为这件事情上，eBPF 改变了游戏规则。\nLinux 内核 Linux 内核是应用程序和它们所运行的硬件之间的软件层。应用程序运行在被称为用户空间的非特权层，它不能直接访问硬件。相反，应用程序使用系统调用（syscall）接口发出请求，要求内核代表它行事。这种硬件访问可能涉及到文件的读写，发送或接收网络流量，或者只是访问内存。内核还负责协调并发进程，使许多应用程序可以同时运行。\n应用程序开发者通常不直接使用系统调用接口，因为编程语言给了我们更高级别的抽象和标准库，开发者更容易掌握这些接口。因此，很多人都不知道在程序运行时内核做了什么。如果你想了解内核调用频率，你可以使用 strace 工具来显示程序所做的所有系统调用。这里有一个例子，用 cat 从文件中读取 hello 这个词并将其写到屏幕上涉及到 100 多个系统调用：\nliz@liz-ebpf-demo-1:~$ strace -c cat liz.txt hello % time …","relpermalink":"/what-is-ebpf/changing-the-kernel-is-hard/","summary":"由于 eBPF 允许在 Linux 内核中运行自定义代码，在解释 eBPF 之前我需要确保你对内核的作用有所了解。然后我们将讨论为什么在修改内核行为这件事情上，eBPF 改变了游戏规则。 Linux 内核 Linux 内核是应用程序和它们所运行的硬件之间的软","title":"第二章：修改内核很困难"},{"content":"为什么提交小型 CL? 小且简单的 CL 是指：\n审查更快。审查者更容易抽多次五分钟时间来审查小型 CL，而不是留出 30 分钟来审查一个大型 CL。 审查得更彻底。如果是大的变更，审查者和提交者往往会因为大量细节的讨论翻来覆去而感到沮丧——有时甚至到了重要点被遗漏或丢失的程度。 不太可能引入错误。由于您进行的变更较少，您和您的审查者可以更轻松有效地推断 CL 的影响，并查看是否已引入错误。 如果被拒绝，减少浪费的工作。如果您写了一个巨大的 CL，您的评论者说整个 CL 的方向都错误了，你就浪费了很多精力和时间。 更容易合并。处理大型 CL 需要很长时间，在合并时会出现很多冲突，并且必须经常合并。 更容易设计好。打磨一个小变更的设计和代码健康状况比完善一个大变更的所有细节要容易得多。 减少对审查的阻碍。发送整体变更的自包含部分可让您在等待当前 CL 审核时继续编码。 更简单的回滚。大型 CL 更有可能触及在初始 CL 提交和回滚 CL 之间更新的文件，从而使回滚变得复杂（中间的 CL 也可能需要回滚）。 请注意，审查者可以仅凭 CL 过大而自行决定完全拒绝您的变更。通常他们会感谢您的贡 …","relpermalink":"/eng-practices/review/developer/small-cls/","summary":"为什么提交小型 CL? 小且简单的 CL 是指： 审查更快。审查者更容易抽多次五分钟时间来审查小型 CL，而不是留出 30 分钟来审查一个大型 CL。 审查得更彻底。如果是大的变更，审查者和提交者往往会因为大量细节的讨论翻来覆去","title":"小型 CL"},{"content":"本教程在SPIRE Envoy-X.509 教程的基础上构建，演示如何使用 SPIRE 代替 X.509 SVID 进行工作负载的 JWT SVID 身份验证。在这个教程中展示了实现 JWT SVID 身份验证所需的更改，因此你应该首先运行或至少阅读 X.509 教程。\n为了说明 JWT 身份验证，我们在 Envoy X.509 教程中使用的每个服务中添加了 sidecar。每个 sidecar 都充当 Envoy 的外部授权过滤器。\n如图所示，前端服务通过 Envoy 实例连接到后端服务，这些服务之间通过 Envoy 建立的 mTLS 连接进行通信。Envoy 通过携带的 JWT-SVID 进行身份验证的 HTTP 请求通过 mTLS 连接发送，并由 SPIRE Agent 提供和验证。\n在本教程中，你将学习如何：\n将 Envoy JWT Auth Helper gRPC 服务添加到 Envoy X.509 教程中现有的前端和后端服务中 将外部授权过滤器添加到 Envoy 配置中，将 Envoy 连接到 Envoy JWT Auth Helper 在 SPIRE Server …","relpermalink":"/spiffe-and-spire/examples/envoy-jwt/","summary":"本教程在SPIRE Envoy-X.509 教程的基础上构建，演示如何使用 SPIRE 代替 X.509 SVID 进行工作负载的 JWT SVID 身份验证。在这个教程中展示了实现 JWT SVID 身份验证所需的更改，因此你应该首先运行或至少阅读 X.509 教程。 为了说明 JWT 身份验证，我们在 Envoy X.509","title":"使用 Envoy 和 JWT-SVID 进行安全的微服务通信"},{"content":"JWT-SVID 是 SPIFFE 规范集中的第一个基于令牌的 SVID。旨在在解决跨第 7 层边界断言身份时提供即时价值，与现有应用程序和库的兼容性是核心要求。\nJWT-SVID 是一种带有一些限制的标准 JWT 令牌。JOSE 在安全实现上一直存在困难，在安全社区中被认为是一项可能在部署和实现中引入漏洞的技术。JWT-SVID 采取措施尽量减轻这些问题，同时不破坏与现有应用程序和库的兼容性。\nJWT-SVID 是使用 JWS 紧凑序列化的 JSON Web Signature (JWS) 数据结构。不得使用 JWS JSON 序列化。\nJOSE 头 历史上，JOSE 头的密码灵活性引入了一系列流行的 JWT 实现中的漏洞。为了避免这样的陷阱，本规范限制了一些最初允许的内容。本节描述了允许的注册头以及其值。JWT-SVID JOSE 头中未描述的任何头部，无论是注册的还是私有的，都不得包含在其中。\n只支持 JWS。\n算法 alg 头必须设置为 RFC 7518 第 3.3、3.4 或 3.5 节定义的值之一。接收到 alg 参数设置为其他值的令牌的验证器必须拒绝该令牌。 …","relpermalink":"/spiffe-and-spire/standard/jwt-svid/","summary":"JWT-SVID 是 SPIFFE 规范集中的第一个基于令牌的 SVID。旨在在解决跨第 7 层边界断言身份时提供即时价值，与现有应用程序和库的兼容性是核心要求。 JWT-SVID 是一种带有一些限制的标准 JWT 令牌。JOSE 在安全实现上一直存在困难，在安全","title":"JWT SPIFFE 可验证身份文档"},{"content":"本教程展示了如何对由两个不同 SPIRE 服务器识别的两个 SPIFFE 标识的工作负载进行身份验证。\n本文的第一部分演示了如何通过显示 SPIRE 配置文件更改和 spire-server 命令来配置 SPIFFE 联邦，以设置股票报价 web 应用的前端和服务后端为例。本文的第二部分列出了你可以在此教程目录中包含的 Docker Compose 文件中运行的步骤，以显示场景的实际操作。\n在本教程中，你将学到如何：\n配置每个 SPIRE 服务器以使用 SPIFFE 身份验证和 Web PKI 身份验证公开其 SPIFFE 联邦捆绑点。 配置 SPIRE 服务器以从彼此检索信任捆绑点。 使用不同的信任域引导两个 SPIRE 服务器之间的联合。 为工作负载创建注册条目，以便它们可以与其他信任域进行联合。 先决条件 SPIFFE 联邦的基线组件包括：\n运行版本为 1.5.1 的两个 SPIRE 服务器实例。 运行版本为 1.5.1 的两个 SPIRE 代理。一个连接到一个 SPIRE 服务器，另一个连接到另一个 SPIRE 服务器。 两个需要通过 mTLS 进行通信的工作负载，并使用工作负 …","relpermalink":"/spiffe-and-spire/architecture/federation/","summary":"本教程展示了如何对由两个不同 SPIRE 服务器识别的两个 SPIFFE 标识的工作负载进行身份验证。 本文的第一部分演示了如何通过显示 SPIRE 配置文件更改和 spire-server 命令来配置 SPIFFE 联邦，以设置股票报价 web 应用的前端和服务后端为例。本文的第二部分","title":"SPIRE 联邦：验证来自不同 SPIRE 服务器的工作负载"},{"content":"本文指导你如何在 Linux 和 Kubernetes 上安装 SPIRE Agent。\n步骤 1：获取 SPIRE 二进制文件 可以在 SPIRE 下载页面 找到预构建的 SPIRE 发行版。tarball 包含服务器和 Agent 二进制文件。\n如果愿意，也可以从源代码 构建 SPIRE。\n步骤 2：安装服务器和 Agent 本入门指南描述了如何在同一节点上安装服务器和 Agent。在典型的生产部署中，服务器安装在一个节点上，而一个或多个 Agent 安装在不同的节点上。\n要安装服务器和 Agent：\n从 SPIRE 下载页面 获取最新的 tarball，然后使用以下命令将其解压到 /opt/spire 目录中：\nwget https://github.com/spiffe/spire/releases/download/v1.8.2/spire-1.8.2-linux-amd64-musl.tar.gz tar zvxf spire-1.8.2-linux-amd64-musl.tar.gz sudo cp -r spire-1.8.2/. /opt/spire/ 为了方便起见， …","relpermalink":"/spiffe-and-spire/installation/install-agent/","summary":"本文指导你如何在 Linux 和 Kubernetes 上安装 SPIRE Agent。 步骤 1：获取 SPIRE 二进制文件 可以在 SPIRE 下载页面 找到预构建的 SPIRE 发行版。tarball 包含服务器和 Agent 二进制文件。 如果愿意，也可以从源代码 构建 SPIRE。 步骤 2：安装服务","title":"安装 SPIRE 代理"},{"content":"本文指导你如何编写与 SPIFFE SVID 相关的代码。\nSPIRE 等符合 SPIFFE 的身份提供者将通过 SPIFFE Workload API 公开 SPIFFE 可验证身份文档（SVID）。工作负载可以使用从此 API 检索到的 SVID 来验证消息的来源或在两个工作负载之间建立相互 TLS 安全通道。\n与 Workload API 交互 开发需要与 SPIFFE 进行交互的新工作负载的开发人员可以直接与 SPIFFE Workload API 进行交互，以便：\n检索工作负载的身份，描述为 SPIFFE ID，例如 spiffe://prod.acme.com/billing/api 代表工作负载生成短期密钥和证书，具体包括： 与该 SPIFFE ID 相关联的私钥，可用于代表工作负载签署数据。 对应的短期 X.509 证书 - 一种称为 X509-SVID 的证书。该证书可用于建立 TLS 或以其他方式对其他工作负载进行身份验证。 一组证书 - 称为信任捆绑包（trust bundle） - …","relpermalink":"/spiffe-and-spire/configuration/svids/","summary":"本文指导你如何编写与 SPIFFE SVID 相关的代码。 SPIRE 等符合 SPIFFE 的身份提供者将通过 SPIFFE Workload API 公开 SPIFFE 可验证身份文档（SVID）。工作负载可以使用从此 API 检索到的 SVID 来验证消息的来源或在两个工作负载之间建立相互 TLS 安全通道。 与 Workload API 交","title":"使用 SVID"},{"content":"本文描述了创建 Azure 中的应用程序以允许 TSB 使用云帐户进行 OIDC 以及从 Azure AD 同步用户和组的步骤。\n创建应用程序 登录到 Azure 门户，然后转到Active Directory \u0026gt; 应用程序注册 \u0026gt; 新应用程序注册。\n将应用程序类型设置为 Web，并将重定向 URI 配置为指向 TSB 地址以及**/iam/v2/oidc/callback**端点。\n配置应用程序秘密 一旦应用程序创建完毕，转到证书和密码以创建一个用于在 TSB 中使用的客户端秘密：\n配置名称和到期时间，然后单击添加。\n一旦添加完成，你将能够看到秘密的Value。复制它，因为稍后将使用它，并且将不再显示。\n配置 OIDC 令牌 一旦创建了秘密，转到身份验证菜单项，然后单击添加平台以配置 OIDC 令牌。\n选择Web\n在接下来的屏幕中，将重定向 URI 配置为指向 TSB 地址以及**/iam/v2/oidc/callback**端点，选择两个令牌（访问令牌和 ID 令牌），可以选择启用 tctl 设备代码流的移动和桌面流程：\n配置应用程序权限 一旦为 OIDC 配置了应用程序，就需要授 …","relpermalink":"/tsb/operations/users/oidc-azure/","summary":"本文描述了创建 Azure 中的应用程序以允许 TSB 使用云帐户进行 OIDC 以及从 Azure AD 同步用户和组的步骤。 创建应用程序 登录到 Azure 门户，然后转到Active Directory \u003e 应用程序注册 \u003e 新应用程序注册。 将应用程序类型设置为 Web，并将重定向","title":"Azure AD 作为身份提供者"},{"content":"在开始之前，你必须拥有：\nVault 1.3.1 或更新版本 Vault 注入器 0.3.0 或更新版本 设置 Vault 安装 Vault（它不必安装在 Kubernetes 集群中，但应该可以从 Kubernetes 集群内部访问）。必须将 Vault 注入器（agent-injector）安装到集群中，并配置以注入 sidecar。Helm chart v0.5.0+ 会自动完成这些工作，该 Helm chart 安装了 Vault 0.12+ 和 Vault-Injector 0.3.0+。下面的示例假定 Vault 安装在 tsb 命名空间中。\n有关更多详细信息，请参阅 Vault 文档。\nhelm install --name=vault --set=\u0026#39;server.dev.enabled=true\u0026#39; 将 Vault 服务端口转发到本地，并设置环境变量以对 API 进行身份验证\nkubectl port-forward svc/vault 8200:8200 \u0026amp; # 这将在后台运行 export VAULT_ADDR=\u0026#39;http://[::]:8200\u0026#39; export …","relpermalink":"/tsb/operations/vault/istiod-ca/","summary":"在开始之前，你必须拥有： Vault 1.3.1 或更新版本 Vault 注入器 0.3.0 或更新版本 设置 Vault 安装 Vault（它不必安装在 Kubernetes 集群中，但应该可以从 Kubernetes 集群内部访问）。必须将 Vault 注入器（agent-injector）安装到集群中，并配置以","title":"Istio CA"},{"content":"默认情况下，Istio 会将 sidecar 代理注入到应用程序的 pod 中，以便处理该 pod 的流量。这些 sidecar 需要成为特权容器，因为它们需要在 pod 网络命名空间中操作 iptables 规则，以便拦截进出该 pod 的流量。\n从安全性的角度来看，这种默认行为并不理想，因为它实际上授予了应用程序 pod 使用这些高级权限的权限。Istio 提供的替代方案是使用 CNI 插件，它在 pod 创建时处理 pod 网络命名空间的修改。\n在控制平面中启用 Istio CNI 为了在你的控制平面中启用 Istio CNI 插件，你需要编辑 ControlPlane CR 或 Helm 值，以包括 CNI 配置。\nspec: components: istio: kubeSpec: CNI: chained: true binaryDirectory: /opt/cni/bin configurationDirectory: /etc/cni/net.d traceSamplingRate: 100 hub: \u0026lt;registry-location\u0026gt; …","relpermalink":"/tsb/operations/features/istio-cni/","summary":"默认情况下，Istio 会将 sidecar 代理注入到应用程序的 pod 中，以便处理该 pod 的流量。这些 sidecar 需要成为特权容器，因为它们需要在 pod 网络命名空间中操作 iptables 规则，以便拦截进出该 pod 的流量。 从安全性的角度来看，这种默认行为并不","title":"Istio CNI"},{"content":"让我们开始 在开始之前，请确保你已经做到了以下几点：\n熟悉 TSB 概念 安装了 TSB 环境。你可以使用 TSB 演示进行快速安装。 完成了 TSB 的快速入门。本文假定你已经创建了租户并熟悉工作区和配置组，并且需要将 tctl 配置到你的 TSB 环境。 在这个示例中，将使用httpbin作为工作负载。通过 wasm 扩展执行，发送到 Ingress GW 的请求将在 HTTP 响应中添加一个头部。\n部署httpbin服务 请按照此文档中的所有说明创建httpbin服务。\n接下来的命令将假定你已经有一个组织=tetrate，租户=tetrate，工作区=httpbin，网关组=httpbin-gateway\n构建和部署 WASM 扩展 让我们使用一个已经存在的 WASM 扩展代码，该扩展将在 HTTP 响应中添加头部。 为了构建 WASM 扩展，下载存储库并按照这些说明进行操作：\nmake build.example name=http_headers 然后需要将其打包为 OCI 镜像：\ndocker build . -t docker.io/\u0026lt;your …","relpermalink":"/tsb/howto/wasm/wasm-try/","summary":"让我们开始 在开始之前，请确保你已经做到了以下几点： 熟悉 TSB 概念 安装了 TSB 环境。你可以使用 TSB 演示进行快速安装。 完成了 TSB 的快速入门。本文假定你已经创建了租户并熟悉工作区和配置组，并且需要将 tctl 配置到你的 TSB 环境。","title":"Wasm 扩展示例"},{"content":"安全性已融入 Tetrate Service Bridge（TSB）架构的方方面面。TSB 的方法建立在零信任原则之上，其中安全性是网格管理环境中每个特性和功能的首要考虑因素。本节深入探讨 TSB 如何将安全视为一等公民，并提供全面的措施来保护应用程序、支持合规性工作并防止中断。\n在本节中，你将深入了解以下关键方面：\n租赁和抽象基础设施 访问控制策略 可审计性和日志记录 安全通信的服务身份 多租户 TSB 构建了一个逻辑资源层次结构，摆脱了物理基础设施的束缚。这意味着你不再受限于单个虚拟机或 Pod 的安全考虑和操作。\n相反，你在 TSB 的管理平台上所做的任何更改都会影响到你环境中的资源集合。TSB 通过将物理基础设施抽象化，使服务配置更加安全和易于使用。\n通过将的基础设施分组成易于查看和管理的块，TSB 为你提供了清晰的资源归属视图，帮助你更好地了解资源的共享情况。共享资源可能导致更多问题，如共享故障或嘈杂的邻居，根据我们的经验，这些是复杂系统最容易出现问题的地方。TSB 简化了你对系统中共享所有权位置的理解，并协助你安全地管理它。\n资源层次结构 Tetrate Service …","relpermalink":"/tsb/concepts/security/","summary":"安全性已融入 Tetrate Service Bridge（TSB）架构的方方面面。TSB 的方法建立在零信任原则之上，其中安全性是网格管理环境中每个特性和功能的首要考虑因素。本节深入探讨 TSB 如何将安全视为一等公民，并提供全面的措施来","title":"安全"},{"content":"有时候拥有一个什么都不做的工作负载是很方便的。在这个示例中，使用已安装 curl 的容器作为 sleep 服务的基础，以便更容易进行测试。\nsleep 服务在 TSB 文档中的多个示例中使用。本文档提供了该服务的基本安装步骤。\n请确保查阅每个 TSB 文档，以获取示例正常运行所需的特定注意事项或自定义设置，因为本文档描述了最通用的安装步骤。\n以下示例假设您已经设置好了 TSB，并且已经注册了要安装 sleep 工作负载的 Kubernetes 集群。\n除非另有说明，使用 kubectl 命令的示例必须指向同一集群。在运行这些命令之前，请确保您的 kubeconfig 指向所需的集群。\n命名空间 除非另有说明，假定 sleep 服务安装在 sleep 命名空间中。如果不存在，请在目标集群中创建此命名空间。\n运行以下命令以创建命名空间（如果尚未存在）：\nkubectl create namespace sleep 此命名空间中的 sleep Pod 必须运行 Istio sidecar 代理。要自动启用对所有 Pod 的 sidecar 注入，请执行以下操作：\nkubectl label …","relpermalink":"/tsb/reference/samples/sleep-service/","summary":"有时候拥有一个什么都不做的工作负载是很方便的。在这个示例中，使用已安装 curl 的容器作为 sleep 服务的基础，以便更容易进行测试。 sleep 服务在 TSB 文档中的多个示例中使用。本文档提供了该服务的基本安装步骤。 请确保查阅每个 TSB","title":"安装 sleep"},{"content":"应用程序所有者的用户体验在他们直接使用原生 Kubernetes API 与 Kubernetes 平台交互，或者依赖于间接方法（如 CD 流水线）来部署和配置平台时几乎没有变化。\n应用程序所有者 (“Apps”) 将按以下步骤部署和公开服务：\n部署服务\n在由平台所有者提供的 Kubernetes 命名空间中部署目标服务。\n公开服务\n配置一个 Gateway 资源，通过本地 Ingress 网关公开服务。\nApps: 开始之前 在开始之前，你需要从你的管理员（平台所有者）那里获得以下信息：\nKubernetes 集群和命名空间：在所选择的命名空间中部署和管理服务所需的 API 访问权限 Tetrate 拓扑：Kubernetes 命名空间被分组为 Tetrate 工作空间，一个工作空间可以包含来自多个集群的命名空间。你需要了解拓扑，包括： Tetrate 工作空间的名称 该工作空间中的命名空间和集群 Tetrate Gateway Groups 的名称 - 通常每个工作空间每个集群有一个 Gateway Group 请注意，Tetrate 部署的最常见安全姿态是允许 Workspace …","relpermalink":"/tsb/design-guides/app-onboarding/deploy-service/","summary":"应用程序所有者的用户体验在他们直接使用原生 Kubernetes API 与 Kubernetes 平台交互，或者依赖于间接方法（如 CD 流水线）来部署和配置平台时几乎没有变化。 应用程序所有者 (“Apps”) 将按以下步骤部署和公开服务： 部署服务 在由平台所有者提供的 Kubernetes 命","title":"部署服务和配置网关规则"},{"content":"在本部分中，你将了解如何使用 TSB UI 或 tctl 创建 TSB 租户。\n先决条件 在继续之前，请确保你已完成以下任务：\n熟悉 TSB 概念。 安装 TSB 演示环境。 部署 Istio Bookinfo 示例应用程序。 使用用户界面 在左侧面板的组织下，选择租户。 单击该卡以添加新租户。 输入租户 ID tetrate 。 向你的租户提供显示名称和描述。 单击添加。 使用 tctl 创建以下 tenant.yaml 文件：\napiVersion: api.tsb.tetrate.io/v2 kind: Tenant metadata: organization: tetrate name: tetrate spec: displayName: Tetrate 使用 tctl 应用配置：\ntctl apply -f tenant.yaml 通过执行这些步骤，你将成功创建一个名为 tetrate 的 TSB 租户。该租户可用于组织和管理你的 TSB 环境。\n","relpermalink":"/tsb/quickstart/tenant/","summary":"在本部分中，你将了解如何使用 TSB UI 或 tctl 创建 TSB 租户。 先决条件 在继续之前，请确保你已完成以下任务： 熟悉 TSB 概念。 安装 TSB 演示环境。 部署 Istio Bookinfo 示例应用程序。 使用用户界面 在左侧面板的组织下，选择租户。 单击该卡以添加新","title":"创建租户"},{"content":"启动工作负载载入代理 创建文件 /etc/onboarding-agent/onboarding.config.yaml，并填入以下内容。将 \u0026lt;ONBOARDING_ENDPOINT_ADDRESS\u0026gt; 替换为之前获取的值。\napiVersion: config.agent.onboarding.tetrate.io/v1alpha1 kind: OnboardingConfiguration onboardingEndpoint: host: \u0026#34;\u0026lt;ONBOARDING_ENDPOINT_ADDRESS\u0026gt;\u0026#34; transportSecurity: tls: sni: onboarding-endpoint.example # (1) workloadGroup: # (2) namespace: bookinfo name: ratings workload: labels: version: v5 # (3) 此配置指示工作负载载入代理使用一个地址连接到工作负载载入终端点，但对 DNS 名称 onboarding-endpoint.example 验证 TLS 证书 (1)。\n代理将尝试 …","relpermalink":"/tsb/setup/workload-onboarding/quickstart/on-premise/onboard-vm/","summary":"启动工作负载载入代理 创建文件 /etc/onboarding-agent/onboarding.config.yaml，并填入以下内容。将 \u003cONBOARDING_ENDPOINT_ADDRESS\u003e 替换为之前获取的值。 apiVersion: config.agent.onboarding.tetrate.io/v1alpha1 kind: OnboardingConfiguration onboardingEndpoint: host: \"\u003cONBOARDING_ENDPOINT_ADDRESS\u003e\" transportSecurity: tls: sni: onboarding-endpoint.example # (1) workloadGroup: # (2) namespace: bookinfo name: ratings workload:","title":"从本地虚拟机上进行工作负载载入"},{"content":"TSB 能够为网关和 sidecar 都应用速率限制。在本文档中，我们将启用 sidecar 的速率限制，以控制服务之间的流量配额。\n在开始之前，请确保你已经完成以下步骤：\n熟悉 TSB 概念 安装 TSB 环境。你可以使用 TSB 演示 进行快速安装 完成了 TSB 使用快速入门。本文档假定你已经创建了租户，熟悉工作区和配置组，并配置了 tctl 到你的 TSB 环境。 启用速率限制服务器 请阅读并按照 启用速率限制服务器文档 中的说明操作。\n演示安装\n如果你使用 TSB 演示 安装，你已经有一个正在运行并且可以使用的速率限制服务，可以跳过这一部分。 如果你打算在多集群设置中使用相同的速率限制服务器，所有集群都必须指向相同的 Redis 后端和域。\n部署 httpbin 服务 请按照 本文档中的说明 创建 httpbin 服务。你可以跳过 “暴露 httpbin 服务”、“创建证书” 和 “载入 httpbin 应用程序” 部分。\n创建 TrafficSetting 创建一个 TrafficSetting 对象， …","relpermalink":"/tsb/howto/rate-limiting/service-to-service/","summary":"TSB 能够为网关和 sidecar 都应用速率限制。在本文档中，我们将启用 sidecar 的速率限制，以控制服务之间的流量配额。 在开始之前，请确保你已经完成以下步骤： 熟悉 TSB 概念 安装 TSB 环境。你可以使用 TSB 演示 进行快速安装 完成了 TSB 使用快速入","title":"服务之间的速率限制"},{"content":"Tetrate Service Bridge 默认情况下会为每个工作区部署一个入口网关。我们这样做是为了保持团队隔离，防止共享故障和以信心实现高速度。然而，在大规模部署中，由于诸如负载均衡器地址过多或网关 Pod 在低流量应用中利用率低等原因，这可能是不可取的。因此，TSB 支持配置共享入口网关部署：换句话说，个别应用团队仍然可以发布自己的配置，但它们在运行时都会配置相同的 Envoy 实例。\nTSB 允许许多团队配置单个共享网关部署。 什么是网关？ 在 Istio 中，“网关” 这个词有点令人困惑，因为它指的是几个不同的事物：\n作为 Kubernetes 入口网关运行的一组 Envoy，我们将其称为 “网关部署\u0026#34;。 Istio 配置资源，即 Istio 网关 API — 用于在运行时配置 网关部署 的端口、协议和证书。我们将其称为 “Istio 网关 API 资源\u0026#34;。 用于配置 Kubernetes 入口的 Kubernetes 网关 API — 它与 Istio 的 网关 API 资源 做相同的事情，但是是一个原生的 Kubernetes 构造。我们将其称为 “Kubernetes …","relpermalink":"/tsb/howto/gateway/shared-ingress/","summary":"Tetrate Service Bridge 默认情况下会为每个工作区部署一个入口网关。我们这样做是为了保持团队隔离，防止共享故障和以信心实现高速度。然而，在大规模部署中，由于诸如负载均衡器地址过多或网关 Pod 在低流量应用中利用率低等原因，这可","title":"共享入口网关"},{"content":"Tetrate Service Bridge 收集了大量的指标。此页面是由 Tetrate 内部运行的仪表板生成的，将根据 Tetrate 的操作经验和用户部署中学到的最佳实践定期更新。每个标题代表一个不同的仪表板，每个子标题是该仪表板上的一个面板。因此，你可能会看到多次出现相同的指标。\n本文档中描述的指标构建了一系列 Grafana 仪表板，可以从此处下载，以便将它们导入到你的 Grafana 设置中。\n指标过多，不在此文中列出，详细指标请见 Tetrate Service Bridge 文档。\n","relpermalink":"/tsb/operations/telemetry/key-metrics/","summary":"Tetrate Service Bridge 收集了大量的指标。此页面是由 Tetrate 内部运行的仪表板生成的，将根据 Tetrate 的操作经验和用户部署中学到的最佳实践定期更新。每个标题代表一个不同的仪表板，每个子标题是该仪表板上的一个面板。因此，你可能会看到多次","title":"关键指标"},{"content":"下面的 YAML 文件包含三个对象：\n用于应用程序的 Workspace 用于配置应用程序入口的 GatewayGroup 以及一个允许你配置金丝雀发布流程的 TrafficGroup 将文件存储为helloworld-ws-groups.yaml，并使用tctl应用：\nhelloworld-ws-group.yaml apiversion: api.tsb.tetrate.io/v2 kind: Workspace metadata: organization: tetrate tenant: tetrate name: helloworld-ws spec: namespaceSelector: names: - \u0026#39;*/helloworld\u0026#39; --- apiVersion: gateway.tsb.tetrate.io/v2 kind: Group metadata: organization: tetrate tenant: tetrate workspace: helloworld-ws name: helloworld-gw spec: namespaceSelector: …","relpermalink":"/tsb/howto/traffic/load-balance/","summary":"下面的 YAML 文件包含三个对象： 用于应用程序的 Workspace 用于配置应用程序入口的 GatewayGroup 以及一个允许你配置金丝雀发布流程的 TrafficGroup 将文件存储为helloworld-ws-groups.yaml，并使用tctl应用： helloworld-ws-group.yaml apiversion: api.tsb.tetrate.io/v2 kind: Workspace metadata: organization:","title":"客户端负载均衡"},{"content":"此 Chart 安装 TSB 控制平面 Operator 以将集群引入。与管理平面 Helm Chart类似，它还允许你使用TSB ControlPlane CR安装 TSB 控制平面组件，以及使其正常运行所需的所有密钥。\n在开始之前，请确保你已完成以下操作：\n检查 Helm 安装过程 已安装 TSB 管理平面 使用 tctl 登录到管理平面 安装 yq。这将用于从创建集群响应中获取 Helm 值。 隔离边界\nTSB 1.6 引入了隔离边界，允许你在 Kubernetes 集群内或跨多个集群中拥有多个 TSB 管理的 Istio 环境。隔离边界的好处之一是你可以执行控制平面的金丝雀升级。\n要启用隔离边界，你必须使用环境变量 ISTIO_ISOLATION_BOUNDARIES=true 更新 Operator 部署，并在控制平面 CR 中包括 isolationBoundaries 字段。 有关更多信息，请参见隔离边界。\n先决条件 在开始之前，你需要创建一个 集群对象 在 TSB 中表示你将安装 TSB 控制平面的集群。将 \u0026lt;cluster-name-in-tsb\u0026gt; …","relpermalink":"/tsb/setup/helm/controlplane/","summary":"此 Chart 安装 TSB 控制平面 Operator 以将集群引入。与管理平面 Helm Chart类似，它还允许你使用TSB ControlPlane CR安装 TSB 控制平面组件，以及使其正常运行所需的所有密钥。 在开始之前，请确保你已完成以下操作： 检查 Helm 安装过程 已安装 TSB 管","title":"控制平面安装"},{"content":"添加第二个 Edge Gateway 以提供 Edge 高可用性。\n我们将扩展在演示环境说明中描述的演示环境。我们将在第二个区域中添加一个额外的 Edge 集群，并部署一个 Edge Gateway：\n在 region-2 上登记一个额外的 Edge 集群 扩展 Tetrate 配置以涵盖 edge-ws，并创建第二个 edge-gwgroup-2 组 创建一个 edge 命名空间并在该命名空间中部署一个 Edge Gateway 部署一个为该 Edge Gateway 暴露服务的 Gateway 资源 Edge 和工作负载负载均衡 开始之前 在配置中有许多组件，因此在继续之前，标识并命名每个组件将非常有帮助：\ncluster-1 cluster-2 cluster-edge cluster-edge-2 AWS 区域： eu-west-1 eu-west-2 eu-west-1 eu-west-2 命名空间： bookinfo bookinfo edge edge 工作空间： bookinfo-ws bookinfo-ws edge-ws edge-ws 网络： …","relpermalink":"/tsb/design-guides/ha-multicluster/demo-2/","summary":"添加第二个 Edge Gateway 以提供 Edge 高可用性。 我们将扩展在演示环境说明中描述的演示环境。我们将在第二个区域中添加一个额外的 Edge 集群，并部署一个 Edge Gateway： 在 region-2 上登记一个额外的 Edge 集群 扩展 Tetrate 配置以涵盖 edge-ws","title":"扩展演示环境"},{"content":"你将在 AWS EC2 实例上部署 ratings 应用程序，并将其加入到服务网格中。\n创建 WorkloadGroup 执行以下命令创建一个 WorkloadGroup：\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: networking.istio.io/v1alpha3 kind: WorkloadGroup metadata: name: ratings namespace: bookinfo labels: app: ratings spec: template: labels: app: ratings class: vm cloud: aws network: aws # (1) serviceAccount: bookinfo-ratings # (2) EOF 字段 spec.template.network 设置为非空值，以指示 Istio 控制平面，你稍后将创建的 VM 没有直接连接到 Kubernetes Pods。\n字段 spec.template.serviceAccount 声明工作负载具有 Kubernetes 集 …","relpermalink":"/tsb/setup/workload-onboarding/quickstart/aws-ec2/configure-workload-onboarding/","summary":"你将在 AWS EC2 实例上部署 ratings 应用程序，并将其加入到服务网格中。 创建 WorkloadGroup 执行以下命令创建一个 WorkloadGroup： cat \u003c\u003cEOF | kubectl apply -f - apiVersion: networking.istio.io/v1alpha3 kind: WorkloadGroup metadata: name: ratings namespace: bookinfo labels: app: ratings spec: template: labels: app: ratings class: vm cloud: aws network: aws # (1) serviceAccount: bookinfo-ratings # (2) EOF 字段 spec.template.network 设置为非空","title":"配置 WorkloadGroup 和 Sidecar"},{"content":"Tetrate Service Bridge 的 tctl CLI 允许你与 TSB API 交互以应用对象的配置。本文档介绍如何使用 tctl 了解系统中资源配置的部署状态。\n资源状态 TSB 通过 ResourceStatus 跟踪配置更改的生命周期。你可以使用 tctl x status 获取它们。运行 tctl x status --help 可查看所有可能的选项。\n根据资源的配置状态计算方式，有不同类型的资源。\n资源类型 配置状态 示例 父资源 聚合其子资源的状态。 workspace, trafficgroup, gatewaygroup, securitygroup 子资源 不依赖于其他资源。 ingressgateway, egressgateway, trafficsettings 等 不可配置资源 不会直接在目标集群中实体化为配置。 organizations, tenants, users 具有依赖关系的资源 高级别资源。 applications 和 apis 资源状态可以具有多个值，这取决于其配置在TSB 组件中的传播程度。\n类型 状态 条件 子资源和不可配置 …","relpermalink":"/tsb/troubleshooting/configuration-status/","summary":"Tetrate Service Bridge 的 tctl CLI 允许你与 TSB API 交互以应用对象的配置。本文档介绍如何使用 tctl 了解系统中资源配置的部署状态。 资源状态 TSB 通过 ResourceStatus 跟踪配置更改的生命周期。你可以使用 tctl x status 获取它们。运行 tctl x status --help 可查看所有可能的选项。 根据","title":"配置状态故障排除"},{"content":"本文档描述了如何配置 Argo CD 并将 Argo Rollout 与 TSB GitOps 支持集成，以及如何使用 SkyWalking 作为金丝雀部署分析和渐进式交付自动化的指标提供者。\n在开始之前，请确保以下事项：\nArgo CD 已安装在你的集群中，并且已配置 Argo CD CLI 以连接到你的 Argo CD 服务器 Argo Rollout 已安装在你的集群中 TSB 已启动并运行，并且已为目标集群启用了 GitOps 配置 从 Git 仓库创建应用程序 使用以下命令创建一个示例应用程序。一个包含 Istio 的示例仓库，其中包含 Istio 的 bookinfo 应用程序和 TSB 配置，可以在 https://github.com/tetrateio/tsb-gitops-demo 上找到。 你可以使用 Argo CD CLI 或其 Web UI 直接从 Git 导入应用程序配置。\nargocd app create bookinfo-app --repo https://github.com/tetrateio/tsb-gitops-demo.git --path …","relpermalink":"/tsb/howto/gitops/argo-rollouts/","summary":"本文档描述了如何配置 Argo CD 并将 Argo Rollout 与 TSB GitOps 支持集成，以及如何使用 SkyWalking 作为金丝雀部署分析和渐进式交付自动化的指标提供者。 在开始之前，请确保以下事项： Argo CD 已安装在你的集群中，并且已配置 Argo CD CLI 以连接到你的 Argo CD 服务","title":"使用 Argo Rollout 和 SkyWalking 进行金丝雀分析和渐进式交付"},{"content":" 裸机服务器\n在本指南中，我们仅提及虚拟机（VM）。如果你想将运行在裸机服务器上的工作负载接入到 TSB 服务网格中，只需将 “VM” 替换为 “裸机” 即可。在处理它们时没有任何区别。 问题定义 Istio 和底层的 Kubernetes 平台共同构建了一个封闭的生态系统，控制平面和数据平面组件紧密集成。例如，运行在每个节点上的控制平面组件创建了相互信任的关系。当新的 Pod 被调度在一个节点上运行时，该节点是一个受信任的实体，其关键资源，如 iptables，会被修改。\n当一个虚拟机（VM）被引入这个生态系统时，它是一个外部实体。要成功地将一个 Istio/Kubernetes 集群扩展到一个 VM 上，必须执行以下步骤：\n认证。VM 必须与控制平面建立一个经过认证的加密会话，证明它被允许载入集群。 路由。VM 必须知道在 Kubernetes 集群中定义的服务，反之亦然。如果 VM 运行一个服务，它必须对在集群内运行的 Pod 可见。 概述 将虚拟机（VM）接入 TSB 管理的 Istio 服务网格可以分为以下步骤：\n使用 Istio 控制平面注册 VM 工作负 …","relpermalink":"/tsb/setup/workload-onboarding/onboarding-vms/","summary":"裸机服务器 在本指南中，我们仅提及虚拟机（VM）。如果你想将运行在裸机服务器上的工作负载接入到 TSB 服务网格中，只需将 “VM” 替换为 “裸机” 即可。在处理它们时没有任何区别。 问题定义 Istio 和底层","title":"使用 tctl 将虚拟机（VM）接入 TSB 服务网格"},{"content":"TSB 支持指定用于保护与外部授权服务器通信的TLS 或 mTLS参数。本文将向你展示如何通过将 CA 证书添加到授权配置来为外部授权服务器配置 TLS 验证。\n在开始之前，请确保你：\n熟悉TSB 概念 安装了 TSB 环境。你可以使用TSB 演示进行快速安装 完成了TSB 用法快速入门。本文假设你已经创建了租户并熟悉工作空间和配置组。还需要将 tctl 配置到你的 TSB 环境中。 本文中的示例将建立在“在 Ingress Gateways 中配置外部授权”之上。在继续之前，请确保已完成该文档，并注意你将在命名空间 httpbin 上工作。\n创建 TLS 证书 为了使 Ingress Gateway 到授权服务的流量启用 TLS，你必须拥有 TLS 证书。本文假设你已经有 TLS 证书，通常包括服务器证书和私钥，以及用于客户端的根证书作为 CA。本文使用以下文件：\nauthz.crt 作为服务器证书 authz.key 作为证书私钥 authz-ca.crt 作为 CA 证书 如果你决定使用其他文件名，请在下面的示例中相应地替换它们。\n自签名证书\n出于示例目的，你可以使用此脚本创建自 …","relpermalink":"/tsb/howto/authorization/tls-verification/","summary":"TSB 支持指定用于保护与外部授权服务器通信的TLS 或 mTLS参数。本文将向你展示如何通过将 CA 证书添加到授权配置来为外部授权服务器配置 TLS 验证。 在开始之前，请确保你： 熟悉TSB 概念 安装了 TSB 环境。你可以使用TS","title":"使用 TLS 验证的外部授权"},{"content":"本页介绍如何利用 TSB Operator 来管理数据平面的网关配置。\nTSB Operator 配置为监督数据平面网关组件的生命周期，主动监控所有命名空间中的 IngressGateway 、 Tier1Gateway 和 EgressGateway 自定义资源 (CR) 集群。默认情况下，数据平面网关组件驻留在 istio-gateway 命名空间中。你可以在数据平面安装 API 参考文档中找到有关自定义资源 API 的全面详细信息。\n数据平面 Operator 监视其创建的 Kubernetes 资源。每当它检测到监视事件（例如删除部署）时，它都会启动协调以将系统恢复到所需状态，从而有效地重新创建任何已删除的部署。\n控制平面要求\n为了让 TSB Operator 管理数据平面网关组件，同一集群中必须存在功能齐全的控制平面。这就需要有一个有效的 TSB Operator 来管理控制平面，以及有效的 ControlPlane 自定义资源 (CR)。 组件 数据平面 Operator 以下是你可以使用数据平面 Operator 配置和管理的自定义组件类型：\n组件 Service …","relpermalink":"/tsb/concepts/operators/data-plane/","summary":"本页介绍如何利用 TSB Operator 来管理数据平面的网关配置。 TSB Operator 配置为监督数据平面网关组件的生命周期，主动监控所有命名空间中的 IngressGateway 、 Tier1Gateway 和 EgressGateway 自定义资源 (CR) 集群。默认情况下，数据平面网关组件驻留在 istio-gateway 命名空间中。你可以在数据","title":"数据平面"},{"content":"在继续之前，请确保你熟悉 Istio 隔离边界 功能。\n升级方法 尽管默认情况下首选并假定使用原地网关升级，但你可以通过在 ControlPlane CR 或 Helm 值文件中设置 ENABLE_INPLACE_GATEWAY_UPGRADE 变量来控制以版本为基础的网关升级的两种方式。\nENABLE_INPLACE_GATEWAY_UPGRADE=true 是默认行为。在使用原地网关升级时，现有的网关部署将使用新的代理镜像进行修补，并将继续使用相同的网关服务。这意味着你无需进行任何更改以配置网关的外部 IP。 ENABLE_INPLACE_GATEWAY_UPGRADE=false 意味着将创建一个新的网关服务和部署以进行金丝雀版本的升级，因此现在可能会有两个服务： \u0026lt;网关名称\u0026gt;/\u0026lt;网关名称\u0026gt;-old，负责处理非修订版/旧版本控制平面工作负载流量。 \u0026lt;网关名称\u0026gt;-1-6-0，负责处理版本控制平面工作负载流量，将为此新创建的 \u0026lt;网关名称\u0026gt;-canary 服务分配新的外部 IP。 你可以通过使用外部负载均衡器或更新 DNS 条目来控制两个版本之间的流量。\n由于原地网关升级是默认行为，你 …","relpermalink":"/tsb/setup/upgrades/gateway-upgrade/","summary":"在继续之前，请确保你熟悉 Istio 隔离边界 功能。 升级方法 尽管默认情况下首选并假定使用原地网关升级，但你可以通过在 ControlPlane CR 或 Helm 值文件中设置 ENABLE_INPLACE_GATEWAY_UPGRADE 变量来控制以版本为基础的网关升级的两种方式。 ENABLE_INPLACE_GATEWAY_UPGRADE=true 是默认行为。在使用原地网关升","title":"网关升级"},{"content":"本页介绍如何将 Kubernetes 集群加入现有的 Tetrate Service Bridge（TSB）管理平面。\n在开始之前，请确保你已经完成以下操作：\n检查 要求 安装 TSB 管理平面 或 演示安装 使用 tctl 登录管理平面（tctl 连接） 检查 TSB 控制平面组件 隔离边界\nTSB 1.6 引入了隔离边界，允许你在 Kubernetes 集群内或跨多个集群中拥有多个 TSB 管理的 Istio 环境。隔离边界的一个好处是你可以执行控制平面的金丝雀升级。\n要启用隔离边界，你必须使用环境变量 ISTIO_ISOLATION_BOUNDARIES=true 更新 Operator 部署，并在控制平面 CR 中包含 isolationBoundaries 字段。 有关更多信息，请参阅 隔离边界。\n创建集群对象 要为集群创建正确的凭据，以便与管理平面通信，你需要使用管理平面 API 创建一个集群对象。\n根据你的需求调整以下 yaml 对象，并保存到名为 new-cluster.yaml 的文件中。\napiVersion: api.tsb.tetrate.io/v2 kind: …","relpermalink":"/tsb/setup/self-managed/onboarding-clusters/","summary":"本页介绍如何将 Kubernetes 集群加入现有的 Tetrate Service Bridge（TSB）管理平面。 在开始之前，请确保你已经完成以下操作： 检查 要求 安装 TSB 管理平面 或 演示安装 使用 tctl 登录管理平面（tctl 连接） 检查 TSB 控制平面组件 隔离边界 TSB 1.6","title":"载入集群"},{"content":"在继续之前，请确保你已经完成了 设置工作负载载入文档 中描述的步骤。\n载入 VM 创建工作负载载入代理配置 默认情况下，工作负载载入代理期望将其配置指定在一个名为 /etc/onboarding-agent/onboarding.config.yaml 的文件中。\n创建文件 /etc/onboarding-agent/onboarding.config.yaml，并使用以下内容替换。 将 onboarding-endpoint-dns-name 替换为要连接的工作负载载入端点，以及将 workload-group-namespace 和 workload-group-name 替换为要加入的 Istio WorkloadGroup 的命名空间和名称。\napiVersion: config.agent.onboarding.tetrate.io/v1alpha1 kind: OnboardingConfiguration onboardingEndpoint: host: \u0026lt;onboarding-endpoint-dns-name\u0026gt; workloadGroup: namespace: …","relpermalink":"/tsb/setup/workload-onboarding/guides/onboarding/","summary":"在继续之前，请确保你已经完成了 设置工作负载载入文档 中描述的步骤。 载入 VM 创建工作负载载入代理配置 默认情况下，工作负载载入代理期望将其配置指定在一个名为 /etc/onboarding-agent/onboarding.config.yaml 的文件中。 创建文件 /etc/onboarding-","title":"载入虚拟机"},{"content":"TSB 支持为 TSB 组件进行自动证书管理。你可以启用 TSB 以创建自签名根 CA，用于签发证书，例如 TSB 管理平面的 TLS 证书，用于控制平面与管理平面之间的通信的 内部证书，以及应用程序集群的中间 CA 证书，Istio 在集群中将使用它们来签发应用程序工作负载的证书。\n外部根 CA\n目前，TSB 的自动证书管理不支持使用外部根 CA。将来的版本将添加对外部根 CA 的支持。 启用自动证书管理 要启用自动证书管理，你需要在 TSB 管理平面 CR 或 helm values 中设置 certIssuer 字段：\nspec: certIssuer: selfSigned: {} tsbCerts: {} clusterIntermediateCAs: {} certIssuer 字段是一个你要启用的证书颁发者的映射。目前，TSB 支持以下颁发者：\nselfSigned：这将创建一个自签名的根 CA，用于签发 TSB 组件的证书。 tsbCerts：这将为 TSB 端点提供 TSB TLS 证书，还将提供 TSB 内部证书。 clusterIntermediateCAs：这将 …","relpermalink":"/tsb/setup/certificate/automated-certificate-management/","summary":"TSB 支持为 TSB 组件进行自动证书管理。你可以启用 TSB 以创建自签名根 CA，用于签发证书，例如 TSB 管理平面的 TLS 证书，用于控制平面与管理平面之间的通信的 内部证书，以及应用程序集群的中间 CA 证书，Istio 在集群中将使用","title":"自动证书管理"},{"content":"假设你熟悉 Git、Docker、Kubernetes、持续交付和 GitOps 的核心概念。以下是 Argo CD 特有的一些概念：\n应用程序由清单定义的一组 Kubernetes 资源。这是自定义资源定义 (CRD)。 应用程序源类型使用哪个工具来构建应用程序。 目标状态应用程序的所需状态，由 Git 存储库中的文件表示。 实时状态该应用程序的实时状态。部署了哪些 pod 等。 同步状态实时状态是否与目标状态匹配。部署的应用程序是否与 Git 所说的一样？ 同步使应用程序移动到其目标状态的过程。例如，通过将更改应用到 Kubernetes 集群。 同步操作状态同步是否成功。 刷新将 Git 中的最新代码与实时状态进行比较。弄清楚有什么不同。 健康应用程序的健康状况，是否正常运行？它可以满足请求吗？ 工具从文件目录创建清单的工具。例如定制。请参阅应用程序源类型。 配置管理工具请参阅工具。 配置管理插件自定义工具。 ","relpermalink":"/argo-cd/core-concepts/","summary":"假设你熟悉 Git、Docker、Kubernetes、持续交付和 GitOps 的核心概念。以下是 Argo CD 特有的一些概念： 应用程序由清单定义的一组 Kubernetes 资源。这是自定义资源定义 (CRD)。 应用程序源类型使用哪个工具来构建应","title":"核心概念"},{"content":"使用生成器的概念，应用集控制器提供了一组强大的工具，用于自动化模板化和修改 Argo CD 应用程序。生成器从各种来源（包括 Argo CD 集群和 Git 存储库）生成模板参数数据，支持和启用新的用例。\n虽然可以将这些工具用于任何目的，但这里是应用集控制器旨在支持的一些特定用例。\n用例：集群附加组件 应用集控制器的初始设计重点是允许基础架构团队的 Kubernetes 集群管理员自动创建大量不同的 Argo CD 应用程序，跨多个集群，并将这些应用程序作为单个单元进行管理。 集群附加组件用例 就是其中一个例子。\n在 集群附加组件用例 中，管理员负责为一个或多个 Kubernetes 集群配置集群附加组件：集群附加组件是 Operator，例如 Prometheus Operator 或控制器，例如 argo-workflows 控制器（Argo 生态系统的一部分）。\n通常，这些附加组件是开发团队的应用程序所需的（例如作为多租户集群的租户，他们可能希望向 Prometheus 提供度量数据或通过 Argo Workflows 编排工作流程）。\n由于安装这些插件需要集群级别的权限，而这些 …","relpermalink":"/argo-cd/operator-manual/applicationset/use-cases/","summary":"使用生成器的概念，应用集控制器提供了一组强大的工具，用于自动化模板化和修改 Argo CD 应用程序。生成器从各种来源（包括 Argo CD 集群和 Git 存储库）生成模板参数数据，支持和启用新的用例。 虽然可以将这些工具用于任何目的，","title":"ApplicationSet 控制器用例"},{"content":"你可以使用 Apache APISIX 和 Apache APISIX Ingress Controller 来进行 Argo Rollouts 的流量管理。\n当使用 Apache APISIX Ingress Controller 作为 Ingress 时，ApisixRoute 是支持 基于权重的流量分流 的对象。\n本指南展示了如何将 ApisixRoute 与 Argo Rollouts 集成，以将其用作加权轮询负载均衡器。\n先决条件 Argo Rollouts 需要 Apache APISIX v2.15 或更新版本以及 Apache APISIX Ingress Controller v1.5.0 或更新版本。\n使用 Helm v3 安装 Apache APISIX 和 Apache APISIX Ingress Controller：\nhelm repo add apisix https://charts.apiseven.com kubectl create ns apisix helm upgrade -i apisix apisix/apisix …","relpermalink":"/argo-rollouts/traffic-management/apisix/","summary":"你可以使用 Apache APISIX 和 Apache APISIX Ingress Controller 来进行 Argo Rollouts 的流量管理。 当使用 Apache APISIX Ingress Controller 作为 Ingress 时，ApisixRoute 是支持 基于权重的流量分流 的对象。 本指南展示了如何将 ApisixRoute 与 Argo Rollouts 集成，以将其用作加权轮询负载均衡器。 先决条件 Argo Rollouts 需","title":"Apache APISIX"},{"content":"本指南介绍了 Argo Rollouts 如何与 AWS 负载均衡器控制器集成以进行流量调整。本指南以基本入门指南的概念为基础。\n要求 安装了 AWS ALB Ingress Controller 的 Kubernetes 集群 🔔 提示：请参阅负载均衡器控制器安装说明，了解如何安装 AWS 负载均衡器控制器。\n1. 部署 Rollout、Services 和 Ingress 当 AWS ALB Ingress 用作流量路由器时，Rollout canary 策略必须定义以下字段：\napiVersion: argoproj.io/v1alpha1 kind: Rollout metadata: name: rollouts-demo spec: strategy: canary: # canaryService 和 stableService 是指向 Rollout 将要修改的 Service 的引用，以便将其定向到金丝雀 ReplicaSet 和稳定 ReplicaSet（必填）。 canaryService: rollouts-demo-canary stableService: …","relpermalink":"/argo-rollouts/getting-started/alb/","summary":"本指南介绍了 Argo Rollouts 如何与 AWS 负载均衡器控制器集成以进行流量调整。本指南以基本入门指南的概念为基础。 要求 安装了 AWS ALB Ingress Controller 的 Kubernetes 集群 🔔 提示：请参阅负载均衡器控制器安装说明，了解如何安装 AWS 负载均衡器控制器。 1. 部署 R","title":"AWS Load Balancer Controller 快速开始"},{"content":"水平 Pod 自动缩放（HPA）根据观察到的 CPU 利用率或用户配置的指标自动调整 Kubernetes 资源拥有的 Pod 数量。为了实现这种行为，HPA 仅支持启用了 scale 端点的资源，该端点具有几个必需字段。scale 端点允许 HPA 了解资源的当前状态并修改资源以适当地进行扩展。Argo Rollouts 在 0.3.0 版本中添加了对 scale 端点的支持。在 HPA 修改资源后，Argo Rollouts 控制器负责在副本中协调该变化。由于 Rollout 中的策略非常不同，因此 Argo Rollouts 控制器会针对各种策略以不同的方式处理 scale 端点。下面是不同策略的行为。\n蓝绿部署 HPA 将使用从接收来自活动服务的流量的 ReplicaSet 中获取的指标来缩放 BlueGreen 策略的 Rollouts。当 HPA 更改副本计数时，Argo Rollouts 控制器将首先缩放接收来自活动服务的 ReplicaSet，然后是接收来自预览服务的 ReplicaSet。控制器将缩放接收来自预览服务的 ReplicaSet，以准备在 Rollout  …","relpermalink":"/argo-rollouts/rollout/hpa-support/","summary":"水平 Pod 自动缩放（HPA）根据观察到的 CPU 利用率或用户配置的指标自动调整 Kubernetes 资源拥有的 Pod 数量。为了实现这种行为，HPA 仅支持启用了 scale 端点的资源，该端点具有几个必需字段。scale 端点允许 HPA 了解资源的当前状态","title":"水平 Pod 自动缩放"},{"content":"Rollout Rollout 是 Kubernetes 工作负载资源，相当于 Kubernetes Deployment 对象。它旨在在需要更高级的部署或渐进式交付功能的场景中替换 Deployment 对象。Rollout 提供以下功能，而 Kubernetes Deployment 无法提供：\n蓝绿部署 金丝雀部署 与入口控制器和服务网格集成，用于高级流量路由 与度量提供程序集成，用于蓝绿和金丝雀分析 基于成功或失败的度量自动升级或回滚 渐进式交付 渐进式交付是以受控和逐步的方式发布产品更新的过程，从而降低发布的风险，通常通过耦合自动化和度量分析来驱动更新的自动升级或回滚。\n渐进式交付通常被描述为持续交付的演进，将 CI/CD 中实现的速度优势扩展到部署过程中。这是通过将新版本的曝光限制为子集用户，并观察和分析其正确行为，然后逐步增加曝光范围以涵盖更广泛的受众并持续验证正确性来实现的。\n部署策略 虽然行业已经使用一致的术语来描述各种部署策略，但这些策略的实现在各种工具之间存在差异。为了清楚地了解 Argo Rollouts 的行为，以下是 Argo Rollouts 提供的各种部 …","relpermalink":"/argo-rollouts/concepts/","summary":"Rollout Rollout 是 Kubernetes 工作负载资源，相当于 Kubernetes Deployment 对象。它旨在在需要更高级的部署或渐进式交付功能的场景中替换 Deployment 对象。Rollout 提供以下功能，而 Kubernetes Deployment 无法提供： 蓝绿部署 金丝雀部署 与入口控制器和服务网格集成，用于高级流量","title":"概念"},{"content":"Argo Rollouts 支持以下指标度量：\nPrometheus DataDog NewRelic Wavefront Job Web Kayenta CloudWatch Graphite InfluxDB Apache SkyWalking ","relpermalink":"/argo-rollouts/analysis/metrics/","summary":"Argo Rollouts 支持以下指标度量： Prometheus DataDog NewRelic Wavefront Job Web Kayenta CloudWatch Graphite InfluxDB Apache SkyWalking","title":"指标度量"},{"content":"本章从业务和技术的角度解释了在基础设施中部署 SPIFFE 和 SPIRE 的好处。\n任何人任何地方都适用 SPIFFE 和 SPIRE 旨在加强对软件组件的识别，以一种通用的方式，任何人在任何地方都可以在分布式系统中加以利用。现代基础设施的技术环境是错综复杂的。环境在硬件和软件投资的混合下变得越来越不一样。通过对系统定义、证明和维护软件身份标准化的方式来维护软件安全，无论系统部署在哪里，也无论谁来部署这些系统，都会带来许多好处。\n对于专注于提高业务便利性和回报的企业领导人来说，SPIFFE 和 SPIRE 可以大大降低管理和签发加密身份文件（如 X.509 证书）的开销，开发人员无需了解服务间通信所需的身份和认证技术，从而加速开发和部署。\n对于专注于提供强大、安全和可互操作产品的服务提供商和软件供应商来说，SPIFFE 和 SPIRE 解决了在将许多解决方案互连到最终产品时普遍存在的关键身份问题。例如，SPIFFE 可以作为产品的 TLS 功能和用户管理/认证功能的基础，一举两得。还有，SPIFFE 可以取代管理和发行平台访问的 API 令牌的需要，免费带来令牌轮换，并消除存储和管理 …","relpermalink":"/spiffe/benefits/","summary":"本章从业务和技术的角度解释了在基础设施中部署 SPIFFE 和 SPIRE 的好处。 任何人任何地方都适用 SPIFFE 和 SPIRE 旨在加强对软件组件的识别，以一种通用的方式，任何人在任何地方都可以在分布式系统中加以利用。现代基础设施的技术环境是错","title":"收益"},{"content":"所有 BPF Map 都是有使用容量上限的。超出限制的插入将失败，从而限制了数据路径的可扩展性。下表显示了映射的默认值。每个限制都可以在源代码中更改。如果需要，将根据要求添加配置选项。\nMap 名称 范围 默认限制 规模影响 连接跟踪 节点或端点 1M TCP/256k UDP 最大 1M 并发 TCP 连接，最大 256k 预期 UDP 应答 NAT 节点 512k 最大 512k NAT 条目 邻居表 节点 512k 最大 512k 邻居条目 端点 节点 64k 每个节点最多 64k 个本地端点 + 主机 IP IP 缓存 节点 512k 最大 256k 端点（IPv4+IPv6），最大 512k 端点（IPv4 或 IPv6）跨所有集群 负载均衡器 节点 64k 跨所有集群的所有服务的最大 64k 累积后端 策略 端点 16k 特定端点的最大允许身份 + 端口 + 协议对 16k 代理 Map 节点 512k 最大 512k 并发重定向 TCP 连接到代理 隧道 节点 64k 跨所有集群最多 32k 节点（IPv4+IPv6）或 64k 节点（IPv4 或 IPv6） IPv4  …","relpermalink":"/cilium-handbook/ebpf/maps/","summary":"所有 BPF Map 都是有使用容量上限的。超出限制的插入将失败，从而限制了数据路径的可扩展性。下表显示了映射的默认值。每个限制都可以在源代码中更改。如果需要，将根据要求添加配置选项。 Map 名称 范围 默认限制 规模影响 连接","title":"eBPF Map"},{"content":"用于 Pod 的 IPv4 地址通常是从 RFC1918 私有地址块分配的，因此不可公开路由。Cilium 会自动将离开集群的所有流量的源 IP 地址伪装成节点的 IPv4 地址，因为节点的 IP 地址已经可以在网络上路由。\nIP 地址伪装示意图 对于 IPv6 地址，只有在使用 iptables 实现模式时才会执行伪装。\n可以使用 IPv4 选项和离开主机的 IPv6 流量禁用此行为。enable-ipv4-masquerade: false``enable-ipv6-masquerade: false\n配置 设置可路由 CIDR\n默认行为是排除本地节点的 IP 分配 CIDR 内的任何目标。如果 pod IP 可在更广泛的网络中路由，则可以使用以下选项指定该网络：ipv4-native-routing-cidr: 10.0.0.0/8 ，在这种情况下，该 CIDR 内的所有目的地都 不会 被伪装。\n设置伪装接口\n请参阅实现模式以配置伪装接口。\n实现模式 基于 eBPF 基于 eBPF 的实现是最有效的实现。它需要 Linux 内核 4.19， …","relpermalink":"/cilium-handbook/networking/masquerading/","summary":"用于 Pod 的 IPv4 地址通常是从 RFC1918 私有地址块分配的，因此不可公开路由。Cilium 会自动将离开集群的所有流量的源 IP 地址伪装成节点的 IPv4 地址，因为节点的 IP 地址已经可以在网络上路由。 IP 地址伪装示意图 对于 IPv6 地址，只有在","title":"IP 地址伪装"},{"content":"所有安全策略的描述都是假设基于会话协议的有状态策略执行。这意味着策略的意图是描述允许的连接建立方向。如果策略允许A =\u0026gt; B，那么从 B 到 A 的回复数据包也会被自动允许。但是，并不自动允许 B 向 A 发起连接。如果希望得到这种结果，那么必须明确允许这两个方向。\n安全策略可以在 ingress 或 egress 处执行。对于 ingress，这意味着每个集群节点验证所有进入的数据包，并确定数据包是否被允许传输到预定的终端。相应地，对于 egress，每个集群节点验证出站数据包，并确定是否允许将数据包传输到预定目的地。\n为了在多主机集群中执行基于身份的安全，发送端点的身份被嵌入到集群节点之间传输的每个网络数据包中。然后，接收集群节点可以提取该身份，并验证一个特定的身份是否被允许与任何本地端点进行通信。\n默认安全策略 如果没有加载任何策略，默认行为是允许所有通信，除非明确启用了策略执行。一旦加载了第一条策略规则，就会自动启用策略执行，然后任何通信必须是白名单，否则相关数据包将被丢弃。\n同样，如果一个端点不受制于四层策略，则允许与所有端口进行通信。将至少一个 四层策略与一个端点相关联，将 …","relpermalink":"/cilium-handbook/security/policyenforcement/","summary":"所有安全策略的描述都是假设基于会话协议的有状态策略执行。这意味着策略的意图是描述允许的连接建立方向。如果策略允许A =\u003e B，那么从 B 到 A 的回复数据包也会被自动允许。但是，并不自动允许 B 向 A 发起连接。如果希","title":"策略执行"},{"content":"本节指定 Cilium 端点的生命周期。\nCilium 中的端点状态包括：\nrestoring：端点在 Cilium 启动之前启动，Cilium 正在恢复其网络配置。 waiting-for-identity：Cilium 正在为端点分配一个唯一的身份。 waiting-to-regenerate：端点接收到一个身份并等待（重新）生成其网络配置。 regenerating：正在（重新）生成端点的网络配置。这包括为该端点编程 eBPF。 ready：端点的网络配置已成功（重新）生成。 disconnecting：正在删除端点。 disconnected：端点已被删除。 端点状态生命周期 可以使用 cilium endpoint list 和 cilium endpoint get CLI 命令查询端点的状态。\n当端点运行时，它会在 waiting-for-identity、waiting-to-regenerate、regenerating 和 ready 状态之间转换。进入 waiting-for-identity 状态的转换表明端点改变了它的身份。 …","relpermalink":"/cilium-handbook/policy/lifecycle/","summary":"本节指定 Cilium 端点的生命周期。 Cilium 中的端点状态包括： restoring：端点在 Cilium 启动之前启动，Cilium 正在恢复其网络配置。 waiting-for-identity：Cilium 正在为端点分配一个唯一的身","title":"端点生命周期"},{"content":"集群网格将网络数据路径扩展到多个集群。它允许所有连接集群中的端点进行通信，同时提供完整的策略执行。负载均衡可通过 Kubernetes 注解获得。\n请参阅如何设置集群网格的说明。\n","relpermalink":"/cilium-handbook/clustermesh/","summary":"集群网格将网络数据路径扩展到多个集群。它允许所有连接集群中的端点进行通信，同时提供完整的策略执行。负载均衡可通过 Kubernetes 注解获得。 请参阅如何设置集群网格的说明。","title":"多集群（集群网格）"},{"content":"可观测性由 Hubble 提供，它可以以完全透明的方式深入了解服务的通信和行为以及网络基础设施。Hubble 能够在多集群（集群网格） 场景中提供节点级别、集群级别甚至跨集群的可视性。有关 Hubble 的介绍以及它与 Cilium 的关系，请阅读 Cilium 和 Hubble 简介部分。\n默认情况下，Hubble API 的范围仅限于 Cilium 代理运行的每个单独节点。换句话说，网络可视性仅提供给本地 Cilium 代理观察到的流量。在这种情况下，与 Hubble API 交互的唯一方法是使用 Hubble CLI（hubble）查询通过本地 Unix Domain Socket 提供的 Hubble API。Hubble CLI 二进制文件默认安装在 Cilium 代理 pod 上。\n部署 Hubble Relay 后，Hubble 提供完整的网络可视性。在这种情况下，Hubble Relay 服务提供了一个 Hubble API，它在 ClusterMesh 场景中涵盖整个集群甚至多个集群。可以通过将 Hubble CLI（hubble）指向 Hubble Relay 服务 …","relpermalink":"/cilium-handbook/concepts/observability/","summary":"可观测性由 Hubble 提供，它可以以完全透明的方式深入了解服务的通信和行为以及网络基础设施。Hubble 能够在多集群（集群网格） 场景中提供节点级别、集群级别甚至跨集群的可视性。有关 Hubble 的介绍以及它与 Cilium 的关系，请阅","title":"可观测性"},{"content":"Kubernetes 版本 以下列出的所有 Kubernetes 版本都经过 e2e 测试，并保证与此 Cilium 版本兼容。此处未列出的旧 Kubernetes 版本不支持 Cilium。较新的 Kubernetes 版本未列出，这取决于新版本的的向后兼容性。\n1.16 1.17 1.18 1.19 1.20 1.21 1.22 1.23 系统要求 Cilium 需要 Linux 内核 \u0026gt;= 4.9。有关所有系统要求的完整详细信息，请参阅系统要求。\n在 Kubernetes 中启用 CNI CNI（容器网络接口）是 Kubernetes 用来委托网络配置的插件层。必须在 Kubernetes 集群中启用 CNI 才能安装 Cilium。这是通过将 --network-plugin=cni 参数在所有节点上传递给 kubelet 来完成的。有关更多信息，请参阅Kubernetes CNI 网络插件文档。\n启用自动节点 CIDR 分配（推荐） Kubernetes 具有自动分配每个节点 IP CIDR 的能力。如果启用，Cilium 会自动使用此功能。这是在 Kubernetes 集群 …","relpermalink":"/cilium-handbook/kubernetes/requirements/","summary":"Kubernetes 版本 以下列出的所有 Kubernetes 版本都经过 e2e 测试，并保证与此 Cilium 版本兼容。此处未列出的旧 Kubernetes 版本不支持 Cilium。较新的 Kubernetes 版本未列出，这取决于新版本的的向后兼容性。 1.16 1.17 1.18 1.19 1.20 1.21 1.22 1.23 系统要求 Cilium 需要 Linux 内核 \u003e= 4.9。有","title":"要求"},{"content":"在这一章中，让我们来谈谈编写 eBPF 代码。我们需要考虑在内核中运行的 eBPF 程序本身，以及与之交互的用户空间代码。\n内核和用户空间代码 首先，你可以用什么编程语言来编写 eBPF 程序？\n内核接受字节码形式的 eBPF 程序 1。人工编写这种字节码是可能的，就像用汇编语言写应用程序代码一样——但对人类来说，使用一种可以被编译（即自动翻译）为字节码的高级语言通常更实用。\n由于一些原因，eBPF 程序不能用任意的高级语言编写。首先，语言编译器需要支持发出内核所期望的 eBPF 字节码格式。其次，许多编译语言都有运行时特性——例如 Go 的内存管理和垃圾回收，使它们不适合。在撰写本文时，编写 eBPF 程序的唯一选择是 C（用 clang/llvm 编译）和最新的 Rust。迄今为止，绝大多数的 eBPF 代码都是用 C 语言发布的，考虑到它是 Linux 内核的语言，这是有道理的。\n至少，用户空间的程序需要加载到内核中，并将其附加到正确的事件中。有一些实用程序，如 bpftool，可以帮助我们解决这个问题，但这些都是低级别的工具，假定你有丰富的 eBPF 知识，它们是为 eBPF  …","relpermalink":"/what-is-ebpf/ebpf-programs/","summary":"在这一章中，让我们来谈谈编写 eBPF 代码。我们需要考虑在内核中运行的 eBPF 程序本身，以及与之交互的用户空间代码。 内核和用户空间代码 首先，你可以用什么编程语言来编写 eBPF 程序？ 内核接受字节码形式的 eBPF 程序 1。人工编写这","title":"第三章：eBPF 程序"},{"content":"现在我们将一些问题转移到了云中的 DevOps 平台。\n分解单体应用 传统的 n 层单体式应用部署到云中后很难维护，因为它们经常对云基础设施提供的部署环境做出不可靠的假设，这些假设云很难提供。例如以下要求：\n可访问已挂载的共享文件系统 P2P 应用服务器集群 共享库 配置文件位于常用的配置文件目录 大多数这些假设都出于这样的事实：单体应用通常都部署在长期运行的基础设施中，并与其紧密结合。不幸的是，单体应用并不太适合弹性和短暂（非长期支持）生命周期的基础设施。\n但是即使我们可以构建一个不需要这些假设的单体应用，我们依然有一些问题需要解决：\n单体式应用的变更周期耦合，使独立业务能力无法按需部署，阻碍创新速度。 嵌入到单体应用中的服务不能独立于其他服务进行扩展，因此负载更难于优化。 新加入组织的开发人员必须适应新的团队，经常学习新的业务领域，并且一次就熟悉一个非常大的代码库。这样会增加 3-6 个月的适应时间，才能实现真正的生产力。 尝试通过堆积开发人员来扩大开发组织，增加了昂贵的沟通和协调成本。 技术栈需要长期承诺。引进新技术太过冒险，可能会对整体产生不利影响。 细心的读者会注意到，该列表 …","relpermalink":"/migrating-to-cloud-native-application-architectures/changes-needed/technical-change/","summary":"现在我们将一些问题转移到了云中的 DevOps 平台。 分解单体应用 传统的 n 层单体式应用部署到云中后很难维护，因为它们经常对云基础设施提供的部署环境做出不可靠的假设，这些假设云很难提供。例如以下要求： 可访问已挂载的共","title":"2.3 技术变革"},{"content":"所涉及的关键原语和实施任务是。\n管道和 CI/CD 管道的概念 CI/CD 管道的构建块 设计和执行 CI/CD 管道 自动化的战略 CI/CD 管道中对安全自动化工具的要求 3.3.1 管道的概念和 CI/CD 管道 DevSecOps 作为一种敏捷应用开发、部署和运维的方法论或框架，与其他方法论一样，是由 各个阶段 组成的。信息在各阶段中的顺序和流动被称为工作流，其中一些阶段可以平行执行，而其他阶段则必须遵循一个顺序。每个阶段可能需要调用一个独特的工作来执行该阶段的活动。\nDevSecOps 在流程工作流中引入的一个独特概念是 管道 的概念。有了管道，就不需要为启动 / 执行流程的每个阶段单独编写作业。相反，只有一个作业从初始阶段开始，自动触发与其他阶段有关的活动 / 任务（包括顺序的和并行的），并创建一个无错误的智能工作流程。\nDevSecOps 中的管道被称为 CI/CD 管道，这是基于它所完成的总体任务和它所包含的两个单独阶段。CD 可以表示持续交付或持续部署阶段。根据这后一个阶段，CI/CD 可以涉及以下任务：\n构建、测试、安全和交付：经过测试的修改后的代码被交付到暂存区。 …","relpermalink":"/service-mesh-devsecops/devsecops/key-primitives-and-implementation-tasks/","summary":"所涉及的关键原语和实施任务是。 管道和 CI/CD 管道的概念 CI/CD 管道的构建块 设计和执行 CI/CD 管道 自动化的战略 CI/CD 管道中对安全自动化工具的要求 3.3.1 管道的概念和 CI/CD 管道 DevSecOps 作为一种敏捷应用开发、部署和运维的方法论或框架，与其他方法","title":"3.3 DevSecOps 关键原语和实施任务"},{"content":"本章中我们讨论了两种帮助我们迁移到云原生应用架构的方法：\n分解原架构\n我们使用以下方式分解单体应用：\n所有新功能都使用微服务形式构建。 通过隔离层将微服务与单体应用集成。 通过定义有界上下文来分解服务，逐步扼杀单体架构。 使用分布式系统\n分布式系统由以下部分组成：\n版本化，分布式，通过配置服务器和管理总线刷新配置。 动态发现远端依赖。 去中心化的负载均衡策略 通过熔断器和隔板阻止级联故障 通过 API 网关集成到特定的客户端上 还有很多其他的模式，包括自动化测试、持续构建与发布管道等。欲了解更多信息，请阅读 Toby Clemson 的《Testing Strategies in a Microservice Architecture》，以及 Jez Humbl 和 David Farley（AddisonWesley）的《Continuous Delivery: Reliable Software Releases through Build, Test, and Deployment Automation》。\n","relpermalink":"/migrating-to-cloud-native-application-architectures/migration-cookbook/summary/","summary":"本章中我们讨论了两种帮助我们迁移到云原生应用架构的方法： 分解原架构 我们使用以下方式分解单体应用： 所有新功能都使用微服务形式构建。 通过隔离层将微服务与单体应用集成。 通过定义有界上下文来分解服务，逐步扼杀","title":"3.3 本章小结"},{"content":"为应用程序分配基础设施的传统方法包括最初用配置参数和持续的任务配置计算和网络资源，如补丁管理（如操作系统和库），建立符合合规法规（如数据隐私），并进行漂移（当前配置不再提供预期的操作状态）纠正。\n基础设施即代码（IaC）是一种声明式的代码，它对计算机指令进行编码，这些指令封装了通过服务的管理 API 在公共云服务或私有数据中心部署虚拟基础设施所需的 参数。换句话说，基础设施是以声明式的方式定义的，并使用用于应用程序代码的相同的源代码控制工具（如 GitOps）进行版本控制。根据特定的 IaC 工具，这种语言可以是脚本语言（如 JavaScript、Python、TypeScript 等）或专有配置语言（如 HCL），可能与标准化语言（如 JSON）兼容也可能不兼容。基本指令包括告诉系统如何配置和管理 基础设施（无论是单个计算实例还是完整的服务器，如物理服务器或虚拟机）、容器、存储、网络连接、连接拓扑和负载均衡器。在某些情况下，基础设施可能是短暂的，基础设施的寿命（无论是不可变的还是可变的）不需要继续配置管理。配置可以与应用程序代码的单个提交相联系，使用的工具可以将应用程序代码和基础设施 …","relpermalink":"/service-mesh-devsecops/implement/ci-cd-pipeline-for-infrastructure-as-code/","summary":"为应用程序分配基础设施的传统方法包括最初用配置参数和持续的任务配置计算和网络资源，如补丁管理（如操作系统和库），建立符合合规法规（如数据隐私），并进行漂移（当前配置不再提供预期的操作状态）纠正。 基础设","title":"4.3 基础设施即代码的 CI/CD 管道"},{"content":"基础架构是指支持应用程序的所有软件和硬件，包括数据中心、操作系统、部署流水线、配置管理以及支持应用程序生命周期所需的任何系统或软件。\n已经有无数的时间和金钱花在了基础架构上。通过多年来不断的技术演化和实践提炼，有些公司已经能够运行大规模的基础架构和应用程序，并且拥有卓越的敏捷性。高效运行的基础架构可以使得迭代更快，缩短投向市场的时间，从而加速业务发展。\n使用云原生基础架构是有效运行云原生应用程序的要求。如果没有正确的设计和实践来管理基础架构，即使是最好的云原生应用程序也会浪费。本书中的实践并不一定需要有巨大的基础架构规模，但如果您想从云计算中获取回报，您应该听从开创了这些模式的人的经验。\n在我们探索如何构建云中运行的应用程序的基础架构之前，我们需要了解我们是如何走到这一步。首先，我们将讨论采用云原生实践的好处。接下来，我们将看一下基础架构的简历，然后讨论下一阶段的功能，称为“云原生”，以及它与您的应用程序、运行的平台及业务之间的关系。\n在明白了这一点后，我们将向您展示解决方案及实现。\n云原生的优势 采用本书中的模式有很多好处。它们仿照谷歌、Netflix 和亚马逊这些成功的公司 ——  …","relpermalink":"/cloud-native-infra/what-is-cloud-native-infrastructure/","summary":"基础架构是指支持应用程序的所有软件和硬件，包括数据中心、操作系统、部署流水线、配置管理以及支持应用程序生命周期所需的任何系统或软件。 已经有无数的时间和金钱花在了基础架构上。通过多年来不断的技术演化和实","title":"第 1 章：什么是云原生基础架构？"},{"content":"Kubernetes® 是一个开源系统，可以自动部署、扩展和管理在容器中运行的应用程序，并且通常托管在云环境中。与传统的单体软件平台相比，使用这种类型的虚拟化基础设施可以提供一些灵活性和安全性的好处。然而，安全地管理从微服务到底层基础设施的所有方面，会引入其他的复杂性。本报告中详述的加固指导旨在帮助企业处理相关风险并享受使用这种技术的好处。\nKubernetes 中三个常见的破坏源是供应链风险、恶意威胁者和内部威胁。\n供应链风险往往是具有挑战性的，可以在容器构建周期或基础设施收购中出现。恶意威胁者可以利用 Kubernetes 架构的组件中的漏洞和错误配置，如控制平面、工作节点或容器化应用程序。内部威胁可以是管理员、用户或云服务提供商。对组织的 Kubernetes 基础设施有特殊访问权的内部人员可能会滥用这些特权。\n本指南描述了与设置和保护 Kubernetes 集群有关的安全挑战。包括避免常见错误配置的加固策略，并指导国家安全系统的系统管理员和开发人员如何部署 Kubernetes，并提供了建议的加固措施和缓解措施的配置示例。本指南详细介绍了以下缓解措施：\n扫描容器和 Pod 的漏 …","relpermalink":"/kubernetes-hardening-guidance/executive-summary/","summary":"Kubernetes® 是一个开源系统，可以自动部署、扩展和管理在容器中运行的应用程序，并且通常托管在云环境中。与传统的单体软件平台相比，使用这种类型的虚拟化基础设施可以提供一些灵活性和安全性的好处。然","title":"执行摘要"},{"content":"本文参考的是 OAM 规范中对云原生应用的定义，并做出了引申。\n云原生应用是一个相互关联但又不独立的组件（service、task、worker）的集合，这些组件与配置结合在一起并在适当的运行时实例化后，共同完成统一的功能目的。\n云原生应用模型 下图是 OAM 定义的云原生应用模型示意图，为了便于理解，图中相同颜色的部分为同一类别的对象定义。\n云原生应用模型 OAM 的规范中定义了以下对象，它们既是 OAM 规范中的基本术语也是云原生应用的基本组成。\nWorkload（工作负载）：应用程序的工作负载类型，由平台提供。 Component 组件）：定义了一个 Workload 的实例，并以基础设施中立的术语声明其运维特性。 Trait（特征）：用于将运维特性分配给组件实例。 ApplicationScope（应用作用域）：用于将组件分组成具有共同特性的松散耦合的应用。 ApplicationConfiguration（应用配置）：描述 Component 的部署、Trait 和 ApplicationScope。 关注点分离 下图是不同角色对于该模型的关注点示意图。\n云原生应用模型中的目 …","relpermalink":"/cloud-native-handbook/intro/define-cloud-native-app/","summary":"本文参考的是 OAM 规范中对云原生应用的定义，并做出了引申。 云原生应用是一个相互关联但又不独立的组件（service、task、worker）的集合，这些组件与配置结合在一起并在适当的运行时实例化后，共同完","title":"什么是云原生应用？"},{"content":"Kubernetes 一词来自希腊语，意思是“飞行员”或“舵手”。这个名字很贴切，Kubernetes 可以帮助你在波涛汹涌的容器海洋中航行。\nKubernetes 是做什么的？什么是 Docker？什么是容器编排？Kubernetes 是如何工作和扩展的？你可能还有很多其他的问题，本文将一一为你解答。\n这篇文章适合初学者，尤其是那些工作忙碌，没有办法抽出太多时间来了解 Kubernetes 和云原生的开发者们，希望本文可以帮助你进入 Kubernetes 的世界。\n简而言之，Kubernetes 提供了一个平台或工具来帮助你快速协调或扩展容器化应用，特别是在 Docker 容器。让我们深入了解一下这些概念。\n容器和容器化 那么什么是容器呢？\n要讨论容器化首先要谈到虚拟机 (VM)，顾名思义，虚拟机就是可以远程连接的虚拟服务器，比如 AWS 的 EC2 或阿里云的 ECS。\n接下来，假如你要在虚拟机上运行一个网络应用 —— 包括一个 MySQL 数据库、一个 Vue 前端和一些 Java 库，在 Ubuntu 操作系统 (OS) 上运行。你不用熟悉其中的每一个技术 —— 你只要记住，一 …","relpermalink":"/cloud-native-handbook/intro/quick-start/","summary":"Kubernetes 一词来自希腊语，意思是“飞行员”或“舵手”。这个名字很贴切，Kubernetes 可以帮助你在波涛汹涌的容器海洋中航行。 Kubernetes 是做什么的？什么是 Docker？什么是容器编排？Kubernetes 是如何工作","title":"云原生快速入门"},{"content":"你可能参加过各种云原生、服务网格相关的 meetup，在社区里看到很多人在分享和讨论 Istio，但是对于自己是否真的需要 Istio 感到踌躇，甚至因为它的复杂性而对服务网格的前景感到怀疑。那么，在你继阅读 Istio SIG 后续文章之前，请先仔细阅读本文，审视一下自己公司的现状，看看你是否有必要使用服务网格，处于 Istio 应用的哪个阶段。\n本文不是对应用服务网格的指导，而是根据社区里经常遇到的问题而整理。在使用 Istio 之前，请先考虑下以下因素：\n你的团队里有多少人？ 你的团队是否有使用 Kubernetes、Istio 的经验？ 你有多少微服务？ 这些微服务使用什么语言？ 你的运维、SRE 团队是否可以支持服务网格管理？ 你有采用开源项目的经验吗？ 你的服务都运行在哪些平台上？ 你的应用已经容器化并使用 Kubernetes 管理了吗？ 你的服务有多少是部署在虚拟机、有多少是部署到 Kubernetes 集群上，比例如何？ 你的团队有制定转移到云原生架构的计划吗？ 你想使用 Istio 的什么功能？ Istio 的稳定性是否能够满足你的需求？ 你是否可以忍受 Istio …","relpermalink":"/cloud-native-handbook/service-mesh/do-you-need-a-service-mesh/","summary":"你可能参加过各种云原生、服务网格相关的 meetup，在社区里看到很多人在分享和讨论 Istio，但是对于自己是否真的需要 Istio 感到踌躇，甚至因为它的复杂性而对服务网格的前景感到怀疑。那么，在你继阅读 Istio SIG 后续","title":"你是否需要 Istio？"},{"content":"眯着眼睛看图表并不是寻找相关性的最佳方式。目前在运维人员头脑中进行的大量工作实际上是可以自动化的。这使运维人员可以在识别问题、提出假设和验证根本原因之间迅速行动。\n为了建立更好的工具，我们需要更好的数据。遥测必须具备以下两个要求以支持高质量的自动分析：\n所有的数据点都必须用适当的索引连接在一个图上。 所有代表常见操作的数据点必须有明确的键和值。 在这一章中，我们将从一个基本的构件开始浏览现代遥测数据模型：属性。\n属性：定义键和值 最基本的数据结构是属性（attribute），定义为一个键和一个值。OpenTelemetry 的每个数据结构都包含一个属性列表。分布式系统的每个组件（HTTP 请求、SQL 客户端、无服务器函数、Kubernetes Pod）在 OpenTelemetry 规范中都被定义为一组特定的属性。这些定义被称为 OpenTelemetry 语义约定。表 2-1 显示了 HTTP 约定的部分列表。\n属性 类型 描述 示例 http.method string HTTP 请求类型 GET; POST; HEAD http.target string 在 HTTP 请求行 …","relpermalink":"/opentelemetry-obervability/the-value-of-structured-data/","summary":"第 2 章：结构化数据的价值","title":"第 2 章：结构化数据的价值"},{"content":"IT 行业正在向微服务架构和云原生解决方案发展。由于使用不同的技术开发了成百上千的微服务，这些系统可能变得复杂，难以调试。\n作为一个应用开发者，你考虑的是业务逻辑——购买产品或生成发票。然而，任何像这样的业务逻辑都会导致不同服务之间的多个服务调用。每个服务可能都有它的超时、重试逻辑和其他可能需要调整或微调的网络特定代码。\n如果在任何时候最初的请求失败了，就很难通过多个服务来追踪，准确地指出失败发生的地方，了解请求为什么失败。是网络不可靠吗？是否需要调整重试或超时？或者是业务逻辑问题或错误？\n服务可能使用不一致的跟踪和记录机制，使这种调试的复杂性增加。这些问题使你很难确定问题发生在哪里，以及如何解决。如果你是一个应用程序开发人员，而调试网络问题不属于你的核心技能，那就更是如此。\n将网络问题从应用程序堆栈中抽离出来，由另一个组件来处理网络部分，让调试网络问题变得更容易。这就是 Envoy 所做的事情。\n在每个服务实例旁边都有一个 Envoy 实例在运行。这种类型的部署也被称为 Sidecar 部署。Envoy 的另一种模式是边缘代理，用于构建 API 网关。\nEnvoy 和应用程序形成一个 …","relpermalink":"/cloud-native-handbook/service-mesh/what-is-envoy/","summary":"IT 行业正在向微服务架构和云原生解决方案发展。由于使用不同的技术开发了成百上千的微服务，这些系统可能变得复杂，难以调试。 作为一个应用开发者，你考虑的是业务逻辑——购买产品或生成发票。然而，任何像这样的业","title":"什么是 Envoy？"},{"content":"总结 现在您已经知道了 Code Review 要点，那么管理分布在多个文件中的评论的最有效方法是什么？\n变更是否有意义？它有很好的描述吗？ 首先看一下变更中最重要的部分。整体设计得好吗？ 以适当的顺序查看 CL 的其余部分。 第一步：全面了解变更 查看 CL 描述和 CL 大致上用来做什么事情。这种变更是否有意义？如果在最初不应该发生这样的变更，请立即回复，说明为什么不应该进行变更。当您拒绝这样的变更时，向开发人员建议应该做什么也是一个好主意。\n例如，您可能会说“看起来你已经完成一些不错的工作，谢谢！但实际上，我们正朝着删除您在这里修改的 FooWidget 系统的方向演进，所以我们不想对它进行任何新的修改。不过，您来重构下新的 BarWidget 类怎么样？“\n请注意，审查者不仅拒绝了当前的 CL 并提供了替代建议，而且他们保持礼貌地这样做。这种礼貌很重要，因为我们希望表明，即使不同意，我们也会相互尊重。\n如果您获得了多个您不想变更的 CL，您应该考虑重整开发团队的开发过程或外部贡献者的发布过程，以便在编写 CL 之前有更多的沟通。最好在他们完成大量工作之前说“不”，避免已经投入心 …","relpermalink":"/eng-practices/review/reviewer/navigate/","summary":"总结 现在您已经知道了 Code Review 要点，那么管理分布在多个文件中的评论的最有效方法是什么？ 变更是否有意义？它有很好的描述吗？ 首先看一下变更中最重要的部分。整体设计得好吗？ 以适当的顺序查看 CL 的其余部分。 第一步：全","title":"查看 CL 的步骤"},{"content":"当您发送 CL 进行审查时，您的审查者可能会对您的 CL 发表一些评论。以下是处理审查者评论的一些有用信息。\n不是针对您 审查的目标是保持代码库和产品的质量。当审查者对您的代码提出批评时，请将其视为在帮助您、代码库和 Google，而不是对您或您的能力的个人攻击。\n有时，审查者会感到沮丧并在评论中表达他们的挫折感。对于审查者来说，这不是一个好习惯，但作为开发人员，您应该为此做好准备。问问自己，“审查者试图与我沟通的建设性意见是什么？”然后像他们实际说的那样操作。\n永远不要愤怒地回应代码审查评论。这严重违反了专业礼仪且将永远存在于代码审查工具中。如果您太生气或恼火而无法好好的回应，那么请离开电脑一段时间，或者做一些别的事情，直到您感到平静，可以礼貌地回答。\n一般来说，如果审查者没有以建设性和礼貌的方式提供反馈，请亲自向他们解释。如果您无法亲自或通过视频通话与他们交谈，请向他们发送私人电子邮件。以友善的方式向他们解释您不喜欢的东西以及您希望他们以怎样不同的方式来做些什么。如果他们也以非建设性的方式回复此私人讨论，或者没有预期的效果，那么请酌情上报给您的经理。\n修复代码 如果审查者说他们不了 …","relpermalink":"/eng-practices/review/developer/handling-comments/","summary":"当您发送 CL 进行审查时，您的审查者可能会对您的 CL 发表一些评论。以下是处理审查者评论的一些有用信息。 不是针对您 审查的目标是保持代码库和产品的质量。当审查者对您的代码提出批评时，请将其视为在帮助您、代码库和","title":"如何处理审查者的评论"},{"content":"对于互联网工作负载而言，可移植和互操作的网络工作负载的加密身份可能是 SPIFFE 的核心用例。为了完全满足这个需求，社区必须达成一致，采用一种标准化的方式来检索、验证和与 SPIFFE 身份进行交互。本规范概述了要支持基于 SPIFFE 的身份验证系统所需的 API 签名和客户端/服务器行为。\n引言 SPIFFE 工作负载 API 是一个 API，它提供了信息和服务，使工作负载或计算进程能够利用 SPIFFE 身份和基于 SPIFFE 的身份验证系统。它由 SPIFFE 工作负载端点提供，并由一些服务或“概要”组成。\n目前，有两个概要：\nX.509-SVID 概要 JWT-SVID 概要 这两个概要是强制性的，并且 SPIFFE 实现必须支持它们。但是，运营商可以在部署中禁用特定的概要。\n本规范的未来版本可能会引入其他概要或使一个或多个概要成为可选项。\n可扩展性 SPIFFE 工作负载 API 不能超出本规范进行扩展。希望提供扩展功能的实现者可以通过引入新的 gRPC 服务来实现，这是根据 SPIFFE 工作负载端点规范中概述的可扩展性方法来实现的。\n服务定义 SPIFFE …","relpermalink":"/spiffe-and-spire/standard/spiffe-workload-api/","summary":"对于互联网工作负载而言，可移植和互操作的网络工作负载的加密身份可能是 SPIFFE 的核心用例。为了完全满足这个需求，社区必须达成一致，采用一种标准化的方式来检索、验证和与 SPIFFE 身份进行交互。本规范概述了要支持基于 SPIFFE 的","title":"SPIFFE 工作负载 API"},{"content":"本文描述 SPIRE Agent 的命令行选项、agent.conf 设置和内置插件。\n本文档是 SPIRE Agent 的配置参考。它包括有关插件类型、内置插件、代理配置文件、插件配置和 spire-agent 命令的命令行选项的信息。\n插件类型 类型 描述 KeyManager 生成并存储代理的私钥。对于将密钥绑定到硬件等很有用。 NodeAttestor 收集用于向服务器证明代理身份的信息。一般与同类型的服务器插件搭配使用。 WorkloadAttestor 内省工作负载以确定其属性，生成一组与其关联的选择器。 SVIDStore 将 X509-SVID（私钥、叶证书和中间体（如果有））、捆绑包和联合捆绑包存储到信任存储中。 内置插件 类型 名称 描述 KeyManager disk 将私钥写入磁盘的密钥管理器 KeyManager memory 不保留私钥的内存密钥管理器（必须在重新启动后重新证明） NodeAttestor aws_iid 使用 AWS 实例身份文档证明代理身份的节点证明者 NodeAttestor azure_msi 使用 Azure MSI 令牌证明代理身 …","relpermalink":"/spiffe-and-spire/configuration/agent/","summary":"本文描述 SPIRE Agent 的命令行选项、agent.conf 设置和内置插件。 本文档是 SPIRE Agent 的配置参考。它包括有关插件类型、内置插件、代理配置文件、插件配置和 spire-agent 命令的命令行选项的信息。 插件类型 类型 描述 KeyManager 生成并存储代理的","title":"SPIRE Agent 配置参考"},{"content":"通过 Envoy 与 X.509-SVIDs 实现安全通信并结合 Open Policy Agent（OPA）进行授权。\nOpen Policy Agent（OPA）是一个开源通用策略引擎，其提供的授权（AuthZ）是对 SPIRE 提供的认证（AuthN）的很好补充。\n本教程将在 SPIRE Envoy-X.509 教程 的基础上添加 Open Policy Agent（OPA）以演示如何将 SPIRE、Envoy 和 OPA 结合使用，实现 X.509 SVID 认证和请求授权。本教程将演示如何在现有教程的基础上实现使用 OPA 进行请求授权。\n为了便于说明，让我们通过将 OPA 代理实例作为后端服务的新侧车来扩展 Envoy X.509 教程中创建的场景。借助 Envoy 的外部授权过滤器功能，结合 OPA 作为授权服务，可以实现对传入后端服务的每个请求执行安全策略。\nSPIRE Envoy OPA 集成图 如图所示，前端服务通过 Envoy 实例连接到后端服务，Envoy 实例使用 SPIRE 代理提供的 SDS 模块进行身份验证，从而建立了 mTLS 连接。Envoy …","relpermalink":"/spiffe-and-spire/examples/envoy-opa/","summary":"通过 Envoy 与 X.509-SVIDs 实现安全通信并结合 Open Policy Agent（OPA）进行授权。 Open Policy Agent（OPA）是一个开源通用策略引擎，其提供的授权（AuthZ）是对 SPIRE 提供的认证（AuthN）的很好补充。 本教程将在 SPIRE Envoy-X.509 教程 的基","title":"使用 Envoy 和 X.509-SVID 进行 OPA 授权"},{"content":"在本文档中，我们将启用 Tier-1 网关中的速率限制，并展示如何根据客户端 IP 地址进行速率限制。\n在开始之前，请确保你已经完成以下准备工作：\n熟悉 TSB 概念 安装 TSB 环境。你可以使用 TSB 演示 进行快速安装 完成 TSB 快速入门。本文档假定你已经创建了租户并熟悉工作区和配置组。还需要将 tctl 配置到你的 TSB 环境中。 部署 Tier-1 网关和 Ingress 网关 在应用任何速率限制之前，请阅读 使用 Tier-1 网关进行多集群流量转移 并熟悉使用 Tier-1 网关设置多集群配置的方法。\n其余文档假定你已完成上述步骤。\n启用速率限制服务器 阅读并按照 启用速率限制服务器文档 中的说明进行操作。\n演示安装\n如果你使用 TSB 演示 安装，你已经有一个正在运行并且可以使用的速率限制服务，可以跳过本节。 部署 httpbin 服务 按照此文档中的说明 创建 httpbin 服务，并确保该服务在 httpbin.tetrate.com 处暴露。\n创建 Tier-1 网关 创建一个名为 rate-limiting-tier1-config.yaml 的文件，该 …","relpermalink":"/tsb/howto/rate-limiting/tier1-gateway/","summary":"在本文档中，我们将启用 Tier-1 网关中的速率限制，并展示如何根据客户端 IP 地址进行速率限制。 在开始之前，请确保你已经完成以下准备工作： 熟悉 TSB 概念 安装 TSB 环境。你可以使用 TSB 演示 进行快速安装 完成 TSB 快速入门。本文档假定","title":"Tier-1 网关中的速率限制"},{"content":"本页描述了从集群中卸载 TSB 的步骤。\n注意\n此过程对于所有平面（管理、控制和数据平面）都是相似的，因为你需要删除平面的任何自定义资源，然后删除 Operator 本身。 数据平面 由于数据平面负责在你的集群中部署入口网关，因此第一步是从你的集群中删除所有 IngressGateway 自定义资源。\nkubectl delete ingressgateways.install.tetrate.io --all --all-namespaces 这将删除集群中每个命名空间中部署的所有 IngressGateway。运行此命令后，数据平面 Operator 将删除每个网关的部署和相关资源。这可能需要一些时间来确保它们都成功删除。\n为确保你正常删除 istio-operator 部署，你必须按以下顺序缩放并删除数据平面 Operator 命名空间中的剩余对象：\nkubectl -n istio-gateway scale deployment tsb-operator-data-plane --replicas=0 kubectl -n istio-gateway delete …","relpermalink":"/tsb/setup/self-managed/uninstallation/","summary":"本页描述了从集群中卸载 TSB 的步骤。 注意 此过程对于所有平面（管理、控制和数据平面）都是相似的，因为你需要删除平面的任何自定义资源，然后删除 Operator 本身。 数据平面 由于数据平面负责在你的集群中部署入口网关，因此第一","title":"TSB 卸载"},{"content":"本指南使用了扩展演示环境中描述的演示环境，包括：\n两个 Edge 集群，位于 region-1 和 region-2 两个工作负载集群，位于 region-1 和 region-2 在工作负载集群中运行的 BookInfo 应用程序 两个 Edge Gateway 负载均衡流量到工作负载集群 Edge and Workload Load Balancing 在本指南中，我们将探讨当工作负载和 Edge 集群发生故障以及如何检测这些故障时会发生什么。然后，平台 Operator 可以使用这些信息配置一种分发流量到 Edge 集群的方式，例如基于 DNS 的 GSLB 解决方案。目标是保持尽可能高的可用性，并在可能的情况下优化流量路由。\n生成和观察测试流量 请按以下方式测试演示环境：\n验证所有组件 验证所有组件 为了测试所有流程（Edge Gateway 到两个工作负载集群的流程），我们将使用加权流量分布。将 bookinfo-edge 和 bookinfo-edge-2 编辑如下，并使用 tctl 应用更改：\nrouting: rules: - route: …","relpermalink":"/tsb/design-guides/ha-multicluster/edge-failover/","summary":"本指南使用了扩展演示环境中描述的演示环境，包括： 两个 Edge 集群，位于 region-1 和 region-2 两个工作负载集群，位于 region-1 和 region-2 在工作负载集群中运行的 BookInfo 应用程序 两个 Edge Gateway 负载均衡流量到工作负载集群 Edge and Workload Load Balancing 在本指南中，我们将探讨当工","title":"边缘网关故障转移"},{"content":"在本节中，你将了解如何创建一个名为 bookinfo-ws 并绑定到 bookinfo 命名空间的 TSB 工作区。\n先决条件 在继续之前，请确保你已完成以下任务：\n熟悉 TSB 概念。 安装 TSB 演示环境。 部署 Istio Bookinfo 示例应用程序。 创建一个租户。 使用用户界面 在左侧面板的“租户”下，选择“工作区”。 单击该卡可添加新的工作区。 输入工作区 ID 作为 bookinfo-ws 。 为你的工作区提供显示名称和描述。 输入 demo/bookinfo 作为初始命名空间选择器。 单击添加。 如果你之前已成功启动演示应用程序，你应该会看到类似以下内容的内容：\n1 个集群 1 命名空间 4 服务 1 个工作区 使用 tctl 创建以下 workspace.yaml 文件：\napiversion: api.tsb.tetrate.io/v2 kind: Workspace metadata: organization: tetrate tenant: tetrate name: bookinfo-ws spec: namespaceSelector: names: …","relpermalink":"/tsb/quickstart/workspace/","summary":"在本节中，你将了解如何创建一个名为 bookinfo-ws 并绑定到 bookinfo 命名空间的 TSB 工作区。 先决条件 在继续之前，请确保你已完成以下任务： 熟悉 TSB 概念。 安装 TSB 演示环境。 部署 Istio Bookinfo 示例应用程序。 创建一个租户。 使用用户界面 在左侧面板的“","title":"创建工作区"},{"content":"当流量通过网关转发时，通常会假定流量的身份为该网关的身份。这种默认行为简化了外部流量的访问控制配置。然而，在多集群环境中，通常需要更精细的访问控制。Tetrate Service Bridge（TSB）提供了通过网关跃点保留请求的原始身份的能力，从而实现跨集群身份验证和细粒度的访问控制。\n本文档解释了如何在 TSB 中启用和利用身份传播，从而实现将消费者身份传播到远程服务、在不同集群之间实施详细的访问控制以及将访问控制规则应用于故障转移目标等场景。\n在继续之前，假定你熟悉 TSB 的概念和术语，如入口网关、Tier-1 网关和东西网关。\nGitOps\n本文档中的示例使用了 TSB 的 GitOps 功能，允许你使用 kubectl 应用 TSB 配置。要在你的 TSB 环境中启用 GitOps，请参见启用 GitOps，并了解如何在 TSB 中使用 GitOps 工作流程的详细信息GitOps 工作原理。 启用身份传播 默认情况下，由于网关处的 TLS 终止，服务身份不会通过网关跃点传播。TSB 使用每个网关跃点上的内部 WebAssembly（WASM）扩展实现身份传播。该扩展验证客 …","relpermalink":"/tsb/howto/gateway/service-identity-propagation/","summary":"当流量通过网关转发时，通常会假定流量的身份为该网关的身份。这种默认行为简化了外部流量的访问控制配置。然而，在多集群环境中，通常需要更精细的访问控制。Tetrate Service Bridge（TSB）提供了通过网关","title":"多集群访问控制和身份传播"},{"content":"工作负载命名 加入到 mesh 中的工作负载由 Kubernetes 资源 WorkloadAutoRegistration 表示。\n当新的工作负载加入到 mesh 中并加入到给定的 WorkloadGroup 时，工作负载载入端点会在该 WorkloadGroup 的命名空间中创建一个 WorkloadAutoRegistration 资源。\n每个 WorkloadAutoRegistration 资源都被分配一个唯一的名称，格式为：\n\u0026lt;workload-group-name\u0026gt;-\u0026lt;workload-identity\u0026gt; 其中 workload-identity 是由 TSB 生成的唯一名称。对于在 AWS EC2 实例上运行的工作负载，它的 workload-identity 将采用以下格式：\naws-\u0026lt;aws-partition\u0026gt;-\u0026lt;aws-account\u0026gt;-\u0026lt;aws-zone\u0026gt;-ec2-\u0026lt;aws-ec2-instance-id\u0026gt; 综合起来，工作负载的唯一名称可能看起来像下面的例子： …","relpermalink":"/tsb/setup/workload-onboarding/guides/managing/","summary":"工作负载命名 加入到 mesh 中的工作负载由 Kubernetes 资源 WorkloadAutoRegistration 表示。 当新的工作负载加入到 mesh 中并加入到给定的 WorkloadGroup 时，工作负载载入端点会在该 WorkloadGroup 的命名空间中创建一个 WorkloadAutoRegistration 资源。 每个 WorkloadAutoRegistration 资源都被分配一个唯一的名称，格式为： \u003cworkload-group-name\u003e-\u003cworkload-identity\u003e 其中 workload-identity 是由 TSB","title":"管理已载入的工作负载"},{"content":"Tetrate 管理平面从每个接入的工作负载平台以及启用了适当 Skywalking 客户端的其他工作负载中收集指标和跟踪信息。这些指标存储在 ElasticSearch 数据库中，并通过 Tetrate UI 中的仪表板提供：\n仪表板 UI：服务实例指标 查看 TSB 或 TSE 示例以获取更多详细信息。\n用户需要访问 Tetrate UI 才能查看这些指标。在简单的单团队部署中可能是可行的，并且在 TSB 中，可以通过为应用程序所有者提供基于角色的访问权限，在非常大规模的情况下也可能是可行的，但在许多情况下，为你的应用程序所有者团队提供访问权限可能不是可行的或不适当的做法。\n另一种方法是将 Tetrate 收集的指标导出到第三方仪表板，如 Grafana。这通常是提供大量用户访问 Tetrate 指标的最合适方式，特别是如果你已经有一个成熟的企业仪表板解决方案。\n在第三方仪表板中公开 Tetrate 指标 你需要公开你的 TSE 指标并安排一个仪表板收集器来抓取并标记它们。以下资源可能会对你有所帮助：\n了解 Tetrate 指标架构 TSE AWS 托管 Grafana …","relpermalink":"/tsb/design-guides/app-onboarding/monitor/","summary":"Tetrate 管理平面从每个接入的工作负载平台以及启用了适当 Skywalking 客户端的其他工作负载中收集指标和跟踪信息。这些指标存储在 ElasticSearch 数据库中，并通过 Tetrate UI 中的仪表板提供： 仪表板 UI：服务实例指标 查看 TSB 或 TSE 示例以获取更多详细信息","title":"监控指标"},{"content":"本文将介绍如何使用 HTTPS 重试和超时将流量发送到外部主机。\n在开始之前，请确保你已经：\n熟悉TSB 概念 安装了 TSB 环境。你可以使用TSB 演示进行快速安装 完成了TSB 使用快速入门。 理解问题 考虑一个通过ServiceEntry添加到网格中的外部应用程序。该应用程序监听 HTTPS，因此你将发送的流量预期使用简单的 TLS。\n网格内的应用程序客户端将发起 HTTP 请求，并在 Sidecar到外部应用程序主机的过程中将其转换为 HTTPS，例如www.tetrate.io。这是由于在 DestinationRule 中定义的出站流量策略实现的。\n以下是你需要设置的内容，以实现客户端与外部主机之间的通信：\n直接模式\n这仅在使用 TSB 直接模式配置时有效。 首先，为你的 Istio 对象创建一个命名空间：\nkubectl create ns tetrate 创建一个名为tetrate.yaml的文件，其中包含以下 ServiceEntry、VirtualService 和 DestinationRule。\nkind: ServiceEntry apiVersion: …","relpermalink":"/tsb/howto/traffic/external-site-https/","summary":"本文将介绍如何使用 HTTPS 重试和超时将流量发送到外部主机。 在开始之前，请确保你已经： 熟悉TSB 概念 安装了 TSB 环境。你可以使用TSB 演示进行快速安装 完成了TSB 使用快速入门。 理解问题 考虑一个通过ServiceE","title":"将流量发送到使用 HTTPS 的外部主机"},{"content":" 注意\nTetrate Service Bridge 收集了大量的指标，而你设置的指标和阈值限制将因环境而异。本文档概述了通用的警报指南，而不是提供详尽的警报配置和阈值列表，因为这些将因不同环境和不同工作负载配置而异。 TSB 运维状态 TSB 可用性 TSB API 的成功请求率。这是一个非常容易被用户看到的信号，应该作为这样对待。\n根据在你的环境中捕获的历史度量数据作为基线来确定 THRESHOLD 值。首次迭代的合理值可能为 0.99。\n示例 PromQL 表达式：\nsum( rate( grpc_server_handled_total{ component=\u0026#34;tsb\u0026#34;, grpc_code=\u0026#34;OK\u0026#34;, grpc_type=\u0026#34;unary\u0026#34;, grpc_method!=\u0026#34;SendAuditLog\u0026#34; }[1m] ) ) BY (grpc_method) / sum( rate( grpc_server_handled_total{ component=\u0026#34;tsb\u0026#34;, grpc_type=\u0026#34;unary\u0026#34;, grpc_method!=\u0026#34;SendAuditLog\u0026#34; }[1m] ) ) BY …","relpermalink":"/tsb/operations/telemetry/alerting-guidelines/","summary":"注意 Tetrate Service Bridge 收集了大量的指标，而你设置的指标和阈值限制将因环境而异。本文档概述了通用的警报指南，而不是提供详尽的警报配置和阈值列表，因为这些将因不同环境和不同工作负载配置而异。 TSB 运维状态 TSB 可用性 TSB API 的成","title":"警报指南"},{"content":"Tetrate Service Bridge (TSB) 提供强大的流量管理功能，允许有效控制其域内服务之间的流量。TSB 简化了流量路由、分阶段部署和迁移等复杂任务，从而增强了应用程序交付的整体方法。\nTSB 中的网关 TSB 中的基本应用程序流量。 TSB 使用一系列网关来管理流量路由。当流量进入你的 TSB 环境时，它会在到达其预期应用程序之前遍历各个网关。该过程涉及：\n应用程序边缘网关：也称为“边缘网关”，这种共享多租户网关有助于跨集群负载平衡。它将传入流量定向到适当的应用程序入口网关。 应用程序入口网关：称为“应用程序网关”，该网关可以在多个应用程序之间共享，也可以专用于特定应用程序。它控制流量的流动方式以及与应用程序的交互方式。应用程序入口网关归工作区所有，提供对流量的控制。 建议部署多个入口网关以隔离和控制流量。随着时间的推移，随着对网格使用的信心不断增强，可以考虑整合到共享入口网关上。\n智能流量路由 TSB 通过利用每个集群内本地控制平面的信息来确保智能流量路由。它优先考虑本地流量以获得最佳性能和可用性。Envoy 的按请求功能可以对流量路由进行细粒度控制，支持请求粘 …","relpermalink":"/tsb/concepts/traffic-management/","summary":"Tetrate Service Bridge (TSB) 提供强大的流量管理功能，允许有效控制其域内服务之间的流量。TSB 简化了流量路由、分阶段部署和迁移等复杂任务，从而增强了应用程序交付的整体方法。 TSB 中的网关 TSB 中的基本应用程序流量。 TSB 使用一系列网关","title":"流量管理"},{"content":"启动 AWS EC2 实例 使用以下配置启动 AWS EC2 实例：\n选择带有 Ubuntu Server（DEB）的 64 位 (x86) AMI 镜像。 选择最小的 实例类型，例如 t2.micro（1 个 vCPU，1 GiB RAM） 或 t2.nano（1 个 vCPU，0.5 GiB RAM） 选择默认 VPC（以使你的实例具有公共 IP） 将 自动分配公共 IP 设置为 启用 配置 安全组，允许从 0.0.0.0/0 的端口 9080 接收流量 为了本指南的目的，你将创建一个具有公共 IP 的 EC2 实例，以便进行配置。\n注意\n这不建议用于生产场景。对于生产场景，你应该做相反的操作，将 Kubernetes 集群和 EC2 实例放在同一网络上，或进行网络对等连接，并不为你的虚拟机分配公共 IP。 安装 Bookinfo Ratings 应用程序 SSH 进入你创建的 AWS EC2 实例，并安装 ratings 应用程序。执行以下命令：\n# 安装最新版本的受信任 CA 证书 sudo apt-get update -y sudo apt-get install -y …","relpermalink":"/tsb/setup/workload-onboarding/quickstart/aws-ec2/configure-vm/","summary":"启动 AWS EC2 实例 使用以下配置启动 AWS EC2 实例： 选择带有 Ubuntu Server（DEB）的 64 位 (x86) AMI 镜像。 选择最小的 实例类型，例如 t2.micro（1 个 vCPU，1 GiB RAM） 或 t2.nano（1 个 vCPU，0.5 GiB RAM","title":"配置虚拟机"},{"content":"Tetrate Service Bridge (TSB) 是一个由各种协议相互连接的复杂组件集合。对于部署在 TSB 提供的服务网格上的应用程序来说，情况可能也是如此。在许多情况下，你需要检查、测试和验证各种 TSB 组件之间的网络连通性，以确保系统按预期工作。\n为了为你节省在 Kubernetes 集群中创建调试环境的时间，Tetrate 提供了一个调试容器，其中已经安装了大多数用于验证网络状态的工具集。例如，诸如 ping、curl、gpcurl、dig 等工具已经在此容器中安装。\n使用调试容器 只要可以访问适当的镜像仓库来下载容器镜像，这个调试容器就可以部署到任何集群中。\n容器镜像包含在 TSB 发行版中，并且将与运行 tctl install image-sync 命令 时的其余镜像一起同步到你的仓库中。\n要部署调试容器，请运行以下命令。将 \u0026lt;registry-location\u0026gt; 替换为你同步了 TSB 镜像的仓库 URL。\nkubectl run debug-container --image …","relpermalink":"/tsb/troubleshooting/debug-container/","summary":"Tetrate Service Bridge (TSB) 是一个由各种协议相互连接的复杂组件集合。对于部署在 TSB 提供的服务网格上的应用程序来说，情况可能也是如此。在许多情况下，你需要检查、测试和验证各种 TSB 组件之间的网络连通性，以确保系统按预期工作。 为了","title":"使用调试容器"},{"content":"此 Chart 安装 TSB 数据平面 Operator，用于管理 网关，如 Ingress 网关、Tier-1 网关和 Egress 网关的生命周期。\n注意\n如果你正在使用基于版本的控制平面，则不再需要数据平面 Operator 来管理 Istio 网关。要了解有关基于版本的控制平面的更多信息，请参阅 Istio 隔离边界 文档。 安装 要安装数据平面 Operator，请运行以下 Helm 命令。确保将 \u0026lt;tsb-version\u0026gt; 和 \u0026lt;registry-location\u0026gt; 替换为正确的值。\nhelm install dp tetrate-tsb-helm/dataplane \\ --version \u0026lt;tsb-version\u0026gt; \\ --namespace istio-gateway --create-namespace \\ --set image.registry=\u0026lt;registry-location\u0026gt; 配置 镜像配置 这是一个 必填 字段。将 image.registry 设置为你的私有注册表位置，你已经同步了 TSB 镜像，将 image.tag 设置为要部署的 TSB 版本。 …","relpermalink":"/tsb/setup/helm/dataplane/","summary":"此 Chart 安装 TSB 数据平面 Operator，用于管理 网关，如 Ingress 网关、Tier-1 网关和 Egress 网关的生命周期。 注意 如果你正在使用基于版本的控制平面，则不再需要数据平面 Operator 来管理 Istio 网关。要了解有关基于版本的控制平面的更","title":"数据平面安装"},{"content":"如果删除网关（例如在缩容事件期间），远程集群将继续尝试将流量发送到网关 IP 地址，直到它们收到网关的 IP 地址已被移除的更新为止。这可能会导致 HTTP 流量的 503 错误或直通跨集群流量的 000 错误。\n自 TSB 1.6 版以来，你可以通过可配置的周期来延迟网关删除，以便提供足够的时间使网关的 IP 地址移除传播到其他集群，以避免 503 或 000 错误。目前，此功能默认处于禁用状态。\n启用网关删除保持 Webhook 为了在控制平面中启用网关删除保持 Webhook，你需要编辑 ControlPlane CR 或 Helm 值中的 xcp 组件，并添加以下环境变量：\nENABLE_GATEWAY_DELETE_HOLD，将其值设置为 true GATEWAY_DELETE_HOLD_SECONDS。这是可选的，默认值为 10 秒 spec: components: xcp: ... kubeSpec: deployment: env: - name: ENABLE_GATEWAY_DELETE_HOLD value: \u0026#34;true\u0026#34; - name: …","relpermalink":"/tsb/operations/features/gateway-deletion-webhook/","summary":"如果删除网关（例如在缩容事件期间），远程集群将继续尝试将流量发送到网关 IP 地址，直到它们收到网关的 IP 地址已被移除的更新为止。这可能会导致 HTTP 流量的 503 错误或直通跨集群流量的 000 错误。 自 TSB 1.6 版以来，你可以通过可","title":"网关删除保持 Webhook"},{"content":"在继续之前，请确保你熟悉Istio 隔离边界功能。\n修订的 Istio CNI 在修订的 Istio 环境中，Istio CNI 可以绑定到特定的 Istio 修订版本。考虑以下隔离边界配置，允许管理修订的 Istio 环境：\nspec: ... components: xcp: isolationBoundaries: - name: global revisions: - name: stable istio: tsbVersion: 1.6.1 一旦有了修订版本，可以按照以下所示的隔离边界配置启用 Istio CNI，并指定修订版本：\n非 OpenShift\napiVersion: install.tetrate.io/v1alpha1 kind: ControlPlane metadata: name: \u0026lt;cluster-name\u0026gt; namespace: istio-system spec: components: istio: kubeSpec: CNI: chained: true binaryDirectory: /opt/cni/bin …","relpermalink":"/tsb/setup/upgrades/cni-upgrade/","summary":"在继续之前，请确保你熟悉Istio 隔离边界功能。 修订的 Istio CNI 在修订的 Istio 环境中，Istio CNI 可以绑定到特定的 Istio 修订版本。考虑以下隔离边界配置，允许管理修订的 Istio 环境： spec: ... components: xcp: isolationBoundaries: - name: global revisions: - name: stable istio: tsbVersion: 1.6.1 一旦有了修","title":"修订的 Istio CNI 和升级"},{"content":"TSB 拥有一个 teamsync 组件，它会定期连接到你的身份提供者（IdP），并将用户和团队信息同步到 TSB 中。\n目前，teamsync 支持LDAP和Azure AD，并会自动为你执行正确的操作。但是，如果你使用其他 IdP，你将需要手动执行这些任务。本文将描述如何执行这些任务。\n在开始之前，请确保你已经：\n安装了 TSB 管理平面 使用管理员帐户登录到 TSB。 获取你的 TSB 的组织名称 - 确保使用在 TSB ManagementPlane CR 中配置的安装时的组织名称。 创建组织 Teamsync 不仅同步你的用户和团队，还会在首次运行 TSB 管理平面组件安装后创建一个组织。\n因此，如果你使用的 IdP 不受 teamsync 支持，你还需要手动执行此步骤。\n要创建一个组织，请创建以下organization.yaml文件，然后使用 tctl 应用它\napiVersion: api.tsb.tetrate.io/v2 kind: Organization metadata: name: \u0026lt;organization-name\u0026gt; tctl apply -f …","relpermalink":"/tsb/operations/users/user-synchronization/","summary":"TSB 拥有一个 teamsync 组件，它会定期连接到你的身份提供者（IdP），并将用户和团队信息同步到 TSB 中。 目前，teamsync 支持LDAP和Azure AD，并会自动为你执行正确的操作。但是，如果你使用其他 IdP，你将","title":"用户同步"},{"content":"TSB 提供了授权功能，用于授权来自公共网络的请求。本文将描述如何使用 Open Policy Agent (OPA) 配置 Tier-1 网关授权。\n在开始之前，请确保你：\n熟悉TSB 概念。 已完成了 Tier-1 网关路由到 Tier-2 网关，并在 TSB 中已配置了httpbin。 创建了一个租户，并了解工作空间和配置组。 针对你的 TSB 环境配置了 tctl。 以下图示展示了在Tier-1网关中使用OPA的请求/响应流程。来到Tier-1网关的请求将由OPA检查。如果请求被视为未经授权，则请求将被拒绝并返回403（禁止）响应，否则它们将被发送到Tier-2网关。\n部署 httpbin 服务 按照本文中的说明创建 httpbin 服务，并确保该服务在 httpbin.tetrate.com 上公开。\n配置 OPA 在此示例中，你将部署 OPA 作为其自己独立的服务。如果尚未这样做，请为 OPA 服务创建一个命名空间：\nkubectl create namespace opa 按照OPA 文档中的说明创建使用基本身份验证的 OPA 策略，并在 opa 命名空间中部署 OPA  …","relpermalink":"/tsb/howto/authorization/tier1-gateway/","summary":"TSB 提供了授权功能，用于授权来自公共网络的请求。本文将描述如何使用 Open Policy Agent (OPA) 配置 Tier-1 网关授权。 在开始之前，请确保你： 熟悉TSB 概念。 已完成了 Tier-1 网关路由到 Tier-2 网关，并在 TSB 中已配置了httpbin。 创建了一个租户","title":"在 Tier-1 网关中使用外部授权"},{"content":"你可以从此存储库的最新发布页面下载最新的 Argo CD 版本，其中将包含argocd CLI。\nLinux 和 WSL ArchLinux pacman -S argocd Homebrew brew install argocd 使用 curl 下载 下载最新版本 curl -sSL -o argocd-linux-amd64 https://github.com/argoproj/argo-cd/releases/latest/download/argocd-linux-amd64 sudo install -m 555 argocd-linux-amd64 /usr/local/bin/argocd rm argocd-linux-amd64 下载具体版本 VERSION将以下命令中的替换设置\u0026lt;TAG\u0026gt;为你要下载的 Argo CD 版本：\nVERSION=\u0026lt;TAG\u0026gt; # Select desired TAG from https://github.com/argoproj/argo-cd/releases curl -sSL -o argocd-linux-amd64 …","relpermalink":"/argo-cd/cli-installation/","summary":"你可以从此存储库的最新发布页面下载最新的 Argo CD 版本，其中将包含argocd CLI。 Linux 和 WSL ArchLinux pacman -S argocd Homebrew brew install argocd 使用 curl 下载 下载最新版本 curl -sSL -o argocd-linux-amd64 https://github.com/argoproj/argo-cd/releases/latest/download/argocd-linux-amd64 sudo install -m 555 argocd-linux-amd64 /usr/local/bin/argocd rm argocd-linux-amd64 下载具体版本 VERSION将以下命令中的替换设置\u0026","title":"安装"},{"content":"要求 AWS 负载均衡器控制器 v1.1.5 或更高版本 概述 AWS 负载均衡器控制器（也称为 AWS ALB Ingress Controller）通过配置 AWS 应用程序负载均衡器（ALB）以将流量路由到一个或多个 Kubernetes 服务的 Ingress 对象，实现流量管理。ALB 通过加权目标组的概念提供了高级流量分割功能。AWS 负载均衡器控制器通过对 Ingress 对象的注解进行配置“操作”来支持此功能。\n工作原理 ALB 通过侦听器和包含操作的规则进行配置。侦听器定义客户端的流量如何进入，规则定义如何使用各种操作处理这些请求。一种操作类型允许用户将流量转发到多个目标组（每个目标组都定义为 Kubernetes 服务）。你可以在此处阅读有关 ALB 概念的更多信息。\n由 AWS 负载均衡器控制器管理的 Ingress 通过注解和规范控制 ALB 的侦听器和规则。为了在多个目标组（例如不同的 Kubernetes 服务）之间分割流量，AWS 负载均衡器控制器查看 Ingress 上的特定“操作”注 …","relpermalink":"/argo-rollouts/traffic-management/alb/","summary":"要求 AWS 负载均衡器控制器 v1.1.5 或更高版本 概述 AWS 负载均衡器控制器（也称为 AWS ALB Ingress Controller）通过配置 AWS 应用程序负载均衡器（ALB）以将流量路由到一个或多个 Kubernetes 服务的 Ingress 对象，实现流量管理。ALB 通过加权目","title":"AWS Load Balancer Controller (ALB)"},{"content":"本指南介绍了如何让 Argo Rollouts 与由 AWS App Mesh 管理的服务网格集成。本指南基于 基本入门指南 的概念构建。\n要求\n安装了 AWS App Mesh Controller for K8s 的 Kubernetes 集群 🔔 提示：请参阅 App Mesh Controler Installation instructions 以了解如何开始使用 Kubernetes 的 App Mesh。\n1. 部署 Rollout、服务和 App Mesh CRD 当 App Mesh 用作流量路由器时，Rollout canary 策略必须定义以下强制字段：\napiVersion: argoproj.io/v1alpha1 kind: Rollout metadata: name: my-rollout spec: strategy: canary: # canaryService 和 stableService 是指向 Rollout 将要修改的服务的引用，以便针对金丝雀 ReplicaSet 和稳定 ReplicaSet（均为必需）。 canaryService: …","relpermalink":"/argo-rollouts/getting-started/appmesh/","summary":"本指南介绍了如何让 Argo Rollouts 与由 AWS App Mesh 管理的服务网格集成。本指南基于 基本入门指南 的概念构建。 要求 安装了 AWS App Mesh Controller for K8s 的 Kubernetes 集群 🔔 提示：请参阅 App Mesh Controler Installation instructions 以了解如何开始使用 Kubernetes 的 App Mesh。 1. 部署 Rollout、服务","title":"AppMesh 快速开始"},{"content":"垂直 Pod 自动缩放（Vertical Pod Autoscaling，VPA）通过自动配置资源需求降低维护成本并提高集群资源利用率。\nVPA 模式 VPAs 有四种操作模式：\nAuto：VPA 在创建 pod 时分配资源请求，使用首选的更新机制更新现有 pod 的资源请求。目前，这相当于“Recreate”（见下文）。一旦在不重启（“in-place”）更新 pod 请求方面可用，它可能会成为“Auto”模式的首选更新机制。注意：此 VPA 功能是实验性的，可能会导致你的应用停机。 Recreate：VPA 在创建 pod 时分配资源请求，并在请求的资源与新建议显著不同时将其从现有 pod 中驱逐出来（如果定义了 Pod Disruption Budget，则会尊重它）。只有在需要确保每当资源请求更改时都重启 pod 时才应使用此模式。否则，请优先考虑“Auto”模式，一旦可用，该模式可以利用不重启更新。注意：此 VPA 功能是实验性的，可能会导致你的应用停机。 Initial：VPA 仅在创建 pod 时分配资源请求，从不更改它们。 Off：VPA 不会自动更改 pod 的资源要 …","relpermalink":"/argo-rollouts/rollout/vpa-support/","summary":"垂直 Pod 自动缩放（Vertical Pod Autoscaling，VPA）通过自动配置资源需求降低维护成本并提高集群资源利用率。 VPA 模式 VPAs 有四种操作模式： Auto：VPA 在创建 pod 时分配资源请求，使用首选的更新机","title":"垂直 Pod 自动缩放"},{"content":"这里是 Argo Rollouts 管理的部署中所有参与的组件的概述。\nArgo Rollouts 架构 Argo Rollouts 控制器 这是主要的控制器，它监视集群中的事件并在更改 Rollout 类型的资源时做出反应。控制器将读取所有 Rollout 的详细信息，并将集群带到与 Rollout 定义中描述的相同状态。\n请注意，Argo Rollouts 不会干涉或响应在普通的 Deployment 资源 上发生的任何更改。这意味着你可以在以其他方法部署应用程序的集群中安装 Argo Rollouts。\n要在你的集群中安装控制器并开始进行渐进式交付，请参见安装页面。\nRollout 资源 Rollout 资源是由 Argo Rollouts 引入和管理的自定义 Kubernetes 资源。它与原生 Kubernetes Deployment 资源大多兼容，但具有控制高级部署方法（如金丝雀和蓝/绿部署）的阶段、阈值和方法的额外字段。\n请注意，Argo Rollouts 控制器仅响应在 Rollout 源中发生的那些更改。它不会对普通的 Deployment 资源做任何事情。这意味 …","relpermalink":"/argo-rollouts/architecture/","summary":"这里是 Argo Rollouts 管理的部署中所有参与的组件的概述。 Argo Rollouts 架构 Argo Rollouts 控制器 这是主要的控制器，它监视集群中的事件并在更改 Rollout 类型的资源时做出反应。控制器将读取所有 Rollout 的详细信息，并将集群带到与 Rollout 定义中描述的相同状态。 请","title":"架构"},{"content":"本章解释了什么是身份，以及分配、管理和使用身份的基本知识。这些是你需要知道的概念，以便了解 SPIFFE 和 SPIRE 的工作方式。\n什么是身份？ 对于人类来说，身份是复杂的。人类是独特的个体，不能被克隆，也不能用代码取代他们的思想，而且一生中可能会有多种社会身份。软件服务也同样复杂。\n一个单一的程序可能会扩展到成千上万的节点，或者在构建系统推送新的更新时，一天内多次改变其代码。在这样一个快速变化的环境中，一个身份必须代表服务的特定逻辑目的（例如，客户计费数据库）和与已建立的权威或信任根（例如，my-company.example.org 或生产工作负载的发行机构）的关联。\n一旦为一个组织中的所有服务发布了身份，它们就可以被用于认证：证明一个服务是它所声称的那样。一旦服务经过相互认证，它们就可以使用身份进行授权，或控制谁可以访问这些服务，以及保密性，或保持它们相互传输的数据的秘密。虽然 SPIFFE 本身并不包括认证、授权或保密性，但它发出的身份可用于所有这些。\n为一个组织指定服务身份与设计该组织基础设施的任何其他部分类似：它密切依赖于该组织的需求。当一个服务扩大规模、改变代码或移动 …","relpermalink":"/spiffe/general-concepts-behind-identity/","summary":"本章解释了什么是身份，以及分配、管理和使用身份的基本知识。这些是你需要知道的概念，以便了解 SPIFFE 和 SPIRE 的工作方式。 什么是身份？ 对于人类来说，身份是复杂的。人类是独特的个体，不能被克隆，也不能用代码取代他们的","title":"身份背后的通用概念"},{"content":"根据所使用的 Linux 内核版本，eBPF 数据路径可以完全在 eBPF 中实现不同的功能集。如果某些所需功能不可用，则使用旧版 iptables 实现提供该功能。有关详细信息，请参阅 IPsec 要求。\nkube-proxy 互操作性 下图显示了 kube-proxy 安装的 iptables 规则和 Cilium 安装的 iptables 规则的集成。\n图片 下一章 ","relpermalink":"/cilium-handbook/ebpf/iptables/","summary":"根据所使用的 Linux 内核版本，eBPF 数据路径可以完全在 eBPF 中实现不同的功能集。如果某些所需功能不可用，则使用旧版 iptables 实现提供该功能。有关详细信息，请参阅 IPsec 要求。 kube-proxy 互操作性 下图显示了 kube-proxy 安装的 iptables 规则和 Cilium 安装的 iptables 规","title":"Iptables 用法"},{"content":"默认情况下，Cilium 将 eBPF 数据路径配置为执行 IP 分片跟踪，以允许不支持分片的协议（例如 UDP）通过网络透明地传输大型消息。IP 分片跟踪在 eBPF 中使用 LRU（最近最少使用）映射实现，需要 Linux 4.10 或更高版本。可以使用以下选项配置此功能：\n--enable-ipv4-fragment-tracking：启用或禁用 IPv4 分片跟踪。默认启用。 --bpf-fragments-map-max：控制使用 IP 分片的最大活动并发连接数。对于默认值，请参阅eBPF Maps。 注意\n当使用 kube-proxy 运行 Cilium 时，碎片化的 NodePort 流量可能会由于内核错误而中断，其中路由 MTU 不受转发数据包的影响。Cilium 碎片跟踪需要第一个逻辑碎片首先到达。由于内核错误，可能会在外部封装层上发生额外的碎片，从而导致数据包重新排序并导致无法跟踪碎片。\n内核错误已被 修复 并向后移植到所有维护的内核版本。如果您发现连接问题，请确保您的节点上的内核包最近已升级，然后再报告问题。\n提示\n这是一个测试版功能。如果你遇到任何问题，请提供反 …","relpermalink":"/cilium-handbook/networking/fragmentation/","summary":"默认情况下，Cilium 将 eBPF 数据路径配置为执行 IP 分片跟踪，以允许不支持分片的协议（例如 UDP）通过网络透明地传输大型消息。IP 分片跟踪在 eBPF 中使用 LRU（最近最少使用）映射实现，需要 Linux 4.10 或更高版本。可以","title":"IPv4 分片处理"},{"content":"Cilium 能够透明地将四层代理注入任何网络连接中。这被用作执行更高级别网络策略的基础（请参阅基于 DNS 和 七层示例）。\n可以注入以下代理：\nEnvoy 注入 Envoy 代理 注意\n此功能目前正处于测试阶段。 如果你有兴趣编写 Envoy 代理的 Go 扩展，请参考开发者指南。\nEnvoy 代理注入示意图 如上所述，该框架允许开发人员编写少量 Go 代码（绿色框），专注于解析新的 API 协议，并且该 Go 代码能够充分利用 Cilium 功能，包括高性能重定向 Envoy、丰富的七层感知策略语言和访问日志记录，以及通过 kTLS 对加密流量的可视性（即将推出）。总而言之，作为开发者的你只需要关心解析协议的逻辑，Cilium + Envoy + eBPF 就完成了繁重的工作。\n本指南基于假设的 r2d2 协议（参见 proxylib/r2d2/r2d2parser.go）的简单示例，该协议可用于很久以前在遥远的星系中与简单的协议机器人对话。但它也指向了 cilium/proxylib 目录中已经存在的其他真实协议，例如 Memcached 和 Cassandra。\n下一章 ","relpermalink":"/cilium-handbook/security/proxy/","summary":"Cilium 能够透明地将四层代理注入任何网络连接中。这被用作执行更高级别网络策略的基础（请参阅基于 DNS 和 七层示例）。 可以注入以下代理： Envoy 注入 Envoy 代理 注意 此功能目前正处于测试阶段。 如果你有兴趣编写 Envoy 代理的 Go 扩展，请参","title":"代理注入"},{"content":"策略规则到端点映射 确定哪些策略规则当前对端点有效，并且可以将来自 cilium endpoint list 和 cilium endpoint get 的数据与 cilium policy get 配对。cilium endpoint get 将列出适用于端点的每个规则的标签。可以传递标签列表给 cilium policy get 以显示确切的源策略。请注意，不能单独获取没有标签的规则（无标签会返回节点上的完整策略）。具有相同标签的规则将一起返回。\n在下面的示例中，其中一个 deathstar pod 的端点 id 是 568。我们可以打印应用于它的所有策略：\n$ # Get a shell on the Cilium pod $ kubectl exec -ti cilium-88k78 -n kube-system -- /bin/bash $ # print out the ingress labels $ # clean up the data $ # fetch each policy via each set of labels $ # (Note that while …","relpermalink":"/cilium-handbook/policy/troubleshooting/","summary":"策略规则到端点映射 确定哪些策略规则当前对端点有效，并且可以将来自 cilium endpoint list 和 cilium endpoint get 的数据与 cilium policy get 配对。cilium endpoint get 将列出适用于端点的每个规则的标签。可以传递标签列表给 cilium policy get 以显示确切的源策略。请注意，","title":"故障排除"},{"content":"现在你已经看到了 eBPF 编程的例子，了解到它是如何工作的。虽然基础示例使得 eBPF 看起来相对简单，但也有一些复杂的地方使得 eBPF 编程充满挑战。\n长期以来，有个问题使得编写和发布 eBPF 程序相对困难，那就是内核兼容性。\n跨内核的可移植性 eBPF 程序可以访问内核数据结构，而这些结构可能在不同的内核版本中发生变化。这些结构本身被定义在头文件中，构成了 Linux 源代码的一部分。在过去编译 eBPF 程序时，必须基于你想运行这些程序的内核兼容的头文件集。\nBCC 对可移植性的处理方法 为了解决跨内核的可移植性问题，BCC 1（BPF 编译器集合，BPF Compiler Collection）项目采取了在运行时编译 eBPF 代码的方法，在目标机器上就地进行。这意味着编译工具链需要安装到每个你想让代码运行 2 的目标机器上，而且你必须在工具启动之前等待编译完成，而且文件系统上必须有内核头文件（实际上并不总是这样）。这就引出了 BPF CO-RE。\nCO-RE CO-RE（Compile Once, Run Everyone，编译一次，到处运行）方法由以下元素组成。 …","relpermalink":"/what-is-ebpf/ebpf-complexity/","summary":"现在你已经看到了 eBPF 编程的例子，了解到它是如何工作的。虽然基础示例使得 eBPF 看起来相对简单，但也有一些复杂的地方使得 eBPF 编程充满挑战。 长期以来，有个问题使得编写和发布 eBPF 程序相对困难，那就是内核兼容性。 跨内核的","title":"第四章：eBPF 的复杂性"},{"content":"这一章将介绍 Kubernetes 的设计理念及基本概念。\nKubernetes 设计理念与分布式系统 分析和理解 Kubernetes 的设计理念可以使我们更深入地了解 Kubernetes 系统，更好地利用它管理分布式部署的云原生应用，另一方面也可以让我们借鉴其在分布式系统设计方面的经验。\n分层架构 Kubernetes 设计理念和功能其实就是一个类似 Linux 的分层架构，如下图所示\nKubernetes 分层架构示意图 核心层：Kubernetes 最核心的功能，对外提供 API 构建高层的应用，对内提供插件式应用执行环境 应用层：部署（无状态应用、有状态应用、批处理任务、集群应用等）和路由（服务发现、DNS 解析等） 管理层：系统度量（如基础设施、容器和网络的度量），自动化（如自动扩展、动态 Provision 等）以及策略管理（RBAC、Quota、PSP、NetworkPolicy 等） 接口层：kubectl 命令行工具、客户端 SDK 以及集群联邦 生态系统：在接口层之上的庞大容器集群管理调度的生态系统，可以划分为两个范畴 Kubernetes 外部：日志、监控、配 …","relpermalink":"/kubernetes-handbook/architecture/perspective/","summary":"这一章将介绍 Kubernetes 的设计理念及基本概念。 Kubernetes 设计理念与分布式系统 分析和理解 Kubernetes 的设计理念可以使我们更深入地了解 Kubernetes 系统，更好地利用它管理分布式部署的云原生应用，另一方面也可以让我们借鉴其在分布式系统设计方面的经","title":"Kubernetes 的设计理念"},{"content":"从理论上讲，DevSecOps 原语可以应用于许多应用架构，但最适合于基于微服务的架构，由于应用是由相对较小的、松散耦合的模块组成的，被称为微服务，因此允许采用敏捷开发模式。即使在基于微服务的架构中，DevSecOps 原语的实现也可以采取不同的形式，这取决于平台。在本文中，所选择的平台是一个容器编排和资源管理平台（如 Kubernetes）。该平台是物理（裸机）或虚拟化（如虚拟机、容器）基础设施上的一个抽象层。为了在本文中明确提及该平台或应用环境，它被称为 DevSecOps 原语参考平台，或简称为 参考平台。\n在描述参考平台的 DevSecOps 原语的实现之前，我们假设在部署服务网格组件方面采用了以下 尽职调查：\n用于部署和管理基于服务网格的基础设施（如网络路由）、策略执行和监控组件的安全设计模式 测试证明这些服务网格组件在应用的各个方面（如入口、出口和内部服务）的各种情况下都能按预期工作。 为参考平台实施 DevSecOps 原语所提供的指导与 (a) DevSecOps 管道中使用的工具和 (b) 提供应用服务的服务网格软件无关，尽管来自 Istio 等服务网格产品的例子被用 …","relpermalink":"/service-mesh-devsecops/intro/scope/","summary":"从理论上讲，DevSecOps 原语可以应用于许多应用架构，但最适合于基于微服务的架构，由于应用是由相对较小的、松散耦合的模块组成的，被称为微服务，因此允许采用敏捷开发模式。即使在基于微服务的架构中，D","title":"1.1 范围"},{"content":"本章中，我们讨论了希望通过软件赋予我们业务的能力并迁移到云原生应用架构的动机：\n速度\n比我们的竞争对手更快速得创新、试验并传递价值。\n安全\n在保持稳定性、可用性和持久性的同时，具有快速行动的能力。\n扩展\n根据需求变化弹性扩展。\n移动性\n客户可以随时随地通过任何设备无缝的跟我们交互。\n我们还研究了云原生应用架构的独特特征，以及如何赋予我们这些能力：\n12 因素应用\n一套优化应用设计速度，安全性和规模的模式。\n微服务\n一种架构模式，可帮助我们将部署单位与业务能力保持一致，使每个业务能够独立和自主地移动，这样一来也更快更安全。\n自服务敏捷基础设施\n云平台使开发团队能够在应用和服务抽象层面上运行，提供基础架构级速度，安全性和扩展性。\n基于 API 的协作\n将服务间交互定义为可自动验证协议的架构模式，通过简化的集成工作实现速度和安全性。\n抗脆弱性\n随着速度和规模扩大，系统的压力随之增加，系统的响应能力和安全性也随之提高。\n在下一章中，我们将探讨大多数企业为采用云原生应用架构而需要做出哪些改变。\n下一章 ","relpermalink":"/migrating-to-cloud-native-application-architectures/the-rise-of-cloud-native/summary/","summary":"本章中，我们讨论了希望通过软件赋予我们业务的能力并迁移到云原生应用架构的动机： 速度 比我们的竞争对手更快速得创新、试验并传递价值。 安全 在保持稳定性、可用性和持久性的同时，具有快速行动的能力。 扩展 根据需求","title":"1.3：本章小结"},{"content":"本章中，我们探讨了大多数企业采用云原生应用架构所需要做出的变革。从宏观总体上看是权力下放和自治：\nDevOps\n技能集中化转变为跨职能团队。\n持续交付\n发行时间表和流程的权力下放。\n自治\n决策权力下放。\n我们将这种权力下放编成两个主要的团队结构：\n业务能力团队\n自主决定设计、流程和发布时间表的跨职能团队。\n平台运营团队\n为跨职能团队提供他们所需要运行平台。\n而在技术上，我们也分散自治：\n单体应用到微服务\n将个人业务能力的控制分配给单个自主服务。\n有界上下文\n将业务领域模型的内部一致子集的控制分配到微服务。\n容器化\n将对应用包装的控制分配给业务能力团队。\n编排\n将服务集成控制分配给服务端点。\n所有这些变化造就了无数的自治单元，辅助我们以期望的创新速度安全前行。\n在最后一章中，我们将通过一组操作手册，深入研究迁移到云原生应用架构的技术细节。\n下一章 ","relpermalink":"/migrating-to-cloud-native-application-architectures/changes-needed/summary/","summary":"本章中，我们探讨了大多数企业采用云原生应用架构所需要做出的变革。从宏观总体上看是权力下放和自治： DevOps 技能集中化转变为跨职能团队。 持续交付 发行时间表和流程的权力下放。 自治 决策权力下放。 我们将这种权力下放编","title":"2.4 本章小结"},{"content":"策略即代码涉及编纂所有策略，并作为 CI/CD 管道的一部分运行，使其成为应用程序运行时的一个组成部分。策略类别的例子包括授权策略、网络策略和实现工件策略（例如，容器策略）。典型的 策略即代码软件的策略管理能力可能带有一套预定义的策略类别和策略，也支持通过提供策略模板定义新的策略类别和 相关策略。策略即代码所要求的尽职调查是，它应该提供保护，防止与应用环境（包括基础设施）相关的所有已知威胁，只有当该代码被定期扫描和更新，以应对与应用类别（如网络应用）和托管基础设施相关的威胁，才能确保这一点。下面的表 1 中给出了一些策略类别和相关策略的例子。\n表 1：策略类别和策略实例\n策略类别 策略示例 网络策略和零信任策略 - 封锁指定端口 - 指定入口主机名称 - 一般来说，所有的网络访问控制策略 实施工件策略（例如，容器策略） - 对服务器进行加固，对基础镜像进行漏洞扫描 - 确保容器不以 root 身份运行 - 阻止容器的权限升级 存储策略 - 设置持久性卷大小 - 设置持久性卷回收策略 访问控制策略 - 确保策略涵盖所有数据对象 - 确保策略涵盖管理和应用访问的所有角色 - 确保数据保护策 …","relpermalink":"/service-mesh-devsecops/implement/ci-cd-pipeline-for-policy-as-code/","summary":"策略即代码涉及编纂所有策略，并作为 CI/CD 管道的一部分运行，使其成为应用程序运行时的一个组成部分。策略类别的例子包括授权策略、网络策略和实现工件策略（例如，容器策略）。典型的 策略即代码软件的策略管理能力可能","title":"4.4 策略即代码的 CI/CD 管道"},{"content":"云原生基础架构并不适合所有人。任何架构设计都经过了一系列的权衡。您只有熟悉自己的需求才能决定哪些权衡是有益的，哪些是有害的。\n不要在不了解架构的影响和限制的情况下采用工具或设计。我们相信云原生基础架构有很多好处，但需要意识到不应该盲目的采用。我们不愿意引导大家通过错误的方式来满足需求。\n怎么知道是否应该使用云原生基础架构设计？确定云原生基础架构是否适合您，下面是一些需要了解的问题：\n您有云原生应用程序吗？(有关可从云原生基础架构中受益的应用程序功能，请参阅第 1 章) 您的工程团队是否愿意且能够编写出体现其作业功能的生产质量代码？ 您在本地或公有云是否拥有基于 API 驱动的基础架构（IaaS）？ 您的业务是否需要更快的开发迭代或非线性人员 / 系统缩放比例？ 如果您对所有这些问题都回答“yes”，那么您可能会从本书其余部分介绍的基础架构中受益。如果您对这些问题中的某个问题回答是“no”，这并不意味着您无法从某些云原生实践中受益，但是您可能需要做更多工作，然后才能从此类基础架构中充分受益。\n在业务准备好之前武断地采用云原生基础架构效果会很糟糕，因为这会强制使用一个不正确的解决方案。在没 …","relpermalink":"/cloud-native-infra/when-to-adopt-cloud-native/","summary":"云原生基础架构并不适合所有人。任何架构设计都经过了一系列的权衡。您只有熟悉自己的需求才能决定哪些权衡是有益的，哪些是有害的。 不要在不了解架构的影响和限制的情况下采用工具或设计。我们相信云原生基础架构有","title":"第 2 章：采纳云原生基础架构的时机"},{"content":"勘误 1\nPDF 原文第 4 页，kubelet 端口，默认应为 10250，而不是 10251。\n","relpermalink":"/kubernetes-hardening-guidance/corrigendum/","summary":"勘误 1 PDF 原文第 4 页，kubelet 端口，默认应为 10250，而不是 10251。","title":"勘误"},{"content":"我们在前面看到了通过客户端库来治理服务的架构图，那是我们在改造成服务网格架构前使用微服务架构通常的形式，下图是使用服务网格架构的最终形式。\n服务网格架构示意图 当然在达到这一最终形态之前我们需要将架构一步步演进，下面给出的是参考的演进路线。\nIngress 或边缘代理 如果你使用的是 Kubernetes 做容器编排调度，那么在进化到服务网格架构之前，通常会使用 Ingress Controller，做集群内外流量的反向代理，如使用 Traefik 或 Nginx Ingress Controller。\nIngress 或边缘代理架构示意图 这样只要利用 Kubernetes 的原有能力，当你的应用微服务化并容器化需要开放外部访问且只需要 L7 代理的话这种改造十分简单，但问题是无法管理服务间流量。\n路由器网格 Ingress 或者边缘代理可以处理进出集群的流量，为了应对集群内的服务间流量管理，我们可以在集群内加一个 Router 层，即路由器层，让集群内所有服务间的流量都通过该路由器。\n路由器网格架构示意图 这个架构无需对原有的单体应用和新的微服务应用做什么改造，可以很轻易的迁移进 …","relpermalink":"/cloud-native-handbook/service-mesh/service-mesh-patterns/","summary":"我们在前面看到了通过客户端库来治理服务的架构图，那是我们在改造成服务网格架构前使用微服务架构通常的形式，下图是使用服务网格架构的最终形式。 服务网格架构示意图 当然在达到这一最终形态之前我们需要将架构一步","title":"服务网格的部署模式"},{"content":"自动化开始听起来很神奇，但我们要面对现实：计算机分析不能每天告诉你系统有什么问题或为你修复它。它只能为你节省时间。\n至此，我想停止夸赞，我必须承认自动化的一些局限性。我这样做是因为围绕结合人工智能和可观测性会有相当多的炒作。这种炒作导致了惊人的论断，大意是：“人工智能异常检测和根源分析可以完全诊断问题！” 和 “人工智能操作将完全管理你的系统！”\n谨防炒作 为了摆脱此类炒作，我想明确的是，这类梦幻般的营销主张并不是我所宣称的现代可观测性将提供的。事实上，我预测许多与人工智能问题解决有关的主张大多是夸大其词。\n为什么人工智能不能解决我们的问题？一般来说，机器无法识别软件中的 “问题”，因为定义什么是 “问题” 需要一种主观的分析方式。而我们所讨论的那种现实世界的人工智能不能以任何程度的准确性进行主观决策。机器将始终缺乏足够的背景。\n例如，我们说该版本降低了性能，这里面也隐含了一个期望，就是这个版本包含了每个用户都期望的新功能。对于这个版本，性能退步是一个特征，而不是一个错误。\n虽然它们可能看起来像类似的活动，但在确定相关关系和确定根本原因之间存在着巨大的鸿沟。当你有正确的数据结构时，相关 …","relpermalink":"/opentelemetry-obervability/the-limitations-of-automated-analysis/","summary":"第 3 章：自动分析的局限性","title":"第 3 章：自动分析的局限性"},{"content":"为什么尽快进行 Code Review？ 在 Google，我们优化了开发团队共同开发产品的速度。，而不是优化单个开发者编写代码的速度。个人开发的速度很重要，它并不如整个团队的速度那么重要。\n当代码审查很慢时，会发生以下几件事：\n整个团队的速度降低了。是的，对审查没有快速响应的个人的确完成了其他工作。但是，对于团队其他人来说重要的新功能与缺陷修復将会被延迟数天、数周甚至数月，只因为每个 CL 正在等待审查和重新审查。 开发者开始抗议代码审查流程。如果审查者每隔几天只响应一次，但每次都要求对 CL 进行重大更改，那么开发者可能会变得沮丧。通常，开发者将表达对审查者过于“严格”的抱怨。如果审查者请求相同实质性更改（确实可以改善代码健康状况），但每次开发者进行更新时都会快速响应，则抱怨会逐渐消失。大多数关于代码审查流程的投诉实际上是通过加快流程来解决的。 代码健康状况可能会受到影响。如果审查速度很慢，则造成开发者提交不尽如人意的 CL 的压力会越来越大。审查太慢还会阻止代码清理、重构以及对现有 CL 的进一步改进。 Code Review 应该有多快？ 如果您没有处于重点任务的中，那么您应该 …","relpermalink":"/eng-practices/review/reviewer/speed/","summary":"为什么尽快进行 Code Review？ 在 Google，我们优化了开发团队共同开发产品的速度。，而不是优化单个开发者编写代码的速度。个人开发的速度很重要，它并不如整个团队的速度那么重要。 当代码审查很慢时，会发生","title":"Code Review 速度"},{"content":"对于网络工作负载而言，可移植且互操作的加密身份可能是 SPIFFE 的核心用例之一。为了完全满足这一要求，社区必须达成一致，以标准化检索身份和在运行时使用身份相关服务的方式。\nSPIFFE 工作负载终端点规范通过定义一个终端点来提供 SPIFFE 可验证身份文档（SVIDs）和相关服务。具体而言，它概述了如何定位终端点以及如何服务或使用它。这个终端点所暴露的服务集合超出了本文档的范围，但有一个例外，即 SPIFFE 工作负载 API。\n引言 SPIFFE 工作负载终端点是一个 API 终端点，工作负载或正在运行的计算进程可以通过它在运行时访问与身份相关的服务（如身份签发或身份验证）。这个终端点可以暴露任意数量的与身份相关的服务，但至少，符合规范的环境中运行的工作负载可以期望 SPIFFE 工作负载 API 可用。\n本文档详细介绍了 SPIFFE 工作负载终端点的可访问性和范围、传输协议、身份验证过程以及可扩展性/发现机制。\n可访问性 SPIFFE 工作负载终端点通常用作初始身份引导的机制，包括传递和管理信任根的过程。由于在早期阶段，工作负载可能对自己的身份或应该信任的对象没有任何先验知 …","relpermalink":"/spiffe-and-spire/standard/spiffe-workload-endpoint/","summary":"对于网络工作负载而言，可移植且互操作的加密身份可能是 SPIFFE 的核心用例之一。为了完全满足这一要求，社区必须达成一致，以标准化检索身份和在运行时使用身份相关服务的方式。 SPIFFE 工作负载终端点规范通过定义一个终端点来","title":"SPIFFE 工作负载端点"},{"content":"本文描述了 SPIRE Server 的命令行选项、server.conf 设置和内置插件。\n本文档是 SPIRE Server 的配置参考。它包括有关插件类型、内置插件、服务器配置文件、插件配置和 spire-server 命令的命令行选项的信息。\n插件类型 类型 描述 DataStore 提供持久存储和 HA 功能。注意：不再支持数据存储的可插入性。只能使用内置的 SQL 插件。 KeyManager 为服务器的签名操作实现签名和密钥存储逻辑。对于利用基于硬件的关键操作很有用。 NodeAttestor 为尝试断言其身份的节点实现验证逻辑。一般与同类型的代理插件搭配使用。 UpstreamAuthority 允许 SPIRE 服务器与现有 PKI 系统集成。 Notifier 由 SPIRE 服务器通知正在发生或已经发生的某些事件。对于正在发生的事件，通知者可以将结果告知 SPIRE 服务器。 内置插件 类型 名称 描述 DataStore sql 用于 SPIRE 数据存储的 SQLite、PostgreSQL 和 MySQL 数据库的 SQL 数据库存储 KeyManager …","relpermalink":"/spiffe-and-spire/configuration/server/","summary":"本文描述了 SPIRE Server 的命令行选项、server.conf 设置和内置插件。 本文档是 SPIRE Server 的配置参考。它包括有关插件类型、内置插件、服务器配置文件、插件配置和 spire-server 命令的命令行选项的信息。 插件类型 类型 描述 DataStore 提供持久存","title":"SPIRE Server 配置参考"},{"content":"本文指导你如何使用 Envoy 和 JWT-SVIDs 以及开放策略代理进行安全通信。\n开放策略代理（OPA）是一个开源的、通用的策略引擎。OPA 提供的授权（AuthZ）可以很好地补充 SPIRE 提供的认证（AuthN）。\n本教程基于 SPIRE Envoy-JWT 教程，演示如何结合 SPIRE、Envoy 和 OPA 进行 JWT SVID 认证和请求授权。实现 OPA 请求授权所需的更改在本教程中以增量形式展示，因此你应先运行或至少阅读 SPIRE Envoy-JWT 教程。\n架构图 为了说明如何使用 OPA 进行请求授权，我们在 SPIRE Envoy JWT 教程中使用的后端服务中添加了一个新的 sidecar。新的 sidecar 充当 Envoy 的新外部授权过滤器。\n如图所示，前端服务通过由 Envoy 实例建立的 mTLS 连接连接到后端服务。Envoy 通过 mTLS 连接发送 HTTP 请求，其中携带了用于认证的 JWT-SVID。JWT-SVID 由 SPIRE Agent 提供并验证，然后，请求会根据安全策略由 OPA Agent 实例授权或拒绝。\n在本教 …","relpermalink":"/spiffe-and-spire/examples/envoy-jwt-opa/","summary":"本文指导你如何使用 Envoy 和 JWT-SVIDs 以及开放策略代理进行安全通信。 开放策略代理（OPA）是一个开源的、通用的策略引擎。OPA 提供的授权（AuthZ）可以很好地补充 SPIRE 提供的认证（AuthN）。 本教程基于 SPIRE Envoy-JWT 教程，演","title":"使用 Envoy 和 JWT-SVIDs 进行 OPA 授权"},{"content":"本文档解释了如何利用 Helm Chart 来升级 TSB 的不同元素。本文假定 Helm 已经安装 在系统中。\n本文档仅适用于使用 Helm 创建的 TSB 实例，不适用于从基于 TCTL 的安装升级。\n在开始之前，请确保你已经：\n检查新版本的 要求 先决条件 已经 安装 Helm 已经 安装 TSB cli tctl 已经 安装 kubectl Tetrate 的镜像仓库的凭据 配置 Helm 仓库 添加仓库：\nhelm repo add tetrate-tsb-helm \u0026#39;https://charts.dl.tetrate.io/public/helm/charts/\u0026#39; helm repo update 列出可用版本：\nhelm search repo tetrate-tsb-helm -l 备份 PostgreSQL 数据库 创建 PostgreSQL 数据库的备份。\n根据你的环境，连接到数据库的确切过程可能会有所不同，请参考你环境的文档。\n升级过程 管理平面 升级管理平面 Chart：\nhelm upgrade mp …","relpermalink":"/tsb/setup/helm/upgrade/","summary":"本文档解释了如何利用 Helm Chart 来升级 TSB 的不同元素。本文假定 Helm 已经安装 在系统中。 本文档仅适用于使用 Helm 创建的 TSB 实例，不适用于从基于 TCTL 的安装升级。 在开始之前，请确保你已经： 检查新版本的 要求 先决条件 已经 安装 Helm 已经 安","title":"TSB Helm 升级"},{"content":"本页将指导你如何使用 tctl CLI 升级 TSB，呈现不同 Operator 的 Kubernetes 清单，并使用 kubectl 将它们应用到集群中以进行升级。\n在开始之前，请确保你已完成以下操作：\n检查新版本的 要求 基于 Operator 的版本之间的升级过程相对简单。一旦 Operator 的 Pod 使用新的发布镜像进行了更新，新生成的 Operator Pod 将升级所有必要的组件以适应新版本。\n创建备份 为了确保在出现问题时可以恢复一切，请为管理平面和每个集群的本地控制平面创建备份。\n备份管理平面 备份 tctl 二进制文件 由于每个新的 tctl 二进制文件可能都附带了新的 Operator 和配置来部署和配置 TSB，因此你应该备份当前正在使用的 tctl 二进制文件。请在同步新镜像之前执行此操作。\n将带有版本后缀的 tctl 二进制文件（例如 -1.3.0）复制到一个位置，以便在需要时快速还原旧版本。\ncp ~/.tctl/bin/tctl ~/.tctl/bin/tctl-{version} 如果你不小心丢失了二进制文件，你可以从 此网址 找到正确的版本。但 …","relpermalink":"/tsb/setup/self-managed/upgrade/","summary":"本页将指导你如何使用 tctl CLI 升级 TSB，呈现不同 Operator 的 Kubernetes 清单，并使用 kubectl 将它们应用到集群中以进行升级。 在开始之前，请确保你已完成以下操作： 检查新版本的 要求 基于 Operator 的版本之间的升级过程相对简单。一旦 Operator 的 Pod 使用新的","title":"TSB 升级"},{"content":"TSB 的用户界面显示了你服务的指标和健康状态。然而，如果没有显示任何指标或跟踪信息，那么你可能面临的问题可能是与你的服务或 TSB 中的某个指标组件之一有关。\n本指南将引导你了解问题是与服务还是与 TSB 中的某个指标组件有关的方法。\n指标 如果你看不到指标，请使用本指南的本节进行故障排除。\n首先，请确保你的应用程序中有流量流动。你需要流量来生成指标。\n检查你在 TSB 中设置的时间范围窗口是否正确，并且在该期间是否有流量。\n检查在浏览器中运行 UI 查询是否返回状态。使用浏览器的 inspect 命令并检查请求/响应详细信息。\n从检查器中选择 Network 选项卡，然后从 TSB 用户界面中打开你的应用程序。你应该看到浏览器和 TSB 后端之间的所有请求列表。\n搜索最后一个 graphql 请求。\n如果你看不到查询，这可能表明你的应用程序没有处理任何流量，或者你在 OAP 部署方面存在问题。\n要检查 OAP，请执行以下步骤：\n通过确认tsb命名空间中的OAP Pod 是否正在运行来检查OAP Pod 是否正常运行，并检查 Pod 的日志中是否有任何错误：\nkubectl -n …","relpermalink":"/tsb/troubleshooting/tsb-ui-metrics/","summary":"TSB 的用户界面显示了你服务的指标和健康状态。然而，如果没有显示任何指标或跟踪信息，那么你可能面临的问题可能是与你的服务或 TSB 中的某个指标组件之一有关。 本指南将引导你了解问题是与服务还是与 TSB 中的某个指标组件","title":"UI 指标故障排除"},{"content":"为了配置 bookinfo 应用程序，你需要创建一个网关组、一个流量组和一个安全组。每个组都提供特定的 API 来配置服务的各个方面。\n先决条件 在继续阅读本指南之前，请确保你已完成以下步骤：\n熟悉 TSB 概念 安装 TSB 演示环境 部署 Istio Bookinfo 示例应用程序 创建租户 创建工作区 使用用户界面 在左侧面板的“租户”下，选择“工作区”。 单击 bookinfo-ws 工作区卡。 单击网关组按钮。 单击带有 + 图标的卡以添加新的网关组。 输入组 ID 作为 bookinfo-gw 。 为你的网关组提供显示名称和描述。 输入 */bookinfo 作为初始命名空间选择器。 将配置模式设置为 BRIDGED 。 单击添加。 从左侧面板中选择“工作区”返回到“工作区”。 对使用组 ID bookinfo-traffic 的流量组和使用组 ID bookinfo-security 的安全组重复相同的步骤。\n使用 tctl 创建 groups.yaml 文件：\ngroups.yaml apiVersion: gateway.tsb.tetrate.io/v2 kind: …","relpermalink":"/tsb/quickstart/config-groups/","summary":"为了配置 bookinfo 应用程序，你需要创建一个网关组、一个流量组和一个安全组。每个组都提供特定的 API 来配置服务的各个方面。 先决条件 在继续阅读本指南之前，请确保你已完成以下步骤： 熟悉 TSB 概念 安装 TSB 演示环境 部署 Istio Bookinfo 示例应","title":"创建配置组"},{"content":" 需要基本的分布式跟踪知识\n本文假设读者具有基本的分布式跟踪概念和名词的知识。如果对分布式跟踪不熟悉，建议在阅读本文和调整 TSB 的分布式跟踪配置之前，首先了解分布式跟踪。关于分布式跟踪概念的很好的介绍，请阅读 Nic Munroe 的优秀博客。 所需的服务行为\n分布式跟踪不会自动工作，因为重要的是你的部署服务传播跟踪上下文。如果不在服务中启用上下文传播，你将遇到跟踪中断问题，并且在跟踪中看到大大降低的价值。我们建议至少支持传播 B3 和 W3C 跟踪上下文头，以及x-request-id以进行请求关联。另请参阅 Istio 文档中的跟踪上下文传播解释。除了上下文传播，将x-request-id（以及分布式跟踪的trace id，如果有的话）包含在服务的所有请求绑定日志行中是一个很好的主意。这样可以在请求跟踪和服务日志之间实现几乎无需努力的关联，并加速故障排除。 默认情况下，TSB 提供了一个基于SkyWalking的分布式跟踪后端，与Zipkin兼容。在 TSB 控制下的所有Envoy入口网关和 sidecar 都具有其内部 Zipkin 跟踪仪器，用于将跨度数据直接发送到 TSB …","relpermalink":"/tsb/operations/telemetry/distributed-tracing/","summary":"需要基本的分布式跟踪知识 本文假设读者具有基本的分布式跟踪概念和名词的知识。如果对分布式跟踪不熟悉，建议在阅读本文和调整 TSB 的分布式跟踪配置之前，首先了解分布式跟踪。关于分布式跟踪概念的很好的介绍，请阅读","title":"分布式跟踪集成"},{"content":"工作负载无法加入 mesh 如果新的工作负载未出现在已载入的工作负载列表中，请按照以下步骤操作。\n检查 Workload Onboarding Agent 的状态 虚拟机 (VM) 工作负载 在工作负载的主机上，例如在 VM 上运行：\nsystemctl status onboarding-agent 你应该会得到类似于以下的输出：\n● onboarding-agent.service - Workload Onboarding Agent Loaded: loaded (/usr/lib/systemd/system/onboarding-agent.service; enabled; vendor preset: disabled) Active: active (running) since Thu 2021-10-07 14:57:23 UTC; 1 minute ago # (1) Docs: https://tetrate.io/ Main PID: 3519 (bash) CGroup: /system.slice/onboarding-agent.service …","relpermalink":"/tsb/setup/workload-onboarding/guides/troubleshooting/","summary":"工作负载无法加入 mesh 如果新的工作负载未出现在已载入的工作负载列表中，请按照以下步骤操作。 检查 Workload Onboarding Agent 的状态 虚拟机 (VM) 工作负载 在工作负载的主机上，例如在 VM 上运行： systemctl status onboarding-agent 你应该会得到类似于以下的输出： ● onboarding-agent.service - Workload Onboarding","title":"故障排除指南"},{"content":"TSB 提供了细粒度的权限管理，以控制对 TSB 资源的访问。你可以授予对资源（如组织、租户、工作空间等）的访问权限。一组权限可以放入角色中，然后可以重复使用这些角色来分配权限给适当的资源，例如用户或团队。一旦定义了角色，就可以使用访问绑定对象将角色绑定到一组用户或团队。\n资源模型 为了了解如何处理 TSB 权限，你首先需要了解 TSB 中资源的模型。\n在 TSB 中，资源被建模为一个分层树，其中组织是所有资源的根。组织包含一个或多个集群、租户、团队和用户。租户包含一个或多个工作空间。最后，工作空间可以包含一个或多个网关组、流量组和安全组。\n注意\n在本文档中，术语“配置组”用于指代所有网关组、流量组和安全组。 TSB 资源层次结构 角色 角色是一组针对特定资源的权限。例如，你可以创建一个允许对工作空间进行读/写权限的角色，并且对其父租户仅允许读权限。\n下面是可用的权限列表：\n权限（操作） 描述 Read 允许读取资源。 Write 允许更新资源。 Create 允许创建子资源。(*1) Delete 允许删除资源。 SetPolicy 允许将资源的控制委派给其他用户。 (*1) 当用户 …","relpermalink":"/tsb/operations/users/roles-and-permissions/","summary":"TSB 提供了细粒度的权限管理，以控制对 TSB 资源的访问。你可以授予对资源（如组织、租户、工作空间等）的访问权限。一组权限可以放入角色中，然后可以重复使用这些角色来分配权限给适当的资源，例如用户或团队。一旦定义","title":"角色和权限"},{"content":"本操作指南将向你展示如何配置路由到通过单个ServiceRoute配置公开多个端口的服务。\n场景 考虑一个名为tcp-echo的后端服务，它通过 TCP 公开了两个端口，9000和9001。该服务有两个版本v1和v2，需要在这两个版本之间实现对这两个端口的流量分配。为了实现这一目标，需要配置具有端口级设置的ServiceRoute。\n部署tcp-echo服务 从 Istio 的示例目录中安装tcp-echo应用程序到echo命名空间中，安装这些清单。\nTSB 配置 部署工作区和流量组 应用以下配置来创建一个工作区和一个流量组。\n提示\n这些示例假设你已经创建了一个名为 tetrateio 的组织和一个名为 tetrate 的租户。 apiVersion: api.tsb.tetrate.io/v2 kind: Workspace metadata: name: tcp-multiport-ws organization: tetrateio tenant: tetrate spec: namespaceSelector: names: - \u0026#34;*/echo\u0026#34; --- apiVersion: …","relpermalink":"/tsb/howto/traffic/configure-multi-port-service-route/","summary":"本操作指南将向你展示如何配置路由到通过单个ServiceRoute配置公开多个端口的服务。 场景 考虑一个名为tcp-echo的后端服务，它通过 TCP 公开了两个端口，9000和9001。该服务有两个版本v1和","title":"配置（多端口、多协议）服务的 ServiceRoute"},{"content":"配置保护是一个功能，它有助于保护你的 Istio 配置免受意外更改。这允许你配置允许对 TSB 生成的 Istio 配置进行更改的用户，从而保护你的 Istio 配置免受意外更改的影响。默认情况下，拥有 Kube 命名空间特权的用户可以创建新的 Istio 配置或编辑 TSB 创建的配置。尽管用户管理的配置不会被更改，但当更改 TSB 管理的配置时，它们将在下一个同步周期中被 TSB 覆盖。\n此功能有两个变体：\nenableAuthorizedUpdateDeleteOnXcpConfigs：允许用户创建和管理不受 TSB 管理的 Istio 配置。你可以将一组特定的用户添加到 authorizedUsers 列表中，以赋予这些用户编辑或删除 TSB 管理的配置的权限。 enableAuthorizedCreateUpdateDeleteOnXcpConfigs：阻止未经授权的用户在集群中创建、更新或删除任何 Istio 配置。 你可以通过在你的 ControlPlane CR 中为 XCP 组件添加以下内容来配置允许对 TSB 生成的 Istio 配置进行更改的用户： …","relpermalink":"/tsb/operations/features/enable-config-protection/","summary":"配置保护是一个功能，它有助于保护你的 Istio 配置免受意外更改。这允许你配置允许对 TSB 生成的 Istio 配置进行更改的用户，从而保护你的 Istio 配置免受意外更改的影响。默认情况下，拥有 Kube 命名空间特权的用户可以创建新的 Istio 配置或编","title":"配置保护"},{"content":"TSB 支持使用外部速率限制服务器。本文档将通过一个示例描述如何配置 Envoy rate limit service 并将其用作 TSB Ingress Gateway 中的外部速率限制服务器。\n在开始之前，请确保你已经完成以下准备工作：\n熟悉 TSB 概念 安装 TSB 环境。你可以使用 TSB 演示 进行快速安装 完成 TSB 快速入门。本文档假定你已经创建了租户并熟悉工作区和配置组。还需要将 tctl 配置到你的 TSB 环境中。 注意\n虽然本文档只描述了如何为 Ingress Gateway 应用外部服务器的速率限制，但你可以使用类似的配置方式对 Tier-1 网关和服务对服务（通过 TSB 流量设置）应用相同的速率限制。 创建命名空间 在本示例中，我们将在 ext-ratelimit 命名空间中安装外部速率限制服务。如果目标集群中尚未存在该命名空间，请运行以下命令创建它：\nkubectl create namespace ext-ratelimit 配置速率限制服务 注意\n请阅读 Envoy rate limit 文档 以了解关于域和描述符概念的详细信息。 …","relpermalink":"/tsb/howto/rate-limiting/external-rate-limiting/","summary":"TSB 支持使用外部速率限制服务器。本文档将通过一个示例描述如何配置 Envoy rate limit service 并将其用作 TSB Ingress Gateway 中的外部速率限制服务器。 在开始之前，请确保你已经完成以下准备工作： 熟悉 TSB 概念 安装 TSB 环境。你可以使用 TSB 演示 进行快速安","title":"配置外部速率限制服务器"},{"content":"Tetrate Service Bridge (TSB) 提供强大的全局可观测性功能，可提供对整个服务网格基础设施的全面洞察。TSB 简化了监控和了解服务运行状况和性能的过程，从而实现高效运营和故障排除。\n全局拓扑视图 TSB 的显着特征之一是它能够提供跨所有注册集群的服务网格拓扑的综合视图。这使组织能够掌握分布在各个可用区和区域的应用程序、服务和集群之间的复杂关系。全局拓扑视图允许全面了解应用程序如何在更大的基础设施环境中进行通信和交互。\n服务指标概述 TSB 提供以服务为中心的视角，使用户能够监控其应用程序的运行状况和性能，而不管底层部署详细信息或服务版本如何。此聚合视图简化了跨所有集群和区域评估应用程序整体运行状况的过程。\n此外，TSB 允许用户深入了解服务指标、单个集群甚至特定服务实例的特定方面。这种精细的可观测性使用户能够精确识别潜在问题和瓶颈，从而促进有效的故障排除和优化。\nEnvoy 指标分析 TSB 的全局可观测性还扩展到 Envoy，它是负责在服务网格内路由和管理流量的代理。用户可以访问与各个 Envoy 实例相关的详细指标，从而监控性能指标并深入了解网格内特定组件的 …","relpermalink":"/tsb/concepts/observability/","summary":"Tetrate Service Bridge (TSB) 提供强大的全局可观测性功能，可提供对整个服务网格基础设施的全面洞察。TSB 简化了监控和了解服务运行状况和性能的过程，从而实现高效运营和故障排除。 全局拓扑视图 TSB 的显着特征之一是它能够提供跨所有注","title":"全局可观测性"},{"content":"通过东西网关，任何内部服务都可以实现高可用性，实现自动的跨集群故障转移，而无需通过入口网关将其发布为对外访问的服务。\n在本指南中，你将会：\n在一个集群 cluster-1 中部署 bookinfo 应用程序。然后在第二个集群 cluster-2 中部署相同的 bookinfo 应用程序。 使 cluster-1 中的 reviews、details 或 ratings 服务失败，并观察到 bookinfo 应用程序的失败情况。 将 reviews、details 和 ratings 服务添加到东西网关。 重复故障场景并观察应用程序不受影响，内部流量路由到 cluster-2 中的服务。 了解东西网关 东西网关是外部面向的入口网关（Tier-1 或 Tier-2 网关）的替代方案。东西网关是本地于集群的，不会自动向外部流量公开。东西网关不需要入口网关所需的资源，例如公共 DNS、公共 TLS 证书和入口配置。\n使用入口网关 用于可以外部访问且具有公共 DNS、TLS 证书和 URL 的服务。使用 Tier 1 网关 或全局服务器负载均衡（GSLB）解决方案，使这些服务具有高可用性。 使用 …","relpermalink":"/tsb/howto/gateway/multi-cluster-traffic-routing-with-eastwest-gateway/","summary":"通过东西网关，任何内部服务都可以实现高可用性，实现自动的跨集群故障转移，而无需通过入口网关将其发布为对外访问的服务。 在本指南中，你将会： 在一个集群 cluster-1 中部署 bookinfo 应用程序。然后在第二个集群 cluster-2 中部署相同的 bookinfo 应","title":"使用东西网关实现多集群流量故障转移"},{"content":"启动 Workload Onboarding Agent 创建文件 /etc/onboarding-agent/onboarding.config.yaml，内容如下。 将 ONBOARDING_ENDPOINT_ADDRESS 替换为 你之前获取的值。\napiVersion: config.agent.onboarding.tetrate.io/v1alpha1 kind: OnboardingConfiguration onboardingEndpoint: host: \u0026#34;\u0026lt;ONBOARDING_ENDPOINT_ADDRESS\u0026gt;\u0026#34; transportSecurity: tls: sni: onboarding-endpoint.example # (1) workloadGroup: # (2) namespace: bookinfo name: ratings workload: labels: version: v5 # (3) settings: connectedOver: INTERNET # (4) 此配置指示 Workload Onboarding Agent 使用 …","relpermalink":"/tsb/setup/workload-onboarding/quickstart/aws-ec2/onboard-vm/","summary":"启动 Workload Onboarding Agent 创建文件 /etc/onboarding-agent/onboarding.config.yaml，内容如下。 将 ONBOARDING_ENDPOINT_ADDRESS 替换为 你之前获取的值。 apiVersion: config.agent.onboarding.tetrate.io/v1alpha1 kind: OnboardingConfiguration onboardingEndpoint: host: \"\u003cONBOARDING_ENDPOINT_ADDRESS\u003e\" transportSecurity: tls: sni: onboarding-endpoint.example # (1) workloadGroup: # (2) namespace: bookinfo name: ratings workload: labels: version: v5 # (3) settings: connectedOver:","title":"虚拟机工作负载载入"},{"content":"本文档解释了如何测试故障转移，并运维任务，如排空和恢复集群。\n平台 Operator 可能需要手动执行以下操作：\n在执行维护之前，将工作负载集群排除在旋转之外，以允许现有请求完成 在执行维护之前，将 Edge Gateway 排除在旋转之外，以允许缓存的 DNS 记录超时 定义一个区域为“活动”或“被动”（Tetrate 默认模型是全活动） 工作负载集群故障转移 和 Edge Gateway 故障转移 指南中的示例演示了以可控且可预测的方式将组件排除在服务之外的各种方式，最佳实现将受到特定拓扑和 GSLB 解决方案选择的影响。\n将工作负载集群排除在旋转之外 选项 1：编辑 Edge Gateway 配置 编辑 Edge Gateway 集群列表不会影响服务可用性。已删除集群的请求将允许完成，并且新的请求将不会路由到该集群。\n你需要在 Edge Gateway 的 Gateway 配置中显式列出工作负载集群：\nspec: workloadSelector: namespace: edge labels: app: edgegw http: - name: bookinfo port: …","relpermalink":"/tsb/design-guides/ha-multicluster/operations/","summary":"本文档解释了如何测试故障转移，并运维任务，如排空和恢复集群。 平台 Operator 可能需要手动执行以下操作： 在执行维护之前，将工作负载集群排除在旋转之外，以允许现有请求完成 在执行维护之前，将 Edge Gateway 排除在旋转之外，以允许","title":"运维和测试高可用性与故障转移"},{"content":"🔔 提示：本指南假设你对 Argo CD 所基于的工具有一定的了解。请阅读了解基础知识以了解这些工具。\n要求 安装 kubectl 命令行工具。 有一个 kubeconfig 文件（默认位置是~/.kube/config）。 CoreDNS。可以通过以下方式为 microk8s 启用microk8s enable dns \u0026amp;\u0026amp; microk8s stop \u0026amp;\u0026amp; microk8s start 1. 安装 Argo CD kubectl create namespace argocd kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml 这将创建一个新的命名空间，argocd，Argo CD 服务和应用程序资源将驻留在其中。\n🔔 警告：安装清单包括ClusterRoleBinding引用命名空间的资源argocd。如果你要将 Argo CD 安装到不同的命名空间中，请确保更新命名空间引用。\n如果你对 UI、SSO、多集群功能不感兴 …","relpermalink":"/argo-cd/getting-started/","summary":"🔔 提示：本指南假设你对 Argo CD 所基于的工具有一定的了解。请阅读了解基础知识以了解这些工具。 要求 安装 kubectl 命令行工具。 有一个 kubeconfig 文件（默认位置是~/.kube/config）。 CoreDNS。可以通过以下方式为 microk8s","title":"入门"},{"content":"本指南介绍了 Argo Rollouts 如何与Istio Service Mesh集成进行流量塑形。本指南建立在基本入门指南的概念基础上。\n要求 安装了 Istio 的 Kubernetes 集群 提示\n请参见 Istio 环境设置指南，了解如何在本地 minikube 环境中设置 Istio。\n1. 部署 Rollout、服务、Istio VirtualService 和 Istio Gateway 当使用 Istio 作为流量路由器时，Rollout 金丝雀策略必须定义以下强制字段：\napiVersion: argoproj.io/v1alpha1 kind: Rollout metadata: name: rollouts-demo spec: strategy: canary: # 引用控制器用于指向金丝雀副本集的 Service canaryService: rollouts-demo-canary # 引用控制器用于指向稳定副本集的 Service stableService: rollouts-demo-stable trafficRouting: istio: …","relpermalink":"/argo-rollouts/getting-started/istio/","summary":"本指南介绍了 Argo Rollouts 如何与Istio Service Mesh集成进行流量塑形。本指南建立在基本入门指南的概念基础上。 要求 安装了 Istio 的 Kubernetes 集群 提示 请参见 Istio 环境设置指南，了解如何在本地 minikube 环境中设置 Istio。 1. 部署 Rollou","title":"Istio"},{"content":"Istio 是一种服务网格，通过一组 CRD 提供了丰富的功能集，用于控制流向 Web 服务的流量。Istio 通过调整 Istio VirtualService 中定义的流量权重来实现此功能。使用 Argo Rollouts 时，用户可以部署包含至少一个 HTTP 路由 的 VirtualService，其中包含两个 HTTP 路由目标：一个路由目标针对金丝雀 ReplicaSet 的 pod，一个路由目标针对稳定 ReplicaSet 的 pod。Istio 提供了两种带权流量分割方法，这两种方法都可以作为 Argo Rollouts 的选项。\n主机级流量分割 子集级流量分割 主机级流量分割 使用 Argo Rollouts 和 Istio 进行流量分割的第一种方法是在两个主机名或 Kubernetes Service 之间进行分割：一个金丝雀 Service 和一个稳定 Service。这种方法类似于所有其他 Argo Rollouts mesh/ingress-controller 集成方式的工作方式（例如 ALB、SMI、Nginx）。使用此方法，用户需要部署以下资源： …","relpermalink":"/argo-rollouts/traffic-management/istio/","summary":"Istio 是一种服务网格，通过一组 CRD 提供了丰富的功能集，用于控制流向 Web 服务的流量。Istio 通过调整 Istio VirtualService 中定义的流量权重来实现此功能。使用 Argo Rollouts 时，用户可以部署包含至少一个 HTTP 路由 的 VirtualService，","title":"Istio"},{"content":"🔔 重要提示：自 v0.10.0 起可用于金丝雀发布。\n🔔 重要提示：自 v1.0 起可用于蓝绿发布。\n一个用例是，在 Rollout 标记或注释所需/稳定的 Pod 时，用用户定义的标签/注释，仅在它们是所需或稳定的集合期间，并且在 ReplicaSet 切换角色（例如从所需到稳定）时更新/删除标签。此举使 prometheus、wavefront、datadog 等查询和仪表板可以依赖一致的标签，而不是不可预测且从版本到版本不断变化的 rollouts-pod-template-hash。\n使用金丝雀策略的 Rollout 有能力使用 stableMetadata 和 canaryMetadata 字段将短暂元数据附加到稳定或金丝雀 Pod 上。\nspec: strategy: canary: stableMetadata: labels: role: stable canaryMetadata: labels: role: canary 使用蓝绿策略的 Rollout 有能力使用 activeMetadata 和 previewMetadata …","relpermalink":"/argo-rollouts/rollout/ephemeral-metadata/","summary":"🔔 重要提示：自 v0.10.0 起可用于金丝雀发布。 🔔 重要提示：自 v1.0 起可用于蓝绿发布。 一个用例是，在 Rollout 标记或注释所需/稳定的 Pod 时，用用户定义的标签/注释，仅在它们是所需或稳定的集合期间，并且在 ReplicaSet 切换角色（例如从所需到","title":"短暂元数据"},{"content":"在第三章介绍的概念基础上，本章说明了 SPIFFE 标准。解释 SPIRE 实现的组成部分以及它们是如何结合在一起的。最后，讨论威胁模型以及如果特定组件被破坏会发生什么。\n什么是 SPIFFE？ 普适安全生产身份框架（SPIFFE）是一套软件身份的开源标准。为了以一种与组织和平台无关的方式实现可互操作的软件身份，SPIFFE 定义了必要的接口和文件，以完全自动化的方式获得和验证加密身份。\n图 4.1：SPIFFE 组件。 SPIFFE ID，代表软件服务的名称（或身份）。 SPIFFE 可验证身份文件（SVID），这是一个可加密验证的文件，用于向对等者证明服务的身份。 SPIFFE Workload API，这是一个简单的节点本地 API，服务用它来获得身份，而不需要认证。 SPIFFE Trust Bundle（信任包），一种代表特定 SPIFFE 发行机构使用的公钥集合的格式。 SPIFFE Federation，这是一个简单的机制，通过它可以共享 SPIFFE Trust Bundle。 SPIFFE 不是什么 SPIFFE 旨在识别服务器、服务和其他通过计算机网络通信的非人类实 …","relpermalink":"/spiffe/introduction-to-spiffe-and-spire-concepts/","summary":"在第三章介绍的概念基础上，本章说明了 SPIFFE 标准。解释 SPIRE 实现的组成部分以及它们是如何结合在一起的。最后，讨论威胁模型以及如果特定组件被破坏会发生什么。 什么是 SPIFFE？ 普适安全生产身份框架（SPIFFE）","title":"SPIFFE 和 SPIRE 概念介绍"},{"content":"本节介绍 Kubernetes 的网络策略方面。\n命名空间 命名空间 用于在 Kubernetes 中创建虚拟集群。包括 NetworkPolicy 和 CiliumNetworkPolicy 在内的所有 Kubernetes 对象都属于一个特定的命名空间。根据定义和创建策略的方式，会自动考虑 Kubernetes 命名空间：\n作为 CiliumNetworkPolicy CRD 和 NetworkPolicy 创建和导入的网络策略适用于命名空间内，即该策略仅适用于该命名空间内的 pod。但是，可以授予对其他命名空间中的 pod 的访问权限，如下所述。 通过 API 参考 直接导入的网络策略适用于所有命名空间，除非如下所述指定命名空间选择器。 提示\n虽然有意支持通过 fromEndpoints 和 toEndpoints 中的 k8s:io.kubernetes.pod.namespace 标签指定命名空间。禁止在 endpointSelector 中指定命名空间，因为这将违反 Kubernetes 的命名空间隔离原则。endpointSelector …","relpermalink":"/cilium-handbook/policy/kubernetes/","summary":"本节介绍 Kubernetes 的网络策略方面。 命名空间 命名空间 用于在 Kubernetes 中创建虚拟集群。包括 NetworkPolicy 和 CiliumNetworkPolicy 在内的所有 Kubernetes 对象都属于一个特定的命名空间。根据定义和创建策略的方式，会自动考虑 Kubernetes 命名空间： 作为 CiliumNetworkPolicy CRD 和 NetworkPolicy 创建和导入的网络策略适","title":"Kubernetes 网络策略"},{"content":"如果你在 Kubernetes 上运行 Cilium，你可以从 Kubernetes 为你分发的策略中受益。在这种模式下，Kubernetes 负责在所有节点上分发策略，Cilium 会自动应用这些策略。三种格式可用于使用 Kubernetes 本地配置网络策略：\n在撰写本文时，标准的 NetworkPolicy 资源支持指定三层/四层入口策略，并带有标记为 beta 的有限出口支持。 扩展的 CiliumNetworkPolicy 格式可用作 CustomResourceDefinition 支持入口和出口的三层到七层层策略规范。 CiliumClusterwideNetworkPolicy 格式，它是集群范围的CustomResourceDefinition，用于指定由 Cilium 强制执行的集群范围的策略。规范与 CiliumNetworkPolicy 相同，没有指定命名空间。 Cilium 支持同时运行多个策略类型。但是，在同时使用多种策略类型时应谨慎，因为理解跨多种策略类型的完整允许流量集可能会令人困惑。如果不注意，这可能会导致意外的策略允许行为。\n网络策略 有关详细信息， …","relpermalink":"/cilium-handbook/kubernetes/policy/","summary":"如果你在 Kubernetes 上运行 Cilium，你可以从 Kubernetes 为你分发的策略中受益。在这种模式下，Kubernetes 负责在所有节点上分发策略，Cilium 会自动应用这些策略。三种格式可用于使用 Kubernetes 本地配置网络策略： 在撰写本","title":"网络策略"},{"content":"近年来，云原生应用已呈指数级增长。在本章中，我将讨论为什么 eBPF 如此适合于云原生环境。为了更具象化，我将提到 Kubernetes，但同样适用于任何容器平台。\n每台主机一个内核 要理解为什么 eBPF 在云原生世界中如此强大，你需要搞清楚一个概念：每台机器（或虚拟机）只有一个内核，所有运行在该机器上的容器都共享同一个内核 1 如 图 5-1 所示，内核了解主机上运行的所有应用代码。\n图 5-1. 同一主机上的所有容器共享一个内核 通过对内核的检测，就像我们在使用 eBPF 时做的那样，我们可以同时检测在该机器上运行的所有应用程序代码。当我们将 eBPF 程序加载到内核并将其附加到事件上时，它就会被触发，而不考虑哪个进程与该事件有关。\neBPF 与 sidecar 模式的比较 在 eBPF 之前，Kubernetes 的可观测性和安全工具大多都采用了 sidecar 模式。这种模式允许你在与应用程序相同的 pod 中，单独部署一个工具容器。这种模式的发明是一个进步，因为这意味着不再需要直接在应用程序中编写工具代码。仅仅通过部署 sidecar，工具就获得了同一 pod 中的其他容器 …","relpermalink":"/what-is-ebpf/ebpf-in-cloud-native-environments/","summary":"近年来，云原生应用已呈指数级增长。在本章中，我将讨论为什么 eBPF 如此适合于云原生环境。为了更具象化，我将提到 Kubernetes，但同样适用于任何容器平台。 每台主机一个内核 要理解为什么 eBPF 在云原生世界中如此","title":"第五章：云原生环境中的 eBPF"},{"content":"Etcd 是 Kubernetes 集群中的一个十分重要的组件，用于保存集群所有的网络配置和对象的状态信息。在后面具体的安装环境中，我们安装的 etcd 的版本是 v3.1.5，整个 Kubernetes 系统中一共有两个服务需要用到 etcd 用来协同和存储配置，分别是：\n网络插件 flannel、对于其它网络插件也需要用到 etcd 存储网络的配置信息 Kubernetes 本身，包括各种对象的状态和元信息配置 注意：flannel 操作 etcd 使用的是 v2 的 API，而 Kubernetes 操作 etcd 使用的 v3 的 API，所以在下面我们执行 etcdctl 的时候需要设置 ETCDCTL_API 环境变量，该变量默认值为 2。\n原理 Etcd 使用的是 raft 一致性算法来实现的，是一款分布式的一致性 KV 存储，主要用于共享配置和服务发现。关于 raft 一致性算法请参考 该动画演示。\n关于 Etcd 的原理解析请参考 Etcd 架构与实现解析。\n使用 Etcd 存储 Flannel 网络信息 我们在安装 Flannel …","relpermalink":"/kubernetes-handbook/architecture/etcd/","summary":"Etcd 是 Kubernetes 集群中的一个十分重要的组件，用于保存集群所有的网络配置和对象的状态信息。在后面具体的安装环境中，我们安装的 etcd 的版本是 v3.1.5，整个 Kubernetes 系统中一共有两个服务需要用到 etcd 用来协同和存储配置，分别是：","title":"Etcd 解析"},{"content":"在联邦政府的各个机构中，有几个 DevSecOps 倡议，其重点和焦点各不相同，这取决于软件和任务需求所带来的流程。尽管并不详尽，但以下是对 这些倡议 的简要概述：\nDevSecOps 管道参与构建、签入和签出一个名为 Iron Bank 的容器镜像仓库，这是一个经过国防部审查的强化容器镜像库。 空军的 Platform One，也就是实现了连续操作授权（C-ATO）概念的 DevSecOps 平台，这又简化了国防部的授权程序，以适应现代连续软件部署的速度和频率。 国家地理空间情报局（NGA）在 \u0026#34;NGA 软件之路\u0026#34; 中概述了其 DevSecOps 战略，其中为其每个软件产品规定了三个关键指标：可用性、准备时间和部署频率，以及用于实现 DevSecOps 管道的七个不同产品系列的规格，包括消息传递和工作流工具。 医疗保险和医疗补助服务中心（CMS）正在采用一种 DevSecOps 方法，其中一个重点是为软件材料清单（SBOM）奠定基 —— 这是一种正式记录，包含用于构建软件的各种组件的细节和供应链关系。制作 SBOM 的目的是为了实现持续诊断和缓解（CDM）计划下的目标。 在海军水面作 …","relpermalink":"/service-mesh-devsecops/intro/related-devsecops-initiatives/","summary":"在联邦政府的各个机构中，有几个 DevSecOps 倡议，其重点和焦点各不相同，这取决于软件和任务需求所带来的流程。尽管并不详尽，但以下是对 这些倡议 的简要概述： DevSecOps 管道参与构建、签入和签出一个名为 Iron Bank 的容器镜像仓库，这是一","title":"1.2 相关的 DevSecOps 倡议"},{"content":"可观测性即代码在应用程序的每个服务组件中部署一个监控代理，以收集三种类型的数据（在第 4.1 节中描述），将它们发送到专门的工具，将它们关联起来，进行分析，并在仪表板上显示分析后的综合数据，以呈现整个应用程序级别的情况。这种综合数据的一个例子是日志模式，它提供了一个日志数据的视图，该视图是在使用一些标准（例如，一个服务或一个事件）对日志数据进行过滤后呈现的。数据根据共同的模式（例如，基于时间戳或 IP 地址范围）被分组，以方便解释。不寻常的发生被识别出来，然后这些发现可以被用来指导和加速 进一步的调查。\n","relpermalink":"/service-mesh-devsecops/implement/ci-cd-pipeline-for-observability-as-code/","summary":"可观测性即代码在应用程序的每个服务组件中部署一个监控代理，以收集三种类型的数据（在第 4.1 节中描述），将它们发送到专门的工具，将它们关联起来，进行分析，并在仪表板上显示分析后的综合数据，以呈现整个应用程序","title":"4.5 可观测性即代码的 CI/CD 管道"},{"content":"我们在前一章中讨论了在采用云原生基础架构的前提。在部署之前，需要有由 API 驱动的基础架构（IaaS）供给。\n在本章中，我们将探讨云原生基础架构拓扑的概念，并在云中实现它们。我们将学习可以帮助运维人员控制其基础架构的常用工具和模式。\n部署基础架构的第一步应该是能够将其表述出来。传统上，可以在白板上处理，或者如果幸运的话，可以在公司 wiki 上存储的文档中处理。今天，一切都变得更加程序化，基础架构表述通常以便于应用程序解释的方式记录。无论如何表述，全面的表述基础架构的需求是不变的。\n正如人们所期望的那样，精巧的云基础架构可以从简单的设计到非常复杂的设计。无论复杂性如何，必须对基础架构的表现给予高度的重视，以确保设计的可重复性。能够清晰地传递想法更为重要。因此，明确、准确和易于理解的基础架构级资源表述势在必行。\n我们也将从制作精良的表述中获得很多好处：\n随着时间的推移，基础架构设计可以共享和版本化。 基础架构设计可以被 fork 和修改以适应特殊情况。 表述隐含的是文档。 随着本章向前推进，我们将看到基础架构表述是如何成为基础架构部署的第一步。我们将以不同的方式探索表述基础架构的能力和 …","relpermalink":"/cloud-native-infra/evolution-of-cloud-native-developments/","summary":"我们在前一章中讨论了在采用云原生基础架构的前提。在部署之前，需要有由 API 驱动的基础架构（IaaS）供给。 在本章中，我们将探讨云原生基础架构拓扑的概念，并在云中实现它们。我们将学习可以帮助运维人员控制其基","title":"第 3 章：云原生部署的演变"},{"content":"Kubernetes，经常被缩写为 “K8s”，是一个开源的容器或编排系统，用于自动部署、扩展和管理容器化应用程序。它管理着构成集群的所有元素，从应用中的每个微服务到整个集群。与单体软件平台相比，将容器化应用作为微服务使用可以提供更多的灵活性和安全优势，但也可能引入其他复杂因素。\n图 1：Kubernetes 集群组件的高层视图 本指南重点关注安全挑战，并尽可能提出适用于国家安全系统和关键基础设施管理员的加固策略。尽管本指南是针对国家安全系统和关键基础设施组织的，但也鼓励联邦和州、地方、部落和领土（SLTT）政府网络的管理员实施所提供的建议。Kubernetes 集群的安全问题可能很复杂，而且经常在利用其错误配置的潜在威胁中被滥用。以下指南提供了具体的安全配置，可以帮助建立更安全的 Kubernetes 集群。\n建议 每个部分的主要建议摘要如下：\nKubernetes Pod 安全 使用构建的容器，以非 root 用户身份运行应用程序 在可能的情况下，用不可变的文件系统运行容器 扫描容器镜像，以发现可能存在的漏洞或错误配置 使用 Pod 安全政策来执行最低水平的安全，包括： 防止有特权 …","relpermalink":"/kubernetes-hardening-guidance/introduction/","summary":"Kubernetes，经常被缩写为 “K8s”，是一个开源的容器或编排系统，用于自动部署、扩展和管理容器化应用程序。它管理着构成集群的所有元素，从应用中的每个微服务到整个集群。与","title":"简介"},{"content":"到目前为止，我们已经从数据的角度讨论了现代可观测性。但是，现代可观测性还有另一个方面，从长远来看，它可能被证明同样重要：如何对待产生数据的仪表（instrumention）。\n大多数软件系统都是用现成的部件构建的：网络框架、数据库、HTTP 客户端、代理服务器、编程语言。在大多数组织中，很少有这种软件基础设施是在内部编写的。相反，这些组件是在许多组织中共享的。最常见的是，这些共享组件是以开放源码（OSS）库的形式出现的，并具有许可权。\n由于这些开放源码软件库几乎囊括了一般系统中的所有关键功能，因此获得这些库的高质量说明对大多数可观测性系统来说至关重要。\n传统上，仪表是 “单独出售” 的。这意味着，软件库不包括产生追踪、日志或度量的仪表。相反，特定解决方案的仪表是在事后添加的，作为部署可观测系统的一部分。\n什么是特定解决方案仪表？\n在本章中，术语 “特定解决方案仪表” 是指任何旨在与特定的可观测系统一起工作的仪表，使用的是作为该特定系统的数据存储系统的产物而开发的客户端。在这些系统中，客户端和存储系统常常深深地融合在一起。因此，如果一个应用要从一个观测系统切换到另一个观测系统，通常需要进 …","relpermalink":"/opentelemetry-obervability/supporting-open-source-and-native-instrumentation/","summary":"第 4 章：支持开源和原生监测","title":"第 4 章：支持开源和原生监测"},{"content":"在这一节中，我们将解释 Envoy 的基本构建模块。\nEnvoy 配置的根被称为引导配置。它包含了一些字段，我们可以在这里提供静态或动态的资源和高级别的 Envoy 配置（例如，Envoy 实例名称、运行时配置、启用管理界面等等）。\n为了开始学习，我们将主要关注静态资源，在课程的后面，我们将介绍如何配置动态资源。\nEnvoy 输出许多统计数据，这取决于启用的组件和它们的配置。我们会在整个课程中提到不同的统计信息，在课程后面的专门模块中，我们会更多地讨论统计信息。\n下图显示了通过这些概念的请求流。\nEnvoy 构建块 这一切都从监听器开始。Envoy 暴露的监听器是命名的网络位置，可以是一个 IP 地址和一个端口，也可以是一个 Unix 域套接字路径。Envoy 通过监听器接收连接和请求。考虑一下下面的 Envoy 配置。\nstatic_resources: listeners: - name: listener_0 address: socket_address: address: 0.0.0.0 port_value: 10000 filter_chains: [{}] …","relpermalink":"/cloud-native-handbook/service-mesh/envoy-building-blocks/","summary":"在这一节中，我们将解释 Envoy 的基本构建模块。 Envoy 配置的根被称为引导配置。它包含了一些字段，我们可以在这里提供静态或动态的资源和高级别的 Envoy 配置（例如，Envoy 实例名称、运行时配置、启用管理界面等等）。 为了开","title":"Envoy 的构建模块"},{"content":"在使用 ApplicationSet 之前，了解其安全性影响非常重要。\n只有管理员可以创建/更新/删除 ApplicationSet ApplicationSet 可以在任意 Project 下创建应用程序。Argo CD 设置通常包括高权限的 Project（例如 default），往往包括管理 Argo CD 自身资源的能力（例如 RBAC ConfigMap）。\nApplicationSets 还可以快速创建任意数量的应用程序，并同样快速删除它们。\n最后，ApplicationSets 可以显示特权信息。例如，git generator 可以读取 Argo CD 命名空间中的 Secrets，并将其作为 Auth 标头发送到任意 URL（例如为 api 字段提供的 URL）。 （此功能旨在为 SCM 提供程序（如 GitHub）授权请求，但可能会被恶意用户滥用。）\n出于这些原因，只有管理员可以通过 Kubernetes RBAC 或任何其他机制获得创建、更新或删除 ApplicationSets 的权限。\n管理员必须为 ApplicationSets 的真实来源应用适当的控制 即 …","relpermalink":"/argo-cd/operator-manual/applicationset/security/","summary":"在使用 ApplicationSet 之前，了解其安全性影响非常重要。 只有管理员可以创建/更新/删除 ApplicationSet ApplicationSet 可以在任意 Project 下创建应用程序。Argo CD 设置通常包括高权限的 Project（例如 default），往往包括管理 Argo CD 自身资源的能力","title":"ApplicationSet 安全性"},{"content":"总结 保持友善。 解释你的推理。 在给出明确的指示与只指出问题并让开发人员自己决定间做好平衡。 鼓励开发人员简化代码或添加代码注释，而不仅仅是向你解释复杂性。 礼貌 一般而言，对于那些正在被您审查代码的人，除了保持有礼貌且尊重以外，重要的是还要确保您（的评论）是非常清楚且有帮助的。你并不总是必须遵循这种做法，但在说出可能令人不安或有争议的事情时你绝对应该使用它。例如：\n糟糕的示例：“为什么这里你使用了线程，显然并发并没有带来什么好处？”\n好的示例：“这里的并发模型增加了系统的复杂性，但没有任何实际的性能优势，因为没有性能优势，最好是将这些代码作为单线程处理而不是使用多线程。”\n解释为什么 关于上面的“好”示例，您会注意到的一件事是，它可以帮助开发人员理解您发表评论的原因。并不总是需要您在审查评论中包含此信息，但有时候提供更多解释，对于表明您的意图，您在遵循的最佳实践，或为您建议如何提高代码健康状况是十分恰当的。\n给予指导 一般来说，修复 CL 是开发人员的责任，而不是审查者。 您无需为开发人员详细设计解决方案或编写代码。\n但这并不意味着审查者应该没有帮助。一般来说，您应该在指出问题和提 …","relpermalink":"/eng-practices/review/reviewer/comments/","summary":"总结 保持友善。 解释你的推理。 在给出明确的指示与只指出问题并让开发人员自己决定间做好平衡。 鼓励开发人员简化代码或添加代码注释，而不仅仅是向你解释复杂性。 礼貌 一般而言，对于那些正在被您审查代码的人，除了保","title":"如何撰写 Code Review 评论"},{"content":"SPIFFE 标准提供了一个规范，用于在异构环境和组织中引导和发放可互操作的服务身份。它定义了一个称为\u0026#34;信任域\u0026#34;的概念，用于划分管理和/或安全边界。信任域隔离发放机构并区分身份命名空间，但也可以松散耦合以提供联合身份。\n本文档描述了 SPIFFE 信任域的语义、表示方式以及它们如何耦合在一起的机制。\n引言 SPIFFE 信任域表示 SPIFFE ID 有资格的基础，指示任何给定 SPIFFE ID 已经发放的领域或发放机构。它们由发放机构支持，负责管理其相应信任域中的 SPIFFE 身份发放。尽管信任域的名称由一个简单的人类可读字符串组成，但还必须表达由信任域的发放机构使用的密码密钥，以使其他人能够验证其发放的身份。这些密钥被表示为\u0026#34;SPIFFE Bundle\u0026#34;，与其所代表的信任域紧密相连。\n本规范定义了 SPIFFE 信任域和 SPIFFE Bundle 的性质和语义。\n信任域 SPIFFE 信任域是由一组密码密钥支持的身份命名空间。这些密钥共同为驻留在信任域中的所有身份提供了密码锚点。\n信任域与支持它们的密钥之间存在一对多的关系。一个信任域可以由多个密钥和密钥类型来表示。例如，前者 …","relpermalink":"/spiffe-and-spire/standard/spiffe-trust-domain-and-bundle/","summary":"SPIFFE 标准提供了一个规范，用于在异构环境和组织中引导和发放可互操作的服务身份。它定义了一个称为\"信任域\"的概念，用于划分管理和/或安全边界。信任域隔离发放机构并区分身份命名空间，但也可","title":"SPIFFE 信任域和 Bundle"},{"content":"如果在配置东西向网关的cluster-external-addresses 注释时使用 DNS 主机名，你需要在 XCP 边缘启用 DNS 解析，以便 DNS 解析在 XCP 边缘发生。\n在 XCP 边缘启用 DNS 解析 要在 XCP 边缘启用 DNS 解析，你需要在 ControlPlane CR 或 Helm 值中编辑 xcp 组件，并添加一个名为 ENABLE_DNS_RESOLUTION_AT_EDGE 的环境变量，并将其值设置为 true：\nspec: components: xcp: ... kubeSpec: overlays: - apiVersion: install.xcp.tetrate.io/v1alpha1 kind: EdgeXcp name: edge-xcp patches: ... - path: spec.components.edgeServer.kubeSpec.deployment.env[-1] value: name: ENABLE_DNS_RESOLUTION_AT_EDGE value: \u0026#34;true\u0026#34; ... 有关如何启用东西部路由的 …","relpermalink":"/tsb/operations/features/edge-dns-resolution/","summary":"如果在配置东西向网关的cluster-external-addresses 注释时使用 DNS 主机名，你需要在 XCP 边缘启用 DNS 解析，以便 DNS 解析在 XCP 边缘发生。 在 XCP 边缘启用 DNS 解析 要在 XCP 边缘启用 DNS 解析，你需要在 ControlPlane CR 或 Helm","title":"Edge 处的 DNS 解析"},{"content":"要卸载使用 Helm 安装的 TSB，你可以使用 helm uninstall 来卸载一个发布。卸载必须按照以下顺序进行：\n数据平面 控制平面 管理平面 数据平面卸载 helm uninstall dp tetrate-tsb-helm/dataplane --namespace istio-gateway 一旦 Helm 删除了与数据平面 Chart 的最后一个发布关联的所有资源，你将需要手动删除一些在卸载过程中创建的资源，这些资源不受 Helm 跟踪。\nkubectl delete serviceaccount tsb-helm-delete-hook --ignore-not-found kubectl delete clusterrole tsb-helm-delete-hook --ignore-not-found kubectl delete clusterrolebinding tsb-helm-delete-hook --ignore-not-found kubectl delete istiooperators.install.istio.io --all -n …","relpermalink":"/tsb/setup/helm/uninstallation/","summary":"要卸载使用 Helm 安装的 TSB，你可以使用 helm uninstall 来卸载一个发布。卸载必须按照以下顺序进行： 数据平面 控制平面 管理平面 数据平面卸载 helm uninstall dp tetrate-tsb-helm/dataplane --namespace istio-gateway 一旦 Helm 删除了与数据平面 Chart 的最后一个发布关联的所有资源，你将需要手动删除","title":"Helm TSB 卸载"},{"content":"New Relic 的流行软件分析平台使企业能够监视其应用程序、服务器和数据库的健康和性能。它从各种来源收集和分析数据，包括应用程序日志、服务器指标和用户交互，以提供详细的洞察和指标。\nTetrate 的丰富的可观测性数据可以与 New Relic 平台无缝集成。本文介绍了如何在 New Relic 中使 Istio 和 Tetrate Service Bridge 的遥测数据可用。有关与应用程序负载相关的指标，请参阅New Relic 文章，该文章描述了 Istio 数据平面指标的检索。\n注意\n下面的步骤经过验证，但一些客户可能需要额外的定制来满足其自定义的 New Relic 设置。 数据流 下面的图表显示了 Tetrate Service Bridge 导出到 New Relic 的指标工作流程处理。\nTetrate Service Bridge 到 New Relic 的工作流程图 每个 Tetrate Service Bridge 控制平面都使用OpenTelemetry Collector来收集一组高级指标，并将它们聚合在全局 TSB 管理平面中，以及与管理平面中的活动相关 …","relpermalink":"/tsb/operations/telemetry/new-relic/","summary":"New Relic 的流行软件分析平台使企业能够监视其应用程序、服务器和数据库的健康和性能。它从各种来源收集和分析数据，包括应用程序日志、服务器指标和用户交互，以提供详细的洞察和指标。 Tetrate 的丰富的可观测性数据可以与 New Relic","title":"New Relic 集成"},{"content":"要将部署在 AWS Auto Scaling Group（ASG）上的工作负载加入，你需要在实例启动脚本中执行所有设置操作，而不是在 EC2 实例上执行命令。\n简而言之，你需要将先前步骤中的设置命令移到与 Auto Scaling Group 中的实例关联的 cloud-init 配置中。\n具体来说，\n将来自 安装 Bookinfo Ratings 应用程序 步骤的设置命令移到云初始化配置中。 将来自 安装 Istio Sidecar 步骤的设置命令移到云初始化配置中。 将来自 在 AWS EC2 实例上安装工作负载 Onboarding Agent 步骤的设置命令移到云初始化配置中。 将来自 从 AWS EC2 实例上加入工作负载 步骤的设置命令移到云初始化配置中。 以下配置是将所有步骤合并在一起的示例。将 \u0026lt;example-ca-certificate\u0026gt; 替换为 example-ca.crt.pem 的值，将 \u0026lt;ONBOARDING_ENDPOINT_ADDRESS\u0026gt; 替换为 你之前获取的值。\n#cloud-config write_files: # 自定义 CA 的证书 - …","relpermalink":"/tsb/setup/workload-onboarding/quickstart/aws-ec2/onboard-asg/","summary":"要将部署在 AWS Auto Scaling Group（ASG）上的工作负载加入，你需要在实例启动脚本中执行所有设置操作，而不是在 EC2 实例上执行命令。 简而言之，你需要将先前步骤中的设置命令移到与 Auto Scaling Group 中的实例关联的 cloud-init 配置中。 具体来","title":"从 AWS Auto Scaling Group 上加入工作负载"},{"content":"一旦你配置了外部速率限制服务器，你可能希望确保与速率限制服务的通信安全。TSB 支持指定TLS 或 mTLS参数来确保与外部速率限制服务器的通信的安全性。本文档将向你展示如何通过将 CA 证书添加到速率限制配置中来为外部速率限制服务器配置 TLS 验证。\n在开始之前，请确保你已完成以下准备工作：\n熟悉 TSB 概念 安装 TSB 环境。你可以使用 TSB 演示 进行快速安装 完成 TSB 使用快速入门。本文假定你已经创建了租户，并熟悉工作空间和配置组。还需要配置 tctl 到你的 TSB 环境。 完成了 设置外部速率限制服务器。本文将继续你在设置外部速率限制服务器中所做的工作。你将在 ext-ratelimit 命名空间中工作，并且应该已经正确配置了带有外部速率限制的 Ingress 网关。 TLS 证书 要启用 Ingress 网关到速率限制服务的 TLS，请确保你拥有一个 TLS 证书。本文假定你已经拥有 TLS 证书，通常包括服务器证书和私钥，以及客户端将使用的 CA 作为根证书。\n本文假定以下文件已存在。如果你使用不同的文件名，请相应地更改它们： …","relpermalink":"/tsb/howto/rate-limiting/tls-validation/","summary":"一旦你配置了外部速率限制服务器，你可能希望确保与速率限制服务的通信安全。TSB 支持指定TLS 或 mTLS参数来确保与外部速率限制服务器的通信的安全性。本文档将向你展示如何通过将 CA 证书添加到速率限制配置中","title":"带 TLS 验证的外部速率限制"},{"content":"集群之间的连接与故障切换 Tetrate 使应用所有者能够轻松地在多个集群中部署应用程序。可以在选择的工作区中部署东西向网关，并将一些或所有服务暴露给其他集群。Tetrate 的中央服务注册表用于在其他集群中为这些暴露的服务创建虚拟端点（ServiceEntries），以便客户端可以发现和使用这些服务，就像它们在本地运行一样。\n这项功能可用于：\n在不同的集群中分发应用程序的组件 从远程集群访问集中式共享服务，如数据库微服务 在不同集群之间为服务实例提供故障切换 通过确保相同的应用程序和寻址方案在一个集群中或分布在多个集群中，简化测试 平台所有者（“Platform”）为故障转移案例准备了平台，应用程序所有者（“Apps”）可以在对其工作流程进行最少修改的情况下利用这些功能。\n平台所有者将按以下方式准备平台：\n部署东西向网关 在包含要共享的服务的命名空间中部署东西向网关。\n更新工作区以暴露所需的服务 更新工作区以暴露所需的服务，以便它们可以在其他集群中被发现和使用。\n为内部故障切换部署东西向网关 在其他集群中部署额外的东西向网关以进行服务故障切换。 然后，应用程序所有者可以：\n访问已暴露 …","relpermalink":"/tsb/design-guides/app-onboarding/cross-cluster/","summary":"集群之间的连接与故障切换 Tetrate 使应用所有者能够轻松地在多个集群中部署应用程序。可以在选择的工作区中部署东西向网关，并将一些或所有服务暴露给其他集群。Tetrate 的中央服务注册表用于在其他集群中为这些暴露","title":"集群间的流量"},{"content":"本操作指南将向你展示如何对新的示例服务进行金丝雀发布。你将学习如何在 TSB 中部署和注册服务，以及如何调整其设置以遵循金丝雀部署过程。\n你将创建一个工作区和你需要注册应用程序的组 通过应用程序入口网关公开应用程序 执行金丝雀发布。 在开始之前，请确保：\n你已经启动并运行了一个 TSB 管理平面。 你已经配置了 tctl 以与 TSB 管理平面通信。 你将要部署应用程序的集群正在运行一个 TSB 控制平面，并且已经正确注册到 TSB 管理平面。 本指南使用一个hello world应用程序，如果你将其用于生产，请根据你的应用程序的正确信息更新相关字段。\n开始 以下 YAML 文件包含三个对象 - 用于应用程序的工作区、用于配置应用程序入口的网关组以及用于配置金丝雀发布过程的流量组。将其存储为ws-groups.yaml。\nws-groups.yaml apiVersion: api.tsb.tetrate.io/v2 kind: Workspace metadata: name: helloworld organization: tetrate tenant: tetrate …","relpermalink":"/tsb/howto/traffic/canary-releases/","summary":"本操作指南将向你展示如何对新的示例服务进行金丝雀发布。你将学习如何在 TSB 中部署和注册服务，以及如何调整其设置以遵循金丝雀部署过程。 你将创建一个工作区和你需要注册应用程序的组 通过应用程序入口网关公开应用程","title":"金丝雀发布"},{"content":"Tetrate 允许你创建一个明确定义、准确可持续发展的安全策略，可以与你的 Tetrate 管理的系统平滑地同步发展。\n平台所有者（“Platform”）和应用程序所有者（“Apps”）共同合作创建你的安全策略的元素：\n识别安全阻塞 应用程序所有者识别当前 Tetrate 安全策略阻止的必需流量。\n应用安全规则 平台所有者验证所需的安全例外，并使用 Tetrate API 实施它。\n审计安全规则 定期，平台所有者可能希望审计当前的安全策略，以确保它们足以满足安全和合规性需求。\n平台：开始之前 本指南将涵盖在零信任环境中的工作负载之间配置访问控制。工作负载通过 Istio 提供的 Tetrate 平台发放和更新 SPIFFE 标识进行身份验证。本指南不涵盖 JWT、OAuth 或 OIDC 用户身份验证等更高级别的功能。\n指南从推荐的起始姿态开始：\n所有工作负载都使用 SPIFFE 标识，并且对于所有流量，都需要 mTLS。这意味着外部第三方（如集群中的其他服务或具有对数据路径的访问权限的服务）无法读取事务、修改事务或冒充客户端或服务。 默认情况下，所有通信都被拒绝，只有解锁了工作区， …","relpermalink":"/tsb/design-guides/app-onboarding/security/","summary":"Tetrate 允许你创建一个明确定义、准确可持续发展的安全策略，可以与你的 Tetrate 管理的系统平滑地同步发展。 平台所有者（“Platform”）和应用程序所有者（“Apps\u0026rdquo","title":"扩展安全策略"},{"content":"Tetrate Service Bridge (TSB) 采用结构化数据流机制来确保配置更改和更新在整个服务网格基础设施中高效、准确地传播。这个复杂的过程涉及各种组件，包括管理平面、全局控制平面（XCP Central）和本地控制平面（XCP Edge），每个组件在配置生命周期中都发挥着关键作用。\n简化数据流从用户输入通过 TSB、XCP 到本地控制平面。 管理平面 TSB 中的所有配置更改均源自管理平面。用户通过各种接口与 TSB 配置交互，例如 gRPC API、TSB UI 和 tctl 命令行界面。配置更改随后会保存在数据库中，作为整个系统的事实来源。管理平面将这些更改推送到 XCP Central 以便进一步分发。\nMPC 组件\n由于遗留原因，XCP Central 通过 Kubernetes CRD 接收其配置。名为“MPC”的 shim 服务器建立到 TSB 的 API 服务器的 gRPC 流，以接收配置并将相应的 CR 推送到托管 XCP Central 的 Kubernetes 集群中。MPC 还会从 XCP Central 向 TSB 发送系统运行时状态的报告，以帮 …","relpermalink":"/tsb/concepts/configuration-dataflow/","summary":"Tetrate Service Bridge (TSB) 采用结构化数据流机制来确保配置更改和更新在整个服务网格基础设施中高效、准确地传播。这个复杂的过程涉及各种组件，包括管理平面、全局控制平面（XCP Central）和本地控制平面（XCP Edge）","title":"配置的数据流"},{"content":"在本部分中，你将了解如何使用 TSB 中的 AccessBindings 配置访问策略来管理不同团队和用户的权限。\n先决条件 在继续之前，请确保你已完成以下任务：\n熟悉 TSB 概念。 安装 TSB 演示环境。 部署 Istio Bookinfo 示例应用程序。 创建租户和工作区。 创建配置组。 授予团队对工作区的完全访问权限 你将配置一个访问策略，授予团队对工作区的完全访问权限。团队成员将能够创建并完全管理工作区中的资源，但无法修改工作区对象本身。这将通过分配 Creator 角色来实现。\n在左侧面板的“租户”下，选择“工作区”。 单击所需的工作区以访问其详细信息页面。 单击权限选项卡。 选择“按团队”选项可查看团队列表。 找到并单击所需团队右侧的编辑图标。 选择 Creator 角色。 单击右下角的保存更改按钮。 向组的用户授予写入权限 要向组的特定用户授予写入权限，请遵循类似的过程：\n导航到组的权限选项卡。 选择“按用户”选项可查看用户列表。 找到并单击所需用户旁边的编辑图标。 选择 Writer 角色。 单击右下角的保存更改按钮。 使用 tctl 你还可以通过应用 YAML 文 …","relpermalink":"/tsb/quickstart/permissions/","summary":"在本部分中，你将了解如何使用 TSB 中的 AccessBindings 配置访问策略来管理不同团队和用户的权限。 先决条件 在继续之前，请确保你已完成以下任务： 熟悉 TSB 概念。 安装 TSB 演示环境。 部署 Istio Bookinfo 示例应用程序。 创建租户和工作区。 创建配置组。","title":"配置权限"},{"content":" 事实来源\n我们不建议在本文档中概述的部署架构：TSB 的单个实例设计为在所有环境（测试、QA、暂存和生产）中部署。最佳做法是在中心位置部署单个 TSB，并将所有环境指向该单个 TSB。Tetrate Service Bridge 内置的控制保持了你环境配置的隔离和安全性。 一些站点已经为每个环境部署了单独的 TSB 实例。本指南的存在是为了确保这些站点能够建立一个跨环境控制配置推广的流程，不依赖于 TSB 本身。\n配置推广注意事项和建议 TSB 配置主要由许多 Kubernetes 对象 组成，例如 Tenant 是 api.tsb.tetrate.io/v2 中的一个对象。\n因此，主要建议是将 TSB 配置视为任何其他 Kubernetes 资源定义，例如：\n声明性地定义资源。 使用 GitOps 进行资源应用。 使用 kustomize 或类似工具进行配置模板化和渲染。 在将一个 TSB 安装的资源应用到另一个 TSB 安装时的主要注意事项是在计算和评估 NGAC 访问规则、集群/服务参数等时 TSB 存储配置数据的方式。\nTetrate Service Bridge 使用持久存 …","relpermalink":"/tsb/operations/configuration-promotion/","summary":"事实来源 我们不建议在本文档中概述的部署架构：TSB 的单个实例设计为在所有环境（测试、QA、暂存和生产）中部署。最佳做法是在中心位置部署单个 TSB，并将所有环境指向该单个 TSB。Tetrate Service Bridge 内置的","title":"配置推广"},{"content":"本文将介绍在从 Istio 入口网关或边车转发到应用程序时，TSB 和 Istio 代理如何处理标头。\n在开始之前，请确保你已经：\n熟悉TSB 概念 安装了 TSB 环境。你可以使用TSB 演示进行快速安装 完成了TSB 使用快速入门。 安装了示例应用程序httpbin。 Envoy（istio-proxy）中的请求标头大小 Envoy或 istio-proxy 可以处理相当大的标头。传入连接的默认最大请求标头大小为 60 KiB。\n在这种情况下，对于大多数应用程序来说，这不会成为问题，并且传入连接的请求标头将通过 istio-proxy 代理。但是，根据每个 Web 服务器的标头大小配置，你的应用程序可能会有限制。\n例如： 在Spring Boot 2和Gunicorn中，默认的最大标头大小为 8 KiB。如果需要，你可以覆盖默认设置。\n调试请求标头大小 对于此实验，你需要在集群中部署httpbin示例应用程序。你将执行两个请求，一个请求的标头大小低于最大值，另一个请求的标头大小超出应用程序容器的限制。\n低于最大值的标头 你的标头可以是任何内容，只需确保低于 8 KiB，你可以将其导 …","relpermalink":"/tsb/troubleshooting/maximum-header-size-exceed/","summary":"本文将介绍在从 Istio 入口网关或边车转发到应用程序时，TSB 和 Istio 代理如何处理标头。 在开始之前，请确保你已经： 熟悉TSB 概念 安装了 TSB 环境。你可以使用TSB 演示进行快速安装 完成了TSB 使用快速入门。 安装了示例应","title":"请求头大小超限"},{"content":"Tier-2 网关或 IngressGateway 配置工作负载以充当进入网格的流量网关。入口网关还提供基本的 API 网关功能，如 JWT 令牌验证和请求授权。\n在本指南中，你将会：\n部署 bookinfo 应用程序 分为两个不同的集群，配置为 Tier-2，一个集群中有 productpage，另一个集群中有 reviews、details 和 rating。 在开始之前，请确保你已经在 cluster 1 中部署了 productpage，并在 cluster 2 中部署了 details、ratings 和 reviews。对于此演示，我们假设你已经在 TSB 中部署和配置了 Bookinfo。\n所有组件中的控制平面都需要共享相同的信任根。 场景 在这个场景中，我们将配置两个配置为 Tier-2 的控制平面集群（tsb-tier2gcp1 和 tsb-tier2gcp2）。我们将在两个 Tier-2 集群中都部署 Bookinfo，tsb-tier2gcp1 中将安装 productpage，而 tsb-tier2gcp2 中将安装 reviews、details …","relpermalink":"/tsb/howto/gateway/multi-cluster-traffic-routing-using-tier2gw/","summary":"Tier-2 网关或 IngressGateway 配置工作负载以充当进入网格的流量网关。入口网关还提供基本的 API 网关功能，如 JWT 令牌验证和请求授权。 在本指南中，你将会： 部署 bookinfo 应用程序 分为两个不同的集群，配置为 Tier-2，一个集群中有 produ","title":"使用 Tier-2 网关进行多集群流量路由"},{"content":"本文档描述了使用工作负载载入功能将本地工作负载载入 TSB 的步骤。\n在继续之前，请确保你已完成设置工作负载载入文档中描述的步骤。\n背景 通过工作负载载入加入网格的每个工作负载都必须具有可验证的身份。\n云中的 VM 具有开箱即用的可验证身份。此类身份由各云平台提供。\n然而，在本地环境中，是一个黑盒。你的本地工作负载是否具有可验证的身份完全取决于你自己的技术堆栈。\n因此，要能够载入本地工作负载，你需要确保它们具有JWT 令牌形式的可验证身份。\n概述 本地工作负载的工作负载载入设置包括以下额外步骤：\n配置受信任的 JWT 发行者 允许本地工作负载加入 WorkloadGroup 配置工作负载载入代理以使用你的自定义凭证插件 载入本地工作负载 配置受信任的 JWT 发行者 要配置一组受信任的 JWT 发行者，用于断言本地工作负载的身份，请按以下方式编辑 TSB ControlPlane CR 或 Helm values：\nspec: ... meshExpansion: onboarding: ... # 专用于本地工作负载的额外配置 workloads: authentication: …","relpermalink":"/tsb/setup/workload-onboarding/guides/on-premise-workloads/","summary":"本文档描述了使用工作负载载入功能将本地工作负载载入 TSB 的步骤。 在继续之前，请确保你已完成设置工作负载载入文档中描述的步骤。 背景 通过工作负载载入加入网格的每个工作负载都必须具有可验证的身份。 云中的 VM 具有开","title":"在本地载入工作负载"},{"content":"Argo Rollouts Kubectl 插件可以提供本地 UI 仪表板来可视化你的 Rollouts。\n要启动它，请在包含你的 Rollouts 的命名空间中运行 kubectl argo rollouts dashboard 。然后访问 localhost:3100 查看用户界面。\n列表视图 Rollouts 列表视图 单独的 Rollout 视图 Rollouts 视图 ","relpermalink":"/argo-rollouts/dashboard/","summary":"Argo Rollouts Kubectl 插件可以提供本地 UI 仪表板来可视化你的 Rollouts。 要启动它，请在包含你的 Rollouts 的命名空间中运行 kubectl argo rollouts dashboard 。然后访问 localhost:3100 查看用户界面。 列表视图 Rollouts 列表视图 单独的 Rollout 视图 Rollouts 视图","title":"UI Dashboard"},{"content":"本指南介绍了 Argo Rollouts 如何与 NGINX Ingress Controller 集成进行流量整形。本指南基于 基本入门指南 的概念。\n要求 安装了 NGINX Ingress 控制器的 Kubernetes 集群 🔔 提示：请参阅 NGINX 环境设置指南 以了解如何使用 nginx 设置本地 minikube 环境。\n1. 部署 Rollout、服务和 Ingress 当使用 NGINX Ingress 作为流量路由器时，Rollout 金丝雀策略必须定义以下强制字段：\napiVersion: argoproj.io/v1alpha1 kind: Rollout metadata: name: rollouts-demo spec: strategy: canary: # 引用控制器将更新并指向金丝雀 ReplicaSet 的服务 canaryService: rollouts-demo-canary # 引用控制器将更新并指向稳定 ReplicaSet 的服务 stableService: rollouts-demo-stable trafficRouting: …","relpermalink":"/argo-rollouts/getting-started/nginx/","summary":"本指南介绍了 Argo Rollouts 如何与 NGINX Ingress Controller 集成进行流量整形。本指南基于 基本入门指南 的概念。 要求 安装了 NGINX Ingress 控制器的 Kubernetes 集群 🔔 提示：请参阅 NGINX 环境设置指南 以了解如何使用 nginx 设置本地 minikube 环境。 1. 部署 Rollout、服务和 Ingress 当使用","title":"Nginx Ingress 快速开始"},{"content":"Nginx Ingress Controller 允许通过一个或多个 Ingress 对象进行流量管理，以配置直接将流量路由到 Pod 的 Nginx 部署。每个 Nginx Ingress 都包含多个注释，可以修改 Nginx 部署的行为。对于应用程序不同版本之间的流量管理，Nginx Ingress 控制器提供了通过引入第二个 Ingress 对象（称为金丝雀 Ingress）进行流量拆分的功能。你可以在官方的 金丝雀注释文档页面 上阅读更多关于这些金丝雀注释的信息。金丝雀 Ingress 忽略任何其他非金丝雀 nginx 注释。取而代之，它利用来自主要 Ingress 的注释设置。\nRollout 控制器始终会在金丝雀 Ingress 上设置以下两个注释（使用你配置的或默认的 nginx.ingress.kubernetes.io 前缀）：\ncanary: true 表示这是金丝雀 Ingress canary-weight: \u0026lt;num\u0026gt; 表示将发送到金丝雀的流量百分比。如果所有流量都路由到稳定服务，则设置为 0 …","relpermalink":"/argo-rollouts/traffic-management/nginx/","summary":"Nginx Ingress Controller 允许通过一个或多个 Ingress 对象进行流量管理，以配置直接将流量路由到 Pod 的 Nginx 部署。每个 Nginx Ingress 都包含多个注释，可以修改 Nginx 部署的行为。对于应用程序不同版本之间的流量管理，Nginx Ingress 控制器提供了通过引入第二个 Ingress","title":"Nginx"},{"content":"出于各种原因，应用程序通常需要重新启动，例如出于健康目的或强制执行启动逻辑，例如重新加载修改的 Secret。在这些情况下，不希望进行整个蓝绿或金丝雀更新过程。Argo Rollouts 支持通过执行所有 Rollout 中的 Pod 的滚动重建来重启其所有 Pod 的能力，同时跳过常规的 BlueGreen 或 Canary 更新策略。\n它是如何工作的 可以通过 kubectl 插件使用 kubectl-argo-rollouts restart 命令来重新启动 Rollout：\nkubectl-argo-rollouts restart ROLLOUT 或者，如果 Rollouts 与 Argo CD 一起使用，则可以通过 Argo CD UI 或 CLI 执行捆绑的“restart”操作：\nargocd app actions run my-app restart --kind Rollout --resource-name my-rollout 这两种机制都会将 Rollout 的.spec.restartAt更新为以RFC 3339 格式的 UTC 字符串的当前时间形式（ …","relpermalink":"/argo-rollouts/rollout/restart/","summary":"出于各种原因，应用程序通常需要重新启动，例如出于健康目的或强制执行启动逻辑，例如重新加载修改的 Secret。在这些情况下，不希望进行整个蓝绿或金丝雀更新过程。Argo Rollouts 支持通过执行所有 Rollout 中的 Pod 的滚动重","title":"重启 Rollouts Pod"},{"content":"本章旨在让你为上线 SPIFFE/SPIRE 时需要做出的许多决定做好准备。\n准备人力 如果你读了前面的章节，你一定很想开始使用 SPIRE，以一种可以在许多不同类型的系统和所有组织的服务中利用的方式管理身份。然而，在你开始之前，你需要考虑，部署 SPIRE 是一个重大的基础设施变化，有可能影响到许多不同的系统。本章是关于如何开始规划 SPIRE 的部署：获得认同，以不中断的方式启用 SPIRE 支持，然后利用它来实施新的安全控制。\n组建团队并确定其他利益相关者 要部署 SPIRE，你需要确定来自安全、软件开发和 DevOps 团队的利益相关者。谁来维护 SPIRE 服务器本身？谁来部署代理？谁来编写注册条目？谁将把 SPIFFE 功能集成到应用程序中？它将如何影响现有的 CI/CD 管道？如果发生了服务中断，谁来修复它？性能要求和服务水平目标是什么？\n在这本书中，以及许多公开的博客文章和会议演讲中，都有一些成功部署 SPIRE 的组织的例子，既可以作为一种模式，也可以作为向同事宣传 SPIRE 的有用材料。\n说明你的情况并获得支持 SPIRE 跨越了几个不同的传统信息技术孤岛，因此， …","relpermalink":"/spiffe/before-you-start/","summary":"本章旨在让你为上线 SPIFFE/SPIRE 时需要做出的许多决定做好准备。 准备人力 如果你读了前面的章节，你一定很想开始使用 SPIRE，以一种可以在许多不同类型的系统和所有组织的服务中利用的方式管理身份。然而，在你开始之前，你","title":"开始前的准备"},{"content":"在 Kubernetes 中管理 pod 时，Cilium 将创建一个 CiliumEndpoint 的自定义资源定义（CRD）。每个由 Cilium 管理的 pod 都会创建一个 CiliumEndpoint，名称相同且在同一命名空间。CiliumEndpoint 对象包含的信息与 cilium endpoint get 在.status 字段下的 json 输出相同，但可以为集群中的所有 pod 获取。添加 -o json 将导出每个端点的更多信息。这包括端点的标签、安全身份和对其有效的策略。\n例如：\n$ kubectl get ciliumendpoints --all-namespaces NAMESPACE NAME AGE default app1-55d7944bdd-l7c8j 1h default app1-55d7944bdd-sn9xj 1h default app2 1h default app3 1h kube-system cilium-health-minikube 1h kube-system microscope 1h …","relpermalink":"/cilium-handbook/kubernetes/ciliumendpoint/","summary":"在 Kubernetes 中管理 pod 时，Cilium 将创建一个 CiliumEndpoint 的自定义资源定义（CRD）。每个由 Cilium 管理的 pod 都会创建一个 CiliumEndpoint，名称相同且在同一命名空间。CiliumEndpoint 对象包含的信息与 cilium endpoint","title":"端点 CRD"},{"content":"现在你已经了解了什么是 eBPF 以及它是如何工作的，我们再探索一些可能会在生产部署中使用的基于 eBPF 技术的工具。我们将举一些基于 eBPF 的开源项目的例子，这些项目提供了三方面的能力：网络、可观测性和安全。\n网络 eBPF 程序可以连接到网络接口和内核的网络堆栈的各个点。在每个点上，eBPF 程序可以丢弃数据包，将其发送到不同的目的地，甚至修改其内容。这就实现了一些非常强大的功能。让我们来看看通常用 eBPF 实现的几个网络功能。\n负载均衡 Facebook 正在大规模的使用 eBPF 的网络功能，因此你不必对 eBPF 用于网络的可扩展性有任何怀疑。他们是 BPF 的早期采用者，并在 2018 年推出了 Katran，一个开源的四层负载均衡器。\n另一个高度扩展的负载均衡器的例子是来自 Cloudflare 的 Unimog 边缘负载均衡器。通过在内核中运行，eBPF 程序可以操作网络数据包，并将其转发到适当的目的地，而不需要数据包通过网络堆栈和用户空间。\nCilium 项目作为一个 eBPF Kubernetes 网络插件更为人所知（我一会儿会讨论），但作为独立的负载均衡 …","relpermalink":"/what-is-ebpf/ebpf-tools/","summary":"现在你已经了解了什么是 eBPF 以及它是如何工作的，我们再探索一些可能会在生产部署中使用的基于 eBPF 技术的工具。我们将举一些基于 eBPF 的开源项目的例子，这些项目提供了三方面的能力：网络、可观测性和安全。 网络 eBPF 程序可以","title":"第六章：eBPF 工具"},{"content":"由于 DevSecOps 的基本要素跨越了开发（安全的构建和测试、打包）、交付 / 部署和持续监控（以确保运行期间的安全状态），本文建议的目标受众包括软件开发、运维和安全团队。\n","relpermalink":"/service-mesh-devsecops/intro/target-audience/","summary":"由于 DevSecOps 的基本要素跨越了开发（安全的构建和测试、打包）、交付 / 部署和持续监控（以确保运行期间的安全状态），本文建议的目标受众包括软件开发、运维和安全团队。","title":"1.3 目标受众"},{"content":"无论代码类型如何，CI/CD 管道都有一些共同的实施问题需要解决。确保流程安全涉及到为操作构建任务分配角色。自动化工具（例如，Git Secrets）可用于此目的。为保证 CI/CD 管道的安全，以下安全任务应被视为最低限度：\n强化托管代码和工件库的服务器。 确保用于访问存储库的凭证，如授权令牌和生成拉动请求的凭证。 控制谁可以在容器镜像注册处签入和签出，因为它们是 CI 管道产生的工件的存储处，是 CI 和 CD 管道之间的桥梁。 记录所有的代码和构建更新活动。 如果在 CI 管道中构建或测试失败 —— 向开发人员发送构建报告并停止进一步的管道任务。配置代码库自动阻止来自 CD 管道的所有拉取请求。 如果审计失败，将构建报告发送给安全团队，并停止进一步的管道任务。 确保开发人员只能访问应用程序代码，而不能访问五种管道代码类型中的任何一种。 在构建和发布过程中，在每个需要的 CI/CD 阶段签署发布工件（最好是多方签署）。 在生产发布期间，验证所有需要的签名（用多个阶段的密钥生成），以确保没有人绕过管道。 ","relpermalink":"/service-mesh-devsecops/implement/securing-the-ci-cd-pipeline/","summary":"无论代码类型如何，CI/CD 管道都有一些共同的实施问题需要解决。确保流程安全涉及到为操作构建任务分配角色。自动化工具（例如，Git Secrets）可用于此目的。为保证 CI/CD 管道的安全，以下安全任务应被视为","title":"4.6 确保 CI/CD 管道的安全"},{"content":"在前一章中，我们了解了基础架构的各种表示以及围绕其部署工具的各种方法。在本章中，我们将看看如何设计部署和管理基础架构的应用程序。在上一章中我们重点关注基础架构即软件的开放世界，有时称为基础架构即应用。\n在云原生环境中，传统的基础架构运维人员需要转变为基础架构软件工程师。与过去的其他运维角色不同，这仍然是一种新兴的做法。我们迫切需要开始探索这种模式和制定标准。\n基础架构即软件与基础架构即代码之间的根本区别在于，软件会持续运行，并会根据调节器模式创建或改变基础架构，我们将在本章后面对其进行解释。此外，基础架构即软件的新范例是，软件现在与数据存储具有更传统的关系，并公开用于定义所需状态的 API。例如，该软件可能会根据数据存储中的需要改变基础架构的表示形式，并且可以很好地管理数据存储本身！希望进行协调的状态更改通过 API 发送到软件，而不是通过运行静态代码库中的程序。\n迈向基础架构即软件的第一步是让基础架构的运维人员意识到自己是软件工程师。我们热烈欢迎您来到这个领域！先前的工具（例如配置管理）也有类似的改变基础架构运维人员的工作职能的目标， …","relpermalink":"/cloud-native-infra/designing-infrastructure-applicaitons/","summary":"在前一章中，我们了解了基础架构的各种表示以及围绕其部署工具的各种方法。在本章中，我们将看看如何设计部署和管理基础架构的应用程序。在上一章中我们重点关注基础架构即软件的开放世界，有时称为基础架构即应用。","title":"第 4 章：设计基础架构应用程序"},{"content":"Kubernetes 可以成为数据和 / 或计算能力盗窃的重要目标。虽然数据盗窃是传统上的主要动机，但寻求计算能力（通常用于加密货币挖掘）的网络行为者也被吸引到 Kubernetes 来利用其底层基础设施。除了资源盗窃，网络行为者还可能针对 Kubernetes 造成拒绝服务。下面的威胁代表了 Kubernetes 集群最可能的破坏源。\n供应链风险 - 对供应链的攻击载体是多种多样的，并且在减轻风险方面具有挑战性。供应链风险是指对手可能颠覆构成系统的任何元素的风险，包括帮助提供最终产品的产品组件、服务或人员。这可能包括用于创建和管理 Kubernetes 集群的第三方软件和供应商。供应链的潜在威胁会在多个层面上影响 Kubernetes，包括： 容器 / 应用层面 - 在 Kubernetes 中运行的应用及其第三方依赖的安全性，它们依赖于开发者的可信度和开发基础设施的防御能力。来自第三方的恶意容器或应用程序可以为网络行为者在集群中提供一个立足点。 基础设施 - 托管 Kubernetes 的底层系统有其自身的软件和硬件依赖性。系统作为工作节点或控制平面一部分的，任何潜在威胁都可能为网 …","relpermalink":"/kubernetes-hardening-guidance/threat-model/","summary":"Kubernetes 可以成为数据和 / 或计算能力盗窃的重要目标。虽然数据盗窃是传统上的主要动机，但寻求计算能力（通常用于加密货币挖掘）的网络行为者也被吸引到 Kubernetes 来利用其底层基础设施。除了资源盗窃，网络行为者还可能针对 Kubernetes 造成","title":"威胁建模"},{"content":"第 2 章描述了实现自动分析所需的数据模型，第 4 章描述了支持原生 开源仪表的额外要求，并赋予各角色（应用程序所有者、运维和响应者）自主权。这就是我们对现代可观测性的概念模型。\n在本报告的其余部分，我们描述了这个新模型的一个事实实现，即 OpenTelemetry。这一章描述了构成 OpenTelemetry 遥测管道的所有组件。后面的章节将描述稳定性保证、建议的设置以及 OpenTelemetry 现实中的部署策略。有关该项目的更多细节可以在附录中找到。\n信号 OpenTelemetry 规范被组织成不同类型的遥测，我们称之为信号（signal）。主要的信号是追踪。日志和度量是其他例子。信号是 OpenTelemetry 中最基本的设计单位。\n每一个额外的信号首先是独立开发的，然后与追踪和其他相关信号整合。这种分离允许开发新的、实验性的信号，而不影响已经变得稳定的信号的兼容性保证。\nOpenTelemetry 是一个跨领域的关注点（cross-cutting concern），它在事务通过每个库和服务时追踪其执行。为了达到这个目的，所有的信号都建立在低级别的上下文传播系统之上，该系 …","relpermalink":"/opentelemetry-obervability/architectural-overview/","summary":"第 5 章：OpenTelemetry 架构概述","title":"第 5 章：OpenTelemetry 架构概述"},{"content":"HCM 是一个网络级的过滤器，将原始字节转译成 HTTP 级别的消息和事件（例如，收到的 Header，收到的 Body 数据等）。\nHCM 过滤器还处理标准的 HTTP 功能。它支持访问记录、请求 ID 生成和跟踪、Header 操作、路由表管理和统计等功能。\n从协议的角度来看，HCM 原生支持 HTTP/1.1、WebSockets、HTTP/2 和 HTTP/3（仍在 Alpha 阶段）。\nEnvoy 代理被设计成一个 HTTP/2 复用代理，这体现在描述 Envoy 组件的术语中。\nHTTP/2 术语\n在 HTTP/2 中，流是已建立的连接中的字节的双向流动。每个流可以携带一个或多个消息（message）。消息是一个完整的帧（frame）序列，映射到一个 HTTP 请求或响应消息。最后，帧是 HTTP/2 中最小的通信单位。每个帧都包含一个帧头（frame header），它至少可以识别该帧所属的流。帧可以携带有关 HTTP Header、消息有效载荷等信息。\n无论流来自哪个连接（HTTP/1.1、HTTP/2 或 HTTP/3），Envoy …","relpermalink":"/cloud-native-handbook/service-mesh/http-conneciton-manager/","summary":"HCM 是一个网络级的过滤器，将原始字节转译成 HTTP 级别的消息和事件（例如，收到的 Header，收到的 Body 数据等）。 HCM 过滤器还处理标准的 HTTP 功能。它支持访问记录、请求 ID 生成和跟踪、Header 操作、路由表管理和统计","title":"HTTP 连接管理器介绍"},{"content":"有时开发人员会拖延（Pushback）代码审查。他们要么不同意您的建议，要么抱怨您太严格。\n谁是对的？ 当开发人员不同意您的建议时，请先花点时间考虑一下是否正确。通常，他们比你更接近代码，所以他们可能真的对它的某些方面有更好的洞察力。他们的论点有意义吗？从代码健康的角度来看它是否有意义？如果是这样，让他们知道他们是对的，把问题解决。\n但是，开发人员并不总是对的。在这种情况下，审查人应进一步解释为什么认为他们的建议是正确的。好的解释在描述对开发人员回复的理解的同时，还会解释为什么请求更改。\n特别是，当审查人员认为他们的建议会改善代码健康状况时，他们应该继续提倡更改，如果他们认为最终的代码质量改进能够证明所需的额外工作是合理的。提高代码健康状况往往只需很小的几步。\n有时需要几轮解释一个建议才能才能让对方真正理解你的用意。只要确保始终保持礼貌，让开发人员知道你有听到他们在说什么，只是你不同意该论点而已。\n沮丧的开发者 审查者有时认为，如果审查者人坚持改进，开发人员会感到不安。有时候开发人员会感到很沮丧，但这样的感觉通常只会持续很短的时间，后来他们会非常感谢您在提高代码质量方面给他们的帮助。通 …","relpermalink":"/eng-practices/review/reviewer/pushback/","summary":"有时开发人员会拖延（Pushback）代码审查。他们要么不同意您的建议，要么抱怨您太严格。 谁是对的？ 当开发人员不同意您的建议时，请先花点时间考虑一下是否正确。通常，他们比你更接近代码，所以他们可能真的","title":"处理 Code Review 中的拖延"},{"content":"背景 SPIFFE 规范定义了建立一个平台无关的工作负载身份框架所需的文档和接口，该框架能够在不需要实现身份转换或凭证交换逻辑的情况下连接不同域中的系统。它们定义了一个“信任域”，它作为一个身份命名空间。\nSPIFFE 的本质是分散的。每个信任域都根据自己的授权行事，与驻留在其他信任域中的系统在管理上是隔离的。虽然信任域划定了行政和/或安全域，但核心的 SPIFFE 用例是在需要时跨越这些边界进行通信。因此，有必要定义一种机制，使实体可以被引入到外部信任域中，从而允许其验证由“其他”SPIFFE 授权机构颁发的凭证，并允许一个信任域中的工作负载安全地验证一个外部信任域中的工作负载。\nSPIFFE 包是一个包含验证特定信任域凭证所需的公钥材料的资源。本文档介绍了一种规范，用于安全地获取 SPIFFE 包，以便验证外部机构颁发的身份。其中包括有关如何提供 SPIFFE 包、如何检索 SPIFFE 包以及如何验证提供它们的端点的信息。\n简介 SPIFFE 联邦使得在信任域之间验证身份凭证 (SVIDs) 成为可能。具体来说，它是获取验证来自不同信任域颁发的 SVIDs 所需的 SPIFFE  …","relpermalink":"/spiffe-and-spire/standard/spiffe-federation/","summary":"背景 SPIFFE 规范定义了建立一个平台无关的工作负载身份框架所需的文档和接口，该框架能够在不需要实现身份转换或凭证交换逻辑的情况下连接不同域中的系统。它们定义了一个“信任域”，它作为一个身份命名空间。 SPIFFE 的本质是","title":"SPIFFE 联邦"},{"content":"本文档描述了如何为 Tetrate Service Bridge（TSB）配置 GitOps 集成。\nTSB 中的 GitOps 集成允许你与应用程序打包和部署的生命周期以及不同的持续部署（CD）系统进行集成。\n本文假设你已经具备了配置 GitOps CD 系统的工作知识，例如 FluxCD 或 ArgoCD。\n工作原理 一旦在管理平面集群和/或应用程序集群中启用了 GitOps，CD 系统将能够将 TSB 配置应用于其中，然后将其推送到 TSB 管理平面。\n启用 GitOps 可以通过 ManagementPlane 或 ControlPlane CR 或 Helm 值为每个集群配置 GitOps 组件。\n注意\n在同时在管理平面和控制平面中启用 GitOps 时，如果两个平面部署在同一个集群中（通常用于小型环境或演示安装），则只有两者之一会生效。具体来说，控制平面将是唯一启用的平面。这是为了避免两个平面多次推送相同的资源。 ManagementPlane 和 ControlPlane CR 都有一个名为 gitops 的组件，设置 enabled: true …","relpermalink":"/tsb/operations/features/configure-gitops/","summary":"本文档描述了如何为 Tetrate Service Bridge（TSB）配置 GitOps 集成。 TSB 中的 GitOps 集成允许你与应用程序打包和部署的生命周期以及不同的持续部署（CD）系统进行集成。 本文假设你已经具备了配置 GitOps CD 系统的工作知识，例如 FluxCD 或 Ar","title":"GitOps"},{"content":"本文描述了当标头中包含多个transfer-encoding:chunked时，TSB 将如何处理请求/响应，并帮助你确定问题是来自源还是目标。\n我们建议解决此问题的方法是确保请求头和响应头中都只有一个transfer-encoding:chunked，否则 Envoy 将拒绝请求。\n在开始之前，请确保你已经：\n熟悉TSB 概念 安装TSB 演示环境 部署Istio Bookinfo示例应用程序 注意：对于响应部分，我们在这里使用的应用程序特意生成了多个transfer-encoding:chunked标头，仅用于文档目的。\n我们经常看到请求/响应的标头包含多个transfer-encoding:chunked，这不是有效的标头，因为 Envoy 会拒绝这种请求。在我们安装的 bookinfo 应用程序中，我们可以更深入地看一看当 Envoy 拒绝简单请求时，它将以特定的错误代码拒绝。\n带有\u0026#34;Transfer-Encoding: chunked,chunked\u0026#34;的请求头 对于我们的 bookinfo 应用程序，我们将使用 curl 创建一个简单的请求，发送多 …","relpermalink":"/tsb/troubleshooting/multiple-transfer-encoding-chunked/","summary":"本文描述了当标头中包含多个transfer-encoding:chunked时，TSB 将如何处理请求/响应，并帮助你确定问题是来自源还是目标。 我们建议解决此问题的方法是确保请求头和响应头中都只有一个t","title":"多重传输编码块处理"},{"content":"通过 TSB 和 TSE，你有多种选项可以配置你的平台，使应用程序在集群间以高可用性的方式运行。在每种情况下，一旦平台所有者（“平台”）合适地准备了平台，应用程序所有者（“应用”）只需要从两个或多个集群中部署和发布其服务，即可利用 HA 的优势。\n高可用性的选项 选项 1：使用 Tetrate 的 Edge Gateway 解决方案 使用 Tetrate 的 Edge Gateway 来为多个集群提供前端，并在它们之间分发流量。\n选项 2：使用 Tetrate 的 Route 53 控制器与 AWS Route 53 使用 AWS Route 53 控制器为从 TSE 或 TSB 发布的服务自动配置 Route 53。\n选项 3：手动配置 GSLB 解决方案 手动配置 GSLB 解决方案，以在集群入口点之间分发流量并执行健康检查。\n开始之前 在跨集群负载均衡时，你可能需要依赖 DNS GSLB 解决方案将流量分发到每个服务的入口点（边缘网关等）。在这种情况下，你需要考虑健康检查的功能。\n一旦在集群上部署了应用程序，可能需要详细的每应用程序健康检查，但首先，基础设施健康检查是一个很好的起 …","relpermalink":"/tsb/design-guides/app-onboarding/high-availability/","summary":"通过 TSB 和 TSE，你有多种选项可以配置你的平台，使应用程序在集群间以高可用性的方式运行。在每种情况下，一旦平台所有者（“平台”）合适地准备了平台，应用程序所有者（“应","title":"跨集群实现高可用性"},{"content":"本文档介绍了如何在 TSB 中调整不同组件的日志级别，包括平台组件、Envoy Sidecar 和入口网关的运行时，以及查看日志的过程。\n在开始之前，请确保：\n你已正确安装和配置了 TSB。 你已安装和配置了kubectl以访问应用程序集群。 对于示例命令，我们假设你在helloworld命名空间中部署了一些应用程序。\nTSB 组件产生大量日志\n小心在较长时间内启用 TSB 的各个范围的调试日志 - TSB 组件产生大量日志！由于自动日志摄取与 TSB 或 Sidecar 的日志级别提高相结合，可能会面临大额的日志摄取费用。 列出可用的组件 为了更改每个组件的日志级别，你需要知道哪些组件可用。为此，在tctl中有一个实用命令，它将利用当前的kubectl连接信息（上下文）并列出该集群中可用的组件。\n$ tctl experimental debug list-components PLANE COMPONENT DEPLOYMENTS management ldap ldap management mpc mpc management xcp central, …","relpermalink":"/tsb/operations/configure-log-levels/","summary":"本文档介绍了如何在 TSB 中调整不同组件的日志级别，包括平台组件、Envoy Sidecar 和入口网关的运行时，以及查看日志的过程。 在开始之前，请确保： 你已正确安装和配置了 TSB。 你已安装和配置了kubectl以访问应用","title":"配置日志级别"},{"content":"在本部分中，你将配置 Ingress Gateway 以允许外部流量到达 TSB 环境中的 bookinfo 应用程序。\n先决条件 在继续之前，请确保你已完成以下任务：\n熟悉 TSB 概念。 安装 TSB 演示环境。 部署 Istio Bookinfo 示例应用程序。 创建租户和工作区。 创建配置组。 配置权限。 创建入口网关对象 你将创建一个 Ingress Gateway 对象来为你的 bookinfo 应用程序启用外部流量。\n创建 ingress.yaml 创建一个名为 ingress.yaml 的文件，其中包含以下内容：\napiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: tsb-gateway-bookinfo namespace: bookinfo spec: selector: app: tsb-gateway-bookinfo servers: - port: number: 80 name: http protocol: HTTP hosts: - \u0026#34;*\u0026#34; 使用 kubectl  …","relpermalink":"/tsb/quickstart/ingress-gateway/","summary":"在本部分中，你将配置 Ingress Gateway 以允许外部流量到达 TSB 环境中的 bookinfo 应用程序。 先决条件 在继续之前，请确保你已完成以下任务： 熟悉 TSB 概念。 安装 TSB 演示环境。 部署 Istio Bookinfo 示例应用程序。 创建租户和工作区。 创建配置组。 配置权限。 创","title":"入口网关"},{"content":"在这个页面中，你将学习如何使用 tctl CLI 连接到 TSB 以及使用 CLI 的一些基础知识。\n在开始之前：\n安装 TSB 管理平面 (仅自管理) 下载 Tetrate Service Bridge CLI (tctl) 获取 TSB 的 organization 名称 - 你可以在 TSB UI 中找到，或在 TSB ManagementPlane CR 的安装时配置 TSB 提供了一个用户界面，但本站点中的大多数示例 - 以及大多数脚本和自动化 - 将使用 tctl CLI。本文档将涵盖如何登录，以便你可以使用 CLI，并提供更新凭据的步骤。\n概述 有三种配置 tctl 的方法：使用 tctl login、从 UI 下载凭据包，或手动配置它。本文档的其余部分详细描述了每种方法。\n首选的方法是使用 tctl login 连接到 TSB，使用 default 配置文件： tctl config clusters set default --bridge-address $TSB_ADDRESS tctl login Organization: tetrate Tenant: …","relpermalink":"/tsb/setup/tctl-connect/","summary":"在这个页面中，你将学习如何使用 tctl CLI 连接到 TSB 以及使用 CLI 的一些基础知识。 在开始之前： 安装 TSB 管理平面 (仅自管理) 下载 Tetrate Service Bridge CLI (tctl) 获取 TSB 的 organization 名称 - 你可以在 TSB UI 中找到，或在 TSB ManagementPlane CR 的安装时配置 TSB 提供了一个用户界面，","title":"使用 tctl 连接到 TSB"},{"content":"本文档描述了如何使用 Tier-1 网关 进行多集群流量切换。你将创建一个用于 Tier-1 网关部署的集群，以及两个用于运行 bookinfo 应用程序 的集群。\n每个应用程序集群都将配置一个 入口网关，用于将流量路由到 bookinfo 应用程序。最后，你将配置 Tier-1 网关，将流量从一个集群上运行的应用程序切换到另一个集群上运行的应用程序。\n在开始之前，请确保你已经：\n熟悉了 TSB 概念 熟悉了 TSB 管理平面 和 集群载入。以下场景将假定你已经安装了 TSB 管理平面，并且已经将 tctl 配置为正确的管理平面。 Kubernetes 供应商\n以下场景在 GKE Kubernetes 集群上进行了测试。然而，这里描述的步骤应该足够通用，可以在其他 Kubernetes 供应商上使用。 证书\n本场景使用自签名证书来进行 Istio CA。这里的说明仅供演示目的。对于生产集群设置，强烈建议使用生产就绪的 CA。 Tier-1 网关 在 TSB 中，有两种接收入站流量的网关类型：Tier-1 网关和 Ingress 网关（也称为 Tier-2 网关）。Tier-1 网关将流 …","relpermalink":"/tsb/howto/gateway/multi-cluster-traffic-shifting/","summary":"本文档描述了如何使用 Tier-1 网关 进行多集群流量切换。你将创建一个用于 Tier-1 网关部署的集群，以及两个用于运行 bookinfo 应用程序 的集群。 每个应用程序集群都将配置一个 入口网关，用于将流量路由到 bookinfo 应用程序。最后，你将配置 Tier-1 网关","title":"使用 Tier-1 网关进行多集群流量切换"},{"content":" 技术预览\nTetrate WAF 是 TSB 未来功能的技术预览。目前不建议在生产环境中使用。 本文将描述如何在 TSB 中启用和配置 Web 应用程序防火墙（WAF）功能。\n概述 Web 应用程序防火墙（WAF）检查入站和出站的 HTTP 流量。它会将流量与一系列签名进行匹配，以检测攻击尝试、格式不正确的流量和敏感数据的泄漏。可疑的流量可以被阻止，警报可以被触发，并且流量可以被记录以供后续分析。\n传统的 WAF 解决方案在网络的边缘运行，检查从互联网进出的流量。它们基于这样的假设：恶意行为者是外部的，不在你的内部基础设施中。\nTetrate WAF 在应用程序内部运行，以非常精细的方式保护个别服务。通过 Tetrate WAF，你可以增强你的零信任姿态，保护内部和外部攻击者。\n启用此功能的好处包括：\n使用行业标准的 OWASP 核心规则集（CRS）检测规则来检测攻击流量和数据泄漏的检测和阻止。在撰写本文时，Tetrate WAF 使用 CRS v4.0.0-rc1。 使用定制的自定义规则进行保护的微调。 快速应对已知 CVE 或 0day 攻击的能力。 基于你的基础设施和已知的易受 …","relpermalink":"/tsb/howto/waf/","summary":"技术预览 Tetrate WAF 是 TSB 未来功能的技术预览。目前不建议在生产环境中使用。 本文将描述如何在 TSB 中启用和配置 Web 应用程序防火墙（WAF）功能。 概述 Web 应用程序防火墙（WAF）检查入站和出站的 HTTP 流量。它会将流量与一系列签","title":"使用 WAF 功能"},{"content":"该文档描述了如何使用工作负载载入功能将 AWS Elastic Container Service (ECS) 任务载入到 TSB。\n在继续之前，请确保你已完成 设置工作负载载入文档 中描述的步骤。如果你不计划载入虚拟机，可以跳过配置本地仓库和安装软件包的步骤，因为 ECS 任务的流程略有不同。\n背景 通过工作负载载入将工作负载引入 mesh 的每个工作负载都必须具有可验证的身份。对于 AWS ECS 任务，使用 任务 IAM 角色 来标识尝试加入 mesh 的任务。\nOnboarding Agent 容器作为 AWS ECS 任务中的边车与工作负载容器一起运行。在启动时，Onboarding Agent 与 AWS ECS 环境交互，获取与任务的 IAM 角色关联的凭据，并使用这些凭据与 Workload Onboarding Plane 进行身份验证。\n概述 与虚拟机工作负载相比，AWS ECS 工作负载的工作负载载入设置包括以下额外步骤：\n允许 AWS ECS 任务加入 WorkloadGroup 配置 AWS ECS 任务定义，包括 IAM 角色和运行 Workload …","relpermalink":"/tsb/setup/workload-onboarding/guides/ecs-workloads/","summary":"该文档描述了如何使用工作负载载入功能将 AWS Elastic Container Service (ECS) 任务载入到 TSB。 在继续之前，请确保你已完成 设置工作负载载入文档 中描述的步骤。如果你不计划载入虚拟机，可以跳过配置本地仓库和安装软件包的步骤，因为 ECS 任务","title":"载入 AWS ECS 工作负载"},{"content":"🔔 提醒：自 1.5 版本起可用 - 状态：Alpha\nArgo Rollouts 支持通过第三方插件系统获取分析指标。这允许用户扩展 Rollouts 的功能以支持原生不支持的度量提供者。Rollouts 使用一个名为 go-plugin 的插件库来实现这一点。你可以在这里找到一个示例插件：rollouts-plugin-trafficrouter-sample-nginx\n使用 Traffic Router 插件 安装和使用 Argo Rollouts 插件有两种方法。第一种方法是将插件可执行文件挂载到 rollouts 控制器容器中。第二种方法是使用 HTTP（S）服务器托管插件可执行文件。\n将插件可执行文件挂载到 rollouts 控制器容器中 有几种方法可以将插件可执行文件挂载到 rollouts 控制器容器中。其中一些将取决于你的特定基础设施。这里有几种方法：\n使用 init 容器下载插件可执行文件 使用 Kubernetes 卷挂载共享卷，如 NFS、EBS 等。 将插件构建到 rollouts 控制器容器中 然后，你可以使用 configmap 将插件可执行文件位置指向 …","relpermalink":"/argo-rollouts/traffic-management/plugins/","summary":"🔔 提醒：自 1.5 版本起可用 - 状态：Alpha Argo Rollouts 支持通过第三方插件系统获取分析指标。这允许用户扩展 Rollouts 的功能以支持原生不支持的度量提供者。Rollouts 使用一个名为 go-plugin 的插件库来实现这一点。你可以在这里找到","title":"流量路由插件"},{"content":"在回滚更新时，我们可能会为所有策略缩小新的副本集。用户可以通过将 abortScaleDownDelaySeconds 设置为 0 来选择永久保留新的副本集，或者将该值调整为更大或更小的值。\n下表总结了在 Rollout 策略和 abortScaleDownDelaySeconds 的组合下的行为。请注意，abortScaleDownDelaySeconds 不适用于 argo-rollouts v1.0。 abortScaleDownDelaySeconds = nil 是默认值，这意味着在 v1.1 中，对于所有 Rollout 策略，默认情况下在 Rollout 后 30 秒内缩小新的副本集。\n策略 v1.0 行为 abortScaleDownDelaySeconds v1.1 行为 蓝绿部署 不缩小 nil 回滚后 30 秒内缩小 蓝绿部署 不缩小 0 不缩小 蓝绿部署 不缩小 N 回滚后 N 秒内缩小 基本金丝雀 回滚到稳定状态 N/A 回滚到稳定状态 带流量路由的金丝雀 立即缩小 nil 回滚后 30 秒内缩小 带流量路由的金丝雀 立即缩小 0 不缩小 带流量路由的金丝雀  …","relpermalink":"/argo-rollouts/rollout/scaledown-aborted-rs/","summary":"在回滚更新时，我们可能会为所有策略缩小新的副本集。用户可以通过将 abortScaleDownDelaySeconds 设置为 0 来选择永久保留新的副本集，或者将该值调整为更大或更小的值。 下表总结了在 Rollout 策略和 abortScaleDownDelaySeconds 的组合下的行为。请注意，abortScaleD","title":"Rollout 失败时缩小新的 ReplicaSet"},{"content":"读者将了解到 SPIRE 部署的组成部分，有哪些部署模式，以及在部署 SPIRE 时需要考虑哪些性能和安全问题。\n你的 SPIRE 部署的设计应满足你的团队和组织的技术要求。它还应包括支持可用性、可靠性、安全性、可扩展性和性能的要求。该设计将作为你的部署活动的基础。\n身份命名方案 请记住，在前面的章节中，SPIFFE ID 是一个结构化的字符串，代表一个工作负载的身份名称，正如你在第四章中看到的那样。工作负载标识符部分（URI 的路径部分）附加在信任域名（URI 的主机部分）上，可以组成关于服务所有权的含义，以表示它在什么平台上运行，谁拥有它，它的预期目的，或其他惯例。它是特意为你定义的灵活和可定制的。\n你的命名方案可能是分层的，就像文件系统的路径。也就是说，为了减少歧义，命名方案不应该以尾部的正斜杠（/）结束。下面你将看到一些不同的样例，它们遵循三种不同的约定，你可以遵循，或者如果你感到特别有灵感，也可以想出你自己的。\n直接命名服务 你可能会发现，作为软件开发生命周期的一部分，直接通过它从应用角度呈现的功能和它运行的环境来识别一个服务是很有用的。例如，管理员可能会规定，在特定环境中运 …","relpermalink":"/spiffe/designing-a-spire-deployment/","summary":"读者将了解到 SPIRE 部署的组成部分，有哪些部署模式，以及在部署 SPIRE 时需要考虑哪些性能和安全问题。 你的 SPIRE 部署的设计应满足你的团队和组织的技术要求。它还应包括支持可用性、可靠性、安全性、可扩展性和性能的要求。该设","title":"设计一个 SPIRE 部署"},{"content":"在 Kubernetes 中管理 pod 时，Cilium 将为 Cilium 管理的每个 pod 创建一个 CiliumEndpoint（CEP）的自定义资源定义（CRD）。如果启用了 enable-cilium-endpoint-slice，那么 Cilium 还会创建一个 CiliumEndpointSlice （CES）类型的 CRD，将一组具有相同安全身份的 CEP 对象分组到一个 CES 对象中，并广播 CES 对象来向其他代理传递身份，而不是通过广播 CEP 来实现。在大多数情况下，这减少了控制平面上的负载，可以使用相同的主资源维持更大规模的集群。\n例如：\n$ kubectl get ciliumendpointslices --all-namespaces NAME AGE ces-548bnpgsf-56q9f 171m ces-dy4d8x6j2-qgc2z 171m ces-f6qfylrxh-84vxm 171m ces-k29rv92f5-qb4sw 171m ces-m9gs68csm-w2qg8 171m ","relpermalink":"/cilium-handbook/kubernetes/ciliumendpointslice/","summary":"在 Kubernetes 中管理 pod 时，Cilium 将为 Cilium 管理的每个 pod 创建一个 CiliumEndpoint（CEP）的自定义资源定义（CRD）。如果启用了 enable-cilium-endpoint-slice，那么 Cilium 还会创","title":"端点切片 CRD"},{"content":"我希望这个简短的报告能让你了解 eBPF 和它的强大之处。我真正希望的是，你已经准备好尝试一些基于 eBPF 的工具！如果你想在技术方面深入研究，可以从 ebpf.io 开始，在那里你会找到更多关于技术和 ebPF 基金会的信息。对于编码实例，可以在 GitHub 上的 ebpf-beginners 仓库里找到。\n为了了解其他人是如何利用 eBPF 工具的，请参加 eBPF Summit 和 Cloud Native eBPF Day 等活动，在这些活动中，用户分享他们的成功和学习经验。还有一个活跃的 Slack 频道 ebpf.io/slack。我希望能在那里见到你！\n","relpermalink":"/what-is-ebpf/conclusion/","summary":"我希望这个简短的报告能让你了解 eBPF 和它的强大之处。我真正希望的是，你已经准备好尝试一些基于 eBPF 的工具！如果你想在技术方面深入研究，可以从 ebpf.io 开始，在那里你会找到更多关于技术和 ebPF 基金会的信息。对于编码实例，可","title":"第七章：结论"},{"content":"容器运行时接口（Container Runtime Interface），简称 CRI。CRI 中定义了 容器 和 镜像 的服务的接口，因为容器运行时与镜像的生命周期是彼此隔离的，因此需要定义两个服务。该接口使用 Protocol Buffer，基于 gRPC，在 Kubernetes v1.10 + 版本中是在 pkg/kubelet/apis/cri/runtime/v1alpha2 的 api.proto 中定义的。\nCRI 架构 Container Runtime 实现了 CRI gRPC Server，包括 RuntimeService 和 ImageService。该 gRPC Server 需要监听本地的 Unix socket，而 kubelet 则作为 gRPC Client 运行。\n启用 CRI 除非集成了 rktnetes，否则 CRI 都是被默认启用了，从 Kubernetes 1.7 版本开始，旧的预集成的 docker CRI 已经被移除。\n要想启用 CRI 只需要在 kubelet 的启动参数重传入此参 …","relpermalink":"/kubernetes-handbook/architecture/open-interfaces/cri/","summary":"容器运行时接口（Container Runtime Interface），简称 CRI。CRI 中定义了 容器 和 镜像 的服务的接口，因为容器运行时与镜像的生命周期是彼此隔离的，因此需要定义两个服务。该接口使用 Protocol Buffer，","title":"容器运行时接口（CRI）"},{"content":"由于参考平台是由容器编排和资源管理平台以及服务网格软件组成的，以下出版物为确保该平台的安全提供了指导，并为本文件的内容提供了背景信息。\nSP800-204，基于微服务的应用系统的安全策略，讨论了基于微服务的应用的特点和安全要求，以及满足这些要求的总体策略。 SP800-204A，使用服务网格构建基于微服务的安全应用，为基于微服务的应用的各种安全服务（如建立安全会话、安全监控等）提供了部署指导，这些服务使用基于独立于应用代码运行的服务代理的专用基础设施（即服务网格）。 SP800-204B，使用服务网格的基于微服务的应用的基于属性的访问控制，为在服务网格中构建满足安全要求的认证和授权框架提供了部署指导，例如：（1）通过在任何一对服务之间的通信中实现相互认证来实现零信任；（2）基于访问控制模型，如基于属性的访问控制（ABAC）模型的强大访问控制机制，可用于表达广泛的策略集，并在用户群、对象（资源）和部署环境方面可扩展。 SP800-190，应用容器安全指南，解释了与容器技术相关的安全问题，并为在规划、实施和维护容器时解决这些问题提出了实用建议。这些建议是针对容器技术架构中的每个层级提供的。 …","relpermalink":"/service-mesh-devsecops/intro/relationship-to-other-nist-guidance-documents/","summary":"由于参考平台是由容器编排和资源管理平台以及服务网格软件组成的，以下出版物为确保该平台的安全提供了指导，并为本文件的内容提供了背景信息。 SP800-204，基于微服务的应用系统的安全策略，讨论了基于微服","title":"1.4 与其他 NIST 指导文件的关系"},{"content":"下一个常见问题涉及工作流模型。所有的 CI/CD 管道都可以有两种类型的工作流程模型，这取决于作为管道一部分部署的自动化工具。\n基于推的模式 基于拉的模式 在支持基于推模式的 CI/CD 工具中，在管道的一个阶段或阶段所做的改变会触发后续阶段或阶段的改变。例如，通过一系列的编码脚本，CI 系统中的新构建会触发管道中 CD 部分的变化，从而改变部署基础设施（如 Kubernetes 集群）。使用 CI 系统作为部署变化的基础，其安全方面的缺点是有可能将凭证暴露在部署环境之外，尽管已尽最大努力确保 CI 脚本的安全，因为 CI 脚本是在部署基础设施的信任域之外运行的。由于 CD 工具拥有生产系统的 key，基于推送的模式就变得不安全了。\n在基于拉的工作流程模型中，与部署环境有关的运维（例如 Kubernetes 运维、Flux、ArgoCD）一旦观察到有新镜像被推送到注册表，就会从环境内部拉动新镜像。新镜像被从注册表中拉出，部署清单被自动更新，新镜像被部署在环境（如集群）中。因此，实际的部署基础设施状态与 Git 部署库中声明性描述的状态实现了衔接。此外，部署环境凭证（例如集群凭证）不会暴 …","relpermalink":"/service-mesh-devsecops/implement/workflow-models-in-ci-cd-pipelines/","summary":"下一个常见问题涉及工作流模型。所有的 CI/CD 管道都可以有两种类型的工作流程模型，这取决于作为管道一部分部署的自动化工具。 基于推的模式 基于拉的模式 在支持基于推模式的 CI/CD 工具中，在管道的一个阶段或阶段所做的改变会","title":"4.7 CI/CD 管道中的工作流模型"},{"content":"Pod 是 Kubernetes 中最小的可部署单元，由一个或多个容器组成。Pod 通常是网络行为者在利用容器时的初始执行环境。出于这个原因，Pod 应该被加固，以使利用更加困难，并限制成功入侵的影响。\n图 3：有 sidecar 代理作为日志容器的 Pod 组件 “非 root”容器和“无 root”容器引擎 默认情况下，许多容器服务以有特权的 root 用户身份运行，应用程序在容器内以 root 用户身份执行，尽管不需要有特权的执行。\n通过使用非 root 容器或无 root 容器引擎来防止 root 执行，可以限制容器受损的影响。这两种方法都会对运行时环境产生重大影响，因此应该对应用程序进行全面测试，以确保兼容性。\n非 root 容器：容器引擎允许容器以非 root 用户和非 root 组成员身份运行应用程序。通常情况下，这种非默认设置是在构建容器镜像的时候配置的。附录 A：非 root 应用的 Dockerfile 示例 显示了一个 Dockerfile 示例，它以非 root 用户身份运行一个应用。\n非 root 用户。另外，Kubernetes …","relpermalink":"/kubernetes-hardening-guidance/kubernetes-pod-security/","summary":"Pod 是 Kubernetes 中最小的可部署单元，由一个或多个容器组成。Pod 通常是网络行为者在利用容器时的初始执行环境。出于这个原因，Pod 应该被加固，以使利用更加困难，并限制成功入侵的影响。 图 3：有 sidecar 代理作为日志容器的 Pod","title":"Kubernetes Pod 安全"},{"content":"在构建应用程序以管理基础架构时，我们要将需要公开的 API 与要创建的应用程序等量看待。这些 API 将代表您的基础架构的抽象，而应用程序将使用 API 消费这些这些基础架构。\n务必牢牢掌握两者的重要性，和如何利用它们来创建可扩展的弹性基础架构。\n在本章中，我们将举一个虚构的云原生应用程序和 API 示例，这些应用程序和 API 会经历正常的应用程序周期。如果您想了解更多有关管理云原生应用程序的信息，请参阅第 7 章。\n设计 API 这里的 API 是指处理数据结构中的基础架构表示，而不关心如何暴露或消费这些 API。通常使用 HTTP RESTful 端点来传递数据结构，API 如何实现对本章并不重要。\n随着基础架构的不断发展，运行在基础架构之上的应用程序也要随之演变。为这些应用程序的功能将随着时间而改变，因此基础架构是也是隐性地演变。随着基础架构的不断发展，管理它的应用程序也必须发展。\n基础架构的功能、需求和发展将永无止境。如果幸运的话，云供应商的 API 将会保持稳定，不会频繁更改。作为基础架构工程师，我们需要做好准备，以适应这些需求。我们需要准备好发展我们的基础架构和运行其上的 …","relpermalink":"/cloud-native-infra/developing-infrastructure-applications/","summary":"在构建应用程序以管理基础架构时，我们要将需要公开的 API 与要创建的应用程序等量看待。这些 API 将代表您的基础架构的抽象，而应用程序将使用 API 消费这些这些基础架构。 务必牢牢掌握两者的重要性，和如何利用它们来创建可","title":"第 5 章：开发基础架构应用程序"},{"content":"OpenTelemetry 被设计成允许长期稳定性和不确定性并存的局面。在 OpenTelemetry 中，稳定性保证是在每个信号的基础上提供的。与其看版本号，不如检查你想使用的信号的稳定性等级。\n信号生命周期 图 6-1 显示了新信号是如何被添加到 OpenTelemetry 的。实验性信号仍在开发中。它们可能在任何时候改变并破坏兼容性。实验性信号的开发是以规范提案的形式开始的，它是与一组原型一起开发的。一旦实验性信号准备好在生产中使用，信号的特性就会被冻结，新信号的测试版就会以多种语言创建。测试版可能不是完整的功能，它们可能会有一些突破性的变化，但它们被认为是为早期采用者的产品反馈做好准备。一旦一个信号被认为可以被宣布为稳定版本，就会发布一个候选版本。如果候选版本能够在一段时间内保持稳定，没有问题，那么该信号的规范和测试版都被宣布为稳定。\n一旦一个信号变得稳定，它就属于 OpenTelemetry 的长期支持保障范围。OpenTelemetry 非常重视向后兼容和无缝升级。详情见以下章节。\n如果 OpenTelemetry 信号的某个组件需要退役，该组件将被标记为废弃的。被废弃的组 …","relpermalink":"/opentelemetry-obervability/stability-and-long-term-support/","summary":"第 6 章：稳定和长期支持","title":"第 6 章：稳定和长期支持"},{"content":"有时候紧急 CL 必须尽快通过 code review 过程。\n什么是紧急情况？ 紧急 CL 是这样的小更新：允许主要发布继续而不是回滚，修复显著影响用户生产的错误，处理紧迫的法律问题，关闭主要安全漏洞等。\n在紧急情况下，我们确实关心 Code Review 的整体速度，而不仅仅是响应的速度。仅在这种情况下，审查人员应该更关心审查的速度和代码的正确性（是否解决了紧急情况？）。此外（显然）这类状况的审查应该优先于所有其他 code reivew。\n但是，在紧急情况解决后，您应该再次查看紧急 CL 并进行更彻底的审查。\n什么不是紧急情况？ 需要说明的是，以下情况并非紧急情况：\n想要在本周而不是下周推出（除非有一些实际硬性截止日期，例如合作伙伴协议）。 开发人员已经在很长一段时间内完成了一项功能想要获得 CL。 审查者都在另一个时区，目前是夜间或他们已离开现场。 现在是星期五，在开发者在过周末之前获得这个 CL 会很棒。 今天因为软（非硬）截止日期，经理表示必须完成此审核并签入 CL。 回滚导致测试失败或构建破坏的 CL。 等等。\n什么是 Hard Deadline？ 硬性截止日期（Hard …","relpermalink":"/eng-practices/review/emergencies/","summary":"有时候紧急 CL 必须尽快通过 code review 过程。 什么是紧急情况？ 紧急 CL 是这样的小更新：允许主要发布继续而不是回滚，修复显著影响用户生产的错误，处理紧迫的法律问题，关闭主要安全漏洞等。 在紧急情况下，我们确实关心 Code Review 的","title":"紧急情况"},{"content":"默认情况下，授权策略是使用服务器的工作负载端口来匹配流量的。但是在某些情况下，比如使用 curl 时使用 --haproxy-protocol，Envoy 代理会尝试在服务端口而不是工作负载端口上匹配传入的流量。本文档提供了一种允许用户执行此操作的方法。\n在开始之前，请确保你已经做了以下准备：\n熟悉 TSB 概念 安装 TSB 演示 环境 部署 Istio Bookinfo 示例应用程序 创建一个 Tenant 创建一个 Workspace 创建 Config Groups 配置 Permissions 配置 Ingress Gateway 应用 haproxy-protocol EnvoyFilter 在监听器上启用 haproxy-protocol。创建以下 haproxy-filter.yaml 文件：\napiVersion: networking.istio.io/v1alpha3 kind: EnvoyFilter metadata: name: proxy-protocol namespace: bookinfo spec: workloadSelector: …","relpermalink":"/tsb/howto/gateway/https-with-proxy-protocol/","summary":"默认情况下，授权策略是使用服务器的工作负载端口来匹配流量的。但是在某些情况下，比如使用 curl 时使用 --haproxy-protocol，Envoy 代理会尝试在服务端口而不是工作负载端口上匹配传入的流量。本","title":"配置 Authz 以支持代理协议"},{"content":"本文描述如何配置管理平面 IAM 服务以具有多个密钥来验证 JWT 令牌。当轮换 IAM 签名密钥但仍允许访问使用旧密钥签发且尚未过期的令牌时，这将非常有用。\n以下示例演示了从tsb-certs证书使用主 IAM 签名密钥迁移至使用自定义签名密钥的过程。\n获取当前签名密钥 首先，你需要使用以下命令检索带有令牌签发者配置的配置：\nkubectl -n tsb get managementplane managementplane 并找到token 签发者配置。 在此示例中，配置如下所示：\ntokenIssuer: jwt: expiration: 3600s issuers: - name: https://demo.tetrate.io signingKey: tls.key refreshExpiration: 2592000s signingKeysSecret: tsb-certs tokenPruneInterval: 3600s 这表示当前的签名密钥是tsb-certs秘密中的tls.key条目。我们可以检索它并保存以备将来使用：\nkubectl get secret -n …","relpermalink":"/tsb/operations/multiple-iam-keys/","summary":"本文描述如何配置管理平面 IAM 服务以具有多个密钥来验证 JWT 令牌。当轮换 IAM 签名密钥但仍允许访问使用旧密钥签发且尚未过期的令牌时，这将非常有用。 以下示例演示了从tsb-certs证书使用主 IAM 签名密钥迁移至使用自","title":"配置多个 IAM 令牌验证密钥"},{"content":"此功能允许通过 IngressGateway 或 Tier1Gateway 安装 CR 覆盖已注册集群的外部地址。然后将使用提供的 IP 地址/主机名从外部世界访问集群。请注意，此功能仅在你已经配置了其他 IP 地址/主机名以从外部世界访问你的 Kubernetes 集群时才有用。\n数据平面 要在 IngressGateway 中使用此功能，请在你的 IngressGateway 安装（数据平面）CR 中的 kubeSpec/service 下设置 xcp.tetrate.io/cluster-external-addresses 注释，并使用 kubectl 应用它。你可以使用：\n单个 IP 地址 单个 DNS 名称 多个 IP 地址（以逗号分隔） 但你不能配置多个 DNS 名称或将 IP 地址与 DNS 名称组合在一起。\napiVersion: install.tetrate.io/v1alpha1 kind: IngressGateway metadata: name: bookinfo namespace: bookinfo spec: kubeSpec: …","relpermalink":"/tsb/operations/features/configure-cluster-external-addresses/","summary":"此功能允许通过 IngressGateway 或 Tier1Gateway 安装 CR 覆盖已注册集群的外部地址。然后将使用提供的 IP 地址/主机名从外部世界访问集群。请注意，此功能仅在你已经配置了其他 IP 地址/主机名以从外部世界访问你的 Kubernetes 集群时才有用。 数据平面 要在 IngressGateway","title":"配置集群外部地址"},{"content":"Apache SkyWalking Cloud on Kubernetes (SWCK) 提供了一个外部度量适配器，从中 Kubernetes 水平 Pod 自动缩放（HPA） 控制器可以检索度量数据。用户可以将 SWCK 适配器部署到 TSB 控制平面中，以从 Observability Analysis Platform (OAP) 服务中获取目标度量数据。\n在开始之前，请确保你已经：\n熟悉 TSB 概念 安装了 TSB 演示 环境 部署了 Istio Bookinfo 示例应用程序 验证 SWCK 度量适配器 SWCK 适配器负责管理 OAP 组件等。\n该适配器应在安装 TSB 演示配置文件时已安装。要验证适配器是否成功部署，请检查相应的 Pod 是否已正确启动。\nkubectl get po -n istio-system ... \u0026lt;snip\u0026gt; ... istio-system-custom-metrics-apiserver-7cdbb5bdbb-zmwh7 1/1 Running 0 5m54s 如果由于某种原因以下资源未正确更新/生成，请尝试手动删除它们。删除它们应触发创 …","relpermalink":"/tsb/howto/hpa-using-skywalking/","summary":"Apache SkyWalking Cloud on Kubernetes (SWCK) 提供了一个外部度量适配器，从中 Kubernetes 水平 Pod 自动缩放（HPA） 控制器可以检索度量数据。用户可以将 SWCK 适配器部署到 TSB 控制平面中，以从 Observability Analysis Platform (OAP) 服务中获取目标度量数据。 在开始之前，请确保你已经： 熟悉 TSB 概","title":"使用 SkyWalking 进行 HPA"},{"content":"服务性能降级可能非常难以理解和隔离：\n数据太多，难以查找性能问题的原因 应用程序行为的专家（开发团队）通常无法访问运行中的集群 Tetrate Service Bridge 提供了一组工具，可以：\n允许 TSB Operator 从运行中的集群中检索服务性能数据的存档 允许应用程序开发人员查询此数据以识别最慢的事务（或带有错误的事务）并确定与慢响应相关的调用图。 在开始之前，请确保你已经：\n熟悉TSB 概念 安装TSB 演示环境 部署Istio Bookinfo示例应用程序 收集数据 TSB Operator 可以使用 tctl 收集集群状态。该状态包括来自工作负载的代理日志、Istio 控制平面信息、节点信息、istioctl analyze 和其他运行时信息。数据导出为一个 tar 文件。\nUsage: tctl collect [flags] Examples: # 收集数据，不进行任何模糊处理或删除 tctl collect # 收集数据但不存档结果（用于本地调试） tctl collect --disable-archive # …","relpermalink":"/tsb/troubleshooting/identify-underperforming-services/","summary":"服务性能降级可能非常难以理解和隔离： 数据太多，难以查找性能问题的原因 应用程序行为的专家（开发团队）通常无法访问运行中的集群 Tetrate Service Bridge 提供了一组工具，可以： 允许 TSB Operator 从运行中的集群中检索服务性能数据的存档 允许","title":"识别性能不佳的服务"},{"content":"在本部分中，你将检查 TSB 环境中 bookinfo 演示应用程序的服务拓扑和指标。\n先决条件 在继续之前，请确保你已完成以下任务：\n熟悉 TSB 概念。 安装 TSB 演示环境。 部署 Istio Bookinfo 示例应用程序。 创建租户和工作区。 创建配置组。 配置权限。 设置入口网关。 生成指标流量 在检查拓扑和指标之前，你需要为 bookinfo 应用程序生成流量以获得有意义的数据。使用提供的脚本生成流量：\n将以下脚本保存为 send-requests.sh ：\n#!/bin/bash while true; do curl -s https://bookinfo.tetrate.com/productpage \u0026gt; /dev/null sleep 1 done 使脚本可执行并运行它：\nchmod +x send-requests.sh ./send-requests.sh 该脚本每 1 秒向 bookinfo 产品页面发送一个请求，生成指标流量。\n查看拓扑和指标 现在，你可以在 TSB UI 中检查服务拓扑和指标。\n在左侧面板中的“租户”下，选择“仪表板”。 …","relpermalink":"/tsb/quickstart/observability/","summary":"在本部分中，你将检查 TSB 环境中 bookinfo 演示应用程序的服务拓扑和指标。 先决条件 在继续之前，请确保你已完成以下任务： 熟悉 TSB 概念。 安装 TSB 演示环境。 部署 Istio Bookinfo 示例应用程序。 创建租户和工作区。 创建配置组。 配置权限。 设置入","title":"拓扑和指标"},{"content":"当一个服务使用 Gateway 资源公开时，应用所有者可以配置一系列措施来保护他们的服务。Gateway 资源（TSB / TSE）支持以下功能：\n在集群或服务实例之间提供跨集群的 HTTP 支持，支持 TLS、路由、负载平衡和重定向 在集群或服务实例之间提供跨集群的 TCP 支持，支持 TLS、路由和负载平衡 使用 JWT 令牌进行身份验证 针对外部授权服务进行授权 基于客户端地址、HTTP 标头、路径和方法的每秒、每分钟或每小时的速率限制 使用 WASM 插件实现自定义流量管理功能 安全功能 从安全角度来看，两个有用的功能是：\n身份验证和授权 与其将身份验证方法构建到目标服务中，不如让 Ingress Gateway 根据 JWT 令牌的内容检查和授权流量。这简化了服务配置，集中了身份验证，确保与其他服务的一致性，并允许在没有任何应用程序更改的情况下采用不同的安全姿态（例如，开发、测试和生产）。\n速率限制 通过在 Ingress Gateway 中限制每秒/每分钟/每小时的请求，可以保护上游服务免受流量激增的影响。可以对资源的个体 ‘贪婪’ 使用者（例如爬取机器人和蜘蛛）进行限制， …","relpermalink":"/tsb/design-guides/app-onboarding/gateway-security/","summary":"当一个服务使用 Gateway 资源公开时，应用所有者可以配置一系列措施来保护他们的服务。Gateway 资源（TSB / TSE）支持以下功能： 在集群或服务实例之间提供跨集群的 HTTP 支持，支持 TLS、路由、负载平衡和重定向 在","title":"针对服务配置额外的安全措施"},{"content":"本文描述了 Tetrate Service Bridge (TSB) 在管理平面和控制平面的容量规划的保守指南。\n这些参数适用于生产环境的安装：如果你使用类似演示的环境，TSB 将以最小的资源运行。\n注意事项\n本文描述的资源配置指南非常保守。\n另请注意，本文描述的资源配置适用于 垂直 资源扩展。相同 TSB 组件的多个副本不会共享负载，因此你不能期望多个组件的合并资源产生相同的效果。TSB 组件的副本应仅用于实现高可用性。\n推荐的基线生产安装资源要求 对于一个具有 1 个注册集群和该集群内部部署的 1 个服务的 TSB 基线安装，建议如下资源。\n再次强调，下面描述的内存量非常保守。此外，vCPU 数量给出的实际性能会根据你的基础设施而波动。建议你在你的环境中验证结果。\n组件 vCPU 数量 内存 MiB TSB 服务器 (管理平面) 1 2 512 XCP Central 组件 2 2 128 XCP Edge 1 128 Front Envoy 1 50 IAM 1 128 TSB UI 1 256 OAP 4 5192 OTEL 收集器 2 1024 1 包括 Kubernetes …","relpermalink":"/tsb/setup/resource-planning/","summary":"本文描述了 Tetrate Service Bridge (TSB) 在管理平面和控制平面的容量规划的保守指南。 这些参数适用于生产环境的安装：如果你使用类似演示的环境，TSB 将以最小的资源运行。 注意事项 本文描述的资源配置指南非常保守。 另请注意，本文描述","title":"资源消耗与容量规划"},{"content":"🔔 重要提示：自 v1.4 起支持蓝绿部署和金丝雀部署。\n默认情况下，当重新应用旧 Rollout 操作时，控制器会像处理规范更改一样处理它，并执行完整的步骤列表，并执行分析。但有两个例外：\n控制器检测到正在返回到仍在其 scaleDownDelay 范围内且已缩放的蓝绿 ReplicaSet。 控制器检测到正在返回到金丝雀的“稳定”ReplicaSet，并且升级尚未完成。 通常，当期望行为是尽快回滚时，重新运行分析和步骤对于回滚操作来说是不可取的。为了帮助实现这一点，回滚窗口功能允许用户指示在窗口内提升到 ReplicaSet 时将跳过所有步骤。\n示例：\nspec: rollbackWindow: revisions: 3 revisionHistoryLimit: 5 假设有线性修订历史记录：1、2、3、4、5（当前）。从修订版本 5 回滚到 4 或 3 将落在窗口内，因此将快速跟踪。\n","relpermalink":"/argo-rollouts/rollout/rollback/","summary":"🔔 重要提示：自 v1.4 起支持蓝绿部署和金丝雀部署。 默认情况下，当重新应用旧 Rollout 操作时，控制器会像处理规范更改一样处理它，并执行完整的步骤列表，并执行分析。但有两个例外： 控制器检测到正在返回到仍在其 scaleDownDelay 范围内且已","title":"回滚窗口"},{"content":"本章探讨了 SPIFFE 和 SPIRE 如何与环境集成。\nSPIFFE 从一开始就被设计成可插拔和可扩展的，所以将 SPIFFE 和 SPIRE 与其他软件系统集成的话题是一个广泛的话题。一个特定的集成的架构超出了本书的范围。相反，本章意在捕捉一些可能的常见集成，以及一个高层次的概述，以及进行集成工作的策略。\n使软件能够使用 SVID 在考虑如何调整软件以使用 SVID 时，有许多选项可供选择。本节介绍了其中的几个选项，以及与之相关的注意事项。\n本地 SPIFFE 支持 这种方法需要修改现有的服务，以使它们能够感知 SPIFFE。当所需的修改最小，或者可以在跨应用服务使用的通用库或框架中引入时，它是首选。对于那些对延迟敏感的数据平面服务，或希望在应用层利用身份的服务，本地集成是最好的方法。SPIFFE 提供了一些库，如用于 Go 编程语言的 GO-SPIFFE 和用于 Java 编程语言的 JAVA-SPIFFE，它们有助于开发支持 SPIFFE 的工作负载。\n当用支持 SPIFFE 库的语言构建软件时，这通常是利用 SPIFFE 最直接的方式。上面提到的 Go 和 Java 库有使 …","relpermalink":"/spiffe/integrating-with-others/","summary":"本章探讨了 SPIFFE 和 SPIRE 如何与环境集成。 SPIFFE 从一开始就被设计成可插拔和可扩展的，所以将 SPIFFE 和 SPIRE 与其他软件系统集成的话题是一个广泛的话题。一个特定的集成的架构超出了本书的范围。相反，本章意在捕捉一些可能的常见集成，","title":"与其他系统集成"},{"content":"Cilium 与多个 Kubernetes API 组兼容。有些是废弃的或测试版的，可能只在 Kubernetes 的特定版本中可用。\n所有列出的 Kubernetes 版本都经过 e2e 测试，保证与 Cilium 兼容。本表中未列出的旧版 Kubernetes 不支持 Cilium。较新的 Kubernetes 版本，虽然没有列出，但将取决于 Kubernetes 提供的后向兼容性。\nKubernetes 版本 Kubernetes NetworkPolicy API CiliumNetworkPolicy 1.16, 1.17, 1.18, 1.19, 1.20, 1.21, 1.22, 1.23 networking.k8s.io/v1 cilium.io/v2 有一个 CRD Cilium 在 Kubernetes 中使用了一个网络策略的 CRD。这个 CRD 的模式验证可能会有变化，它可以验证 Cilium Clusterwide Network Policy（CCNP）或 Cilium Network Policy（CNP）的正确性。\nCRD 本身有一个注解， …","relpermalink":"/cilium-handbook/kubernetes/compatibility/","summary":"Cilium 与多个 Kubernetes API 组兼容。有些是废弃的或测试版的，可能只在 Kubernetes 的特定版本中可用。 所有列出的 Kubernetes 版本都经过 e2e 测试，保证与 Cilium 兼容。本表中未列出的旧版 Kubernetes 不支持 Cilium。较新的 Kubernetes 版本，虽然没有列出，但将取决于 Kubernetes 提供","title":"Kubernetes 兼容性"},{"content":"容器网络接口（Container Network Interface），简称 CNI，是 CNCF 旗下的一个项目，由一组用于配置 Linux 容器的网络接口的规范和库组成，同时还包含了一些插件。CNI 仅关心容器创建时的网络分配，和当容器被删除时释放网络资源。有关详情请查看 GitHub。\nKubernetes 源码的 vendor/github.com/containernetworking/cni/libcni 目录中已经包含了 CNI 的代码，也就是说 Kubernetes 中已经内置了 CNI。\n接口定义 CNI 的接口中包括以下几个方法：\ntype CNI interface { AddNetworkList (net *NetworkConfigList, rt *RuntimeConf) (types.Result, error) DelNetworkList (net *NetworkConfigList, rt *RuntimeConf) error AddNetwork (net *NetworkConfig, rt *RuntimeConf) …","relpermalink":"/kubernetes-handbook/architecture/open-interfaces/cni/","summary":"容器网络接口（Container Network Interface），简称 CNI，是 CNCF 旗下的一个项目，由一组用于配置 Linux 容器的网络接口的规范和库组成，同时还包含了一些插件。CNI 仅关心容器创建时的网络分配，和当容器被","title":"容器网络接口（CNI）"},{"content":"本文件的结构如下：\n第二章简要介绍了参考平台，为其提供了实施 DevSecOps 原语的指导。\n第三章介绍了 DevSecOps 的基本要素（即管道），设计和执行管道的方法，以及自动化在执行中的作用。\n第四章涵盖了管道的所有方面，包括（a）所有管道需要解决的共同问题，（b）对第 1.1 节中列出的参考平台中五种代码类型的管道的描述，以及（c）DevSecOps 在整个生命周期中对整个应用环境（有五种代码类型的参考平台，因此承载着 DevSecOps 的实施）的安全保证的好处，包括 持续授权操作（C-ATO）。\n第五章提供了摘要和结论。\n下一章 ","relpermalink":"/service-mesh-devsecops/intro/organization-of-this-document/","summary":"本文件的结构如下： 第二章简要介绍了参考平台，为其提供了实施 DevSecOps 原语的指导。 第三章介绍了 DevSecOps 的基本要素（即管道），设计和执行管道的方法，以及自动化在执行中的作用。 第四章涵盖了管道的所有方面，包括（a）所有管","title":"1.5 本文件的组织"},{"content":"最后一个常见的问题是安全测试。无论代码类型是什么（例如，应用服务、Iac、Pac 或可观测性），基于微服务的基础设施的 DevSecOps 的 CI/CD 管道与服务网格应包括由自动化工具或作为服务提供的应用安全测试（AST）。这些工具会分析和测试应用程序的安全漏洞。根据 Gartner 的说法，有 四种 主要的 AST 技术：\n静态 AST（SAST）工具：分析应用程序的源码、字节码或二进制代码的安全漏洞，通常在编程和 / 或测试软件生命周期（SLC）阶段。具体来说，这项技术涉及到在提交中查看应用程序并分析其依赖关系的 技术。如果任何依赖关系包含问题或已知的安全漏洞，提交将被标记为不安全的，不允许继续部署。这也可以包括在代码中找到应该被删除的硬编码密码 / 秘密。 动态 AST（DAST）工具：在测试或运行阶段，分析应用程序的动态运行状态。它们模拟针对应用程序（通常是支持网络的应用程序、服务和 API）的攻击，分析应用程序的反应，并确定它是否有漏洞。特别是，DAST 工具比 SAST 更进一步，在 CI 工作中启动生产环境的副本，以扫描所产生的容器和 可执行文件。动态方面有助于系统捕 …","relpermalink":"/service-mesh-devsecops/implement/security-testing-common-requirement-for-ci-cd-pipelines-for-all-code-types/","summary":"最后一个常见的问题是安全测试。无论代码类型是什么（例如，应用服务、Iac、Pac 或可观测性），基于微服务的基础设施的 DevSecOps 的 CI/CD 管道与服务网格应包括由自动化工具或作为服务提供的应用安全测试（AST）。这些工","title":"4.8 安全测试——所有代码类型的 CI/CD 管道的共同要求"},{"content":"基础架构是用来支撑应用程序的。可信任的软件对于工程成功至关重要。如果每次在终端输入 ls 命令，都会发生随机动作，那么你将永远也不会相信 ls，而是去找另一种方式来列出目录中的文件。\n我们的基础架构必须值得信任。本章旨在建立信任和验证基础架构的意识形态。我们将描述的实践旨在增加对应用程序和基础架构工程的信心。\n软件测试在当今的软件工程领域非常普遍。然而，如何测试基础架构还没有很明确的最佳实践。\n这意味着本书中的所有章节中，这一节应该是最令人兴奋的！在该领域像您这样的工程师有充分的空间发挥出色的影响力。\n软件测试是一种证明软件可以正常工作的有效做法，证明软件在各种特殊情况下软件仍然可以正常运行。因此，如果我们将相同的范例应用于基础架构测试，测试目标如下：\n证明基础架构按预期运行。 证明基础架构不会失败。 证明这两种情况在各种边缘情况下都是正确的。 衡量基础架构是否有效需要我们先定义什么叫有效。现在，您应该对使用基础架构和工程代表应用程序的想法感到满意。\n定义基础架构 API 的人应该花时间构思一个可以创建有效基础架构的理智的 API。例如，如果创建了一个定义虚拟机的 API 但里面没有使 …","relpermalink":"/cloud-native-infra/testing-cloud-native-infrastructure/","summary":"基础架构是用来支撑应用程序的。可信任的软件对于工程成功至关重要。如果每次在终端输入 ls 命令，都会发生随机动作，那么你将永远也不会相信 ls，而是去找另一种方式来列出目录中的文件。 我们的基础架构必须值得信任","title":"第 6 章：测试云原生基础架构"},{"content":"集群网络是 Kubernetes 的一个核心概念。容器、Pod、服务和外部服务之间的通信必须被考虑在内。默认情况下，很少有网络策略来隔离资源，防止集群被破坏时的横向移动或升级。资源隔离和加密是限制网络行为者在集群内转移和升级的有效方法。\n关键点\n使用网络策略和防火墙来隔离资源。 确保控制平面的安全。 对流量和敏感数据（例如 Secret）进行静态加密。 命名空间 Kubernetes 命名空间是在同一集群内的多个个人、团队或应用程序之间划分集群资源的一种方式。默认情况下，命名空间不会被自动隔离。然而，命名空间确实为一个范围分配了一个标签，这可以用来通过 RBAC 和网络策略指定授权规则。除了网络隔离之外，策略可以限制存储和计算资源，以便在命名空间层面上对 Pod 进行更好的控制。\n默认有三个命名空间，它们不能被删除：\nkube-system（用于 Kubernetes 组件） kube-public（用于公共资源） default（针对用户资源） 用户 Pod 不应该放在 kube-system 或 kube-public 中，因为这些都是为集群服务保留的。可以用 YAML 文件，如  …","relpermalink":"/kubernetes-hardening-guidance/network-separation-and-hardening/","summary":"集群网络是 Kubernetes 的一个核心概念。容器、Pod、服务和外部服务之间的通信必须被考虑在内。默认情况下，很少有网络策略来隔离资源，防止集群被破坏时的横向移动或升级。资源隔离和加密是限制网络行为者在集群内转移和升","title":"网络隔离和加固"},{"content":"现在我们了解了组成 OpenTelemetry 的各个构件，我们应该如何将它们组合成一个强大的生产管道？\n答案取决于你的出发点是什么。OpenTelemetry 是模块化的，设计成可以在各种不同的规模下工作。你只需要使用相关的部分。这就是说，我们已经创建了一个建议的路线图供你遵循。\n安装 OpenTelemetry 客户端 可以单独使用 OpenTelemetry 客户端而不部署收集器。这种基本设置通常是绿地部署的充分起点，无论是测试还是初始生产。OpenTelemetry SDK 可以被配置为直接向大多数可观测性服务传输遥测数据。\n挑选一个导出器 默认情况下，OpenTelemetry 使用 OTLP 导出数据。该 SDK 提供了几种常见格式的导出器。Zipkin、Prometheus、StatsD 等。如果你使用的可观测性后端没有原生支持 OTLP，那么这些其他格式中的一种很可能会被支持。安装正确的导出器并将数据直接发送到你的后端系统。\n安装库仪表 除了 SDK，OpenTelemetry 仪表必须安装在所有 HTTP 客户端、Web 框架、数据库和应用程序的消息队列中。如果这些库 …","relpermalink":"/opentelemetry-obervability/suggested-setups-and-telemetry-pipelines/","summary":"第 7 章：建议的设置和遥测管道","title":"第 7 章：建议的设置和遥测管道"},{"content":"本页面将详细解释 TSB 组件和你必须提供和连接以运行 TSB 的外部依赖项。\n在继续之前，请确保你已经：\n查看了 TSB 架构 并理解了 TSB 的四个层次：数据平面（Envoy 代理）、本地控制平面（Istio）、全局控制平面（XCP）和管理平面（TSB 本身）。\n管理平面 下图显示了管理平面（MP）组件：\nTSB 管理平面组件 front-envoy 端口\n默认 Envoy 网关（或前置 Envoy）端口是 8443，可以由用户配置（例如更改为 443）。如果更改了默认端口，则通过前置 Envoy 进行通信的组件需要相应地进行调整以匹配用户定义的值。 front-envoy 作为 Elasticsearch 代理\nTSB 的前置 Envoy 可以作为配置在 ManagementPlane CR 中的 Elasticsearch 的代理。 要使用此功能，请在 ControlPlane CR 中将 Elastic 主机和端口设置为 TSB 前置 Envoy 主机和端口。然后，来自控制平面 OAP 到 Elasticsearch 的流量将通过前置 Envoy 进行传递。 在管理平面中有 …","relpermalink":"/tsb/setup/components/","summary":"本页面将详细解释 TSB 组件和你必须提供和连接以运行 TSB 的外部依赖项。 在继续之前，请确保你已经： 查看了 TSB 架构 并理解了 TSB 的四个层次：数据平面（Envoy 代理）、本地控制平面（Istio）、全局控制平面（XCP）","title":"TSB 组件"},{"content":"本文描述了在将 PostgreSQL 用作 TSB 数据存储时如何创建备份以及如何使用备份进行恢复。 建议每 24 小时创建一次 TSB 数据存储的备份，以便在发生损坏时可以轻松恢复所有信息。\n在开始之前，请确保：\n你已经安装和配置了 TSB。 你已经安装并配置了 kubectl 以访问管理集群。 你对存储 TSB 数据的 PostgreSQL 系统具有完全访问权限。 创建 TSB 配置的备份 TSB 需要 PostgreSQL 11.1 或更高版本。我们将在示例中使用 11.1 版本。你可以通过运行以下命令创建数据库备份：\npg_dump tsb \u0026gt; tsb_backup.sql 注意\n确保备份文件包含完整的信息。在文件的末尾，应该有如下的完成消息：\n-- -- PostgreSQL database dump complete -- 备份日志的大小可以通过删除审计日志来减小（请确保你有一个快照以符合你组织的合规性规则）。要进行审计日志截断，你可以使用以下命令（请根据要保留的日志的时间间隔进行调整，以下示例中为2day）：\nDELETE FROM audit_log WHERE …","relpermalink":"/tsb/operations/postgresql/","summary":"本文描述了在将 PostgreSQL 用作 TSB 数据存储时如何创建备份以及如何使用备份进行恢复。 建议每 24 小时创建一次 TSB 数据存储的备份，以便在发生损坏时可以轻松恢复所有信息。 在开始之前，请确保： 你已经安装和配置了 TSB。 你已经安","title":"备份和恢复 PostgreSQL"},{"content":"本文档解释了将新的控制平面引入 TSB 时最常见的问题。\n连接性 部署 tsb-operator-control-plane 需要与管理平面 URL 具有连接性。通信是通过 tsb 命名空间中的 front-envoy 组件执行的，由 envoy 服务提供。\n请确保控制平面可以访问它，且没有被网络策略、安全组或任何防火墙阻止。\n故障排除 一旦你已经应用了必要的机密，安装了控制平面 Operator，并创建了控制平面 CR，如果存在一些配置错误，一些 Pod 可能无法启动。始终检查 tsb-operator-control-plane 的日志，因为它将提供有关可能出错的详细信息。\n服务帐号问题 如果未创建用于生成令牌的服务帐号，你将收到以下错误：\nerror\tcontrolplane\ttoken rotation failed, retrying in 15m0s: secret istio-system/cluster-service-account not found: Secret \u0026#34;cluster-service-account\u0026#34; not found …","relpermalink":"/tsb/troubleshooting/cluster-onboarding/","summary":"本文档解释了将新的控制平面引入 TSB 时最常见的问题。 连接性 部署 tsb-operator-control-plane 需要与管理平面 URL 具有连接性。通信是通过 tsb 命名空间中的 front-envoy 组件执行的，由 envoy 服务提供。 请确保控制平面可以访问它，且没有被网络策略、安全组或任何防火","title":"集群载入故障排除"},{"content":"TSB 服务账号可以在平台内部用于管理集群载入 tctl install cluster-service-account 和 GitOps 功能，也可以在外部用于第三方系统执行各种 TSB 功能的配置，利用 TSB API 接口。本文将重点介绍如何创建和使用 TSB 服务账号，以及如何利用 tctl 实用工具作为处理程序。\n使用 tctl 实用工具处理 TSB 服务账号 对于服务账号，你所需的大多数交互已经在 tctl experimental service-account 命令中可用：\n$ tctl x sa -h Commands to manage TSB service accounts Usage: tctl experimental service-account [command] Aliases: service-account, sa Available Commands: get Get one or multiple service accounts create Creates a new service account delete Deletes a …","relpermalink":"/tsb/howto/service-accounts/","summary":"TSB 服务账号可以在平台内部用于管理集群载入 tctl install cluster-service-account 和 GitOps 功能，也可以在外部用于第三方系统执行各种 TSB 功能的配置，利用 TSB API 接口。本文将重点介绍如何创建和使用 TSB 服务账号，以及如何利用 tctl 实用工具作为处理程序。 使用 tctl","title":"利用 TSB 服务账号"},{"content":"在此场景中，你将学习如何使用服务路由在演示应用程序中的不同版本的评论服务之间转移流量。\n先决条件 在继续之前，请确保你已完成以下任务：\n熟悉 TSB 概念。 安装 TSB 演示环境。 部署 Istio Bookinfo 示例应用程序。 创建租户、工作区、配置组、权限、入口网关，并检查服务拓扑和指标。 仅提供 v1 服务 使用用户界面 在左侧面板的“租户”下，选择“工作区”。 在 bookinfo-ws 工作区卡上，单击“流量组”。 单击你之前创建的 bookinfo-traffic 流量组。 选择流量设置选项卡。 在流量设置下，单击服务路由。 单击“添加新…”以使用默认名称 default-serviceroute 创建新的服务路由。 将其重命名为 bookinfo-traffic-reviews 。 将服务设置为 bookinfo/reviews.bookinfo.svc.cluster.local 。 展开 bookinfo-traffic-reviews。 展开子集。 单击“添加新子集…”以创建一个名为 subset-0 的新子集。 点击 subset-0 ： 将名称设置为 v1 …","relpermalink":"/tsb/quickstart/traffic-shifting/","summary":"在此场景中，你将学习如何使用服务路由在演示应用程序中的不同版本的评论服务之间转移流量。 先决条件 在继续之前，请确保你已完成以下任务： 熟悉 TSB 概念。 安装 TSB 演示环境。 部署 Istio Bookinfo 示例应用程序。 创建租户、工作区、配","title":"流量转移"},{"content":"在这个操作指南中，你将使用 Keycloak 作为身份提供者，在 Ingress Gateway 中添加用户身份验证和授权。\n在开始之前，请确保你已经：\n安装了 TSB 管理平面 载入了一个 集群 安装了启用了 HTTPS 的 Keycloak 注意\n本示例将使用经过 GKE 测试的 httpbin 应用程序的演示。如果你打算用于生产环境，请确保在相关字段中更新应用程序信息以适应你的情况。 在本指南中，你将：\n为演示的 httpbin 应用程序的 Ingress Gateway 添加身份验证和授权。 定义两个角色和两个用户：一个 admin 用户（称为 Jack），可以执行所有操作，以及一个 normal 用户（Sally），只能执行 GET /status 操作。 配置你的 Ingress Gateway，允许 admin 角色的用户访问所有内容，只允许 normal 角色的用户访问 GET /status。 什么是 OpenID 提供者？ OpenID 提供者是一个 OAuth 2.0 授权服务器，提供身份验证作为一项服务。它确保终端用户已经进行了身份验证，并提供了关于终端用户和身 …","relpermalink":"/tsb/howto/gateway/end-user-auth-keycloak/","summary":"在这个操作指南中，你将使用 Keycloak 作为身份提供者，在 Ingress Gateway 中添加用户身份验证和授权。 在开始之前，请确保你已经： 安装了 TSB 管理平面 载入了一个 集群 安装了启用了 HTTPS 的 Keycloak 注意 本示例将使用经过 GKE 测试的 httpbin 应用程序的演示。如果","title":"使用 Keycloak 进行终端用户身份验证"},{"content":"Tetrate 概念 组织（Organization） ‘组织’ 是对象层次结构的根。组织包含了租户、用户、团队、集群和工作区的层次结构。组织还是定义了 TSB 全局设置的位置。\n配置组织\n在 TSB 中，你可以定义根组织的名称和属性，以反映你的企业层次结构。\n在 TSE 中，组织名称是硬编码为 tse，不可编辑。\n组织用户（Organization Users） TSB 和 TSE 中的组织与身份提供者（IdP）关联。用户和团队代表组织结构，定期从你提供的 IdP 同步，以便进行访问策略配置。\n用户、团队和角色\nTSB 允许你定义多个团队，并关联具有精细化角色的团队，然后将用户映射到团队成员。\nTSE 使用默认的 tse-admin 用户，以及单个团队和角色。其他用户可以从 IdP 同步，它们都继承了单个团队成员和角色。\n租户（Tenant） 租户是服务桥对象层次结构中组织内的一个自包含实体。租户可以是业务单元、组织单元或与公司结构匹配的任何逻辑分组。\nTSE 简化\n而 TSB 支持多个租户和丰富的组织层次结构，TSE 提供了一个单一的 tse 租户和扁平的层次结构。 工作 …","relpermalink":"/tsb/concepts/glossary/","summary":"Tetrate 概念 组织（Organization） ‘组织’ 是对象层次结构的根。组织包含了租户、用户、团队、集群和工作区的层次结构。组织还是定义了 TSB 全局设置的位置。 配置组织 在 TSB 中，你可以定义","title":"Tetrate 术语表与定义"},{"content":"我删除/损坏了我的存储库，无法删除我的应用程序。 如果 Argo CD 无法生成清单，则无法删除应用程序。你需要：\n恢复/修复你的存储库。 使用cascade=false删除应用程序，然后手动删除资源。 为什么我的应用程序在成功同步后仍然处于 OutOfSync 状态？ 请查看 差异比较 文档，了解资源可能处于 OutOfSync 状态的原因，以及配置 Argo CD 忽略字段的方法 当存在不同之处时。\n为什么我的应用程序一直处于“Progressing”状态？ Argo CD 为几种标准 Kubernetes 类型提供健康状态。 Ingress、StatefulSet 和 SealedSecret 类型存在已知问题，可能会导致健康检查返回 Progressing 状态而不是 Healthy。\nIngress 如果 status.loadBalancer.ingress 列表是非空的，并且至少有一个值为 hostname 或 IP，则被视为健康状态。一些 Ingress 控制器（contour、traefik）不会更新 status.loadBalancer.ingress 字段，这会 …","relpermalink":"/argo-cd/faq/","summary":"我删除/损坏了我的存储库，无法删除我的应用程序。 如果 Argo CD 无法生成清单，则无法删除应用程序。你需要： 恢复/修复你的存储库。 使用cascade=false删除应用程序，然后手动删除资源。 为什么我的应用程序","title":"FAQ"},{"content":"你可以使用Traefik Proxy来进行 Traffic Management with Argo Rollouts。\nTraefikService是支持Traefik 作为 ingress 时加权轮询负载均衡和Traefik 作为 ingress 时流量镜像能力的对象。\n如何将 TraefikService 与 Argo Rollouts 集成，作为加权轮询负载均衡器 首先，我们需要使用 TraefikService 对象的加权轮询负载平衡能力创建 TraefikService 对象。\napiVersion: traefik.containo.us/v1alpha1 kind: TraefikService metadata: name: traefik-service spec: weighted: services: - name: stable-rollout # 为稳定应用程序版本创建的 k8s 服务名称 port: 80 - name: canary-rollout # 为新应用程序版本创建的 k8s 服务名称 port: 80 请注意，我们不指定“weight”字段。它 …","relpermalink":"/argo-rollouts/traffic-management/traefik/","summary":"你可以使用Traefik Proxy来进行 Traffic Management with Argo Rollouts。 TraefikService是支持Traefik 作为 ingress 时加权轮询负载均衡和Traefik 作为 ingress 时流量镜像能力的对象。 如何将 TraefikService 与 Argo Rollouts 集成","title":"Traefik"},{"content":"背景 根据集群的配置，蓝绿部署（或使用流量管理的金丝雀部署）可能会导致新创建的 Pod 在部署新版本后重新启动。这可能会导致问题，特别是对于无法快速启动或无法正常退出的应用程序。\n这种行为发生的原因是集群自动缩放器想要缩小创建的额外容量以支持运行在双倍容量中的部署。当节点缩小时，它所拥有的 Pod 会被删除并重新创建。这通常发生在部署具有自己的专用实例组时，因为部署对集群自动缩放器的影响更大。因此，具有大量共享节点的集群较少经历这种行为。\n例如，此处有一个正在运行的部署，它有 8 个 Pod 分布在 2 个节点上。每个节点最多可容纳 6 个 Pod：\n原 Rollout 正在运行，跨越两个节点 当部署的 spec.template 发生变化时，控制器会创建一个新的 ReplicaSet，其中包含规范更新和 Pod 总数翻倍的版本。在这种情况下，Pod 的数量增加到 16。\n由于每个节点只能容纳 6 个 Pod，所以集群自动缩放器必须将节点数增加到 3 个来容纳所有 16 个 Pod。Pod 在节点之间的分布如下所示：\nRollout 容量扩大到两倍 部署完成后，旧版本会被缩小。这会使集 …","relpermalink":"/argo-rollouts/rollout/anti-affinity/","summary":"背景 根据集群的配置，蓝绿部署（或使用流量管理的金丝雀部署）可能会导致新创建的 Pod 在部署新版本后重新启动。这可能会导致问题，特别是对于无法快速启动或无法正常退出的应用程序。 这种行为发生的原因是集群自动缩放","title":"反亲和性"},{"content":"本章解释了如何实施使用 SPIFFE 身份的授权策略。\n在 SPIFFE 的基础上建立授权 SPIFFE 专注于软件安全加密身份的发布和互操作性，但正如本书前面提到的，它并不直接解决这些身份的使用或消费问题。\nSPIFFE 经常作为一个强大的授权系统的基石，而 SPIFFE ID 本身在这个故事中扮演着重要角色。在这一节中，我们将讨论使用 SPIFFE 来建立授权的选择。\n认证与授权（AuthN Vs AuthZ） 一旦一个工作负载有了安全的加密身份，它就可以向其他服务证明其身份。向外部服务证明身份被称为认证（Authentication）。一旦通过认证，该服务就可以选择允许哪些行动。这个过程被称为授权（Authorization）。\n在一些系统中，任何被认证的实体也被授权。因为 SPIFFE 会在服务启动时自动授予其身份，所以清楚地认识到并不是每一个能够验证自己的实体都应该被授权，这一点至关重要。\n授权类型 有很多方法可以对授权进行建模。最简单的解决方案是在每个资源上附加一个授权身份的允许列表（allowlist）。然而，随着我们的探索，我们会注意到在处理生态系统的规模和复杂性时，允 …","relpermalink":"/spiffe/using-spiffe-identities-to-inform-authorization/","summary":"本章解释了如何实施使用 SPIFFE 身份的授权策略。 在 SPIFFE 的基础上建立授权 SPIFFE 专注于软件安全加密身份的发布和互操作性，但正如本书前面提到的，它并不直接解决这些身份的使用或消费问题。 SPIFFE 经常作为一个强大的授权系统的基石，","title":"使用 SPIFFE 身份通知授权"},{"content":"验证安装 检查 DaemonSet 的状态并验证所有需要的实例都处于 ready 状态：\n$ kubectl --namespace kube-system get ds NAME DESIRED CURRENT READY NODE-SELECTOR AGE cilium 1 1 0 \u0026lt;none\u0026gt; 3s 在此示例中，我们看到 1 个期望的状态，0 个 ready 状态。这表明有问题。下一步是通过在 k8s-app=cilium 标签上匹配列出所有 cilium pod，并根据每个 pod 的重启次数对列表进行排序，以便轻松识别失败的 pod：\n$ kubectl --namespace kube-system get pods --selector k8s-app=cilium \\ --sort-by=\u0026#39;.status.containerStatuses[0].restartCount\u0026#39; NAME READY STATUS RESTARTS AGE cilium-813gf 0/1 CrashLoopBackOff 2 44s cilium-813gf pod …","relpermalink":"/cilium-handbook/kubernetes/troubleshooting/","summary":"验证安装 检查 DaemonSet 的状态并验证所有需要的实例都处于 ready 状态： $ kubectl --namespace kube-system get ds NAME DESIRED CURRENT READY NODE-SELECTOR AGE cilium 1 1 0 \u003cnone\u003e 3s 在此示例中，我们看到 1 个期望的状态，0 个 ready 状态。这表明有问题。下一步是通过在 k8s-app=cilium 标签上匹配列出所有 cilium pod，并根","title":"故障排除"},{"content":"容器存储接口（Container Storage Interface），简称 CSI，CSI 试图建立一个行业标准接口的规范，借助 CSI 容器编排系统（CO）可以将任意存储系统暴露给自己的容器工作负载。有关详细信息，请查看设计方案。\ncsi 卷类型是一种 out-tree（即跟其它存储插件在同一个代码路径下，随 Kubernetes 的代码同时编译的）的 CSI 卷插件，用于 Pod 与在同一节点上运行的外部 CSI 卷驱动程序交互。部署 CSI 兼容卷驱动后，用户可以使用 csi 作为卷类型来挂载驱动提供的存储。\nCSI 持久化卷支持是在 Kubernetes v1.9 中引入的，作为一个 alpha 特性，必须由集群管理员明确启用。换句话说，集群管理员需要在 apiserver、controller-manager 和 kubelet 组件的“--feature-gates =”标志中加上“CSIPersistentVolume = true”。\nCSI 持久化卷具有以下字段可供用户指定：\ndriver：一个字符串值，指定要使用的卷驱动程序的名称。必须少于 63 个字符，并以一个 …","relpermalink":"/kubernetes-handbook/architecture/open-interfaces/csi/","summary":"容器存储接口（Container Storage Interface），简称 CSI，CSI 试图建立一个行业标准接口的规范，借助 CSI 容器编排系统（CO）可以将任意存储系统暴露给自己的容器工作负载。有关详细信息，请查看设计","title":"容器存储接口（CSI）"},{"content":"DevSecOps 的好处包括：\n各个 IT 团队之间，特别是开发人员、运维和安全团队以及其他利益相关者之间更好的沟通和协作。导致 更好的生产力。 简化软件开发、交付和部署过程 —— 由于自动化，停机时间减少，发布时间加快，基础设施和运维成本降低，效率提高。 通过实施零信任来减少攻击面，这也限制了横向移动，从而防止攻击升级。具有现代行为预防能力的持续监控进一步促进了这一点。 安全优势。通过对每个请求的验证监控、警报和反馈机制来提高安全性，因为可观测性是代码。这些将在以下段落中详细描述。具体的能力包括： a. 运行时：杀死恶意容器。 b. 反馈：由于一个错误的程序更新了代码并重新触发了管道，所以反馈到了正确的存储库。 c. 监测新的和终止的服务，并调整相关服务（如服务代理）。 d. 启用安全断言。不可绕过 —— 通过在同一空间执行的代理、安全会话、强大的认证和授权以及安全的状态转换。 启用持续授权操作（C-ATO），在本节末尾详细描述。 对每个请求的验证和上述的反馈机制将在下面进一步描述： 每个请求的验证。来自用户或客户端应用程序（服务）的每个请求都要经过验证和授权（使用 OPA 或任何 …","relpermalink":"/service-mesh-devsecops/implement/benefits-of-devsecops-primitives-to-application-security-in-the-service-mesh/","summary":"DevSecOps 的好处包括： 各个 IT 团队之间，特别是开发人员、运维和安全团队以及其他利益相关者之间更好的沟通和协作。导致 更好的生产力。 简化软件开发、交付和部署过程 —— 由于自动化，停机时间减少，发布时间加快，基础设施和","title":"4.9 DevSecOps 原语对服务网格中应用安全的好处"},{"content":"云原生应用程序依赖基础架构才能运行，反过来说，云原生基础架构也需要云原生应用程序来维持。\n对于传统基础架构，维护和升级基本上都是由人工完成。可能是在单机上手动运行服务或使用自动化工具定义基础结构和应用程序的快照。\n但是，如果基础架构可以由应用程序管理，同时基础架构又可以管理应用程序，那么基础架构工具就会成为另一种应用程序。工程师对于基础架构的责任可以用调解器模式表示，内置于该基础架构上运行的应用程序中。\n我们花了三章来说明如何构建可以管理基础架构的应用程序。本章将介绍如何在基础架构上运行云原生应用或其它任何应用。\n如前所述，保持基础架构和应用程序的简单非常重要。解决应用程序复杂性最常用的方法就是把应用程序分解成小的，易于理解的组件。通常通过创建单一职责的服务来实现，或者将代码分解为一系列事件触发的函数。\n随着小型、可部署单元的扩容成多份即使是最自动化的基础架构也可能被压垮。管理大量应用程序的唯一方法是让它们承担第 1 章中所述的功能性操作。应用程序需要在可以按规模管理之前变成原生云。\n学习完本章不会帮助您建下一个伟大的应用程序，但您将了解一些基础知识，这块可以让您的应用程序在云原生基础 …","relpermalink":"/cloud-native-infra/managing-cloud-native-applications/","summary":"云原生应用程序依赖基础架构才能运行，反过来说，云原生基础架构也需要云原生应用程序来维持。 对于传统基础架构，维护和升级基本上都是由人工完成。可能是在单机上手动运行服务或使用自动化工具定义基础结构和应用程","title":"第 7 章：管理云原生应用程序"},{"content":"认证和授权是限制访问集群资源的主要机制。如果集群配置错误，网络行为者可以扫描知名的 Kubernetes 端口，访问集群的数据库或进行 API 调用，而不需要经过认证。用户认证不是 Kubernetes 的一个内置功能。然而，有几种方法可以让管理员在集群中添加认证。\n认证 管理员必须向集群添加一个认证方法，以实现认证和授权机制。\nKubernetes 集群有两种类型的用户：服务账户和普通用户账户。服务账户代表 Pod 处理 API 请求。认证通常由 Kubernetes 通过 ServiceAccount Admission Controller 使用承载令牌自动管理。不记名令牌被安装到 Pod 中约定俗成的位置，如果令牌不安全，可能会在集群外使用。正因为如此，对 Pod Secret 的访问应该限制在那些需要使用 Kubernetes RBAC 查看的人身上。对于普通用户和管理员账户，没有自动的用户认证方法。管理员必须在集群中添加一个认证方法，以实现认证和授权机制。\nKubernetes 假设由一个独立于集群的服务来管理用户认证。Kubernetes 文档中列出了几种实现用户认证的方 …","relpermalink":"/kubernetes-hardening-guidance/authentication-and-authorization/","summary":"认证和授权是限制访问集群资源的主要机制。如果集群配置错误，网络行为者可以扫描知名的 Kubernetes 端口，访问集群的数据库或进行 API 调用，而不需要经过认证。用户认证不是 Kubernetes 的一个内置功能。然而，有几种方法可以让管理员在集","title":"认证和授权"},{"content":"上线一个新的遥测系统可能是一项复杂的工作。它需要整个工程组织的支持，不能一蹴而就。不要低估这可能会产生的问题！\n在大型组织中，通常有许多服务团队负责系统的不同部分。通常情况下，每个团队都需要付出一定的努力来使他们所管理的服务得到充分的工具化。而这些团队都有自己积压的工作，他们当然希望能够优先处理这些工作。\n不幸的是，可观测性计划在开始提供价值和证明其价值之前就会耗尽人的耐心。但通过仔细的计划和协调，这种情况是可以避免的。\n主要目标 在推广 OpenTelemetry 时，重要的是要记住，任何基于分布式追踪的可观测性系统都需要对参与事务的每个服务进行检测，以提供最大价值。如果只有部分服务被检测到，那么追踪就会被分割成小的、不相连的部分。\n这种散乱的仪表的结果是不可取的。这种情况——不一致的仪表和断裂的追踪是你想要避免的主要事情。如果追踪是断开的，运维人员仍然需要在他们的头脑中把所有的东西拼凑起来，以获得他们系统的情况。更糟糕的是，自动分析工具可以使用的数据非常有限。与人类运维人员不同，他们可以运用直觉，跳出框框来思考问题，而分析工具却只能使用他们得到的数据。由于数据有限，他们提供有用的见 …","relpermalink":"/opentelemetry-obervability/roll-out/","summary":"第 8 章：如何在组织中推广 OpenTelemetry","title":"第 8 章：如何在组织中推广 OpenTelemetry"},{"content":"在此场景中，你将了解如何使用 TSB 安全设置来限制来自工作区外部的访问。这有助于通过控制服务之间的通信来增强环境的安全性。\n先决条件 在继续之前，请确保你已完成以下任务：\n熟悉 TSB 概念。 安装 TSB 演示环境。 部署 Istio Bookinfo 示例应用程序。 创建租户、工作区和配置组。 为团队和用户配置权限。 设置入口网关。 使用可观测性工具检查服务拓扑和指标。 配置流量转移。 部署 sleep 服务 首先，让我们在不属于 bookinfo 应用程序工作区的另一个命名空间中部署“睡眠”服务。这将用于测试安全设置。\n创建以下 sleep.yaml 文件：\nsleep.yaml # Copyright Istio Authors # # Licensed under the Apache License, Version 2.0 (the \u0026#34;License\u0026#34;); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at …","relpermalink":"/tsb/quickstart/security/","summary":"在此场景中，你将了解如何使用 TSB 安全设置来限制来自工作区外部的访问。这有助于通过控制服务之间的通信来增强环境的安全性。 先决条件 在继续之前，请确保你已完成以下任务： 熟悉 TSB 概念。 安装 TSB 演示环境。 部署 Istio Bookinfo 示例","title":"安全"},{"content":"什么是安全域（Security Domains）？ 安全域（Security Domains） 允许你在配置层次结构的任何位置 - 租户、工作区 或 安全组 - 创建跨 TSB 层次结构 的配置分组。将 securityDomain 视为可以附加到这些 TSB 资源中的名称，然后可以在 TSB 规则中使用它们。\n一旦资源被标识为具有 securityDomain，就可以在创建规则时将安全域用作源或目标。这允许 Operator 在新对象创建时在 Tetrate Service Bridge (TSB) 层次结构中持续建立一组要求。\n何时应该使用安全域（Security Domains）？ 随着服务网格的扩展，你需要执行的策略数量也会增加。在某些情况下，这些安全控制与你当前的 TSB 层次结构选择不太匹配：你有一组服务、工作区和租户，共享一组公共安全控制。\n安全域（Security Domains）为你提供了创建单一的 授权规则 的能力，该规则可以使用它们共享的 securityDomain 名称来处理多个租户、工作区和安全组。然后，你可以使用简单的 from 和 to 子句创建广泛和包 …","relpermalink":"/tsb/howto/security-domains/","summary":"什么是安全域（Security Domains）？ 安全域（Security Domains） 允许你在配置层次结构的任何位置 - 租户、工作区 或 安全组 - 创建跨 TSB 层次结构 的配置分组。将 securityDomain 视为可以附加到这些 TSB 资源中","title":"创建安全域"},{"content":"如果你的环境有严格的网络策略，防止两个命名空间之间进行任何未经授权的通信，你可能需要添加一个或多个例外到你的网络策略，以允许 sidecar 与本地 Istio 控制平面之间的通信，以及本地 Istio 控制平面与 TSB 管理平面之间的通信。\n以下信息可用于推导适当的防火墙规则集。\nTSB、控制平面和工作负载之间的通信 TSB 和 Istio 之间 TSB 负载均衡器端口\nTSB 负载均衡器（也称为 front-envoy）的默认端口为 8443。此端口值是可配置的。例如，它可以更改为 443。如果更改了默认端口，则通过 front-envoy 通信的所有组件都需要相应调整以匹配用户定义的 front-envoy 端口的值。 源 目标 xcp-edge.istio-system TSB 负载均衡器 IP，端口 9443 oap.istio-system TSB 负载均衡器 IP，端口 8443 或用户定义的 front-envoy 端口 otel-collector.istio-system TSB 负载均衡器 IP，端口 8443 或用户定义的 front-envoy …","relpermalink":"/tsb/setup/firewall-information/","summary":"如果你的环境有严格的网络策略，防止两个命名空间之间进行任何未经授权的通信，你可能需要添加一个或多个例外到你的网络策略，以允许 sidecar 与本地 Istio 控制平面之间的通信，以及本地 Istio 控制平面与 TSB 管理平面之间的通信。 以下","title":"防火墙信息"},{"content":"本文描述了如何将所有配置、用户和组从一个组织迁移到一个新创建的组织。\n获取数据 首先，提取每个租户的所有配置。对于每个租户，执行以下命令：\ntctl get all --tenant \u0026lt;tenant\u0026gt; \u0026gt; config.yaml 一旦你将所有配置保存在 config.yaml 中，请确保手动复制其中的各种绑定（例如 ApplicationAccessBindings、APIAccessBindings 等）到一个名为 bindings.yaml 的文件中，并从 config.yaml 中删除它们。\n这是因为当你稍后使用 config.yaml 的内容时，绑定的全限定名称将不存在，这将导致在应用配置时出现错误。绑定必须在将对象移动到新组织后才能应用。\n你还需要编辑 config.yaml，将文件中每个 metadata.organization 字段中的值替换为指向你将要创建的新组织。\n如果你正在创建一个新的租户，你还应该更改租户部分。\n以下是一个示例 YAML 文件，显示了你的 config.yaml 中的一个条目应该是什么样子的。还请注意，下面的 YAML 文件不包含全限定名 …","relpermalink":"/tsb/operations/migrate-organization/","summary":"本文描述了如何将所有配置、用户和组从一个组织迁移到一个新创建的组织。 获取数据 首先，提取每个租户的所有配置。对于每个租户，执行以下命令： tctl get all --tenant \u003ctenant\u003e \u003e config.yaml 一旦你将所有配置保存在 config.yaml 中，请确保手动复制其中的各种","title":"将数据迁移到新的组织"},{"content":"Egress Gateway 充当流出网格的流量网关。用户可以定义允许通过网关向外部服务发送流量的服务。\n目前只能发送外部的 HTTPS 流量。但是，原始的出站请求应使用 HTTP。这些出站 HTTP 请求会转换为 HTTPS 请求并发送到外部服务。例如，从通过 Egress Gateway 经过的服务发送到 http://tetrate.io 的请求会被转换为对 https://tetrate.io 的请求，并代表原始服务进行代理。目前不支持最终需要是 HTTP 的请求。例如，如果你的最终目的地是 http://tetrate.io，则无法使用 Egress Gateway。\n本文将描述如何配置 Egress Gateway，允许服务只能向特定服务发送出站请求。以下图示显示了在使用 Egress Gateway 时的请求和响应流程：\nEgress Gateway 流程 在开始之前，请确保你已经：\n熟悉 TSB 概念 安装了 TSB 环境。你可以使用 TSB 演示 进行快速安装。 完成了 TSB 快速入门。本文假定你已经创建了租户，并熟悉 Workspace 和 Config …","relpermalink":"/tsb/howto/gateway/egress-gateways/","summary":"Egress Gateway 充当流出网格的流量网关。用户可以定义允许通过网关向外部服务发送流量的服务。 目前只能发送外部的 HTTPS 流量。但是，原始的出站请求应使用 HTTP。这些出站 HTTP 请求会转换为 HTTPS 请求并发送到外部服务。例如，从通过 Egress","title":"控制对外部服务的访问"},{"content":"Argo Rollouts 将始终响应 Rollouts 资源的更改，无论更改是如何进行的。这意味着 Argo Rollouts 与你可能用来管理部署的所有模板解决方案兼容。\nArgo Rollouts 清单可以使用 Helm 包管理器进行管理。如果你的 Helm Chart 包含 Rollout 资源，那么一旦你安装或升级 Chart，Argo Rollouts 将接管并启动渐进式交付流程。\n以下是使用 Helm 管理的 Rollout 示例：\napiVersion: argoproj.io/v1alpha1 kind: Rollout metadata: name: {{ template \u0026#34;helm-guestbook.fullname\u0026#34; . }} labels: app: {{ template \u0026#34;helm-guestbook.name\u0026#34; . }} chart: {{ template \u0026#34;helm-guestbook.chart\u0026#34; . }} release: {{ .Release.Name }} heritage: {{ .Release.Service }} spec: …","relpermalink":"/argo-rollouts/rollout/helm/","summary":"Argo Rollouts 将始终响应 Rollouts 资源的更改，无论更改是如何进行的。这意味着 Argo Rollouts 与你可能用来管理部署的所有模板解决方案兼容。 Argo Rollouts 清单可以使用 Helm 包管理器进行管理。如果你的 Helm Chart 包含 Rollout 资源，那么一旦你安装或升级 Chart，A","title":"将 Argo Rollouts 和 Helm 一起使用"},{"content":"🔔 提示：自 Argo Rollouts v1.2 起提供了多个 trafficRouting。\n多个提供程序的使用旨在涵盖一些情况，例如我们必须在南北和东西流量路由或需要使用多个提供程序的任何混合架构的情况下。\n何时可以使用多个提供程序的示例 避免在 Ingress 控制器上注入 Sidecars 这是服务网格的常见要求，通过使用多个 trafficRoutings，你可以利用南北交通转移到 NGiNX 和西东交通转移到 SMI，避免将 Ingress 控制器添加到网格中。\n避免操作 Ingress 中的主机 Header 将一些 Ingress 控制器添加到网格中的另一个常见副作用是使用这些网格主机标头将其指向网格主机名以进行路由。\n避免大爆炸 这发生在存在的机群中，其中停机时间非常短或几乎不可能。为了避免大爆炸采用，使用多个提供程序可以缓解团队如何逐步实施新技术。例如，现有机群正在使用提供程序（例如大使），已经在其推出的一部分中使用金丝雀方式进行南北方向的金丝雀测试，可以逐渐实现更多提供程序，例如 Istio，SMI 等。\n混合方案 在这种情况下，它非常类似于避免大爆炸，无论是作 …","relpermalink":"/argo-rollouts/traffic-management/mixed/","summary":"🔔 提示：自 Argo Rollouts v1.2 起提供了多个 trafficRouting。 多个提供程序的使用旨在涵盖一些情况，例如我们必须在南北和东西流量路由或需要使用多个提供程序的任何混合架构的情况下。 何时可以使用多个提供程序的示","title":"多提供方"},{"content":"什么是实验 CRD？ 实验 CRD 允许用户对一个或多个 ReplicaSet 进行短暂运行。除了运行短暂 ReplicaSet 外，实验 CRD 还可以在 ReplicaSet 旁边启动 AnalysisRuns。通常，这些 AnalysisRun 用于确认新的 ReplicaSet 是否按预期运行。\n如果设置了权重（需要流量路由）或该实验的 Service 属性，则还会生成一个服务，用于将流量路由到实验 ReplicaSet。\n实验用例 用户想要运行应用程序的两个版本以进行 Kayenta 风格的分析来启用。实验 CRD 基于实验的 spec.templates 字段创建 2 个 ReplicaSet（基线和金丝雀），并等待两者都健康。经过一段时间后，实验会缩小 ReplicaSet 的规模，用户可以开始 Kayenta 分析运行。 用户可以使用实验来启用 A/B/C 测试，通过为不同版本的应用程序启动多个实验来进行长时间测试。每个实验都有一个 PodSpec 模板，定义用户要运行的特定版本。实验允许用户同时启动多个实验，并保持每个实验的独立性。 使用不同的标签启动现有应用程序的新 …","relpermalink":"/argo-rollouts/experiment/","summary":"什么是实验 CRD？ 实验 CRD 允许用户对一个或多个 ReplicaSet 进行短暂运行。除了运行短暂 ReplicaSet 外，实验 CRD 还可以在 ReplicaSet 旁边启动 AnalysisRuns。通常，这些 AnalysisRun 用于确认新的 ReplicaSet 是否按预期运行。 如果设置了权重（需要流量路由）","title":"实验 CRD"},{"content":"本章将 SPIFFE 与其他解决类似问题的技术进行了比较。\n简介 SPIFFE 和 SPIRE 所解决的问题并不新鲜。每一个分布式系统都必须有某种形式的身份认证才是安全的。网络公钥基础设施、Kerberos/Active Directory、OAuth、秘密存储和服务网格就是例子。\n然而，这些现有的身份识别形式并不适合用于识别组织内的内部服务。网络 PKI 的实施具有挑战性，对于典型的内部部署来说也是不安全的。Kerberos，Active Directory 的认证组件，需要一个永远在线的票证授予服务器，并且没有任何同等的证明。服务网格、秘密管理器和覆盖网络都解决了服务身份的部分难题，但并不完整。SPIFFE 和 SPIRE 是目前服务身份问题的唯一完整解决方案。\n网络公钥基础设施 网络公钥基础设施（Web PKI）是广泛使用的从我们的网络浏览器连接到安全网站的方法。它利用 X.509 证书来断言用户正在连接到他们打算访问的网站。由于你可能对这种模式很熟悉，所以有理由问：为什么我们不能在我们的组织内使用 Web PKI 进行服务识别？\n在传统的 Web PKI 中，证书的发放和更新完 …","relpermalink":"/spiffe/comparing-spiffe-to-other-security-technologies/","summary":"本章将 SPIFFE 与其他解决类似问题的技术进行了比较。 简介 SPIFFE 和 SPIRE 所解决的问题并不新鲜。每一个分布式系统都必须有某种形式的身份认证才是安全的。网络公钥基础设施、Kerberos/Active Directory、O","title":"9. SPIFFE 与其他安全技术对比"},{"content":"在参考平台中，整个应用系统的运行状态或执行状态是由于基础设施代码（例如，用于服务间通信的网络路由、资源配置代码）、策略代码（例如，指定认证和授权策略的代码）和会话管理代码（例如，建立 mTLS 会话的代码、生成 JWT 令牌的代码）的执行的组合，这些代码由可观测性作为代码的执行所揭示。服务网格的可观测性代码在运行期间将基础设施、策略和会话管理代码的执行输出转发给各种监控工具，这些工具产生适用的指标和日志聚合工具以及追踪工具，这些工具又将其输出转发给集中式仪表板。作为这些工具输出的组成部分的分析，使系统管理员能够获得整个应用系统运行时状态的综合全局视图。正是通过持续监控和零信任设计功能实现的 DevSecOps 平台的运行时性能，为云原生应用提供了所有必要的安全保障。\n在 DevSecOps 管道中，实现持续 ATO 的活动是：\n检查合规的代码。可以检查以下代码是否符合《风险管理框架》的规定 (a) IaC：生成网络路线，资源配置 (b) 策略即代码：对 AuthN 和 AuthZ 策略进行编码 (c) 会话管理代码：mTLS 会话，JWT 令牌 (d) 可观测性代码\n具体的风险评估功能 …","relpermalink":"/service-mesh-devsecops/implement/leveraging-devsecops-for-continuous-authorization-to-operate-c-ato/","summary":"在参考平台中，整个应用系统的运行状态或执行状态是由于基础设施代码（例如，用于服务间通信的网络路由、资源配置代码）、策略代码（例如，指定认证和授权策略的代码）和会话管理代码（例如，建立 mTLS 会话的代码、生成","title":"4.10 利用 DevSecOps 进行持续授权操作（C-ATO）"},{"content":"我们讨论过只能使用云原生应用程序来创建基础架构。同时基础架构也负责运行这些应用程序。\n运行由应用程序配置和控制的基础架构可以轻松得扩展。我们通过学习如何通过扩展应用的方式来扩展基础架构。我们还通过学习如何保护应用程序来保护基础架构。\n在动态环境中，无法通过增加人手来管理这样的复杂性，同样也不能靠增加人手来处理策略和安全问题。\n这意味着，就像我们必须创建通过协调器模式强制执行基础架构状态的应用程序一样，我们需要创建实施安全策略的应用程序。在创建应用程序以执行的策略之前，我们需要以机器可解析的格式编写策略。\n策略即代码 由于策略没有明确定义的技术实现，所以策略难以纳入代码。它更多地关注业务如何实现而不是谁来实现。\n如何实现和谁来实现都会经常变化，但是实现方式变化更频繁且不容易被抽象化。它也是组织特定的，可能需要了解创建基础架构人员的沟通结构的具体细节。\n策略需要应用于应用程序生命周期的多个阶段。正如我们在第 7 章中所讨论的，应用程序通常有三个阶段：部署、运行和退役。\n部署阶段将在应用程序和基础架构变更发布之前先应用策略。这将包括部署规则和一致性测试。运行阶段将包括持续的遵守和执行访问控制 …","relpermalink":"/cloud-native-infra/securing-applications/","summary":"我们讨论过只能使用云原生应用程序来创建基础架构。同时基础架构也负责运行这些应用程序。 运行由应用程序配置和控制的基础架构可以轻松得扩展。我们通过学习如何通过扩展应用的方式来扩展基础架构。我们还通过学习如","title":"第 8 章：保护应用程序"},{"content":"日志记录了集群中的活动。审计日志是必要的，这不仅是为了确保服务按预期运行和配置，也是为了确保系统的安全。系统性的审计要求对安全设置进行一致和彻底的检查，以帮助识别潜在威胁。Kubernetes 能够捕获集群操作的审计日志，并监控基本的 CPU 和内存使用信息；然而，它并没有提供深入的监控或警报服务。\n关键点\n在创建时建立 Pod 基线，以便能够识别异常活动。 在主机层面、应用层面和云端（如果适用）进行日志记录。 整合现有的网络安全工具，进行综合扫描、监控、警报和分析。 设置本地日志存储，以防止在通信失败的情况下丢失。 日志 在 Kubernetes 中运行应用程序的系统管理员应该为其环境建立一个有效的日志、监控和警报系统。仅仅记录 Kubernetes 事件还不足以了解系统上发生的行动的全貌。还应在主机级、应用级和云上（如果适用）进行日志记录。而且，这些日志可以与任何外部认证和系统日志相关联，以提供整个环境所采取的行动的完整视图，供安全审计员和事件响应者使用。\n在 Kubernetes 环境中，管理员应监控 / 记录以下内容：\nAPI 请求历史 性能指标 部署情况 资源消耗 操作系统调 …","relpermalink":"/kubernetes-hardening-guidance/logging/","summary":"日志记录了集群中的活动。审计日志是必要的，这不仅是为了确保服务按预期运行和配置，也是为了确保系统的安全。系统性的审计要求对安全设置进行一致和彻底的检查，以帮助识别潜在威胁。Kubernetes 能够捕获","title":"日志审计"},{"content":"OpenTelemetry 是一个大型项目。OpenTelemetry 项目的工作被划分为特殊兴趣小组（SIG）。虽然所有的项目决策最终都是通过 GitHub issue 和 pull request 做出的，但 SIG 成员经常通过 CNCF 的官方 Slack 保持联系，而且大多数 SIG 每周都会在 Zoom 上会面一次。\n任何人都可以加入一个 SIG。要想了解更多关于当前 SIG、项目成员和项目章程的细节，请查看 GitHub 上的 OpenTelemetry 社区档案库。\n规范 OpenTelemetry 是一个规范驱动的项目。OpenTelemetry 技术委员会负责维护该规范，并通过管理规范的 backlog 来指导项目的发展。\n小的改动可以以 GitHub issue 的方式提出，随后的 pull request 直接提交给规范。但是，对规范的重大修改是通过名为 OpenTelemetry Enhancement Proposals（OTEPs）的征求意见程序进行的。\n任何人都可以提交 OTEP。OTEP 由技术委员会指定的具体审批人进行审查，这些审批人根据其专业领域进 …","relpermalink":"/opentelemetry-obervability/organization/","summary":"附录 A：OpenTelemetry 项目组织","title":"附录 A：OpenTelemetry 项目组织"},{"content":"Istio 隔离边界可以在 Kubernetes 集群内或跨多个集群中运行多个由 TSB（Tetrate Service Bridge）管理的 Istio 环境。这些 Istio 环境在服务发现和配置分发方面彼此隔离。隔离边界带来了以下几个好处：\n强大的网络隔离默认提供了在高度受管制的环境中严格且易于演示的安全性。 在集群内运行不同的 Istio 版本允许你在同一集群中支持传统和现代应用程序。 金丝雀发布提供了在测试和部署 TSB 升级时的灵活性。 安装 升级\n要从非修订的控制平面升级到修订的控制平面，请按照 非修订版到修订版的升级 中提到的步骤进行操作。 OpenShift\n如果你使用 OpenShift，请将以下 kubectl 命令替换为 oc。 对于全新安装，你可以按照使用 tctl 或 helm 来加入控制平面集群的标准步骤，以及以下更改进行操作：\n通过将 ISTIO_ISOLATION_BOUNDARIES 设置为 true 在 TSB 控制平面 Operator 中启用隔离边界。 在 ControlPlane CR 或控制平面 Helm 值中添加隔离边界定义。 在以下示例 …","relpermalink":"/tsb/setup/isolation-boundaries/","summary":"Istio 隔离边界可以在 Kubernetes 集群内或跨多个集群中运行多个由 TSB（Tetrate Service Bridge）管理的 Istio 环境。这些 Istio 环境在服务发现和配置分发方面彼此隔离。隔离边界带来了以下几个好处： 强大的网络隔离默认提供了在高","title":"Istio 隔离边界"},{"content":"在本部分中，你将创建一个名为 bookinfo 的应用程序并向其附加一个 API。API 将根据 OpenAPI 规范进行配置。\nTSB 中的应用程序是服务的逻辑分组，这些服务公开了应用程序的不同功能和用例。它们属于租户，可用于观察服务的行为并配置如何使用这些服务及其 API。应用程序公开一组 API 对象，这些对象定义可用内容和使用条件。\n创建应用程序 先决条件 在继续阅读本指南之前，请确保你已完成以下步骤：\n熟悉 TSB 概念 安装 TSB 演示环境 部署 Istio Bookinfo 示例应用程序 创建租户 创建工作区 配置权限 设置 Ingress 网关 检查服务拓扑和指标 配置流量转移 配置安全控制 创建应用程序 创建 application.yaml 文件：\napiVersion: application.tsb.tetrate.io/v2 kind: Application metadata: organization: tetrate tenant: tetrate name: bookinfo spec: displayName: Bookinfo …","relpermalink":"/tsb/quickstart/apps/","summary":"在本部分中，你将创建一个名为 bookinfo 的应用程序并向其附加一个 API。API 将根据 OpenAPI 规范进行配置。 TSB 中的应用程序是服务的逻辑分组，这些服务公开了应用程序的不同功能和用例。它们属于租户，可用于观察服务的行为并配","title":"创建应用程序和 API"},{"content":"对于此场景，你需要两个已入网的集群，以配置它们之间的轮询和故障切换。\n先决条件 在开始之前，请确保你已经：\n熟悉 TSB 概念 安装了 TSB 演示 环境 创建了 租户 创建工作区和网关组 以下 YAML 文件包含两个对象：一个用于应用程序的 工作区，以及一个 网关 组，以便你可以配置应用程序的入口。\n将其保存为 httpbin-mgmt.yaml，然后使用 tctl 应用：\ntctl apply -f httpbin-mgmt.yaml 部署 httpbin 以下配置应该应用于两个集群，以部署你的应用程序，首先创建命名空间并启用 Istio sidecar 注入。\nkubectl create namespace httpbin kubectl label namespace httpbin istio-injection=enabled 然后部署你的应用程序。\nkubectl apply -f \\ https://raw.githubusercontent.com/istio/istio/master/samples/httpbin/httpbin.yaml \\ -n …","relpermalink":"/tsb/howto/gateway/distributed-ingress/","summary":"对于此场景，你需要两个已入网的集群，以配置它们之间的轮询和故障切换。 先决条件 在开始之前，请确保你已经： 熟悉 TSB 概念 安装了 TSB 演示 环境 创建了 租户 创建工作区和网关组 以下 YAML 文件包含两个对象：一个用于应用程序的 工","title":"分布式入口网关"},{"content":"本文描述了如何通过使用 TSB 流量设置来降低网格中的控制平面和所有网关使用的 CPU 和内存数量，该设置将生成一个 sidecar 资源。\n先决条件 熟悉 TSB 概念。 安装 TSB 演示 环境。 创建一个 租户。 准备环境 在此场景中，我们将部署三个不同的应用程序：bookinfo、httpbin 和 helloworld。每个应用程序都将在接收流量并将其转发到应用程序的同一命名空间中拥有其 ingressgateway。\n首先，为每个应用程序创建一个命名空间并启用 sidecar 注入：\n$ kubectl create ns \u0026lt;ns\u0026gt; $ kubectl label namespace \u0026lt;ns\u0026gt; istio-injection=enabled 现在，在每个命名空间中部署应用程序：\n$ kubectl apply -n bookinfo -f https://raw.githubusercontent.com/istio/istio/master/samples/bookinfo/platform/kube/bookinfo.yaml $ kubectl apply -n …","relpermalink":"/tsb/operations/lower-istio-resources/","summary":"本文描述了如何通过使用 TSB 流量设置来降低网格中的控制平面和所有网关使用的 CPU 和内存数量，该设置将生成一个 sidecar 资源。 先决条件 熟悉 TSB 概念。 安装 TSB 演示 环境。 创建一个 租户。 准备环境 在此场景中，我们将部署三个不同的应","title":"降低 Istio 资源消耗"},{"content":"可以通过使用转换器配置来扩展 Kustomize 以理解 CRD 对象。使用转换器配置，可以“教授”kustomize 有关 Rollout 对象的结构，并利用 kustomize 功能，例如 ConfigMap/Secret 生成器、变量引用以及通用标签和注释。要将 Rollouts 与 kustomize 结合使用：\n下载 rollout-transform.yaml 到你的 kustomize 目录。\n在你的 kustomize configurations 部分中包含 rollout-transform.yaml：\nkind: Kustomization apiVersion: kustomize.config.k8s.io/v1beta1 configurations: - rollout-transform.yaml 展示了使用 Rollouts 中的转换器的能力的 kustomize 应用程序示例可以在这里看到。\n在 Kustomize 3.6.1 中，可以直接从远程资源引用配置： configurations: - …","relpermalink":"/argo-rollouts/rollout/kustomize/","summary":"可以通过使用转换器配置来扩展 Kustomize 以理解 CRD 对象。使用转换器配置，可以“教授”kustomize 有关 Rollout 对象的结构，并利用 kustomize 功能，例如 ConfigMap/Secret 生成器、变量引用以及通用标签和注释。要将 Rollouts 与 kustomize 结合使用： 下载 rollout-transform.yaml 到你的 kustomize 目","title":"Kustomize 集成"},{"content":"本章包括五个业界案例，他们是现实世界中部署了 SPIFFE 和 SPIRE 的企业的工程师。\nUber：用加密身份确保下一代和传统基础设施的安全 Ryan Turner，软件工程师，Uber\n在过去十年中，Uber 已经成为爆炸性增长的典型代表。随着软件服务的数量和我们运营的地理规模的增长，复杂性和风险也在增加。为了满足不断增长的需求，我们开始建立我们的下一代基础设施平台。同时，几年前，我们看到开源项目 SPIFFE 和 SPIRE 的一些早期动力。\n我们立即看到了 SPIFFE 所能带来的价值，使我们能够加强我们的下一代基础设施安全态势。我们在 Uber 上线了 SPIRE，现在正使用它在各种工作负载环境中使用可加密验证的身份建立信任。我们从一些应用服务和内部服务开始，比如一个工作流引擎，它通过访问整个平台的数据，旋转多个动态工作负载来完成特定任务。SPIRE 向我们的工作负载提供 SPIFFE 身份，跨越我们的应用周期。SPIFFE 用于验证服务，帮助我们避免可能导致生产问题的错误配置。\n使用 SPIRE 改造传统堆栈 SPIRE 现在是 Uber 的下一个基础设施的关键组成部分， …","relpermalink":"/spiffe/practitioners-stories/","summary":"本章包括五个业界案例，他们是现实世界中部署了 SPIFFE 和 SPIRE 的企业的工程师。 Uber：用加密身份确保下一代和传统基础设施的安全 Ryan Turner，软件工程师，Uber 在过去十年中，Uber 已经成为爆炸性增长的典型代","title":"10. 业界案例"},{"content":"该节将带领大家了解 Kubernetes 中的基本概念，尤其是作为 Kubernetes 中调度的最基本单位 Pod。\n本节中包括以下内容：\n了解 Pod 的构成 Pod 的生命周期 Pod 中容器的启动顺序模板定义 Kubernetes 中的基本组件 kube-controller-manager 就是用来控制 Pod 的状态和生命周期的，在了解各种 controller 之前我们有必要先了解下 Pod 本身和其生命周期。\n","relpermalink":"/kubernetes-handbook/architecture/pod-state-and-lifecycle/","summary":"该节将带领大家了解 Kubernetes 中的基本概念，尤其是作为 Kubernetes 中调度的最基本单位 Pod。 本节中包括以下内容： 了解 Pod 的构成 Pod 的生命周期 Pod 中容器的启动顺序模板定义 Kubernetes 中的基本组件 kube-controller-manager 就是用来控制 Pod 的状态和生命周期的，在了解各","title":"Pod 状态与生命周期管理"},{"content":"如果您认为云原生基础架构是可购买的产品或是从云供应商那购买的服务器，我们很抱歉让您失望了。如果不采用这些做法并改变您建设和维护基础架构的方式，您就不会受益。\n它不仅仅影响服务器、网络和存储。它关乎的是工程师如何管理应用程序，就像接受故障一样。\n围绕云原生实践建立的文化与传统技术和工程组织有很大不同。我们并不是解决组织文化或结构问题的专家，但如果您希望改变组织结构，我们建议您从高绩效组织中实施 DevOps 实践的角度来看待价值观和经验教训。\n一些需要探索的地方是 Netflix 的文化套餐，它促进了自由和责任感，还有亚马逊的双比萨团队，这些团队以低开销推广自治团体。云原生应用程序需要与构建它们的团队具有相同的解耦特征。康威定律很好地描述了这一点：“设计系统的架构受制于产生这些设计的组织的沟通结构。”\n在我们结束本书时，我们希望关注哪些领域是您采用云原生实践时最重要的。我们还将讨论一些预测变化的基础架构模式，以便您知道将来要寻找什么。\n关注改变的地方 如果您拥有现有的基础架构或传统数据中心，则过渡到云原生不会在一夜之间发生。如果您有足够大的基础架构或两个以上的人员管理它，试图强制实施基础 …","relpermalink":"/cloud-native-infra/implementing-cloud-native-infrasctructure/","summary":"如果您认为云原生基础架构是可购买的产品或是从云供应商那购买的服务器，我们很抱歉让您失望了。如果不采用这些做法并改变您建设和维护基础架构的方式，您就不会受益。 它不仅仅影响服务器、网络和存储。它关乎的是工","title":"第 9 章：实施云原生基础架构"},{"content":"遵循本文件中概述的加固指南是确保在 Kubernetes 协调容器上运行的应用程序安全的一个步骤。然而，安全是一个持续的过程，跟上补丁、更新和升级是至关重要的。具体的软件组件因个人配置的不同而不同，但整个系统的每一块都应尽可能保持安全。这包括更新：Kubernetes、管理程序、虚拟化软件、插件、环境运行的操作系统、服务器上运行的应用程序，以及 Kubernetes 环境中托管的任何其他软件。\n互联网安全中心（CIS）发布了保护软件安全的基准。管理员应遵守 Kubernetes 和任何其他相关系统组件的 CIS 基准。管理员应定期检查，以确保其系统的安全性符合当前安全专家对最佳实践的共识。应定期对各种系统组件进行漏洞扫描和渗透测试，主动寻找不安全的配置和零日漏洞。任何发现都应在潜在的网络行为者发现和利用它们之前及时补救。\n随着更新的部署，管理员也应该跟上从环境中删除任何不再需要的旧组件。使用托管的 Kubernetes 服务可以帮助自动升级和修补 Kubernetes、操作系统和网络协议。然而，管理员仍然必须为他们的容器化应用程序打补丁和升级。\n","relpermalink":"/kubernetes-hardening-guidance/upgrading-and-application-security-practices/","summary":"遵循本文件中概述的加固指南是确保在 Kubernetes 协调容器上运行的应用程序安全的一个步骤。然而，安全是一个持续的过程，跟上补丁、更新和升级是至关重要的。具体的软件组件因个人配置的不同而不同，但整个系统的每一块都应尽","title":"升级和应用安全实践"},{"content":"路线图很快就会过时。有关最新的路线图，请参见 OpenTelemetry 状态页面。也就是说，以下是截至目前 OpenTelemetry 的状态。\n核心组件 目前，OpenTelemetry 追踪信号已被宣布为稳定的，并且在许多语言中都有稳定的实现。\n度量信号刚刚被宣布稳定，测试版的实施将在 2022 年第一季度广泛使用。\n日志信号预计将在 2022 年第一季度宣布稳定，测试版实现预计将在 2022 年第二季度广泛使用。2021 年，Stanza 项目被捐赠给 OpenTelemetry，为 OpenTelemetry 收集器增加了高效的日志处理能力。\n用于 HTTP/RPC、数据库和消息系统的语义公约预计将在 2022 年第一季度宣布稳定。\n未来 完成上述路线图就完成了对 OpenTelemetry 核心功能的稳定性要求。这是一个巨大的里程碑，因为它打开了进一步采用 OpenTelemetry 的大门，包括与数据库、管理服务和 OSS 库的原生集成。这些集成工作大部分已经以原型和测试版支持的形式在进行。随着 OpenTelemetry 的稳定性在 2022 年上半年完成， …","relpermalink":"/opentelemetry-obervability/roadmap/","summary":"附录 B：OpenTelemetry 项目路线图","title":"附录 B：OpenTelemetry 项目路线图"},{"content":"从版本 1.5 开始，TSB 提供了一种自动获取来自远程私有 Docker 容器仓库的镜像的方式，方法是在 ManagementPlane 和 ControlPlane CRs 中定义 imagePullSecrets。 如果定义了 imagePullSecrets，则将使用秘密中的凭据对必需的 ServiceAccounts 进行补丁，从而允许安全地访问存储在远程私有仓库中的容器。以下步骤概述了配置过程：\n同步镜像 TSB 镜像位于 Tetrate 的仓库中，只能复制到你的仓库（不允许直接在任何环境中下载）。第一步是将镜像传输到你的仓库。要同步镜像，你需要按照 文档 使用 tctl install image-sync（需要 Tetrate 提供的许可密钥）。\n获取私有仓库的 JSON 密钥 指定为 imagePullSecrets 的秘密将存储凭据，允许 Kubernetes 从私有仓库中拉取所需的容器。获取凭据的方式取决于仓库。请参考以下链接以获得主要云提供商的指导 - AWS、GCP 和 Azure。\n在 TSB 使用的每个命名空间中创建机密 如 Kubernetes 文档 中 …","relpermalink":"/tsb/setup/remote-registry/","summary":"从版本 1.5 开始，TSB 提供了一种自动获取来自远程私有 Docker 容器仓库的镜像的方式，方法是在 ManagementPlane 和 ControlPlane CRs 中定义 imagePullSecrets。 如果定义了 imagePullSecrets，则将使用秘密中的凭据对必需","title":"仓库机密"},{"content":"本文描述了如何自定义 Kubernetes 部署的 TSB 组件，包括使用覆盖来执行由 Tetrate Service Bridge (TSB) Operator 部署的资源的高级配置，使用示例说明。\n背景 TSB 广泛使用Operator（Operator）模式在 Kubernetes 中部署和配置所需的部分。\n通常，通过 Operator 来进行自定义和微调参数，Operator 负责创建必要的资源并控制其生命周期。\n例如，当你创建一个 IngressGateway CR 时，TSB Operator会获取此信息并部署和/或更新相关资源，如 Kubernetes Service 对象，通过创建清单并应用它们。清单将使用你提供的某些参数，以及由 TSB 计算的其他默认值。\n然而，TSB 并不一定会暴露用于微调 Service 对象的所有参数。如果 TSB 提供了所有用于配置 Service 对象的钩子，那么 TSB 将不得不实际上复制整个 Kubernetes API，这在现实中既不可行也不可取。\n这就是我们使用覆盖的地方，它允许你覆盖并应用于正在部署的资源的自定义配置。有关覆盖工作 …","relpermalink":"/tsb/operations/kube-customization/","summary":"本文描述了如何自定义 Kubernetes 部署的 TSB 组件，包括使用覆盖来执行由 Tetrate Service Bridge (TSB) Operator 部署的资源的高级配置，使用示例说明。 背景 TSB 广泛使用Operator（Operator）模式在 Kubernetes 中部署和配置所需的部分。 通常，通过 Operator 来进","title":"定制 TSB Kubernetes 组件"},{"content":"本操作指南将向你展示如何配置 TSB 中的非 HTTP 服务器。阅读本文档后，你应该熟悉在 IngressGateway 和 Tier1Gateway API 中使用 TCP 部分的方法。\n概要 工作流程与配置 IngressGateway 和 Tier1Gateway 中的 HTTP 服务器完全相同。但是，非 HTTP 支持多端口服务。\n在开始之前，请确保你已经：\n熟悉 TSB 概念 熟悉 载入集群 创建了 租户 设置 安装了 TSB 的四个集群 - 管理平面、Tier-1 和 Tier-2 边缘集群。 在 Tier-2 集群中，在 echo 命名空间部署了 Tier-2 网关。 在 Tier-1 集群中，在 tier1 命名空间部署了 Tier-1 网关。 在两个网关中，端口 8080 和 9999 应可用（为简单起见，我们认为服务和目标端口相同）。要安装网关，请参阅此处。 在 Tier-2 集群中部署与 HTTP 和非 HTTP 流量配合使用的应用程序。在此演示中，你将部署 Istio 示例目录中的应用程序到 echo 命名空间中。 helloworld 用于 HTTP 流量。清 …","relpermalink":"/tsb/howto/gateway/configure-and-route-nonhttp-traffic/","summary":"本操作指南将向你展示如何配置 TSB 中的非 HTTP 服务器。阅读本文档后，你应该熟悉在 IngressGateway 和 Tier1Gateway API 中使用 TCP 部分的方法。 概要 工作流程与配置 IngressGateway 和 Tier1Gateway 中的 HTTP 服务器完全相同。但是，非 HTTP 支持多端口服务。 在开始之前，请确保你已经： 熟","title":"配置和路由 TSB 中的 HTTP、非 HTTP（多协议）和多端口服务流量"},{"content":"Argo Rollouts 控制器已经安装了Prometheus 指标，可以在 8090 端口的/metrics中获取。你可以使用这些指标查看控制器的健康状况，无论是通过仪表板还是通过其他 Prometheus 集成。\n安装和配置 Prometheus 要利用指标，你需要在 Kubernetes 集群中安装 Prometheus。如果你没有现有的 Prometheus 安装，你可以使用任何常见的方法在你的集群中安装它。流行的选项包括Prometheus Helm Chat 或 Prometheus Operator。\n一旦 Prometheus 在你的集群中运行，你需要确保它抓取 Argo Rollouts 端点。Prometheus 已经包含了针对 Kubernetes 的服务发现机制，但你需要首先进行配置。根据你的安装方法，你可能需要采取其他操作来抓取 Argo Rollouts 端点。\n例如，如果你使用了 Prometheus 的 Helm 图表，则需要使用以下注释标注你的 Argo Rollouts Controller：\nspec: template: metadata: …","relpermalink":"/argo-rollouts/rollout/controller-metrics/","summary":"Argo Rollouts 控制器已经安装了Prometheus 指标，可以在 8090 端口的/metrics中获取。你可以使用这些指标查看控制器的健康状况，无论是通过仪表板还是通过其他 Prometheus 集成。 安装和配置 Prometheus 要利用指标，你需要在 Kubernetes 集群中","title":"控制器指标"},{"content":"下面的例子是一个 Dockerfile，它以非 root 用户和非 group 成员身份运行一个应用程序。\nFROM ubuntu:latest # 升级和安装 make 工具 RUN apt update \u0026amp;\u0026amp; apt install -y make # 从一个名为 code 的文件夹中复制源代码，并使用 make 工具构建应用程序。 COPY ./code RUN make /code # 创建一个新的用户（user1）和新的组（group1）；然后切换到该用户的上下文中。 RUN useradd user1 \u0026amp;\u0026amp; groupadd group1 USER user1:group1 # 设置容器的默认入口 CMD /code/app ","relpermalink":"/kubernetes-hardening-guidance/appendix/a/","summary":"下面的例子是一个 Dockerfile，它以非 root 用户和非 group 成员身份运行一个应用程序。 FROM ubuntu:latest # 升级和安装 make 工具 RUN apt update \u0026\u0026 apt install -y make # 从一个名为 code 的文件夹中复制源代码，并使用 make 工具构建应用程序。 COPY ./code RUN make /code # 创建一","title":"附录 A：非 root 应用的 Dockerfile 示例"},{"content":"在云环境中运行时，应用程序需要具有弹性。网络通信方面特别容易出现故障。添加网络弹性的一种常见模式是创建一个导入到应用程序中的库，该库提供本附录中描述的网络弹性模式。但是，导入的库很难维护以多种语言编写的服务，且当新版本的网络库发布时，会增加应用程序测试和重新部署的负担。\n取代应用程序处理网络弹性逻辑的另一种方式是，可以将代理置于适当的位置，作为应用程序的保护和增强层。代理的优势在于避免应用程序需要额外的复杂代码，尽量减少开发人员的工作量。\n可以在连接层（物理或 SDN），应用程序或透明代理中处理网络弹性逻辑。虽然代理不是传统网络堆栈的一部分，但它们可用于透明地管理应用程序的网络弹性。\n透明代理可以在基础架构中的任何位置运行，但与应用程序的距离越近越有利。代理支持的协议还要尽可能全面，且可以代理的开放系统互连模型（OSI 模型）层。\n通过实施以下模式，代理在基础架构的弹性中扮演着积极的角色：\n负载均衡 负载切分（Load shedding） 服务发现 重试和 deadline 断路 代理也可以用来为应用程序添加功能。包括：\n安全和认证 路由（入口和出口） 洞察和监测 负载均衡 应用程序负 …","relpermalink":"/cloud-native-infra/appendix-a-patterns-for-network-resiilency/","summary":"在云环境中运行时，应用程序需要具有弹性。网络通信方面特别容易出现故障。添加网络弹性的一种常见模式是创建一个导入到应用程序中的库，该库提供本附录中描述的网络弹性模式。但是，导入的库很难维护以多种语言编写","title":"附录 A：网络弹性模式"},{"content":"本文将为您讲解 Pod 的基础概念。\n理解 Pod Pod 是 kubernetes 中你可以创建和部署的最小也是最简的单位。Pod 代表着集群中运行的进程。\nPod 中封装着应用的容器（有的情况下是好几个容器），存储、独立的网络 IP，管理容器如何运行的策略选项。Pod 代表着部署的一个单位：kubernetes 中应用的一个实例，可能由一个或者多个容器组合在一起共享资源。\nDocker 是 kubernetes 中最常用的容器运行时，但是 Pod 也支持其他容器运行时。\n在 Kubernetes 集群中 Pod 有如下两种使用方式：\n一个 Pod 中运行一个容器。“每个 Pod 中一个容器”的模式是最常见的用法；在这种使用方式中，你可以把 Pod 想象成是单个容器的封装，kuberentes 管理的是 Pod 而不是直接管理容器。 在一个 Pod 中同时运行多个容器。一个 Pod 中也可以同时封装几个需要紧密耦合互相协作的容器，它们之间共享资源。这些在同一个 Pod 中的容器可以互相协作成为一个 service 单位 —— 一个容器共享文件，另一个“sidecar”容器来更新这些文 …","relpermalink":"/kubernetes-handbook/objects/pod-overview/","summary":"本文将为您讲解 Pod 的基础概念。 理解 Pod Pod 是 kubernetes 中你可以创建和部署的最小也是最简的单位。Pod 代表着集群中运行的进程。 Pod 中封装着应用的容器（有的情况下是好几个容器），存储、独立的网络 IP，管理容器如何运行的策","title":"Pod 概览"},{"content":"Argo CD 具有在检测到 Git 中所需清单与集群中的实际状态之间存在差异时自动同步应用程序的功能。自动同步的好处是，CI/CD 管道不再需要直接访问 Argo CD API 服务器以执行部署。相反，管道将更改的清单提交并推送到跟踪 Git 存储库中。\n要配置自动同步，请运行：\nargocd app set \u0026lt;APPNAME\u0026gt; --sync-policy automated 或者，如果创建应用程序清单，则使用 automated 策略指定 syncPolicy。\nspec: syncPolicy: automated: {} 自动修整 默认情况下（作为一种安全机制），当 Argo CD 检测到资源不再在 Git 中定义时，自动同步不会删除资源。始终可以执行手动同步（并检查修整）来修整资源。也可以通过运行以下命令设置自动修整：\nargocd app set \u0026lt;APPNAME\u0026gt; --auto-prune 或通过在自动同步策略中将 prune 选项设置为 true：\nspec: syncPolicy: automated: prune: true 带有允许空值的自动修整（v1.8） 默认 …","relpermalink":"/argo-cd/user-guide/auto-sync/","summary":"Argo CD 具有在检测到 Git 中所需清单与集群中的实际状态之间存在差异时自动同步应用程序的功能。自动同步的好处是，CI/CD 管道不再需要直接访问 Argo CD API 服务器以执行部署。相反，管道将更改的清单提交并推送到跟踪 Git 存储","title":"自动同步策略"},{"content":"在 TSB 中，Application 表示一组逻辑上相关的 Services，这些服务与彼此相关，并公开一组实现完整业务逻辑的 APIs。\nTSB 可以在配置 API 运行时策略时利用 OpenAPI 注解。在本文档中，你将启用通过 Open Policy Agent (OPA) 进行授权，以及通过外部服务进行速率限制。每个请求都需要经过基本授权，并为每个有效用户强制执行速率限制策略。\n在开始之前，请确保你已经：\n熟悉 TSB 概念 熟悉 Open Policy Agent (OPA) 熟悉 Envoy 外部授权和速率限制 安装了 TSB 演示 环境 熟悉 Istio Bookinfo 示例应用程序 创建了 租户 部署 httpbin 服务 按照 本文档中的说明 创建 httpbin 服务。完成该文档中的所有步骤。\nTSB 特定注解 以下额外的 TSB 特定注解可以添加到 OpenAPI 规范中，以配置 API。\n注解 描述 x-tsb-service TSB 中提供 API 的上游服务名称，如在 TSB 服务注册表中看到的（可以使用 tctl get services 来检查）。 …","relpermalink":"/tsb/howto/gateway/application-gateway-with-openapi-annotations/","summary":"在 TSB 中，Application 表示一组逻辑上相关的 Services，这些服务与彼此相关，并公开一组实现完整业务逻辑的 APIs。 TSB 可以在配置 API 运行时策略时利用 OpenAPI 注解。在本文档中，你将启用通过 Open Policy Agent (OPA) 进行","title":"使用 OpenAPI 注解配置应用程序网关"},{"content":"本文档解释了当删除一个启用了 istio-proxy sidecar 的 pod 时会发生什么，特别是连接是如何处理的，以及如何配置 sidecar 以优雅地处理正在进行的连接。\n注意\n本文档仅适用于 TSB 版本 \u0026lt;= 1.4.x。 在开始之前，请确保你已经完成了以下工作：\n熟悉 TSB 概念 安装了 TSB 环境。你可以使用 TSB 演示 进行快速安装 完成了 TSB 使用快速入门。本文假设你已经创建了租户并熟悉了工作区和配置组。另外，你需要将 tctl 配置到 TSB 环境 安装 httpbin 当你发出删除请求来删除 Kubernetes 集群中的一个 pod 时，将发送 SIGTERM 给该 pod 中的所有容器。如果 pod 中仅包含一个容器，则它将接收到 SIGTERM 并进入终止状态。 然而，如果 pod 包含一个 sidecar（在我们的情况下是一个 istio-proxy sidecar），则不能自动保证主应用程序在 sidecar 之前终止。\n如果在应用程序之前终止了 istio-proxy sidecar，可能会发生以下问题：\n所有 TCP 连接（包括入站和出 …","relpermalink":"/tsb/operations/graceful-connection-drain/","summary":"本文档解释了当删除一个启用了 istio-proxy sidecar 的 pod 时会发生什么，特别是连接是如何处理的，以及如何配置 sidecar 以优雅地处理正在进行的连接。 注意 本文档仅适用于 TSB 版本 \u003c= 1.4.x。 在开始之前，请确保你已经完成了以下工作： 熟悉 TSB","title":"优雅关闭 istio-proxy 连接"},{"content":"有两种方法可以迁移到 Rollout：\n将现有的 Deployment 资源转换为 Rollout 资源。 使用 workloadRef 字段从 Rollout 引用现有的 Deployment。 将 Deployment 转换为 Rollout 将 Deployment 转换为 Rollout 时，需要更改三个字段：\n将 apiVersion 从 apps/v1 更改为 argoproj.io/v1alpha1 将 kind 从 Deployment 更改为 Rollout 使用蓝绿或金丝雀策略替换部署策略 以下是使用金丝雀策略的 Rollout 资源示例。\napiVersion: argoproj.io/v1alpha1 # 从 apps/v1 更改而来 kind: Rollout # 从 Deployment 更改而来 metadata: name: rollouts-demo spec: selector: matchLabels: app: rollouts-demo template: metadata: labels: app: rollouts-demo spec: …","relpermalink":"/argo-rollouts/migrating/","summary":"有两种方法可以迁移到 Rollout： 将现有的 Deployment 资源转换为 Rollout 资源。 使用 workloadRef 字段从 Rollout 引用现有的 Deployment。 将 Deployment 转换为 Rollout 将 Deployment 转换为 Rollout 时，需要更改三个字段： 将 apiVersion 从 apps/v1 更改为 argoproj.io/v1alpha1 将 kind 从 Deployment 更改为 Rollout 使用蓝绿或金","title":"迁移到 Rollouts"},{"content":"Pod 是 Kubernetes 中可以创建的最小部署单元，也是 Kubernetes REST API 中的顶级资源类型。\n在 Kuberentes V1 core API 版本中的 Pod 的数据结构如下图所示：\nPod Cheatsheet 什么是 Pod？ Pod 就像是豌豆荚一样，它由一个或者多个容器组成（例如 Docker 容器），它们共享容器存储、网络和容器运行配置项。Pod 中的容器总是被同时调度，有共同的运行环境。你可以把单个 Pod 想象成是运行独立应用的“逻辑主机”—— 其中运行着一个或者多个紧密耦合的应用容器 —— 在有容器之前，这些应用都是运行在几个相同的物理机或者虚拟机上。\n尽管 kubernetes 支持多种容器运行时，但是 Docker 依然是最常用的运行时环境，我们可以使用 Docker 的术语和规则来定义 Pod。\nPod 中共享的环境包括 Linux 的 namespace、cgroup 和其他可能的隔绝环境，这一点跟 Docker 容器一致。在 Pod 的环境中，每个容器中可能还有更小的子隔离环境。\nPod 中的容器共享 IP 地址和端口号，它们 …","relpermalink":"/kubernetes-handbook/objects/pod/","summary":"Pod 是 Kubernetes 中可以创建的最小部署单元，也是 Kubernetes REST API 中的顶级资源类型。 在 Kuberentes V1 core API 版本中的 Pod 的数据结构如下图所示： Pod Cheatsheet 什么是 Pod？ Pod 就像是豌豆荚一样，它由一个或者多个容器组成（例如 Docker 容器），它们共享容器存储、网","title":"Pod 解析"},{"content":"关于使用云提供商和避免供应商锁定存在很多争议。这种辩论充满了意识心态之争。\n锁定通常是工程师和管理层关心的问题。应该将其与选择编程语言或框架一样作为应用程序的风险来权衡。编程语言和云提供商的选择是锁定的形式，工程师有责任了解风险并评估这些风险是否可以接受。\n当您选择供应商或技术时，请记住以下几点：\n锁定是不可避免的。 锁定是一种风险，但并不总是很高。 不要外包思维。 锁定是不可避免的 在技术上有两种类型的锁定：\n技术锁定\n整个开发技术栈中底层技术\n供应商锁定\n大多数情况下，作为项目的一部分而使用的服务和软件（供应商锁定还可能包括硬件和操作系统，但我们只关注服务）\n技术锁定 开发人员将选择他们熟悉的技术或为正在开发的应用程序提供最大利益的技术。这些技术可以是供应商提供的技术（例如.NET 和 Oracle 数据库）到开源软件（例如 Python 和 PostgreSQL）。\n在此级别提供的锁定通常要求符合 API 或规范，这将影响应用程序的开发。也可以选择一些替代技术，但是这通常有很高的转换成本，因为技术对应用程序的设计有很大影响。\n供应商锁定 供应商，如云提供商，是另一种不同形式的锁 …","relpermalink":"/cloud-native-infra/appendix-b-lock-in/","summary":"关于使用云提供商和避免供应商锁定存在很多争议。这种辩论充满了意识心态之争。 锁定通常是工程师和管理层关心的问题。应该将其与选择编程语言或框架一样作为应用程序的风险来权衡。编程语言和云提供商的选择是锁定的","title":"附录 B：锁定"},{"content":"下面是一个使用只读根文件系统的 Kubernetes 部署模板的例子。\napiVersion: apps/v1 kind: Deployment metadata: labels: app: web name: web spec: selector: matchLabels: app: web template: metadata: labels: app: web name: web spec: containers: - command: [\u0026#34;sleep\u0026#34;] args: [\u0026#34;999\u0026#34;] image: ubuntu:latest name: web securityContext: readOnlyRootFilesystem: true #使容器的文件系统成为只读 volumeMounts: - mountPath: /writeable/location/here #创建一个可写卷 name: volName volumes: - emptyDir: {} name: volName ","relpermalink":"/kubernetes-hardening-guidance/appendix/b/","summary":"下面是一个使用只读根文件系统的 Kubernetes 部署模板的例子。 apiVersion: apps/v1 kind: Deployment metadata: labels: app: web name: web spec: selector: matchLabels: app: web template: metadata: labels: app: web name: web spec: containers: - command: [\"sleep\"] args: [\"999\"] image: ubuntu:latest name: web securityContext: readOnlyRootFilesystem: true #使容器的文件系统成为只读 volumeMounts: - mountPath: /writeable/location/here #创建一个可写卷 name: volName volumes: - emptyDir: {} name: volName","title":"附录 B：只读文件系统的部署模板示例"},{"content":"这份文档将覆盖如何迁移使用 tctl 的 TSB 的实时安装，并迁移到 Helm。文档假设 Helm 已经安装 在系统中。\n在开始之前，请确保你：\n熟悉 TSB 概念 安装了 TSB 环境。你可以使用 TSB 演示 进行快速安装。 完成了 TSB 使用快速入门。 准备 Helm Chart 在进行之前，你必须熟悉 Helm。请按照我们的指南中的 先决条件 安装 TSB 与 Helm。\n迁移管理平面 迁移当前的安装只需要标记和注释平面安装的资源。所有其他组件将由 tsb-operator 升级和管理。以下是标记每个将由 Helm 管理的资源的命令列表。\nkubectl -n tsb label deployment tsb-operator-management-plane \u0026#34;app.kubernetes.io/managed-by=Helm\u0026#34; kubectl -n tsb annotate deployment tsb-operator-management-plane \u0026#34;meta.helm.sh/release-name=mp\u0026#34; …","relpermalink":"/tsb/setup/migrate-tctl-to-helm/","summary":"这份文档将覆盖如何迁移使用 tctl 的 TSB 的实时安装，并迁移到 Helm。文档假设 Helm 已经安装 在系统中。 在开始之前，请确保你： 熟悉 TSB 概念 安装了 TSB 环境。你可以使用 TSB 演示 进行快速安装。 完成了 TSB 使用快速入门。 准备 Helm Chart 在进","title":"从 tctl 迁移到 Helm"},{"content":"应用程序入口是一个 L7 入口，允许应用程序开发人员直接利用可用的 Envoy 功能。 与 TSB 中其他类型的 Ingress 不同，配置它不需要管理员权限， 并以一种使应用程序开发人员更容易定义意图的方式公开 Envoy 代理的功能。\n应用程序入口是 Istio 的一个简化版本，使用 Istiod 作为控制平面组件和 Istio IngressGateway（即 Envoy 代理）作为数据平面组件。应用程序入口在由应用程序拥有的命名空间中为每个应用程序部署，并只能使用来自其部署的命名空间的 Istio 配置。\n应用程序入口还具有一个 OpenAPI 翻译器附加组件，允许用户使用 OpenAPI 规范配置入口。\n此功能需要 tctl 版本 1.4.5 或更高版本。\n使用 Istio 进行配置 在此示例中，你将在一个命名空间中安装 httpbin，并在相同命名空间中创建一个应用程序入口，以路由访问 httpbin 工作负载。\n创建名为 httpbin-appingress 的命名空间。\nkubectl create namespace httpbin-appingress 你将在同一个 …","relpermalink":"/tsb/howto/gateway/app-ingress/","summary":"应用程序入口是一个 L7 入口，允许应用程序开发人员直接利用可用的 Envoy 功能。 与 TSB 中其他类型的 Ingress 不同，配置它不需要管理员权限， 并以一种使应用程序开发人员更容易定义意图的方式公开 Envoy 代理的功能。 应用程序入口是 Istio 的一","title":"应用程序入口"},{"content":"本文介绍了在使用 Argo Rollouts 时的一些最佳实践、技巧和窍门。\nIngress 目标/稳定主机路由 出于各种原因，通常希望外部服务能够访问预期的 pod（即 canary/preview）或特定的稳定 pod，而不会将流量任意分配给两个版本。一些使用场景包括：\n新版本的服务可以在内部/私有环境中访问（例如进行手动验证），然后再将其外部公开。 外部 CI/CD 管道在将蓝/绿预览堆栈升级到生产环境之前运行测试。 运行比较旧版本和新版本行为的测试。 如果使用 Ingress 来将流量路由到服务，则可以添加其他主机规则到 Ingress 规则中，以便能够特别到达期望的（canary/preview）pod 或稳定的 pod。\napiVersion：networking.k8s.io/v1beta1 kind：Ingress metadata： name：guestbook spec： rules： ＃仅到达所需的Pod（也称为金丝雀/预览）的主机规则 -主机：guestbook-desired.argoproj.io http： paths： -后端： …","relpermalink":"/argo-rollouts/best-practices/","summary":"本文介绍了在使用 Argo Rollouts 时的一些最佳实践、技巧和窍门。 Ingress 目标/稳定主机路由 出于各种原因，通常希望外部服务能够访问预期的 pod（即 canary/preview）或特定的稳定 pod，而不会将流量任意分配给两个","title":"最佳实践"},{"content":"该特性在自 Kubernetes 1.6 版本推出 beta 版本。Init 容器可以在 PodSpec 中同应用程序的 containers 数组一起来指定。此前 beta 注解的值仍将保留，并覆盖 PodSpec 字段值。\n本文讲解 Init 容器的基本概念，这是一种专用的容器，在应用程序容器启动之前运行，用来包含一些应用镜像中不存在的实用工具或安装脚本。\n理解 Init 容器 Pod 能够具有多个容器，应用运行在容器里面，但是它也可能有一个或多个先于应用容器启动的 Init 容器。\nInit 容器与普通的容器非常像，除了如下两点：\nInit 容器总是运行到成功完成为止。 每个 Init 容器都必须在下一个 Init 容器启动之前成功完成。 如果 Pod 的 Init 容器失败，Kubernetes 会不断地重启该 Pod，直到 Init 容器成功为止。然而，如果 Pod 对应的 restartPolicy 为 Never，它不会重新启动。\n指定容器为 Init 容器，在 PodSpec 中添加 initContainers 字段，以 v1.Container 类型对象的 JSON …","relpermalink":"/kubernetes-handbook/objects/init-containers/","summary":"该特性在自 Kubernetes 1.6 版本推出 beta 版本。Init 容器可以在 PodSpec 中同应用程序的 containers 数组一起来指定。此前 beta 注解的值仍将保留，并覆盖 PodSpec 字段值。 本文讲解 Init 容器的基本概念，这是一种专用的容器，在应用程序容器启动之前运行，用来","title":"Init 容器"},{"content":" 以下内容最初由 CNCF 发布在 Kubernetes.io 上，并且在此获得许可。\n在 2014 年夏天，Box 对沉淀了十年的硬件和软件基础架构的痛苦，这无法与公司的需求保持一致。\n该平台为超过 5000 万用户（包括政府和大型企业如通用电气公司）管理和共享云中的内容，Box 最初是一个使用 PHP 写的具有数百万行的庞大代码，内置裸机数据中心。它已经开始将单体应用分解成微服务。 “随着我们扩展到全球各地，公有云战争正在升温，我们开始专注于如何在许多不同的环境和许多不同的云基础架构提供商之间运行我们的工作负载”，Box 联合创始人和服务架构师 Sam Ghods 说。 “迄今为止，这是一个巨大的挑战，因为所有这些不同的提供商，特别是裸机，都有非常不同的接口和与合作方式。”\n当 Ghods 参加 DockerCon 时，Box 的云原生之旅加速了。该公司已经认识到，它不能再仅仅使用裸机来运行应用程序，正在研究 Docker 容器化，使用 OpenStack 进行虚拟化以及支持公有云。\n在那次会议上，Google 宣布发布 Kubernetes 容器管理系统，Ghods 成功了。 “ …","relpermalink":"/cloud-native-infra/appendix-c-box-case-study/","summary":"以下内容最初由 CNCF 发布在 Kubernetes.io 上，并且在此获得许可。 在 2014 年夏天，Box 对沉淀了十年的硬件和软件基础架构的痛苦，这无法与公司的需求保持一致。 该平台为超过 5000 万用户（包括政府和大型企业如通用电气公司）管理和共享云","title":"附录 C Box：案例研究"},{"content":"下面是一个 Kubernetes Pod 安全策略的例子，它为集群中运行的容器执行了强大的安全要求。这个例子是基于官方的 Kubernetes 文档。我们鼓励管理员对该策略进行修改，以满足他们组织的要求。\napiVersion: policy/v1beta1 kind: PodSecurityPolicy metadata: name: restricted annotations: seccomp.security.alpha.kubernetes.io/allowedProfileNames: \u0026#39;docker/default,runtime/default\u0026#39; apparmor.security.beta.kubernetes.io/allowedProfileNames: \u0026#39;runtime/default\u0026#39; seccomp.security.alpha.kubernetes.io/defaultProfileName: \u0026#39;runtime/default\u0026#39; apparmor.security.beta.kubernetes.io/defaultProfileName: …","relpermalink":"/kubernetes-hardening-guidance/appendix/c/","summary":"下面是一个 Kubernetes Pod 安全策略的例子，它为集群中运行的容器执行了强大的安全要求。这个例子是基于官方的 Kubernetes 文档。我们鼓励管理员对该策略进行修改，以满足他们组织的要求。 apiVersion: policy/v1beta1 kind: PodSecurityPolicy metadata: name: restricted annotations: seccomp.security.alpha.kubernetes.io/allowedProfileNames: 'docker/default,runtime/default' apparmor.security.beta.kubernetes.io/allowedProfileNames: 'runtime/default' seccomp.security.alpha.kubernetes.io/defaultProfileName: 'runtime/default' apparmor.security.beta.kubernetes.io/defaultProfileName: 'runtime/default' spec: privileged: false # 需要防","title":"附录 C：Pod 安全策略示例"},{"content":"忽略无关资源 v1.1\n在某些情况下，你可能希望从应用程序的整体同步状态中排除资源。例如。如果它们是由工具生成的。这可以通过在你想要排除的资源上添加此注释来完成：\nmetadata: annotations: argocd.argoproj.io/compare-options: IgnoreExtraneous 对比选项需要修整 🔔 提示：这仅影响同步状态。如果资源的运行状况降级，那么应用程序也会降级。\nKustomize 具有允许你生成配置映射的功能（了解更多）。你可以设置 generatorOptions 添加此注释，以便你的应用保持同步：\nconfigMapGenerator: - name: my-map literals: - foo=bar generatorOptions: annotations: argocd.argoproj.io/compare-options: IgnoreExtraneous kind: Kustomization 🔔 提示：generatorOptions 向配置映射和秘密添加注释（了解更多）。\n你可能希望将其与 Prune=false 同 …","relpermalink":"/argo-cd/user-guide/compare-options/","summary":"忽略无关资源 v1.1 在某些情况下，你可能希望从应用程序的整体同步状态中排除资源。例如。如果它们是由工具生成的。这可以通过在你想要排除的资源上添加此注释来完成： metadata: annotations: argocd.argoproj.io/compare-options: IgnoreExtraneous 对比选项需要修整 🔔 提示：这仅影响同步状态","title":"对比选项"},{"content":"一般问题 Argo Rollouts 是依赖 Argo CD 或其他 Argo 项目吗？ Argo Rollouts 是一个独立的项目。虽然它与 Argo CD 和其他 Argo 项目配合使用效果很好，但它也可以单独用于渐进式交付场景。更具体地说，Argo Rollouts 不需要你也在同一集群上安装 Argo CD。\nArgo Rollouts 如何与 Argo CD 集成？ 通过 Argo CD 的 Lua 健康检查，Argo CD 可以了解 Argo Rollouts 资源的健康状况。这些健康检查了解 Argo Rollout 对象何时在进展、暂停、退化或健康。此外，Argo CD 具有基于 Lua 的资源操作，可以改变 Argo Rollouts 资源（例如取消暂停 Rollout）。\n因此，操作员可以构建自动化程序以反应 Argo Rollouts 资源的状态。例如，如果 Argo CD 创建的 Rollout 被暂停，Argo CD 会检测到并将该应用程序标记为暂停。一旦确定新版本是好的，操作员就可以使用 Argo CD 的 resume …","relpermalink":"/argo-rollouts/faq/","summary":"一般问题 Argo Rollouts 是依赖 Argo CD 或其他 Argo 项目吗？ Argo Rollouts 是一个独立的项目。虽然它与 Argo CD 和其他 Argo 项目配合使用效果很好，但它也可以单独用于渐进式交付场景。更具体地说，Argo Rollouts 不需要你也在同一集群上安装 Argo CD。 Argo Rollouts 如何","title":"FAQ"},{"content":"Pause 容器，又叫 Infra 容器，本文将探究该容器的作用与原理。\n我们知道在 kubelet 的配置中有这样一个参数：\nKUBELET_POD_INFRA_CONTAINER=--pod-infra-container-image=registry.access.redhat.com/rhel7/pod-infrastructure:latest 上面是 openshift 中的配置参数，kubernetes 中默认的配置参数是：\nKUBELET_POD_INFRA_CONTAINER=--pod-infra-container-image=gcr.io/google_containers/pause-amd64:3.0 Pause 容器，是可以自己来定义，官方使用的 gcr.io/google_containers/pause-amd64:3.0 容器的代码见 Github，使用 C 语言编写。\nPause 容器特点 镜像非常小，目前在 700KB 左右 永远处于 Pause (暂停) 状态 Pause 容器背景 像 Pod 这样一个东西，本身是一个逻辑概念。那在机器上，它究 …","relpermalink":"/kubernetes-handbook/objects/pause-container/","summary":"Pause 容器，又叫 Infra 容器，本文将探究该容器的作用与原理。 我们知道在 kubelet 的配置中有这样一个参数： KUBELET_POD_INFRA_CONTAINER=--pod-infra-container-image=registry.access.redhat.com/rhel7/pod-infrastructure:latest 上面是 openshift 中的配置参数，kubernetes 中默认的配置参数是： KUBELET_POD_INFRA_CONTAINER=--pod-infra-container-image=gcr.io/google_containers/pause-amd64:3.0 Pause 容器，是可以自己来定义，官方使用的 gcr.io/google_containers/pause-amd64:3.0 容器的代码","title":"Pause 容器"},{"content":"下面的例子是为每个团队或用户组，可以使用 kubectl 命令或 YAML 文件创建一个 Kubernetes 命名空间。应避免使用任何带有 kube 前缀的名称，因为它可能与 Kubernetes 系统保留的命名空间相冲突。\nKubectl 命令来创建一个命名空间。\nkubectl create namespace \u0026lt;insert-namespace-name-here\u0026gt; 要使用 YAML 文件创建命名空间，创建一个名为 my-namespace.yaml 的新文件，内容如下：\napiVersion: v1 kind: Namespace metadata: name: \u0026lt;insert-namespace-name-here\u0026gt; 应用命名空间，使用：\nkubectl create –f ./my-namespace.yaml 要在现有的命名空间创建新的 Pod，请切换到所需的命名空间：\nkubectl config use-context \u0026lt;insert-namespace-here\u0026gt; 应用新的 Deployment，使用：\nkubectl apply -f …","relpermalink":"/kubernetes-hardening-guidance/appendix/d/","summary":"下面的例子是为每个团队或用户组，可以使用 kubectl 命令或 YAML 文件创建一个 Kubernetes 命名空间。应避免使用任何带有 kube 前缀的名称，因为它可能与 Kubernetes 系统保留的命名空间相冲突。 Kubectl 命令来创建一个命名空间。 kubectl create namespace \u003cinsert-namespace-name-here\u003e 要使用 YAML 文件创建命名","title":"附录 D：命名空间示例"},{"content":"报告漏洞 如果你在 Argo Rollouts 中发现与安全相关的错误，我们恳请你负责任地进行披露，并给我们适当的时间来反应、分析和开发修复程序，以减轻发现的安全漏洞。\n请通过电子邮件将漏洞报告至以下地址：\ncncf-argo-security@lists.cncf.io 所有漏洞和相关信息都将得到完全保密。\n公开披露 我们将使用 GitHub 安全建议功能发布安全建议，以使我们的社区充分了解情况，并将感谢你的发现（当然，除非你愿意保持匿名）。\nInternet Bug Bounty 合作 我们很高兴地宣布，Argo 项目正在与 Hacker One 及其互联网错误赏金计划的优秀人员合作，以奖励在四个主要 Argo 项目（CD、活动、推出和工作流程）中发现安全漏洞的优秀人员然后与我们一起以负责任的方式修复和披露它们。\n如果你按照本安全政策中的规定向我们报告漏洞，我们将与你共同确定你的发现是否符合领取赏金的条件，以及如何领取赏金。\n","relpermalink":"/argo-rollouts/security/","summary":"报告漏洞 如果你在 Argo Rollouts 中发现与安全相关的错误，我们恳请你负责任地进行披露，并给我们适当的时间来反应、分析和开发修复程序，以减轻发现的安全漏洞。 请通过电子邮件将漏洞报告至以下地址： cncf-argo-security@lists.cncf.io 所有漏洞和相关信息都将","title":"Argo Rollouts 的安全策略"},{"content":"PodSecurityPolicy 类型的对象能够控制，是否可以向 Pod 发送请求，该 Pod 能够影响被应用到 Pod 和容器的 SecurityContext。\n什么是 Pod 安全策略？ Pod 安全策略 是集群级别的资源，它能够控制 Pod 运行的行为，以及它具有访问什么的能力。 PodSecurityPolicy对象定义了一组条件，指示 Pod 必须按系统所能接受的顺序运行。它们允许管理员控制如下方面：\n控制面 字段名称 已授权容器的运行 privileged 为容器添加默认的一组能力 defaultAddCapabilities 为容器去掉某些能力 requiredDropCapabilities 容器能够请求添加某些能力 allowedCapabilities 控制卷类型的使用 volumes 主机网络的使用 hostNetwork 主机端口的使用 hostPorts 主机 PID namespace 的使用 hostPID 主机 IPC namespace 的使用 hostIPC 主机路径的使用 allowedHostPaths 容器的 SELinux …","relpermalink":"/kubernetes-handbook/objects/pod-security-policy/","summary":"PodSecurityPolicy 类型的对象能够控制，是否可以向 Pod 发送请求，该 Pod 能够影响被应用到 Pod 和容器的 SecurityContext。 什么是 Pod 安全策略？ Pod 安全策略 是集群级别的资源，它能够控制 Pod 运行的行为，以及它具有访问什么的能力","title":"Pod 安全策略"},{"content":"网络策略根据使用的网络插件而不同。下面是一个网络策略的例子，参考 Kubernetes 文档将 nginx 服务的访问限制在带有标签访问的 Pod 上。\napiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: example-access-nginx namespace: prod #这可以是任何一个命名空间，或者在不使用命名空间的情况下省略。 spec: podSelector: matchLabels: app: nginx ingress: - from: - podSelector: matchLabels: access: \u0026#34;true\u0026#34; 新的 NetworkPolicy 可以通过以下方式应用：\nkubectl apply -f policy.yaml 一个默认的拒绝所有入口的策略：\napiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: deny-all-ingress spec: podSelector: {} …","relpermalink":"/kubernetes-hardening-guidance/appendix/e/","summary":"网络策略根据使用的网络插件而不同。下面是一个网络策略的例子，参考 Kubernetes 文档将 nginx 服务的访问限制在带有标签访问的 Pod 上。 apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: example-access-nginx namespace: prod #这可以是任何一个命名空间，或者在不使用命名空间的情况下省略。 spec: podSelector: matchLabels: app: nginx","title":"附录 E：网络策略示例"},{"content":"Argo CD 允许用户定制同步目标集群中所需状态的某些方面。某些同步选项可以定义为特定资源中的注释。大多数同步选项在应用程序资源 spec.syncPolicy.syncOptions 属性中配置。使用 argocd.argoproj.io/sync-options 注释配置的多个同步选项可以在注释值中使用 , 进行连接；空格将被删除。\n下面你可以找到有关每个可用同步选项的详细信息：\n无修整资源 v1.1\n你可能希望防止修整对象：\nmetadata: annotations: argocd.argoproj.io/sync-options: Prune=false 在 UI 中，Pod 将仅显示为不同步：\n同步选项无修整 同步状态面板显示跳过修整的原因：\n同步选项无修正 如果 Argo CD 期望剪切资源，则应用程序将失去同步。你可能希望与 比较选项 结合使用。\n禁用 Kubectl 验证 对于某些对象类，需要使用 --validate=false 标志使用 kubectl apply 将其应用。例如使用 RawExtension 的 Kubernetes 类型， …","relpermalink":"/argo-cd/user-guide/sync-options/","summary":"Argo CD 允许用户定制同步目标集群中所需状态的某些方面。某些同步选项可以定义为特定资源中的注释。大多数同步选项在应用程序资源 spec.syncPolicy.syncOptions 属性中配置。使用 argocd.argoproj.io/sync-options 注释配置的多个同步选项可以在注释值中使用 , 进行连接；空格将被删","title":"同步选项 "},{"content":"本文讲解的是 Kubernetes 中 Pod 的生命周期，包括生命周期的不同阶段、存活和就绪探针、重启策略等。\nPod phase Pod 的 status 字段是一个 PodStatus 对象，PodStatus 中有一个 phase 字段。\nPod 的相位（phase）是 Pod 在其生命周期中的简单宏观概述。该字段并不是对容器或 Pod 的综合汇总，也不是为了做为综合状态机。\nPod 相位的数量和含义是严格指定的。除了本文档中列举的状态外，不应该再假定 Pod 有其他的 phase 值。\n下面是 phase 可能的值：\n挂起（Pending）：Pod 已被 Kubernetes 系统接受，但有一个或者多个容器镜像尚未创建。等待时间包括调度 Pod 的时间和通过网络下载镜像的时间，这可能需要花点时间。 运行中（Running）：该 Pod 已经绑定到了一个节点上，Pod 中所有的容器都已被创建。至少有一个容器正在运行，或者正处于启动或重启状态。 成功（Succeeded）：Pod 中的所有容器都被成功终止，并且不会再重启。 失败（Failed）：Pod 中的所有容器都已终止了，并 …","relpermalink":"/kubernetes-handbook/objects/pod-lifecycle/","summary":"本文讲解的是 Kubernetes 中 Pod 的生命周期，包括生命周期的不同阶段、存活和就绪探针、重启策略等。 Pod phase Pod 的 status 字段是一个 PodStatus 对象，PodStatus 中有一个 phase 字段。 Pod 的相位（phase）是 Pod 在其生命周期中的简单宏观概述。","title":"Pod 的生命周期"},{"content":"在 Kubernetes 1.10 和更新版本中，LimitRange 支持被默认启用。下面的 YAML 文件为每个容器指定了一个 LimitRange，其中有一个默认的请求和限制，以及最小和最大的请求。\napiVersion: v1 kind: LimitRange metadata: name: cpu-min-max-demo-lr spec: limits - default: cpu: 1 defaultRequest: cpu: 0.5 max: cpu: 2 min: cpu 0.5 type: Container LimitRange 可以应用于命名空间，使用：\nkubectl apply -f \u0026lt;example-LimitRange\u0026gt;.yaml --namespace=\u0026lt;Enter-Namespace\u0026gt; 在应用了这个 LimitRange 配置的例子后，如果没有指定，命名空间中创建的所有容器都会被分配到默认的 CPU 请求和限制。命名空间中的所有容器的 CPU 请求必须大于或等于最小值，小于或等于最大 CPU 值，否则容器将不会被实例化。\n","relpermalink":"/kubernetes-hardening-guidance/appendix/f/","summary":"在 Kubernetes 1.10 和更新版本中，LimitRange 支持被默认启用。下面的 YAML 文件为每个容器指定了一个 LimitRange，其中有一个默认的请求和限制，以及最小和最大的请求。 apiVersion: v1 kind: LimitRange metadata: name: cpu-min-max-demo-lr spec: limits - default: cpu: 1 defaultRequest: cpu: 0.5 max: cpu: 2 min:","title":"附录 F：LimitRange 示例"},{"content":"Pod Hook（钩子）是由 Kubernetes 管理的 kubelet 发起的，当容器中的进程启动前或者容器中的进程终止之前运行，这是包含在容器的生命周期之中。可以同时为 Pod 中的所有容器都配置 hook。\nHook 的类型包括两种：\nexec：执行一段命令 HTTP：发送 HTTP 请求。 参考下面的配置：\napiVersion: v1 kind: Pod metadata: name: lifecycle-demo spec: containers: - name: lifecycle-demo-container image: nginx lifecycle: postStart: exec: command: [\u0026#34;/bin/sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;echo Hello from the postStart handler\u0026gt; /usr/share/message\u0026#34;] preStop: exec: command: [\u0026#34;/usr/sbin/nginx\u0026#34;,\u0026#34;-s\u0026#34;,\u0026#34;quit\u0026#34;] Kubernetes 在容器创建后立即发送 postStart 事件。但是，不能保证在调用容器的 …","relpermalink":"/kubernetes-handbook/objects/pod-hook/","summary":"Pod Hook（钩子）是由 Kubernetes 管理的 kubelet 发起的，当容器中的进程启动前或者容器中的进程终止之前运行，这是包含在容器的生命周期之中。可以同时为 Pod 中的所有容器都配置 hook。 Hook 的类型包括两种： exec：执行一段命令","title":"Pod Hook"},{"content":"通过将 YAML 文件应用于命名空间或在 Pod 的配置文件中指定要求来创建 ResourceQuota 对象，以限制命名空间内的总体资源使用。下面的例子是基于 Kubernetes 官方文档的一个命名空间的配置文件示例：\napiVersion: v1 kind: ResourceQuota metadata: name: example-cpu-mem-resourcequota spec: hard: requests.cpu: \u0026#34;1\u0026#34; requests.memory: 1Gi limits.cpu: \u0026#34;2\u0026#34; limits.memory: 2Gi 可以这样应用这个 ResourceQuota：\nkubectl apply -f example-cpu-mem-resourcequota.yaml -- namespace=\u0026lt;insert-namespace-here\u0026gt; 这个 ResourceQuota 对所选择的命名空间施加了以下限制：\n每个容器都必须有一个内存请求、内存限制、CPU 请求和 CPU 限制。 所有容器的总内存请求不应超过 1 GiB 所有容器的总内存限制不应超过 2 …","relpermalink":"/kubernetes-hardening-guidance/appendix/g/","summary":"通过将 YAML 文件应用于命名空间或在 Pod 的配置文件中指定要求来创建 ResourceQuota 对象，以限制命名空间内的总体资源使用。下面的例子是基于 Kubernetes 官方文档的一个命名空间的配置文件示例： apiVersion: v1 kind: ResourceQuota metadata: name: example-cpu-mem-resourcequota spec: hard: requests.cpu: \"1\" requests.memory: 1Gi limits.cpu: \"2\" limits.memory: 2Gi 可以这样应用","title":"附录 G：ResourceQuota 示例"},{"content":"以下环境变量可与 argocd CLI 一起使用：\n环境变量 描述 ARGOCD_SERVER 不带 https:// 前缀的 ArgoCD 服务器地址（而不是为每个命令指定 --server ）例如：ARGOCD_SERVER=argocd.mycompany.com 如果通过 DNS 入口提供服务 ARGOCD_AUTH_TOKEN ArgoCD apiKey 以便你的 ArgoCD 用户能够进行身份验证 ARGOCD_OPTS 传递到 argocd CLI 的命令行选项，例如 ARGOCD_OPTS=\u0026#34;--grpc-web\u0026#34; ","relpermalink":"/argo-cd/user-guide/environment-variables/","summary":"以下环境变量可与 argocd CLI 一起使用： 环境变量 描述 ARGOCD_SERVER 不带 https:// 前缀的 ArgoCD 服务器地址（而不是为每个命令指定 --server ）例如：ARGOCD_SERVER=argocd.mycompany.com 如果通过 DNS 入口提供服务 ARGOCD_AUTH_TOKEN ArgoCD apiKey 以便","title":"环境变量"},{"content":"Preset 就是预设，有时候想要让一批容器在启动的时候就注入一些信息，比如 secret、volume、volume mount 和环境变量，而又不想一个一个的改这些 Pod 的 template，这时候就可以用到 PodPreset 这个资源对象了。\n本页是关于 PodPreset 的概述，该对象用来在 Pod 创建的时候向 Pod 中注入某些特定信息。该信息可以包括 secret、volume、volume mount 和环境变量。\n理解 Pod Preset Pod Preset 是用来在 Pod 被创建的时候向其中注入额外的运行时需求的 API 资源。\n您可以使用 label selector 来指定为哪些 Pod 应用 Pod Preset。\n使用 Pod Preset 使得 pod 模板的作者可以不必为每个 Pod 明确提供所有信息。这样一来，pod 模板的作者就不需要知道关于该服务的所有细节。\n关于该背景的更多信息，请参阅 PodPreset 的设计方案。\n如何工作 Kubernetes 提供了一个准入控制器（PodPreset），当其启用时，Pod Preset 会将 …","relpermalink":"/kubernetes-handbook/objects/pod-preset/","summary":"Preset 就是预设，有时候想要让一批容器在启动的时候就注入一些信息，比如 secret、volume、volume mount 和环境变量，而又不想一个一个的改这些 Pod 的 template，这时候就可以用到 PodPreset 这个资源对象了。 本","title":"Pod Preset"},{"content":"要对秘密数据进行静态加密，下面的加密配置文件提供了一个例子，以指定所需的加密类型和加密密钥。将加密密钥存储在加密文件中只能稍微提高安全性。Secret 将被加密，但密钥将在 EncryptionConfiguration 文件中被访问。这个例子是基于 Kubernetes 的官方文档。\napiVersion: apiserver.config.k8s.io/v1 kind: EncryptionConfiguration resources: - resources: - secrets providers: - aescbc: keys: - name: key1 secret: \u0026lt;base 64 encoded secret\u0026gt; - identity: {} 要使用该加密文件进行静态加密，请在重启 API 服务器时设置 --encryption-provider-config 标志，并注明配置文件的位置。\n","relpermalink":"/kubernetes-hardening-guidance/appendix/h/","summary":"要对秘密数据进行静态加密，下面的加密配置文件提供了一个例子，以指定所需的加密类型和加密密钥。将加密密钥存储在加密文件中只能稍微提高安全性。Secret 将被加密，但密钥将在 EncryptionConfiguration 文件中被访问。这个例子是基于","title":"附录 H：加密示例"},{"content":"这篇文档适用于要构建高可用应用程序的所有者，因此他们需要了解 Pod 可能发生什么类型的中断。也适用于要执行自动集群操作的集群管理员，如升级和集群自动扩容。\n自愿中断和非自愿中断 Pod 不会消失，直到有人（人类或控制器）将其销毁，或者当出现不可避免的硬件或系统软件错误。\n我们把这些不可避免的情况称为应用的非自愿性中断。例如：\n后端节点物理机的硬件故障 集群管理员错误地删除虚拟机（实例） 云提供商或管理程序故障使虚拟机消失 内核恐慌（kernel panic） 节点由于集群网络分区而从集群中消失 由于节点资源不足而将容器逐出 除资源不足的情况外，大多数用户应该都熟悉以下这些情况；它们不是特定于 Kubernetes 的。\n我们称这些情况为”自愿中断“。包括由应用程序所有者发起的操作和由集群管理员发起的操作。典型的应用程序所有者操作包括：\n删除管理该 pod 的 Deployment 或其他控制器 更新了 Deployment 的 pod 模板导致 pod 重启 直接删除 pod（意外删除） 集群管理员操作包括：\n排空（drain）节点进行修复或升级。 从集群中排空节点以缩小集群。 从节 …","relpermalink":"/kubernetes-handbook/objects/pod-disruption-budget/","summary":"这篇文档适用于要构建高可用应用程序的所有者，因此他们需要了解 Pod 可能发生什么类型的中断。也适用于要执行自动集群操作的集群管理员，如升级和集群自动扩容。 自愿中断和非自愿中断 Pod 不会消失，直到有人（人类或控制","title":"Pod 中断与 PDB（Pod 中断预算）"},{"content":"要用密钥管理服务（KMS）提供商插件来加密 Secret，可以使用以下加密配置 YAML 文件的例子来为提供商设置属性。这个例子是基于 Kubernetes 的官方文档。\napiVersion: apiserver.config.k8s.io/v1 kind: EncryptionConfiguration resources: - resources: - secrets providers: - kms: name: myKMSPlugin endpoint: unix://tmp/socketfile.sock cachesize: 100 timeout: 3s - identity: {} 要配置 API 服务器使用 KMS 提供商，请将 --encryption-provider-config 标志与配置文件的位置一起设置，并重新启动 API 服务器。\n要从本地加密提供者切换到 KMS，请将 EncryptionConfiguration 文件中的 KMS 提供者部分添加到当前加密方法之上，如下所示。\napiVersion: …","relpermalink":"/kubernetes-hardening-guidance/appendix/i/","summary":"要用密钥管理服务（KMS）提供商插件来加密 Secret，可以使用以下加密配置 YAML 文件的例子来为提供商设置属性。这个例子是基于 Kubernetes 的官方文档。 apiVersion: apiserver.config.k8s.io/v1 kind: EncryptionConfiguration resources: - resources: - secrets providers: - kms: name: myKMSPlugin endpoint: unix://tmp/socketfile.sock cachesize: 100 timeout: 3s - identity: {} 要配置 API 服务器使用","title":"附录 I：KMS 配置实例"},{"content":"众所周知，Kubernetes 是 Google 于 2014 年 6 月基于其内部使用的 Borg 系统开源出来的容器编排调度引擎。其实从 2000 年开始，Google 就开始基于容器研发三个容器管理系统，分别是 Borg、Omega 和 Kubernetes。这篇由 Google 工程师 Brendan Burns、Brian Grant、David Oppenheimer、Eric Brewer 和 John Wilkes 几人在 2016 年发表的《Borg, Omega, and Kubernetes》论文里，阐述了 Google 从 Borg 到 Kubernetes 这个旅程中所获得知识和经验教训。\nBorg、Omega 和 Kubernetes Google 从 2000 年初就开始使用容器（Linux 容器）系统，Google 开发出来的第一个统一的容器管理系统在内部称之为“Borg”，用来管理长时间运行的生产服务和批处理服务。由于 Borg 的规模、功能的广泛性和超高的稳定性，一直到现在 Borg 在 Google 内部依然是主要的容器管理系统。\nGoogle 的 …","relpermalink":"/cloud-native-handbook/kubernetes/history/","summary":"众所周知，Kubernetes 是 Google 于 2014 年 6 月基于其内部使用的 Borg 系统开源出来的容器编排调度引擎。其实从 2000 年开始，Google 就开始基于容器研发三个容器管理系统，分别是 Borg、Omega 和 Kuberne","title":"Kubernetes 的历史"},{"content":"要创建一个 pod-reader 角色，创建一个 YAML 文件，内容如下：\napiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: namespace: your-namespace-name name: pod-reader rules: - apiGroups: [\u0026#34;\u0026#34;] # \u0026#34;\u0026#34; 表示核心 API 组 resources: [\u0026#34;pods\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;list\u0026#34;] 应用角色：\nkubectl apply --f role.yaml 要创建一个全局性的 pod-reader ClusterRole：\napiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: default # \u0026#34;namespace\u0026#34; 被省略了，因为 ClusterRoles 没有被绑定到一个命名空间上 name: global-pod-reader rules: - apiGroups: [\u0026#34;\u0026#34;] # \u0026#34;\u0026#34; 表示核心 API …","relpermalink":"/kubernetes-hardening-guidance/appendix/j/","summary":"要创建一个 pod-reader 角色，创建一个 YAML 文件，内容如下： apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: namespace: your-namespace-name name: pod-reader rules: - apiGroups: [\"\"] # \"\" 表示核心 API 组 resources: [\"pods\"] verbs: [\"get\", \"watch\", \"list\"] 应用角色： kubectl apply --f role.yaml 要创建一个全局性的 pod-reader ClusterRole： apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: default # \"namespace\" 被省略了，因为 ClusterRoles 没有被绑","title":"附录 J：pod-reader RBAC 角色"},{"content":"Node 是 Kubernetes 集群的工作节点，可以是物理机也可以是虚拟机。\nNode 的状态 Node 包括如下状态信息：\nAddress HostName：可以被 kubelet 中的 --hostname-override 参数替代。 ExternalIP：可以被集群外部路由到的 IP 地址。 InternalIP：集群内部使用的 IP，集群外部无法访问。 Condition OutOfDisk：磁盘空间不足时为 True Ready：Node controller 40 秒内没有收到 node 的状态报告为 Unknown，健康为 True，否则为 False。 MemoryPressure：当 node 有内存压力时为 True，否则为 False。 DiskPressure：当 node 有磁盘压力时为 True，否则为 False。 Capacity CPU 内存 可运行的最大 Pod 个数 Info：节点的一些版本信息，如 OS、kubernetes、docker 等 Node 管理 禁止 Pod 调度到该节点上。\nkubectl cordon \u0026lt;node\u0026gt; 驱逐该 …","relpermalink":"/kubernetes-handbook/cluster/node/","summary":"Node 是 Kubernetes 集群的工作节点，可以是物理机也可以是虚拟机。 Node 的状态 Node 包括如下状态信息： Address HostName：可以被 kubelet 中的 --hostname-override 参数替代。 ExternalIP：可以被集群外部路由到的 IP 地址。 InternalIP：集群","title":"Node"},{"content":"要创建一个 RoleBinding，需创建一个 YAML 文件，内容如下：\napiVersion: rbac.authorization.k8s.io/v1 # 这个角色绑定允许 \u0026#34;jane\u0026#34; 读取 \u0026#34;your-namespace-name\u0026#34; 的 Pod 命名空间 # 你需要在该命名空间中已经有一个名为 \u0026#34;pod-reader\u0026#34;的角色。 kind: RoleBinding metadata: name: read-pods namespace: your-namespace-name subjects: # 你可以指定一个以上的 \u0026#34;subject\u0026#34; - kind: User name: jane # \u0026#34;name\u0026#34; 是大小写敏感的 apiGroup: rbac.authorization.k8s.io roleRef: # \u0026#34;roleRef\u0026#34; 指定绑定到一个 Role/ClusterRole kind: Role # 必须是 Role 或 ClusterRole name: pod-reader # 这必须与你想绑定的 Role 或 ClusterRole …","relpermalink":"/kubernetes-hardening-guidance/appendix/k/","summary":"要创建一个 RoleBinding，需创建一个 YAML 文件，内容如下： apiVersion: rbac.authorization.k8s.io/v1 # 这个角色绑定允许 \"jane\" 读取 \"your-namespace-name\" 的 Pod 命名空间 # 你需要在该命名空间中已经有一个名为 \"pod-reader\"的角色。 kind: RoleBinding metadata: name: read-pods namespace:","title":"附录 K：RBAC RoleBinding 和 ClusterRoleBinding 示例"},{"content":"在一个 Kubernetes 集群中可以使用 namespace 创建多个“虚拟集群”，这些 namespace 之间可以完全隔离，也可以通过某种方式，让一个 namespace 中的 service 可以访问到其他的 namespace 中的服务。\n哪些情况下适合使用多个 namespace 因为 namespace 可以提供独立的命名空间，因此可以实现部分的环境隔离。当你的项目和人员众多的时候可以考虑根据项目属性，例如生产、测试、开发划分不同的 namespace。\nNamespace 使用 获取集群中有哪些 namespace\nkubectl get ns\n集群中默认会有 default 和 kube-system 这两个 namespace。\n在执行 kubectl 命令时可以使用 -n 指定操作的 namespace。\n用户的普通应用默认是在 default 下，与集群管理相关的为整个集群提供服务的应用一般部署在 kube-system 的 namespace 下，例如我们在安装 kubernetes 集群时部署的 kubedns、heapseter、EFK …","relpermalink":"/kubernetes-handbook/cluster/namespace/","summary":"在一个 Kubernetes 集群中可以使用 namespace 创建多个“虚拟集群”，这些 namespace 之间可以完全隔离，也可以通过某种方式，让一个 namespace 中的 service 可以访问到其他的 namespace 中的服务。 哪些情况下适合使用多个 namespace 因为 namespace 可以提供独立的命名空间，因此可以实现部","title":"Namespace"},{"content":"下面是一个审计策略，它以最高级别记录所有审计事件：\napiVersion: audit.k8s.io/v1 kind: Policy rules: - level: RequestResponse # 这个审计策略记录了 RequestResponse 级别的所有审计事件 这种审计策略在最高级别上记录所有事件。如果一个组织有可用的资源来存储、解析和检查大量的日志，那么在最高级别上记录所有事件是一个很好的方法，可以确保当事件发生时，所有必要的背景信息都出现在日志中。如果资源消耗和可用性是一个问题，那么可以建立更多的日志规则来降低非关键组件和常规非特权操作的日志级别，只要满足系统的审计要求。如何建立这些规则的例子可以在 Kubernetes 官方文档中找到。\n","relpermalink":"/kubernetes-hardening-guidance/appendix/l/","summary":"下面是一个审计策略，它以最高级别记录所有审计事件： apiVersion: audit.k8s.io/v1 kind: Policy rules: - level: RequestResponse # 这个审计策略记录了 RequestResponse 级别的所有审计事件 这种审计策略在最高级别上记录所有事件。如果一个组织有可用的资源来存储、解析和检查大量的日志，那","title":"附录 L：审计策略"},{"content":"选择性同步是仅同步某些资源的同步。你可以从 UI 中选择哪些资源：\n选择性同步 这样做时，请记住： 你的同步不会记录在历史记录中，因此无法回滚。 Hook 未运行。 选择性同步选项 v1.8\n打开选择性同步选项，该选项将仅同步不同步的资源。有关更多详细信息，请参阅同步选项文档。\n","relpermalink":"/argo-cd/user-guide/selective-sync/","summary":"选择性同步是仅同步某些资源的同步。你可以从 UI 中选择哪些资源： 选择性同步 这样做时，请记住： 你的同步不会记录在历史记录中，因此无法回滚。 Hook 未运行。 选择性同步选项 v1.8 打开选择性同步选项，该选项将仅同步不同步的","title":"选择性同步"},{"content":"Label 是附着到 object 上（例如 Pod）的键值对。可以在创建 object 的时候指定，也可以在 object 创建后随时指定。Labels 的值对系统本身并没有什么含义，只是对用户才有意义。\n\u0026#34;labels\u0026#34;: { \u0026#34;key1\u0026#34; : \u0026#34;value1\u0026#34;, \u0026#34;key2\u0026#34; : \u0026#34;value2\u0026#34; } Kubernetes 最终将对 labels 最终索引和反向索引用来优化查询和 watch，在 UI 和命令行中会对它们排序。不要在 label 中使用大型、非标识的结构化数据，记录这样的数据应该用 annotation。\n动机 Label 能够将组织架构映射到系统架构上（就像是康威定律），这样能够更便于微服务的管理，你可以给 object 打上如下类型的 label：\n\u0026#34;release\u0026#34; : \u0026#34;stable\u0026#34;, \u0026#34;release\u0026#34; : \u0026#34;canary\u0026#34; \u0026#34;environment\u0026#34; : \u0026#34;dev\u0026#34;, \u0026#34;environment\u0026#34; : \u0026#34;qa\u0026#34;, \u0026#34;environment\u0026#34; : \u0026#34;production\u0026#34; \u0026#34;tier\u0026#34; : \u0026#34;frontend\u0026#34;, \u0026#34;tier\u0026#34; : \u0026#34;backend\u0026#34;, …","relpermalink":"/kubernetes-handbook/cluster/label/","summary":"Label 是附着到 object 上（例如 Pod）的键值对。可以在创建 object 的时候指定，也可以在 object 创建后随时指定。Labels 的值对系统本身并没有什么含义，只是对用户才有意义。 \"labels\": { \"key1\" : \"value1\", \"key2\" : \"value2\" } Kubernetes 最终将对 labels 最终索引和反向索引用","title":"Label"},{"content":"在控制平面，用文本编辑器打开 kube-apiserver.yaml 文件。编辑 kube-apiserver 配置需要管理员权限。\nsudo vi /etc/kubernetes/manifests/kube-apiserver.yaml 在 kube-apiserver.yaml 文件中添加以下文字：\n--audit-policy-file=/etc/kubernetes/policy/audit-policy.yaml --audit-log-path=/var/log/audit.log --audit-log-maxage=1825 audit-policy-file 标志应该设置为审计策略的路径，而 audit-log-path 标志应该设置为所需的审计日志写入的安全位置。还有一些其他的标志，比如这里显示的 audit-log-maxage 标志，它规定了日志应该被保存的最大天数，还有一些标志用于指定要保留的最大审计日志文件的数量，最大的日志文件大小（兆字节）等等。启用日志记录的唯一必要标志是 audit-policy-file 和 audit-log-path 标志。其他 …","relpermalink":"/kubernetes-hardening-guidance/appendix/m/","summary":"在控制平面，用文本编辑器打开 kube-apiserver.yaml 文件。编辑 kube-apiserver 配置需要管理员权限。 sudo vi /etc/kubernetes/manifests/kube-apiserver.yaml 在 kube-apiserver.yaml 文件中添加以下文字： --audit-policy-file=/etc/kubernetes/policy/audit-policy.yaml --audit-log-path=/var/log/audit.log --audit-log-maxage=1825 audit-policy-file 标志应该设置为审计策略的路径，而 audit-log-path 标志应该设置为所需的审计日志写入的安全位置。还有一些其他的标志，比","title":"附录 M：向 kube-apiserver 提交审计策略文件的标志示例"},{"content":"Annotation，顾名思义，就是注解。Annotation 可以将 Kubernetes 资源对象关联到任意的非标识性元数据。使用客户端（如工具和库）可以检索到这些元数据。\n关联元数据到对象 Label 和 Annotation 都可以将元数据关联到 Kubernetes 资源对象。Label 主要用于选择对象，可以挑选出满足特定条件的对象。相比之下，annotation 不能用于标识及选择对象。annotation 中的元数据可多可少，可以是结构化的或非结构化的，也可以包含 label 中不允许出现的字符。\nAnnotation 和 label 一样都是 key/value 键值对映射结构：\n\u0026#34;annotations\u0026#34;: {\u0026#34;key1\u0026#34;:\u0026#34;value1\u0026#34;,\u0026#34;key2\u0026#34;:\u0026#34;value2\u0026#34;} 以下列出了一些可以记录在 annotation 中的对象信息：\n声明配置层管理的字段。使用 annotation 关联这类字段可以用于区分以下几种配置来源：客户端或服务器设置的默认值，自动生成的字段或自动生成的 auto-scaling 和 auto-sizing 系统配置的字段。\n创建信息、版 …","relpermalink":"/kubernetes-handbook/cluster/annotation/","summary":"Annotation，顾名思义，就是注解。Annotation 可以将 Kubernetes 资源对象关联到任意的非标识性元数据。使用客户端（如工具和库）可以检索到这些元数据。 关联元数据到对象 Label 和 Annotation 都可以将元数据关联到 Kubernetes 资源","title":"Annotation"},{"content":"YAML 文件示例：\napiVersion: v1 kind: Config preferences: {} clusters: - name: example-cluster cluster: server: http://127.0.0.1:8080 #web endpoint address for the log files to be sent to name: audit-webhook-service users: - name: example-users user: username: example-user password: example-password contexts: - name: example-context context: cluster: example-cluster user: example-user current-context: example-context #source: https://dev.bitolog.com/implement-audits-webhook/ 由 webhook 发送的审计事件是以 HTTP …","relpermalink":"/kubernetes-hardening-guidance/appendix/n/","summary":"YAML 文件示例： apiVersion: v1 kind: Config preferences: {} clusters: - name: example-cluster cluster: server: http://127.0.0.1:8080 #web endpoint address for the log files to be sent to name: audit-webhook-service users: - name: example-users user: username: example-user password: example-password contexts: - name: example-context context: cluster: example-cluster user: example-user current-context: example-context #source: https://dev.bitolog.com/implement-audits-webhook/ 由 webhook 发送的审计事件是以 HTTP POST 请求的形式发送的，请求体中包含 JSON 审计事件。指定的地址应该指向一个能","title":"附录 N：webhook 配置"},{"content":"Taint（污点）和 Toleration（容忍）可以作用于 node 和 pod 上，其目的是优化 pod 在集群间的调度，这跟节点亲和性类似，只不过它们作用的方式相反，具有 taint 的 node 和 pod 是互斥关系，而具有节点亲和性关系的 node 和 pod 是相吸的。另外还有可以给 node 节点设置 label，通过给 pod 设置 nodeSelector 将 pod 调度到具有匹配标签的节点上。\nTaint 和 toleration 相互配合，可以用来避免 pod 被分配到不合适的节点上。每个节点上都可以应用一个或多个 taint，这表示对于那些不能容忍这些 taint 的 pod，是不会被该节点接受的。如果将 toleration 应用于 pod 上，则表示这些 pod 可以（但不要求）被调度到具有相应 taint 的节点上。\n示例 以下分别以为 node 设置 taint 和为 pod 设置 toleration 为例。\n为 node 设置 taint 为 node1 设置 taint：\nkubectl taint nodes node1 …","relpermalink":"/kubernetes-handbook/cluster/taint-and-toleration/","summary":"Taint（污点）和 Toleration（容忍）可以作用于 node 和 pod 上，其目的是优化 pod 在集群间的调度，这跟节点亲和性类似，只不过它们作用的方式相反，具有 taint 的 node 和 pod 是互斥关系，而具有节点亲和性关系的 node 和 pod 是","title":"Taint 和 Toleration（污点和容忍）"},{"content":"Kubernetes 垃圾收集器的角色是删除指定的对象，这些对象曾经有但以后不再拥有 Owner 了。\nOwner 和 Dependent 一些 Kubernetes 对象是其它一些的 Owner。例如，一个 ReplicaSet 是一组 Pod 的 Owner。具有 Owner 的对象被称为是 Owner 的 Dependent。每个 Dependent 对象具有一个指向其所属对象的 metadata.ownerReferences 字段。\n有时，Kubernetes 会自动设置 ownerReference 的值。例如，当创建一个 ReplicaSet 时，Kubernetes 自动设置 ReplicaSet 中每个 Pod 的 ownerReference 字段值。在 1.6 版本，Kubernetes 会自动为一些对象设置 ownerReference 的值，这些对象是由 ReplicationController、ReplicaSet、StatefulSet、DaemonSet 和 Deployment 所创建或管理。\n也可以通过手动设置 ownerReference 的值， …","relpermalink":"/kubernetes-handbook/cluster/garbage-collection/","summary":"Kubernetes 垃圾收集器的角色是删除指定的对象，这些对象曾经有但以后不再拥有 Owner 了。 Owner 和 Dependent 一些 Kubernetes 对象是其它一些的 Owner。例如，一个 ReplicaSet 是一组 Pod 的 Owner。具有 Owner 的对象被称为是 Owner 的 Dependent。每个 Dependent 对象具","title":"垃圾收集"},{"content":"Deployment 为 Pod 和 ReplicaSet 提供了一个声明式定义（declarative）方法，用来替代以前的 ReplicationController 来方便的管理应用。典型的应用场景包括：\n定义 Deployment 来创建 Pod 和 ReplicaSet 滚动升级和回滚应用 扩容和缩容 暂停和继续 Deployment 比如一个简单的 nginx 应用可以定义为：\napiVersion: extensions/v1beta1 kind: Deployment metadata: name: nginx-deployment spec: replicas: 3 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.7.9 ports: - containerPort: 80 扩容：\nkubectl scale deployment nginx-deployment --replicas 10 如果集群支持 horizontal pod …","relpermalink":"/kubernetes-handbook/controllers/deployment/","summary":"Deployment 为 Pod 和 ReplicaSet 提供了一个声明式定义（declarative）方法，用来替代以前的 ReplicationController 来方便的管理应用。典型的应用场景包括： 定义 Deployment 来创建 Pod 和 ReplicaSet 滚动升级和回滚应用 扩容和缩容 暂停和继续 Deployment 比如一个简单的 nginx 应用可以定","title":"Deployment"},{"content":"StatefulSet 作为 Controller 为 Pod 提供唯一的标识。它可以保证部署和 scale 的顺序。\n使用案例参考：kubernetes contrib - statefulsets，其中包含 zookeeper 和 kakfa 的 statefulset 设置和使用说明。\nStatefulSet 是为了解决有状态服务的问题（对应 Deployments 和 ReplicaSets 是为无状态服务而设计），其应用场景包括：\n稳定的持久化存储，即 Pod 重新调度后还是能访问到相同的持久化数据，基于 PVC 来实现 稳定的网络标志，即 Pod 重新调度后其 PodName 和 HostName 不变，基于 Headless Service（即没有 Cluster IP 的 Service）来实现 有序部署，有序扩展，即 Pod 是有顺序的，在部署或者扩展的时候要依据定义的顺序依次依次进行（即从 0 到 N-1，在下一个 Pod 运行之前所有之前的 Pod 必须都是 Running 和 Ready 状态），基于 init containers 来实现 有序收缩，有序删除（ …","relpermalink":"/kubernetes-handbook/controllers/statefulset/","summary":"StatefulSet 作为 Controller 为 Pod 提供唯一的标识。它可以保证部署和 scale 的顺序。 使用案例参考：kubernetes contrib - statefulsets，其中包含 zookeeper 和 kakfa 的 statefulset 设置和使用说明。 StatefulSet 是为了解决有状态服务的问题（对应 Deployments 和 ReplicaSets 是为无状","title":"StatefulSet"},{"content":"将应用程序的功能划分为单独的进程运行在同一个最小调度单元中（例如 Kubernetes 中的 Pod）可以被视为 sidecar 模式。如下图所示，sidecar 模式允许您在应用程序旁边添加更多功能，而无需额外第三方组件配置或修改应用程序代码。\nSidecar 模式示意图 就像连接了 Sidecar 的三轮摩托车一样，在软件架构中，Sidecar 连接到父应用并且为其添加扩展或者增强功能。Sidecar 应用与主应用程序松散耦合。它可以屏蔽不同编程语言的差异，统一实现微服务的可观测性、监控、日志记录、配置、断路器等功能。\n使用 Sidecar 模式的优势 使用 sidecar 模式部署服务网格时，无需在节点上运行代理，但是集群中将运行多个相同的 sidecar 副本。在 sidecar 部署方式中，每个应用的容器旁都会部署一个伴生容器，这个容器称之为 sidecar 容器。Sidecar 接管进出应用容器的所有流量。在 Kubernetes 的 Pod 中，在原有的应用容器旁边注入一个 Sidecar 容器，两个容器共享存储、网络等资源，可以广义的将这个包含了 sidecar …","relpermalink":"/cloud-native-handbook/kubernetes/sidecar-pattern/","summary":"将应用程序的功能划分为单独的进程运行在同一个最小调度单元中（例如 Kubernetes 中的 Pod）可以被视为 sidecar 模式。如下图所示，sidecar 模式允许您在应用程序旁边添加更多功能，而无需额外第三方组件配置或修改应用程序代","title":"Sidecar 模式"},{"content":"本文将为您介绍 DaemonSet 的基本概念。\n什么是 DaemonSet？ DaemonSet 确保全部（或者一些）Node 上运行一个 Pod 的副本。当有 Node 加入集群时，也会为他们新增一个 Pod。当有 Node 从集群移除时，这些 Pod 也会被回收。删除 DaemonSet 将会删除它创建的所有 Pod。\n使用 DaemonSet 的一些典型用法：\n运行集群存储 daemon，例如在每个 Node 上运行 glusterd、ceph。 在每个 Node 上运行日志收集 daemon，例如fluentd、logstash。 在每个 Node 上运行监控 daemon，例如 Prometheus Node Exporter、collectd、Datadog 代理、New Relic 代理，或 Ganglia gmond。 一个简单的用法是，在所有的 Node 上都存在一个 DaemonSet，将被作为每种类型的 daemon 使用。 一个稍微复杂的用法可能是，对单独的每种类型的 daemon 使用多个 DaemonSet，但具有不同的标志，和/或对不同硬件类型具有不同的 …","relpermalink":"/kubernetes-handbook/controllers/daemonset/","summary":"本文将为您介绍 DaemonSet 的基本概念。 什么是 DaemonSet？ DaemonSet 确保全部（或者一些）Node 上运行一个 Pod 的副本。当有 Node 加入集群时，也会为他们新增一个 Pod。当有 Node 从集群移除时，这些 Pod 也会被回收。删除 DaemonSet 将会删除","title":"DaemonSet"},{"content":"ReplicationController 用来确保容器应用的副本数始终保持在用户定义的副本数，即如果有容器异常退出，会自动创建新的 Pod 来替代；而如果异常多出来的容器也会自动回收。\n在新版本的 Kubernetes 中建议使用 ReplicaSet 来取代 ReplicationController。ReplicaSet 跟 ReplicationController 没有本质的不同，只是名字不一样，并且 ReplicaSet 支持集合式的 selector。\n虽然 ReplicaSet 可以独立使用，但一般还是建议使用 Deployment 来自动管理 ReplicaSet，这样就无需担心跟其他机制的不兼容问题（比如 ReplicaSet 不支持 rolling-update 但 Deployment 支持）。\nReplicaSet 示例：\napiVersion: extensions/v1beta1 kind: ReplicaSet metadata: name: frontend # these labels can be applied automatically # …","relpermalink":"/kubernetes-handbook/controllers/replicaset/","summary":"ReplicationController 用来确保容器应用的副本数始终保持在用户定义的副本数，即如果有容器异常退出，会自动创建新的 Pod 来替代；而如果异常多出来的容器也会自动回收。 在新版本的 Kubernetes 中建议使用 ReplicaSet 来取代 ReplicationContro","title":"ReplicationController 和 ReplicaSet"},{"content":"Job 负责批处理任务，即仅执行一次的任务，它保证批处理任务的一个或多个 Pod 成功结束。\nJob Spec 格式 spec.template 格式同 Pod RestartPolicy 仅支持 Never 或 OnFailure 单个 Pod 时，默认 Pod 成功运行后 Job 即结束 .spec.completions 标志 Job 结束需要成功运行的 Pod 个数，默认为 1 .spec.parallelism 标志并行运行的 Pod 的个数，默认为 1 spec.activeDeadlineSeconds 标志失败 Pod 的重试最大时间，超过这个时间不会继续重试 一个简单的例子：\napiVersion: batch/v1 kind: Job metadata: name: pi spec: template: metadata: name: pi spec: containers: - name: pi image: perl command: [\u0026#34;perl\u0026#34;, \u0026#34;-Mbignum=bpi\u0026#34;, \u0026#34;-wle\u0026#34;, \u0026#34;print bpi(2000)\u0026#34;] …","relpermalink":"/kubernetes-handbook/controllers/job/","summary":"Job 负责批处理任务，即仅执行一次的任务，它保证批处理任务的一个或多个 Pod 成功结束。 Job Spec 格式 spec.template 格式同 Pod RestartPolicy 仅支持 Never 或 OnFailure 单个 Pod 时，默认 Pod 成功运行后 Job 即结束 .spec.completions 标志 Job 结束需要成功运行的 Pod 个数，默认为 1 .spec.parallelism 标志并行运行","title":"Job"},{"content":"Cron Job 管理基于时间的 Job，即：\n在给定时间点只运行一次 周期性地在给定时间点运行 一个 CronJob 对象类似于 crontab （cron table）文件中的一行。它根据指定的预定计划周期性地运行一个 Job，格式可以参考 Cron 。\n前提条件 当前使用的 Kubernetes 集群，版本 \u0026gt;= 1.8（对 CronJob）。对于先前版本的集群，版本 \u0026lt; 1.8，启动 API Server（参考 为集群开启或关闭 API 版本 获取更多信息）时，通过传递选项 --runtime-config=batch/v2alpha1=true 可以开启 batch/v2alpha1 API。\n典型的用法如下所示：\n在给定的时间点调度 Job 运行 创建周期性运行的 Job，例如：数据库备份、发送邮件。 CronJob Spec .spec.schedule：调度，必需字段，指定任务运行周期，格式同 Cron\n.spec.jobTemplate：Job 模板，必需字段，指定需要运行的任务，格式同 Job\n.spec.startingDeadlineSeconds ： …","relpermalink":"/kubernetes-handbook/controllers/cronjob/","summary":"Cron Job 管理基于时间的 Job，即： 在给定时间点只运行一次 周期性地在给定时间点运行 一个 CronJob 对象类似于 crontab （cron table）文件中的一行。它根据指定的预定计划周期性地运行一个 Job，格式可以参考 Cron 。 前提条件 当","title":"CronJob"},{"content":"为了使 Ingress 正常工作，集群中必须运行 Ingress controller。这与其他类型的控制器不同，其他类型的控制器通常作为 kube-controller-manager 二进制文件的一部分运行，在集群启动时自动启动。你需要选择最适合自己集群的 Ingress controller 或者自己实现一个。\nKubernetes 社区和众多厂商开发了大量的 Ingress Controller，你可以在 这里 找到。\n使用多个 Ingress 控制器 你可以使用 IngressClass 在集群中部署任意数量的 Ingress 控制器。请注意你的 Ingress 类资源的 .metadata.name 字段。当你创建 Ingress 时，你需要用此字段的值来设置 Ingress 对象的 ingressClassName 字段（请参考 IngressSpec v1 reference）。 ingressClassName 是之前的注解做法的替代。\n如果你不为 Ingress 指定 IngressClass，并且你的集群中只有一个 IngressClass 被标记为了集群默认，那 …","relpermalink":"/kubernetes-handbook/controllers/ingress-controller/","summary":"为了使 Ingress 正常工作，集群中必须运行 Ingress controller。这与其他类型的控制器不同，其他类型的控制器通常作为 kube-controller-manager 二进制文件的一部分运行，在集群启动时自动启动。你需要选择最适合自己集群的 Ingress controller 或者自己实现一个","title":"Ingress 控制器"},{"content":"Kubernetes 中不仅支持 CPU、内存为指标的 HPA，还支持自定义指标的 HPA，例如 QPS。\n设置自定义指标 Kubernetes 1.6\n在 Kubernetes1.6 集群中配置自定义指标的 HPA 的说明已废弃。\n在设置定义指标 HPA 之前需要先进行如下配置：\n将 heapster 的启动参数 --api-server 设置为 true\n启用 custom metric API\n将 kube-controller-manager 的启动参数中 --horizontal-pod-autoscaler-use-rest-clients 设置为 true，并指定 --master 为 API server 地址，如 --master=http://172.20.0.113:8080\n在 Kubernetes1.5 以前很容易设置，参考 1.6 以前版本的 kubernetes 中开启自定义 HPA，而在 1.6 中因为取消了原来的 annotation 方式设置 custom metric，只能通过 API server 和 kube-aggregator …","relpermalink":"/kubernetes-handbook/controllers/hpa/custom-metrics-hpa/","summary":"Kubernetes 中不仅支持 CPU、内存为指标的 HPA，还支持自定义指标的 HPA，例如 QPS。 设置自定义指标 Kubernetes 1.6 在 Kubernetes1.6 集群中配置自定义指标的 HPA 的说明已废弃。 在设置定义指标 HPA 之前需要先进行如下配置： 将 heapster 的启动参数 --api-server 设置为","title":"自定义指标 HPA"},{"content":"准入控制器（Admission Controller）位于 API Server 中，在对象被持久化之前，准入控制器拦截对 API Server 的请求，一般用来做身份验证和授权。其中包含两个特殊的控制器：MutatingAdmissionWebhook 和 ValidatingAdmissionWebhook。分别作为配置的变异和验证准入控制 webhook。\n准入控制器包括以下两种：\n变更（Mutating）准入控制：修改请求的对象 验证（Validating）准入控制：验证请求的对象 准入控制器是在 API Server 的启动参数重配置的。一个准入控制器可能属于以上两者中的一种，也可能两者都属于。当请求到达 API Server 的时候首先执行变更准入控制，然后再执行验证准入控制。\n我们在部署 Kubernetes 集群的时候都会默认开启一系列准入控制器，如果没有设置这些准入控制器的话可以说你的 Kubernetes 集群就是在裸奔，应该只有集群管理员可以修改集群的准入控制器。\n例如我会默认开启如下的准入控制器。 …","relpermalink":"/kubernetes-handbook/controllers/admission-controller/","summary":"准入控制器（Admission Controller）位于 API Server 中，在对象被持久化之前，准入控制器拦截对 API Server 的请求，一般用来做身份验证和授权。其中包含两个特殊的控制器：MutatingAdmissionW","title":"准入控制器（Admission Controller）"},{"content":"Kubernetes Pod 是有生命周期的，它们可以被创建，也可以被销毁，然而一旦被销毁生命就永远结束。通过 ReplicationController 能够动态地创建和销毁 Pod。每个 Pod 都会获取它自己的 IP 地址，即使这些 IP 地址不总是稳定可依赖的。这会导致一个问题：在 Kubernetes 集群中，如果一组 Pod（称为 backend）为其它 Pod （称为 frontend）提供服务，那么 frontend Pod 该如何发现和连接哪些 backend Pod 呢？\n关于 Service Kubernetes Service 定义了这样一种抽象：Pod 的逻辑分组，一种可以访问它们的策略 —— 通常称为微服务。这一组 Pod 能够被 Service 访问到，通常是通过 Label Selector（查看下面了解，为什么可能需要没有 selector 的 Service）实现的。\n举个例子，假设有一个用于图片处理的运行了三个副本的 backend。这些副本是可互换的 —— frontend 不需要关心它们调用了哪个 backend 副本。 …","relpermalink":"/kubernetes-handbook/service-discovery/service/","summary":"Kubernetes Pod 是有生命周期的，它们可以被创建，也可以被销毁，然而一旦被销毁生命就永远结束。通过 ReplicationController 能够动态地创建和销毁 Pod。每个 Pod 都会获取它自己的 IP 地址，即使这些 IP 地址不总是稳定可依赖的。这会导致一个问题：在 Kubernetes","title":"Service"},{"content":"拓扑感知路由指的是客户端对一个服务的访问流量，可以根据这个服务的端点拓扑，优先路由到与该客户端在同一个节点或者可用区的端点上的路由行为。\n先决条件 为了开启服务感知路由，你需要：\n开启 TopologyAwareHints 智能感知提示门控 开启 EndpointSlice 控制器 安装 kube-proxy 端点切片 我们知道 Endpoint 通常情况下是由 Service 资源自动创建和管理的，但是随着 Kubernetes 集群的规模越来越大和管理的服务越来越多，Endpoint API 的局限性变得越来越明显。 端点切片（EndpointSlices）提供了一种简单的方法来跟踪 Kubernetes 集群中的网络端点。它们为 Endpoint 提供了一种可伸缩和可拓展的替代方案，同时还可以被用到拓扑感知路由中。\nEndpointSlices 示例如下：\napiVersion: discovery.k8s.io/v1 kind: EndpointSlice metadata: name: example-hints labels: …","relpermalink":"/kubernetes-handbook/service-discovery/topology-aware-routing/","summary":"拓扑感知路由指的是客户端对一个服务的访问流量，可以根据这个服务的端点拓扑，优先路由到与该客户端在同一个节点或者可用区的端点上的路由行为。 先决条件 为了开启服务感知路由，你需要： 开启 TopologyAwareHints 智能感知提示门控 开启","title":"拓扑感知路由"},{"content":"Ingress 是从 Kubernetes 集群外部访问集群内部服务的入口，是将 Kubernetes 集群内部服务暴露到外界的几种方式之一。本文将为你详细介绍 Ingress 资源对象。\n术语\n在本篇文章中你将会看到一些在其他地方被交叉使用的术语，为了防止产生歧义，我们首先来澄清下。\n节点：Kubernetes 集群中的一台物理机或者虚拟机。 集群：位于 Internet 防火墙后的节点，这是 Kubernetes 管理的主要计算资源。 边界路由器：为集群强制执行防火墙策略的路由器。这可能是由云提供商或物理硬件管理的网关。 集群网络：一组逻辑或物理链接，可根据 Kubernetes 网络模型 实现群集内的通信。集群网络的实现包括 Overlay 模型的 flannel 和基于 SDN 的 OVS。 服务：使用标签选择器标识一组 pod 成为的 Kubernetes 服务。除非另有说明，否则服务假定在集群网络内仅可通过虚拟 IP 访问。 什么是 Ingress？ 通常情况下，service 和 pod 仅可在集群内部网络中通过 IP 地址访问。所有到达边界路由器的流量或被丢弃或被转发到 …","relpermalink":"/kubernetes-handbook/service-discovery/ingress/","summary":"Ingress 是从 Kubernetes 集群外部访问集群内部服务的入口，是将 Kubernetes 集群内部服务暴露到外界的几种方式之一。本文将为你详细介绍 Ingress 资源对象。 术语 在本篇文章中你将会看到一些在其他地方被交叉使用的术语，为了防止产生歧义，我们首先来","title":"Ingress"},{"content":" 注意\n本文根据 Gateway API 0.5.1 版本撰写。目前 Gateway API 仅用于处理 Kubernetes 集群中的南北向（入口）流量，未来也有可能处理集群内（东西向）流量。关于使用 Gateway API 处理东西向流量的提议详见此文档。 除了直接使用 Service 和 Ingress 之外，Kubernetes 社区还发起了 Gateway API 项目，它可以帮助我们将 Kubernetes 中的服务暴露到集群外。\nGateway API 是一个由 SIG-NETWORK 管理的开源项目。该项目的目标是在 Kubernetes 生态系统中发展服务网络 API。Gateway API 提供了暴露 Kubernetes 应用的资源——GatewayClass、Gateway、HTTPRoute、TCPRoute 等。\nGateway API 已经得到了大量的网关和服务网格项目支持，请在 Gateway 官方文档中查看支持状况。\n目标 Gateway API 旨在通过提供表现性的、可扩展的、面向角色的接口来改善服务网络，这些接口由许多厂商实现，并得到了业界的广泛支 …","relpermalink":"/kubernetes-handbook/service-discovery/gateway/","summary":"注意 本文根据 Gateway API 0.5.1 版本撰写。目前 Gateway API 仅用于处理 Kubernetes 集群中的南北向（入口）流量，未来也有可能处理集群内（东西向）流量。关于使用 Gateway API 处理东西向流量的提议详见此文档。 除了直接使用 Service 和 Ingress 之外，Kubernete","title":"Gateway API"},{"content":"ServiceAccount 为 Pod 中的进程提供身份信息。\n注意\n本文档描述的关于 ServiceAccount 的行为只有当你按照 Kubernetes 项目建议的方式搭建集群的情况下才有效。集群管理员可能在你的集群中进行了自定义配置，这种情况下该文档可能并不适用。 当你（真人用户）访问集群（例如使用 kubectl 命令）时，API 服务器会将你认证为一个特定的 User Account（目前通常是 admin，除非你的系统管理员自定义了集群配置）。Pod 容器中的进程也可以与 API 服务器联系。当它们在联系 API 服务器的时候，它们会被认证为一个特定的 ServiceAccount（例如default）。\n使用默认的 ServiceAccount 访问 API 服务器 当你创建 pod 的时候，如果你没有指定一个 ServiceAccount，系统会自动得在与该 pod 相同的 namespace 下为其指派一个 default ServiceAccount。如果你获取刚创建的 pod 的原始 json 或 yaml 信息（例如使用kubectl get …","relpermalink":"/kubernetes-handbook/auth/serviceaccount/","summary":"ServiceAccount 为 Pod 中的进程提供身份信息。","title":"ServiceAccount"},{"content":"注意：本文基于 Kubernetes 1.6 撰写，当时 RBAC 模式处于 beta 版本。\n基于角色的访问控制（Role-Based Access Control，即”RBAC”）使用 rbac.authorization.k8s.io API Group 实现授权决策，允许管理员通过 Kubernetes API 动态配置策略。\n要启用 RBAC，请使用 --authorization-mode=RBAC 启动 API Server。\nAPI 概述 本节将介绍 RBAC API 所定义的四种顶级类型。用户可以像使用其他 Kubernetes API 资源一样（例如通过 kubectl、API 调用等）与这些资源进行交互。例如，命令 kubectl create -f (resource).yml 可以被用于以下所有的例子，当然，读者在尝试前可能需要先阅读以下相关章节的内容。\nRole 与 ClusterRole 在 RBAC API 中，一个角色包含了一套表示一组权限的规则。权限以纯粹的累加形式累积（没有”否定”的规则）。角色可以由命名空间（namespace）内的 Role 对 …","relpermalink":"/kubernetes-handbook/auth/rbac/","summary":"注意：本文基于 Kubernetes 1.6 撰写，当时 RBAC 模式处于 beta 版本。 基于角色的访问控制（Role-Based Access Control，即”RBAC”）使用 rbac.authorization.k8s.io API Group 实现授权决策，允许管理员通过 Kubernetes API 动态配置策略。 要启用 RBAC，请使用 --authorization-mode=RBAC","title":"基于角色的访问控制（RBAC）"},{"content":"网络策略说明一组 Pod 之间是如何被允许互相通信，以及如何与其它网络 Endpoint 进行通信。 NetworkPolicy 资源使用标签来选择 Pod，并定义了一些规则，这些规则指明允许什么流量进入到选中的 Pod 上。关于 Network Policy 的详细用法请参考 Kubernetes 官网。\nNetwork Policy 的作用对象是 Pod，也可以应用到 Namespace 和集群的 Ingress、Egress 流量。Network Policy 是作用在 L3/4 层的，即限制的是对 IP 地址和端口的访问，如果需要对应用层做访问限制需要使用如 Istio 这类 Service Mesh。\n前提条件 网络策略通过网络插件来实现，所以必须使用一种支持 NetworkPolicy 的网络方案（如 calico）—— 非 Controller 创建的资源，是不起作用的。\n隔离的与未隔离的 Pod 默认 Pod 是未隔离的，它们可以从任何的源接收请求。具有一个可以选择 Pod 的网络策略后，Pod 就会变成隔离的。一旦 Namespace 中配置的网络策略能够选择一个特定 …","relpermalink":"/kubernetes-handbook/auth/network-policy/","summary":"网络策略说明一组 Pod 之间是如何被允许互相通信，以及如何与其它网络 Endpoint 进行通信。 NetworkPolicy 资源使用标签来选择 Pod，并定义了一些规则，这些规则指明允许什么流量进入到选中的 Pod 上。关于 Network Policy 的详细用法请参考 Kubernetes 官网。 Network Policy 的","title":"NetworkPolicy"},{"content":"SPIFFE，即普适安全生产身份框架（Secure Production Identity Framework for Everyone），是一套开源标准，用于在动态和异构环境中安全地进行身份识别。采用 SPIFFE 的系统无论在哪里运行，都可以轻松可靠地相互认证。\nSPIFFE 开源规范的核心是——通过简单 API 定义了一个短期的加密身份文件 SVID。然后，工作负载进行认证时可以使用该身份文件，例如建立 TLS 连接或签署和验证 JWT 令牌等。\nSPIFFE 已经在云原生应用中得到了大量的应用，尤其是在 Istio 和 Envoy 中。下面将向你介绍 SPIFFE 的一些基本概念。\n工作负载 工作负载是个单一的软件，以特定的配置部署，用于单一目的；它可能包括软件的多个运行实例，所有这些实例执行相同的任务。工作负载这个术语可以包含一系列不同的软件系统定义，包括：\n一个运行 Python 网络应用程序的网络服务器，在一个虚拟机集群上运行，前面有一个负载均衡器。 一个 MySQL 数据库的实例。 一个处理队列中项目的 worker 程序。 独立部署的系统的集合，它们一起工作，例如一个 …","relpermalink":"/kubernetes-handbook/auth/spiffe/","summary":"SPIFFE，即普适安全生产身份框架（Secure Production Identity Framework for Everyone），是一套开源标准，用于在动态和异构环境中安全地进行身份识别。采用 SPIFFE 的系统无论在哪里运行，都可以轻松可靠地相互认证。 SPIFFE 开源规范","title":"SPIFFE"},{"content":"这篇文章将向你介绍 SPIRE 的架构、基本概念及原理。\nSPIRE 是 SPIFFE API 的一个生产就绪的实现，它执行节点和工作负载认证，以便根据一组预先定义的条件，安全地向工作负载发出 SVID，并验证其他工作负载的 SVID。\nSPIRE 架构和组件 SPIRE 部署由一个 SPIRE 服务器和一个或多个 SPIRE 代理组成。服务器充当通过代理向一组工作负载发放身份的签名机构。它还维护一个工作负载身份的注册表，以及为签发这些身份而必须验证的条件。代理在本地向工作负载公开 SPIFFE 工作负载 API，必须安装在工作负载运行的每个节点上。\nSPIRE 架构图 服务器 SPIRE 服务器负责管理和发布其配置的 SPIFFE 信任域中的所有身份。它存储注册条目（指定决定特定 SPIFFE ID 应被签发的条件的选择器）和签名密钥，使用节点证明来自动验证代理的身份，并在被验证的代理请求时为工作负载创建 SVID。\nSPIRE 服务器 服务器的行为是通过一系列的插件决定的。SPIRE 包含几个插件，你可以建立额外的插件来扩展 SPIRE 以满足特定的使用情况。插件的类型包括：\n节点 …","relpermalink":"/kubernetes-handbook/auth/spire/","summary":"这篇文章将向你介绍 SPIRE 的架构、基本概念及原理。","title":"SPIRE"},{"content":"SPIRE Kubernetes 工作负载注册器实现了一个 Kubernetes ValidatingAdmissionWebhook，便于在 Kubernetes 内自动注册工作负载。\n配置 命令行配置 注册器有以下命令行选项：\n标志 描述 默认值 -config 磁盘上 HCL 配置文件的路径 k8s-workload-registrar.conf HCL 配置 配置文件是注册器所必需的。它包含 HCL 编码的配置项。\n键 类型 必需的？ 描述 默认 log_level string 必需的 日志级别（panic、 fatal、 error、 warn、 warning、 info、 debug、trace 之一） info log_path string 可选的 写入日志的磁盘路径 trust_domain string 必需的 SPIRE 服务器的信任域 agent_socket_path string 可选的 SPIRE 代理的 Unix 域套接字的路径。如果 server_address 不是 unix 域套接字地址，则为必需。 server_address string  …","relpermalink":"/kubernetes-handbook/auth/spire-k8s-workload-registar/","summary":"本文介绍了如何在 Kubernetes 中使用 SPIRE 工作负载注册器，包括工作负载注册器部署的方式，注册模式等。","title":"SPIRE Kubernetes 工作负载注册器"},{"content":"本文介绍 SPIRE 如何向工作负载颁发身份的详细步骤，叙述了从代理在节点上启动到同一节点上的工作负载收到 X.509 SVID 形式的有效身份的全过程。请注意，JWT 格式的 SVID 的处理方式是不同的。为了简单演示的目的，工作负载在 AWS EC2 上运行。\nSPIRE 服务器启动。 除非用户配置了 UpstreamAuthority 插件，否则服务器会生成自签名证书（使用自己的私钥签名的证书）；服务器将使用此证书为该服务器信任域中的所有工作负载签署 SVID。 如果是第一次启动，服务器会自动生成一个信任包（trust bundle），其内容存储在你指定的 sql 数据存储中 —— 在服务器插件：DataStore sql 的“内置插件”部分中描述。 服务器打开其注册 API，以允许你注册工作负载。 工作负载节点上的 SPIRE 代理启动。 代理执行节点证明，向服务器证明它正在运行的节点的身份。例如，在 AWS EC2 实例上运行时，它通常会通过向服务器提供 AWS 实例身份文档来执行节点证明。 代理通过 TLS 连接向服务器提供此身份证明，该 TLS 连接通过代理配置的引导程序 …","relpermalink":"/kubernetes-handbook/auth/svid/","summary":"本文介绍了为工作负载颁发 X.509 SVID 身份的详细步骤。","title":"SVID 身份颁发过程"},{"content":"如果你安装了拥有三个节点的 Kubernetes 集群，节点的状态如下所述。\n[root@node1 ~]# kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME node1 Ready \u0026lt;none\u0026gt; 2d v1.9.1 \u0026lt;none\u0026gt; CentOS Linux 7 (Core) 3.10.0-693.11.6.el7.x86_64 docker://1.12.6 node2 Ready \u0026lt;none\u0026gt; 2d v1.9.1 \u0026lt;none\u0026gt; CentOS Linux 7 (Core) 3.10.0-693.11.6.el7.x86_64 docker://1.12.6 node3 Ready \u0026lt;none\u0026gt; 2d v1.9.1 \u0026lt;none\u0026gt; CentOS Linux 7 (Core) 3.10.0-693.11.6.el7.x86_64 docker://1.12.6 当前 Kubernetes 集群中运行的所有 Pod 信息： …","relpermalink":"/kubernetes-handbook/networking/flannel/","summary":"如果你安装了拥有三个节点的 Kubernetes 集群，节点的状态如下所述。 [root@node1 ~]# kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME node1 Ready \u003cnone\u003e 2d v1.9.1 \u003cnone\u003e CentOS Linux 7 (Core) 3.10.0-693.11.6.el7.x86_64 docker://1.12.6 node2 Ready \u003cnone\u003e 2d v1.9.1 \u003cnone\u003e CentOS Linux 7 (Core) 3.10.0-693.11.6.el7.x86_64 docker://1.12.6 node3 Ready \u003cnone\u003e 2d v1.9.1 \u003cnone\u003e CentOS Linux 7 (Core) 3.10.0-693.11.6.el7.x86_64 docker://1.12.6 当前 Kubernetes 集群中运行的所有 Pod 信息： [root@node1 ~]# kubectl get pods --all-namespaces","title":"扁平网络 Flannel"},{"content":"Calico 原意为”有斑点的“，如果说一只猫为 calico cat 的话，就是说这是只花猫，也叫三色猫，所以 calico 的 logo 是只三色猫。\nCalico 概念 Calico 创建和管理一个扁平的三层网络（不需要 overlay），每个容器会分配一个可路由的 IP。由于通信时不需要解包和封包，网络性能损耗小，易于排查，且易于水平扩展。\n小规模部署时可以通过 BGP client 直接互联，大规模下可通过指定的 BGP Route Reflector 来完成，这样保证所有的数据流量都是通过 IP 路由的方式完成互联的。\nCalico 基于 iptables 还提供了丰富而灵活的网络 Policy，保证通过各个节点上的 ACL 来提供 Workload 的多租户隔离、安全组以及其他可达性限制等功能。\nCalico 架构 Calico 由以下组件组成，在部署 Calico 的时候部分组件是可选的。\nCalico API server Felix BIRD confd Dikastes CNI 插件 数据存储插件 IPAM 插件 kube-controllers Typha …","relpermalink":"/kubernetes-handbook/networking/calico/","summary":"Calico 原意为”有斑点的“，如果说一只猫为 calico cat 的话，就是说这是只花猫，也叫三色猫，所以 calico 的 logo 是只三色猫。 Calico 概念 Calico 创建和管理一个扁平的三层网络（不需要 overlay），每个容器会分配一个可路由的 IP。由于通信","title":"非 Overlay 扁平网络 Calico"},{"content":"Cilium 是一款开源软件，也是 CNCF 的孵化项目，目前已有公司提供商业化支持，还有基于 Cilium 实现的服务网格解决方案。最初它仅是作为一个 Kubernetes 网络组件。Cilium 在 1.7 版本后推出并开源了 Hubble，它是专门为网络可视化设计，能够利用 Cilium 提供的 eBPF 数据路径，获得对 Kubernetes 应用和服务的网络流量的深度可视性。这些网络流量信息可以对接 Hubble CLI、UI 工具，可以通过交互式的方式快速进行问题诊断。除了 Hubble 自身的监控工具，还可以对接主流的云原生监控体系——Prometheus 和 Grafana，实现可扩展的监控策略。\nCilium 本节将带你了解什么是 Cilium 及选择它的原因。\nCilium 是什么？ Cilium 为基于 Kubernetes 的 Linux 容器管理平台上部署的服务，透明地提供服务间的网络和 API 连接及安全。\nCilium 底层是基于 Linux 内核的新技术 eBPF，可以在 Linux 系统中动态注入强大的安全性、可视性和网络控制逻辑。Cilium …","relpermalink":"/kubernetes-handbook/networking/cilium/","summary":"Cilium 是一款开源软件，也是 CNCF 的孵化项目，目前已有公司提供商业化支持，还有基于 Cilium 实现的服务网格解决方案。最初它仅是作为一个 Kubernetes 网络组件。Cilium 在 1.7 版本后推出并开源了 Hubble，它是专门为网络可视化设计","title":"基于 eBPF 的网络 Cilium"},{"content":"Secret 解决了密码、token、密钥等敏感数据的配置问题，而不需要把这些敏感数据暴露到镜像或者 Pod Spec 中。Secret 可以以 Volume 或者环境变量的方式使用。\nSecret 有三种类型：\nService Account ：用来访问 Kubernetes API，由 Kubernetes 自动创建，并且会自动挂载到 Pod 的 /run/secrets/kubernetes.io/serviceaccount 目录中； Opaque ：base64 编码格式的 Secret，用来存储密码、密钥等； kubernetes.io/dockerconfigjson ：用来存储私有 docker registry 的认证信息。 Opaque Secret Opaque 类型的数据是一个 map 类型，要求 value 是 base64 编码格式：\n$ echo -n \u0026#34;admin\u0026#34; | base64 YWRtaW4= $ echo -n \u0026#34;1f2d1e2e67df\u0026#34; | base64 MWYyZDFlMmU2N2Rm secrets.yml\napiVersion: v1 …","relpermalink":"/kubernetes-handbook/storage/secret/","summary":"Secret 解决了密码、token、密钥等敏感数据的配置问题，而不需要把这些敏感数据暴露到镜像或者 Pod Spec 中。Secret 可以以 Volume 或者环境变量的方式使用。 Secret 有三种类型： Service Account ：用来访问 Kubernetes API，由 Kubernetes 自动创建，并且会自","title":"Secret"},{"content":"其实 ConfigMap 功能在 Kubernetes1.2 版本的时候就有了，许多应用程序会从配置文件、命令行参数或环境变量中读取配置信息。这些配置信息需要与 docker image 解耦，你总不能每修改一个配置就重做一个 image 吧？ConfigMap API 给我们提供了向容器中注入配置信息的机制，ConfigMap 可以被用来保存单个属性，也可以用来保存整个配置文件或者 JSON 二进制大对象。\nConfigMap 概览 ConfigMap API 资源用来保存 key-value pair 配置数据，这个数据可以在 pods 里使用，或者被用来为像 controller 一样的系统组件存储配置数据。虽然 ConfigMap 跟 Secrets 类似，但是 ConfigMap 更方便的处理不含敏感信息的字符串。注意：ConfigMaps 不是属性配置文件的替代品。ConfigMaps 只是作为多个 properties 文件的引用。你可以把它理解为 Linux 系统中的 /etc 目录，专门用来存储配置文件的目录。下面举个例子，使用 ConfigMap …","relpermalink":"/kubernetes-handbook/storage/configmap/","summary":"其实 ConfigMap 功能在 Kubernetes1.2 版本的时候就有了，许多应用程序会从配置文件、命令行参数或环境变量中读取配置信息。这些配置信息需要与 docker image 解耦，你总不能每修改一个配置就重做一个 image 吧？ConfigMap API 给我们提供了向容器中","title":"ConfigMap"},{"content":"ConfigMap 是用来存储配置文件的 kubernetes 资源对象，所有的配置内容都存储在 etcd 中，下文主要是探究 ConfigMap 的创建和更新流程，以及对 ConfigMap 更新后容器内挂载的内容是否同步更新的测试。\n测试示例 假设我们在 default namespace 下有一个名为 nginx-config 的 ConfigMap，可以使用 kubectl 命令来获取：\n$ kubectl get configmap nginx-config NAME DATA AGE nginx-config 1 99d 获取该 ConfigMap 的内容。\nkubectl get configmap nginx-config -o yaml apiVersion: v1 data: nginx.conf: |- worker_processes 1; events { worker_connections 1024; } http { sendfile on; server { listen 80; # a test endpoint that returns http …","relpermalink":"/kubernetes-handbook/storage/configmap-hot-update/","summary":"ConfigMap 是用来存储配置文件的 kubernetes 资源对象，所有的配置内容都存储在 etcd 中，下文主要是探究 ConfigMap 的创建和更新流程，以及对 ConfigMap 更新后容器内挂载的内容是否同步更新的测试。 测试示例 假设我们在 default namespace 下有一个名为 nginx-config 的 ConfigMa","title":"ConfigMap 的热更新"},{"content":"Volume 容器磁盘上的文件的生命周期是短暂的，这就使得在容器中运行重要应用时会出现一些问题。首先，当容器崩溃时，kubelet 会重启它，但是容器中的文件将丢失——容器以干净的状态（镜像最初的状态）重新启动。其次，在 Pod 中同时运行多个容器时，这些容器之间通常需要共享文件。Kubernetes 中的 Volume 抽象就很好的解决了这些问题。\n建议先熟悉 pod。\n背景 Docker 中也有一个 volume 的概念，尽管它稍微宽松一些，管理也很少。在 Docker 中，卷就像是磁盘或是另一个容器中的一个目录。它的生命周期不受管理，直到最近才有了 local-disk-backed 卷。Docker 现在提供了卷驱动程序，但是功能还非常有限（例如 Docker1.7 只允许每个容器使用一个卷驱动，并且无法给卷传递参数）。\n另一方面，Kubernetes 中的卷有明确的寿命——与封装它的 Pod 相同。所以，卷的生命比 Pod 中的所有容器都长，当这个容器重启时数据仍然得以保存。当然，当 Pod 不再存在时，卷也将不复存在。也许更重要的是，Kubernetes 支持多种类型的 …","relpermalink":"/kubernetes-handbook/storage/volume/","summary":"Volume 容器磁盘上的文件的生命周期是短暂的，这就使得在容器中运行重要应用时会出现一些问题。首先，当容器崩溃时，kubelet 会重启它，但是容器中的文件将丢失——容器以干净的状态（镜像最初的状态）重新启动。其","title":"Volume"},{"content":"本文档介绍了 Kubernetes 中 PersistentVolume 的当前状态。建议您在阅读本文档前先熟悉 volume。\n介绍 对于管理计算资源来说，管理存储资源明显是另一个问题。PersistentVolume 子系统为用户和管理员提供了一个 API，该 API 将如何提供存储的细节抽象了出来。为此，我们引入两个新的 API 资源：PersistentVolume 和 PersistentVolumeClaim。\nPersistentVolume（PV）是由管理员设置的存储，它是群集的一部分。就像节点是集群中的资源一样，PV 也是集群中的资源。PV 是 Volume 之类的卷插件，但具有独立于使用 PV 的 Pod 的生命周期。此 API 对象包含存储实现的细节，即 NFS、iSCSI 或特定于云供应商的存储系统。\nPersistentVolumeClaim（PVC）是用户存储的请求。它与 Pod 相似。Pod 消耗节点资源，PVC 消耗 PV 资源。Pod 可以请求特定级别的资源（CPU 和内存）。声明可以请求特定的大小和访问模式（例如，可以以读/写一次或 只读多次模式挂 …","relpermalink":"/kubernetes-handbook/storage/persistent-volume/","summary":"本文档介绍了 Kubernetes 中 PersistentVolume 的当前状态。建议您在阅读本文档前先熟悉 volume。 介绍 对于管理计算资源来说，管理存储资源明显是另一个问题。PersistentVolume 子系统为用户和管理员提供了一个 API，该 API","title":"持久化卷（Persistent Volume）"},{"content":"本文介绍了 Kubernetes 中 StorageClass 的概念。在阅读本文之前建议先熟悉 卷 和 Persistent Volume（持久卷）。\n介绍 StorageClass 为管理员提供了描述存储 “class（类）” 的方法。不同的 class 可能会映射到不同的服务质量等级或备份策略，或由群集管理员确定的任意策略。Kubernetes 本身不清楚各种 class 代表的什么。这个概念在其他存储系统中有时被称为“配置文件”。\nStorageClass 资源 StorageClass 中包含 provisioner、parameters 和 reclaimPolicy 字段，当 class 需要动态分配 PersistentVolume 时会使用到。\nStorageClass 对象的名称很重要，用户使用该类来请求一个特定的方法。当创建 StorageClass 对象时，管理员设置名称和其他参数，一旦创建了对象就不能再对其更新。\n管理员可以为没有申请绑定到特定 class 的 PVC 指定一个默认的 StorageClass ： …","relpermalink":"/kubernetes-handbook/storage/storageclass/","summary":"本文介绍了 Kubernetes 中 StorageClass 的概念。在阅读本文之前建议先熟悉 卷 和 Persistent Volume（持久卷）。 介绍 StorageClass 为管理员提供了描述存储 “class（类）” 的方法。不同的 class 可能会映射到不同的服务质量等级或备","title":"Storage Class"},{"content":"本地持久化卷允许用户通过标准 PVC 接口以简单便携的方式访问本地存储。PV 中包含系统用于将 Pod 安排到正确节点的节点亲和性信息。\n一旦配置了本地卷，外部静态配置器（provisioner）可用于帮助简化本地存储管理。请注意，本地存储配置器与大多数配置器不同，并且尚不支持动态配置。相反，它要求管理员预先配置每个节点上的本地卷，并且这些卷应该是：\nFilesystem volumeMode（默认）PV—— 将它们挂载到发现目录下。 Block volumeMode PV——在发现目录下为节点上的块设备创建一个符号链接。 配置器将通过为每个卷创建和清除 PersistentVolumes 来管理发现目录下的卷。\n配置要求 本地卷插件希望路径稳定，包括在重新启动时和添加或删除磁盘时。 静态配置器仅发现挂载点（对于文件系统模式卷）或符号链接（对于块模式卷）。对于基于目录的本地卷必须绑定到发现目录中。 版本兼容性 推荐配置器版本与 Kubernetes 版本\nProvisioner version K8s version Reason 2.1.0 1.10 Beta API …","relpermalink":"/kubernetes-handbook/storage/local-persistent-storage/","summary":"本地持久化卷允许用户通过标准 PVC 接口以简单便携的方式访问本地存储。PV 中包含系统用于将 Pod 安排到正确节点的节点亲和性信息。 一旦配置了本地卷，外部静态配置器（provisioner）可用于帮助简化本地存储管","title":"本地持久化存储"},{"content":"自定义资源是对 Kubernetes API 的扩展，Kubernetes 中的每个资源都是一个 API 对象的集合，例如我们在 YAML 文件里定义的那些 spec 都是对 Kubernetes 中的资源对象的定义，所有的自定义资源可以跟 Kubernetes 中内建的资源一样使用 kubectl 操作。\n自定义资源 Kubernetes 从 1.6 版本开始包含一个内建的资源叫做 TPR（ThirdPartyResource），可以用它来创建自定义资源，但该资源在 Kubernetes 1.7 版本开始已被 CRD（CustomResourceDefinition）取代。\n扩展 API 自定义资源实际上是为了扩展 Kubernetes 的 API，向 Kubernetes API 中增加新类型，可以使用以下三种方式：\n修改 Kubernetes 的源码，显然难度比较高，也不太合适 创建自定义 API server 并聚合到 API 中 编写自定义资源是扩展 Kubernetes API 的最简单的方式，是否编写自定义资源来扩展 API 请参考 Should I add a …","relpermalink":"/kubernetes-handbook/extend/custom-resource/","summary":"自定义资源是对 Kubernetes API 的扩展，Kubernetes 中的每个资源都是一个 API 对象的集合，例如我们在 YAML 文件里定义的那些 spec 都是对 Kubernetes 中的资源对象的定义，所有的自定义资源可以跟 Kubernetes 中内建的资源一样使用 kubectl 操作。 自定义资源","title":"使用自定义资源扩展 API"},{"content":"本文是如何创建 CRD 来扩展 Kubernetes API 的教程。CRD 是用来扩展 Kubernetes 最常用的方式，在 Service Mesh 和 Operator 中也被大量使用。因此读者如果想在 Kubernetes 上做扩展和开发的话，是十分有必要了解 CRD 的。\n在阅读本文前您需要先了解使用自定义资源扩展 API，以下内容译自 Kubernetes 官方文档，有删改，推荐阅读如何从零开始编写一个 Kubernetes CRD。\n创建 CRD（CustomResourceDefinition） 创建新的 CustomResourceDefinition（CRD）时，Kubernetes API Server 会为您指定的每个版本创建新的 RESTful 资源路径。CRD 可以是命名空间的，也可以是集群范围的，可以在 CRD scope 字段中所指定。与现有的内置对象一样，删除命名空间会删除该命名空间中的所有自定义对象。CustomResourceDefinition 本身是非命名空间的，可供所有命名空间使用。\n参考下面的 CRD， …","relpermalink":"/kubernetes-handbook/extend/crd/","summary":"本文是如何创建 CRD 来扩展 Kubernetes API 的教程。CRD 是用来扩展 Kubernetes 最常用的方式，在 Service Mesh 和 Operator 中也被大量使用。因此读者如果想在 Kubernetes 上做扩展和开发的话，是十分有必要了解 CRD 的。 在阅读本文前您需要先了解使用自定义资源扩展 API","title":"使用 CRD 扩展 Kubernetes API"},{"content":"Aggregated（聚合的）API server 是为了将原来的 API server 这个巨石（monolithic）应用给拆分成，为了方便用户开发自己的 API server 集成进来，而不用直接修改 kubernetes 官方仓库的代码，这样一来也能将 API server 解耦，方便用户使用实验特性。这些 API server 可以跟 core API server 无缝衔接，使用 kubectl 也可以管理它们。\n架构 我们需要创建一个新的组件，名为 kube-aggregator，它需要负责以下几件事：\n提供用于注册 API server 的 API 汇总所有的 API server 信息 代理所有的客户端到 API server 的请求 注意：这里说的 API server 是一组“API Server”，而不是说我们安装集群时候的那个 API server，而且这组 API server 是可以横向扩展的。\n安装配置聚合的 API server 有两种方式来启用 kube-aggregator：\n使用 test mode/single-user mode，作为一个独立 …","relpermalink":"/kubernetes-handbook/extend/aggregated-api-server/","summary":"Aggregated（聚合的）API server 是为了将原来的 API server 这个巨石（monolithic）应用给拆分成，为了方便用户开发自己的 API server 集成进来，而不用直接修改 kubernetes 官方仓库的代码，这样一来也能将 API server 解耦，方便用","title":"Aggregated API Server"},{"content":"APIService 是用来表示一个特定的 GroupVersion 的中的 server，它的结构定义位于代码 staging/src/k8s.io/kube-aggregator/pkg/apis/apiregistration/types.go 中。\n下面是一个 APIService 的示例配置：\napiVersion: apiregistration.k8s.io/v1beta1 kind: APIService metadata: name: v1alpha1.custom-metrics.metrics.k8s.io spec: insecureSkipTLSVerify: true group: custom-metrics.metrics.k8s.io groupPriorityMinimum: 1000 versionPriority: 5 service: name: api namespace: custom-metrics version: v1alpha1 APIService 详解 使用 apiregistration.k8s.io/v1beta1 …","relpermalink":"/kubernetes-handbook/extend/apiservice/","summary":"APIService 是用来表示一个特定的 GroupVersion 的中的 server，它的结构定义位于代码 staging/src/k8s.io/kube-aggregator/pkg/apis/apiregistration/types.go 中。 下面是一个 APIService 的示例配置： apiVersion: apiregistration.k8s.io/v1beta1 kind: APIService metadata: name: v1alpha1.custom-metrics.metrics.k8s.io spec: insecureSkipTLSVerify: true group: custom-metrics.metrics.k8s.io groupPriorityMinimum: 1000 versionPriority: 5 service: name: api namespace: custom-metrics version: v1alpha1 APIService 详解 使用 apiregistration.k8s.io/v1beta1 版本的 APIService，在 metadata.name 中定义该 API 的名字","title":"APIService"},{"content":"服务目录（Service Catalog）是 Kubernetes 的扩展 API，它使运行在 Kubernetes 集群中的应用程序可以轻松使用外部托管软件产品，例如由云提供商提供的数据存储服务。\n它提供列表清单、提供 (provision) 和绑定 (binding) 来自服务代理（Service Brokers）的外部托管服务，而不需要关心如何创建或管理这些服务的详细情况。\n由 Open Service Broker API 规范定义的 Service broker 是由第三方提供和维护的一组托管服务的端点 (endpoint)，该第三方可以是 AWS，GCP 或 Azure 等云提供商。\n托管服务可以是 Microsoft Azure Cloud Queue，Amazon Simple Queue Service 和 Google Cloud Pub/Sub 等，它们可以是应用可以使用的提供的各种软件。\n通过 Service Catalog，集群运营者可以浏览由 Service Broker 提供的托管服务列表，提供的托管服务实例，并与其绑定，使其可被 Kubernetes 集 …","relpermalink":"/kubernetes-handbook/extend/service-catalog/","summary":"服务目录（Service Catalog）是 Kubernetes 的扩展 API，它使运行在 Kubernetes 集群中的应用程序可以轻松使用外部托管软件产品，例如由云提供商提供的数据存储服务。 它提供列表清单、提供 (provision) 和绑定 (binding) 来自服务代理（Ser","title":"服务目录（Service Catalog）"},{"content":"2020 年初，Kubernetes 社区提议 Multi-Cluster Services API，旨在解决长久以来就存在的 Kubernetes 多集群服务管理问题。\nKubernetes 用户可能希望将他们的部署分成多个集群，但仍然保留在这些集群中运行的工作负载之间的相互依赖关系，这有很多原因。今天，集群是一个硬边界，一个服务对远程的 Kubernetes 消费者来说是不透明的，否则就可以利用元数据（如端点拓扑结构）来更好地引导流量。为了支持故障转移或在迁移过程中的临时性，用户可能希望消费分布在各集群中的服务，但今天这需要非复杂的定制解决方案。\n多集群服务 API 旨在解决这些问题。\n参考 KEP-1645: Multi-Cluster Services API - github.com ","relpermalink":"/kubernetes-handbook/multi-cluster/multi-cluster-services-api/","summary":"2020 年初，Kubernetes 社区提议 Multi-Cluster Services API，旨在解决长久以来就存在的 Kubernetes 多集群服务管理问题。 Kubernetes 用户可能希望将他们的部署分成多个集群，但仍然保留在这些集群中运行的工作负载之间的相互依赖关系，这有很多原","title":"多集群服务 API（Multi-Cluster Services API）"},{"content":" 注意\nKubefed 项目计划归档，详见 Fellow-up: discussion on archiving Kubefed。 Kubernetes 从 1.8 版本起就声称单集群最多可支持 5000 个节点和 15 万个 Pod，我相信很少有公司会部署如此庞大的一个单集群，总有很多情况下因为各种各样的原因我们可能会部署多个集群，但是有时候有想将他们统一起来管理，这时候就需要用到集群联邦（Federation）。\n为什么要使用集群联邦 Federation 使管理多个集群变得简单。它通过提供两个主要构建模块来实现：\n跨集群同步资源：Federation 提供了在多个集群中保持资源同步的能力。例如，可以保证同一个 deployment 在多个集群中存在。 跨集群服务发现：Federation 提供了自动配置 DNS 服务以及在所有集群后端上进行负载均衡的能力。例如，可以提供一个全局 VIP 或者 DNS 记录，通过它可以访问多个集群后端。 Federation 还可以提供一些其它用例：\n高可用：通过在集群间分布负载并自动配置 DNS 服务和负载均衡，federation 最大限度地减 …","relpermalink":"/kubernetes-handbook/multi-cluster/federation/","summary":"注意 Kubefed 项目计划归档，详见 Fellow-up: discussion on archiving Kubefed。 Kubernetes 从 1.8 版本起就声称单集群最多可支持 5000 个节点和 15 万个 Pod，我相信很少有公司会部署如此庞大的一个单集群，总有很多情况下因为各种各样的原因我们可能会部署多个","title":"集群联邦（Cluster Federation）"},{"content":"Kubernetes 作为一个容器编排调度引擎，资源调度是它的最基本也是最重要的功能，这一节中我们将着重讲解 Kubernetes 中是如何做资源调度的。\nKubernetes 中有一个叫做 kube-scheduler 的组件，该组件就是专门监听 kube-apiserver 中是否有还未调度到 node 上的 pod，再通过特定的算法为 pod 指定分派 node 运行。\nKubernetes 中的众多资源类型，例如 Deployment、DaemonSet、StatefulSet 等都已经定义了 Pod 运行的一些默认调度策略，但是如果我们细心的根据 node 或者 pod 的不同属性，分别为它们打上标签之后，我们将发现 Kubernetes 中的高级调度策略是多么强大。当然如果要实现动态的资源调度，即 pod 已经调度到某些节点上后，因为一些其它原因，想要让 pod 重新调度到其它节点。\n考虑以下两种情况：\n集群中有新增节点，想要让集群中的节点的资源利用率比较均衡一些，想要将一些高负载的节点上的 pod 驱逐到新增节点上，这是 kuberentes 的 scheduler 所不 …","relpermalink":"/kubernetes-handbook/cluster/scheduling/","summary":"Kubernetes 作为一个容器编排调度引擎，资源调度是它的最基本也是最重要的功能，这一节中我们将着重讲解 Kubernetes 中是如何做资源调度的。 Kubernetes 中有一个叫做 kube-scheduler 的组件，该组件就是专门监听 kube-apiserver 中是否有还未调度到 node 上的 pod，再通过特定的","title":"资源调度"},{"content":"QoS（Quality of Service），大部分译为“服务质量等级”，又译作“服务质量保证”，是作用在 Pod 上的一个配置，当 Kubernetes 创建一个 Pod 时，它就会给这个 Pod 分配一个 QoS 等级，可以是以下等级之一：\nGuaranteed：Pod 里的每个容器都必须有内存/CPU 限制和请求，而且值必须相等。 Burstable：Pod 里至少有一个容器有内存或者 CPU 请求且不满足 Guarantee 等级的要求，即内存/CPU 的值设置的不同。 BestEffort：容器必须没有任何内存或者 CPU 的限制或请求。 该配置不是通过一个配置项来配置的，而是通过配置 CPU/内存的 limits 与 requests 值的大小来确认服务质量等级的。使用 kubectl get pod -o yaml 可以看到 pod 的配置输出中有 qosClass 一项。该配置的作用是为了给资源调度提供策略支持，调度算法根据不同的服务质量等级可以确定将 pod 调度到哪些节点上。\n例如，下面这个 YAML 配置中的 Pod …","relpermalink":"/kubernetes-handbook/cluster/qos/","summary":"QoS（Quality of Service），大部分译为“服务质量等级”，又译作“服务质量保证”，是作用在 Pod 上的一个配置，当 Kubernetes 创建一个 Pod 时，它就会给这个 Pod 分配一个 QoS 等级，可以是以下等级之一： Guarant","title":"服务质量等级（QoS）"},{"content":"当你使用 Kubernetes 的时候，有没有遇到过 Pod 在启动后一会就挂掉然后又重新启动这样的恶性循环？你有没有想过 Kubernetes 是如何检测 pod 是否还存活？虽然容器已经启动，但是 Kubernetes 如何知道容器的进程是否准备好对外提供服务了呢？让我们通过 Kubernetes 官网的这篇文章 Configure Liveness and Readiness Probes，来一探究竟。\n本文将展示如何配置容器的存活和可读性探针。\nKubelet 使用 liveness probe（存活探针）来确定何时重启容器。例如，当应用程序处于运行状态但无法做进一步操作，liveness 探针将捕获到 deadlock，重启处于该状态下的容器，使应用程序在存在 bug 的情况下依然能够继续运行下去（谁的程序还没几个 bug 呢）。\nKubelet 使用 readiness probe（就绪探针）来确定容器是否已经就绪可以接受流量。只有当 Pod 中的容器都处于就绪状态时 kubelet 才会认定该 Pod 处于就绪状态。该信号的作用是控制哪些 Pod 应该作为 service …","relpermalink":"/kubernetes-handbook/config/liveness-readiness-probes/","summary":"当你使用 Kubernetes 的时候，有没有遇到过 Pod 在启动后一会就挂掉然后又重新启动这样的恶性循环？你有没有想过 Kubernetes 是如何检测 pod 是否还存活？虽然容器已经启动，但是 Kubernetes 如何知道容器的进程是否准备好对外提供服务了呢？让我们通过 Kubernetes","title":"配置 Pod 的 liveness 和 readiness 探针"},{"content":"Service account 为 Pod 中的进程提供身份信息。\n本文是关于 Service Account 的用户指南，管理指南另见 Service Account 的集群管理指南。\n注意：本文档描述的关于 Service Account 的行为只有当您按照 Kubernetes 项目建议的方式搭建起集群的情况下才有效。您的集群管理员可能在您的集群中有自定义配置，这种情况下该文档可能并不适用。\n当您（真人用户）访问集群（例如使用kubectl命令）时，apiserver 会将您认证为一个特定的 User Account（目前通常是admin，除非您的系统管理员自定义了集群配置）。Pod 容器中的进程也可以与 apiserver 联系。当它们在联系 apiserver 的时候，它们会被认证为一个特定的 Service Account（例如default）。\n使用默认的 Service Account 访问 API server 当您创建 pod 的时候，如果您没有指定一个 service account，系统会自动得在与该 pod 相同的 namespace 下为其指派一 …","relpermalink":"/kubernetes-handbook/config/service-account/","summary":"Service account 为 Pod 中的进程提供身份信息。 本文是关于 Service Account 的用户指南，管理指南另见 Service Account 的集群管理指南。 注意：本文档描述的关于 Service Account 的行为只有当您按照 Kubernetes 项目建议的方式搭建起集群的情况下才有效。您的集群管理员可能在您的集","title":"配置 Pod 的 Service Account"},{"content":"Secret 对象类型用来保存敏感信息，例如密码、OAuth 令牌和 ssh key。将这些信息放在 secret 中比放在 pod 的定义中或者 docker 镜像中来说更加安全和灵活。\nSecret 概览 Secret 是一种包含少量敏感信息例如密码、token 或 key 的对象。这样的信息可能会被放在 Pod spec 中或者镜像中；将其放在一个 secret 对象中可以更好地控制它的用途，并降低意外暴露的风险。\n用户可以创建 secret，同时系统也创建了一些 secret。\n要使用 secret，pod 需要引用 secret。Pod 可以用两种方式使用 secret：作为 volume 中的文件被挂载到 pod 中的一个或者多个容器里，或者当 kubelet 为 pod 拉取镜像时使用。\n内置 secret Service Account 使用 API 凭证自动创建和附加 secret Kubernetes 自动创建包含访问 API 凭据的 secret，并自动修改您的 pod 以使用此类型的 secret。\n如果需要，可以禁用或覆盖自动创建和使用 API 凭据。但是，如 …","relpermalink":"/kubernetes-handbook/config/secret/","summary":"Secret 对象类型用来保存敏感信息，例如密码、OAuth 令牌和 ssh key。将这些信息放在 secret 中比放在 pod 的定义中或者 docker 镜像中来说更加安全和灵活。 Secret 概览 Secret 是一种包含少量敏感信息例如密码、token 或 key 的对象。这样的信","title":"Secret 配置"},{"content":"当用多个团队或者用户共用同一个集群的时候难免会有资源竞争的情况发生，这时候就需要对不同团队或用户的资源使用配额做出限制。\n开启资源配额限制功能 目前有两种资源分配管理相关的控制策略插件 ResourceQuota 和 LimitRange。\n要启用它们只要 API Server 的启动配置的 KUBE_ADMISSION_CONTROL 参数中加入了 ResourceQuota 的设置，这样就给集群开启了资源配额限制功能，加入 LimitRange 可以用来限制一个资源申请的范围限制，参考 为 namesapce 配置默认的内存请求与限额 和 在 namespace 中配置默认的 CPU 请求与限额。\n两种控制策略的作用范围都是对于某一 namespace，ResourceQuota 用来限制 namespace 中所有的 Pod 占用的总的资源 request 和 limit，而 LimitRange 是用来设置 namespace 中 Pod 的默认的资源 request 和 limit 值。\n资源配额分为三种类型：\n计算资源配额 存储资源配额 对象数量配额 关于资源配额的详细信息 …","relpermalink":"/kubernetes-handbook/config/quota/","summary":"当用多个团队或者用户共用同一个集群的时候难免会有资源竞争的情况发生，这时候就需要对不同团队或用户的资源使用配额做出限制。 开启资源配额限制功能 目前有两种资源分配管理相关的控制策略插件 ResourceQuota 和 LimitRan","title":"管理 namespace 中的资源配额"},{"content":"对于没有使用过 Kubernetes 的 Docker 用户，如何快速掌握 kubectl 命令？\n在本文中，我们将向 docker-cli 用户介绍 Kubernetes 命令行如何与 api 进行交互。该命令行工具——kubectl，被设计成 docker-cli 用户所熟悉的样子，但是它们之间又存在一些必要的差异。该文档将向您展示每个 docker 子命令和 kubectl 与其等效的命令。\n在使用 kubernetes 集群的时候，docker 命令通常情况是不需要用到的，只有在调试程序或者容器的时候用到，我们基本上使用 kubectl 命令即可，所以在操作 kubernetes 的时候我们抛弃原先使用 docker 时的一些观念。\ndocker run 如何运行一个 nginx Deployment 并将其暴露出来？查看 kubectl run 。\n使用 docker 命令：\n$ docker run -d --restart=always -e DOMAIN=cluster --name nginx-app -p 80:80 nginx …","relpermalink":"/kubernetes-handbook/cli/docker-cli-to-kubectl/","summary":"对于没有使用过 Kubernetes 的 Docker 用户，如何快速掌握 kubectl 命令？ 在本文中，我们将向 docker-cli 用户介绍 Kubernetes 命令行如何与 api 进行交互。该命令行工具——kubectl，被设计成 docker-cli 用户所熟悉的样子，但是它们之间又存在一些必要的差异。该文档","title":"Docker 用户过渡到 kubectl 命令行指南"},{"content":"Kubernetes 提供的 kubectl 命令是与集群交互最直接的方式，v1.6 版本的 kubectl 命令参考图如下：\nkubectl cheatsheet Kubectl 的子命令主要分为 8 个类别：\n基础命令（初学者都会使用的） 基础命令（中级） 部署命令 集群管理命令 故障排查和调试命令 高级命令 设置命令 其他命令 熟悉这些命令有助于大家来操作和管理 kubernetes 集群。\n命令行提示 为了使用 kubectl 命令更加高效，我们可以选择安装一下开源软件来增加操作 kubectl 命令的快捷方式，同时为 kubectl 命令增加命令提示。\n增加 kubeclt 命令的工具（图片来自网络） kubectx：用于切换 Kubernetes context kube-ps1：为命令行终端增加$PROMPT字段 kube-shell：交互式带命令提示的 kubectl 终端 全部配置完成后的 kubectl 终端如下图所示：\n增强的 kubectl 命令 开源项目 kube-shell 可以为 kubectl 提供自动的命令提示和补全，使用起来特别方便，推荐给大家。 …","relpermalink":"/kubernetes-handbook/cli/using-kubectl/","summary":"Kubernetes 提供的 kubectl 命令是与集群交互最直接的方式，v1.6 版本的 kubectl 命令参考图如下： kubectl cheatsheet Kubectl 的子命令主要分为 8 个类别： 基础命令（初学者都会使用的） 基础命令（中级） 部署命令 集群管理命令 故障排查和调试命令 高级命令 设置命","title":"Kubectl 命令概览"},{"content":"kubectl 命令是操作 Kubernetes 集群的最直接和最高效的途径，这个 60 多 MB 大小的二进制文件，到底有啥能耐呢？\nKubectl 自动补全 $ source \u0026lt;(kubectl completion bash) # setup autocomplete in bash, bash-completion package should be installed first. $ source \u0026lt;(kubectl completion zsh) # setup autocomplete in zsh Kubectl 上下文和配置 设置 kubectl 命令交互的 kubernetes 集群并修改配置信息。参阅 使用 kubeconfig 文件进行跨集群验证 获取关于配置文件的详细信息。\n$ kubectl config view # 显示合并后的 kubeconfig 配置 # 同时使用多个 kubeconfig 文件并查看合并后的配置 $ KUBECONFIG=~/.kube/config:~/.kube/kubconfig2 kubectl config view …","relpermalink":"/kubernetes-handbook/cli/kubectl-cheatsheet/","summary":"kubectl 命令是操作 Kubernetes 集群的最直接和最高效的途径，这个 60 多 MB 大小的二进制文件，到底有啥能耐呢？ Kubectl 自动补全 $ source \u003c(kubectl completion bash) # setup autocomplete in bash, bash-completion package should be installed first. $ source \u003c(kubectl completion zsh) # setup autocomplete in zsh Kubectl 上下文和配置 设置 kubectl 命令交互的 kubernetes 集群并修改配置信息。","title":"Kubectl 命令技巧大全"},{"content":"Kubenretes1.6 中使用 etcd V3 版本的 API，使用 etcdctl 直接 ls 的话只能看到 /kube-centos 一个路径。需要在命令前加上 ETCDCTL_API=3 这个环境变量才能看到 kuberentes 在 etcd 中保存的数据。\nETCDCTL_API=3 etcdctl get /registry/namespaces/default -w=json|python -m json.tool 如果是使用 kubeadm 创建的集群，在 Kubenretes 1.11 中，etcd 默认使用 tls，这时你可以在 master 节点上使用以下命令来访问 etcd：\nETCDCTL_API=3 etcdctl --cacert=/etc/kubernetes/pki/etcd/ca.crt \\ --cert=/etc/kubernetes/pki/etcd/peer.crt \\ --key=/etc/kubernetes/pki/etcd/peer.key \\ get /registry/namespaces/default -w=json | …","relpermalink":"/kubernetes-handbook/cli/etcdctl/","summary":"Kubenretes1.6 中使用 etcd V3 版本的 API，使用 etcdctl 直接 ls 的话只能看到 /kube-centos 一个路径。需要在命令前加上 ETCDCTL_API=3 这个环境变量才能看到 kuberentes 在 etcd 中保存的数据。 ETCDCTL_API=3 etcdctl get /registry/namespaces/default -w=json|python -m json.tool 如果是使用 kubeadm 创建的集群，在 Kubenretes 1.11 中，etcd 默认使用 tls，这时你可","title":"使用 etcdctl 访问 Kubernetes 数据"},{"content":"在使用二进制文件部署 Kubernetes 集群的时候，很多人在进行到部署证书时遇到各种各样千奇百怪的问题，这一步是创建集群的基础，我们有必要详细了解一下其背后的流程和原理。\n概览 每个 Kubernetes 集群都有一个集群根证书颁发机构（CA）。集群中的组件通常使用 CA 来验证 API server 的证书，由 API 服务器验证 kubelet 客户端证书等。为了支持这一点，CA 证书包被分发到集群中的每个节点，并作为一个 secret 附加分发到默认 service account 上。或者，你的 workload 可以使用此 CA 建立信任。你的应用程序可以使用类似于 ACME 草案的协议，使用 certificates.k8s.io API 请求证书签名。\n集群中的 TLS 信任 让 Pod 中运行的应用程序信任集群根 CA 通常需要一些额外的应用程序配置。您将需要将 CA 证书包添加到 TLS 客户端或服务器信任的 CA 证书列表中。例如，您可以使用 golang TLS 配置通过解析证书链并将解析的证书添加到 tls.Config结构中的 Certificates 字 …","relpermalink":"/kubernetes-handbook/security/managing-tls-in-a-cluster/","summary":"在使用二进制文件部署 Kubernetes 集群的时候，很多人在进行到部署证书时遇到各种各样千奇百怪的问题，这一步是创建集群的基础，我们有必要详细了解一下其背后的流程和原理。 概览 每个 Kubernetes 集群都有一个集群根证书颁发机构（CA）","title":"管理集群中的 TLS"},{"content":"Kubelet 的 HTTPS 端点对外暴露了用于访问不同敏感程度数据的 API，并允许您在节点或者容器内执行不同权限级别的操作。\n本文档向您描述如何通过认证授权来访问 kubelet 的 HTTPS 端点。\nKubelet 认证 默认情况下，所有未被配置的其他身份验证方法拒绝的，对 kubelet 的 HTTPS 端点的请求将被视为匿名请求，并被授予 system:anonymous 用户名和 system:unauthenticated 组。\n如果要禁用匿名访问并发送 401 Unauthorized 的未经身份验证的请求的响应：\n启动 kubelet 时指定 --anonymous-auth=false 标志 如果要对 kubelet 的 HTTPS 端点启用 X509 客户端证书身份验证：\n启动 kubelet 时指定 --client-ca-file 标志，提供 CA bundle 以验证客户端证书 启动 apiserver 时指定 --kubelet-client-certificate 和 --kubelet-client-key 标志 参阅 apiserver …","relpermalink":"/kubernetes-handbook/security/kubelet-authentication-authorization/","summary":"Kubelet 的 HTTPS 端点对外暴露了用于访问不同敏感程度数据的 API，并允许您在节点或者容器内执行不同权限级别的操作。 本文档向您描述如何通过认证授权来访问 kubelet 的 HTTPS 端点。 Kubelet 认证 默认情况下，所有未被配置的其他身份验证方法拒","title":"Kublet 的认证授权"},{"content":"本文档介绍如何为 kubelet 设置 TLS 客户端证书引导（bootstrap）。\nKubernetes 1.4 引入了一个用于从集群级证书颁发机构（CA）请求证书的 API。此 API 的原始目的是为 kubelet 提供 TLS 客户端证书。可以在 这里 找到该提议，在 feature #43 追踪该功能的进度。\nkube-apiserver 配置 您必须提供一个 token 文件，该文件中指定了至少一个分配给 kubelet 特定 bootstrap 组的“bootstrap token”。\n该组将作为 controller manager 配置中的默认批准控制器而用于审批。随着此功能的成熟，您应该确保 token 被绑定到基于角色的访问控制（RBAC）策略上，该策略严格限制了与证书配置相关的客户端请求（使用 bootstrap token）。使用 RBAC，将 token 范围划分为组可以带来很大的灵活性（例如，当您配置完成节点后，您可以禁用特定引导组的访问）。\nToken 认证文件 Token 可以是任意的，但应该可以表示为从安全随机数生成器（ …","relpermalink":"/kubernetes-handbook/security/tls-bootstrapping/","summary":"本文档介绍如何为 kubelet 设置 TLS 客户端证书引导（bootstrap）。 Kubernetes 1.4 引入了一个用于从集群级证书颁发机构（CA）请求证书的 API。此 API 的原始目的是为 kubelet 提供 TLS 客户端证书。可以在 这里 找到该提议，在 feature #43 追踪该功","title":"TLS Bootstrap"},{"content":"本文将讲述如何配置和启用 ip-masq-agent。\n创建 ip-masq-agent 要创建 ip-masq-agent，运行下面的 kubectl 命令：\nkubectl create -f https://raw.githubusercontent.com/kubernetes-incubator/ip-masq-agent/master/ip-masq-agent.yaml 关于 ip-masq-agent 的更多信息请参考 该文档。\n在大多数情况下，默认的一套规则应该是足够的；但是，如果内置的规则不适用于您的集群，您可以创建并应用 ConfigMap 来自定义受影响的 IP 范围。例如，为了仅允许 ip-masq-agent 考虑 10.0.0.0/8，您可以在名为“config”的文件中创建以下 ConfigMap。\nnonMasqueradeCIDRs: - 10.0.0.0/8 resyncInterval: 60s 注意：重要的是，该文件被命名为 config，因为默认情况下，该文件将被用作 ip-masq-agent 查找的关键字。 …","relpermalink":"/kubernetes-handbook/security/ip-masq-agent/","summary":"本文将讲述如何配置和启用 ip-masq-agent。 创建 ip-masq-agent 要创建 ip-masq-agent，运行下面的 kubectl 命令： kubectl create -f https://raw.githubusercontent.com/kubernetes-incubator/ip-masq-agent/master/ip-masq-agent.yaml 关于 ip-masq-agent 的更多信息请参考 该文档。 在大多数情况下，默认的一套规则应该是足够的；但是，如","title":"IP 伪装代理"},{"content":"当我们安装好集群后，如果想要把 kubectl 命令交给用户使用，就不得不对用户的身份进行认证和对其权限做出限制。\n下面以创建一个 devuser 用户并将其绑定到 dev 和 test 两个 namespace 为例说明。\n创建 CA 证书和秘钥 创建 devuser-csr.json 文件\n{ \u0026#34;CN\u0026#34;: \u0026#34;devuser\u0026#34;, \u0026#34;hosts\u0026#34;: [], \u0026#34;key\u0026#34;: { \u0026#34;algo\u0026#34;: \u0026#34;rsa\u0026#34;, \u0026#34;size\u0026#34;: 2048 }, \u0026#34;names\u0026#34;: [ { \u0026#34;C\u0026#34;: \u0026#34;CN\u0026#34;, \u0026#34;ST\u0026#34;: \u0026#34;BeiJing\u0026#34;, \u0026#34;L\u0026#34;: \u0026#34;BeiJing\u0026#34;, \u0026#34;O\u0026#34;: \u0026#34;k8s\u0026#34;, \u0026#34;OU\u0026#34;: \u0026#34;System\u0026#34; } ] } 生成 CA 证书和私钥\n下面我们在 master 节点上为 devuser 创建证书和秘钥，在 /etc/kubernetes/ssl 目录下执行以下命令：\n执行该命令前请先确保该目录下已经包含如下文件：\nca-key.pem ca.pem ca-config.json devuser-csr.json $ cfssl gencert -ca=ca.pem …","relpermalink":"/kubernetes-handbook/security/kubectl-user-authentication-authorization/","summary":"当我们安装好集群后，如果想要把 kubectl 命令交给用户使用，就不得不对用户的身份进行认证和对其权限做出限制。 下面以创建一个 devuser 用户并将其绑定到 dev 和 test 两个 namespace 为例说明。 创建 CA 证书和秘钥 创建 devuser-csr.json 文件 { \"CN\": \"devuser\", \"hosts\": [], \"key\": { \"algo\": \"rsa\", \"size\": 2048","title":"创建用户认证授权的 kubeconfig 文件"},{"content":"在开启了 TLS 的集群中，每当与集群交互的时候少不了的是身份认证，使用 kubeconfig（即证书）和 token 两种认证方式是最简单也最通用的认证方式，在 dashboard 的登录功能就可以使用这两种登录功能。\n下文分两块以示例的方式来讲解两种登陆认证方式：\n为 brand 命名空间下的 brand 用户创建 kubeconfig 文件 为集群的管理员（拥有所有命名空间的 amdin 权限）创建 token 使用 kubeconfig 如何生成kubeconfig文件请参考创建用户认证授权的 kubeconfig 文件。\n注意我们生成的 kubeconfig 文件中没有 token 字段，需要手动添加该字段。\n比如我们为 brand namespace 下的 brand 用户生成了名为 brand.kubeconfig 的 kubeconfig 文件，还要再该文件中追加一行 token 的配置（如何生成 token 将在下文介绍），如下所示：\nkubeconfig 文件 对于访问 dashboard 时候的使用 kubeconfig 文件如brand.kubeconfig 必 …","relpermalink":"/kubernetes-handbook/security/auth-with-kubeconfig-or-token/","summary":"在开启了 TLS 的集群中，每当与集群交互的时候少不了的是身份认证，使用 kubeconfig（即证书）和 token 两种认证方式是最简单也最通用的认证方式，在 dashboard 的登录功能就可以使用这两种登录功能。 下文分两块以示例的方式","title":"使用 kubeconfig 或 token 进行用户身份认证"},{"content":"在安装集群的时候我们在 master 节点上生成了一堆证书、token，还在 kubelet 的配置中用到了 bootstrap token，安装各种应用时，为了能够与 API server 通信创建了各种 service account，在 Dashboard 中使用了 kubeconfig 或 token 登陆，那么这些都属于什么认证方式？如何区分用户的？我特地翻译了下这篇官方文档，想你看了之后你将找到答案。\n重点查看 bearer token 和 HTTP 认证中的 token 使用，我们已经有所应用，如 使用 kubeconfig 或 token 进行用户身份认证。\n认识 Kubernetes 中的用户 Kubernetes 集群中包含两类用户：一类是由 Kubernetes 管理的 service account，另一类是普通用户。\n普通用户被假定为由外部独立服务管理。管理员分发私钥，用户存储（如 Keystone 或 Google 帐户），甚至包含用户名和密码列表的文件。在这方面，Kubernetes 没有代表普通用户帐户的对象。无法通过 API 调用的方式向集群中添加普通 …","relpermalink":"/kubernetes-handbook/security/authentication/","summary":"在安装集群的时候我们在 master 节点上生成了一堆证书、token，还在 kubelet 的配置中用到了 bootstrap token，安装各种应用时，为了能够与 API server 通信创建了各种 service account，在 Dashboard 中使用了 kubeconfig 或 token 登陆，那么这些都属于什么认证","title":"Kubernetes 中的用户与身份认证授权"},{"content":"本文是对 Kubernetes 集群安全性管理的最佳实践。\n端口 请注意管理好以下端口。\n端口 进程 描述 4149/TCP kubelet 用于查询容器监控指标的 cAdvisor 端口 10250/TCP kubelet 访问节点的 API 端口 10255/TCP kubelet 未认证的只读端口，允许访问节点状态 10256/TCP kube-proxy kube-proxy 的健康检查服务端口 9099/TCP calico-felix calico 的健康检查服务端口（如果使用 calico/canal） 6443/TCP kube-apiserver Kubernetes API 端口 Kubernetes 安全扫描工具 kube-bench kube-bench 可以消除大约 kubernetes 集群中 95％的配置缺陷。通过应用 CIS Kubernetes Benchmark 来检查 master 节点、node 节点及其控制平面组件，从而确保集群设置了特定安全准则。在经历特定的 Kubernetes 安全问题或安全增强功能之前，这应该是第一步。\nAPI 设置 授 …","relpermalink":"/kubernetes-handbook/security/kubernetes-security-best-practice/","summary":"本文是对 Kubernetes 集群安全性管理的最佳实践。 端口 请注意管理好以下端口。 端口 进程 描述 4149/TCP kubelet 用于查询容器监控指标的 cAdvisor 端口 10250/TCP kubelet 访问节点的 API 端口 10255/TCP kubelet 未认证的只读端口，允许访问节点状态 10256/TCP kube-proxy kube-proxy 的健康检查服务端口 9099/TCP calico-felix calico 的健康","title":"Kubernetes 集群安全性配置最佳实践"},{"content":"本文列举了集中访问 Kubernetes 集群的方式。\n第一次使用 kubectl 访问 如果您是第一次访问 Kubernetes API 的话，我们建议您使用 Kubernetes 命令行工具：kubectl。\n为了访问集群，您需要知道集群的地址，并且需要有访问它的凭证。通常，如果您完成了入门指南那么这些将会自动设置，或者其他人为您部署的集群提供并给您凭证和集群地址。\n使用下面的命令检查 kubectl 已知的集群的地址和凭证：\n$ kubectl config view 直接访问 REST API Kubectl 处理对 apiserver 的定位和认证。如果您想直接访问 REST API，可以使用像 curl、wget 或浏览器这样的 http 客户端，有以下几种方式来定位和认证：\n以 proxy 模式运行 kubectl。 推荐方法。 使用已保存的 apiserver 位置信息。 使用自签名证书验证 apiserver 的身份。没有 MITM（中间人攻击）的可能。 认证到 apiserver。 将来，可能会做智能的客户端负载均衡和故障转移。 直接向 http 客户端提供位置和凭 …","relpermalink":"/kubernetes-handbook/access/methods/","summary":"本文列举了集中访问 Kubernetes 集群的方式。 第一次使用 kubectl 访问 如果您是第一次访问 Kubernetes API 的话，我们建议您使用 Kubernetes 命令行工具：kubectl。 为了访问集群，您需要知道集群的地址，并且需要有访问它的凭证。通常，如果您完成了入","title":"访问集群"},{"content":"Kubernetes 的认证方式对于不同的人来说可能有所不同。\n运行 kubelet 可能有一种认证方式（即证书）。 用户可能有不同的认证方式（即令牌）。 管理员可能具有他们为个人用户提供的证书列表。 我们可能有多个集群，并希望在同一个地方将其全部定义——这样用户就能使用自己的证书并重用相同的全局配置。 所以为了能够让用户轻松地在多个集群之间切换，对于多个用户的情况下，我们将其定义在了一个 kubeconfig 文件中。\n此文件包含一系列与昵称相关联的身份验证机制和集群连接信息。它还引入了一个（用户）认证信息元组和一个被称为上下文的与昵称相关联的集群连接信息的概念。\n如果明确指定，则允许使用多个 kubeconfig 文件。在运行时，它们与命令行中指定的覆盖选项一起加载并合并（参见下面的 规则）。\n相关讨论 http://issue.k8s.io/1755\nKubeconfig 文件的组成 Kubeconifg 文件示例 current-context: federal-context apiVersion: v1 clusters: - cluster: api-version: …","relpermalink":"/kubernetes-handbook/access/authenticate-across-clusters-kubeconfig/","summary":"Kubernetes 的认证方式对于不同的人来说可能有所不同。 运行 kubelet 可能有一种认证方式（即证书）。 用户可能有不同的认证方式（即令牌）。 管理员可能具有他们为个人用户提供的证书列表。 我们可能有多个集群，并希望在同一个地方将其","title":"使用 kubeconfig 文件配置跨集群认证"},{"content":"本页向您展示如何使用 kubectl port-forward 命令连接到运行在 Kubernetes 集群中的 Redis 服务器。这种类型的连接对于数据库调试很有帮助。\n创建一个 Pod 来运行 Redis 服务器 创建一个 Pod：\nkubectl create -f https://k8s.io/docs/tasks/access-application-cluster/redis-master.yaml 命令运行成功后将有以下输出验证该 Pod 是否已经创建：\npod \u0026#34;redis-master\u0026#34; created 检查 Pod 是否正在运行且处于就绪状态：\nkubectl get pods 当 Pod 就绪，输出显示 Running 的状态：\nNAME READY STATUS RESTARTS AGE redis-master 2/2 Running 0 41s 验证 Redis 服务器是否已在 Pod 中运行，并监听 6379 端口：\n{% raw %} kubectl get pods redis-master --template=\u0026#39;{{(index (index …","relpermalink":"/kubernetes-handbook/access/connecting-to-applications-port-forward/","summary":"本页向您展示如何使用 kubectl port-forward 命令连接到运行在 Kubernetes 集群中的 Redis 服务器。这种类型的连接对于数据库调试很有帮助。 创建一个 Pod 来运行 Redis 服务器 创建一个 Pod： kubectl create -f https://k8s.io/docs/tasks/access-application-cluster/redis-master.yaml 命令运行成功后将有以下输出验证该 Pod 是否已经创建： pod \"redis-master\" created","title":"通过端口转发访问集群中的应用程序"},{"content":"本文向您展示如何创建 Kubernetes Service 对象，外部客户端可以使用它来访问集群中运行的应用程序。该 Service 可以为具有两个运行实例的应用程序提供负载均衡。\n目的 运行 Hello World 应用程序的两个实例。 创建一个暴露 node 节点端口的 Service 对象。 使用 Service 对象访问正在运行的应用程序。 为在两个 pod 中运行的应用程序创建 service 在集群中运行 Hello World 应用程序：\nkubectl run hello-world --replicas=2 --labels=\u0026#34;run=load-balancer-example\u0026#34; --image=gcr.io/google-samples/node-hello:1.0 --port=8080 上述命令创建一个 Deployment 对象和一个相关联的 ReplicaSet 对象。该 ReplicaSet 有两个 Pod，每个 Pod 中都运行一个 Hello World 应用程序。\n显示关于该 Deployment 的信息：\nkubectl get …","relpermalink":"/kubernetes-handbook/access/service-access-application-cluster/","summary":"本文向您展示如何创建 Kubernetes Service 对象，外部客户端可以使用它来访问集群中运行的应用程序。该 Service 可以为具有两个运行实例的应用程序提供负载均衡。 目的 运行 Hello World 应用程序的两个实例。 创建一个暴露 node 节点端口的 Service 对象。 使用 Service 对","title":"使用 service 访问群集中的应用程序"},{"content":"前面几节讲到如何访问 Kubernetes 集群，本文主要讲解访问 Kubernetes 中的 Pod 和 Serivce 的几种方式，包括如下几种：\nhostNetwork hostPort NodePort LoadBalancer Ingress 说是暴露 Pod 其实跟暴露 Service 是一回事，因为 Pod 就是 Service 的后端。\nhostNetwork: true 这是一种直接定义 Pod 网络的方式。\n如果在 Pod 中使用 hostNotwork:true 配置的话，在这种 pod 中运行的应用程序可以直接看到 pod 启动的主机的网络接口。在主机的所有网络接口上都可以访问到该应用程序。以下是使用主机网络的 pod 的示例定义：\napiVersion: v1 kind: Pod metadata: name: influxdb spec: hostNetwork: true containers: - name: influxdb image: influxdb 部署该 Pod：\n$ kubectl create -f …","relpermalink":"/kubernetes-handbook/access/accessing-kubernetes-pods-from-outside-of-the-cluster/","summary":"前面几节讲到如何访问 Kubernetes 集群，本文主要讲解访问 Kubernetes 中的 Pod 和 Serivce 的几种方式，包括如下几种： hostNetwork hostPort NodePort LoadBalancer Ingress 说是暴露 Pod 其实跟暴露 Service 是一回事，因为 Pod 就是 Service 的后端。 hostNetwork: true 这是一种直接定义 Pod 网络的方式。 如果在 Pod 中使用 hostNotwork:true 配置的","title":"从外部访问 Kubernetes 中的 Pod"},{"content":"Lens 是一款开源的 Kubenretes IDE，也可以作为桌面客户端，官方网站 https://k8slens.dev，具有以下特性：\n完全开源，GitHub 地址 https://github.com/lensapp/lens 实时展示集群状态 内置 Prometheus 监控 多集群，多个 namespace 管理 原生 Kubernetes 支持 支持使用 chart 安装应用 使用 kubeconfig 登陆认证 支持多平台，Windows、Mac、Linux Visual Studio Code 友好的风格设计 Lens 界面图下图所示。\nLens Kubernetes IDE 界面 参考 Lens, Kubernetes IDE - k8slens.dev ","relpermalink":"/kubernetes-handbook/access/lens/","summary":"Lens 是一款开源的 Kubenretes IDE，也可以作为桌面客户端，官方网站 https://k8slens.dev，具有以下特性： 完全开源，GitHub 地址 https://github.com/lensapp/lens 实时展示集群状态 内置 Prometheus 监控 多集群，多个 namespace 管理 原生 Kubernetes 支持 支持使用 chart","title":"Lens - Kubernetes IDE"},{"content":"Kubernator 相较于 Kubernetes Dashboard 来说，是一个更底层的 Kubernetes UI，Dashboard 操作的都是 Kubernetes 的底层对象，而 Kubernator 是直接操作 Kubernetes 各个对象的 YAML 文件。\nKubernator 提供了一种基于目录树和关系拓扑图的方式来管理 Kubernetes 的对象的方法，用户可以在 Web 上像通过 GitHub 的网页版一样操作 Kubernetes 的对象，执行修改、拷贝等操作，详细的使用方式见 https://github.com/smpio/kubernator。\n安装 Kubernator Kubernator 的安装十分简单，可以直接使用 kubectl 命令来运行，它不依赖任何其它组件。\nkubectl create ns kubernator kubectl -n kubernator run --image=smpio/kubernator --port=80 kubernator kubectl -n kubernator expose deploy …","relpermalink":"/kubernetes-handbook/access/kubernator-kubernetes-ui/","summary":"Kubernator 相较于 Kubernetes Dashboard 来说，是一个更底层的 Kubernetes UI，Dashboard 操作的都是 Kubernetes 的底层对象，而 Kubernator 是直接操作 Kubernetes 各个对象的 YAML 文件。 Kubernator 提供了一种基于目录树和关系拓扑图的方式来管理 Kubernetes 的对象的方法，用户可以在 Web 上像通过 GitHub","title":"Kubernator - 更底层的 Kubernetes UI"},{"content":"本文讲解了如何开发容器化应用，并使用 Wercker 持续集成工具构建 docker 镜像上传到 docker 镜像仓库中，然后在本地使用 docker-compose 测试后，再使用 kompose 自动生成 kubernetes 的 yaml 文件，再将注入 Envoy sidecar 容器，集成 Istio 服务网格中的详细过程。\n整个过程如下图所示。\n流程图 为了讲解详细流程，我特意写了用 Go 语言开发的示例程序放在 GitHub 中，模拟监控流程：\nk8s-app-monitor-test：生成模拟的监控数据，在接收到 http 请求返回 json 格式的 metrics 信息 K8s-app-monitor-agent：获取监控 metrics 信息并绘图，访问该服务将获得监控图表 API 文档见 k8s-app-monitor-test 中的 api.html 文件，该文档在 API blueprint 中定义，使用 aglio 生成，打开后如图所示：\nAPI 关于服务发现 K8s-app-monitor-agent …","relpermalink":"/kubernetes-handbook/devops/deploy-applications-in-kubernetes/","summary":"本文讲解了如何开发容器化应用，并使用 Wercker 持续集成工具构建 docker 镜像上传到 docker 镜像仓库中，然后在本地使用 docker-compose 测试后，再使用 kompose 自动生成 kubernetes 的 yaml 文件，再将注入 Envoy sidecar 容器，集成 Istio 服务网格中的详细过程。 整个过程如下图所示。 流","title":"适用于 Kubernetes 的应用开发部署流程"},{"content":"本文档不是说明如何在 kubernetes 中开发和部署应用程序，如果您想要直接开发应用程序在 kubernetes 中运行可以参考 适用于 kubernetes 的应用开发部署流程。\n本文旨在说明如何将已有的应用程序尤其是传统的分布式应用程序迁移到 kubernetes 中。如果该类应用程序符合云原生应用规范（如 12 因素法则）的话，那么迁移会比较顺利，否则会遇到一些麻烦甚至是阻碍。具体请参考 迁移至云原生应用架构。\n下图是将单体应用迁移到云原生的步骤。\n将单体应用迁移到云原生 (图片来自 DevOpsDay Toronto) 接下来我们将以 Spark on YARN with kubernetes 为例来说明，该例子足够复杂也很有典型性，了解了这个例子可以帮助大家将自己的应用迁移到 kubernetes 集群上去。\n下图即整个架构的示意图，所有的进程管理和容器扩容直接使用 Makefile。\nspark on yarn with kubernetes 注意：该例子仅用来说明具体的步骤划分和复杂性，在生产环境应用还有待验证，请谨慎使用。\n术语 对于为曾接触过 kubernetes …","relpermalink":"/kubernetes-handbook/devops/migrating-hadoop-yarn-to-kubernetes/","summary":"本文档不是说明如何在 kubernetes 中开发和部署应用程序，如果您想要直接开发应用程序在 kubernetes 中运行可以参考 适用于 kubernetes 的应用开发部署流程。 本文旨在说明如何将已有的应用程序尤其是传统的分布式应用程序迁移到 kubernetes 中。如果该类应用程","title":"迁移传统应用到 Kubernetes 步骤详解——以 Hadoop YARN 为例"},{"content":"StatefulSet 这个对象是专门用来部署用状态应用的，可以为 Pod 提供稳定的身份标识，包括 hostname、启动顺序、DNS 名称等。\n下面以在 Kubernetes1.6 版本中部署 zookeeper 和 kafka 为例讲解 StatefulSet 的使用，其中 kafka 依赖于 zookeeper。\nDockerfile 和配置文件见 zookeeper 和 kafka。\n注：所有的镜像基于 CentOS 系统的 JDK 制作，为我的私人镜像，外部无法访问，yaml 中没有配置持久化存储。\n部署 Zookeeper Dockerfile 中从远程获取 zookeeper 的安装文件，然后在定义了三个脚本：\nzkGenConfig.sh：生成 zookeeper 配置文件 zkMetrics.sh：获取 zookeeper 的 metrics zkOk.sh：用来做 ReadinessProb 我们在来看下这三个脚本的执行结果。\nzkMetrics.sh 脚本实际上执行的是下面的命令：\n$ echo mntr | nc localhost …","relpermalink":"/kubernetes-handbook/devops/using-statefulset/","summary":"StatefulSet 这个对象是专门用来部署用状态应用的，可以为 Pod 提供稳定的身份标识，包括 hostname、启动顺序、DNS 名称等。 下面以在 Kubernetes1.6 版本中部署 zookeeper 和 kafka 为例讲解 StatefulSet 的使用，其中 kafka 依赖于 zookeeper。 Dockerfile 和配置文件","title":"使用 StatefulSet 部署有状态应用"},{"content":"持续集成与交付，简称 CI/CD（Continous Integration/Continous Delivery），是一种软件开发实践，旨在通过自动化软件构建、测试和部署过程，提高应用程序的交付速度和质量。它涉及多个阶段，包括代码管理、构建、测试、部署和监控。CI/CD 可以帮助开发团队更快地迭代和交付新功能，同时减少故障和错误。\n什么是持续集成与交付？ 持续集成（Continuous Integration，CI）是指开发团队通过自动化将代码的集成过程与频率增加到了一个可持续的水平。在持续集成中，开发人员经常将代码提交到共享存储库，并使用自动化构建系统（如 Jenkins、Travis CI 等）对代码进行构建、测试和部署。这有助于尽早发现和解决潜在的问题，并确保代码的稳定性和质量。\n持续交付（Continuous Delivery，CD）是在持续集成的基础上进一步扩展的概念。它强调在持续集成的基础上，通过自动化的部署流程，使软件随时处于可部署状态。持续交付的目标是实现在任何时候都能够轻松、可靠地将软件部署到生产环境中，以便快速响应需求变化或发布新功能。\nCI/CD 的主要优势包 …","relpermalink":"/kubernetes-handbook/devops/ci-cd/","summary":"持续集成与交付，简称 CI/CD（Continous Integration/Continous Delivery），是一种软件开发实践，旨在通过自动化软件构建、测试和部署过程，提高应用程序的交付速度和质量。它涉及多个阶段，包括代码管理、构建、测","title":"持续集成与交付（CI/CD）"},{"content":"Kustomize是一个开源的 Kubernetes 配置管理工具，用于对 Kubernetes 清单文件进行自定义和修改。它允许用户通过分层和声明式的方式管理和定制应用程序的配置，而无需直接修改原始的清单文件，促进了配置的复用和可维护性。\nKustomize 的主要功能包括：\n配置合并：Kustomize 允许用户通过定义基础配置和覆盖配置的方式来合并和定制 Kubernetes 清单文件。基础配置可以作为一个基准，而覆盖配置可以包含对基础配置进行修改和定制的内容。这种分层的方式使得对配置进行管理和修改更加灵活和可维护。\n声明式的配置：Kustomize 使用基于文件的声明式配置格式，使得用户可以以清晰的方式描述应用程序的配置和定制需求。用户可以定义资源的名称、标签、注释、环境变量等，并指定资源之间的关系和依赖。\n配置重用：Kustomize 支持配置的重用和共享。用户可以定义可重用的配置片段，并在多个应用程序中进行引用。这样可以避免重复的配置，提高配置的可维护性和复用性。\n多环境管理：Kustomize 支持多个环境（例如开发、测试、生产）的管理。用户可以根据不同环境的需求，为每个 …","relpermalink":"/kubernetes-handbook/devops/kustomize/","summary":"Kustomize是一个开源的 Kubernetes 配置管理工具，用于对 Kubernetes 清单文件进行自定义和修改。它允许用户通过分层和声明式的方式管理和定制应用程序的配置，而无需直接修改原始的清单文件，促进了配置的复用和可维护性。 Kustomize 的","title":"使用 Kustomize 配置 Kubernetes 应用"},{"content":"Kubernetes 的社区是以 SIG（Special Interest Group 特别兴趣小组）和工作组的形式组织起来的，每个工作组都会定期召开视频会议。\n所有的 SIG 和工作组都使用 slack 和邮件列表沟通。\nKubernetes SIG 主要 SIG 列表 api-machinery：所有 API 级别的功能，包括了 API server、API 注册和发现、通用的 API CRUD 语义，准入控制，编码 / 解码，转换，默认值，持久化层（etcd），OpenAPI，第三方资源，垃圾回收（gc）和客户端库的方方面面。 aws：如何在 AWS 上支持和使用 kubernetes。 apps：在 kubernetes 上部署和运维应用程序。关注开发者和 DevOps 在 kubernetes 上运行应用程序的体验。 architecture：维持 kubernetes 在架构设计上的一致性和原则。 auth：kubernetes 的认证授权、权限管理和安全性策略。 autoscaling：集群的自动缩放，pod 的水平和垂直自动缩放，pod 的资源初始化，pod 监控和指标 …","relpermalink":"/kubernetes-handbook/develop/sigs-and-working-group/","summary":"Kubernetes 的社区是以 SIG（Special Interest Group 特别兴趣小组）和工作组的形式组织起来的，每个工作组都会定期召开视频会议。 所有的 SIG 和工作组都使用 slack 和邮件列表沟通。 Kubernetes SIG 主要 SIG 列表 api-machinery：所有 API 级","title":"SIG 和工作组"},{"content":"我们将在 Mac 上使用 docker 环境编译 kuberentes。\n安装依赖 brew install gnu-tar Docker 环境，至少需要给容器分配 4G 内存，在低于 3G 内存的时候可能会编译失败。\n执行编译 切换目录到 kuberentes 源码的根目录下执行：\n./build/run.sh make 可以在 docker 中执行跨平台编译出二进制文件。\n需要用的的 docker 镜像：\ngcr.io/google_containers/kube-cross:v1.7.5-2 该镜像基于 Ubuntu 构建，大小 2.15G，编译环境中包含以下软件：\nGo1.7.5 etcd protobuf g++ 其他 golang 依赖包 在我自己的电脑上的整个编译过程大概要半个小时。\n编译完成的二进制文件在 /_output/local/go/bin/ 目录下。\n","relpermalink":"/kubernetes-handbook/develop/developing-environment/","summary":"我们将在 Mac 上使用 docker 环境编译 kuberentes。 安装依赖 brew install gnu-tar Docker 环境，至少需要给容器分配 4G 内存，在低于 3G 内存的时候可能会编译失败。 执行编译 切换目录到 kuberentes 源码的根目录下执行： ./build/run.sh make 可以在 docker 中执行跨平台编译出","title":"配置 Kubernetes 开发环境"},{"content":"这篇文章将指导你如何测试 Kubernetes。\n单元测试 单元测试仅依赖于源代码，是测试代码逻辑是否符合预期的最简单方法。\n运行所有的单元测试\nmake test 仅测试指定的 package\n# 单个package make test WHAT=./pkg/api # 多个packages make test WHAT=./pkg/{api,kubelet} 或者，也可以直接用 go test\ngo test -v k8s.io/kubernetes/pkg/kubelet 仅测试指定 package 的某个测试 case\n# Runs TestValidatePod in pkg/api/validation with the verbose flag set make test WHAT=./pkg/api/validation KUBE_GOFLAGS=\u0026#34;-v\u0026#34; KUBE_TEST_ARGS=\u0026#39;-run ^TestValidatePod$\u0026#39; # Runs tests that match the regex ValidatePod|ValidateConfigMap in …","relpermalink":"/kubernetes-handbook/develop/testing/","summary":"这篇文章将指导你如何测试 Kubernetes。 单元测试 单元测试仅依赖于源代码，是测试代码逻辑是否符合预期的最简单方法。 运行所有的单元测试 make test 仅测试指定的 package # 单个package make test WHAT=./pkg/api # 多个package","title":"测试 Kubernetes"},{"content":"访问 kubernetes 集群有以下几种方式：\n方式 特点 支持者 Kubernetes dashboard 直接通过 Web UI 进行操作，简单直接，可定制化程度低 官方支持 kubectl 命令行操作，功能最全，但是比较复杂，适合对其进行进一步的分装，定制功能，版本适配最好 官方支持 client-go 从 kubernetes 的代码中抽离出来的客户端包，简单易用，但需要小心区分 kubernetes 的 API 版本 官方支持 client-python python 客户端，kubernetes-incubator 官方支持 Java client fabric8 中的一部分，kubernetes 的 java 客户端 Red Hat 下面，我们基于 client-go，对 Deployment 升级镜像的步骤进行了定制，通过命令行传递一个 Deployment 的名字、应用容器名和新 image 名字的方式来升级。\nkubernetes-client-go-sample 项目的 main.go 代码如下：\npackage main import ( \u0026#34;flag\u0026#34; …","relpermalink":"/kubernetes-handbook/develop/client-go-sample/","summary":"访问 kubernetes 集群有以下几种方式： 方式 特点 支持者 Kubernetes dashboard 直接通过 Web UI 进行操作，简单直接，可定制化程度低 官方支持 kubectl 命令行操作，功能最全，但是比较复杂，适合对其进行进一步的分装，定制功能，版本适配最好 官方支持 client-go 从 kubernetes 的","title":"client-go 示例"},{"content":"Operator 最初是由 CoreOS（后被 Red Hat 收购）开发的，下面是关于 Operator 的一些基础知识：\nOperator 是用来扩展 Kubernetes API 的特定的应用程序控制器； Operator 用来创建、配置和管理复杂的有状态应用，如数据库、缓存和监控系统； Operator 基于 Kubernetes 的资源和控制器概念之上构建，但同时又包含了应用程序特定的领域知识； 创建 Operator 的关键是 CRD（自定义资源）的设计； Operator 通常作为 Deployment 资源部署在 Kubernetes 中，删掉 Operator 不会影响已使用它创建的自定义资源； Operator Hub 中罗列了目前已知的 Operator。 工作原理 Operator 是将运维人员对软件操作的知识给代码化，同时利用 Kubernetes 强大的抽象来管理大规模的软件应用。\nOperator 使用了 Kubernetes 的自定义资源扩展 API 机制，如使用 CRD（CustomResourceDefinition）来创建。Operator 通过这 …","relpermalink":"/kubernetes-handbook/develop/operator/","summary":"关于 Kubernetes Operator 的原理、用途等基础知识介绍。","title":"Operator"},{"content":"Operator SDK 由 CoreOS 开源，它是用于构建 Kubernetes 原生应用的 SDK，它提供更高级别的 API、抽象和项目脚手架。在阅读本文前请先确认您已经了解 Operator是什么。\n使用 Kubernetes 中原生的对象来部署和管理复杂的应用程序不是那么容易，尤其是要管理整个应用的生命周期、组件的扩缩容，我们之前通常是编写各种脚本，通过调用 Kubernetes 的命令行工具来管理 Kubernetes 上的应用。现在可以通过 CRD（CustomResourceDefinition）来自定义这些复杂操作，通过将运维的知识封装在自定义 API 里来减轻运维人员的负担。同时我们还可以像操作 Kubernetes 的原生资源对象一样，使用 kubectl 来操作 CRD。\n下面我们将安装和试用一下 Operator SDK。\n安装 Operator SDK $ mkdir -p $GOPATH/src/github.com/operator-framework $ cd …","relpermalink":"/kubernetes-handbook/develop/operator-sdk/","summary":"Operator SDK 由 CoreOS 开源，它是用于构建 Kubernetes 原生应用的 SDK，它提供更高级别的 API、抽象和项目脚手架。在阅读本文前请先确认您已经了解 Operator是什么。 使用 Kubernetes 中原生的对象来部署和管理复杂的应用程序不是那么容易，","title":"Operator SDK"},{"content":"Kubebuilder 是一个基于 CRD 来构建 Kubernetes API 的框架，可以使用 CRD 来构建 API、Controller 和 Admission Webhook。\n动机 目前扩展 Kubernetes 的 API 的方式有创建 CRD、使用 Operator SDK 等方式，都需要写很多的样本文件（boilerplate），使用起来十分麻烦。为了能够更方便构建 Kubernetes API 和工具，就需要一款能够事半功倍的工具，与其他 Kubernetes API 扩展方案相比，kubebuilder 更加简单易用，并获得了社区的广泛支持。\n工作流程 Kubebuilder 的工作流程如下：\n创建一个新的工程目录 创建一个或多个资源 API CRD 然后将字段添加到资源 在控制器中实现协调循环（reconcile loop），watch 额外的资源 在集群中运行测试（自动安装 CRD 并自动启动控制器） 更新引导集成测试测试新字段和业务逻辑 使用用户提供的 Dockerfile 构建和发布容器 设计哲学 Kubebuilder …","relpermalink":"/kubernetes-handbook/develop/kubebuilder/","summary":"Kubebuilder 是一个基于 CRD 来构建 Kubernetes API 的框架，可以使用 CRD 来构建 API、Controller 和 Admission Webhook。 动机 目前扩展 Kubernetes 的 API 的方式有创建 CRD、使用 Operator SDK 等方式，都需要写很多的样本文件（boilerplate），","title":"Kubebuilder"},{"content":"本页假定您已经熟悉 Kubernetes 的核心概念并可以轻松的部署自己的应用程序。在浏览了本页面及其链接的内容后，您将会更好的理解如下部分：\n可以在应用程序中使用的高级功能 扩展 Kubernetes API 的各种方法 使用高级功能部署应用 现在您知道了 Kubernetes 中提供的一组 API 对象。理解了 daemonset 和 deployment 之间的区别对于应用程序部署通常是足够的。也就是说，熟悉 Kubernetes 中其它的鲜为人知的功能也是值得的。因为这些功能有时候对于特别的用例是非常强大的。\n容器级功能 如您所知，将整个应用程序（例如容器化的 Rails 应用程序，MySQL 数据库以及所有应用程序）迁移到单个 Pod 中是一种反模式。这就是说，有一些非常有用的模式超出了容器和 Pod 之间的 1:1 的对应关系：\nSidecar 容器：虽然 Pod 中依然需要有一个主容器，你还可以添加一个副容器作为辅助（见 日志示例)。单个 Pod 中的两个容器可以通过共享卷进行通信。 Init 容器：Init 容器在 Pod 的应用容器（如主容器和 sidecar 容器） …","relpermalink":"/kubernetes-handbook/develop/advance-developer/","summary":"本页假定您已经熟悉 Kubernetes 的核心概念并可以轻松的部署自己的应用程序。在浏览了本页面及其链接的内容后，您将会更好的理解如下部分： 可以在应用程序中使用的高级功能 扩展 Kubernetes API 的各种方法 使用高级功能部署应用 现在您知道了","title":"高级开发指南"},{"content":"如果您想参与 Kubernetes 社区，请先阅读下Kubernetes Community这个 GitHub Repo 中的文档，该文档中包括社区的治理形式、社区成员资格申请、提交 Issue、查找问题和提交 PR 的指导等。\n参考 Kubernetes Community Kubernetes Developer Guide Enhencement Tracking and Backlog Kubernetes 官方网站项目 ","relpermalink":"/kubernetes-handbook/develop/contribute/","summary":"如果您想参与 Kubernetes 社区，请先阅读下Kubernetes Community这个 GitHub Repo 中的文档，该文档中包括社区的治理形式、社区成员资格申请、提交 Issue、查找问题和提交 PR 的指导等。 参考 Kubernetes Community Kubernetes Developer Guide Enhencement Tracking and Backlog Kubernetes 官","title":"参与 Kubernetes 社区贡献"},{"content":"Minikube 用于在本地运行 kubernetes 环境，用来开发和测试。\n安装 Minikube 到 GitHub 下载 minikube，我安装的是 minikube v1.11.0。\n下载完成后修改文件名为 minikube，然后 chmod +x minikube，移动到 $PATH 目录下：\nsudo mv ~/Download/minikube-darwin-adm64 /usr/local/bin/ sudo chmod +x /usr/local/bin/minikube 安装 kubectl 方式一\n参考 Install and Set Up kubectl，直接使用二进制文件安装即可。\ncurl -LO https://storage.googleapis.com/kubernetes-release/release/`curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt`/bin/darwin/amd64/kubectl …","relpermalink":"/kubernetes-handbook/develop/minikube/","summary":"Minikube 用于在本地运行 kubernetes 环境，用来开发和测试。 安装 Minikube 到 GitHub 下载 minikube，我安装的是 minikube v1.11.0。 下载完成后修改文件名为 minikube，然后 chmod +x minikube，移动到 $PATH 目录下： sudo mv ~/Download/minikube-darwin-adm64 /usr/local/bin/ sudo chmod +x /usr/local/bin/minikube 安","title":"Minikube"},{"content":"上个周末的 OpenAI“宫斗”大戏相信大家都知晓了，今天正巧看到 Ben Thompson 的这篇总结文章，感觉不错，分享给大家。\n关于作者：Ben Thompson（本·汤普森）是一位知名的科技和商业评论家，他是 Stratechery（《战略》）博客的创始人和作者。Stratechery 是一家专注于科技产业和商业策略分析的网站，汤普森在博客中深入剖析科技公司、产品、行业趋势和商业模式。他的文章通常涵盖了互联网、数字媒体、云计算、人工智能等领域的重要话题。\nBen Thompson 因其深刻的洞察力和独特的分析方法而闻名，他的文章常常引发行业内外的广泛讨论。他的分析不仅关注技术本身，还关注技术如何塑造和影响商业和社会。由于其博客的高质量内容，他已经建立了一支忠实的读者群，并成为了科技产业和商业领域的重要声音之一。\n下文译自：https://stratechery.com/2023/openais-misalignment-and-microsofts-gain/\n摘要： 这篇文章分析了 OpenAI 的使命与商业模式之间的不一致，以及 Microsoft 如何从中受益。文章认 …","relpermalink":"/blog/openais-misalignment-and-microsofts-gain/","summary":" 这篇文章分析了 OpenAI 的使命与商业模式之间的不一致，以及 Microsoft 如何从中受益。文章认为，OpenAI 的 GPT-3 模型是一种强大的通用人工智能，但是它的许可协议限制了它的应用范围。Microsoft 作为 OpenAI 的合作伙伴和投资者，拥有 GPT-3 的独家许可权，可以将其用于自己的产品和服务，从而获得竞争优势。","title":"微软零成本收购 OpenAI？——OpenAI 的错位与微软的收益"},{"content":"本文译自：https://www.infracloud.io/blogs/decoding-workload-specification-for-effective-platform-engineering/\n摘要：这篇文章介绍了工作负载规范的概念和作用，它是一种标准化的方法，可以简化和统一不同环境下的软件部署，减轻开发者的认知负担。\n在理想的情况下，开发人员专注于编写应用程序，而运维团队专注于将它们的工作负载无缝部署到不同的环境中。然而，现实远非如此。\n开发人员经常难以理解每个部署目标的细微差别，这增加了他们的认知负担。例如，他们可能在本地开发中使用 Docker Compose，但他们的工作负载可能在 Kubernetes 环境中部署。这种差距可能导致错误和不一致，影响工作负载的开发和部署。\n这就是为什么有迫切需要通过内部开发者平台（IDP）的标准化方法来配置工作负载。平台工程试图通过提供工具来管理工作负载来解决这个问题。\n在这篇博文中，我们将深入探讨工作负载规范的复杂性，这有助于简化代码从一个环境过渡到另一个环境，并减轻开发人员的认知负担。听起来激动人心吗？让我们深入了解。\n了解 …","relpermalink":"/blog/decoding-workload-specification/","summary":"这篇文章介绍了工作负载规范的概念和作用，它是一种标准化的方法，可以简化和统一不同环境下的软件部署，减轻开发者的认知负担。","title":"如何用工作负载规范打造高效的平台工程服务"},{"content":"本文译自：https://medium.com/@aditya.barik32/ordering-of-container-within-pod-a423d2e5ba52\n摘要：讨论了在 Kubernetes Pod 中排序容器的需求，介绍了开源工具 Kubexit 实现容器的有序启动和终止，提高工作流灵活性。\n为什么要在 Pod 中对容器进行排序？ 在某些情况下，Pod 的排序可能是一个使用案例，我们需要确保某些容器在启动应用程序代码之前已经正常运行。假设我们有一个 Java 应用程序，需要一个数据库（Mysql）、缓存（Aerospike/Redis）和 Kafka 来提供流量。与此同时，我们还需要这些依赖关系是特定于实例或与应用程序堆栈本地关联的。在这种情况下，在 v1.28 版本之前，Kubernetes 没有提供一个开箱即用的解决方案。对于版本小于 1.28 的集群，没有正式的解决方法。为了缓解这个问题，我们有另一种不太知名的开源解决方法，叫做 Kubexit。\n什么是 Kubexit？ Kubexit 是一个开源项目，旨在提供一种协调的方式来启动和终止 Pod 内的容器。\n …","relpermalink":"/blog/ordering-containers-within-pod/","summary":"本文讨论了在 Kubernetes Pod 中排序容器的需求，介绍了开源工具 Kubexit 实现容器的有序启动和终止，提高工作流灵活性。","title":"解密 Kubernetes Pod 中容器的有序部署：Kubexit 工具的妙用"},{"content":"摘要：本文回顾了 KubeCon NA 2023 五个引人入胜的主题演讲，涵盖微型容器、Kubernetes LTS、Kubernetes 未来愿景、优化 Kubernetes 资源使用、生成式 AI 在平台工程的应用。讨论了微型容器的尺寸对可持续性的影响，稳定性与创新的平衡，Kubernetes 未来发展方向，以及 AI 和资源优化在云原生环境中的重要性。整体看，Kubernetes 作为云原生平台在稳定性、安全性、简易性方面迎来了新的发展阶段。\nKubeCon + CloudNativeCon North America 2023，云原生的旗舰会议，已于几天前正式结束。本文包含了这三天中最有趣的演讲和公告。废话不多说，让我们进入回顾吧。✌️\n微型容器的微型讨论 — Eric Gregory，Mirantis 镜像大小的问题对可持续性、存储和网络效率等方面产生了重要影响。这就是为什么我们将首先讨论的话题是 Eric Gregory 的“微型容器的微型讨论”。这个话题对我个人也很有兴趣，我尝试过使用 Docker Slim 来减小镜像大小（包括集成到 CI/CD 流水线中），结果确实令 …","relpermalink":"/blog/kubecon-na-5-interesting-keynotes/","summary":"本文回顾了 KubeCon NA 2023 五个引人入胜的主题演讲，涵盖微型容器、Kubernetes LTS、Kubernetes 未来愿景、优化 Kubernetes 资源使用、生成式 AI 在平台工程的应用。讨论了微型容器的尺寸对可持续性的影响，稳定性与创新的平衡，Kubernetes 未来发展方向，以及 AI 和资源优化在云原生环境中的重要性。整体看，Kubernetes 作为云原生平台在稳定性、安全性、简易性方面迎来了新的发展阶段。","title":"KubeCon North America 2023:5 个有趣的主题演讲摘要"},{"content":"本文译自：https://danielbryantuk.medium.com/kubecon-chicago-key-takeaways-3de5ca13b375\n摘要：本文讨论了 KubeCon 的主要议题，包括平台工程、Kubernetes 的不断发展、开发者体验的重要性、对应用开发和集成的关注、云原生通信的捆绑问题、安全问题的重要性、对可持续性的关注，以及社区的力量。文章强调了在标准化和创新之间取得平衡的重要性，并预测云原生的未来将看到更多的 AI/LLMs。\n这次 KubeCon NA 对我来说是一次非常不同的体验，因为我不是代表公司或办展位。然而，我仍然度过了一段愉快的时光，与云原生社区的许多人见面真是太棒了。回到芝加哥也很不错，这座城市对我们很好 —— 我几乎忘记了我有多喜欢芝加哥式深盘披萨！\n以下是我从 KubeCon NA 2023 中得出的主要收获：\n云原生社区正在缓慢接受AI/LLM DevOps 已经过时：平台工程万物 山中有金！在平台激战中销售镐和铲子 Kubernetes 应该保持不完整（并不断发展） 别忘了开发者体验！ 增加对应用程序开发和集成的关注 云原生 …","relpermalink":"/blog/kubecon-chicago-key-takeaways/","summary":"本文讨论了 KubeCon 的主要议题，包括平台工程、Kubernetes 的不断发展、开发者体验的重要性、对应用开发和集成的关注、云原生通信的捆绑问题、安全问题的重要性、对可持续性的关注，以及社区的力量。文章强调了在标准化和创新之间取得平衡的重要性，并预测云原生的未来将看到更多的 AI/LLMs。","title":"KubeCon Chicago 主要收获：人工智能（AI）的缓慢崛起，平台工程的主导地位，以及 KubeCon NA 2023 对开发者体验的重新关注"},{"content":"本文译自：https://www.uipath.com/inside-the-rocketship/tech-stories/optimizing-traffic-routing-uipath-automation-suite-envoy-wasm\n在这篇博文中，我们将深入探讨 UiPath Automation Suite 的路由基础设施，揭示关键见解和创新方法以提升性能。\nUiPath Automation Suite 内部的流量路由 UiPath Automation Suite使您能够在 Linux 上部署完整的UiPath Business Automation Platform，该平台由 40 多个微服务组成的 Kubernetes 集群。它有一个 API 网关位于所有微服务之前，具有自定义的路由业务逻辑，例如服务发现、请求拦截、请求操作和错误处理等。\n我们使用 Istio 和 Envoy 这个流行的组合作为路由基础设施，路由业务逻辑通过 Envoy 过滤器内的 Lua 脚本实现。除了以下几个痛点外，此解决方案运作良好：\n无法直接连接 Redis： …","relpermalink":"/blog/optimizing-traffic-routing-uipath-automation-suite-envoy-wasm/","summary":"在这篇博文中，我们将深入探讨 UiPath Automation Suite 的路由基础设施，揭示关键见解和创新方法以提升性能。","title":"在 UiPath Automation Suite 中使用 Envoy 和 WASM 优化流量路由"},{"content":"本文译自：https://www.infoq.com/presentations/dapr-cloud-services/，是 Ibryam 今年三月在 QCon London 的分享。\n摘要：软件堆栈的商品化是如何改变应用为先的云服务的游戏规则。通过使用开源项目和 API 将应用程序与集成逻辑绑定在一起，可以实现更高级别的抽象。Dapr 是一组作为 API 公开的分布式基元，可以作为 Sidecar 部署。应用程序云服务将计算和集成能力作为 SaaS 提供，开发人员可以将核心应用程序逻辑绑定到云服务上。这种应用程序优先云服务的出现使得应用程序可以更加专业化，并且可以在不同的云提供商之间灵活迁移。\nIbryam: 我叫 Bilgin Ibryam。我在这里要告诉大家我对云服务的演变以及这将如何影响我们正在构建的分布式应用的看法。这将是一个快节奏的演讲，我们没有时间深入研究每个技术和模式，而是快速概述架构的演变，并试图分析这对云原生应用的未来可能意味着什么。我是 Diagrid 的产品经理，我们正在为开发人员构建 API。在此之前，我是红帽公司的顾问、架构师和产品经理，在那里我使用了一些 …","relpermalink":"/blog/dapr-cloud-services/","summary":"软件堆栈的商品化是如何改变应用为先的云服务的游戏规则。通过使用开源项目和 API 将应用程序与集成逻辑绑定在一起，可以实现更高级别的抽象。Dapr 是一组作为 API 公开的分布式基元，可以作为 Sidecar 部署。应用程序云服务将计算和集成能力作为 SaaS 提供，开发人员可以将核心应用程序逻辑绑定到云服务上。这种应用程序优先云服务的出现使得应用程序可以更加专业化，并且可以在不同的云提供商之间灵活迁移。","title":"软件堆栈的商品化：应用为先的云服务如何改变游戏规则"},{"content":"本文译自：https://medium.com/@shrishs/understanding-grpc-load-balancing-in-kubernetes-with-istio-c6e71634724c\n摘要：本文介绍了在 Kubernetes 和 Istio 中使用 gRPC 负载均衡的行为。首先，通过创建命名空间、部署资源和配置文件来准备环境。然后，介绍了没有 Istio 的情况下，gRPC 服务的负载均衡行为。接下来，介绍了如何使用 Istio 创建虚拟服务和目标规则来实现负载均衡。还讨论了 ConnectionPoolSetting 对负载均衡行为的影响。最后，介绍了如何通过入口网关访问 gRPC 服务，并提供了验证步骤。\ngRPC（gRPC 远程过程调用）是一个跨平台的开源高性能远程过程调用（RPC）框架，使用 HTTP/2 进行传输。gRPC 相对于传统的 HTTP 机制具有诸多优势，例如在一个连接上多路复用多个请求（HTTP/2）。本文将解释在 Kubernetes（RedHat Openshift）和（Redhat Openshift Service Mesh）中 …","relpermalink":"/blog/understanding-grpc-load-balancing-in-kubernetes-with-istio/","summary":"本文介绍了在 Kubernetes 和 Istio 中使用 gRPC 负载均衡的行为。首先，通过创建命名空间、部署资源和配置文件来准备环境。然后，介绍了没有 Istio 的情况下，gRPC 服务的负载均衡行为。接下来，介绍了如何使用 Istio 创建虚拟服务和目标规则来实现负载均衡。还讨论了 ConnectionPoolSetting 对负载均衡行为的影响。最后，介绍了如何通过入口网关访问 gRPC 服务，并提供了验证步骤。","title":"理解使用 Istio 在 Kubernetes 中的 gRPC 负载均衡"},{"content":" 本文译自：https://sysdig.com/blog/top-owasp-kubernetes/\n摘要：OWASP Kubernetes Top 10 强调了 Kubernetes 生态系统中的关键风险和漏洞。它涵盖了诸如准入控制器中的配置错误，密钥管理故障，漏洞管理，身份验证机制失效以及过时和易受攻击的 Kubernetes 组件等主题。建议包括使用像 Falco 这样的工具来检测安全问题，对静态密钥进行加密，解决安全配置问题，确保日志记录和审计，扫描容器镜像以检测漏洞，管理依赖关系，保护对 Kubernetes 的访问以及及时了解 CVE 情况。\n使用 Kubernetes 时最大的关切之一是是否符合安全态势，并考虑到所有可能的威胁。因此，OWASP 创造了 OWASP Kubernetes Top 10，以帮助识别最可能的风险。\nOWASP Top 10 项目是对安全从业人员和工程师非常有用的认知和指导资源。它们也可以映射到其他安全框架，帮助事件响应工程师了解 Kubernetes 的威胁。MITRE ATT\u0026amp;CK 技术也常用于记录攻击者的技术，并帮助蓝队了解保护环境的最佳 …","relpermalink":"/blog/top-owasp-kubernetes/","summary":"OWASP Kubernetes Top 10 强调了 Kubernetes 生态系统中的关键风险和漏洞。它涵盖了诸如准入控制器中的配置错误，密钥管理故障，漏洞管理，身份验证机制失效以及过时和易受攻击的 Kubernetes 组件等主题。建议包括使用像 Falco 这样的工具来检测安全问题，对静态密钥进行加密，解决安全配置问题，确保日志记录和审计，扫描容器镜像以检测漏洞，管理依赖关系，保护对 Kubernetes 的访问以及及时了解 CVE 情况。","title":"构建安全的 Kubernetes 环境：OWASP Kubernetes Top 10"},{"content":"本文译自：https://fiberplane.com/blog/why-are-prometheus-queries-hard\nPrometheus 是一个强大的开源可观测性工具。但是许多人，包括我自己，都很难理解其查询语言。在这篇文章中，我将从头开始建立一个基本的查询，并使用每个步骤来解释 PromQL 中一些较难理解的方面。希望这能更直观地展示 Prometheus 的工作原理，帮助你编写查询并理解数据。\n关于Autometrics项目的一个快速介绍：它是一个开源微型框架，使你可以轻松地为代码添加最有用的指标，并为你编写 Prometheus 查询。无需手动编写查询，即可在生产环境中识别和调试问题！\n用“简单”的查询回答问题 假设我们正在运行一个 HTTP API，并且我们想了解用户遇到错误的频率。这似乎是一个简单的问题，对吧？\n为了从 Prometheus 中获取这个答案，我们需要进行类似以下的查询，其中已经涉及了很多内容：\nsum by (status) (rate(http_requests_total[5m])) 为了理解为什么这个查询有效，以及为什么我们需要这个查询，我 …","relpermalink":"/blog/why-are-prometheus-queries-hard/","summary":"Prometheus 是一个强大的开源可观测性工具。但是许多人，包括我自己，都很难理解其查询语言。在这篇文章中，我将从头开始建立一个基本的查询，并使用每个步骤来解释 PromQL 中一些较难理解的方面。希望这能更直观地展示 Prometheus 的工作原理，帮助你编写查询并理解数据。","title":"为什么 Prometheus 查询很难？"},{"content":"Istio 可以轻松创建具有丰富路由、负载均衡、服务间身份验证、监控等功能的已部署服务网络 - 所有这些都无需对应用程序代码进行任何更改。Istio 致力于以最小的资源开销提供这些优势，并旨在支持具有高请求率的大型网格，同时增加最小的延迟。\nIstio 数据平面组件（Envoy 代理）处理流经系统的数据。Istio 控制平面组件 Istiod 配置数据平面。数据平面和控制平面具有不同的性能问题。\nIstio 1.18 的性能摘要 Istio 负载测试网格由 1000 个服务和 2000 个 sidecar 组成，每秒有 70,000 个网格范围的请求。\n控制平面性能 Istiod 根据用户编写的配置文件和系统的当前状态来配置 sidecar 代理。在 Kubernetes 环境中，自定义资源定义 (CRD) 和部署构成了系统的配置和状态。Istio 配置对象（例如 Gateway 和 VirtualService）提供用户编写的配置。为了生成代理的配置，Istiod 处理来自 Kubernetes 环境的组合配置和系统状态以及用户编写的配置。\n控制平面支持数千个服务， …","relpermalink":"/blog/performance-and-scalability/","summary":"Istio 官方公布 Istio 1.18 性能测试结果。","title":"Istio 1.18 性能测试结果"},{"content":" AccessBindings is an assignment of roles to a set of users or teams to access resources. The user or team information is obtained from an user directory (such an LDAP server or an external OIDC server) that should have been configured as part of Service Bridge installation. Note that an AccessBinding can be created or modified only by users who have SET_POLICY permission on the target resource.\nThe following example assigns the workspace-admin role to users alice, bob, and members of the t1 …","relpermalink":"/tsb/refs/tsb/rbac/v2/access-bindings/","summary":"AccessBindings is an assignment of roles to a set of users or teams to access resources. The user or team information is obtained from an user directory (such an LDAP server or an external OIDC server) that should have been configured as part of Service Bridge installation. Note that an AccessBinding can be created or modified only by users who have SET_POLICY permission on the target resource.\nThe following example assigns the workspace-admin role to users alice, bob, and members of the t1 team for the workspace w1 owned by the tenant mycompany.\nUse fully-qualified name (fqn) when specifying the target resource, as well as for the users and teams.","title":"Access Bindings"},{"content":" Agent Configuration specifies configuration of the Workload Onboarding Agent.\nIn most cases, Workload Onboarding Agent can automatically recognize the host environment, e.g. AWS EC2, which makes explicit Agent Configuration optional.\nBy default, Workload Onboarding Agent comes with the minimal configuration:\napiVersion: config.agent.onboarding.tetrate.io/v1alpha1 kind: AgentConfiguration which at runtime is interpreted as an equivalent of:\napiVersion: config.agent.onboarding.tetrate.io/v1alpha1 …","relpermalink":"/tsb/refs/onboarding/config/agent/v1alpha1/agent-configuration/","summary":"Agent Configuration specifies configuration of the Workload Onboarding Agent.\nIn most cases, Workload Onboarding Agent can automatically recognize the host environment, e.g. AWS EC2, which makes explicit Agent Configuration optional.\nBy default, Workload Onboarding Agent comes with the minimal configuration:\napiVersion: config.agent.onboarding.tetrate.io/v1alpha1 kind: AgentConfiguration which at runtime is interpreted as an equivalent of:\napiVersion: config.agent.onboarding.tetrate.io/v1alpha1 kind: AgentConfiguration host: auto: {} sidecar: istio: {} stdout: filename: /dev/stdout stderr: filename: /dev/stderr The above configuration means that Workload Onboarding Agent should infer host environment automatically, should be in control of the Istio Sidecar pre-installed on that host, should redirect standard output of the Istio Sidecar into its own output.","title":"Agent Configuration"},{"content":" API objects define a set of servers and endpoints that expose the business logic for an Application. APIs are attached to existing Applications to configure how the features exposed by the different services that are part of the Application can be accessed.\nThe format used to define APIs is based on the OpenAPI v3 spec. Users can attach OpenAPI documents to the applications, and Service Bridge will generate all the configuration that is needed to make the APIs available. Service Bridge also …","relpermalink":"/tsb/refs/tsb/application/v2/api/","summary":"API objects define a set of servers and endpoints that expose the business logic for an Application. APIs are attached to existing Applications to configure how the features exposed by the different services that are part of the Application can be accessed.\nThe format used to define APIs is based on the OpenAPI v3 spec. Users can attach OpenAPI documents to the applications, and Service Bridge will generate all the configuration that is needed to make the APIs available. Service Bridge also provides a set of custom extensions to the OpenAPI spec that can be used to further customize the APIs in those cases where the standard OpenAPI properties are not sufficient.","title":"API"},{"content":" DEPRECATED: use Access Bindings instead.\nAPIAccessBindings is an assignment of roles to a set of users or teams to access API resources. The user or team information is obtained from an LDAP server that should have been configured as part of Service Bridge installation. Note that a APIAccessBinding can be created or modified only by users who have set_rbac permission on the API resource.\nThe following example assigns the api-admin role to users alice, bob, and members of the t1 team for the …","relpermalink":"/tsb/refs/tsb/rbac/v2/api-access-bindings/","summary":"DEPRECATED: use Access Bindings instead.\nAPIAccessBindings is an assignment of roles to a set of users or teams to access API resources. The user or team information is obtained from an LDAP server that should have been configured as part of Service Bridge installation. Note that a APIAccessBinding can be created or modified only by users who have set_rbac permission on the API resource.\nThe following example assigns the api-admin role to users alice, bob, and members of the t1 team for the APIs openapi in the application app owned by the tenant mycompany. Use fully-qualified name (fqn) when specifying user and team","title":"API Access Bindings"},{"content":" Applications are logical groupings of services that are related to each other, typically within a trusted group. A common example are three tier applications composed of a frontend, a backend and a datastore service.\nApplications are often consumed through APIs, and a single Application can expose one or more of those APIs. These APIs will define the hostnames that are exposed and the methods exposed in each hostname.\napiVersion: application.tsb.tetrate.io/v2 kind: Application metadata: name: …","relpermalink":"/tsb/refs/tsb/application/v2/application/","summary":"Applications are logical groupings of services that are related to each other, typically within a trusted group. A common example are three tier applications composed of a frontend, a backend and a datastore service.\nApplications are often consumed through APIs, and a single Application can expose one or more of those APIs. These APIs will define the hostnames that are exposed and the methods exposed in each hostname.\napiVersion: application.tsb.tetrate.io/v2 kind: Application metadata: name: three-tier organization: myorg tenant: tetrate spec: workspace: organizations/myorg/tenants/tetrate/three-tier Application An Application represents a set of logical groupings of services that are related to each other and expose a set of APIs that implement a complete set of business logic.","title":"Application"},{"content":" DEPRECATED: use Access Bindings instead.\nApplicationAccessBindings is an assignment of roles to a set of users or teams to access Application resources. The user or team information is obtained from an LDAP server that should have been configured as part of Service Bridge installation. Note that a ApplicationAccessBinding can be created or modified only by users who have SET_POLICY permission on the Application.\nThe following example assigns the application-admin role to users alice, bob, and …","relpermalink":"/tsb/refs/tsb/rbac/v2/application-access-bindings/","summary":"DEPRECATED: use Access Bindings instead.\nApplicationAccessBindings is an assignment of roles to a set of users or teams to access Application resources. The user or team information is obtained from an LDAP server that should have been configured as part of Service Bridge installation. Note that a ApplicationAccessBinding can be created or modified only by users who have SET_POLICY permission on the Application.\nThe following example assigns the application-admin role to users alice, bob, and members of the t1 team for the application app owned by the tenant mycompany. Use fully-qualified name (fqn) when specifying user and team\napiVersion: rbac.","title":"Application Access Bindings"},{"content":" Service to manage Applications and APis\nApplications The Applications service exposes methods to manage Applications and API definitions in Service Bridge.\nCreateApplication rpc CreateApplication (tetrateio.api.tsb.application.v2.CreateApplicationRequest) returns (tetrateio.api.tsb.application.v2.Application)\nRequires CREATE\nCreates a new Application in TSB.\nGetApplication rpc GetApplication (tetrateio.api.tsb.application.v2.GetApplicationRequest) returns …","relpermalink":"/tsb/refs/tsb/application/v2/application-service/","summary":"Service to manage Applications and APis\nApplications The Applications service exposes methods to manage Applications and API definitions in Service Bridge.\nCreateApplication rpc CreateApplication (tetrateio.api.tsb.application.v2.CreateApplicationRequest) returns (tetrateio.api.tsb.application.v2.Application)\nRequires CREATE\nCreates a new Application in TSB.\nGetApplication rpc GetApplication (tetrateio.api.tsb.application.v2.GetApplicationRequest) returns (tetrateio.api.tsb.application.v2.Application)\nRequires READ\nGet the details of an existing application.\nUpdateApplication rpc UpdateApplication (tetrateio.api.tsb.application.v2.Application) returns (tetrateio.api.tsb.application.v2.Application)\nRequires WRITE\nModify an existing application.\nListApplications rpc ListApplications (tetrateio.api.tsb.application.v2.ListApplicationsRequest) returns (tetrateio.api.tsb.application.v2.ListApplicationsResponse)\nList all existing applications for the given tenant.\nDeleteApplication rpc DeleteApplication (tetrateio.api.tsb.application.v2.DeleteApplicationRequest) returns (google.protobuf.Empty)\nRequires DELETE\nDelete an existing Application. Note that deleting resources in TSB is a recursive operation. Deleting a application will delete all API objects that exist in it.","title":"Application Service"},{"content":" Service to manage centralized approval policies.\nApprovals The Approvals service exposes methods for working with approval policies. $hide_from_yaml\nSetPolicy rpc SetPolicy (tetrateio.api.tsb.q.v2.ApprovalPolicy) returns (google.protobuf.Empty)\nRequires CreateApprovalPolicy, WriteApprovalPolicy\nSetPolicy enables authorization policy checks for the given resource and applies any provided request or approval settings. If the resource has existing policies settings, they will be replaced. Once the …","relpermalink":"/tsb/refs/tsb/q/v2/approvals-service/","summary":"Service to manage centralized approval policies.\nApprovals The Approvals service exposes methods for working with approval policies. $hide_from_yaml\nSetPolicy rpc SetPolicy (tetrateio.api.tsb.q.v2.ApprovalPolicy) returns (google.protobuf.Empty)\nRequires CreateApprovalPolicy, WriteApprovalPolicy\nSetPolicy enables authorization policy checks for the given resource and applies any provided request or approval settings. If the resource has existing policies settings, they will be replaced. Once the policy is set, authorization checks will be performed for the given resource.\nGetPolicy rpc GetPolicy (tetrateio.api.tsb.q.v2.GetPolicyRequest) returns (tetrateio.api.tsb.q.v2.ApprovalPolicy)\nRequires ReadApprovalPolicy\nGetPolicy returns the approval policy for the given resource.\nQueryPolicies rpc QueryPolicies (tetrateio.api.tsb.q.v2.QueryPoliciesRequest) returns (tetrateio.api.tsb.q.v2.QueryPoliciesResponse)\nDeletePolicy rpc DeletePolicy (tetrateio.api.tsb.q.v2.DeletePolicyRequest) returns (google.protobuf.Empty)\nRequires DeleteApprovalPolicy","title":"Approvals Service"},{"content":" Audit Log Service\nAuditService The Audit Service provides access to the Service Bridge audit log APIs.\nAll operations performed against TSB resources generate audit log events that can be queried using the Audit log APIs. Those events include information about the users that performed each action and about the actions themselves.\nThis API is integrated with the TSB permission system, and all its methods will only return audit logs for those resources the users making the queries have …","relpermalink":"/tsb/refs/audit/v1/audit/","summary":"Audit Log Service\nAuditService The Audit Service provides access to the Service Bridge audit log APIs.\nAll operations performed against TSB resources generate audit log events that can be queried using the Audit log APIs. Those events include information about the users that performed each action and about the actions themselves.\nThis API is integrated with the TSB permission system, and all its methods will only return audit logs for those resources the users making the queries have permissions on.\nListAuditLogs rpc ListAuditLogs (tetrateio.api.audit.v1.ListAuditLogsRequest) returns (tetrateio.api.audit.v1.ListAuditLogsResponse)\nList audit logs. If no ‘count’ parameter has been specified, the last 25 audit logs are returned.","title":"Audit"},{"content":" Authentication and authorization configs at gateways, security group level\nAuthentication Field Description Validation Rule jwt\ntetrateio.api.tsb.auth.v2.Authentication.JWT oneof authn Authenticate an HTTP request from a JWT Token attached to it.\n–\nrules\ntetrateio.api.tsb.auth.v2.Authentication.Rules oneof authn List of rules how to authenticate an HTTP request.\n–\nJWT Field Description Validation Rule issuer\nstring REQUIRED Identifies the issuer that issued the JWT. See issuer A JWT with …","relpermalink":"/tsb/refs/tsb/auth/v2/auth/","summary":"Authentication and authorization configs at gateways, security group level\nAuthentication Field Description Validation Rule jwt\ntetrateio.api.tsb.auth.v2.Authentication.JWT oneof authn Authenticate an HTTP request from a JWT Token attached to it.\n–\nrules\ntetrateio.api.tsb.auth.v2.Authentication.Rules oneof authn List of rules how to authenticate an HTTP request.\n–\nJWT Field Description Validation Rule issuer\nstring REQUIRED Identifies the issuer that issued the JWT. See issuer A JWT with different iss claim will be rejected.\nExample: https://foobar.auth0.com Example: 1234567-compute@developer.gserviceaccount.com\nstring = { min_len: 1}\naudiences\nList of string The list of JWT audiences. that are allowed to access. A JWT containing any of these audiences will be accepted.","title":"Auth"},{"content":" Authentication and authorization configs at gateways\n","relpermalink":"/tsb/refs/tsb/gateway/v2/auth/","summary":"Authentication and authorization configs at gateways","title":"Auth"},{"content":" AwsIdentity represents an AWS-specific identity of a workload.\nE.g.,\nAWS EC2 instance identity:\npartition: aws account: \u0026#39;123456789012\u0026#39; region: ca-central-1 zone: ca-central-1b ec2: instance_id: i-1234567890abcdef0 iam_role: name: example-role AWS ECS task identity:\npartition: aws account: \u0026#39;123456789012\u0026#39; region: ca-central-1 zone: ca-central-1b ecs: task_id: 16aeded318d842bb8226e5bc678cd446 cluster: bookinfo iam_role: name: example-role AwsIdentity AwsIdentity represents an AWS-specific identity …","relpermalink":"/tsb/refs/onboarding/config/types/identity/aws/v1alpha1/aws/","summary":"AwsIdentity represents an AWS-specific identity of a workload.\nE.g.,\nAWS EC2 instance identity:\npartition: aws account: '123456789012' region: ca-central-1 zone: ca-central-1b ec2: instance_id: i-1234567890abcdef0 iam_role: name: example-role AWS ECS task identity:\npartition: aws account: '123456789012' region: ca-central-1 zone: ca-central-1b ecs: task_id: 16aeded318d842bb8226e5bc678cd446 cluster: bookinfo iam_role: name: example-role AwsIdentity AwsIdentity represents an AWS-specific identity of a workload.\nField Description Validation Rule partition\nstring REQUIRED AWS Partition.\nE.g., aws, aws-cn, aws-us-gov, etc.\nSee https://docs.aws.amazon.com/general/latest/gr/aws-arns-and-namespaces.html\nstring = { min_len: 1}\naccount\nstring REQUIRED AWS Account.\nE.g., 123456789012.\nSee https://docs.aws.amazon.com/general/latest/gr/acct-identifiers.html\nstring = { pattern: ^[0-9]{12}$}\nregion\nstring REQUIRED AWS Region.\nE.g., us-east-2, eu-west-3, cn-north-1, etc.","title":"AWS Identity"},{"content":" AwsIdentityMatcher specifies matching workloads with AWS-specific identities.\nFor example, the following configuration will match any EC2 VM instance in account 123456789012, region ca-central-1 and zone ca-central-1b:\npartitions: - aws accounts: - \u0026#39;123456789012\u0026#39; regions: - ca-central-1 zones: - ca-central-1b ec2: {} The matcher can also be used to to limit to VMs associated with a specific IAM role as shown below:\npartitions: - aws accounts: - \u0026#39;123456789012\u0026#39; regions: - ca-central-1 zones: - …","relpermalink":"/tsb/refs/onboarding/config/authorization/aws/v1alpha1/aws/","summary":"AwsIdentityMatcher specifies matching workloads with AWS-specific identities.\nFor example, the following configuration will match any EC2 VM instance in account 123456789012, region ca-central-1 and zone ca-central-1b:\npartitions: - aws accounts: - '123456789012' regions: - ca-central-1 zones: - ca-central-1b ec2: {} The matcher can also be used to to limit to VMs associated with a specific IAM role as shown below:\npartitions: - aws accounts: - '123456789012' regions: - ca-central-1 zones: - ca-central-1b ec2: iamRoleNames: - example-role The following matcher will limit to ECS instances in the bookinfo cluster and with a specific IAM role:\npartitions: - aws accounts: - '123456789012' regions: - ca-central-1 zones: - ca-central-1b ecs: clusters: - prod-cluster iamRoleNames: - example-role AwsIdentityMatcher AwsIdentityMatcher specifies matching workloads with AWS-specific identities.","title":"AWS Identity Matcher"},{"content":" Service to manage clusters onboarded in TSB.\nClusters The Clusters service exposes methods to manage the registration of clusters that are managed by TSB. Before TSB can takeover networking for a given cluster, it must be onboarded in the platform. This onboarding process usually involves two steps:\nCreating the cluster object so the platform knows about it. Generate the agent tokens for the cluster, so the TSB agents installed in the actual cluster can talk to TSB. Once a cluster has been …","relpermalink":"/tsb/refs/tsb/v2/cluster-service/","summary":"Service to manage clusters onboarded in TSB.\nClusters The Clusters service exposes methods to manage the registration of clusters that are managed by TSB. Before TSB can takeover networking for a given cluster, it must be onboarded in the platform. This onboarding process usually involves two steps:\nCreating the cluster object so the platform knows about it. Generate the agent tokens for the cluster, so the TSB agents installed in the actual cluster can talk to TSB. Once a cluster has been onboarded into TSB, it will start receiving configuration updates from the management plane, and the agents will keep the management updated with the status of the cluster.","title":"Cluster Service"},{"content":" Each Kubernetes cluster managed by Service Bridge should be onboarded first before configurations can be applied to the services in the cluster. Onboarding a cluster is a two step process. First, create a cluster object under the appropriate tenant. Once a cluster object is created, its status field should provide the set of join tokens that will be used by the Service Bridge agent on the cluster to talk to Service Bridge management plane. The second step is to deploy the Service Bridge agent …","relpermalink":"/tsb/refs/tsb/v2/cluster/","summary":"Each Kubernetes cluster managed by Service Bridge should be onboarded first before configurations can be applied to the services in the cluster. Onboarding a cluster is a two step process. First, create a cluster object under the appropriate tenant. Once a cluster object is created, its status field should provide the set of join tokens that will be used by the Service Bridge agent on the cluster to talk to Service Bridge management plane. The second step is to deploy the Service Bridge agent on the cluster with the join tokens and deploy Istio on the cluster. The following example creates a cluster named c1 under the tenant mycompany, indicating that the cluster is deployed on a network “vpc-01” corresponding to the AWS VPC where it resides.","title":"Clusters"},{"content":" Common configuration objects shared by the different install APIs.\nCertManagerSettings CertManagerSettings represents the settings used for the cert-manager installation. TSB supports installing and managing the lifecycle of the cert-manager installation.\nField Description Validation Rule managed\ntetrateio.api.install.common.CertManagerSettings.Managed Managed specifies whether TSB should manage the lifecycle of cert-manager.\n–\ncertManagerSpec …","relpermalink":"/tsb/refs/install/common/common-config/","summary":"Common configuration objects shared by the different install APIs.\nCertManagerSettings CertManagerSettings represents the settings used for the cert-manager installation. TSB supports installing and managing the lifecycle of the cert-manager installation.\nField Description Validation Rule managed\ntetrateio.api.install.common.CertManagerSettings.Managed Managed specifies whether TSB should manage the lifecycle of cert-manager.\n–\ncertManagerSpec\ntetrateio.api.install.common.CertManagerSettings.CertManagerSpec Configure kubernetes specific settings for cert-manager.\n–\ncertManagerWebhookSpec\ntetrateio.api.install.common.CertManagerSettings.CertManagerWebhookSpec Configure kubernetes specific settings for cert-manager-webhook.\n–\ncertManagerCaInjector\ntetrateio.api.install.common.CertManagerSettings.CertManagerCAInjector Configure kubernetes specific settings for cert-manager-cainjector.\n–\ncertManagerStartupapicheck\ntetrateio.api.install.common.CertManagerSettings.CertManagerStartupAPICheck Configure kubernetes specific settings for cert-manager-startupapicheck. DEPRECATED. Startup API Check is disabled.\n–\nCertManagerCAInjector CertManagerCAInjector represents the settings used for cert-manager CAInjector installation in the clusters.","title":"Common Configuration Objects"},{"content":" Definition of objects shared by different APIs.\nConfigGenerationMetadata ConfigGenerationMetadata allows to setup extra metadata that will be added in the final Istio generated configurations. Like new labels or annotations. Defining the config generation metadata in tenancy resources (like organization, tenant, workspace or groups) works as default values for those configs that belong to it. Defining same config generation metadata in configuration resources (like ingress gateways, service …","relpermalink":"/tsb/refs/tsb/types/v2/types/","summary":"Definition of objects shared by different APIs.\nConfigGenerationMetadata ConfigGenerationMetadata allows to setup extra metadata that will be added in the final Istio generated configurations. Like new labels or annotations. Defining the config generation metadata in tenancy resources (like organization, tenant, workspace or groups) works as default values for those configs that belong to it. Defining same config generation metadata in configuration resources (like ingress gateways, service routes, etc.) will replace the ones defined in the tenancy resources.\nField Description Validation Rule labels\nmap\u003cstring, string\u003e Set of key value paris that will be added into the metadata.labels field of the Istio generated configurations.","title":"Common Object Types"},{"content":" Condition contains details for one aspect of the current state of an API Resource.\nCondition Condition contains details for one aspect of the current state of an API Resource.\nField Description Validation Rule type\nstring REQUIRED Type of condition in CamelCase or in foo.example.com/CamelCase.\nstring = { min_len: 1}\nstatus\nstring REQUIRED Status of the condition, one of True, False, Unknown.\nstring = { in: True,False,Unknown}\nreason\nstring REQUIRED Reason contains a programmatic identifier …","relpermalink":"/tsb/refs/onboarding/config/types/core/v1alpha1/condition/","summary":"Condition contains details for one aspect of the current state of an API Resource.\nCondition Condition contains details for one aspect of the current state of an API Resource.\nField Description Validation Rule type\nstring REQUIRED Type of condition in CamelCase or in foo.example.com/CamelCase.\nstring = { min_len: 1}\nstatus\nstring REQUIRED Status of the condition, one of True, False, Unknown.\nstring = { in: True,False,Unknown}\nreason\nstring REQUIRED Reason contains a programmatic identifier indicating the reason for the condition’s last transition. Producers of specific condition types may define expected values and meanings for this field, and whether the values are considered a guaranteed API.","title":"Condition"},{"content":" ControlPlane resource exposes a set of configurations necessary to automatically install the Service Bridge control plane on a cluster. The installation API is an override API so any unset fields that aren’t required will use sensible defaults.\nPrior to creating the ControlPlane resource, a cluster needs to be created in the management plane. Control plane install scripts would create the following secrets in the Kubernetes namespace the control plane is deployed into. Make sure they exist: …","relpermalink":"/tsb/refs/install/controlplane/v1alpha1/spec/","summary":"ControlPlane resource exposes a set of configurations necessary to automatically install the Service Bridge control plane on a cluster. The installation API is an override API so any unset fields that aren’t required will use sensible defaults.\nPrior to creating the ControlPlane resource, a cluster needs to be created in the management plane. Control plane install scripts would create the following secrets in the Kubernetes namespace the control plane is deployed into. Make sure they exist:\noap-token otel-token If your Elasticsearch backend requires authentication, ensure you create the following secret:\nelastic-credentials A minimal resource must have the container registry hub, telemetryStore, and managementPlane fields set.","title":"Control Plane"},{"content":" Core types.\nNamespacedName NamespacedName specifies a namespace-scoped name.\nField Description Validation Rule namespace\nstring REQUIRED Namespace name.\nstring = { min_len: 1}\nname\nstring REQUIRED Resource name.\nstring = { min_len: 1}\n","relpermalink":"/tsb/refs/onboarding/config/types/core/v1alpha1/namespaced-name/","summary":"Core types.\nNamespacedName NamespacedName specifies a namespace-scoped name.\nField Description Validation Rule namespace\nstring REQUIRED Namespace name.\nstring = { min_len: 1}\nname\nstring REQUIRED Resource name.\nstring = { min_len: 1}","title":"Core types"},{"content":" A minimal resource should have an empty spec.\napiVersion: install.tetrate.io/v1alpha1 kind: IngressGateway metadata: name: bookinfo namespace: bookinfo spec: {} To configure infrastructure specific settings such as the service type, set the relevant field in kubeSpec. Remember that the installation API is an override API so if these fields are unset the operator will use sensible defaults. Only a subset of Kubernetes configuration is available.\napiVersion: install.tetrate.io/v1alpha1 kind: …","relpermalink":"/tsb/refs/install/dataplane/v1alpha1/spec/","summary":"A minimal resource should have an empty spec.\napiVersion: install.tetrate.io/v1alpha1 kind: IngressGateway metadata: name: bookinfo namespace: bookinfo spec: {} To configure infrastructure specific settings such as the service type, set the relevant field in kubeSpec. Remember that the installation API is an override API so if these fields are unset the operator will use sensible defaults. Only a subset of Kubernetes configuration is available.\napiVersion: install.tetrate.io/v1alpha1 kind: IngressGateway metadata: name: bookinfo namespace: bookinfo spec: kubeSpec: service: type: NodePort EgressGateway and Tier1Gateway are configured in the same manner.\nEgressGatewaySpec EgressGatewaySpec defines the desired installed state of a single egress gateway for a given namespace in Service Bridge.","title":"Data Plane"},{"content":" Configuration for east/west gateway settings\nEastWestGateway EastWestGateway is for configuring a gateway to handle east-west traffic of the services that are not exposed through Ingress or Tier1 gateways (internal services). Currently, this is restricted to specifying at Workspace level in WorkspaceSetting.\nField Description Validation Rule workloadSelector\ntetrateio.api.tsb.types.v2.WorkloadSelector REQUIRED Specify the gateway workloads (pod labels and Kubernetes namespace) under the gateway …","relpermalink":"/tsb/refs/tsb/gateway/v2/eastwest-gateway/","summary":"Configuration for east/west gateway settings\nEastWestGateway EastWestGateway is for configuring a gateway to handle east-west traffic of the services that are not exposed through Ingress or Tier1 gateways (internal services). Currently, this is restricted to specifying at Workspace level in WorkspaceSetting.\nField Description Validation Rule workloadSelector\ntetrateio.api.tsb.types.v2.WorkloadSelector REQUIRED Specify the gateway workloads (pod labels and Kubernetes namespace) under the gateway group that should be configured with this gateway. There can be only one gateway for a workload selector in a namespace.\nmessage = { required: true}\nexposedServices\nList of tetrateio.api.tsb.types.v2.ServiceSelector Exposed services is used to specify the match criteria to select specific services for internal multicluster routing (east-west routing between clusters).","title":"East/West Gateway"},{"content":" EgressGateway configures a workload to act as a gateway for traffic exiting the mesh. The egress gateway is meant to be the destination of unknown traffic within the mesh (traffic sent to non-mesh services). The gateway allows authorization control of traffic sent to it to more finely tune which services are allowed to send unknown traffic through the gateway. Only HTTP is supported at this time.\nThe following example declares an egress gateway running on pods in istio-system with the label …","relpermalink":"/tsb/refs/tsb/gateway/v2/egress-gateway/","summary":"EgressGateway configures a workload to act as a gateway for traffic exiting the mesh. The egress gateway is meant to be the destination of unknown traffic within the mesh (traffic sent to non-mesh services). The gateway allows authorization control of traffic sent to it to more finely tune which services are allowed to send unknown traffic through the gateway. Only HTTP is supported at this time.\nThe following example declares an egress gateway running on pods in istio-system with the label app=istio-egressgateway. This gateway is setup to allow traffic from anywhere in the cluster to access www.httpbin.org and from the bookinfo details app specifically, you can access any external host.","title":"Egress Gateway"},{"content":" The Gateway configuration combines the functionalities of both the existing Tier1Gateway and IngressGateway, providing a unified approach for configuring a workload as a gateway in the mesh. Each server within the Gateway is configured to route requests to either destination clusters or services, but configuring one server to route requests to a destination cluster and another server to route requests to a service is not supported. To ensure consistency and compatibility, the Gateway …","relpermalink":"/tsb/refs/tsb/gateway/v2/gateway/","summary":"The Gateway configuration combines the functionalities of both the existing Tier1Gateway and IngressGateway, providing a unified approach for configuring a workload as a gateway in the mesh. Each server within the Gateway is configured to route requests to either destination clusters or services, but configuring one server to route requests to a destination cluster and another server to route requests to a service is not supported. To ensure consistency and compatibility, the Gateway configuration requires that all servers within the gateway either forward traffic to other clusters, similar to a Tier1Gateway, or route traffic to specific services, similar to an IngressGateway.","title":"Gateway"},{"content":" DEPRECATED: use Access Bindings instead.\nGatewayAccessBindings is an assignment of roles to a set of users or teams to access resources under a Gateway group. The user or team information is obtained from an LDAP server that should have been configured as part of Service Bridge installation. Note that a GatewayAccessBinding can be created or modified only by users who have SET_POLICY permission on the Gateway group.\nThe following example assigns the gateway-admin role to users alice, bob, and …","relpermalink":"/tsb/refs/tsb/rbac/v2/gateway-access-bindings/","summary":"DEPRECATED: use Access Bindings instead.\nGatewayAccessBindings is an assignment of roles to a set of users or teams to access resources under a Gateway group. The user or team information is obtained from an LDAP server that should have been configured as part of Service Bridge installation. Note that a GatewayAccessBinding can be created or modified only by users who have SET_POLICY permission on the Gateway group.\nThe following example assigns the gateway-admin role to users alice, bob, and members of the gateway-ops team for all the gateways in group g1 under workspace w1 owned by the tenant mycompany. Use fully-qualified name (fqn) when specifying user and team","title":"Gateway Access Bindings"},{"content":" Configurations used to build gateways.\nClusterDestination Field Description Validation Rule name\nstring The name of the destination cluster. Only one of name or labels must be specified.\n–\nlabels\nmap\u0026lt;string, string\u0026gt; Labels associated with the cluster. Any cluster with matching labels will be selected as a target. Only one of name or labels must be specified.\n–\nnetwork\nstring The network associated with the destination clusters. In addition to name/label selectors, only clusters matching the …","relpermalink":"/tsb/refs/tsb/gateway/v2/gateway-common/","summary":"Configurations used to build gateways.\nClusterDestination Field Description Validation Rule name\nstring The name of the destination cluster. Only one of name or labels must be specified.\n–\nlabels\nmap\u003cstring, string\u003e Labels associated with the cluster. Any cluster with matching labels will be selected as a target. Only one of name or labels must be specified.\n–\nnetwork\nstring The network associated with the destination clusters. In addition to name/label selectors, only clusters matching the selected networks will be used as a target. At least one of name/labels, and/or network must be specified.\nDeprecated: The network field is deprecated and will be removed in future releases.","title":"Gateway Common Configuration Messages"},{"content":" Gateway Groups allow grouping the gateways in a set of namespaces owned by its parent workspace. Gateway related configurations can then be applied on the group to control the behavior of these gateways. The group can be in one of two modes: BRIDGED and DIRECT. BRIDGED mode is a minimalistic mode that allows users to quickly configure the most commonly used features in the service mesh using Tetrate specific APIs, while the DIRECT mode provides more flexibility for power users by allowing them …","relpermalink":"/tsb/refs/tsb/gateway/v2/gateway-group/","summary":"Gateway Groups allow grouping the gateways in a set of namespaces owned by its parent workspace. Gateway related configurations can then be applied on the group to control the behavior of these gateways. The group can be in one of two modes: BRIDGED and DIRECT. BRIDGED mode is a minimalistic mode that allows users to quickly configure the most commonly used features in the service mesh using Tetrate specific APIs, while the DIRECT mode provides more flexibility for power users by allowing them to configure the gateways’s traffic and security properties using a restricted subset of Istio Networking and Security APIs.","title":"Gateway Group"},{"content":" Service to manage the configuration for Gateways.\nGateways The Gateway service provides methods to manage gateway settings in TSB.\nIt provides methods to create and manage gateway groups within a workspace, allowing to create fine-grained groupings to configure a subset of the workspace namespaces. Access policies can be assigned at group level, providing a fine-grained access control to the gateway configuration features.\nThe Gateway service also provides methods to configure the different …","relpermalink":"/tsb/refs/tsb/gateway/v2/gateway-service/","summary":"Service to manage the configuration for Gateways.\nGateways The Gateway service provides methods to manage gateway settings in TSB.\nIt provides methods to create and manage gateway groups within a workspace, allowing to create fine-grained groupings to configure a subset of the workspace namespaces. Access policies can be assigned at group level, providing a fine-grained access control to the gateway configuration features.\nThe Gateway service also provides methods to configure the different gateway settings that are allowed within each group.\nCreateGroup rpc CreateGroup (tetrateio.api.tsb.gateway.v2.CreateGatewayGroupRequest) returns (tetrateio.api.tsb.gateway.v2.Group)\nRequires CreateGatewayGroup\nCreate a new gateway group in the given workspace.\nGroups will by default configure all the namespaces owned by their workspace, unless explicitly configured.","title":"Gateway Service"},{"content":" Host Info specifies information about the host the workload is running on.\nAddress Address specifies network address.\nField Description Validation Rule ip\nstring REQUIRED IP address.\nstring = { ip: true}\ntype\ntetrateio.api.onboarding.config.types.registration.v1alpha1.AddressType REQUIRED Address type.\nenum = { not_in: 0}\nHostInfo HostInfo specifies information about the host the workload is running on.\nField Description Validation Rule addresses\nList of …","relpermalink":"/tsb/refs/onboarding/config/types/registration/v1alpha1/hostinfo/","summary":"Host Info specifies information about the host the workload is running on.\nAddress Address specifies network address.\nField Description Validation Rule ip\nstring REQUIRED IP address.\nstring = { ip: true}\ntype\ntetrateio.api.onboarding.config.types.registration.v1alpha1.AddressType REQUIRED Address type.\nenum = { not_in: 0}\nHostInfo HostInfo specifies information about the host the workload is running on.\nField Description Validation Rule addresses\nList of tetrateio.api.onboarding.config.types.registration.v1alpha1.Address REQUIRED Network addresses of the host the workload is running on.\nrepeated = { min_items: 1 items: {message:{required:true}}}\nAddressType AddressType specifies type of a network address associated with the workload.\nField Number Description UNSPECIFIED\n0\nNot specified.\nVPC","title":"Host Info"},{"content":" IAM APIs for authentication.\nOAuth Token rpc Token (tetrateio.api.iam.v2.GrantRequest) returns (tetrateio.api.iam.v2.GrantResponse)\nGrants tokens for a given grant type.\nThis is used by clients to obtain an access token by presenting required parameters for the requested grant type. Current only “urn:ietf:params:oauth:grant-type:device_code” is supported. When an error occurs, this will return a 4xx status code with an Error and ErrorMessage in the response.\nDeviceCode rpc DeviceCode …","relpermalink":"/tsb/refs/iam/v2/oauth-service/","summary":"IAM APIs for authentication.\nOAuth Token rpc Token (tetrateio.api.iam.v2.GrantRequest) returns (tetrateio.api.iam.v2.GrantResponse)\nGrants tokens for a given grant type.\nThis is used by clients to obtain an access token by presenting required parameters for the requested grant type. Current only “urn:ietf:params:oauth:grant-type:device_code” is supported. When an error occurs, this will return a 4xx status code with an Error and ErrorMessage in the response.\nDeviceCode rpc DeviceCode (tetrateio.api.iam.v2.DeviceCodeRequest) returns (tetrateio.api.iam.v2.DeviceCodeResponse)\nRequests device codes that can be used with a token grant with grant type “urn:ietf:params:oauth:grant-type:device_code”. For additional information please refer to the Device Authorization Grant RFC https://datatracker.ietf.org/doc/html/rfc8628\nDeviceCodeResponse Response with device codes for use with the Device Authorization flow.","title":"IAM (OAuth)"},{"content":" IAM APIs for authentication.\nOIDC The IAM OIDC service is a service used with Open ID Connect provider integrations.\nCallback rpc Callback (tetrateio.api.iam.v2.CallbackRequest) returns (google.protobuf.Empty)\nCallback endpoint for OAuth2 Authorization Code grant flows as part of the OIDC spec.\nLogin rpc Login (tetrateio.api.iam.v2.LoginRequest) returns (google.protobuf.Empty)\nLogin endpoint to start an OIDC Authentication flow.\nCallbackRequest Request with parameters for an OAuth2 …","relpermalink":"/tsb/refs/iam/v2/oidc-service/","summary":"IAM APIs for authentication.\nOIDC The IAM OIDC service is a service used with Open ID Connect provider integrations.\nCallback rpc Callback (tetrateio.api.iam.v2.CallbackRequest) returns (google.protobuf.Empty)\nCallback endpoint for OAuth2 Authorization Code grant flows as part of the OIDC spec.\nLogin rpc Login (tetrateio.api.iam.v2.LoginRequest) returns (google.protobuf.Empty)\nLogin endpoint to start an OIDC Authentication flow.\nCallbackRequest Request with parameters for an OAuth2 Authorization Code grant redirect.\nField Description Validation Rule code\nstring oneof result OAuth2 Authorization Code. When present this indicates the user authorized the request. TSB will use this code to acquire a token from the OIDC token endpoint and complete the login flow.","title":"IAM (OIDC)"},{"content":" Provide information about the Service bridge platform.\nInfo The Info service provides information about the service Bridge platform.\nGetVersion rpc GetVersion (tetrateio.api.tsb.v2.GetVersionRequest) returns (tetrateio.api.tsb.v2.Version)\nGetVersion returns the version of the TSB binary\nGetCurrentUser rpc GetCurrentUser (tetrateio.api.tsb.v2.GetCurrentUserRequest) returns (tetrateio.api.tsb.v2.CurrentUser)\nGetCurrentUser returns the information of the user or service account that made the …","relpermalink":"/tsb/refs/tsb/v2/info/","summary":"Provide information about the Service bridge platform.\nInfo The Info service provides information about the service Bridge platform.\nGetVersion rpc GetVersion (tetrateio.api.tsb.v2.GetVersionRequest) returns (tetrateio.api.tsb.v2.Version)\nGetVersion returns the version of the TSB binary\nGetCurrentUser rpc GetCurrentUser (tetrateio.api.tsb.v2.GetCurrentUserRequest) returns (tetrateio.api.tsb.v2.CurrentUser)\nGetCurrentUser returns the information of the user or service account that made the request.\nCurrentUser CurrentUser contains the information of the user or service account that made the request.\nField Description Validation Rule loginName\nstring login_name is the name used in the login credentials.\n–\ntype\ntetrateio.api.tsb.v2.CurrentUser.Type The type of the current user, e.g. USER or SERVICE_ACCOUNT\n–\nsourceType\nstring Indicates the Identity Provider where the user has been synchronized from.","title":"Info"},{"content":" DEPRECATION: The functionality provided by the IngressGateway is now provided in Gateway object, and using it is the recommended approach. The IngressGateway resource will be removed in future releases.\nIngressGateway configures a workload to act as a gateway for traffic entering the mesh. The ingress gateway also provides basic API gateway functionalities such as JWT token validation and request authorization. Gateways in privileged workspaces can route to services outside the workspace while …","relpermalink":"/tsb/refs/tsb/gateway/v2/ingress-gateway/","summary":"DEPRECATION: The functionality provided by the IngressGateway is now provided in Gateway object, and using it is the recommended approach. The IngressGateway resource will be removed in future releases.\nIngressGateway configures a workload to act as a gateway for traffic entering the mesh. The ingress gateway also provides basic API gateway functionalities such as JWT token validation and request authorization. Gateways in privileged workspaces can route to services outside the workspace while those in unprivileged workspaces can only route to services inside the workspace.\nThe following example declares an ingress gateway running on pods with app: gateway labels in the ns1 namespace.","title":"Ingress Gateway"},{"content":" Image Values for the TSB operator image.\nField Description Validation Rule registry\nstring Registry used to download the operator image.\n–\ntag\nstring The tag of the operator image.\n–\nOperator Operator values for the TSB operator application.\nField Description Validation Rule deployment\ntetrateio.api.install.helm.common.v1alpha1.Operator.Deployment Values for the TSB operator deployment.\n–\nservice\ntetrateio.api.install.helm.common.v1alpha1.Operator.Service Values for the TSB operator service.\n– …","relpermalink":"/tsb/refs/install/helm/common/v1alpha1/common/","summary":"Image Values for the TSB operator image.\nField Description Validation Rule registry\nstring Registry used to download the operator image.\n–\ntag\nstring The tag of the operator image.\n–\nOperator Operator values for the TSB operator application.\nField Description Validation Rule deployment\ntetrateio.api.install.helm.common.v1alpha1.Operator.Deployment Values for the TSB operator deployment.\n–\nservice\ntetrateio.api.install.helm.common.v1alpha1.Operator.Service Values for the TSB operator service.\n–\nserviceAccount\ntetrateio.api.install.helm.common.v1alpha1.Operator.ServiceAccount Values for the TSB operator service account.\n–\nDeployment Values for the TSB operator deployment.\nField Description Validation Rule affinity\ntetrateio.api.install.kubernetes.Affinity Affinity configuration for the pod. https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity\n–\nannotations\nmap\u003cstring, string\u003e Custom collection of annotations to add to the deployment.","title":"install/helm/common/v1alpha1/common.proto"},{"content":" Secrets Secrets available in the ControlPlane installation.\nField Description Validation Rule tsb\ntetrateio.api.install.helm.controlplane.v1alpha1.Secrets.TSB Secrets to reach the TSB Management Plane.\n–\nelasticsearch\ntetrateio.api.install.helm.controlplane.v1alpha1.Secrets.ElasticSearch Secrets to reach the Elasticsearch.\n–\nxcp\ntetrateio.api.install.helm.controlplane.v1alpha1.Secrets.XCP Secrets to reach the XCP Central in the Management Plane.\n–\nclusterServiceAccount …","relpermalink":"/tsb/refs/install/helm/controlplane/v1alpha1/values/","summary":"Secrets Secrets available in the ControlPlane installation.\nField Description Validation Rule tsb\ntetrateio.api.install.helm.controlplane.v1alpha1.Secrets.TSB Secrets to reach the TSB Management Plane.\n–\nelasticsearch\ntetrateio.api.install.helm.controlplane.v1alpha1.Secrets.ElasticSearch Secrets to reach the Elasticsearch.\n–\nxcp\ntetrateio.api.install.helm.controlplane.v1alpha1.Secrets.XCP Secrets to reach the XCP Central in the Management Plane.\n–\nclusterServiceAccount\ntetrateio.api.install.helm.controlplane.v1alpha1.Secrets.ClusterServiceAccount Cluster service account used to authenticate to the Management Plane.\n–\nClusterServiceAccount Cluster service account used to authenticate to the Management Plane.\nField Description Validation Rule clusterFQN\nstring TSB FQN of the onboarded cluster resource. This will be generate tokens for all Control Plane agents.\n–\nJWK\nstring Literal JWK used to generate and sign the tokens for all the Control Plane agents.","title":"install/helm/controlplane/v1alpha1/values.proto"},{"content":" Service to manage gateway settings in Istio Direct mode.\nIstioGateway The Istio Gateway service provides methods to manage gateway settings in Istio direct mode.\nThe methods in this service allow users to push Istio gateway configuration resources into TSB. All properties of the TSB resource hierarchies apply as well to these resources: grouping, access control policies in the management plane, etc.\nCreateVirtualService rpc CreateVirtualService …","relpermalink":"/tsb/refs/tsb/gateway/v2/istio-gateway-direct/","summary":"Service to manage gateway settings in Istio Direct mode.\nIstioGateway The Istio Gateway service provides methods to manage gateway settings in Istio direct mode.\nThe methods in this service allow users to push Istio gateway configuration resources into TSB. All properties of the TSB resource hierarchies apply as well to these resources: grouping, access control policies in the management plane, etc.\nCreateVirtualService rpc CreateVirtualService (tetrateio.api.tsb.types.v2.CreateIstioObjectRequest) returns (tetrateio.api.tsb.types.v2.IstioObject)\nRequires CREATE\nCreate a new Istio VirtualService in the gateway group. Note that the VirtualService must be in one of the namespaces owned by the group.\nGetVirtualService rpc GetVirtualService (tetrateio.api.tsb.types.v2.GetIstioObjectRequest) returns (tetrateio.api.tsb.types.v2.IstioObject)","title":"Istio Direct Mode Gateway Service"},{"content":" Service to manage security settings in Istio Direct mode.\nIstioSecurity The Istio Security service provides methods to manage security settings in Istio direct mode.\nThe methods in this service allow users to push Istio security configuration resources into TSB. All properties of the TSB resource hierarchies apply as well to these resources: grouping, access control policies in the management plane, etc.\nCreatePeerAuthentication rpc CreatePeerAuthentication …","relpermalink":"/tsb/refs/tsb/security/v2/istio-security-direct/","summary":"Service to manage security settings in Istio Direct mode.\nIstioSecurity The Istio Security service provides methods to manage security settings in Istio direct mode.\nThe methods in this service allow users to push Istio security configuration resources into TSB. All properties of the TSB resource hierarchies apply as well to these resources: grouping, access control policies in the management plane, etc.\nCreatePeerAuthentication rpc CreatePeerAuthentication (tetrateio.api.tsb.types.v2.CreateIstioObjectRequest) returns (tetrateio.api.tsb.types.v2.IstioObject)\nRequires CREATE\nCreate a new Istio PeerAuthentication resource in the given group.\nGetPeerAuthentication rpc GetPeerAuthentication (tetrateio.api.tsb.types.v2.GetIstioObjectRequest) returns (tetrateio.api.tsb.types.v2.IstioObject)\nRequires READ\nGet the details of the given Istio PeerAuthentication resource.\nUpdatePeerAuthentication rpc UpdatePeerAuthentication (tetrateio.","title":"Istio Direct Mode Security Service"},{"content":" Service to manage traffic settings in Istio Direct mode.\nIstioTraffic The Istio Traffic service provides methods to manage traffic settings in Istio direct mode.\nThe methods in this service allow users to push Istio traffic configuration resources into TSB. All properties of the TSB resource hierarchies apply as well to these resources: grouping, access control policies in the management plane, etc.\nCreateVirtualService rpc CreateVirtualService …","relpermalink":"/tsb/refs/tsb/traffic/v2/istio-traffic-direct/","summary":"Service to manage traffic settings in Istio Direct mode.\nIstioTraffic The Istio Traffic service provides methods to manage traffic settings in Istio direct mode.\nThe methods in this service allow users to push Istio traffic configuration resources into TSB. All properties of the TSB resource hierarchies apply as well to these resources: grouping, access control policies in the management plane, etc.\nCreateVirtualService rpc CreateVirtualService (tetrateio.api.tsb.types.v2.CreateIstioObjectRequest) returns (tetrateio.api.tsb.types.v2.IstioObject)\nRequires CREATE\nCreate an Istio VirtualService in the given traffic group. Note that the VirtualService must be in one of the namespaces owned by the group.\nGetVirtualService rpc GetVirtualService (tetrateio.api.tsb.types.v2.GetIstioObjectRequest) returns (tetrateio.api.tsb.types.v2.IstioObject)","title":"Istio Direct Mode Traffic Service"},{"content":" DEPRECATED: use Access Bindings instead.\nIstioInternalAccessBindings is an assignment of roles to a set of users or teams to access resources under a Istio internal group. The user or team information is obtained from an LDAP server that should have been configured as part of Service Bridge installation. Note that a IstioInternalAccessBinding can be created or modified only by users who have SET_POLICY permission on the Istio internal group.\nThe following example assigns the istiointernal-admin …","relpermalink":"/tsb/refs/tsb/rbac/v2/istio-internal-access-bindings/","summary":"DEPRECATED: use Access Bindings instead.\nIstioInternalAccessBindings is an assignment of roles to a set of users or teams to access resources under a Istio internal group. The user or team information is obtained from an LDAP server that should have been configured as part of Service Bridge installation. Note that a IstioInternalAccessBinding can be created or modified only by users who have SET_POLICY permission on the Istio internal group.\nThe following example assigns the istiointernal-admin role to users alice, bob, and members of the istiointernal-ops team for istio internal group g1 under workspace w1 owned by the tenant mycompany. Use fully-qualified name (fqn) when specifying user and team","title":"Istio Internal Access Bindings"},{"content":" IstioInternalDirect service provides methods to manage resources in Istio direct mode.\nIstioInternalDirect IstioInternalDirect service provides methods to manage resources in Istio direct mode.\nThe methods in this service allow users to push resources like Istio Envoy filters or service entries, into TSB. All properties of the TSB resource hierarchies apply as well to these resources: grouping, access control policies in the management plane, etc.\nCreateEnvoyFilter rpc CreateEnvoyFilter …","relpermalink":"/tsb/refs/tsb/istiointernal/v2/istio-istiointernal-direct/","summary":"IstioInternalDirect service provides methods to manage resources in Istio direct mode.\nIstioInternalDirect IstioInternalDirect service provides methods to manage resources in Istio direct mode.\nThe methods in this service allow users to push resources like Istio Envoy filters or service entries, into TSB. All properties of the TSB resource hierarchies apply as well to these resources: grouping, access control policies in the management plane, etc.\nCreateEnvoyFilter rpc CreateEnvoyFilter (tetrateio.api.tsb.types.v2.CreateIstioObjectRequest) returns (tetrateio.api.tsb.types.v2.IstioObject)\nRequires CREATE\nCreate an Istio EnvoyFilter in the given istio internal group. Note that the EnvoyFilter must be in one of the namespaces owned by the group.\nGetEnvoyFilter rpc GetEnvoyFilter (tetrateio.","title":"Istio Internal Direct Mode Service"},{"content":" IstioInternal service provides methods to manage istio internal TSB resources.\nIstioInternal IstioInternal service provides methods to manage istio internal TSB resources.\nIt provides methods to create and manage istio internal groups within a workspace, allowing to create fine-grained groupings to configure a subset of the workspace namespaces. Access policies can be assigned at group level, providing a fine-grained access control to the istio internal configuration features.\nCreateGroup rpc …","relpermalink":"/tsb/refs/tsb/istiointernal/v2/istiointernal-service/","summary":"IstioInternal service provides methods to manage istio internal TSB resources.\nIstioInternal IstioInternal service provides methods to manage istio internal TSB resources.\nIt provides methods to create and manage istio internal groups within a workspace, allowing to create fine-grained groupings to configure a subset of the workspace namespaces. Access policies can be assigned at group level, providing a fine-grained access control to the istio internal configuration features.\nCreateGroup rpc CreateGroup (tetrateio.api.tsb.istiointernal.v2.CreateIstioInternalGroupRequest) returns (tetrateio.api.tsb.istiointernal.v2.Group)\nRequires CREATE\nCreate a new Istio internal group in the given workspace.\nGroups will by default configure all the namespaces owned by their workspace, unless explicitly configured. If a specific set of namespaces is set for the group, it must be a subset of the namespaces defined by its workspace.","title":"Istio Internal Direct Mode Service"},{"content":" Istio internal groups only allow grouping DIRECT mode mesh resources in a set of namespaces owned by its parent workspace. This group is aimed for grouping resources not directly related to traffic, security, or gateway like EnvoyFilters and ServiceEntry for instance. Istio internal group is meant to group highly coupled and implementation-detailed oriented istio resources that don’t provide any BRIDGE mode guarantees or backward/forward compatibilities that other groups like traffic, security …","relpermalink":"/tsb/refs/tsb/istiointernal/v2/istio-internal-group/","summary":"Istio internal groups only allow grouping DIRECT mode mesh resources in a set of namespaces owned by its parent workspace. This group is aimed for grouping resources not directly related to traffic, security, or gateway like EnvoyFilters and ServiceEntry for instance. Istio internal group is meant to group highly coupled and implementation-detailed oriented istio resources that don’t provide any BRIDGE mode guarantees or backward/forward compatibilities that other groups like traffic, security of gateway can provide. Especially, and mainly because resources like EnvoyFilters, are highly customizable and can interfere in unpredictable ways, with any other routing, security, listeners, or filter chains among other configurations that TSB may have setup.","title":"Istio internal Group"},{"content":" Configuration affecting Istio control plane installation version and shape. Note: unlike other Istio protos, field names must use camelCase. This is asserted in tests. Without camelCase, the json tag on the Go struct will not match the user’s JSON representation. This leads to Kubernetes merge libraries, which rely on this tag, to fail. All other usages use jsonpb which does not use the json tag.\nistio.operator.v1alpha1.Affinity See k8s.io.api.core.v1.Affinity.\nField Description Validation Rule …","relpermalink":"/tsb/refs/istio.io/api/operator/v1alpha1/operator/","summary":"Configuration affecting Istio control plane installation version and shape. Note: unlike other Istio protos, field names must use camelCase. This is asserted in tests. Without camelCase, the json tag on the Go struct will not match the user’s JSON representation. This leads to Kubernetes merge libraries, which rely on this tag, to fail. All other usages use jsonpb which does not use the json tag.\nistio.operator.v1alpha1.Affinity See k8s.io.api.core.v1.Affinity.\nField Description Validation Rule nodeAffinity\nistio.operator.v1alpha1.NodeAffinity –\npodAffinity\nistio.operator.v1alpha1.PodAffinity –\npodAntiAffinity\nistio.operator.v1alpha1.PodAntiAffinity –\nistio.operator.v1alpha1.BaseComponentSpec Configuration for base component.\nField Description Validation Rule enabled\ngoogle.protobuf.BoolValue Selects whether this component is installed.\n–","title":"IstioOperator Options"},{"content":" JwtIdentity represents an JWT identity of a workload.\nE.g.,\nJWT identity of a workload:\nissuer: https://mycompany.corp subject: us-east-datacenter1-vm007 attributes: region: us-east datacenter: datacenter1 instance_name: vm007 instance_hostname: vm007.internal.corp instance_role: app-ratings JwtIdentity JwtIdentity represents an JWT identity of a workload.\nField Description Validation Rule issuer\nstring REQUIRED JWT Issuer identifier.\nThe value must be a case sensitive URL using the https …","relpermalink":"/tsb/refs/onboarding/config/types/identity/jwt/v1alpha1/jwt/","summary":"JwtIdentity represents an JWT identity of a workload.\nE.g.,\nJWT identity of a workload:\nissuer: https://mycompany.corp subject: us-east-datacenter1-vm007 attributes: region: us-east datacenter: datacenter1 instance_name: vm007 instance_hostname: vm007.internal.corp instance_role: app-ratings JwtIdentity JwtIdentity represents an JWT identity of a workload.\nField Description Validation Rule issuer\nstring REQUIRED JWT Issuer identifier.\nThe value must be a case sensitive URL using the https scheme that contains scheme, host, and optionally, port number and path components and no query or fragment components.\nE.g., https://mycompany.corp, https://accounts.google.com, https://sts.windows.net/9edbd6c9-0e5b-4cfd-afec-fdde27cdd928/, etc.\nSee https://openid.net/specs/openid-connect-core-1_0.html#IDToken\nstring = { prefix: https:// uri: true}\nsubject\nstring REQUIRED Workload identifier (JWT subject).\nA locally unique identifier within the Issuer.","title":"JWT Identity"},{"content":" JwtIdentityMatcher specifies matching workloads with JWT identities.\nFor example, the following configuration will match only those workloads that were authenticated by means of an OIDC ID Token issued by https://mycompany.corp for one of the subjects us-east-datacenter1-vm007 or us-west-datacenter2-vm008:\nissuer: \u0026#34;https://mycompany.corp\u0026#34; subjects: - \u0026#34;us-east-datacenter1-vm007\u0026#34; - \u0026#34;us-west-datacenter2-vm008\u0026#34; In those cases where an OIDC ID Token from a given issuer includes a map of fine-grained …","relpermalink":"/tsb/refs/onboarding/config/authorization/jwt/v1alpha1/jwt/","summary":"JwtIdentityMatcher specifies matching workloads with JWT identities.\nFor example, the following configuration will match only those workloads that were authenticated by means of an OIDC ID Token issued by https://mycompany.corp for one of the subjects us-east-datacenter1-vm007 or us-west-datacenter2-vm008:\nissuer: \"https://mycompany.corp\" subjects: - \"us-east-datacenter1-vm007\" - \"us-west-datacenter2-vm008\" In those cases where an OIDC ID Token from a given issuer includes a map of fine-grained attributes associated with a workload, it is possible to define rules that match those attributes.\nE.g., the following configuration will match a set workloads that were authenticated by means of an OIDC ID Token issued by https://mycompany.corp and include 1) attribute region with one of the values us-east or us-west and 2) attribute instance_role with the value app-ratings:","title":"JWT Identity Matcher"},{"content":" JwtIssuer specifies configuration associated with a JWT issuer.\nFor example,\nissuer: \u0026#34;https://mycompany.corp\u0026#34; jwksUri: \u0026#34;https://mycompany.corp/jwks.json\u0026#34; shortName: \u0026#34;mycorp\u0026#34; tokenFields: attributes: jsonPath: .custom_attributes JwtIssuer JwtIssuer specifies configuration associated with a JWT issuer.\nField Description Validation Rule issuer\nstring REQUIRED JWT Issuer identifier.\nThe value must be a case sensitive URL using the https scheme that contains scheme, host, and optionally, port number …","relpermalink":"/tsb/refs/onboarding/config/install/v1alpha1/jwt-issuer/","summary":"JwtIssuer specifies configuration associated with a JWT issuer.\nFor example,\nissuer: \"https://mycompany.corp\" jwksUri: \"https://mycompany.corp/jwks.json\" shortName: \"mycorp\" tokenFields: attributes: jsonPath: .custom_attributes JwtIssuer JwtIssuer specifies configuration associated with a JWT issuer.\nField Description Validation Rule issuer\nstring REQUIRED JWT Issuer identifier.\nThe value must be a case sensitive URL using the https scheme that contains scheme, host, and optionally, port number and path components and no query or fragment components.\nE.g., https://mycompany.corp, https://accounts.google.com, https://sts.windows.net/9edbd6c9-0e5b-4cfd-afec-fdde27cdd928/, etc.\nSee https://openid.net/specs/openid-connect-core-1_0.html#IDToken\nstring = { prefix: https:// uri: true}\njwksUri\nstring oneof jwks_source URL of the JSON Web Key Set document.\nSource of public keys the Workload Onboarding Plane should use to validate the signature of an OIDC ID Token.","title":"JWT Issuer"},{"content":" When installing on Kubernetes, these configuration settings can be used to override the default Kubernetes configuration. Kubernetes configuration can be set on each component in the install API using the kubeSpec field.\nThe API allows for customization of every field in the rendered Kubernetes manifests. The more common configuration fields, such as resources and service type, are supported directly; and can be configured like so:\napiVersion: install.tetrate.io/v1alpha1 kind: ManagementPlane …","relpermalink":"/tsb/refs/install/kubernetes/k8s/","summary":"When installing on Kubernetes, these configuration settings can be used to override the default Kubernetes configuration. Kubernetes configuration can be set on each component in the install API using the kubeSpec field.\nThe API allows for customization of every field in the rendered Kubernetes manifests. The more common configuration fields, such as resources and service type, are supported directly; and can be configured like so:\napiVersion: install.tetrate.io/v1alpha1 kind: ManagementPlane metadata: name: managementplane spec: hub: docker.io/tetrate components: apiServer: kubeSpec: service: type: LoadBalancer deployment: resources: limits: memory: 750Mi requests: memory: 500Mi All components have a deployment and service object. Some, such as apiServer, also have a job object associated with them.","title":"Kubernetes"},{"content":" ManagementPlane resource exposes a set of configurations necessary to automatically install the Service Bridge management plane on a cluster. The installation API is an override API so any unset fields that are not required will use sensible defaults.\nPrior to creating the ManagementPlane resource, verify that the following secrets exist in the namespace the management plane will be installed into:\ntsb-certs ldap-credentials custom-host-ca (if you are using TLS connection and need a custom CA …","relpermalink":"/tsb/refs/install/managementplane/v1alpha1/spec/","summary":"ManagementPlane resource exposes a set of configurations necessary to automatically install the Service Bridge management plane on a cluster. The installation API is an override API so any unset fields that are not required will use sensible defaults.\nPrior to creating the ManagementPlane resource, verify that the following secrets exist in the namespace the management plane will be installed into:\ntsb-certs ldap-credentials custom-host-ca (if you are using TLS connection and need a custom CA to connect to LDAP host) postgres-credentials (non-demo deployments) admin-credentials es-certs (if your Elasticsearch is using a self-signed certificate) elastic-credentials (if your Elasticsearch backend requires authentication) A resource containing only the container registry hub will install a demo of Service Bridge, create a default Organization and install local instances of external dependencies, such as Postgres, Elasticsearch, and LDAP server.","title":"Management Plane"},{"content":" A metric is a measurement about a service, captured at runtime. Logically, the moment of capturing one of these measurements is known as a metric event which consists not only of the measurement itself, but the time that it was captured and associated metadata..\nThe key aspects of a metric are the measure, the metric type, the metric origin, and the metric detect point:\nThe measure describes the type and unit of a metric event also known as measurement. The metric type is the aggregation over …","relpermalink":"/tsb/refs/tsb/observability/telemetry/v2/metric/","summary":"A metric is a measurement about a service, captured at runtime. Logically, the moment of capturing one of these measurements is known as a metric event which consists not only of the measurement itself, but the time that it was captured and associated metadata..\nThe key aspects of a metric are the measure, the metric type, the metric origin, and the metric detect point:\nThe measure describes the type and unit of a metric event also known as measurement. The metric type is the aggregation over time applied to the measurements. The metric origin tells from where the metric measurements come from.","title":"Metric"},{"content":" Service to manage telemetry metrics.\nMetrics The Metrics service exposes methods to manage Telemetry Metrics from Telemetry Sources.\nGetMetric rpc GetMetric (tetrateio.api.tsb.observability.telemetry.v2.GetMetricRequest) returns (tetrateio.api.tsb.observability.telemetry.v2.Metric)\nGet the details of an existing telemetry metric.\nListMetrics rpc ListMetrics (tetrateio.api.tsb.observability.telemetry.v2.ListMetricsRequest) returns …","relpermalink":"/tsb/refs/tsb/observability/telemetry/v2/metric-service/","summary":"Service to manage telemetry metrics.\nMetrics The Metrics service exposes methods to manage Telemetry Metrics from Telemetry Sources.\nGetMetric rpc GetMetric (tetrateio.api.tsb.observability.telemetry.v2.GetMetricRequest) returns (tetrateio.api.tsb.observability.telemetry.v2.Metric)\nGet the details of an existing telemetry metric.\nListMetrics rpc ListMetrics (tetrateio.api.tsb.observability.telemetry.v2.ListMetricsRequest) returns (tetrateio.api.tsb.observability.telemetry.v2.ListMetricsResponse)\nList the telemetry metrics that are available for the requested telemetry source.\nGetMetricRequest Request to retrieve a telemetry metric from a parent telemetry source resource.\nField Description Validation Rule fqn\nstring REQUIRED Fully-qualified name of the telemetry metric.\nstring = { min_len: 1}\nListMetricsRequest Request to retrieve the list of telemetry metrics from a parent telemetry source resource.\nField Description Validation Rule parent","title":"Metric Service"},{"content":" Onboarding Configuration specifies where to onboard the workload to.\nTo be able to onboard a workload into a service mesh, a user must configure Workload Onboarding Agent with the location of the Workload Onboarding Endpoint and a name of the WorkloadGroup to join.\nBy default, Workload Onboarding Agent will read Onboarding Configuration from a file /etc/onboarding-agent/onboarding.config.yaml, which must be created by the user.\nIf Onboarding Configuration file is missing or its contents is not …","relpermalink":"/tsb/refs/onboarding/config/agent/v1alpha1/onboarding-configuration/","summary":"Onboarding Configuration specifies where to onboard the workload to.\nTo be able to onboard a workload into a service mesh, a user must configure Workload Onboarding Agent with the location of the Workload Onboarding Endpoint and a name of the WorkloadGroup to join.\nBy default, Workload Onboarding Agent will read Onboarding Configuration from a file /etc/onboarding-agent/onboarding.config.yaml, which must be created by the user.\nIf Onboarding Configuration file is missing or its contents is not valid, Workload Onboarding Agent will not be able to start.\nConsider the following example of the minimal valid configuration:\napiVersion: config.agent.onboarding.tetrate.io/v1alpha1 kind: OnboardingConfiguration onboardingEndpoint: host: onboarding.","title":"Onboarding Configuration"},{"content":" Onboarding Policy authorizes matching workloads to join the mesh and become a part of a WorkloadGroup.\nBy default, none of the workloads are allowed to join the mesh.\nA workload is only allowed to join the mesh if there is an OnboardingPolicy resource that explicitly authorizes that.\nFor the purposes of authorization, a workload is considered to have the identity of the host it is running on.\nE.g., workloads that run on VMs in the cloud are considered to have cloud-specific identity of that VM. …","relpermalink":"/tsb/refs/onboarding/config/authorization/v1alpha1/policy/","summary":"Onboarding Policy authorizes matching workloads to join the mesh and become a part of a WorkloadGroup.\nBy default, none of the workloads are allowed to join the mesh.\nA workload is only allowed to join the mesh if there is an OnboardingPolicy resource that explicitly authorizes that.\nFor the purposes of authorization, a workload is considered to have the identity of the host it is running on.\nE.g., workloads that run on VMs in the cloud are considered to have cloud-specific identity of that VM. In case of AWS EC2 instances, VM identity includes AWS Partition, AWS Account number, AWS Region, AWS Zone, EC2 instance id, AWS IAM Role name, etc.","title":"Onboarding Policy"},{"content":" OpenAPI Extensions available to configure APIs.\nOpenAPIExtension Metadata describing an extension to the OpenAPI spec.\nField Description Validation Rule name\nstring The name of the OpenAPI extension as it should appear in the OpenAPI document. For example: x-tsb-service\n–\nappliesTo\nList of string Parts of the OpenAPI spec where this custom extension is allowed. This is a list of names of the OpenAPI elements where the extension is supported. For example: [“info”, “path”]\n–\nrequired\nbool Flag …","relpermalink":"/tsb/refs/tsb/application/v2/openapi-extensions/","summary":"OpenAPI Extensions available to configure APIs.\nOpenAPIExtension Metadata describing an extension to the OpenAPI spec.\nField Description Validation Rule name\nstring The name of the OpenAPI extension as it should appear in the OpenAPI document. For example: x-tsb-service\n–\nappliesTo\nList of string Parts of the OpenAPI spec where this custom extension is allowed. This is a list of names of the OpenAPI elements where the extension is supported. For example: [“info”, “path”]\n–\nrequired\nbool Flag that configures if the extension is mandatory for the elements where it is supported.\n–\nOpenAPIExtensions Available OpenAPI extensions to configure APi Gateway features in Service Bridge.","title":"OpenAPI Extensions"},{"content":" Organization is a root of the Service Bridge object hierarchy. Each organization is completely independent of the other with its own set of tenants, users, teams, clusters and workspaces.\nOrganizations in TSB are tied to an Identity Provider (IdP). Users and teams, representing the organizational structure, are periodically synchronized from the IdP into TSB in order to make them available for access policy configuration.\nThe following example creates an organization named myorg.\napiVersion: …","relpermalink":"/tsb/refs/tsb/v2/organization/","summary":"Organization is a root of the Service Bridge object hierarchy. Each organization is completely independent of the other with its own set of tenants, users, teams, clusters and workspaces.\nOrganizations in TSB are tied to an Identity Provider (IdP). Users and teams, representing the organizational structure, are periodically synchronized from the IdP into TSB in order to make them available for access policy configuration.\nThe following example creates an organization named myorg.\napiVersion: api.tsb.tetrate.io/v2 kind: Organization metadata: name: myorg Organization Organization is the root of the Service Bridge object hierarchy.\nField Description Validation Rule deletionProtectionEnabled\nbool When set, prevents the resource from being deleted.","title":"Organization"},{"content":" DEPRECATED: use Access Bindings instead.\nOrganizationAccessBindings is an assignment of roles to a set of users or teams to access resources under an Organization. The user or team information is obtained from an LDAP server that should have been configured as part of Service Bridge installation. Note that a OrganizationAccessBinding can be created or modified only by users who have SET_POLICY permission on the Organization.\nThe following example assigns the org-admin role to users alice, bob, …","relpermalink":"/tsb/refs/tsb/rbac/v2/organization-access-bindings/","summary":"DEPRECATED: use Access Bindings instead.\nOrganizationAccessBindings is an assignment of roles to a set of users or teams to access resources under an Organization. The user or team information is obtained from an LDAP server that should have been configured as part of Service Bridge installation. Note that a OrganizationAccessBinding can be created or modified only by users who have SET_POLICY permission on the Organization.\nThe following example assigns the org-admin role to users alice, bob, and members of the t1 team owned by the organization myorg. Use fully-qualified name (fqn) when specifying user and team\napiVersion: rbac.tsb.tetrate.io/v2 kind: OrganizationAccessBindings metadata: organization: myorg spec: allow: - role: rbac/org-admin subjects: - user: organization/myorg/users/alice - user: organization/myorg/users/bob - team: organization/myorg/teams/t1 OrganizationAccessBindings OrganizationAccessBindings assigns permissions to users of organizations.","title":"Organization Access Bindings"},{"content":" Organization Setting allows configuring global settings for the organization. Settings such as network reachability or regional failover that apply globally to the organization are configured in the Organizations Setting object.\nThis is a global object that uniquely configures the organization, and there can be only one Organization Setting object defined for each organization.\nThe following example shows how these settings can be used to describe the organization’s network reachability …","relpermalink":"/tsb/refs/tsb/v2/organization-setting/","summary":"Organization Setting allows configuring global settings for the organization. Settings such as network reachability or regional failover that apply globally to the organization are configured in the Organizations Setting object.\nThis is a global object that uniquely configures the organization, and there can be only one Organization Setting object defined for each organization.\nThe following example shows how these settings can be used to describe the organization’s network reachability settings and some regional failover configurations.\napiVersion: api.tsb.tetrate.io/v2 kind: OrganizationSetting metadata: name: org-settings organization: myorg spec: networkSettings: networkReachability: vpc01: vpc02,vpc03 regionalFailover: - from: us-east1 to: us-central1 OrganizationSetting Settings that apply globally to the entire organization.","title":"Organization Setting"},{"content":" Service to manage Organizations in TSB\nOrganizations The Organizations service exposes methods to manage the organizations that exist in TSB. Organizations are the root of the Service Bridge object hierarchy. Each organization is completely independent of the other with its own set of tenants, users, teams, clusters and workspaces.\nGetOrganization rpc GetOrganization (tetrateio.api.tsb.v2.GetOrganizationRequest) returns (tetrateio.api.tsb.v2.Organization)\nRequires READ\nGet the details of an …","relpermalink":"/tsb/refs/tsb/v2/organization-service/","summary":"Service to manage Organizations in TSB\nOrganizations The Organizations service exposes methods to manage the organizations that exist in TSB. Organizations are the root of the Service Bridge object hierarchy. Each organization is completely independent of the other with its own set of tenants, users, teams, clusters and workspaces.\nGetOrganization rpc GetOrganization (tetrateio.api.tsb.v2.GetOrganizationRequest) returns (tetrateio.api.tsb.v2.Organization)\nRequires READ\nGet the details of an organization.\nSyncOrganization rpc SyncOrganization (tetrateio.api.tsb.v2.SyncOrganizationRequest) returns (tetrateio.api.tsb.v2.SyncOrganizationResponse)\nRequires CreateUser, CreateTeam, DeleteUser, DeleteTeam, WriteTeam\nSyncOrganization is used by processes that monitor the identity providers to synchronize the users and teams with the ones in TSB.\nThis method will update the state of users and groups in the organization and will create, modify, and delete groups according to the incoming request.","title":"Organizations Service"},{"content":" Permissions.\nPermission A permission defines an action that can be performed on a resource. By default access to resources is denied unless an explicit permission grants access to perform an operation against it.\nField Number Description INVALID\n0\nDefault value to designate no value was explicitly set for the permission.\nREAD\n1\nThe read permission grants read-only access to the resource.\nWRITE\n2\nThe write permission allows the subject to modify an existing resource.\nCREATE\n3\nThe create …","relpermalink":"/tsb/refs/tsb/rbac/v2/permissions/","summary":"Permissions.\nPermission A permission defines an action that can be performed on a resource. By default access to resources is denied unless an explicit permission grants access to perform an operation against it.\nField Number Description INVALID\n0\nDefault value to designate no value was explicitly set for the permission.\nREAD\n1\nThe read permission grants read-only access to the resource.\nWRITE\n2\nThe write permission allows the subject to modify an existing resource.\nCREATE\n3\nThe create permission allows subjects to create child resources on the resource.\nDELETE\n4\nThe delete permission grants permissions to delete the resource.","title":"Permissions"},{"content":" Service to manage centralized approval policies.\nPermissions The Permissions service exposes methods to query permission information on existing records. $hide_from_yaml\nQueryResourcePermissions rpc QueryResourcePermissions (tetrateio.api.tsb.q.v2.QueryResourcePermissionsRequest) returns (tetrateio.api.tsb.q.v2.QueryResourcePermissionsResponse)\nQueryResourcePermission looks up permissions that are allowed for the current principal. Multiple records can be queried with a single request. Query …","relpermalink":"/tsb/refs/tsb/q/v2/permissions-service/","summary":"Service to manage centralized approval policies.\nPermissions The Permissions service exposes methods to query permission information on existing records. $hide_from_yaml\nQueryResourcePermissions rpc QueryResourcePermissions (tetrateio.api.tsb.q.v2.QueryResourcePermissionsRequest) returns (tetrateio.api.tsb.q.v2.QueryResourcePermissionsResponse)\nQueryResourcePermission looks up permissions that are allowed for the current principal. Multiple records can be queried with a single request. Query limit is 100, multiple requests are required to lookup more than the limit.\nGetResourcePermissions rpc GetResourcePermissions (tetrateio.api.tsb.q.v2.GetResourcePermissionsRequest) returns (tetrateio.api.tsb.q.v2.GetResourcePermissionsResponse)\nGetResourcePermission looks up permissions that are allowed for the current principal. on the given resource FQN. This is similar to QueryResourcePermission but limited to a single resource FQN.\nGetResourcePermissionsRequest Request to query permissions on a single record by FQN.","title":"Permissions Service"},{"content":" Access Policy Bindings.\nBinding A binding associates a role with a set of subjects.\nBindings are used to configure policies, where different roles can be assigned to different sets of subjects to configure a fine-grained access control to the resource protected by the policy.\nField Description Validation Rule role\nstring REQUIRED The role that defines the permissions that will be granted to the target resource.\nstring = { min_len: 1}\nsubjects\nList of tetrateio.api.tsb.rbac.v2.Subject The set of …","relpermalink":"/tsb/refs/tsb/rbac/v2/binding/","summary":"Access Policy Bindings.\nBinding A binding associates a role with a set of subjects.\nBindings are used to configure policies, where different roles can be assigned to different sets of subjects to configure a fine-grained access control to the resource protected by the policy.\nField Description Validation Rule role\nstring REQUIRED The role that defines the permissions that will be granted to the target resource.\nstring = { min_len: 1}\nsubjects\nList of tetrateio.api.tsb.rbac.v2.Subject The set of subjects that will be allowed to access the target resource with the permissions defined by the role.\n–\nSubject Subject identifies a user or a team under an organization.","title":"Policy Bindings"},{"content":" Service to manage access control policies for TSB resources\nPolicy The Policy service provides methods to configure the access control policies for TSB resources.\nAll TSB resources have one and exactly one policy document that configures access for it. When resources are created, a default policy is attached to the resource, assigning administration privileges on the resource to the user that created it.\nGetPolicy rpc GetPolicy (tetrateio.api.tsb.rbac.v2.GetPolicyRequest) returns …","relpermalink":"/tsb/refs/tsb/rbac/v2/policy-service/","summary":"Service to manage access control policies for TSB resources\nPolicy The Policy service provides methods to configure the access control policies for TSB resources.\nAll TSB resources have one and exactly one policy document that configures access for it. When resources are created, a default policy is attached to the resource, assigning administration privileges on the resource to the user that created it.\nGetPolicy rpc GetPolicy (tetrateio.api.tsb.rbac.v2.GetPolicyRequest) returns (tetrateio.api.tsb.rbac.v2.AccessPolicy)\nGet the access policy for the given resource.\nSetPolicy rpc SetPolicy (tetrateio.api.tsb.rbac.v2.AccessPolicy) returns (google.protobuf.Empty)\nSet the access policy for the given resource.\nGetRootPolicy rpc GetRootPolicy (tetrateio.api.tsb.rbac.v2.GetAdminPolicyRequest) returns (tetrateio.api.tsb.rbac.v2.AccessPolicy)\nRequires SET_POLICY","title":"Policy Service"},{"content":" Services in the registry represent logically a service that can be running in different compute platforms and different locations. The same service could be running on different Kubernetes clusters at the same time, on VMS, etc. A service in the registry represents an aggregated and logical view for all those individual services, and provides high-level features such as aggregated metrics.\nPort Port exposed by a service. Registration RPC will complete the instances field by assigning the …","relpermalink":"/tsb/refs/tsb/registry/v2/service/","summary":"Services in the registry represent logically a service that can be running in different compute platforms and different locations. The same service could be running on different Kubernetes clusters at the same time, on VMS, etc. A service in the registry represents an aggregated and logical view for all those individual services, and provides high-level features such as aggregated metrics.\nPort Port exposed by a service. Registration RPC will complete the instances field by assigning the physical services FQNs.\nField Description Validation Rule number\nuint32 REQUIRED A valid non-negative integer port number.\nuint32 = { lte: 65535 gte: 1}","title":"Registered Service"},{"content":" Role is a named collection of permissions that can be assigned to any user or team in the system. The set of actions that can be performed by a user, such as the ability to create, delete, or update configuration will depend on the permissions associated with the user’s role. Roles are global resources that are defined once. AccessBindings in each configuration group will bind a user to a specific role defined apriori.\nTSB comes with the following predefined roles:\nRole Permissions Description …","relpermalink":"/tsb/refs/tsb/rbac/v2/role/","summary":"Role is a named collection of permissions that can be assigned to any user or team in the system. The set of actions that can be performed by a user, such as the ability to create, delete, or update configuration will depend on the permissions associated with the user’s role. Roles are global resources that are defined once. AccessBindings in each configuration group will bind a user to a specific role defined apriori.\nTSB comes with the following predefined roles:\nRole Permissions Description rbac/admin * Grants full access to the target resource and its child objects rbac/editor Read Write Create Grants read/write access to a resource and allows creating child resources rbac/creator Read Create Useful to delegate access to a resource without giving write access to the object itself.","title":"Role"},{"content":" Service to manage access roles in Service Bridge.\nRBAC The RBAC service provides methods to manage the roles in the Service Bridge platform. It provides method to configure the roles that can be used in the management plane access control policies and their permissions.\nCreateRole rpc CreateRole (tetrateio.api.tsb.rbac.v2.CreateRoleRequest) returns (tetrateio.api.tsb.rbac.v2.Role)\nRequires CREATE\nCreate a new role.\nListRoles rpc ListRoles (tetrateio.api.tsb.rbac.v2.ListRolesRequest) returns …","relpermalink":"/tsb/refs/tsb/rbac/v2/role-service/","summary":"Service to manage access roles in Service Bridge.\nRBAC The RBAC service provides methods to manage the roles in the Service Bridge platform. It provides method to configure the roles that can be used in the management plane access control policies and their permissions.\nCreateRole rpc CreateRole (tetrateio.api.tsb.rbac.v2.CreateRoleRequest) returns (tetrateio.api.tsb.rbac.v2.Role)\nRequires CREATE\nCreate a new role.\nListRoles rpc ListRoles (tetrateio.api.tsb.rbac.v2.ListRolesRequest) returns (tetrateio.api.tsb.rbac.v2.ListRolesResponse)\nRequires READ\nList all existing roles.\nGetRole rpc GetRole (tetrateio.api.tsb.rbac.v2.GetRoleRequest) returns (tetrateio.api.tsb.rbac.v2.Role)\nRequires READ\nGet the details of the given role.\nUpdateRole rpc UpdateRole (tetrateio.api.tsb.rbac.v2.Role) returns (tetrateio.api.tsb.rbac.v2.Role)\nRequires WRITE\nModify a role.\nDeleteRole rpc DeleteRole (tetrateio.api.tsb.rbac.v2.DeleteRoleRequest) returns (google.","title":"Role Service"},{"content":" DEPRECATED: use Access Bindings instead.\nSecurityAccessBindings is an assignment of roles to a set of users or teams to access resources under a Security group. The user or team information is obtained from an LDAP server that should have been configured as part of Service Bridge installation. Note that a SecurityAccessBinding can be created or modified only by users who have SET_POLICY permission on the Security group.\nThe following example assigns the security-admin role to users alice, bob, …","relpermalink":"/tsb/refs/tsb/rbac/v2/security-access-bindings/","summary":"DEPRECATED: use Access Bindings instead.\nSecurityAccessBindings is an assignment of roles to a set of users or teams to access resources under a Security group. The user or team information is obtained from an LDAP server that should have been configured as part of Service Bridge installation. Note that a SecurityAccessBinding can be created or modified only by users who have SET_POLICY permission on the Security group.\nThe following example assigns the security-admin role to users alice, bob, and members of the security-ops team for the security group g1 under workspace w1 owned by the tenant mycompany. Use fully-qualified name (fqn) when specifying user and team","title":"Security Access Bindings"},{"content":" Security Groups allow grouping the proxy workloads in a set of namespaces owned by its parent workspace. Security related configurations can then be applied on the group to control the behavior of these proxy workloads. The group can be in one of two modes: BRIDGED and DIRECT. BRIDGED mode is a minimalistic mode that allows users to quickly configure the most commonly used features in the service mesh using Tetrate specific APIs, while the DIRECT mode provides more flexibility for power users …","relpermalink":"/tsb/refs/tsb/security/v2/security-group/","summary":"Security Groups allow grouping the proxy workloads in a set of namespaces owned by its parent workspace. Security related configurations can then be applied on the group to control the behavior of these proxy workloads. The group can be in one of two modes: BRIDGED and DIRECT. BRIDGED mode is a minimalistic mode that allows users to quickly configure the most commonly used features in the service mesh using Tetrate specific APIs, while the DIRECT mode provides more flexibility for power users by allowing them to configure the proxy workload’s security properties using a restricted subset of Istio Security APIs.","title":"Security Group"},{"content":" Service to manage security settings.\nSecurity The Security service provides methods to manage security settings in TSB.\nIt provides methods to create and manage security groups within a workspace, allowing to create fine-grained groupings to configure a subset of the workspace namespaces. Access policies can be assigned at group level, providing a fine-grained access control to the security configuration features.\nThe Security service also provides methods to configure the different security …","relpermalink":"/tsb/refs/tsb/security/v2/security-service/","summary":"Service to manage security settings.\nSecurity The Security service provides methods to manage security settings in TSB.\nIt provides methods to create and manage security groups within a workspace, allowing to create fine-grained groupings to configure a subset of the workspace namespaces. Access policies can be assigned at group level, providing a fine-grained access control to the security configuration features.\nThe Security service also provides methods to configure the different security settings that are allowed within each group.\nCreateGroup rpc CreateGroup (tetrateio.api.tsb.security.v2.CreateSecurityGroupRequest) returns (tetrateio.api.tsb.security.v2.Group)\nRequires CREATE\nCreate a new security group in the given workspace.\nGroups will by default configure all the namespaces owned by their workspace, unless explicitly configured.","title":"Security Service"},{"content":" SecuritySetting allows configuring security related properties such as TLS authentication and access control for traffic arriving at a proxy workload in a security group.\nSecurity settings can be propagated along any defined security settings in the configuration hierarchy. How security settings are propagated can be configured by specifying a PropagationStrategy.\nThe following example creates a security group for the proxy workloads in ns1, ns2 and ns3 namespaces owned by its parent workspace …","relpermalink":"/tsb/refs/tsb/security/v2/security-setting/","summary":"SecuritySetting allows configuring security related properties such as TLS authentication and access control for traffic arriving at a proxy workload in a security group.\nSecurity settings can be propagated along any defined security settings in the configuration hierarchy. How security settings are propagated can be configured by specifying a PropagationStrategy.\nThe following example creates a security group for the proxy workloads in ns1, ns2 and ns3 namespaces owned by its parent workspace w1 under tenant mycompany and defines a security setting that only allows mutual TLS authenticated traffic from other proxy workloads in the same group.\napiVersion: security.tsb.tetrate.io/v2 kind: Group metadata: name: t1 workspace: w1 tenant: mycompany organization: myorg spec: namespaceSelector: names: - \"*/ns1\" - \"*/ns2\" - \"*/ns3\" configMode: BRIDGED And the associated security settings for all proxy workloads in the group","title":"Security Setting"},{"content":" Service to map registered services to configuration groups.\nLookup The Lookup API allows resolving the groups that configure a particular service in the registry. It allows lookups given a service, but also reverse lookups to get all the services in the registry that are configured by a particular workspace or group.\nGroups rpc Groups (tetrateio.api.tsb.registry.v2.GroupLookupRequest) returns (tetrateio.api.tsb.registry.v2.GroupLookupResponse)\nRequires ReadTrafficGroup, ReadSecurityGroup, …","relpermalink":"/tsb/refs/tsb/registry/v2/lookup-service/","summary":"Service to map registered services to configuration groups.\nLookup The Lookup API allows resolving the groups that configure a particular service in the registry. It allows lookups given a service, but also reverse lookups to get all the services in the registry that are configured by a particular workspace or group.\nGroups rpc Groups (tetrateio.api.tsb.registry.v2.GroupLookupRequest) returns (tetrateio.api.tsb.registry.v2.GroupLookupResponse)\nRequires ReadTrafficGroup, ReadSecurityGroup, ReadGatewayGroup, ReadIstioInternalGroup\nGet all the groups that configure the given service in the registry.\nServices rpc Services (tetrateio.api.tsb.registry.v2.ServiceLookupRequest) returns (tetrateio.api.tsb.registry.v2.ServiceLookupResponse)\nRequires ReadRegisteredService\nGet all the services in the registry that are part of the given selector. This method can be used to resolve the registered services that are part of a workspace or group.","title":"Service Registry Lookup Service"},{"content":" Service Routes can be used by service owners to configure traffic shifting across different versions of a service in a Traffic Group. The traffic to this service can originate from sidecars in the same or different traffic groups, as well as gateways.\nThe following example yaml defines a Traffic Group g1 in the namespaces ns1, ns2 and ns3, owned by its parent Workspace w1. Then it defines a Service Route for the reviews service in the ns1 namespace with two subsets: v1 and v2, where 80% of the …","relpermalink":"/tsb/refs/tsb/traffic/v2/service-route/","summary":"Service Routes can be used by service owners to configure traffic shifting across different versions of a service in a Traffic Group. The traffic to this service can originate from sidecars in the same or different traffic groups, as well as gateways.\nThe following example yaml defines a Traffic Group g1 in the namespaces ns1, ns2 and ns3, owned by its parent Workspace w1. Then it defines a Service Route for the reviews service in the ns1 namespace with two subsets: v1 and v2, where 80% of the traffic to the reviews service is sent to v1 while the remaining 20% is sent to v2.","title":"Service Route"},{"content":" ServiceSecuritySetting allows configuring security related properties such as TLS authentication and access control for traffic arriving at a particular service in a security group. These settings will replace the security group wide settings for this service.\nThe following example defines a security setting that applies to the service foo in namespace ns1 that only allows mutual TLS authenticated traffic from other proxy workloads in the same group.\napiVersion: security.tsb.tetrate.io/v2 kind: …","relpermalink":"/tsb/refs/tsb/security/v2/service-security-setting/","summary":"ServiceSecuritySetting allows configuring security related properties such as TLS authentication and access control for traffic arriving at a particular service in a security group. These settings will replace the security group wide settings for this service.\nThe following example defines a security setting that applies to the service foo in namespace ns1 that only allows mutual TLS authenticated traffic from other proxy workloads in the same group.\napiVersion: security.tsb.tetrate.io/v2 kind: ServiceSecuritySetting metadata: name: foo-auth group: sg1 workspace: w1 tenant: mycompany org: myorg spec: service: ns1/foo.ns1.svc.cluster.local settings: authentication: REQUIRED authorization: mode: GROUP The following example customizes the Extensions to enable the execution of the WasmExtensions list specified, detailing custom properties for the execution of each extension.","title":"Service Security Setting"},{"content":" Service to manage registration of services in the TSB Service Registry.\nRegistration The service registration API allows to manage the services that exist in the catalog. It exposes methods to register and unregister individual services as well as methods to keep all the services in a given cluster in sync.\nListServices rpc ListServices (tetrateio.api.tsb.registry.v2.ListServicesRequest) returns (tetrateio.api.tsb.registry.v2.ListServicesResponse)\nList the services that have been registered in …","relpermalink":"/tsb/refs/tsb/registry/v2/registration-service/","summary":"Service to manage registration of services in the TSB Service Registry.\nRegistration The service registration API allows to manage the services that exist in the catalog. It exposes methods to register and unregister individual services as well as methods to keep all the services in a given cluster in sync.\nListServices rpc ListServices (tetrateio.api.tsb.registry.v2.ListServicesRequest) returns (tetrateio.api.tsb.registry.v2.ListServicesResponse)\nList the services that have been registered in an organization\nGetService rpc GetService (tetrateio.api.tsb.registry.v2.GetServiceRequest) returns (tetrateio.api.tsb.registry.v2.Service)\nRequires ReadRegisteredService\nGet the details of a registered service\nGetServiceRequest Request to retrieve a registered service.\nField Description Validation Rule fqn\nstring REQUIRED Fully-qualified name of the registered service.","title":"Servive Registry Registration Service"},{"content":" Source describes a set of observed resources that have a group of metrics that emit measurements at runtime. A source specifies what is being observed (which resource types: service, ingress hostnames, relation, …) and how it is being observed (with which scope of observation).\nA telemetry source can observe different types of resources in a single or aggregated way depending on the defined scope. A scope can be of type ServiceScope, IngressScope, or RelationScope, and they define the wingspan …","relpermalink":"/tsb/refs/tsb/observability/telemetry/v2/source/","summary":"Source describes a set of observed resources that have a group of metrics that emit measurements at runtime. A source specifies what is being observed (which resource types: service, ingress hostnames, relation, …) and how it is being observed (with which scope of observation).\nA telemetry source can observe different types of resources in a single or aggregated way depending on the defined scope. A scope can be of type ServiceScope, IngressScope, or RelationScope, and they define the wingspan of the telemetry source in the mesh. Each scope contains information to determine if it is a single standalone source or an aggregation of standalone sources of the same type.","title":"Source"},{"content":" Each resource in TSB is able to provide a status to let the user know it’s current integrity. Some resources, like configurations for ingress, traffic and security, are not immediately applied as soon as TSB accepts any modification from user. In these cases, the status will provide enough information to know when it is really applying to the affected workloads. This allows any user or CI/CD process to poll the status of any desired resource and proceed accordingly.\nThere are two types of …","relpermalink":"/tsb/refs/tsb/v2/status/","summary":"Each resource in TSB is able to provide a status to let the user know it’s current integrity. Some resources, like configurations for ingress, traffic and security, are not immediately applied as soon as TSB accepts any modification from user. In these cases, the status will provide enough information to know when it is really applying to the affected workloads. This allows any user or CI/CD process to poll the status of any desired resource and proceed accordingly.\nThere are two types of resources, the ones that aggregate the status of children resources and the ones that do not. Check the documentation for the different details object types for further information.","title":"Status"},{"content":" Service to retrieve the status for TSB resources\nStatus The Status services exposes methods to retrieve the status for any resource managed by TSB.\nGetStatus rpc GetStatus (tetrateio.api.tsb.v2.GetStatusRequest) returns (tetrateio.api.tsb.v2.ResourceStatus)\nGiven a resource fully-qualified name of a resource returns its current status.\nGetStatusRequest Request to retrieve the status of a resource.\nField Description Validation Rule fqn\nstring REQUIRED Fully-qualified name of the resource to …","relpermalink":"/tsb/refs/tsb/v2/status-service/","summary":"Service to retrieve the status for TSB resources\nStatus The Status services exposes methods to retrieve the status for any resource managed by TSB.\nGetStatus rpc GetStatus (tetrateio.api.tsb.v2.GetStatusRequest) returns (tetrateio.api.tsb.v2.ResourceStatus)\nGiven a resource fully-qualified name of a resource returns its current status.\nGetStatusRequest Request to retrieve the status of a resource.\nField Description Validation Rule fqn\nstring REQUIRED Fully-qualified name of the resource to retrieve the status.\nstring = { min_len: 1}","title":"Status Service"},{"content":"tctl\nSynopsis\nTetrate Service Bridge CLI\nOptions\n-c, --config string Path to the config file to use. Can also be specified via TCTL_CONFIG env variable. This flag takes precedence over the env variable. -p, --profile string Use specific profile (default \u0026#34;default\u0026#34;) --debug Print debug messages for all requests and responses --disable-tctl-version-warn If set, disable the outdated tctl version warning. Can also be specified via TCTL_DISABLE_VERSION_WARN env variable. -h, --help help for tctl ","relpermalink":"/tsb/reference/cli/reference/tctl/","summary":"tctl\nSynopsis\nTetrate Service Bridge CLI\nOptions\n-c, --config string Path to the config file to use. Can also be specified via TCTL_CONFIG env variable. This flag takes precedence over the env variable. -p, --profile string Use specific profile (default \"default\") --debug Print debug messages for all requests and responses --disable-tctl-version-warn If set, disable the outdated tctl version warning. Can also be specified via TCTL_DISABLE_VERSION_WARN env variable. -h, --help help for tctl ","title":"tctl"},{"content":"Apply a configuration to a resource by filename or stdin\ntctl apply [flags] Examples\ntctl apply -f config.yaml Options\n-f, --file string File or directory containing configuration to apply [required] -h, --help help for apply -o, --output-type string Response output type: table, yaml, json Options inherited from parent commands\n-c, --config string Path to the config file to use. Can also be specified via TCTL_CONFIG env variable. This flag takes precedence over the env variable. --debug Print …","relpermalink":"/tsb/reference/cli/reference/apply/","summary":"Apply a configuration to a resource by filename or stdin\ntctl apply [flags] Examples\ntctl apply -f config.yaml Options\n-f, --file string File or directory containing configuration to apply [required] -h, --help help for apply -o, --output-type string Response output type: table, yaml, json Options inherited from parent commands\n-c, --config string Path to the config file to use. Can also be specified via TCTL_CONFIG env variable. This flag takes precedence over the env variable. --debug Print debug messages for all requests and responses --disable-tctl-version-warn If set, disable the outdated tctl version warning. Can also be specified via TCTL_DISABLE_VERSION_WARN env variable.","title":"tctl apply"},{"content":"Collect the state of a Kubernetes cluster for debugging.\ntctl collect [flags] Examples\n# Collect without any obfuscation or redaction tctl collect # Collect without archiving results (useful for local debugging) tctl collect --disable-archive # Collect and redact with user-provided regex tctl collect --redact-regexes \u0026lt;regex-one\u0026gt;,\u0026lt;regex-two\u0026gt; # Collect and redact with presets tctl collect --redact-presets networking Options\n--disable-archive output files rather than tarball -h, --help help for …","relpermalink":"/tsb/reference/cli/reference/collect/","summary":"Collect the state of a Kubernetes cluster for debugging.\ntctl collect [flags] Examples\n# Collect without any obfuscation or redaction tctl collect # Collect without archiving results (useful for local debugging) tctl collect --disable-archive # Collect and redact with user-provided regex tctl collect --redact-regexes \u003cregex-one\u003e,\u003cregex-two\u003e # Collect and redact with presets tctl collect --redact-presets networking Options\n--disable-archive output files rather than tarball -h, --help help for collect -o, --output-directory string the path to write the collected files under (default \"tctl-[timestamp]\") --redact-presets strings Comma-separated list of redaction presets to use in collection data obfuscation. Available presets: - \"networking\": Obfuscate any data that matches IPv4 or IPv6 addresses.","title":"tctl collect"},{"content":"Generates tab completion scripts\ntctl completion \u0026lt;bash|zsh|fish|powershell\u0026gt; Examples\nBash: $ source \u0026lt;(tctl completion bash) # To load completions for each session, execute once: Linux: $ tctl completion bash | sudo tee -a /etc/bash_completion.d/tctl \u0026gt; /dev/null MacOS: $ tctl completion bash | sudo tee -a $(brew --prefix)/etc/bash_completion.d/tctl \u0026gt; /dev/null Zsh: # If shell completion is not already enabled in your environment you will need # to enable it. You can execute the following once: $ …","relpermalink":"/tsb/reference/cli/reference/completion/","summary":"Generates tab completion scripts\ntctl completion \u003cbash|zsh|fish|powershell\u003e Examples\nBash: $ source \u003c(tctl completion bash) # To load completions for each session, execute once: Linux: $ tctl completion bash | sudo tee -a /etc/bash_completion.d/tctl \u003e /dev/null MacOS: $ tctl completion bash | sudo tee -a $(brew --prefix)/etc/bash_completion.d/tctl \u003e /dev/null Zsh: # If shell completion is not already enabled in your environment you will need # to enable it. You can execute the following once: $ echo \"autoload -U compinit; compinit\" \u003e\u003e ~/.zshrc # To load completions for each session, execute once: $ tctl completion zsh \u003e \"${fpath[1]}/_tctl\" # You will need to start a new shell for this setup to take effect.","title":"tctl completion"},{"content":"Manages CLI configuration.\nOptions\n-h, --help help for config Options inherited from parent commands\n-c, --config string Path to the config file to use. Can also be specified via TCTL_CONFIG env variable. This flag takes precedence over the env variable. --debug Print debug messages for all requests and responses --disable-tctl-version-warn If set, disable the outdated tctl version warning. Can also be specified via TCTL_DISABLE_VERSION_WARN env variable. -p, --profile string Use specific …","relpermalink":"/tsb/reference/cli/reference/config/","summary":"Manages CLI configuration.\nOptions\n-h, --help help for config Options inherited from parent commands\n-c, --config string Path to the config file to use. Can also be specified via TCTL_CONFIG env variable. This flag takes precedence over the env variable. --debug Print debug messages for all requests and responses --disable-tctl-version-warn If set, disable the outdated tctl version warning. Can also be specified via TCTL_DISABLE_VERSION_WARN env variable. -p, --profile string Use specific profile (default \"default\") tctl config clusters Manages configuration of clusters\nOptions\n-h, --help help for clusters Options inherited from parent commands\n-c, --config string Path to the config file to use.","title":"tctl config"},{"content":"Delete an object\ntctl delete [\u0026lt;apiVersion/kind\u0026gt; \u0026lt;name\u0026gt;] [flags] Examples\n# Delete a cluster using the apiVersion/Kind pattern tctl delete api.tsb.tetrate.io/v2/Cluster my-cluster # Delete a single workspace using the short form tctl delete ws my-workspace These are the available short forms: aab\tApplicationAccessBindings ab\tAccessBindings ap\tAuthorizationPolicy apiab\tAPIAccessBindings app\tApplication cs\tCluster dr\tDestinationRule ef\tEnvoyFilter eg\tEgressGateway gab\tGatewayAccessBindings gg …","relpermalink":"/tsb/reference/cli/reference/delete/","summary":"Delete an object\ntctl delete [\u003capiVersion/kind\u003e \u003cname\u003e] [flags] Examples\n# Delete a cluster using the apiVersion/Kind pattern tctl delete api.tsb.tetrate.io/v2/Cluster my-cluster # Delete a single workspace using the short form tctl delete ws my-workspace These are the available short forms: aab\tApplicationAccessBindings ab\tAccessBindings ap\tAuthorizationPolicy apiab\tAPIAccessBindings app\tApplication cs\tCluster dr\tDestinationRule ef\tEnvoyFilter eg\tEgressGateway gab\tGatewayAccessBindings gg\tGatewayGroup gw\tnetworking.istio.io/v1beta1/Gateway gwt\tgateway.tsb.tetrate.io/v2/Gateway iab\tIstioInternalAccessBindings ig\tIngressGateway iig\tIstioInternalGroup oab\tOrganizationAccessBindings org\tOrganization os\tOrganizationSetting otm\tMetric ots\tSource pa\tPeerAuthentication ra\tRequestAuthentication sa\tServiceAccount sab\tSecurityAccessBindings sd\tSidecar se\tServiceEntry sg\tSecurityGroup sr\tServiceRoute ss\tSecuritySetting sss\tServiceSecuritySetting svc\tService t1\tTier1Gateway tab\tTrafficAccessBindings tg\tTrafficGroup tnab\tTenantAccessBindings tns\tTenantSetting ts\tTrafficSetting vs\tVirtualService wab\tWorkspaceAccessBindings wext\tWasmExtension wp\tWasmPlugin ws\tWorkspace wss\tWorkspaceSetting For API version and kind, please refer to: https://docs.","title":"tctl delete"},{"content":"Edit one or multiple objects\ntctl edit \u0026lt;apiVersion/kind | kind | shortform\u0026gt; [\u0026lt;name\u0026gt;] [flags] Examples\nEdit will perform a get on the given object and launch $EDITOR (environment variable needs to be set) for editing it then apply the changes back. # Edit a workspace. tctl edit workspace foo # Edit a tenant tctl edit tenant my-department # Edit an IngressGateway tctl edit ingressgateway myIng --workspace foo --gatewaygroup bar # You can also edit lists of objects # Edit multiple gateway groups at …","relpermalink":"/tsb/reference/cli/reference/edit/","summary":"Edit one or multiple objects\ntctl edit \u003capiVersion/kind | kind | shortform\u003e [\u003cname\u003e] [flags] Examples\nEdit will perform a get on the given object and launch $EDITOR (environment variable needs to be set) for editing it then apply the changes back. # Edit a workspace. tctl edit workspace foo # Edit a tenant tctl edit tenant my-department # Edit an IngressGateway tctl edit ingressgateway myIng --workspace foo --gatewaygroup bar # You can also edit lists of objects # Edit multiple gateway groups at once tctl edit gatewaygroup --workspace foo --gatewaygroup bar baz # Or even all workspaces at once tctl edit workspace --tenant foo These are the available short forms: aab\tApplicationAccessBindings ab\tAccessBindings ap\tAuthorizationPolicy apiab\tAPIAccessBindings app\tApplication cs\tCluster dr\tDestinationRule ef\tEnvoyFilter eg\tEgressGateway gab\tGatewayAccessBindings gg\tGatewayGroup gw\tnetworking.","title":"tctl edit"},{"content":"Experimental commands that may be modified or deprecated\nOptions\n-h, --help help for experimental Options inherited from parent commands\n-c, --config string Path to the config file to use. Can also be specified via TCTL_CONFIG env variable. This flag takes precedence over the env variable. --debug Print debug messages for all requests and responses --disable-tctl-version-warn If set, disable the outdated tctl version warning. Can also be specified via TCTL_DISABLE_VERSION_WARN env variable. -p, …","relpermalink":"/tsb/reference/cli/reference/experimental/","summary":"Experimental commands that may be modified or deprecated\nOptions\n-h, --help help for experimental Options inherited from parent commands\n-c, --config string Path to the config file to use. Can also be specified via TCTL_CONFIG env variable. This flag takes precedence over the env variable. --debug Print debug messages for all requests and responses --disable-tctl-version-warn If set, disable the outdated tctl version warning. Can also be specified via TCTL_DISABLE_VERSION_WARN env variable. -p, --profile string Use specific profile (default \"default\") tctl experimental app-ingress Run a Istio based Ingress Controller for your application\nSynopsis\nInstall a dedicated Ingress Controller in your environment to allow incoming/ingress traffic to be routed to your application.","title":"tctl experimental"},{"content":"Get one or multiple objects\ntctl get \u0026lt;apiVersion/kind | kind | shortform\u0026gt; [\u0026lt;name\u0026gt;] [flags] Examples\n# List tenants using the apiVersion/Kind pattern tctl get api.tsb.tetrate.io/v2/Tenant # List workspaces using the kind tctl get workspace # Get a single workspace using the short form tctl get ws my-workspace # List gateway groups of a workspace tctl get --workspace my-workspace GatewayGroup # Get the access bindings of an ingress gateway tctl get accessbindings …","relpermalink":"/tsb/reference/cli/reference/get/","summary":"Get one or multiple objects\ntctl get \u003capiVersion/kind | kind | shortform\u003e [\u003cname\u003e] [flags] Examples\n# List tenants using the apiVersion/Kind pattern tctl get api.tsb.tetrate.io/v2/Tenant # List workspaces using the kind tctl get workspace # Get a single workspace using the short form tctl get ws my-workspace # List gateway groups of a workspace tctl get --workspace my-workspace GatewayGroup # Get the access bindings of an ingress gateway tctl get accessbindings organizations/foo/tenants/foo/workspaces/foo/gatewaygroups/foo/ingressgateways/foo # Get all resources within a tenant tctl get all --tenant foo # Get all resources within a workspace tctl get all --tenant foo --workspace bar # Get all resources within a given group tctl get all --tenant foo --workspace bar --gatewaygroup baz # Get all resources within a tenant, referrencing a given FQDN tctl get all --tenant foo --fqdn some.","title":"tctl get"},{"content":"Generates install manifests and applies it to a cluster\nOptions\n-h, --help help for install Options inherited from parent commands\n-c, --config string Path to the config file to use. Can also be specified via TCTL_CONFIG env variable. This flag takes precedence over the env variable. --debug Print debug messages for all requests and responses --disable-tctl-version-warn If set, disable the outdated tctl version warning. Can also be specified via TCTL_DISABLE_VERSION_WARN env variable. -p, …","relpermalink":"/tsb/reference/cli/reference/install/","summary":"Generates install manifests and applies it to a cluster\nOptions\n-h, --help help for install Options inherited from parent commands\n-c, --config string Path to the config file to use. Can also be specified via TCTL_CONFIG env variable. This flag takes precedence over the env variable. --debug Print debug messages for all requests and responses --disable-tctl-version-warn If set, disable the outdated tctl version warning. Can also be specified via TCTL_DISABLE_VERSION_WARN env variable. -p, --profile string Use specific profile (default \"default\") tctl install cluster-certs Generate cluster certs for securely communicating with the management plane\ntctl install cluster-certs [flags] Examples\n# Retrieve cluster certs tctl install cluster-certs --cluster \u003ccluster-name\u003e\" Options","title":"tctl install"},{"content":"Configures the credentials for the given user\nSynopsis\nConfigures the credentials for the given user.\nThis command will exchange the given credentials for an access token that can be stored in the configuration profile. If the credentials are not provided as arguments to the login command, an interactive prompt will ask for all required information.\nOrganization and Tenant can also be configured with the following environment variables:\nTCTL_LOGIN_ORG TCTL_LOGIN_TENANT Both password based and …","relpermalink":"/tsb/reference/cli/reference/login/","summary":"Configures the credentials for the given user\nSynopsis\nConfigures the credentials for the given user.\nThis command will exchange the given credentials for an access token that can be stored in the configuration profile. If the credentials are not provided as arguments to the login command, an interactive prompt will ask for all required information.\nOrganization and Tenant can also be configured with the following environment variables:\nTCTL_LOGIN_ORG TCTL_LOGIN_TENANT Both password based and OpenID Connect based authentication are supported. Depending on the configured authentication server there are several different authentication flags.\nWhen password based authentication is configured, both –username and –password are required flags.","title":"tctl login"},{"content":"Opens the TSB console in the browser\ntctl ui [flags] Examples\ntctl ui Options\n-h, --help help for ui Options inherited from parent commands\n-c, --config string Path to the config file to use. Can also be specified via TCTL_CONFIG env variable. This flag takes precedence over the env variable. --debug Print debug messages for all requests and responses --disable-tctl-version-warn If set, disable the outdated tctl version warning. Can also be specified via TCTL_DISABLE_VERSION_WARN env variable. …","relpermalink":"/tsb/reference/cli/reference/ui/","summary":"Opens the TSB console in the browser\ntctl ui [flags] Examples\ntctl ui Options\n-h, --help help for ui Options inherited from parent commands\n-c, --config string Path to the config file to use. Can also be specified via TCTL_CONFIG env variable. This flag takes precedence over the env variable. --debug Print debug messages for all requests and responses --disable-tctl-version-warn If set, disable the outdated tctl version warning. Can also be specified via TCTL_DISABLE_VERSION_WARN env variable. -p, --profile string Use specific profile (default \"default\") ","title":"tctl ui"},{"content":"Offline validates a configuration from filename or stdin\ntctl validate [flags] Examples\ntctl validate -f config.yaml This syntactically validates the provided config and verifies the config of the defined selectors. Options\n-f, --file string File or directory containing configuration to validate [required] -h, --help help for validate Options inherited from parent commands\n-c, --config string Path to the config file to use. Can also be specified via TCTL_CONFIG env variable. This flag takes …","relpermalink":"/tsb/reference/cli/reference/validate/","summary":"Offline validates a configuration from filename or stdin\ntctl validate [flags] Examples\ntctl validate -f config.yaml This syntactically validates the provided config and verifies the config of the defined selectors. Options\n-f, --file string File or directory containing configuration to validate [required] -h, --help help for validate Options inherited from parent commands\n-c, --config string Path to the config file to use. Can also be specified via TCTL_CONFIG env variable. This flag takes precedence over the env variable. --debug Print debug messages for all requests and responses --disable-tctl-version-warn If set, disable the outdated tctl version warning. Can also be specified via TCTL_DISABLE_VERSION_WARN env variable.","title":"tctl validate"},{"content":"Show the version of tctl and TSB\ntctl version [flags] Options\n--ascii Display the ASCII art for the TSB release -h, --help help for version --local-only If true, shows client version only (no server required) Options inherited from parent commands\n-c, --config string Path to the config file to use. Can also be specified via TCTL_CONFIG env variable. This flag takes precedence over the env variable. --debug Print debug messages for all requests and responses --disable-tctl-version-warn If set, …","relpermalink":"/tsb/reference/cli/reference/version/","summary":"Show the version of tctl and TSB\ntctl version [flags] Options\n--ascii Display the ASCII art for the TSB release -h, --help help for version --local-only If true, shows client version only (no server required) Options inherited from parent commands\n-c, --config string Path to the config file to use. Can also be specified via TCTL_CONFIG env variable. This flag takes precedence over the env variable. --debug Print debug messages for all requests and responses --disable-tctl-version-warn If set, disable the outdated tctl version warning. Can also be specified via TCTL_DISABLE_VERSION_WARN env variable. -p, --profile string Use specific profile (default \"default\") ","title":"tctl version"},{"content":"Show the current user info\ntctl whoami [flags] Options\n-h, --help help for whoami -o, --output-type string Response output type: table, yaml, json (default \u0026#34;table\u0026#34;) Options inherited from parent commands\n-c, --config string Path to the config file to use. Can also be specified via TCTL_CONFIG env variable. This flag takes precedence over the env variable. --debug Print debug messages for all requests and responses --disable-tctl-version-warn If set, disable the outdated tctl version warning. Can …","relpermalink":"/tsb/reference/cli/reference/whoami/","summary":"Show the current user info\ntctl whoami [flags] Options\n-h, --help help for whoami -o, --output-type string Response output type: table, yaml, json (default \"table\") Options inherited from parent commands\n-c, --config string Path to the config file to use. Can also be specified via TCTL_CONFIG env variable. This flag takes precedence over the env variable. --debug Print debug messages for all requests and responses --disable-tctl-version-warn If set, disable the outdated tctl version warning. Can also be specified via TCTL_DISABLE_VERSION_WARN env variable. -p, --profile string Use specific profile (default \"default\") ","title":"tctl whoami"},{"content":" User represents a user that has been loaded from a configured Identity Provider (IdP) that can log into the platform. Currently, users are automatically synchronized by TSB from a configured LDAP server.\nThe following example creates a user named john under the organization myorg.\napiVersion: api.tsb.tetrate.io/v2 kind: User metadata: name: john organization: myorg spec: loginName: john firstName: John lastName: Doe displayName: John Doe email: john.doe@acme.com ServiceAccount can be created to …","relpermalink":"/tsb/refs/tsb/v2/team/","summary":"User represents a user that has been loaded from a configured Identity Provider (IdP) that can log into the platform. Currently, users are automatically synchronized by TSB from a configured LDAP server.\nThe following example creates a user named john under the organization myorg.\napiVersion: api.tsb.tetrate.io/v2 kind: User metadata: name: john organization: myorg spec: loginName: john firstName: John lastName: Doe displayName: John Doe email: john.doe@acme.com ServiceAccount can be created to leverage machine authentication via JWT tokens. Each service account has a key-pair that can be used to create signed JWT tokens that can be used to authenticate to TSB.","title":"Teams and Users"},{"content":" Service to manage Users and Teams in TSB\nTeams The Teams service provides methods to manage the Users and Teams that exist in an Organization.\nUsers and Teams are periodically synchronized from the Identity Provider (IdP) configured for the Organization, but TSB allows creating local teams to provide extended flexibility in how Users and Teams are grouped, and to provide a comprehensive way of creating more fine-grained access control policies.\nGetUser rpc GetUser …","relpermalink":"/tsb/refs/tsb/v2/team-service/","summary":"Service to manage Users and Teams in TSB\nTeams The Teams service provides methods to manage the Users and Teams that exist in an Organization.\nUsers and Teams are periodically synchronized from the Identity Provider (IdP) configured for the Organization, but TSB allows creating local teams to provide extended flexibility in how Users and Teams are grouped, and to provide a comprehensive way of creating more fine-grained access control policies.\nGetUser rpc GetUser (tetrateio.api.tsb.v2.GetUserRequest) returns (tetrateio.api.tsb.v2.User)\nRequires READ\nGet the details of an existing user.\nListUsers rpc ListUsers (tetrateio.api.tsb.v2.ListUsersRequest) returns (tetrateio.api.tsb.v2.ListUsersResponse)\nList existing users.\nGenerateTokens rpc GenerateTokens (tetrateio.api.tsb.v2.GenerateTokensRequest) returns (tetrateio.","title":"Teams Service"},{"content":" Service to manage the Telemetry Sources.\nSources The Sources service exposes methods to manage telemetry sources from resources.\nGetSource rpc GetSource (tetrateio.api.tsb.observability.telemetry.v2.GetSourceRequest) returns (tetrateio.api.tsb.observability.telemetry.v2.Source)\nGet the details of an existing telemetry source.\nListSources rpc ListSources (tetrateio.api.tsb.observability.telemetry.v2.ListSourcesRequest) returns (tetrateio.api.tsb.observability.telemetry.v2.ListSourcesResponse) …","relpermalink":"/tsb/refs/tsb/observability/telemetry/v2/source-service/","summary":"Service to manage the Telemetry Sources.\nSources The Sources service exposes methods to manage telemetry sources from resources.\nGetSource rpc GetSource (tetrateio.api.tsb.observability.telemetry.v2.GetSourceRequest) returns (tetrateio.api.tsb.observability.telemetry.v2.Source)\nGet the details of an existing telemetry source.\nListSources rpc ListSources (tetrateio.api.tsb.observability.telemetry.v2.ListSourcesRequest) returns (tetrateio.api.tsb.observability.telemetry.v2.ListSourcesResponse)\nList the telemetry sources that are available for the requested parent. It will return telemetry sources that belong to the requested parent and from all its child resources.\nGetSourceRequest Request to retrieve a Telemetry Sources from a parent resource.\nField Description Validation Rule fqn\nstring REQUIRED Fully-qualified name of the Telemetry Sources.\nTODO(marcnavarro): Add pagination information.\nstring = { min_len: 1}","title":"Telemetry Source Service"},{"content":" Tenant is a self-contained entity within an organization in the Service Bridge object hierarchy. Tenants can be business units, organization units, or any logical grouping that matches a corporate structure.\nThe following example creates a tenant named mycompany in an organization named myorg.\napiVersion: api.tsb.tetrate.io/v2 kind: Tenant metadata: organization: myorg name: mycompany Tenant Tenant is a self-contained entity within an organization in the Service Bridge hierarchy.\nField …","relpermalink":"/tsb/refs/tsb/v2/tenant/","summary":"Tenant is a self-contained entity within an organization in the Service Bridge object hierarchy. Tenants can be business units, organization units, or any logical grouping that matches a corporate structure.\nThe following example creates a tenant named mycompany in an organization named myorg.\napiVersion: api.tsb.tetrate.io/v2 kind: Tenant metadata: organization: myorg name: mycompany Tenant Tenant is a self-contained entity within an organization in the Service Bridge hierarchy.\nField Description Validation Rule securityDomain\nstring Security domains can be used to group different resources under the same security domain. Although security domain is not resource itself currently, it follows a fqn format organizations/myorg/securitydomains/mysecuritydomain, and a child cannot override any ancestor’s security domain.","title":"Tenant"},{"content":" DEPRECATED: use Access Bindings instead.\nTenantAccessBindings is an assignment of roles to a set of users or teams to access resources under a Tenant. The user or team information is obtained from an LDAP server that should have been configured as part of Service Bridge installation. Note that a TenantAccessBinding can be created or modified only by users who have SET_POLICY permission on the Tenant.\nThe following example assigns the tenant-admin role to users alice, bob, and members of the t1 …","relpermalink":"/tsb/refs/tsb/rbac/v2/tenant-access-bindings/","summary":"DEPRECATED: use Access Bindings instead.\nTenantAccessBindings is an assignment of roles to a set of users or teams to access resources under a Tenant. The user or team information is obtained from an LDAP server that should have been configured as part of Service Bridge installation. Note that a TenantAccessBinding can be created or modified only by users who have SET_POLICY permission on the Tenant.\nThe following example assigns the tenant-admin role to users alice, bob, and members of the t1 team owned by the tenant mycompany. Use fully-qualified name (fqn) when specifying user and team\napiVersion: rbac.tsb.tetrate.io/v2 kind: TenantAccessBindings metadata: organization: myorg tenant: mycompany spec: allow: - role: rbac/tenant-admin subjects: - user: organization/myorg/users/alice - user: organization/myorg/users/bob - team: organization/myorg/teams/t1 TenantAccessBindings TenantAccessBindings assigns permissions to users of tenants.","title":"Tenant Access Bindings"},{"content":" Service to manage TSB tenants.\nTenants The Tenant service can be used to manage the tenants in TSB. Tenants can be seen as organization units and line of business that have a set of resources. Every resource in TSB belongs to a tenant, and users can be assigned to tenants to get access to those resources (such as workspaces, traffic settings, etc). This service provides methods to manage the tenants that are available in the platform.\nCreateTenant rpc CreateTenant …","relpermalink":"/tsb/refs/tsb/v2/tenant-service/","summary":"Service to manage TSB tenants.\nTenants The Tenant service can be used to manage the tenants in TSB. Tenants can be seen as organization units and line of business that have a set of resources. Every resource in TSB belongs to a tenant, and users can be assigned to tenants to get access to those resources (such as workspaces, traffic settings, etc). This service provides methods to manage the tenants that are available in the platform.\nCreateTenant rpc CreateTenant (tetrateio.api.tsb.v2.CreateTenantRequest) returns (tetrateio.api.tsb.v2.Tenant)\nRequires CREATE\nCreate a new tenant in the platform that will be the home for a set of resources.","title":"Tenant Service"},{"content":" Tenant Setting allows configuring default settings for the tenant.\nTraffic and Security settings can be defined as default for a tenant, meaning that they will be applied to all the workspaces of the tenant. These defaults settings can be overridden by creating proper WorkspaceSetting, TrafficSetting or SecuritySetting into the desired workspace or group.\napiVersion: api.tsb.tetrate.io/v2 kind: TenantSetting metadata: name: tenant-settings organization: myorg tenant: mytenant spec: …","relpermalink":"/tsb/refs/tsb/v2/tenant-setting/","summary":"Tenant Setting allows configuring default settings for the tenant.\nTraffic and Security settings can be defined as default for a tenant, meaning that they will be applied to all the workspaces of the tenant. These defaults settings can be overridden by creating proper WorkspaceSetting, TrafficSetting or SecuritySetting into the desired workspace or group.\napiVersion: api.tsb.tetrate.io/v2 kind: TenantSetting metadata: name: tenant-settings organization: myorg tenant: mytenant spec: defaultTrafficSetting: reachability: mode: WORKSPACE egress: host: bookinfo-perimeter/tsb-egress defaultSecuritySetting: authenticationSettings: trafficMode: REQUIRED authorization: mode: GROUP TenantSetting Default settings that apply to all workspaces under a tenant.\nField Description Validation Rule defaultSecuritySetting\ntetrateio.api.tsb.security.v2.SecuritySetting Security settings for all proxy workloads in this tenant.","title":"Tenant Setting"},{"content":" DEPRECATION: The functionality provided by the Tier1Gateway is now provided in Gateway object, and using it is the recommended approach. The Tier1Gateway resource will be removed in future releases.\nTier1Gateway configures a workload to act as a gateway that distributes traffic across one or more ingress gateways in other clusters.\nNOTE: Tier1 gateways cannot be used to route traffic to the same cluster. A cluster with tier1 gateway cannot have any other gateways or workloads.\nThe following …","relpermalink":"/tsb/refs/tsb/gateway/v2/tier1-gateway/","summary":"DEPRECATION: The functionality provided by the Tier1Gateway is now provided in Gateway object, and using it is the recommended approach. The Tier1Gateway resource will be removed in future releases.\nTier1Gateway configures a workload to act as a gateway that distributes traffic across one or more ingress gateways in other clusters.\nNOTE: Tier1 gateways cannot be used to route traffic to the same cluster. A cluster with tier1 gateway cannot have any other gateways or workloads.\nThe following example declares a tier1 gateway running on pods with app: gateway labels in the ns1 namespace. The gateway exposes host movieinfo.com on ports 8080, 8443 and kafka.","title":"Tier1 Gateway"},{"content":" DEPRECATED: use Access Bindings instead.\nTrafficAccessBindings is an assignment of roles to a set of users or teams to access resources under a Traffic group. The user or team information is obtained from an LDAP server that should have been configured as part of Service Bridge installation. Note that a TrafficAccessBinding can be created or modified only by users who have SET_POLICY permission on the Traffic group.\nThe following example assigns the traffic-admin role to users alice, bob, and …","relpermalink":"/tsb/refs/tsb/rbac/v2/traffic-access-bindings/","summary":"DEPRECATED: use Access Bindings instead.\nTrafficAccessBindings is an assignment of roles to a set of users or teams to access resources under a Traffic group. The user or team information is obtained from an LDAP server that should have been configured as part of Service Bridge installation. Note that a TrafficAccessBinding can be created or modified only by users who have SET_POLICY permission on the Traffic group.\nThe following example assigns the traffic-admin role to users alice, bob, and members of the traffic-ops team for traffic group g1 under workspace w1 owned by the tenant mycompany. Use fully-qualified name (fqn) when specifying user and team","title":"Traffic Access Bindings"},{"content":" Traffic Groups allow grouping the proxy workloads in a set of namespaces owned by its parent workspace. Networking and routing related configurations can then be applied on the group to control the behavior of these proxy workloads. The group can be in one of two modes: BRIDGED and DIRECT. BRIDGED mode is a minimalistic mode that allows users to quickly configure the most commonly used features in the service mesh using Tetrate specific APIs, while the DIRECT mode provides more flexibility for …","relpermalink":"/tsb/refs/tsb/traffic/v2/traffic-group/","summary":"Traffic Groups allow grouping the proxy workloads in a set of namespaces owned by its parent workspace. Networking and routing related configurations can then be applied on the group to control the behavior of these proxy workloads. The group can be in one of two modes: BRIDGED and DIRECT. BRIDGED mode is a minimalistic mode that allows users to quickly configure the most commonly used features in the service mesh using Tetrate specific APIs, while the DIRECT mode provides more flexibility for power users by allowing them to configure the proxy workload behavior using a restricted subset of Istio Networking APIs.","title":"Traffic Group"},{"content":" Service to manage traffic settings.\nTraffic The Traffic service provides methods to manage traffic settings in TSB.\nIt provides methods to create and manage traffic groups within a workspace, allowing to create fine-grained groupings to configure a subset of the workspace namespaces. Access policies can be assigned at group level, providing a fine-grained access control to the traffic configuration features.\nThe Traffic service also provides methods to configure the different traffic settings …","relpermalink":"/tsb/refs/tsb/traffic/v2/traffic-service/","summary":"Service to manage traffic settings.\nTraffic The Traffic service provides methods to manage traffic settings in TSB.\nIt provides methods to create and manage traffic groups within a workspace, allowing to create fine-grained groupings to configure a subset of the workspace namespaces. Access policies can be assigned at group level, providing a fine-grained access control to the traffic configuration features.\nThe Traffic service also provides methods to configure the different traffic settings that are allowed within each group.\nCreateGroup rpc CreateGroup (tetrateio.api.tsb.traffic.v2.CreateTrafficGroupRequest) returns (tetrateio.api.tsb.traffic.v2.Group)\nRequires CREATE\nCreate a new traffic group in the given workspace.\nGroups will by default configure all the namespaces owned by their workspace, unless explicitly configured.","title":"Traffic Service"},{"content":" Traffic Settings allow configuring the behavior of the proxy workloads in a set of namespaces owned by a traffic group. Specifically, it allows configuring the dependencies of proxy workloads on namespaces outside the traffic group as well as reliability settings for outbound calls made by the proxy workloads to other services.\nThe following example creates a traffic group for the proxy workloads in ns1, ns2 and ns3 namespaces owned by its parent workspace w1 under tenant mycompany. It then …","relpermalink":"/tsb/refs/tsb/traffic/v2/traffic-setting/","summary":"Traffic Settings allow configuring the behavior of the proxy workloads in a set of namespaces owned by a traffic group. Specifically, it allows configuring the dependencies of proxy workloads on namespaces outside the traffic group as well as reliability settings for outbound calls made by the proxy workloads to other services.\nThe following example creates a traffic group for the proxy workloads in ns1, ns2 and ns3 namespaces owned by its parent workspace w1 under tenant mycompany. It then defines a traffic setting for the all workloads in these namespaces, adding a dependency on all the services in the shared db namespace, and forwarding all unknown traffic via the egress gateway in the istio-system namespace.","title":"Traffic Setting"},{"content":" Transport layer security config specifies configuration of a TLS client.\nClientTransportSecurity ClientTransportSecurity specifies transport layer security configuration.\nField Description Validation Rule tls\ntetrateio.api.onboarding.config.types.config.v1alpha1.TlsClient oneof kind TLS client configuration.\n–\nnone\ntetrateio.api.onboarding.config.types.config.v1alpha1.PlainTextClient oneof kind Plain-text client configuration.\n–\nTlsClient TlsClient specifies configuration of a TLS client.\nField …","relpermalink":"/tsb/refs/onboarding/config/types/config/v1alpha1/transport-security/","summary":"Transport layer security config specifies configuration of a TLS client.\nClientTransportSecurity ClientTransportSecurity specifies transport layer security configuration.\nField Description Validation Rule tls\ntetrateio.api.onboarding.config.types.config.v1alpha1.TlsClient oneof kind TLS client configuration.\n–\nnone\ntetrateio.api.onboarding.config.types.config.v1alpha1.PlainTextClient oneof kind Plain-text client configuration.\n–\nTlsClient TlsClient specifies configuration of a TLS client.\nField Description Validation Rule sni\nstring SNI string to present to the server during TLS handshake instead of the default value (host address).\nDefaults to empty string, in which case the default SNI value (host address) will be used.\nThis setting is meant for use in non-production scenarios, such as:\nwhen the server is not reachable by a DNS name (e.","title":"Transport layer security config"},{"content":"如何确定 Envoy 是否正常？ 确定 Envoy 是否正常的最佳方法是检查其健康和就绪端点（healthz）。要检查已加入的集群中应用程序的 Envoy 的 healthz 端点，你需要直接连接到应用程序的旁路 Envoy 边车。\n假设你在集群的 bookinfo 命名空间中有一个名为 details-v1-57f8794694-hc7gd 的 Pod，该 Pod 托管你的应用程序。\n使用 kubectl port-forward 建立本地机器到 Envoy 边车上的端口 15021 的端口转发：\nkubectl port-forward -n bookinfo details-v1-57f8794694-hc7gd 15021:15021 一旦上述命令成功执行，你现在应该能够将你喜爱的工具指向 URL http://localhost:15021/healthz/ready 并直接访问 Envoy 的 healthz 端点。请避免使用浏览器进行此操作，因为如果正确配置并运行，则 Envoy 代理将返回一个带有空主体的 200 OK 响应。\n例如，你可以使用 curl 以详细模式执行 …","relpermalink":"/tsb/knowledge-base/faq/","summary":"如何确定 Envoy 是否正常？ 确定 Envoy 是否正常的最佳方法是检查其健康和就绪端点（healthz）。要检查已加入的集群中应用程序的 Envoy 的 healthz 端点，你需要直接连接到应用程序的旁路 Envoy 边车。 假设你在集群的 bookinfo 命名空间中有一个名","title":"TSB 常见问题解答"},{"content":" The following example creates a security group for the sidecars in ns1, ns2 and ns3 namespaces owned by its parent workspace w1 under tenant mycompany, and a security setting that applies the WAF Settings. And the security group and security settings to which this WAF Settings is applied to.\napiVersion: security.tsb.tetrate.io/v2 kind: Group metadata: name: t1 workspace: w1 tenant: mycompany organization: myorg spec: namespaceSelector: names: - \u0026#34;*/ns1\u0026#34; - \u0026#34;*/ns2\u0026#34; - \u0026#34;*/ns3\u0026#34; configMode: BRIDGED …","relpermalink":"/tsb/refs/tsb/security/v2/waf-settings/","summary":"The following example creates a security group for the sidecars in ns1, ns2 and ns3 namespaces owned by its parent workspace w1 under tenant mycompany, and a security setting that applies the WAF Settings. And the security group and security settings to which this WAF Settings is applied to.\napiVersion: security.tsb.tetrate.io/v2 kind: Group metadata: name: t1 workspace: w1 tenant: mycompany organization: myorg spec: namespaceSelector: names: - \"*/ns1\" - \"*/ns2\" - \"*/ns3\" configMode: BRIDGED --- apiVersion: security.tsb.tetrate.io/v2 kind: SecuritySetting metadata: name: defaults group: t1 workspace: w1 tenant: mycompany organization: myorg spec: waf: rules: - Include @recommended-conf In the following examples, the security rule for blocking XSS requests is enabled on Tier1Gateway and IngressGateway respectively, with an ad-hoc debug configuration, instead of the one defined in the security rule.","title":"WAF Settings"},{"content":" The WASM extension resource allows defining custom WASM extensions that are packaged in OCI images. The resource allows specifying extension metadata that helps understand how extensions work and how they can be used. Once defined, extensions can be referenced in Ingress and Egress Gateways and Security Groups so that traffic is captured and processed by the extension accordingly. By default, extensions are globally available, but they can be assigned to specific Tenants as well to further …","relpermalink":"/tsb/refs/tsb/extension/v2/wasm-extension/","summary":"The WASM extension resource allows defining custom WASM extensions that are packaged in OCI images. The resource allows specifying extension metadata that helps understand how extensions work and how they can be used. Once defined, extensions can be referenced in Ingress and Egress Gateways and Security Groups so that traffic is captured and processed by the extension accordingly. By default, extensions are globally available, but they can be assigned to specific Tenants as well to further control and constraint where in the Organization the extensions are allowed to be used.\napiVersion: extension.tsb.tetrate.io/v2 kind: WasmExtension metadata: organization: org name: wasm-auth spec: allowedIn: - organizations/org/tenants/tenant1 url: oci://docker.","title":"WASM Extension"},{"content":" Service to manage WASM extensions.\nWasmExtensions The WasmExtension service provides methods to manage the extensions inside an Organization. WasmExtensions are created inside TSB and assigned later to SecuritySettings and IngressGateways.\nGetWasmExtension rpc GetWasmExtension (tetrateio.api.tsb.extension.v2.GetWasmExtensionRequest) returns (tetrateio.api.tsb.extension.v2.WasmExtension)\nRequires READ\nGet a WASM extension\nListWasmExtension rpc ListWasmExtension …","relpermalink":"/tsb/refs/tsb/extension/v2/wasm-service/","summary":"Service to manage WASM extensions.\nWasmExtensions The WasmExtension service provides methods to manage the extensions inside an Organization. WasmExtensions are created inside TSB and assigned later to SecuritySettings and IngressGateways.\nGetWasmExtension rpc GetWasmExtension (tetrateio.api.tsb.extension.v2.GetWasmExtensionRequest) returns (tetrateio.api.tsb.extension.v2.WasmExtension)\nRequires READ\nGet a WASM extension\nListWasmExtension rpc ListWasmExtension (tetrateio.api.tsb.extension.v2.ListWasmExtensionRequest) returns (tetrateio.api.tsb.extension.v2.ListWasmExtensionResponse)\nList the WASM extensions that are defined for the Organization.\nCreateWasmExtension rpc CreateWasmExtension (tetrateio.api.tsb.extension.v2.CreateWasmExtensionRequest) returns (tetrateio.api.tsb.extension.v2.WasmExtension)\nRequires CREATE\nCreates a new WasmExtension object in TSB. This is needed to let the extensions run. Once a WasmExtension has been created, it can be assigned to IngressGateway and SecuritySetting. This method returns the created extension.","title":"WasmExtension Service"},{"content":" Workload Auto Registration represents a registry record of a workload onboarded into the mesh.\nWorkload Auto Registration captures essential information about the workload allowing Workload Onboarding Plane to generate boot configuration for the Istio Sidecar that will be started alongside this workload.\nWorkloadAutoRegistration resource is not supposed to be edited by the users. Instead, it gets created automatically as part of the Workload Onboarding flow.\nUsers can introspect …","relpermalink":"/tsb/refs/onboarding/config/runtime/v1alpha1/registration/","summary":"Workload Auto Registration represents a registry record of a workload onboarded into the mesh.\nWorkload Auto Registration captures essential information about the workload allowing Workload Onboarding Plane to generate boot configuration for the Istio Sidecar that will be started alongside this workload.\nWorkloadAutoRegistration resource is not supposed to be edited by the users. Instead, it gets created automatically as part of the Workload Onboarding flow.\nUsers can introspect WorkloadAutoRegistration resources for the purposes of observability and troubleshooting of Workload Onboarding.\nTo leverage k8s resource garbage collection (i.e. cascade removal),\nWorkloadAutoRegistration resource is owned by the WorkloadGroup resource the workload has joined to WorkloadAutoRegistration resource owns the Istio WorkloadEntry resource that describes the workload to the Istio Control Plane.","title":"Workload Auto Registration"},{"content":" WorkloadConfiguration specifies configuration of the workload handling.\nFor example,\nauthentication: jwt: issuers: - issuer: \u0026#34;https://mycompany.corp\u0026#34; jwksUri: \u0026#34;https://mycompany.corp/jwks.json\u0026#34; shortName: \u0026#34;mycorp\u0026#34; tokenFields: attributes: jsonPath: .custom_attributes deregistration: propagationDelay: 15s JwtAuthenticationConfiguration JwtAuthenticationConfiguration specifies configuration of the workload authentication by means of an OIDC ID Token.\nField Description Validation Rule issuers\nList …","relpermalink":"/tsb/refs/onboarding/config/install/v1alpha1/workload-configuration/","summary":"WorkloadConfiguration specifies configuration of the workload handling.\nFor example,\nauthentication: jwt: issuers: - issuer: \"https://mycompany.corp\" jwksUri: \"https://mycompany.corp/jwks.json\" shortName: \"mycorp\" tokenFields: attributes: jsonPath: .custom_attributes deregistration: propagationDelay: 15s JwtAuthenticationConfiguration JwtAuthenticationConfiguration specifies configuration of the workload authentication by means of an OIDC ID Token.\nField Description Validation Rule issuers\nList of tetrateio.api.onboarding.config.install.v1alpha1.JwtIssuer List of permitted JWT issuers.\nIf a workload authenticates itself by means of an OIDC ID Token, the issuer of that token must be present in this list, otherwise authentication attempt will be declined.\nrepeated = { items: {message:{required:true}}}\nWorkloadAuthenticationConfiguration WorkloadAuthenticationConfiguration specifies configuration of the workload authentication.\nField Description Validation Rule jwt","title":"Workload Configuration"},{"content":" WorkloadIdentity represents a platform-specific identity of a workload joining the mesh.\nE.g.,\nAWS EC2 instance identity:\naws: partition: aws account: \u0026#39;123456789012\u0026#39; region: ca-central-1 zone: ca-central-1b ec2: instance_id: i-1234567890abcdef0 iam_role: name: example-role GCP GCE instance identity:\ngcp: project_number: \u0026#39;234567890121\u0026#39; project_id: gcp-example region: us-central1 zone: us-central1-a gce: instance_id: \u0026#39;693197132356332126\u0026#39; Azure Compute instance identity:\nazure: subscription: …","relpermalink":"/tsb/refs/onboarding/config/types/identity/v1alpha1/identity/","summary":"WorkloadIdentity represents a platform-specific identity of a workload joining the mesh.\nE.g.,\nAWS EC2 instance identity:\naws: partition: aws account: '123456789012' region: ca-central-1 zone: ca-central-1b ec2: instance_id: i-1234567890abcdef0 iam_role: name: example-role GCP GCE instance identity:\ngcp: project_number: '234567890121' project_id: gcp-example region: us-central1 zone: us-central1-a gce: instance_id: '693197132356332126' Azure Compute instance identity:\nazure: subscription: 531bed28-f708-4fc5-b0c1-2c1edde46e4f resource_group: azure-example compute: instance_id: fc13d26e-d3c0-458e-b353-686d5ca19506 JWT identity:\njwt: issuer: https://mycompany.corp subject: us-east-datacenter1-vm007 attributes: region: us-east datacenter: datacenter1 instance_name: vm007 instance_hostname: vm007.internal.corp instance_role: app-ratings WorkloadIdentity WorkloadIdentity represents a platform-specific identity of a workload joining the mesh.\nField Description Validation Rule aws\ntetrateio.api.onboarding.config.types.identity.aws.v1alpha1.AwsIdentity oneof kind AWS-specific identity of a workload.","title":"Workload Identity"},{"content":" Workload Registration specifies information sent by the Workload Onboarding Agent to the Workload Onboarding Plane to register the workload in the mesh.\nAgentInfo AgentInfo specifies information about the Workload Onboarding Agent installed alongside the workload.\nField Description Validation Rule version\nstring REQUIRED Version of the Workload Onboarding Agent.\nstring = { min_len: 1}\nIstioSidecarInfo IstioInfo specifies information about the Istio Sidecar installed alongside the workload. …","relpermalink":"/tsb/refs/onboarding/config/types/registration/v1alpha1/registration/","summary":"Workload Registration specifies information sent by the Workload Onboarding Agent to the Workload Onboarding Plane to register the workload in the mesh.\nAgentInfo AgentInfo specifies information about the Workload Onboarding Agent installed alongside the workload.\nField Description Validation Rule version\nstring REQUIRED Version of the Workload Onboarding Agent.\nstring = { min_len: 1}\nIstioSidecarInfo IstioInfo specifies information about the Istio Sidecar installed alongside the workload.\nField Description Validation Rule version\nstring REQUIRED Version of the Istio Sidecar.\nstring = { min_len: 1}\nrevision\nstring Istio revision the pre-installed Istio Sidecar corresponds to.\nE.g., canary, alpha, etc.\nIf omitted, it is assumed that the pre-installed Istio Sidecar corresponds to the default Istio revision.","title":"Workload Registration"},{"content":"List of annotations on a WorkloadEntry resource supported by the tctl x sidecar-bootstrap command.\nUsage example apiVersion: networking.istio.io/v1beta1 kind: WorkloadEntry metadata: name: my-vm namespace: my-namespace annotations: sidecar-bootstrap.istio.io/ssh-user: istio-proxy sidecar-bootstrap.istio.io/proxy-config-dir: /etc/istio-proxy sidecar-bootstrap.istio.io/proxy-instance-ip: 10.0.0.1 sidecar.istio.io/logLevel: debug sidecar.istio.io/componentLogLevel: upstream:info,config:trace …","relpermalink":"/tsb/reference/cli/reference/workload-entry-annotations/","summary":"List of annotations on a WorkloadEntry resource supported by the tctl x sidecar-bootstrap command.\nUsage example apiVersion: networking.istio.io/v1beta1 kind: WorkloadEntry metadata: name: my-vm namespace: my-namespace annotations: sidecar-bootstrap.istio.io/ssh-user: istio-proxy sidecar-bootstrap.istio.io/proxy-config-dir: /etc/istio-proxy sidecar-bootstrap.istio.io/proxy-instance-ip: 10.0.0.1 sidecar.istio.io/logLevel: debug sidecar.istio.io/componentLogLevel: upstream:info,config:trace sidecar.istio.io/statsInclusionRegexps: .* # enable all Envoy metrics proxy.istio.io/config: | concurrency: 3 spec: ... Standard Istio annotations proxy.istio.io/config Overrides for the proxy configuration for this specific proxy. Available options can be found at https://istio.io/docs/reference/config/istio.mesh.v1alpha1/#ProxyConfig.\nsidecar.istio.io/interceptionMode Specifies the mode used to redirect inbound connections to Envoy (REDIRECT or TPROXY).\nsidecar.istio.io/proxyImage Specifies the Docker image to be used by the Envoy sidecar.\nsidecar.istio.io/logLevel Specifies the log level for Envoy.","title":"WorkloadEntry Annotations"},{"content":" A Workspace carves a chunk of the cluster resources owned by a tenant into an isolated configuration domain.\nThe following example claims ns1 and ns2 namespaces across all clusters owned by the tenant mycompany.\napiVersion: api.tsb.tetrate.io/v2 kind: Workspace metadata: name: w1 tenant: mycompany organization: myorg spec: namespaceSelector: names: - \u0026#34;*/ns1\u0026#34; - \u0026#34;*/ns2\u0026#34; The following example claims ns1 namespace only from the c1 cluster and claims all namespaces from the c2 cluster.\napiVersion: …","relpermalink":"/tsb/refs/tsb/v2/workspace/","summary":"A Workspace carves a chunk of the cluster resources owned by a tenant into an isolated configuration domain.\nThe following example claims ns1 and ns2 namespaces across all clusters owned by the tenant mycompany.\napiVersion: api.tsb.tetrate.io/v2 kind: Workspace metadata: name: w1 tenant: mycompany organization: myorg spec: namespaceSelector: names: - \"*/ns1\" - \"*/ns2\" The following example claims ns1 namespace only from the c1 cluster and claims all namespaces from the c2 cluster.\napiVersion: api.tsb.tetrate.io/v2 kind: Workspace metadata: name: w1 tenant: mycompany organization: myorg spec: namespaceSelector: names: - \"c1/ns1\" - \"c2/*\" Custom labels and annotations can be propagated to the final Istio translation that will be applied at the clusters.","title":"Workspace"},{"content":" DEPRECATED: use Access Bindings instead.\nWorkspaceAccessBindings is an assignment of roles to a set of users or teams to access resources under a Workspace. The user or team information is obtained from an LDAP server that should have been configured as part of Service Bridge installation. Note that a WorkspaceAccessBinding can be created or modified only by users who have SET_POLICY permission on the Workspace.\nThe following example assigns the workspace-admin role to users alice, bob, and …","relpermalink":"/tsb/refs/tsb/rbac/v2/workspace-access-bindings/","summary":"DEPRECATED: use Access Bindings instead.\nWorkspaceAccessBindings is an assignment of roles to a set of users or teams to access resources under a Workspace. The user or team information is obtained from an LDAP server that should have been configured as part of Service Bridge installation. Note that a WorkspaceAccessBinding can be created or modified only by users who have SET_POLICY permission on the Workspace.\nThe following example assigns the workspace-admin role to users alice, bob, and members of the t1 team for all workspace w1 owned by the tenant mycompany. Use fully-qualified name (fqn) when specifying user and team","title":"Workspace Access Bindings"},{"content":" Service to manage TSB workspaces.\nWorkspaces The Workspaces service provides methods to manage the workspaces for a given tenant.\nWorkspaces are the main containers for the different configuration resources available in TSB, and provide infrastructure isolation constraints.\nCreateWorkspace rpc CreateWorkspace (tetrateio.api.tsb.v2.CreateWorkspaceRequest) returns (tetrateio.api.tsb.v2.Workspace)\nRequires CREATE\nCreate a new workspace. The workspace will own exclusively the namespaces configured …","relpermalink":"/tsb/refs/tsb/v2/workspace-service/","summary":"Service to manage TSB workspaces.\nWorkspaces The Workspaces service provides methods to manage the workspaces for a given tenant.\nWorkspaces are the main containers for the different configuration resources available in TSB, and provide infrastructure isolation constraints.\nCreateWorkspace rpc CreateWorkspace (tetrateio.api.tsb.v2.CreateWorkspaceRequest) returns (tetrateio.api.tsb.v2.Workspace)\nRequires CREATE\nCreate a new workspace. The workspace will own exclusively the namespaces configured in the namespaces selector for the workspace.\nGetWorkspace rpc GetWorkspace (tetrateio.api.tsb.v2.GetWorkspaceRequest) returns (tetrateio.api.tsb.v2.Workspace)\nRequires READ\nGet the details of an existing workspace\nUpdateWorkspace rpc UpdateWorkspace (tetrateio.api.tsb.v2.Workspace) returns (tetrateio.api.tsb.v2.Workspace)\nRequires WRITE\nModify an existing workspace\nListWorkspaces rpc ListWorkspaces (tetrateio.api.tsb.v2.ListWorkspacesRequest) returns (tetrateio.api.tsb.v2.ListWorkspacesResponse)\nList all existing workspaces for the given tenant.","title":"Workspace Service"},{"content":" Workspace Setting allows configuring the default traffic, security and east-west gateway settings for all the workloads in the namespaces owned by the workspace. Any namespace in the workspace that is not part of a traffic or security group with specific settings will use these default settings.\nThe following example sets the default security policy to accept either mutual TLS or plaintext traffic, and only accept connections at a proxy workload from services within the same namespace. The …","relpermalink":"/tsb/refs/tsb/v2/workspace-setting/","summary":"Workspace Setting allows configuring the default traffic, security and east-west gateway settings for all the workloads in the namespaces owned by the workspace. Any namespace in the workspace that is not part of a traffic or security group with specific settings will use these default settings.\nThe following example sets the default security policy to accept either mutual TLS or plaintext traffic, and only accept connections at a proxy workload from services within the same namespace. The default traffic policy allows unknown traffic from a proxy workload to be forwarded via an egress gateway tsb-egress in the perimeter namespace in the same cluster.","title":"Workspace Setting"},{"content":"上周，Kubernetes 项目合并了一个新的 alpha 特性，使用户能够在规范中定义“sidecar containers”。这个新功能旨在帮助定义多容器 pod 中辅助容器的行为，这些容器可能有助于配置、网络、日志和度量收集等方面。\n什么是 sidecar container？ 理论上，Kubernetes 期望您在每个 pod 中运行一个容器。实际上，许多用例需要多容器 pod——例如，当您使用某些服务网格时，几乎所有的 pod 中都可能有 sidecar。\n有时，辅助容器仅用于初始化：例如为主容器配置和管理 secret。Kubernetes 已经为用户提供了定义 initContainer 的方式一段时间了。这个新功能最终为 initContainer 提供了更精细的粒度，以反映 sidecar 的特定要求，简化常见用法模式并为未来开辟了一些有趣的设计空间。\nsidecar container 特性如何工作？ 在这个新的功能门控中，sidecar containers 被定义为…\n在 pod 中比其他容器更早地启动，因为它们可能需要先初始化。这对于像服务网格这样的事情很重 …","relpermalink":"/blog/understanding-kubernetes-new-sidecar-container-feature/","summary":"Kubernetes 的新 sidecar container 特性允许用户在规范中定义辅助容器的行为，以帮助配置、网络、日志和度量收集等方面。这个新功能旨在为多容器 pod 中的 sidecar 容器提供更精细的粒度，使其能够比 initContainer 更好地反映 sidecar 的特定要求，简化常见用法模式并为未来开辟了一些有趣的设计空间。","title":"Kubernetes 将推出新 sidecar container 特性"},{"content":"前言 译者注：本文译自 Codefresh 公司发布的系列博客 Enterprise CI/CD Best Practices。\n如果你正在学习持续集成/交付/部署，你可能会发现主要有两类资源：\nCI/CD 是什么以及为什么需要它的概述。这些对于初学者很好，但不涵盖有关 Day2 操作或如何优化现有流程的任何内容。 仅涵盖 CI/CD 的特定方面（例如仅单元测试或仅部署）的详细教程，使用特定的编程语言和工具。 我们相信这两个极端之间存在差距。我们缺少一份恰当的指南，介于这两个类别之间，讨论最佳实践，但不是以抽象的方式。如果你一直想阅读有关 CI/CD 的指南，不仅解释“为什么”，还解释“如何”应用最佳实践，那么这份指南适合你。\n我们将描述所有有效的 CI/CD 工作流程的基本原理，但不仅以一般术语谈论，而且还将解释每个最佳实践背后的技术细节，更重要的是，如果你不采用它，它可能会对你产生什么影响。\n设置优先级\n一些公司试图在掌握基础知识之前跳上 DevOps 的列车。你很快会发现，CI/CD 流程中出现的一些问题通常是现有流程问题，只有当该公司试图遵循 CI/CD 流程的最佳实践时，才会 …","relpermalink":"/blog/enterprise-ci-cd-best-practices/","summary":"我们将描述所有有效的 CI/CD 工作流程的基本原理，但不仅以一般术语谈论，而且还将解释每个最佳实践背后的技术细节，更重要的是，如果你不采用它，它可能会对你产生什么影响。","title":"企业级 CI/CD 最佳实践"},{"content":"Istio 成为 CNCF 项目的毕业生。这一历史性的时刻代表着 Istio 在云原生领域的成长和成熟，标志着最广泛部署的服务网格迎来了一个令人兴奋的新篇章。Kubernetes 是 第一个获得毕业资格的项目，时间是 2018 年。今天，自它作为一个孵化项目进入 CNCF 不到一年的时间，Istio 就毕业了，成为 CNCF 历史上最快的一个。\nTetrate 是由 Istio 创始团队的成员创立的，旨在推广和扩大服务网格的应用，并自创立以来一直是 Istio 最重要的贡献者之一。我们为 Istio 及其社区的辛勤工作和奉献取得了这一里程碑式的认可而感到自豪和兴奋。\nIstio 毕业意味着什么？ CNCF 项目分为三个类别，作为项目成熟度的标志：\n沙盒。 CNCF“沙盒”是 CNCF 内新项目的入口。它为早期阶段的项目提供支持、指导和可见度，以便从 CNCF 社区中获得支持。沙盒旨在为这些项目提供一个安全和协作的环境，以便它们进行实验、创新和成熟。\n孵化。 孵化项目已经超过了开发的早期阶段，并展示了成为成熟云原生技术的潜力。Istio 凭借其强大的社区和早期采用者的不断生产使用而被接纳 …","relpermalink":"/blog/istio-service-mesh-graduates-cncf/","summary":"本文介绍了 Istio 作为 CNCF 项目的毕业生的成熟度、安全性、生产就绪、采用和生态系统、CNCF 支持和治理以及社区和企业支持。同时，介绍了 Tetrate 对 Istio 的影响，包括代码贡献、唯一的纯 OSS 企业产品、共同的专业知识、制定标准的安全领导力、社区参与、教育和培训、生态系统扩展等。Tetrate 和 Istio 的交织历史的简要时间线也被列出。最后，提供了使用 Istio 和 Tetrate Service Bridge 的方法和资源。","title":"Istio 成为最快的 CNCF 毕业项目"},{"content":"在 Kubernetes 中，同一命名空间中的任何 Pod 都可以使用其 IP 地址相互通信，无论它属于哪个部署或服务。虽然这种默认行为适用于小规模应用，但在规模扩大和复杂度增加的情况下，Pod 之间的无限通信可能会增加攻击面并导致安全漏洞。\n在集群中实施 Kubernetes 网络策略可以改善以下方面：\n安全性： 使用 Kubernetes 网络策略，你可以指定允许哪些 Pod 或服务相互通信，以及应该阻止哪些流量访问特定的资源。这样可以更容易地防止未经授权的访问敏感数据或服务。 合规性： 在医疗保健或金融服务等行业，合规性要求不可妥协。通过确保流量仅在特定的工作负载之间流动，以满足合规要求。 故障排除： 通过提供关于应该相互通信的 Pod 和服务的可见性，可以更轻松地解决网络问题，特别是在大型集群中。策略还可以帮助你确定网络问题的源，从而加快解决速度。 Kubernetes 网络策略组件 强大的网络策略包括：\n策略类型： Kubernetes 网络策略有两种类型：入口和出口。入口策略允许你控制流入 Pod 的流量，而出口策略允许你控制从 Pod 流出的流量。 …","relpermalink":"/blog/understanding-kubernetes-network-policies/","summary":"这篇文章介绍了 Kubernetes 网络策略的概念、作用和使用方法。Kubernetes 网络策略可以让你配置和执行一套规则，来控制集群内部的流量。它们可以提高安全性、符合合规性和简化故障排除。文章分析了网络策略的不同组成部分，包括选择器、入口规则和出口规则，并给出了不同的策略示例和最佳实践。文章的目标是让读者对使用 Kubernetes 网络策略来保护和管理流量有一个坚实的理解。","title":"Kubernetes 网络策略入门：概念、示例和最佳实践"},{"content":"译者注：本文译自 Sysdig 公司的网站，Sysdig 是一家提供容器安全、监控和故障排除解决方案的公司，其产品帮助用户在容器化环境中实现可观测性和安全性。这篇文章介绍了 CNAPP，CNAPP 是一个端到端的云安全解决方案，可提供实时威胁检测、简化符合性、改善 DevOps 协作、操作效率等多种好处。它通过整合安全控件、提供集中式管理和运行时洞察力等方式，增强组织的整体安全姿态。\n总览 CNAPP（容器化应用程序保护平台）是一种综合性的、全方位的安全策略，贯穿整个应用程序的生命周期（SDLC）。随着云计算的快速普及和现代应用程序的日益复杂，传统的安全措施往往无法有效地保护免受复杂的网络威胁。\nCNAPP 结合了“向左倾斜”和“向右防御”安全概念，提供了全面和强大的安全策略，确保了应用程序在整个生命周期中的保护。\n通过将安全向左移动，组织可以利用从应用程序开发过程的最开始阶段就开始的安全控制、漏洞扫描和合规性检查。\n“向右防御”概念侧重于在应用程序运行时阶段实时检测和响应安全事件。尽管在开发过程中尽最大努力保护应用程序，但漏洞可能仍然存在，或者新的威胁可能出现，因此 CNAPP 必须 …","relpermalink":"/blog/what-is-cnapp/","summary":"CNAPP 是一个端到端的云安全解决方案，可提供实时威胁检测、简化符合性、改善 DevOps 协作、操作效率等多种好处。它通过整合安全控件、提供集中式管理和运行时洞察力等方式，增强组织的整体安全姿态。","title":"什么是 CNAPP（容器化应用保护平台）?"},{"content":" 摘要：本文译自 How OpenTelemetry Works with Kubernetes。本文介绍了如何将 OpenTelemetry 与 Kubernetes 配合使用。OpenTelemetry 可以作为 Prometheus 的替代品，也可以将数据导出到各种后端，包括 Prometheus。OpenTelemetry Operator 负责部署和管理 OpenTelemetry Collector，该组件是收集、处理和导出遥测数据的中央组件。OpenTelemetry 日志提供了一种标准化的方式来收集、处理和分析分布式系统中的日志。此外，本文还介绍了 OpenTelemetry 的下一步计划，包括 Web 服务器的自动化仪器化、OpenTelemetry Profile 和 Open Agent Management Protocol。\nOpenTelemetry 的主要目标是提供一种标准的方式，使开发人员和最终用户能够从他们的应用程序和系统中创建、收集和导出遥测数据，并促进不同可观察性工具和平台之间的互操作性。\nOTEL 支持多种编程语言， …","relpermalink":"/blog/how-opentelemetry-works-with-kubernetes/","summary":"本文介绍了如何将 OpenTelemetry 与 Kubernetes 配合使用。OpenTelemetry 可以作为 Prometheus 的替代品，也可以将数据导出到各种后端，包括 Prometheus。OpenTelemetry Operator 负责部署和管理 OpenTelemetry Collector，该组件是收集、处理和导出遥测数据的中央组件。OpenTelemetry 日志提供了一种标准化的方式来收集、处理和分析分布式系统中的日志。此外，本文还介绍了 OpenTelemetry 的下一步计划，包括 Web 服务器的自动化仪器化、OpenTelemetry Profile 和 Open Agent Management Protocol。","title":"如何利用 OpenTelemetry 监控和优化 Kubernetes 的性能"},{"content":"本文译自 A Comprehensive Guide to API Gateways, Kubernetes Gateways, and Service Meshes。\n摘要：本文介绍了 API 网关、Kubernetes 网关和服务网格的综合指南。API 网关和 Kubernetes 网关解决了边缘问题和 API 抽象化，而服务网格解决了服务之间的通信挑战。文章还介绍了如何在不同的网关中配置金丝雀部署，并讨论了 Kubernetes Gateway API 的发展和服务网格接口（SMI）规范。最后，文章提供了一些关于何时使用哪种网关的建议。\n简介 本文将介绍三种技术，它们分别是 API 网关、Kubernetes 网关和 Service Mesh，以及它们之间的区别，以及如何应用它们。\nAPI 网关 API 网关是一个连接客户端和 API 的中介，它接收所有客户端请求，将它们转发到所需的 API，并将响应返回给客户端。\n它基本上是一个具有许多功能的反向代理。\n除此之外，API 网关还可以具有诸如身份验证、安全性、细粒度流量控制和监控等功能，使 API 开发人员只需专注于业务需求。\n有 …","relpermalink":"/blog/gateway-and-mesh/","summary":"本文介绍了 API 网关、Kubernetes 网关和服务网格的综合指南。API 网关和 Kubernetes 网关解决了边缘问题和 API 抽象化，而服务网格解决了服务之间的通信挑战。文章还介绍了如何在不同的网关中配置金丝雀部署，并讨论了 Kubernetes Gateway API 的发展和服务网格接口（SMI）规范。最后，文章提供了一些关于何时使用哪种网关的建议。","title":"API 网关、Kubernetes 网关和 Service Mesh 综合指南"},{"content":"本文译自 Reframing Kubernetes Observability with a Graph。\n摘要：本文介绍了将 DevOps 和 Kubernetes 视为图形的方法，以提高效率和弹性。通过将 Kubernetes 部署中的不同组件建模为图中的节点，组织可以更好地了解不同组件的交互方式以及一个区域的更改如何影响整个系统。这可以帮助组织采取更为主动、战略性的 DevOps 方法，而不仅仅是在问题出现时做出反应。\nKubernetes 可以跨多个主机部署应用程序，同时让团队将它们作为单个逻辑单元进行管理。它抽象了底层基础架构，并提供了一个用于与集群交互的统一 API，以及用于简化工作流程的自动化。它是现代开发实践的完美系统。\n但在这些以云为先的生态系统中确保效率和弹性并不容易。微服务架构使得无法跟上正在不断发生的所有软件和基础架构变化。这个问题只会因分裂的监视和可观测工具以及团队和个人之间的隔离信息而变得更加严重。\n为了跟上，组织必须以一种新的方式考虑 DevOps 和 Kubernetes - 作为一个图形。\n将 DevOps 视为图形 DevOps 通常专注于自动化和集 …","relpermalink":"/blog/reframing-kubernetes-observability-with-a-graph/","summary":"本文介绍了将 DevOps 和 Kubernetes 视为图形的方法，以提高效率和弹性。通过将 Kubernetes 部署中的不同组件建模为图中的节点，组织可以更好地了解不同组件的交互方式以及一个区域的更改如何影响整个系统。这可以帮助组织采取更为主动、战略性的 DevOps 方法，而不仅仅是在问题出现时做出反应。","title":"以图形重构 Kubernetes 可观测性"},{"content":"最近，Docker 宣布与 WasmEdge 合作，在 Docker 生态系统中支持 WebAssembly。\n本文将介绍什么是 WebAssembly，以及为什么它与 Docker 生态系统相关，并提供一些实践示例。我们假设您熟悉 Docker 工具集。我们将使用我们的 WebAssembly PHP 的端口 来演示如何构建 PHP 解释器，将其打包为 OCI 镜像的一部分，并使用 Docker 运行它。\n请注意，本文的重点是获得实践经验，而不是讨论技术细节。您可以复制以下示例，也可以只读到最后，因为我们还将提供输出。\nWebAssembly - 什么？为什么？ 这是一个非常基本的介绍。如果您已经熟悉该技术，则可以跳到动手环节。\n什么是 WebAssembly？ WebAssembly（或 Wasm）是一个开放标准，定义了一种二进制指令格式，可以从不同的源语言创建可移植的二进制可执行文件。\n这些二进制文件可以在各种环境中运行。它起源于 web，并得到所有主要浏览器的支持。\nWasm 在浏览器中是如何工作的？ 浏览器引擎集成了一个 Wasm 虚拟机，通常称为 Wasm 运行时，它可以运 …","relpermalink":"/blog/docker-without-containers/","summary":"本文介绍了如何在 Docker 中使用 WebAssembly（Wasm）来运行 PHP 应用程序。Wasm 容器比传统容器更小，提供更高级别的沙盒性能，并且具有真正的可移植性。本文还提供了一些示例，演示了如何使用 Wasm 在不同的环境中运行 WordPress。","title":"WebAssembly：无需容器就能运行 Docker！"},{"content":" 译者注：本文译自 Docker + WebAssembly: a quick intro | by Fabrizio Guglielmino | Medium，本文介绍了使用 Docker 和 WebAssembly 创建容器的过程。通过比较标准 Docker 容器和 WebAssembly 容器，作者指出 WebAssembly 容器具有性能优势、架构中立等优点，但也存在不成熟的问题。WebAssembly 容器有望彻底改变容器化应用程序的方式。\n今天，我想展示一种实用且有趣的使用 Docker 的方式：在容器中使用 WebAssembly。\n我说“实用的方式”，这就是为什么我假设您有一些经验：\nDocker（当然） Rust（实际上，只是为了理解“Hello World”） WebAssembly；只需要对其有一个基本的了解（注意：我将在讨论中交替使用 WASM 和 WebAssembly 这两个术语） 关于我即将展示的内容，简单说一下：一个 Docker 容器是一个包含运行环境的映像的运行实例。运行环境通常是一个操作系统，大多数情况下是 Linux。操作系统是运行应用程序的必要 …","relpermalink":"/blog/docker-wasm-quick-intro/","summary":"本文介绍了使用 Docker 和 WebAssembly 创建容器的过程。通过比较标准 Docker 容器和 WebAssembly 容器，作者指出 WebAssembly 容器具有性能优势、架构中立等优点，但也存在不成熟的问题。WebAssembly 容器有望彻底改变容器化应用程序的方式。","title":"用 Docker 和 WebAssembly 打造容器的新时代！"},{"content":" 译者注：本文译自 CNCF 平台白皮书，介绍了如何构建云原生计算平台以及平台可能提供的能力。这些能力包括 Web 门户、API、黄金路径模板、自动化、开发环境、可观测性、基础设施服务、数据服务、消息和事件服务、身份和密码管理服务、安全服务和工件存储。此外，该文档还介绍了与平台相关的术语，如平台、平台能力提供者、平台工程师、平台产品经理、平台团队和平台用户。\n介绍 受 DevOps 所承诺的跨职能合作的启发，平台工程正在企业中作为一种明确的合作形式出现。平台策划和呈现基础功能、框架和体验，以促进和加速应用程序开发人员、数据科学家和信息工作者等内部客户的工作。特别是在云计算领域，平台已经帮助企业实现了云计算长期承诺的价值，例如快速的产品发布、跨基础架构的可移植性、更安全和更弹性的产品以及更高的开发者生产力。\n本文旨在支持企业领导、企业架构师和平台团队领导者提倡、调查和计划云计算内部平台。我们相信平台对企业的实际价值流有重大影响，但只是间接的，因此领导共识和支持对平台团队的长期可持续性和成功至关重要。在本文中，我们将通过讨论平台的价值、如何衡量该价值以及如何实施最大化该价值的平台团队来实现 …","relpermalink":"/blog/cncf-platforms-white-paper/","summary":"本文译自 CNCF 平台白皮书，介绍了如何构建云原生计算平台以及平台可能提供的能力。这些能力包括 Web 门户、API、黄金路径模板、自动化、开发环境、可观测性、基础设施服务、数据服务、消息和事件服务、身份和密码管理服务、安全服务和工件存储。此外，该文档还介绍了与平台相关的术语，如平台、平台能力提供者、平台工程师、平台产品经理、平台团队和平台用户。","title":" CNCF 平台白皮书"},{"content":"本文译自：Reducing the Cognitive Load Associated with Observability - The New Stack\n译者注：本文讨论了降低可观测性对认知负荷的影响。在处理大量数据时，我们需要过滤和转换数据点以生成适当的信号，并依赖警报系统来进行人类干预。游戏日是测试响应能力的好机会。在团队中培养协作文化对每个人的福祉至关重要。通过实施这些策略，软件工程团队可以确保他们具备使用和有效理解可观测性信号所需的知识和技能。\n你能想象在没有现代可观测工具的情况下开发或操作分布式系统吗？我们知道可观测性是一项关键的实践，可以让我们提高系统的可靠性，减少服务停机时间，可视化使用模式，提供性能见解并促进问题解决。\n随着过去十年微服务架构和全球“shift left”的意图的广泛采用，工程师的角色——从开发人员和运维人员到 DevOps、站点可靠性工程和平台工程——发生了巨大变化。许多人被赋予更多的责任，并增加了工作量。\n什么是 Shift left？\n“Shift left” 是一种软件开发术语，它指的是在开发生命周期的早期阶段引入测试和安全性措施，以便更早地 …","relpermalink":"/blog/reducing-the-cognitive-load-associated-with-observability/","summary":"本文讨论了降低可观测性对认知负荷的影响。在处理大量数据时，我们需要过滤和转换数据点以生成适当的信号，并依赖警报系统来进行人类干预。游戏日是测试响应能力的好机会。在团队中培养协作文化对每个人的福祉至关重要。通过实施这些策略，软件工程团队可以确保他们具备使用和有效理解可观测性信号所需的知识和技能。","title":"如何降低可观测性带来的认知负荷"},{"content":"本文译自：Startup Fermyon Releases Spin 1.0 for WebAssembly Serverless Applications。\nFermyon 最近宣布推出 Spin 1.0，这是一个用于使用 WebAssembly (Wasm) 开发无服务器应用的开源开发者工具和框架。\nSpin 1.0 是其去年推出 介绍 后的首个稳定版本。在 1.0 版本中，公司增加了对新编程语言（如 JavaScript、TypeScript、Python 或 C#，除了 Rust 和 Go 之外）、连接数据库（关系型 或 Redis）、使用流行的注册表服务分发应用程序（GitHub Container Registry、Docker Hub 或 AWS ECR）、内置的 键值存储 以保持状态、在 Kubernetes 上运行应用程序以及与 HashiCorp Vault 集成以管理运行时配置等方面的支持。\n通过 Spin，该公司为创建运行 Wasm 的应用程序提供了轻松的开发体验，包括部署和安全运行它们的框架。\nFermyon 的首席技术官 Radu Matei 在一篇 博客文 …","relpermalink":"/blog/spin-wasm-ga/","summary":"Fermyon 最近宣布推出 Spin 1.0，这是一个用于使用 WebAssembly (Wasm) 开发无服务器应用的开源开发者工具和框架。","title":"初创公司 Fermyon 发布 Spin 1.0 用于 WebAssembly 无服务器应用"},{"content":"Tetrate Service Express (TSE) 是一款基于开源软件的服务连接、安全和弹性自动化解决方案，专为 Amazon EKS 设计。\n本文译自：Tetrate Service Express 介绍\n快速实现 Amazon EKS 上安全和弹性的服务网格 今天我们很高兴地宣布 Tetrate Service Express (TSE)，这是一款针对 Amazon EKS 的服务连接、安全和弹性自动化解决方案。我们基于 Istio 和 Envoy 等开源服务网格组件构建了 TSE，并针对 AWS 对 TSE 进行了简化安装、配置和操作的优化。如果您的团队正在 AWS 上进行服务网格实验，并且需要快速证明投资回报率，而无需掌握复杂的 Istio 和 AWS 基元，那么 TSE 就是适合您的选择！如果您的团队已经在单个集群上拥有了服务网格，但希望将网格扩展到多个集群甚至区域，那么 TSE 也可以帮助您。事实上，TSE 是唯一一款基于开源软件并针对 AWS 进行优化的产品，预先集成了最受欢迎的 AWS 服务，可在几分钟内让您上手。\n如果您想快速了解 Tetrate …","relpermalink":"/blog/introducing-tetrate-service-express/","summary":"Tetrate Service Express (TSE) 是一款基于开源软件的服务连接、安全和弹性自动化解决方案，专为 Amazon EKS 设计。","title":"Tetrate 推出针对 Amazon EKS 设计的服务网格解决方案 TSE"},{"content":"您是否应该让多个团队使用同一个 Kubernetes 集群？\n您是否可以安全地运行来自不信任用户的不信任工作负载？\nKubernetes 是否具备多租户功能？\n本文将探讨在运行具有多个租户的集群时面临的挑战。\n多租户可分为：\n软多租户，适用于信任您的租户 - 比如与同一家公司的团队共享集群时。 硬多租户，适用于您不信任的租户。 您还可以混合使用！\n在租户之间共享集群的基本构建块是命名空间。\n命名空间在逻辑上对资源进行分组，它们不提供任何安全机制，也不能保证所有资源都部署在同一节点上。\n命名空间中的 Pod 仍然可以与集群中的所有其他 Pod 通信，向 API 发出请求并使用它们想要的任何资源。\n默认情况下，任何用户都可以访问任何命名空间。\n那应该怎么阻止它？\n通过 RBAC，您可以限制用户和应用程序对命名空间内和命名空间中的内容所能做的事情。\n常见的操作是授予有限用户权限。\n使用 Quotas 和 LimitRanges，您可以限制命名空间中部署的资源以及可以使用的内存、CPU 等。\n如果您想限制租户对其命名空间所能做的事情，这是一个绝妙的想法。\n默认情况下，所有 Pod …","relpermalink":"/blog/multi-tenancy-in-kubernetes/","summary":"本文将探讨在运行具有多个租户的集群时面临的挑战。","title":"如何在 Kubernetes 中实现多租户隔离：命名空间、RBAC 和网络策略的应用"},{"content":" 译者注\n这篇文章探讨了在人工智能时代，大数据公司如何适应和创新。作者采访了 Alation 的联合创始人 Aaron Kalb，了解了他们的“数据目录”平台，以及他对 ChatGPT 等生成型 AI 软件的看法。Kalb 认为，生成型 AI 是一种催化剂，推动了一波新的数据智能公司的出现。他还分析了生成型 AI 的优势和挑战，以及如何利用它来提高数据质量和可信度。 就像云计算引入了一系列“大数据”解决方案一样，生成式人工智能是新一波数据智能公司的催化剂。\n还记得“大数据”这个流行语吗？它在云计算时代孕育了许多成功的公司，如 Snowflake、Databricks、DataStax、Splunk 和 Cloudera。但现在我们处于人工智能时代，据说机器学习软件现在已经达到或接近“智能”了（即使它容易 产生幻觉 ——但是，我们所有人不都是吗？）。\n因此，鉴于当前的人工智能热潮，我们是否还需要“大数据”公司来对数据进行分类和组织呢？现在 AI 不是可以为我们做到这一点吗？\n为了了解数据公司如何适应人工智能时代，我采访了 Aaron Kalb，Alation 的联合创始人之 …","relpermalink":"/blog/the-next-wave-of-big-data-companies-in-the-age-of-chatgpt/","summary":"这篇文章探讨了在人工智能时代，大数据公司如何适应和创新。作者采访了 Alation 的联合创始人 Aaron Kalb，了解了他们的“数据目录”平台，以及他对 ChatGPT 等生成型 AI 软件的看法。Kalb 认为，生成型 AI 是一种催化剂，推动了一波新的数据智能公司的出现。他还分析了生成型 AI 的优势和挑战，以及如何利用它来提高数据质量和可信度。","title":"从 Siri 到 ChatGPT：大数据公司如何迎接 AI 新浪潮"},{"content":" 如果你正在阅读这篇博客文章，我假设你已经熟悉 Kubernetes 并且知道它是一个容器编排平台。在创建新的应用程序版本时，你的容器构建过程已经很好了。Kubernetes 提供了广泛的功能，使用户能够执行复杂的部署策略。挑战在于根据你的环境，有正确和错误的使用方式。\n你组织中的平台工程师很可能非常熟悉 Kubernetes，了解将应用程序部署到集群中的正确和错误方式。挑战在于向平台的所有用户传递这些信息。\n大多数运维人员的理解很可能来自于正式培训或花费很多时间构建平台并从错误中学习。要求所有打算与平台交互的应用程序开发人员具有相同的经验是不现实的。这对组织来说在时间、精力和对产品和客户的潜在影响方面都是昂贵的，因为这些经验是从错误中学到的。\n在每个组织内，用户使用内部开发平台的策略和标准是已知的或需要遵循的。挑战在于许多组织使用文档和广泛的沟通来确保用户遵循这些标准。这在人们偏离预期标准并学习正确方法之间提供了长时间的反馈循环。\nKyverno 这就是 Kyverno 的用武之地，它是一个基于 Kubernetes 的策略引擎，提供了一种将平台管理员学到的经验编码化的方 …","relpermalink":"/blog/argo-cd-kyverno-best-practice-policies/","summary":"本文介绍了如何使用 Kyverno 和 Argo CD 来强制执行 Kubernetes 的最佳实践。Kyverno 是一个 Kubernetes 原生的策略引擎，可以用来定义和执行安全和合规的规则。Argo CD 是一个 GitOps 的持续交付解决方案，可以用来管理 Kubernetes 集群的状态。文章通过一个具体的示例，展示了如何使用 Argo CD 来部署 Kyverno 和策略，以及如何处理策略违规的情况。","title":"使用 Kyverno 和 Argo CD 实施 Kubernetes 最佳实践"},{"content":" 译者注：本文介绍了 Istio 的新的目的地导向的 waypoint 代理，它可以简化和扩展 Istio 的功能。文章介绍了 waypoint 代理的架构，部署方式，以及如何将源代理的配置转移到目的地代理，从而提高可扩展性，可调试性，一致性和安全性。文章还展示了如何使用 Istio 的策略和遥测来管理和监控 waypoint 代理。文章的来源是 Istio 官方博客。\nAmbient 将 Istio 的功能分为两个不同的层，一个安全覆盖层和一个七层流量处理层。Waypoint 代理是一个可选组件，它基于 Envoy 并为其管理的工作负载进行七层流量处理。自 2022 年首次启动 Ambient 以来，我们进行了重大更改以简化路点配置、可调试性和可扩展性。\nWaypoint 代理的架构 与 sidecar 类似，waypoint 代理也是基于 Envoy 的，由 Istio 动态配置以服务于您的应用程序。Waypoint 代理的独特之处在于它运行每个命名空间（默认）或每个服务账户。通过在应用程序 pod 之外运行，waypoint 代理可以独立于应用程序安装、升级和扩展，并降低运营成 …","relpermalink":"/blog/waypoint-proxy-made-simple/","summary":"本文介绍了 Istio 的新的目的地导向的 waypoint 代理，它可以简化和扩展 Istio 的功能。文章介绍了 waypoint 代理的架构，部署方式，以及如何将源代理的配置转移到目的地代理，从而提高可扩展性，可调试性，一致性和安全性。文章还展示了如何使用 Istio 的策略和遥测来管理和监控 waypoint 代理。文章的来源是 Istio 官方博客。","title":"Istio Ambient 模式使用 Waypoint 代理简化 Istio 部署"},{"content":" 译者注：本文介绍了如何使用 OCI 容器来运行 WebAssembly 工作负载。WebAssembly（也称为 Wasm）是一种可移植的二进制指令格式，具有可嵌入和隔离的执行环境，适用于客户端和服务器应用。WebAssembly 可以看作是一种小巧、快速、高效、安全的基于栈的虚拟机，设计用于执行不关心 CPU 或操作系统的可移植字节码。WebAssembly 最初是为 web 浏览器设计的，用来作为函数的轻量级、快速、安全、多语言的容器，但它不再局限于 web。在 web 上，WebAssembly 使用浏览器提供的现有 API。WebAssembly System Interface（WASI）是为了填补 WebAssembly 和浏览器外系统之间的空白而创建的。这使得非浏览器系统可以利用 WebAssembly 的可移植性，使 WASI 成为分发和隔离工作负载时的一个很好的选择。文章中介绍了如何配置容器运行时来从轻量级容器镜像中运行 Wasm 工作负载，并给出了一些使用示例。\nWebAssembly（也称为 Wasm）以其可嵌入和隔离的执行环境而成为一种流行的便携式二进制指令格 …","relpermalink":"/blog/wasm-containers/","summary":"本文介绍了如何使用 OCI 容器来运行 WebAssembly 工作负载。WebAssembly（也称为 Wasm）是一种可移植的二进制指令格式，具有可嵌入和隔离的执行环境，适用于客户端和服务器应用。WebAssembly 可以看作是一种小巧、快速、高效、安全的基于栈的虚拟机，设计用于执行不关心 CPU 或操作系统的可移植字节码。WebAssembly 最初是为 web 浏览器设计的，用来作为函数的轻量级、快速、安全、多语言的容器，但它不再局限于 web。在 web 上，WebAssembly 使用浏览器提供的现有 API。WebAssembly System Interface（WASI）是为了填补 WebAssembly 和浏览器外系统之间的空白而创建的。这使得非浏览器系统可以利用 WebAssembly 的可移植性，使 WASI 成为分发和隔离工作负载时的一个很好的选择。文章中介绍了如何配置容器运行时来从轻量级容器镜像中运行 Wasm 工作负载，并给出了一些使用示例。","title":"使用 OCI 容器运行 WebAssembly 工作负载"},{"content":"客户向我们询问服务网格实践中关于开放策略代理 (OPA) 和服务网格如何结合使用的问题。我们探讨了关于服务网格和 OPA 策略的最佳实践，以及它们如何相互补充的想法。为了构建讨论框架，我们使用了 NIST 的零信任架构标准。在即将发布的 NIST 标准文档特别出版物 800-207A 中，基于身份的分段是一个主要概念。最低标准包括五项策略检查，应应用于进入的每个请求系统和每个后续跃点。您可以观看我们在今年的 CloudNativeSecurityCon 上与来自 NIST 的 Ramaswami Chandramouli 进行的深入讨论的演示。\n使用服务网格实现基于身份的分割的五个策略检查：\n传输中加密 服务身份和认证 服务到服务授权 最终用户身份和身份验证 最终用户对资源的授权 简而言之，服务网格是一个专用的基础设施层，专门用于为前四项检查实施策略，并在第五项中发挥一定作用。OPA 的亮点在于第五点：最终用户对资源的授权。\nIstio 的 sidecar 代理充当微服务应用程序的安全内核。Envoy 数据平面是一个通用的策略执行点 (PEP)，可以拦截所有流量并可以在应用层应用策略。 …","relpermalink":"/blog/understanding-istio-and-open-policy-agent-opa/","summary":"本文介绍了如何将服务网格和开放策略代理（OPA）结合使用，以实现基于身份的分割的五个策略检查。服务网格为前四项检查提供了执行点，而 OPA 则在第五项中发挥作用。本文还探讨了如何将特定于业务的策略应用于请求，以及如何使用 OIDC 或专用授权基础设施来实现最终用户对资源的授权。","title":"Istio 中的外部授权过滤器：使用 OPA 实现灵活的授权策略"},{"content":" 译者注：本文译自 Fusion Auth Developer。JSON Web Token（通常缩写为 JWT）是一种通常与 OAuth2 等标准协议一起使用的令牌。本文解释了 JWT 的组成部分和工作原理。\n在我们继续之前，重要的是要注意 JWT 通常被错误地称为 JWT Tokens。在末尾添加 Token 将会使其变成 JSON Web Token Token。因此，在本文中，我们省略末尾的 Token 并简单地称之为 JWT，因为这是更正确的名称。同样地，由于 JWT 通常用作身份验证和授权过程的一部分，一些人将其称为 Authentication Tokens 或 JWT Authentication Tokens。从技术上讲，JWT 只是一个包含 Base64 编码的 JSON 的令牌。它可以用于许多不同的用例，包括身份验证和授权。因此，在本文中，我们不使用这个术语，而是讨论如何在身份验证过程中使用 JWT。\n让我们开始吧！这是一个新生成的 JWT。为清楚起见添加了换行符，但它们通常不存在。 …","relpermalink":"/blog/jwt-components-explained/","summary":"JSON Web Token（通常缩写为 JWT）是一种通常与 OAuth2 等标准协议一起使用的令牌。本文解释了 JWT 的组成部分和工作原理。","title":"JWT 组件详解"},{"content":" 译者注：本文译自 Tetrate 博客。这篇文章介绍了 wazero，一个由 Tetrate 开发的用 Go 语言编写的 WebAssembly 运行时。wazero 可以让开发者用不同的编程语言编写代码，并在安全的沙箱环境中运行。wazero 有以下几个特点：\n纯 Go，无依赖，支持跨平台和跨架构 遵循 WebAssembly 核心规范 1.0 和 2.0 支持 Go 的特性，如并发安全和上下文传递 提供了丰富的编程接口和命令行工具 性能优异，超过了其他同类运行时 WebAssembly，也称为 Wasm，是一种编译用一种编程语言（例如 C 或 Rust）编写的代码并在不同的运行时（例如 Web 浏览器或微服务）上运行它的方法。这使得它成为编写插件、扩展以及在安全沙箱环境中运行任意用户定义代码的绝佳选择。\nWebAssembly 经常被误认为是一种仅限浏览器的技术，而实际上 Wasm 是一种跨平台的二进制文件，可以由任何 WebAssembly 运行时执行。从历史上看，Go 程序员没有太多好的选择，但这种情况已经改变。\n本文介绍了 wazero，它在用 Go 编程语言编写的基础设施 …","relpermalink":"/blog/introducing-wazero-from-tetrate/","summary":"这篇文章介绍了 wazero，一个由 Tetrate 开发的用 Go 语言编写的 WebAssembly 运行时。wazero 可以让开发者用不同的编程语言编写代码，并在安全的沙箱环境中运行。","title":"Tetrate 开源项目 Wazero 简介"},{"content":"本文作者 Bilgin Ibryam 是 Diagrid 的技术产品经理，致力于开发人员生产力工具。在此之前，他曾在 Red Hat 担任顾问和架构师，同时也是 Apache 软件基金会的提交者和成员。Bilgin 还与人合著了两本关于 Kubernetes 模式和 Camel 设计模式的书。在业余时间，Bilgin 喜欢通过博客和其他方式写作和分享他的知识。\n译者注：本文的原标题是《什么是云原生绑定应用》。本文介绍了云绑定应用程序的概念，并探讨了在使用云绑定应用程序时需要考虑的几个关键因素。首先，作者解释了云绑定应用程序是指在构建应用程序时使用云提供的服务和资源。作者强调了使用云绑定应用程序可以带来很多好处，例如降低成本和提高可靠性。然而，作者也指出了在使用云绑定应用程序时需要考虑的几个关键因素，包括云供应商锁定、数据隐私和网络连接可靠性等。最后，作者提供了一些建议，帮助企业在使用云绑定应用程序时避免潜在的风险。例如，选择具有高可用性的云服务提供商，并在使用云绑定应用程序时加强数据安全措施。\n关键要点 云提供商将重点从基础设施服务转移到开发人员直接使用的应用程序优先服务，从而产生了新 …","relpermalink":"/blog/cloud-bound-applications/","summary":"本文作者 Bilgin Ibryam 是 Diagrid 的技术产品经理，致力于开发人员生产力工具。在此之前，他曾在 Red Hat 担任顾问和架构师，同时也是 Apache 软件基金会的提交者和成员。Bilgin 还与人合著了两本关于 Kubernetes 模式和 Camel 设计模式的书。在业余时间，","title":"云原生绑定应用：一种让开发者专注于业务逻辑的新架构"},{"content":" 译者注：这篇文章介绍了 Istio 的 Rust-Based Ztunnel，它是一种基于 Rust 语言的轻量级代理，用于 Istio 的 ambient mesh。在文章中，作者解释了为什么需要一种新的代理，以及 Rust 语言是如何成为最佳选择的。文章还讨论了如何使用 workload xDS 配置来管理工作负载，以及如何查看 ztunnel 日志和 L4 指标。作者表示，Rust-Based Ztunnel 显著简化了 Istio 的 ambient mesh，并提高了性能。此外，Istio ambient mesh 已经合并到了上游主干，可以通过遵循入门指南来尝试 Rust-Based Ztunnel。\nZtunnel（零信任隧道）组件是为 Istio Ambient Mesh 专门构建的每节点代理。它负责安全地连接和验证 Ambient Mesh 中的工作负载。Ztunnel 旨在专注于 Ambient Mesh 中工作负载的一小组功能，例如 mTLS、身份验证、L4 授权和遥测，而无需终止工作负载 HTTP 流量或解析工作负载 HTTP 标头。Ztunnel 确保流量高 …","relpermalink":"/blog/rust-based-ztunnel/","summary":"这篇文章介绍了 Istio 的 Rust-Based Ztunnel，它是一种基于 Rust 语言的轻量级代理，用于 Istio 的 ambient mesh。在文章中，作者解释了为什么需要一种新的代理，以及 Rust 语言是如何成为最佳选择的。文章还讨论了如何使用 workload xDS 配置来管理工作负载，以及如何查看 ztunnel 日志和 L4 指标。作者表示，Rust-Based Ztunnel 显著简化了 Istio 的 ambient mesh，并提高了性能。此外，Istio ambient mesh 已经合并到了上游主干，可以通过遵循入门指南来尝试 Rust-Based Ztunnel。","title":"Istio Ambient Mesh 中基于 Rust 的 Ztunnel 组件介绍"},{"content":" 译者注：这篇文章从多个角度探讨了 WebAssembly（Wasm）的现状和未来。首先，文章引用了 Cloud Native Computing Foundation（CNCF）的报告，指出 WebAssembly 在网页、无服务器、游戏和容器化应用中的应用越来越广泛，并预测 WebAssembly 将显著影响这些应用。其次，文章讨论了 WebAssembly 在容器、边缘计算、编程语言和无服务器应用等方面的应用。虽然 WebAssembly 已经成熟地应用于浏览器，但是在后端应用方面，如边缘设备的应用和部署，仍需要更多的工作。WASI 已经成为将 WebAssembly 扩展到浏览器之外的最佳选择，可以帮助解决在任何配置正确的 CPU 上运行 WebAssembly 运行时的复杂性。WebAssembly 和容器的应用预计将共同增长，尽管 WebAssembly 在某些用例中可以取代容器，但总体来说，两者是互补的产品。WebAssembly 的未来看起来非常光明，但是在可靠和高效地支持 WebAssembly 在浏览器之外的生产用例方面，仍有很多工作要做。 …","relpermalink":"/blog/is-webassembly-really-the-future/","summary":"这篇文章从多个角度探讨了 WebAssembly（Wasm）的现状和未来。首先，文章引用了 Cloud Native Computing Foundation（CNCF）的报告，指出 WebAssembly 在网页、无服务器、游戏和容器化应用中的应用越来越广泛，并预测 WebAssembly 将显著影响这些应用。其次，文章讨论了 WebAssembly 在容器、边缘计算、编程语言和无服务器应用等方面的应用。虽然 WebAssembly 已经成熟地应用于浏览器，但是在后端应用方面，如边缘设备的应用和部署，仍需要更多的工作。WASI 已经成为将 WebAssembly 扩展到浏览器之外的最佳选择，可以帮助解决在任何配置正确的 CPU 上运行 WebAssembly 运行时的复杂性。WebAssembly 和容器的应用预计将共同增长，尽管 WebAssembly 在某些用例中可以取代容器，但总体来说，两者是互补的产品。WebAssembly 的未来看起来非常光明，但是在可靠和高效地支持 WebAssembly 在浏览器之外的生产用例方面，仍有很多工作要做。","title":"WebAssembly 真的代表着未来吗？"},{"content":" 译者注：这篇文章介绍了如何编写云原生网络功能（CNF），即在电信领域的网络应用，它们与大多数云原生企业应用有不同的非功能性需求。CNF 需要满足高性能、高可靠性、高安全性和低延迟等指标。文章提出了一个基本的设计原则：每个容器只负责一个关注点，即一个单一的网络功能或子功能。\n本文主旨 Docker 和 Kubernetes 文档都提倡将一个应用程序或每个容器“一个问题”打包的概念。这也可以作为每个应用程序和容器运行“一种进程类型”的指南。 基于电信的云原生网络功能 (CNF) 具有低延迟、高吞吐量和弹性等特定要求，这激发了多关注点/多进程类型的容器化方法。 使用多种进程类型实现的高性能电信应用程序应该探索使用 unix 域套接字而不是 TCP 或 HTTP 进行通信，因为这可以加快容器之间的通信。 微服务的详细和简明定义 很有价值。厚微服务可以是任何利用康威定律并按产品团队边界部署代码的东西。精益微服务是那些遵循粗粒度代码部署的服务，通常在容器中，具有单一的关注点。\nCloud Native Network Functions（CNFs）是电信领域的网络应用，非功能性需求不同于大多数云 …","relpermalink":"/blog/cloud-native-network-functions-concern/","summary":"这篇文章介绍了如何编写云原生网络功能（CNF），即在电信领域的网络应用，它们与大多数云原生企业应用有不同的非功能性需求。CNF 需要满足高性能、高可靠性、高安全性和低延迟等指标。文章提出了一个基本的设计原则：每个容器只负责一个关注点，即一个单一的网络功能或子功能。","title":"云原生网络功能（CNF）应该让每个容器聚焦一个关注点"},{"content":"Envoy Gateway (EG)首次公开发布 四个月后，我们很高兴地宣布发布 版本 0.3 起。这个最新版本是几位 Tetrate 同事和整个社区其他人辛勤工作的结晶。Envoy Gateway 现在支持整个 Kubernetes Gateway API，包括实验部分——添加了一些强大的新功能，使这个免费的开源软件更接近于功能齐全的 API 网关。\nEG 的一大特点是它配置了新的网关 API，而不是旧的和非常有限的 Ingress API，或任何为了弥补 Ingress 缺陷的专有 API。虽然 EG 0.2 实现了 Gateway API 的核心部分（完全支持“基本”HTTP 路由），但 EG 0.3 在其 Gateway API 支持方面更进了一步，这可能是了解其新功能的最佳方式：\n支持更多 HTTP 功能，例如URL Rewrite、Response Header Operation 和流量镜像。这些来自 API 规范中的扩展字段。 支持路由 gRPC、UDP 和原始 TCP。这些来自 API 的实验性新部分。 请注意这些 API 扩展：我们正在努力为真实用户提供有用的功能。 …","relpermalink":"/blog/envoy-gateways-latest-v0-3-release-extends-the-kubernetes-gateway-api/","summary":"Envoy Gateway 0.3 发布，对 Kubernetes Gateway API 的支持更进一步。","title":"Envoy Gateway 0.3 发布——扩展 Kubernetes Gateway API"},{"content":"我们从最简单的基于 IDP 的 RBAC 开始，最终将基于组的 RBAC 与细粒度的权限和细粒度的资源相结合。\n授权很复杂，因为每个应用程序都必须发明自己的授权模型。但是，有一些陈旧的路径可以作为大多数应用程序的良好起点。这篇文章将描述这些模式以及 Topaz 开源项目或 Aserto 授权服务等授权平台如何帮助你实施他们。\n角色作为用户属性 最简单的授权模式将一组角色建模为用户的属性。这些角色可以在身份提供者 (IDP) 中配置，并且通常作为范围嵌入到 IDP 生成的访问令牌中。\n一些应用程序完全基于嵌入在访问令牌中的角色（或离散权限）进行授权。但这有一些缺点：\n角色/权限/范围爆炸：角色/权限越多，访问令牌中需要嵌入的范围就越多，从而导致大小问题。 IDP 和应用程序之间的耦合：每当向应用程序添加新权限时，也必须修改访问令牌中生成其他范围的代码。这通常由有权访问 IDP 的安全/身份和访问团队完成，并且它引入了工作流程的复杂性。 一旦发布，访问令牌就很难失效。只要访问令牌有效，经过身份验证的用户就拥有权限，即使他们的角色在令牌颁发后发生了变化。这反过来又会导致安全漏洞。 在这种情况 …","relpermalink":"/blog/role-based-access-control-five-common-authorization-patterns/","summary":"本文描述了五种访问控制模式以及 Topaz 开源项目或 Aserto 授权服务等授权平台如何帮助你实施他们。","title":"基于角色的访问控制：五种常见的授权模型"},{"content":"下面是我所知道的关于将 Rust 编译为 WebAssembly 的所有知识。\n前一段时间，我写了一篇如何在没有 Emscripten 的情况下将 C 编译为 WebAssembly 的博客文章，即不默认工具来简化这个过程。在 Rust 中，使 WebAssembly 变得简单的工具称为 wasm-bindgen，我们正在放弃它！同时，Rust 有点不同，因为 WebAssembly 长期以来一直是一流的目标，并且开箱即用地提供了标准库布局。\nRust 编译 WebAssembly 入门 让我们看看如何让 Rust 以尽可能少的偏离标准 Rust 工作流程的方式编译成 WebAssembly。如果你浏览互联网，许多文章和指南都会告诉你使用 cargo init --lib 创建一个 Rust 库项目，然后将 crate-type = [\u0026#34;cdylib\u0026#34;] 添加到你的 cargo.toml，如下所示：\n[package] name = \u0026#34;my_project\u0026#34; version = \u0026#34;0.1.0\u0026#34; edition = \u0026#34;2021\u0026#34; [lib] crate-type = [\u0026#34;cdylib\u0026#34;] …","relpermalink":"/blog/rust-to-wasm/","summary":"关于将 Rust 编译为 WebAssembly 的所有知识。","title":"Rust 编译 WebAssembly 指南"},{"content":" 译者评论\n本文作者观点是：不应该将自由和开源软件（FOSS）置于你的软件供应链中，而是寻找他们的供应商，因为只有能够为软件负责任的供应商存在才能作为供应链的一环。\n在过去的几年里，我们看到了很多围绕软件供应链概念的讨论。这些讨论始于 LeftPad 时代，并随着过去几年发生的各种事件而升级。这个领域所有工作的问题在于它忘记了一个基本点。\n在开始讨论这个基本点之前，我将定义供应链和一般供应商的含义，以及我们申请软件的原因。那么为什么将 FOSS（自由和开源软件）置于该定义之下的尝试被深深地误导了。\n概念 在过去的几十年里，我们看到了 FOSS 的兴起。特别是，这可以极大地增加打包为库的代码片段的重用。这要归功于围绕这个想法蓬勃发展的庞大的基础设施生态系统。今天，每一种编程语言环境中都有一个包管理器，中央存储库保存着查找库和处理它们的分发所需的元数据。\n这是可能的，因为 FOSS 许可证非常宽松，并且可以重复使用和重新混合这些库，否则会出现的大量法律和财务问题。一个现代软件项目可能有成百上千个这样的依赖项，从 OpenSSL 到测试框架或日期选择器，涵盖诸如 JSON 编码器/解码器库甚 …","relpermalink":"/blog/not-a-supplier/","summary":"在过去的几十年里，我们看到了自由和开源软件 (FOSS) 的兴起。这使得打包为库的代码段的重用大幅增长。包管理器适用于当今世界上的每一种编程语言，它们的中央存储库保存着查找库和处理它们的分发所需的元数据。","title":"我不是供应商"},{"content":"这是 服务网格最佳实践系列文章 中的第三篇，摘自 Tetrate 创始工程师 Zack Butcher 即将出版的新书 Istio in Production。\nIstio 就像一组乐高积木：它具有许多功能，可以按照您想要的任何方式进行组装。出现的结构取决于您如何组装零件。在 上一篇中，我们描述了一种运行时拓扑结构，用于构建健壮、有弹性且可靠的基础架构。在本文中，我们将描述一组网格配置，以帮助在运行时实现稳健性、弹性、可靠性和安全性。\nIstio 在其所谓的 根命名空间 中支持全局默认配置 —— 默认在 istio-system。在根命名空间中发布的配置默认适用于所有服务，但在本地命名空间中发布的任何配置都会覆盖它。因此，一些配置应该在根命名空间中发布，并且不允许在其他任何地方发布（例如用于在传输中强制加密的 PeerAuthentication 策略）。其他配置应该在每个服务自己的命名空间中编写（例如 VirtualService 控制它的弹性设置）。\n我们看到的最成功的网格采用将网格本身隐藏在另一个界面后面：例如 Helm 模板、Terraform 或更高级的解决方案， …","relpermalink":"/blog/optimize-traffic-management-and-security-with-these-service-mesh-best-practices/","summary":"本文推荐了服务网格安全性优化的一些最佳实践。","title":"服务网格安全性优化最佳实践"},{"content":"这是 服务网格最佳实践系列文章 中的第二篇，摘自 Tetrate 创始工程师 Zack Butcher 即将出版的书籍 Istio in Production。\n当涉及到在多集群的基础设施中部署服务网格时，有一些可移动的部分。这里主要想强调的是控制平面应该如何部署在应用程序附近，入口应该如何部署以促进安全性和敏捷性，如何使用 Envoy 促进跨集群负载均衡，以及网格内部如何使用证书。\n使服务网格控制平面与故障域保持一致 建议：围绕故障域部署松散耦合的控制平面以实现高可用性。\n构建高可用性系统可能具有挑战性，而且通常成本很高。我们知道的一种经过验证的技术是围绕故障域构建。故障域是当关键系统发生故障时受影响的基础架构部分。我们构建可靠系统的基本方法是将系统跨越的故障域分组为多个独立的孤岛。最终系统的整体可靠性取决于我们可以使孤岛的独立程度。实际上，总是存在一些相互依赖性，将其最小化总是成本与可用性的权衡。\n在没有耦合故障域的情况下创建隔离孤岛的最简单方法是在每个孤岛中运行关键服务的独立副本。我们可以说这些副本是筒仓的本地副本 —— 它们共享相同的故障域。在云原生架构中，Kubernetes …","relpermalink":"/blog/service-mesh-deployment-best-practices-for-security-and-high-availability/","summary":"本文强调的是控制平面应该如何部署在应用程序附近，入口应该如何部署以促进安全性和敏捷性，如何使用 Envoy 促进跨集群负载均衡，以及网格内部如何使用证书。","title":"服务网格安全性和高可用性部署最佳实践"},{"content":"本文是 Tetrate 即将出版的《Istio in Production》一书中摘录的服务网格最佳实践系列的第一篇，作者是 Tetrate 创始工程师 Zack Butcher。\n我们接到许多实施网格的企业的问题，其中之一是“我还需要哪些控制，而网格提供哪些控制？”换句话说，他们想知道网格如何适应现有的安全模型。我们发现，网格最适合作为一组安全控制的内圈，这些控制从物理网络到应用本身的每一层都被实施。\n服务网格作为通用策略执行点 我们看到网格的 Sidecar 作为通用策略执行点（NIST SP 800-204B：使用服务网格的基于属性的访问控制）。由于它拦截了所有进出应用程序的流量，Sidecar 为我们提供了一个强大的位置来实现各种策略。我们可以实现传统的安全策略，如基于应用程序标识而非网络位置的更高保证的应用程序之间的授权。但我们也可以实施之前不切实际或需要与应用程序深度参与的策略。例如，网格允许您编写以下策略：“后端可以从数据库读取（使用应用级身份进行身份验证和授权），但前提是请求具有有效的最终用户凭证并具有读取范围（使用最终用户身份进行身份验证和授权）。”\n虽然服务网格提供 …","relpermalink":"/blog/how-service-mesh-layers-microservices-security-with-traditional-security-to-move-fast-safely/","summary":"我们认为，服务网格最适合作为现有安全模型的一部分，通过在传统安全控制之上添加更细粒度的安全策略来实现。","title":"如何将服务网格作为安全模型的一部分，以分层形式将微服务安全与传统安全结合起来"},{"content":"Istio 的核心功能之一是通过管理网格中服务的身份来促进零信任网络架构。为了在网格中检索用于 mTLS 通信的有效证书，各个工作负载向 istiod 发出证书签名请求 (CSR)。Istiod 反过来验证请求并使用证书颁发机构（CA）签署 CSR 以生成证书。默认情况下，Istio 为此目的使用自己的自签名 CA，但最佳实践是通过为每个 Istio 部署创建一个中间 CA，将 Istio 集成到您现有的 PKI 中。\n如果您正在管理多个集群，这意味着颁发多个中间 CA，每个中间 CA 都应设置为在几个月或更短的时间内到期。管理这些 CA 的生命周期至关重要，因为它们必须在过期或坏事发生之前进行轮换。本文将向您展示如何简化此 CA 管理以降低风险并提高系统的整体稳定性。\n轮换 CA 时的一个关键步骤是确保实际使用新的 CA。默认情况下，Istio 仅在启动时加载其 CA。但是，Istio 可以配置为监视更改并在更新时自动重新加载其 CA。本教程取自我们与管理大量 Istio 部署的企业客户合作开发的生产手册，将展示如何配置 Istio 以自动重新加载其 CA。 …","relpermalink":"/blog/automate-istio-ca-rotation-in-production-at-scale/","summary":"本文将向您展示如何简化 Istio 中的 CA 管理以降低风险并提高系统的整体稳定性。","title":"在生产中大规模自动化 Istio CA 轮换"},{"content":"当我们与想要使用 Istio 的客户或用户交流时，这一个问题时长会出现——Istio 中的证书信任如何工作的？Istio 有自己的证书颁发机构，而我们也有自己的证书颁发机构，如何确保它们相互信任？\n简而言之，通过中间签名证书将 Istio 纳入到您现有的信任链中。\n如果您使用 Istio 作为演示或开箱即用，它将拥有自己的自签名证书 —— 它是自己的根证书。对于在多个集群中运行 Istio 的用户来说，这是一个常见的痛点：他们无意中创建了两个互不不信任的孤岛，因此没有安全通信。\n以下是如何通过让 Istio 信任您现有的 PKI 的步骤。\n简述 这是简短的版本：您应该通过为每个 Istio 部署创建一个中间签名证书来让 Istio 信任您现有的 PKI（并且每个集群应该有一个 Istio 部署）。然后你会：\n启用跨 Istio 部署的通信 允许细粒度的证书撤销，而无需同时在整个基础架构中强制使用新证书（如果这听起来像是等待发生的重大中断，那么您是对的）。 启用签名证书的轻松轮换。您需要做的就是创建一个新的中间件并使用新证书重新启动 Istio。因为它在同一个信任根中，所以一切都继续工 …","relpermalink":"/blog/istio-trust/","summary":"本文讲解了如何让 Istio 信任现有 PKI 的步骤。","title":"将 Istio 纳入信任链：使用现有 PKI 作为信任根"},{"content":"在本文中，我们将探讨如何使用 Hashicorp Vault 作为一种比使用 Kubernetes Secret 更安全的方式来存储 Istio 证书。默认情况下，Secret 使用 base64 编码存储在 etcd 中。在安全策略严格的环境中，这可能是不可接受的，因此需要额外的措施来保护它们。一种此类解决方案涉及将机密存储在外部机密存储提供程序中，例如 HashiCorp Vault。\nVault 可以托管在 Kubernetes 集群内部和外部。在本案例中，我们将探索使用托管在 Kubernetes 外部的 Vault，以便它可以同时为多个集群提供秘密。该设置也非常适合探索 Istio 的多集群功能，它需要一个共享的信任域。\n利用 vault-agent-init 容器，我们可以将证书和私钥材料注入实际的 Istio 控制平面 Pod，以便它们使用外部 CA 证书进行引导。这避免了依赖 Secret 来引导 Istio 控制平面。该技术也完全适用于入口和出口证书。\n有关如何在 Istio 中使用和管理证书的更多信息，请参见官方文档：\n身份和证书管理 插入 CA …","relpermalink":"/blog/how-to-use-hashicorp-vault-as-a-more-secure-way-to-store-istio-certificates/","summary":"本文将指导你使用 Vault 存储 Istio 的证书。","title":"如何使用 Hashicorp Vault 作为一种更安全的方式来存储 Istio 证书"},{"content":"传统的网络安全依赖于围绕可信内部网络的强大防御边界，以将不良行为者拒之门外，将敏感数据拒之门外。在日益复杂的网络环境中，维护强大的边界越来越困难。\n零信任安全正在成为企业保护其传统和现代云原生应用程序的首选方法。零信任网络架构颠覆了边界安全的假设。在零信任网络中，每个资源都在内部受到保护，就好像它暴露在开放的互联网中一样。\n为了为行业和美国联邦政府建立零信任安全指南，美国国家标准与技术研究院 (NIST) 在一系列出版物中建立了零信任安全指南，从 SP 800-207 开始，介绍一般的零信任架构及其配套SP 800-204 微服务安全标准系列。\n以下是 NIST 的核心零信任架构原则以及建议在实践中应用它们的 Kubernetes 和 Istio 参考架构。\n零信任网络的六项原则 无论网络位置如何，所有通信都应该是安全的。网络位置和可达性并不意味着信任。企业拥有或其他专用网络内部的访问请求必须满足与来自任何其他位置的通信相同的安全要求。零信任系统的一个标准是，您可以将它暴露在开放的互联网上，并且它仍然是安全的，没有未经授权的系统、数据或通信访问。 所有通信都应加密。线路上的加密可防止窃 …","relpermalink":"/blog/the-top-6-zero-trust-principles-for-kubernetes-security/","summary":"本文是 NIST 的核心零信任架构原则以及建议在实践中应用它们的 Kubernetes 和 Istio 参考架构。","title":"Kubernetes 安全的 6 大零信任原则"},{"content":"Kubernetes 是编排现代云原生工作负载的事实标准。但是，它不提供开箱即用的安全通信。这意味着每个需要实施传输中加密以对其 Kubernetes 部署采用零信任安全态势的人都需要自己解决这个问题。\n幸运的是，有很多易于理解的方法可以实现，在本文中，我们将介绍在 Kubernetes 中实现双向 TLS（mTLS）的三大最佳实践。\n什么是 mTLS，为什么对安全来说很重要？ 传输层安全性（SSL 的后继者）是部署最广泛的安全通信标准，在 HTTPS 中最为明显。TLS 非常适合在需要向客户端证明其身份的服务器之间建立既保密（防窃听）又真实（防篡改）的安全通信。但是，在双方都需要向对方证明身份的情况下（例如在 Kubernetes 应用程序中的微服务之间），TLS 是不够的。\n这就是双向 TLS (mTLS) 的用武之地。mTLS 是 TLS，但双方在建立安全通信通道之前向对方证明自己的身份。这是 Kubernetes 中安全通信所需的必要部分。mTLS 提供：\n在线加密以确保机密性和防篡改 相互的、加密的安全身份证明以确保真实性 要深入了解 mTLS 的工作原理， …","relpermalink":"/blog/top-3-mtls-best-practices-for-zero-trust-kubernetes-security/","summary":"我们将介绍在 Kubernetes 中实现双向 TLS（mTLS）的三大最佳实践。","title":"零信任 Kubernetes 安全的三大 mTLS 最佳实践"},{"content":"在这篇文章中，我们将亲身体验 Envoy Gateway 和 Gateway API。以下是逐步指导你安装 Envoy Gateway 的说明，以及通过 Envoy 代理在集群外公开 HTTP 应用程序的简单用例。\n如果你不方便运行，我在本文中包含了每个命令的输出，即使你没有 Kubernetes 集群也可以看到它是如何工作的。\n如果你是 GUI 的粉丝，在文章的最后我会附上 Tetrate 基于 Backstage 的概念验证 Envoy Gateway GUI 的屏幕截图和详细信息，以展示针对 Gateway API 构建此类东西是多么容易。\n创建 Kubernetes 集群 首先运行 Envoy Gateway 和 Kubernetes 集群。最简单、最安全的方法是使用 minikube 在本地机器上启动集群。\n$ minikube start –driver=docker --cpus=2 --memory=2g 😄 minikube v1.27.0 on Arch 22.0.0 (x86_64) ▪ KUBECONFIG=... ❗ For more information, …","relpermalink":"/blog/hands-on-with-envoy-gateway/","summary":"最近 Envoy Gateway 0.2 发布了，API 网关的生态系统迎来了新的变化。这篇文章将想你介绍 Kubernetes API 网关领域的最新进展。","title":"使用 Envoy Gateway 0.2 体验新的 Kubernetes Gateway API"},{"content":"最近 Envoy Gateway 0.2 发布了，API 网关的生态系统迎来了新的变化。这篇文章将想你介绍 Kubernetes API 网关领域的最新进展。\n如何将外部的网络请求路由到 Kubernetes 集群？你可以使用入口控制器：一组 HTTP 反向代理，将流量转接到集群中，并由 operator 来管理。也可以使用 Ambassador、Contour、Traefik 或 HAproxy 这类软件。还可以使用云提供商的解决方案，或者只是用默认的的 Nginx Ingress。或者你可能使用一个功能更全面的 API 网关，如 Tyk 或 Kong，或者在 Kubernetes Ingress 前面的另一层有一个单独的网关，如 AWS 的 API 网关，或内部的 F5，可以选择的实在太多。\n为什么我们需要一个新的入口控制器 因为很多入口控制器都有不同程度的限制：有些是基于旧的技术，如 Nginx、HAproxy，甚至是基于 Apache 建立的。这些技术的特性不适用于云原生环境，比如在配置改变时放弃已建立的连接（如果你想深入了解，Ambassador 发表了一篇比较文章）。云供应 …","relpermalink":"/blog/envoy-gateway-to-the-future/","summary":"最近 Envoy Gateway 0.2 发布了，API 网关的生态系统迎来了新的变化。这篇文章将向你介绍 Kubernetes API 网关领域的最新进展。","title":"面向未来的网关：新的 Kubernetes Gateway API 和 Envoy Gateway 0.2 介绍"},{"content":"Istio最近宣布了“Ambient Mesh” —— 一种用于 Istio 的实验性“无 sidecar”部署模型。我们最近在从服务网格中获得最大性能和弹性的背景下写了关于 sidecar 与 sidecar-less 的文章。在本文中，我们将特别介绍我们对 Ambient Mesh 的看法。\n如果你想立即开始使用可用于生产的 Istio 发行版，请尝试Tetrate Istio Distro (TID)。TID 是经过审查的 Istio 上游发行版，它易于安装、管理和升级，基于适用于 FedRAMP 环境的 FIPS 认证构建。如果你需要一种统一且一致的方式来保护和管理一组应用程序中的服务，请查看Tetrate Service Bridge (TSB)，这是我们基于 Istio 和 Envoy 构建的全面的边缘到工作负载应用程序连接平台。\n什么是 Ambient Mesh？ Ambient Mesh 是最近引入 Istio 的一种实验性新部署模型。它将 Envoy sidecar 当前执行的职责分为两个独立的组件：一个用于加密的节点级组件（称为“ztunnel”）和一个为每个服务账 …","relpermalink":"/blog/what-is-ambient-mesh/","summary":"我们对任何能够使服务网格的采用更加容易的事情都充满热情，但我们还不确定 Ambient Mesh 的部署模型能否能兑现这一承诺。","title":"什么是 Ambient Mesh？"},{"content":"我们最近发布了 Istio Ambient Mesh（译者注：笔者更倾向于将其称为 Ambient Mode，即外围模式，但译文中仍然保留了 Ambient Mesh 的叫法），它是 Istio 的无 sidecar 数据平面。如公告博客中所述，我们使用 Ambient Mesh 解决的首要问题是简化操作、更广泛的应用程序兼容性、降低基础设施成本和提高性能。在设计 ambient 数据平面时，我们希望在不牺牲安全性或功能的情况下仔细平衡运维、成本和性能方面的问题。随着 ambient 组件在应用程序 pod 之外运行，安全边界发生了变化 —— 我们相信会变得更好。在这篇博客中，我们将详细介绍这些安全性变化以及它们与 sidecar 部署模式在安全性方面的对比。\nAmbient Mesh 的分层示意图 回顾一下，Istio Ambient Mesh 引入了一个分层网格数据平面，它具有负责传输安全和路由的安全覆盖层，可以选择为需要它们的命名空间添加 L7 功能。要了解更多信息，请参阅公告博客和入门博客。安全覆盖层包括一个节点共享组件 ztunnel，它负责 L4 …","relpermalink":"/blog/ambient-security/","summary":"深入研究最近发布的 Istio Ambient Mesh（Istio 的无 sidecar 数据平面）的安全隐患。","title":"Istio 服务网格 ambient 模式安全详解"},{"content":"今天，我们很高兴地介绍 Ambient Mesh（译者注：笔者更倾向于将其称为 Ambient Mode，即外围模式），这是一种新的 Istio 数据平面模式，旨在简化操作、扩大应用兼容性并降低基础设施成本。用户可以选择将 Ambient Mesh 集成到其基础设施的网格数据平面，放弃 sidecar 代理，同时保持 Istio 的零信任安全、遥测和流量管理等核心功能。我们正在与 Istio 社区分享 Ambient Mesh 的预览版，我们正努力在未来几个月内将其推向生产就绪。\nIstio 和 Sidecar 自项目成立以来，Istio 架构的一个决定性特征是使用 sidecar—— 与应用容器一起部署的可编程代理。利用 sidecar，不需要对应用程序进行重大调整即可以享受服务网格带来的好处，省去运维的负担。\nIstio 的传统模式是将 Envoy 代理作为 sidecar 部署在工作负载的 pod 中。 虽然 sidecar 比起重构应用程序有显著的优势，但它们并没有在应用程序和 Istio 数据平面之间提供一个完美的分离。这导致了一些限制：\n侵入性：sidecar 必须通过修改 …","relpermalink":"/blog/introducing-ambient-mesh/","summary":"Ambient Mesh（外围模式）是一种新的 Istio 数据平面模式，旨在简化操作、扩大应用兼容性并降低基础设施成本。","title":"Istio 无 sidecar 代理数据平面 ambient 模式简介"},{"content":"本文译自 Cloud Native Network Functions Are Here，译者 Jimmy Song。\n主要收获 云原生网络不是另一种方式的 SDN，它以一种完全不同的方式来看待网络。 虽然 SDN 似乎是把物理网络和机器做了虚拟化，但「云原生网络功能」（Cloud-native Network Functions，下文简称 CNF）不仅仅是容器化的网络和虚拟机，它还将网络功能分割成服务，这是 CNF 与 SDN 的一个主要区别。 CNF 是 OSI 网络模型中的网络功能（越底层实现起来就越困难），这些功能是根据云原生实践实现的。 虽然 SDN 数据平面（这里指的是转发数据包）位于硬件 ASIC 上，或在传统内核网络转发的虚拟化盒子里，但 CNF 探索用户平面转发或更新的 eBPF 数据路径转发。 在云原生数据中心中，偏向于三层的解决方案，但 CNF 的一大驱动力是电信服务提供商，他们经常下降到二层的功能。 在三类云资源（计算、存储和网络）中，网络似乎最难满足云原生的非功能性需求。例如，计算弹性可以通过虚拟机、容器和编排器合理分配，并通过 CI/CD 管道进行管理。网络 …","relpermalink":"/blog/cloud-native-network-functions/","summary":"这篇文章中，我们展示了云原生网络功能将网络应用引入云原生世界的一种尝试。究竟什么是 CNF，为什么它们很重要？","title":"云原生网络功能（CNF）介绍"},{"content":"关键要点 零信任通过有选择地只允许访问用户应该被允许访问的特定资源，解决了开放网络访问的问题。 实现持续验证的关键策略是零信任网络访问 (ZTNA)。 实施零信任有助于解决 SOC（安全运营中心）或安全分析师角色中的组织技能短缺问题。 在零信任环境中，开发人员必须全面了解如何保护请求者与应用程序交互的每一步，同时考虑当前的安全上下文。 零信任框架并没有消除在每次部署后持续扫描漏洞的需要，漏洞扫描可以确保应用程序和后端系统保持保护和运作。 什么是零信任模型？ 零信任安全模型是一种设计和实施安全 IT 系统的方法。零信任背后的基本概念是“从不信任，始终验证”。这意味着用户、设备和连接在默认情况下永远不会被信任，即使它们连接到公司网络或之前已经过身份验证。\n现代 IT 环境由许多互连的组件组成，包括本地服务器、基于云的服务、移动设备、边缘位置和物联网 (IoT) 设备。依赖于保护所谓的“网络边界”的传统安全模型在这种复杂的环境中是无效的。\n攻击者可以破坏用户凭据并访问防火墙后面的本地系统。\n他们还可以访问在组织控制之外部署的基于云的或物联网资源。零信任方法在受保护资产周围建立微边 …","relpermalink":"/blog/zero-trust-developer-guide/","summary":"关于零信任，你需要了解这些知识。","title":"开发者需要了解的零信任知识"},{"content":"背景介绍 Apache SkyWalking 观察部署在服务网格中的服务的度量、日志、追踪和事件。在进行故障排除时，SkyWalking 错误分析是一个宝贵的工具，可以帮助确定错误发生的位置。然而，确定性能问题更加困难：利用预先存在的观察数据往往不可能找到性能问题的根本原因。为此，动态调试和故障排除在进行服务性能剖析时就必不可少。在这篇文章中，我们将讨论如何使用 eBPF 技术来改进 SkyWalking 中的剖析功能，并用于分析服务网格中的性能影响。\nSkyWalking 中的追踪剖析 自 SkyWalking 7.0.0 以来，Trace Profiling 通过定期对线程堆栈进行采样，让开发者知道运行哪行代码花费更多时间，从而帮助开发者发现性能问题。然而，Trace Profiling 不适合以下情况：\n线程模型：Trace Profiling 对于剖析在单线程中执行的代码最有用。它对严重依赖异步执行模式的中间件不太有用。例如，Go 中的 Goroutines 或 Kotlin Coroutines。 语言：目前，Trace Profiling 只支持 Java …","relpermalink":"/blog/pinpoint-service-mesh-critical-performance-impact-by-using-ebpf/","summary":"在这篇文章中，我们将讨论如何使用 eBPF 技术来改进 SkyWalking 中的剖析功能，并用于分析服务网格中的性能影响。","title":"使用 eBPF 准确定位服务网格的关键性能问题"},{"content":"编者的话 本文是来自 Tetrate 工程师的分享，Tetrate 的拳头产品是 Tetrate Service Bridge（下文简称 TSB），它是在开源的 Istio 和 Envoy 基础上构建的，但为其增加了管理平面。\n简介 Tetrate 的应用连接平台 Tetrate Service Bridge（TSB）提供两种网关类型，分别为一级网关（Tier-1）和二级网关（Tier-2），它们都基于 Envoy 构建，但是目的有所不同。本文将探讨这两种类型网关的功能，以及何时选用哪种网关。\n关于两级网关的简要介绍：\n一级网关（下文简称 T1）位于应用边缘，用于多集群环境。同一应用会同时托管在不同的集群上，T1 网关将对该应用的请求流量在这些集群之间路由。 二级网关（下文简称 T2）位于一个的集群边缘，用于将流量路由到该集群内由服务网格管理的服务。 两级网关释义 托管在 TSB 管理的集群中的应用部署的设计与开源的 Istio 模型非常相似。它们的结构相同，使用入口网关来路由传入的流量。T2 网关相当于 Istio 的入口网关（Ingress Gateway），在逻辑上与 Istio …","relpermalink":"/blog/designing-traffic-flow-via-tier1-and-tier2-ingress-gateways/","summary":"在 Kubernetes 中我们不能仅依靠网络层加密，还需要 mTLS 来对客户端和服务端进行双向的传输层认证。本文将聚焦于 TLS 的真实性，以及证书管理的难题，说明服务网格对于在 Kubernetes 中开启 mTLS 带来的便利。","title":"通过两级网关设计来路由服务网格流量"},{"content":"编者的话 本文翻译节选自 A Kubernetes engineer’s guide to mTLS，为了便于读者理解，笔者对原文做了一点修改 1。因为笔者最近在研究 Istio 中的身份认证及 SPIFFE，对如何在 Kubernetes 中应用 mTLS 以及为什么要使用 mTLS 产生了浓厚的兴趣，再回想起五年前手动安装 Kubernetes 时，因为给集群开启 TLS 问题而导致安装停滞不前。\n本文的主要观点是：在 Kubernetes 中我们不能仅依靠网络层加密，还需要 mTLS 来对客户端和服务端进行双向的传输层认证。本文将聚焦于 TLS 的真实性，以及证书管理的难题，说明服务网格对于在 Kubernetes 中开启 mTLS 带来的便利。\n简介 Mutual TLS（双向 TLS），或称 mTLS，是 Kubernetes 中的一个热门话题，尤其是对于那些负责为应用程序提供传输层加密的人来说。但是，你有没有考虑过，什么是 mTLS，它提供什么样的安全，为什么需要 mTLS？\n本指南我将介绍什么是 mTLS，它与常规 TLS 的关系，以及为什么它与 Kubernetes 有 …","relpermalink":"/blog/mtls-guide/","summary":"在 Kubernetes 中我们不能仅依靠网络层加密，还需要 mTLS 来对客户端和服务端进行双向的传输层认证。本文将聚焦于 TLS 的真实性，以及证书管理的难题，说明服务网格对于在 Kubernetes 中开启 mTLS 带来的便利。","title":"写给 Kubernetes 工程师的 mTLS 指南"},{"content":"编者的话 Istio 1.14 版本增加了对 SPIRE 集成的支持，这篇文章将指导你如何在 Istio 中集成 SPIRE。\nSPIRE 是 SPIFFE 规范的一个生产就绪的实现，它可以执行节点和工作负载证明，以便安全地将加密身份发给在异构环境中运行的工作负载。通过与 Envoy 的 SDS API 集成，SPIRE 可以被配置为 Istio 工作负载的加密身份来源。Istio 可以检测到一个 UNIX 域套接字的存在，该套接字在定义的套接字路径上实现了 Envoy SDS API，允许 Envoy 直接从它那里进行通信和获取身份。\n这种与 SPIRE 的集成提供了灵活的认证选项，这是默认的 Istio 身份管理所不具备的，同时利用了 Istio 强大的服务管理。例如，SPIRE 的插件架构能够提供多样化的工作负载认证选项，超越 Istio 提供的 Kubernetes 命名空间和服务账户认证。SPIRE 的节点认证将认证扩展到工作负载运行的物理或虚拟硬件上。\n关于这种 SPIRE 与 Istio 集成的快速演示，请参阅通过 Envoy 的 SDS API 将 SPIRE …","relpermalink":"/blog/istio-spire-integration/","summary":"Istio 1.14 版本增加了对 SPIRE 集成的支持，这篇文章将指导你如何在 Istio 中集成 SPIRE。","title":"如何在 Istio 中集成 SPIRE"},{"content":"前言 OpenTelemetry 追踪包含了理解分布式系统和排除故障的信息宝库 —— 但你的服务必须首先被指标化，以发射 OpenTelemetry 追踪来实现这一价值。然后，这些追踪信息需要被发送到一个可观察的后端，使你能够获得关于这些数据的任意问题的答案。可观测性是一个分析问题。\n本周早些时候，我们部分解决了这个问题，宣布在 Promscale 中普遍提供 OpenTelemetry 追踪支持，将由 SQL 驱动的可观测性带给所有开发者。随着对分析语言 ——SQL 的全面支持，我们解决了分析的问题。但我们仍然需要解决第一部分的问题：测量。\n为了让你的服务发出追踪数据，你必须手动添加 OpenTelemetry 测量工具到代码中。而且你必须针对所有服务和你使用的所有框架来做，否则你将无法看到每个请求的执行情况。你还需要部署 OpenTelemetry 收集器来接收所有新的追踪，处理它们，批处理它们，并最终将它们发送到你的可观测性后端。这需要花费大量的时间和精力。\n如果你不需要做所有这些手工工作，并且可以在几分钟内而不是几小时甚至几天内启动和运行呢？如果你还能建立一个完整的可观测性技术 …","relpermalink":"/blog/generate-and-store-opentelemetry-traces-automatically/","summary":"首先，我们将解释一下如何在 Kubernetes 自动生成和存储 OpenTelemetry 追踪，剖析 OpenTelemetry Operator 在内部的真正作用。接下来，我们将通过一个例子演示如何将其直接付诸实践。","title":"一键开启 Kubernetes 可观测性——如何自动生成和存储 OpenTelemetry 追踪"},{"content":"Envoy 基础教程已上线 Tetrate 学院，请转到 新地址 免费参与学习。\n","relpermalink":"/envoy-handbook/","summary":"免费的中文 Envoy 基础教程已上线 Tetrate 学院。","title":"Envoy 基础教程"},{"content":"Istio 基础教程已上线 Tetrate 学院，请转到 新地址 免费参与学习。\n","relpermalink":"/istio-handbook/","summary":"免费的中文 Istio 基础教程已上线 Tetrate 学院。","title":"Istio 基础教程"},{"content":"主要收获 对于现代软件系统来说，可观测性不是关于数学方程。它是关于人类如何与复杂的系统互动并试图理解它们。\n混沌工程利用了可观测性，因为它可以检测到系统稳定状态的偏差。混沌工程借助可观测性可以发现和克服系统的弱点。\n可观测性依赖于系统所发出的信号，这些信号提供了关于系统行为的原始数据。然而，可观测性不仅受限于这些信号的质量，还受限于这些信号的可视化和解释的方式。\n考虑到混沌工程、可观测性和可视化涉及到人类自我的解释，仪表盘的设计者可能会对这些解释产生偏差，这是一个事实。在这个意义上，视觉隐喻并不能保证我们以正确的方式解释这些数据。\n基于视觉隐喻的仪表盘可以提供比经典的可视化更有用的数据。然而，这两种策略都很容易产生偏差；例如，在一项研究中，大多数参与者都注意到，由于显示了糟糕的柱状图和线状图，没有在图中显示出重要的分界点，因此整体结果是有偏差的。\n自从 Netflix、Slack 和 Linkedin 等领先的技术公司采用混沌工程来抵御生产中的意外中断后，这门学科在近来已经成为主流。在这条道路上，可观测性发挥了关键作用，为工程师们带来了数据和监控的力量，他们现在有了了解自己系统的策略， …","relpermalink":"/blog/chaos-engineering-observability-visual-metaphors/","summary":"本文为可观测性引入了一个新的角色：视觉隐喻，并进行了一项研究，对比了视觉隐喻与传统表示的结果。","title":"混沌工程和视觉隐喻的可观测性"},{"content":"前言 今天，Envoy 社区宣布了一个令人兴奋的新项目：Envoy Gateway。该项目将行业领导者联合起来，精简由 Envoy 驱动的应用网关的好处。这种方法使 Envoy Gateway 能够立即为快速创新打下坚实的基础。该项目将提供一套服务来管理 Envoy 代理机群，通过易用性来推动采用，并通过定义明确的扩展机制来支持众多的用例。\n我们为什么要这样做？ Tetrate 是 Envoy Proxy 的第一贡献者（按提交量计算），也是 Envoy Gateway 指导小组的成员，其贡献者涵盖技术和管理领域。我们相信，我们强大的伙伴关系和在开源软件方面的深厚经验将有助于确保 Envoy Gateway 的成功。Tetrate 推动了 EG 计划，因为我们致力于上游项目，因为我们相信这将降低 Envoy Proxy 用户的进入门槛，也因为这与我们开发服务网格作为零信任架构基础的使命相一致。Tetrate 将大力投资建设 Envoy Gateway 的安全功能，包括支持 OAuth2 和 Let’s Encrypt 集成等 API 功能。\n对上游项目的承诺 Tetrate 从第一天起就 …","relpermalink":"/blog/the-gateway-to-a-new-frontier/","summary":"在我们看来，由于 Envoy 的设计、功能设置、安装基础和社区，它是业内最好的 API 网关。有了 Envoy Gateway，企业可以在将 Envoy 嵌入其 API 管理策略方面增加信心。","title":"Envoy API Gateway——推动网关的进一步发展"},{"content":"前言 今天，我们很高兴地宣布 Envoy Gateway 成为 Envoy 代理家族的新成员，该项目旨在大幅降低将 Envoy 作为 API 网关的使用门槛。\n历史 Envoy 在 2016 年秋天开源，令我们惊讶的是，它很快就引领了整个行业。用户被这个项目的许多不同方面所吸引，包括它的包容性社区、可扩展性、API 驱动的配置模型、强大的可观测性输出和越来越广泛的功能集。\n尽管在其早期历史中，Envoy 成为了服务网格的代名词，但它在 Lyft 的首次使用实际上是作为 API 网关 / 边缘代理，提供深入的可观测性输出，帮助 Lyft 从单体架构迁移到微服务架构。\n在过去的 5 年多时间里，我们看到 Envoy 被大量的终端用户采用，既可以作为 API 网关，也可以作为服务网格中的 sidecar 代理。同时，我们看到围绕 Envoy 出现了一个庞大的供应商生态系统，在开源和专有领域提供了大量的解决方案。Envoy 的供应商生态系统对项目的成功至关重要；如果没有对所有在 Envoy 上兼职或全职工作的员工的资助，这个项目肯定不会有今天的成就。\nEnvoy 作为许多不同的架构类型和供应商 …","relpermalink":"/blog/introducing-envoy-gateway/","summary":"今天，我们很高兴地宣布 Envoy Gateway 成为 Envoy 代理家族的新成员，该项目旨在大幅降低将 Envoy 作为 API 网关的使用门槛。","title":"开源项目 Envoy Gateway 简介"},{"content":"什么是 Wasm 插件？ 你可以使用 Wasm 插件在数据路径上添加自定义代码，轻松地扩展服务网格的功能。可以用你选择的语言编写插件。目前，有 AssemblyScript（TypeScript-ish）、C++、Rust、Zig 和 Go 语言的 Proxy-Wasm SDK。\n在这篇博文中，我们描述了如何使用 Wasm 插件来验证一个请求的有效载荷。这是 Wasm 与 Istio 的一个重要用例，也是你可以使用 Wasm 扩展 Istio 的许多方法的一个例子。您可能有兴趣阅读我们关于在 Istio 中使用 Wasm 的博文，并观看我们关于在 Istio 和 Envoy 中使用 Wasm 的免费研讨会的录音。\n何时使用 Wasm 插件？ 当你需要添加 Envoy 或 Istio 不支持的自定义功能时，你应该使用 Wasm 插件。使用 Wasm 插件来添加自定义验证、认证、日志或管理配额。\n在这个例子中，我们将构建和运行一个 Wasm 插件，验证请求 body 是 JSON，并包含两个必要的键 ——id 和 token。\n编写 Wasm 插件 这个示例使用 tinygo …","relpermalink":"/blog/validating-a-request-payload-with-wasm/","summary":"本文是一个使用 Go 语言开发 Wasm 插件验证请求负载，并将其部署到 Istio 或 Envoy 上的教程。","title":"使用 WebAssembly 验证请求负载"},{"content":"前言 Slack 有一个全球客户群，在高峰期有数百万同时连接的用户。用户之间的大部分通信涉及到向对方发送大量的微小信息。在 Slack 的大部分历史中，我们一直使用 HAProxy 作为所有传入流量的负载均衡器。今天，我们将讨论我们在使用 HAProxy 时所面临的问题，我们如何用 Envoy Proxy 来解决这些问题，迁移所涉及的步骤，以及结果是什么。让我们开始吧！\nSlack 的 Websockets 为了即时传递信息，我们使用 websocket 连接，这是一种双向的通信链接，负责让你看到 “有几个人在打字……\u0026#34;，然后是他们打的东西，速度几乎是光速的。websocket 连接被摄取到一个叫做 “wss”（WebSocket 服务）的系统中，可以通过 wss-primary.slack.com 和 wss-backup.slack.com（这不是网站，如果去访问，只会得到一个 HTTP 404）从互联网上访问。\n显示 websockets 工作原理的图表 Websocket 连接一开始是普通的 HTTPS 连接，然后客户端发出协议切换请求，将连接升级为 Websocket。 …","relpermalink":"/blog/migrating-millions-of-concurrent-websockets-to-envoy/","summary":"本文是 Slack 花半年时间从 HAProxy 迁移到 Envoy 上的经验分享。","title":"Slack 将数百万个并发的 Websockets 迁移到 Envoy 上经验分享"},{"content":"编者的话 本文作者是 Isovalent 联合创始人\u0026amp;CTO，原文标题 How eBPF will solve Service Mesh - Goodbye Sidecars，作者回顾了 Linux 内核的连接性，实现服务网格的几种模式，以及如何使用 eBPF 实现无 Sidecar 的服务网格。\n什么是服务网格？ 随着分布式应用的引入，额外的可视性、连接性和安全性要求也浮出水面。应用程序组件通过不受信任的网络跨越云和集群边界进行通信，负载均衡、弹性变得至关重要，安全必须发展到发送者和接收者都可以验证彼此的身份的模式。在分布式应用的早期，这些要求是通过直接将所需的逻辑嵌入到应用中来解决的。服务网格将这些功能从应用程序中提取出来，作为基础设施的一部分提供给所有应用程序使用，因此不再需要修改每个应用程序。\n服务网格示意图 纵观今天服务网格的功能设置，可以总结为以下几点：\n弹性连接：服务与服务之间的通信必须能够跨越边界，如云、集群和场所。通信必须是有弹性的和容错的。 L7 流量管理：负载均衡、速率限制和弹性必须是 L7 感知的（HTTP、REST、gRPC、WebSocket 等）。 基于身 …","relpermalink":"/blog/ebpf-solve-service-mesh-sidecar/","summary":"本文回顾了 Linux 内核的连接性，实现服务网格的几种模式，以及如何使用 eBPF 实现无 Sidecar 的服务网格。","title":"告别 Sidecar——使用 eBPF 解锁内核级服务网格"},{"content":"前言 Istio 1.12 中新的 WebAssembly 基础设施使其能够轻松地将额外的功能注入网格部署中。\n经过三年的努力，Istio 现在有了一个强大的扩展机制，可以将自定义和第三方 Wasm 模块添加到网格中的 sidecar。Tetrate 工程师米田武（Takeshi Yoneda）和周礼赞（Lizan Zhou）在实现这一目标方面发挥了重要作用。这篇文章将介绍 Istio 中 Wasm 的基础知识，以及为什么它很重要，然后是关于建立自己的 Wasm 插件并将其部署到网格的简短教程。\n为什么 Istio 中的 Wasm 很重要 使用 Wasm，开发人员可以更容易的扩展网格和网关。在 Tetrate，我们相信这项技术正在迅速成熟，因此我们一直在投资上游的 Istio，使配置 API、分发机制和从 Go 开始的可扩展性体验更加容易。我们认为这将使 Istio 有一个全新的方向。\n有何期待：新的插件配置 API，可靠的获取和安装机制 有一个新的顶级 API，叫做 WasmPlugin，可以让你配置要安装哪些插件，从哪里获取它们（OCI 镜像、容器本地文件或远程 HTTP 资源）， …","relpermalink":"/blog/istio-wasm-extensions-and-ecosystem/","summary":"Istio 1.12 中新的 WebAssembly 基础设施使其能够轻松地将额外的功能注入网格部署中。","title":"Istio 1.12 引入 Wasm 插件配置 API 用于扩展 Istio 生态"},{"content":"编者的话 本文译自 Istio 官方博客，博客原标题 gRPC Proxyless Service Mesh，其实是 Istio 1.11 版本中支持的实验特性，可以直接将 gRPC 服务添加到 Istio 中，而不需要再向 Pod 中注入 Envoy 代理。本文中还给出了一个 Demo 性能测试数据，这种做法可以极大的提升应用性能，降低网络延迟。\nIstio 使用一组发现 API（统称为 xDS API 来动态配置其 Envoy sidecar 代理。这些 API 的目标是成为一个 通用的数据平面 API。gRPC 项目对 xDS API 有很好的支持，也就是说你可以管理 gRPC 工作负载，而不需要同时部署 Envoy sidecar。你可以在 Megan Yahya 的 KubeCon EU 2021 演讲中了解更多关于该集成的信息。关于 gRPC 支持的最新情况，可以在他们的提案中找到，还有实现状态。\nIstio 1.11 增加了实验性支持，可以直接将 gRPC 服务添加到网格中。我们支持基本的服务发现，一些基于 VirtualService 的流量策略，以及双向 TLS。\n支 …","relpermalink":"/blog/grpc-proxyless-service-mesh/","summary":"编者的话 本文译自 Istio 官方博客，博客原标题 gRPC Proxyless Service Mesh，其实是 Istio 1.11 版本中支持的实验特性，可以直接将 gRPC 服务添加到 Istio 中，而不需要再向 Pod 中注入 Envoy 代理。本文中还给出了一个 Demo 性能测试数据，这种做法可以极大的提升应","title":"基于 gRPC 和 Istio 的无 sidecar 代理的服务网格"},{"content":"前言 今天有几个服务网格的产品和项目，承诺简化应用微服务之间的连接，同时提供额外的功能，如安全连接、可观测性和流量管理。但正如我们在过去几年中反复看到的那样，对服务网格的兴奋已经被对额外的复杂性和开销的实际担忧所抑制。让我们来探讨一下 eBPF 是如何让我们精简服务网格，使服务网格的数据平面更有效率，更容易部署。\nSidecar 问题 今天的 Kubernetes 服务网格解决方案要求你在每一个应用 pod 上添加一个代理 sidecar 容器，如 Envoy 或 Linkerd-proxy。这是正确的：即使在一个非常小的环境中，比如说有 20 个服务，每个服务运行五个 pod，分布在三个节点上，你也有 100 个代理容器。无论代理的实现多么小和有效，这种纯粹的重复都会耗费资源。\n每个代理使用的内存与它需要能够通信的服务数量有关。Pranay Singhal 写了他配置 Istio 的经验，将每个代理的消耗从 1GB 左右减少到更合理的 60-70MB。但是，即使在我们的小环境中，在三个节点上有 100 个代理，这种优化配置仍然需要每个节点 2GB 左右。\n来自 \u0026lt;a …","relpermalink":"/blog/how-ebpf-streamlines-the-service-mesh/","summary":"本文探讨一下 eBPF 是如何让我们精简服务网格，使服务网格的数据平面更有效率，更容易部署。","title":"eBPF 如何简化服务网格"},{"content":"主要收获 了解采用服务网格技术的新兴架构趋势，特别是多云、多集群和多租户模式，如何在异构基础设施（裸机、虚拟机和 Kubernetes）中部署服务网格解决方案，以及从边缘计算层到网格的应用 / 服务连接。 了解服务网格生态系统中的一些新模式，如多集群服务网格、媒体服务网格（Media Service Mesh）和混沌网格，以及经典的微服务反模式，如“死星（Death Star） “架构。 获取最新的关于在部署领域使用服务网格的创新总结，在 Pod（K8s 集群）和 VM（非 K8s 集群）之间进行快速实验、混乱工程和金丝雀部署。 探索服务网格扩展领域的创新，包括：增强身份管理，以确保微服务连接的安全性，包括自定义证书授权插件，自适应路由功能，以提高服务的可用性和可扩展性，以及增强 sidecar 代理。 了解操作方面即将出现的情况，如配置多集群功能和将 Kubernetes 工作负载连接到托管在虚拟机基础设施上的服务器，以及管理多集群服务网格中所有功能和 API 的开发者门户。 在过去的几年里，服务网格技术有了长足的发展。服务网格在各组织采用云原生技术方面发挥着重要作用。通过提供四种主 …","relpermalink":"/blog/service-mesh-ultimate-guide-e2/","summary":"本文是 InfoQ 自 2020 年 2 月发表的服务网格终极指南后的第二版，发布于 2021 年 9 月。","title":"服务网格终极指南第二版——下一代微服务开发"},{"content":"编者的话 本文译自 Envoy 代理的创始人 Matt Klein 于昨晚在个人博客上发布的文章 5 year of Envoy OSS。他在 Twitter 因为自己的程序 bug 造成重大事故而离职，后加入 Lyft，在开源 Envoy 之前几乎没有贡献和管理开源项目的经验，这篇文章分享了他个人及 Envoy 开源的心路历程，在投身开源 Envoy 还是为雇主 Lyft 效命，该如何抉择？看完本文，相信对于开源项目的维护者、创业者及投资人都会大有收获。\n前言 今天是 Envoy Proxy 开源的 5 周年。毫不夸张地说，在专业方面，过去的 5 年是一个史诗般的过山车，我的情绪介于兴奋、自豪、焦虑、尴尬、无聊、倦怠之间。我想分享一下这个项目的前传和历史，以及我在发展大型开源软件项目的过程中所学到的一些经验教训。\n前传和历史 前传 除了一些小的弯路，我在技术行业二十年的职业生涯一直专注于底层系统：嵌入式系统，操作系统，虚拟化，文件系统，以及最近的分布式系统网络。我的分布式系统网络之旅始于 2010 年初在亚马逊，我有幸帮助开发了第一批高性能计算（HPC）EC2 实例类型。我学到了大量 …","relpermalink":"/blog/envoy-oss-5-year/","summary":"开源网络代理 Envoy 的创始人 Matt Klein，在 Twitter 因为自己的程序 bug 造成重大事故而离职，后加入 Lyft，在开源 Envoy 之前几乎没有贡献和管理开源项目的经验，这篇文章分享了他个人及 Envoy 开源的心路历程，在投身开源 Envoy 还是为雇主 Lyft 效命，该如何抉择？","title":"网络代理 Envoy 开源五周年，创始人 Matt Klein 亲述开源心路历程及经验教训"},{"content":"编者的话 本文译自 Istio 官方博客 Security Best Practices。\nIstio 的安全功能提供了强大的身份、策略、透明的 TLS 加密以及认证、授权和审计（AAA）工具来保护你的服务和数据。然而，为了充分安全地利用这些功能，必须注意遵循最佳实践。建议在继续阅读之前，先回顾一下安全概述。\n双向 TLS Istio 将尽可能使用双向 TLS 对流量进行自动加密。然而，代理在默认情况下被配置为许可模式（Permissive Mode），这意味着他们将接受双向 TLS 和明文流量。\n虽然这是为了增量采用或允许来自没有 Istio sidecar 的客户端的流量的需要，但它也削弱了安全立场。建议在可能的情况下迁移到严格模式（Strict Mode），以强制使用双向 TLS。\n然而，仅靠双向 TLS 并不足以保证流量的安全，因为它只提供认证，而不是授权。这意味着，任何拥有有效证书的人仍然可以访问一个服务。\n为了完全锁定流量，建议配置授权策略。这允许创建细粒度的策略来允许或拒绝流量。例如，你可以只允许来自 app 命名空间的请求访问 hello-world 服务。 …","relpermalink":"/blog/istio-security-best-practices/","summary":"本文列举了 Istio 安全的最佳实践。","title":"Istio 安全最佳实践"},{"content":"Kubernetes Hardening Guidance National Security Agency Cybersecurity and Infrastructure Security Agency\nCybersecurity Technical Report\nNotices and history Document change history\nAugust 2021 1.0 Initial release\nDisclaimer of warranties and endorsement\nThe information and opinions contained in this document are provided “as is” and without any warranties or guarantees. Reference herein to any specific commercial products, process, or service by trade name, trademark, manufacturer, or otherwise, …","relpermalink":"/kubernetes-hardening-guidance/kubernetes-hardening-guidance-english/","summary":"Kubernetes Hardening Guidance National Security Agency Cybersecurity and Infrastructure Security Agency\nCybersecurity Technical Report\nNotices and history Document change history\nAugust 2021 1.0 Initial release\nDisclaimer of warranties and endorsement\nThe information and opinions contained in this document are provided “as is” and without any warranties or guarantees. Reference herein to any specific commercial products, process, or service by trade name, trademark, manufacturer, or otherwise, does not necessarily constitute or imply its endorsement, recommendation, or favoring by the United States Government, and this guidance shall not be used for advertising or product endorsement purposes.\nTrademark recognition\nKubernetes is a registered trademark of The Linux Foundation.","title":""},{"content":"","relpermalink":"/service-mesh-devsecops/intro/reference-platform-for-the-implementation-of-devsecops-primitives/","summary":"","title":""}]