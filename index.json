[{"content":"探索此开发人员中心以访问全面的指南和文档，这将加快你对 TSB 的熟悉程度。无论你是应用程序开发人员、平台运营商还是安全管理员，我们都会定制内容来满足你的需求。如果你遇到任何障碍，请放心，我们随时提供支持。\n对于应用程序开发人员 作为使用 TSB 将应用程序部署到环境中的应用程序开发人员，你将体验到简化的过程。首先使用 Sidecar 代理部署你的应用程序。然后，深入研究高级配置，例如将流量路由到应用程序、实施速率限制或在虚拟机和 Kubernetes 应用程序之间划分流量以实现逐步现代化。\n1. 理解关键概念 掌握服务网格架构 探索 TSB 的架构 高效的交通管理 TSB 的全球可观测性 2. 部署和配置应用程序 使用 Sidecar 部署应用程序 - 如果需要，请熟悉 Istio 的故障排除资源。 为外部流量配置 TSB 使用 OpenAPI 注释 3. 高效的应用管理 监控指标和跟踪 4. 解决常见用例 将流量引导至应用程序 实施速率限制 逐步金丝雀发布 将虚拟机流量迁移到 Kubernetes 跨集群故障转移 5. 有价值的参考资料 TSB 常见问题解答 Istio 官方文档  …","relpermalink":"/tsb/","summary":"Tetrate Service Bridge（TSB）文档：安装、使用和升级。","title":"Tetrate Service Bridge 中文文档"},{"content":"欢迎使用 Tetrate Service Bridge (TSB) 版本 1.6 的发行说明。此版本引入了多项新功能，可增强可用性、安全性和可见性。 TSB 继续提供统一的方法来连接和保护不同环境中的服务，包括 Kubernetes 集群、虚拟机和裸机工作负载。\n主要亮点 跨集群高可用性和安全性 TSB 1.6 专注于通过将远程集群更紧密地结合在一起以简化管理和可扩展性来提高可用性和安全性：\n所有服务的跨集群高可用性：引入 EastWestGateway 功能，实现集群之间的自动服务故障转移，无需外部网关。最大限度地提高服务可用性、简化故障转移并增强安全性。 跨集群身份传播和安全域：创建跨集群的可扩展安全策略，确保本地、远程和故障转移服务的访问控制规则一致。 增强可见性和故障排除 高级可见性和跟踪工具：使应用程序开发人员能够解决跨集群的分布式应用程序中的性能问题。利用 tctl collect 导出运行时数据以进行离线分析，并使用 tctl troubleshoot 进行深入调查。 附加功能和灵活性 WASM 扩展支持：使用 WebAssembly (WASM) 扩展通过自定义功能扩展 …","relpermalink":"/tsb/release-notes-announcements/","summary":"欢迎使用 Tetrate Service Bridge (TSB) 版本 1.6 的发行说明。此版本引入了多项新功能，可增强可用性、安全性和可见性。 TSB 继续提供统一的方法来连接和保护不同环境中的服务，包括 Kubernetes 集群、虚拟机和裸机工作负载。 主要亮点 跨集群高可用性和安全","title":"TSB 1.6 发行说明"},{"content":" Splitting Service Traffic between K8S and VMs\nMigrating VM Monoliths to your cluster\nClient Side Load Balancing\nSend traffic to an External Host using HTTPS\nConfigure ServiceRoute for (multi-port, multi-protocol) services\nCanary Releases\n","relpermalink":"/tsb/howto/traffic/","summary":"Splitting Service Traffic between K8S and VMs Migrating VM Monoliths to your cluster Client Side Load Balancing Send traffic to an External Host using HTTPS Configure ServiceRoute for (multi-port, multi-protocol) services Canary Releases","title":"流量管理和迁移"},{"content":"Argo CD是一种开源的持续交付工具，用于自动化和管理应用程序的部署、更新和回滚。它是一个声明式的工具，专为在 Kubernetes 集群中进行应用程序部署而设计。\n🔔 注意：本文档根据 Argo CD v2.8 Commit 4d2cd06f86（北京时间 2023 年 6 月 30 日 19 时）翻译。\nArgo CD 的主要功能包括：\n持续交付：Argo CD 允许用户将应用程序的配置和清单文件定义为 Git 存储库中的声明式资源，从而实现持续交付。它能够自动检测 Git 存储库中的更改，并将这些更改应用于目标 Kubernetes 集群。\n健康监测和回滚：Argo CD 能够监测应用程序的健康状态，并在检测到问题时触发回滚操作。这有助于确保应用程序在部署期间和运行时保持稳定和可靠。\n多环境管理：Argo CD 支持多个环境（例如开发、测试、生产）的管理。它可以帮助用户在不同环境中进行应用程序的部署和配置管理，并确保这些环境之间的一致性。\n基于 GitOps 的操作：Argo CD 采用了 GitOps 的操作模式，即将应用程序的状态和配置定义为 Git 存储库中的声明式资源。 …","relpermalink":"/argo-cd/","summary":"Argo CD 中文文档（非官方）","title":"Argo CD 中文文档"},{"content":"Argo Rollouts 是一个 Kubernetes 控制器，它提供了在应用程序部署过程中执行渐进式发布和蓝绿部署等高级部署策略的能力。它是基于 Kubernetes 原生的 Deployment 资源构建的，通过引入新的 Rollout 资源来扩展和增强部署控制。\n🔔 注意：本文档根据 Argo Rollouts v1.5 Commit 1d53b25（北京时间 2023 年 6 月 21 日 3 时）翻译。\nArgo Rollouts 具有以下主要功能：\n渐进式发布（Progressive Delivery）：Argo Rollouts 允许你逐步增加新版本的流量并监控其性能，以确保新版本稳定可靠。你可以通过配置渐进式发布的步骤和条件来控制流量的切换和回滚。\n蓝绿部署（Blue-Green Deployment）：Argo Rollouts 支持蓝绿部署模式，其中在新旧版本之间进行无缝切换。通过在新版本上运行一些或全部流量，并根据用户的反馈和性能指标来验证新版本的稳定性，可以确保零停机时间的部署。\n金丝雀部署（Canary Deployment）：Argo Rollouts 支 …","relpermalink":"/argo-rollouts/","summary":"Argo Rollouts 中文文档（非官方）","title":"Argo Rollouts 中文文档"},{"content":" 蓝绿部署\n金丝雀部署\n","relpermalink":"/argo-rollouts/rollout/deployment-strategies/","summary":"蓝绿部署 金丝雀部署","title":"部署策略"},{"content":"本书译自 Solving the Bottom Turtle — a SPIFFE Way to Establish Trust in Your Infrastructure via Universal Identity，译者 Jimmy Song。\n《零信任的基石》封面 Copyright Solving the Bottom Turtle — a SPIFFE Way to Establish Trust in Your Infrastructure via Universal Identity\nby Daniel Feldman, Emily Fox, Evan Gilman, Ian Haken, Frederick Kautz, Umair Khan, Max Lambrecht, Brandon Lum, Agustín Martínez Fayó, Eli Nesterov, Andres Vega, Michael Wardrop. 2020.\nThis work is licensed under the Creative Commons Attribution …","relpermalink":"/spiffe/","summary":"本书系统的讲解了 SPFFE 来解决零信任的身份问题。","title":"零信任的基石：使用 SPIFFE 为基础设施创建通用身份"},{"content":" 注意\n本书正在编写中。 ","relpermalink":"/hugo-in-action/","summary":"静态网站","title":"Hugo 实战"},{"content":" 注意\n《Cilium 中文指南》除特殊说明，默认基于 Cilium 1.11 稳定版本。 《Cilium 中文指南》当前内容译自 Cilium 官方文档，节选了以下章节：\n概念：描述了 Cilium 的组件以及部署 Cilium 的不同模式。提供高层次的 运行一个完整的 Cilium 部署并理解其行为所需的高层次理解。 开始：快速开始使用 Cilium。 策略：详细介绍了策略语言结构和支持的格式。 内部原理：介绍了一些组件的内部细节。 其他未翻译部分主要涉及命令、API 及运维，本指南未来会加入笔者个人观点及其他内容。\n大纲 Cilium 和 Hubble 简介\n多集群（集群网格）\nCilium 概念\n网络\n网络安全\neBPF 数据路径\n网络策略\nKubernetes 集成\n开始阅读 ","relpermalink":"/cilium-handbook/","summary":"Cilium 中文指南","title":"Cilium 中文指南"},{"content":"《什么是 eBPF —— 新一代网络、安全和可观测性工具介绍》译自 O’Reilly 发布的报告“What is eBPF”，作者是 Liz Rice，由 JImmy Song 翻译，英文原版可以在 O’Reilly 网站上获取。\n《什么是 eBPF》中文版封面 译者序 最近两年来关于 eBPF 的讨论在云原生社区里越来越多，尤其是当谈到 Cilium 的商业化，使用 eBPF 来优化 Istio 服务网格，甚至扬言干掉 Sidecar 时，eBPF 更是赚足了眼球。\n这本报告是由基于 Cilium 的创业公司 Isovalent 的 Liz Rice 撰写，由 O’Reilly 发布，相信可以为你揭开 eBPF 技术的神秘面纱，带你了解什么是 eBPF 还有它的强大之处。更重要的是它在云原生环境中，在服务网格、可观测性和安全中的应用。\n关于作者 Liz Rice 是云原生网络和安全专家，Isovalent 的首席开源官，是基于 eBPF 的 Cilium 网络项目的创建者。她在 2019-2022 年担任 CNCF 的技术监督委员会（TOC）主席，并在 2018 …","relpermalink":"/what-is-ebpf/","summary":"新一代网络、安全和可观测性工具简介。","title":"什么是 eBPF？"},{"content":" 关于本教程\n本教程迁移自《Kubernetes 中文指南——云原生应用架构实战手册》，原手册使用 Gitbook 发布，内容涵盖 容器、Kubernetes、服务网格、Serverless 等云元生的多个领域，因内容过于宽泛，且 Gitbook 项目已停止维护，现将其中的 Kubernetes 教程部分独立成书，并使用 Hugo 重新构建。 云原生是一种行为方式和设计理念，究其本质，凡是能够提高云上资源利用率和应用交付效率的行为或方式都是云原生的。云计算的发展史就是一部云原生化的历史。Kubernetes 开启了云原生的序幕，服务网格 Istio 的出现，引领了后 Kubernetes 时代的微服务，Serverless 的兴起，使得云原生从基础设施层不断向应用架构层挺进，我们正处于一个云原生的新时代。\n《Kubernetes 基础教程》封面 Kubernetes 是 Google 于 2014 年 6 月基于其内部使用的 Borg 系统开源出来的容器编排调度引擎，Google 将其作为初始和核心项目贡献给 CNCF（云原生计算基金会），近年来逐渐发展出了云原生生态。 …","relpermalink":"/kubernetes-handbook/","summary":"云原生应用架构实战手册","title":"Kubernetes 基础教程"},{"content":"Kubernetes Hardening Guidance（查看英文原版 PDF）是由美国国家安全局（NSA）于 2021 年 8 月发布的，其中文版《Kubernetes 加固指南》（或译作《Kubernetes 强化指南》），译者 Jimmy Song。\n《Kubernetes 加固指南》封面 许可证 您可以使用署名 - 非商业性使用 - 相同方式共享 4.0 (CC BY-NC-SA 4.0) 协议共享。\n交流群 欢迎加入云原生社区可观测性讨论组（微信群）参与讨论交流，加入前请先填写入群申请问卷后联系 Jimmy Song 入群。\n开始阅读 ","relpermalink":"/kubernetes-hardening-guidance/","summary":"本指南译自美国国家安全局（NSA）于 2021 年 8 月发布的的 Kubernetes Hardening Guidance。","title":"Kubernetes 加固指南"},{"content":" 软件正在吞噬世界。\n——Mark Andreessen\n近些年来，在一些长期由领导者支配的行业中，这些领导者的领先地位已经岌岌可危，这都是由以这些行业为核心业务的软件公司造成的。像 Square、Uber、Netflix、Airbnb 和特斯拉这样的公司能够持续快速增长，并且拥有傲人的市场估值，成为它们所在行业的新领导者。这些创新公司有什么共同点？\n快速创新\n持续可用的服务\n弹性可扩展的 Web\n以移动为核心的用户体验\n将软件迁移到云上是一种自演化，使用了云原生应用架构是这些公司能够如此具有破坏性的核心原因。对于云，我们指的是一个任何能够按需、自助弹性提供和释放计算、网络和存储资源的计算环境。云的定义包括公有云（例如 Amazon Web Services、Google Cloud 和 Microsoft Azure）和私有云（例如 VMware vSphere 和 OpenStack）。\n本章中我们将探讨云原生应用架构的创新性，然后验证云原生应用架构的主要特性。\n开始阅读 ","relpermalink":"/migrating-to-cloud-native-application-architectures/the-rise-of-cloud-native/","summary":"软件正在吞噬世界。 ——Mark Andreessen 近些年来，在一些长期由领导者支配的行业中，这些领导者的领先地位已经岌岌可危，这都是由以这些行业为核心业务的软件公司造成的。像 Square、Uber、Netflix、Ai","title":"第一章：云原生的崛起"},{"content":"Google 有许多通用工程实践，几乎涵盖所有语言和项目。此文档为长期积累的最佳实践，是集体经验的结晶。我们尽可能地将其公之于众，您的组织和开源项目也会从中受益。\n《谷歌工程实践》封面 当前包含以下文档：\nGoogle 代码审查指南，实则两套指南：\n代码审查者指南 代码开发者指南 译者序 此仓库翻译自 google/eng-practices，目前为止的主要内容为 Google 总结的如何进行 Code Review（代码审查） 指南，根据原 Github 仓库的标题判断以后会追加更多 Google 工程实践的内容。\n许可证 您可以使用署名 - 非商业性使用 - 相同方式共享 4.0 (CC BY-NC-SA 4.0) 协议共享。\n交流群 欢迎加入云原生社区微信讨论群，加入前请先填写入群申请问卷后联系 Jimmy Song 入群。\n开始阅读 ","relpermalink":"/eng-practices/","summary":"谷歌代码审查实践指南","title":"谷歌工程实践"},{"content":"本书译自美国国家标准标准与技术研究院（NIST）Special Publication 800-204C。\n《利用服务网格为基于微服务的应用程序实施 DevSecOps》封面 本书大纲 声明\n执行摘要\n第一章：简介\n第二章：实施 DevSecOps 原语的参考平台\n第三章：DevSecOps 组织准备、关键基本要素和实施\n第四章：为参考平台实施 DevSecOps 原语\n第五章：摘要和结论\n关于本书 作者：Ramaswamy Chandramouli\n计算机安全司信息技术实验室\n美国商务部\nGina M. Raimondo，秘书\n国家标准和技术研究所\nJames K. Olthoff，履行负责标准和技术的商务部副部长兼国家标准和技术研究所所长的非专属职能和职责\n本出版物可在：https://doi.org/10.6028/NIST.SP.800-204C 免费获取。\n开始阅读 ","relpermalink":"/service-mesh-devsecops/","summary":"本书译自美国国家标准标准与技术研究院（NIST）Special Publication 800-204C。","title":"利用服务网格为基于微服务的应用程序实施 DevSecOps"},{"content":"本书是 Migrating to Cloud Native Application Architectures 的中文版，本书英文版发布于 2015 年 2 月，中文版由 Jimmy Song 翻译，发布于 2017 年 7 月。\n《迁移到云原生应用架构》图书封面 译者序 云时代的云原生应用大势已来，将传统的单体架构应用迁移到云原生架构，你准备好了吗？\n俗话说“意识决定行动”，在迁移到云原生应用之前，我们大家需要先对 Cloud Native（云原生）的概念、组织形式并对实现它的技术有一个大概的了解，这样才能指导我们的云原生架构实践。\n英文版作于 2015 年，其中的示例主要针对 Java 应用，实际上也适用于任何应用类型，云原生应用架构适用于异构语言的程序开发，不仅仅是针对 Java 语言的程序开发。截止到本人翻译本书时，云原生应用生态系统已经初具规模，CNCF 成员不断发展壮大，基于 Cloud Native 的创业公司不断涌现，Kubernetes 引领容器编排潮流，和 Service Mesh 技术（如 Linkerd 和 Istio）的出现，Go 语言的兴起等为我们将应用迁移 …","relpermalink":"/migrating-to-cloud-native-application-architectures/","summary":"本书是 Migrating to Cloud Native Application Architectures 的中文版。","title":"迁移到云原生应用架构"},{"content":"本书为 Cloud Native Infrastructure 中文版，作者 Justin Garrison 和 Kris Nova，英文版发行于 2017 年 11 月，已可以在网上免费获得，本书是关于创建和管理基础架构，以适用于云原生应用全生命周期管理的模式和实践。\n《云原生基础架构》封面 阅读完这本书后，您将会有如下收获：\n理解为什么说云原生基础架构是高效运行云原生应用所必须的 根据准则来决定您的业务何时以及是否应该采用云原生 了解部署和管理基础架构和应用程序的模式 设计测试以证明您的基础架构可以按预期工作，即使在各种边缘情况下也是如此 了解如何以策略即代码的方式保护基础架构 本书大纲 前言\n介绍\n第 1 章：什么是云原生基础架构？\n第 2 章：采纳云原生基础架构的时机\n第 3 章：云原生部署的演变\n第 4 章：设计基础架构应用程序\n第 5 章：开发基础架构应用程序\n第 6 章：测试云原生基础架构\n第 7 章：管理云原生应用程序\n第 8 章：保护应用程序\n第 9 章：实施云原生基础架构\n附录 A：网络弹性模式\n附录 B：锁定\n附录 C Box：案例研究\n免责声明 本书英文版版权属 …","relpermalink":"/cloud-native-infra/","summary":"《云原生基础架构》，Cloud Native Infrastructure 中文版。","title":"云原生基础架构"},{"content":"本报告译自 O’Reilly 出品的 The Future of Observablity with OpeTelemetry，作者 Ted Young，译者 Jimmy Song。\n《OpenTelemetry 可观测性的未来》封面 关于本书 本书内容包括：\nOpenTelemetry 如何满足库作者、应用程序拥有者、运维和响应者的需求 应用程序的不同角色如何围绕 OpenTelemetry 来协同和独立工作 关于在组织中采用和管理 OpenTelemetry 的实用建议 关于作者 Ted Young 是 OpenTelemetry 项目的联合创始人之一。在过去的二十年里，他设计并建立了各种大规模的分布式系统，包括可视化 FX 管道和容器调度系统。他目前在 Lightstep 公司担任开发者教育总监，住在俄勒冈州波特兰的一个小农场里。\n许可证 您可以使用署名 - 非商业性使用 - 相同方式共享 4.0 (CC BY-NC-SA 4.0) 协议共享。\n交流群 欢迎加入云原生社区可观测性讨论组（微信群）参与讨论交流，加入前请先填写入群申请问卷后联系 Jimmy Song 入群。 …","relpermalink":"/opentelemetry-obervability/","summary":"本报告译自 O'Reilly 出品的 The Future of Observablity with OpenTelemetry，作者 Ted Young，译者 Jimmy Song。","title":"OpenTelemetry 可观测性的未来"},{"content":"欢迎来到云原生导览——一站式了解云原生技术体系。\n导览图 关于云原生\nKubernetes\n服务网格\n社区\n开始阅读 ","relpermalink":"/cloud-native-handbook/","summary":"云原生技术体系导览","title":"云原生导览"},{"content":"本章将介绍什么是云原生、云原生应用。\n本章大纲 什么是云原生？\n云原生的设计哲学\n什么是云原生应用？\n云原生快速入门\n阅读本章 ","relpermalink":"/cloud-native-handbook/intro/","summary":"本章介绍什么是云原生。","title":"关于云原生"},{"content":"简介 代码审查是除了代码作者之外，其他人检查代码的过程。\nGoogle 通过 Code Review 来维护代码和产品质量。\n此文档是 Google Code Review 流程和政策的规范说明。\n此页面是我们进行 Code Review 流程的概述。本指南还有另外两套文档：\n如何进行 Code Review：针对代码审查者的详细指南。 代码开发者指南：针对 CL 开发者的的详细指南。 代码审查者应该关注哪些方面？ 代码审查时应该关注以下方面：\n设计：代码是否经过精心设计并适合您的系统？ 功能：代码的行为是否与作者的意图相同？代码是否可以正常响应用户的行为？ 复杂度：代码能更简单吗？将来其他开发人员能轻松理解并使用此代码吗？ 测试：代码是否具有正确且设计良好的自动化测试？ 命名：开发人员是否为变量、类、方法等选择了明确的名称？ 注释：注释是否清晰有用？ 风格：代码是否遵守了风格指南？ 文档：开发人员是否同时更新了相关文档？ 参阅 如何进行 Code Review 获取更多资料。\n选择最合适审查者 一般而言，您希望找到能在合理的时间内回复您的评论的最合适的审查者。\n最合适的审查者应该是能 …","relpermalink":"/eng-practices/review/","summary":"简介 代码审查是除了代码作者之外，其他人检查代码的过程。 Google 通过 Code Review 来维护代码和产品质量。 此文档是 Google Code Review 流程和政策的规范说明。 此页面是我们进行 Code Review 流程的概述。本指南还有另外两套文档： 如何进行 Code Review","title":"代码审查指南"},{"content":"欢迎来到 Tetrate Service Bridge (TSB) 的概念部分。本节向你介绍 TSB 背后的基本思想、其架构以及它如何在你的环境中工作。\nTSB 如何运作？ Tetrate Service Bridge 是一个在基础设施之上运行的服务网格管理平面，提供一个集中平台来管理和配置整个网格管理环境的网络、安全性和可观察性。\n以下是 TSB 工作原理的概述：\n逻辑视图：TSB 通过将资源分组为 services 、 workspaces 和 groups 来组织你的环境，使它们更易于管理。 基于租户的方法：TSB 鼓励你在组织内创建 tenants 。它将用户帐户和团队与你的公司目录同步，从而简化访问管理。 访问控制：TSB 允许你定义细粒度的访问控制，提供编辑权限并实施零信任方法。这增强了安全性并使你能够监控环境中的活动。 审计跟踪：TSB 跟踪服务和共享资源的更改，确保对所有操作（无论是批准还是拒绝）进行审计跟踪。 配置管理：TSB 允许你编写配置更改并将其分组到 services 中，从而使你能够高效地进行更改并集中管理它们。 隔离故障域：TSB …","relpermalink":"/tsb/concepts/","summary":"欢迎来到 Tetrate Service Bridge (TSB) 的概念部分。本节向你介绍 TSB 背后的基本思想、其架构以及它如何在你的环境中工作。 TSB 如何运作？ Tetrate Service Bridge 是一个在基础设施之上运行的服务网格管理平面，提供一个集中平台来管理和配置整个网格管理环境的","title":"概念"},{"content":" 统一网关\nMulticluster Access Control and Identity Propagation\n使用 IngressGateway 和 ServiceRoute 基于子集的流量路由\nMulti-cluster traffic failover with EastWest Gateways\nMulti-cluster traffic routing using Tier-2 gateway\nMulti cluster traffic shifting with Tier-1 Gateway\nEnd User Authentication with Keycloak\nControlling Access to External Services\nDistributed Ingress Gateways\nConfigure and route HTTP, non-HTTP (multi-protocol) and multi-port service traffic in TSB\nConfiguring Application Gateways Using …","relpermalink":"/tsb/howto/gateway/","summary":"统一网关 Multicluster Access Control and Identity Propagation 使用 IngressGateway 和 ServiceRoute 基于子集的流量路由 Multi-cluster traffic failover with EastWest Gateways Multi-cluster traffic routing using Tier-2 gateway Multi cluster traffic shifting with Tier-1 Gateway End User Authentication with Keycloak Controlling Access to External Services Distributed Ingress Gateways Configure and route HTTP, non-HTTP (multi-protocol) and multi-port service traffic in TSB Configuring Application Gateways Using OpenAPI Annotations App Ingress","title":"配置网关"},{"content":"TSB 有两个主要安装选项：\n1. 本地安装 在本地基础设施上安装 TSB，使你能够完全控制 TSB 管理平面的配置和部署。\n请按照本地安装指南中概述的步骤获取详细说明。\n2. 云服务器安装 在云服务器上部署 TSB，在利用云资源的同时保留自定义 TSB 管理平面配置的灵活性。\n请参阅云服务器安装指南以获取分步指导。\n其他资源 在整个安装过程中，你可能会发现参考“安装与升级”部分中介绍的主题很有用。请务必查看下载 tctl 和使用 tctl 连接到 TSB 等主题，以增强你的理解。\n如需快速概览，你还可以浏览我们的演示安装指南。\n通过遵循这些指南，你将能够使用 tctl 无缝安装 TSB，无论是在本地基础设施上还是在云服务器上。\n","relpermalink":"/tsb/setup/self-managed/","summary":"TSB 有两个主要安装选项： 1. 本地安装 在本地基础设施上安装 TSB，使你能够完全控制 TSB 管理平面的配置和部署。 请按照本地安装指南中概述的步骤获取详细说明。 2. 云服务器安装 在云服务器上部署 TSB，在利用云资源的同时","title":"使用 tctl 安装"},{"content":"本章将介绍什么是云原生、云原生应用。\n本章大纲 什么是 Kubernetes?\nKubernetes 的历史\nSidecar 模式\n阅读本章 ","relpermalink":"/cloud-native-handbook/kubernetes/","summary":"关于 Kubernetes。","title":"Kubernetes"},{"content":"本章将介绍服务网格。\n阅读本章 ","relpermalink":"/cloud-native-handbook/service-mesh/","summary":"本章介绍服务网格。","title":"服务网格"},{"content":" How it works\nConfiguring Flux CD for GitOps\nCanary Analysis \u0026amp; Progressive Delivery Using Argo Rollout and SkyWalking\n","relpermalink":"/tsb/howto/gitops/","summary":"How it works\nConfiguring Flux CD for GitOps\nCanary Analysis \u0026 Progressive Delivery Using Argo Rollout and SkyWalking","title":"Using GitOps"},{"content":" 先决条件和下载\n使用 tctl 安装\n使用 Helm 安装\n载入工作负载\nAWS\n证书安装\nConnect to TSB with tctl\nResource Consumption and Capacity Planning\nTSB Components\nFirewall Information\nIstio 隔离边界\nRepository secrets\n控制平面升级\nMigration from tctl to Helm\n","relpermalink":"/tsb/setup/","summary":"先决条件和下载 使用 tctl 安装 使用 Helm 安装 载入工作负载 AWS 证书安装 Connect to TSB with tctl Resource Consumption and Capacity Planning TSB Components Firewall Information Istio 隔离边界 Repository secrets 控制平面升级 Migration from tctl to Helm","title":"安装与升级"},{"content":"本文档解释了如何利用Helm Charts来安装TSB的不同组件。\nTSB Helm Chart\n管理平面安装\n控制平面安装\n数据平面安装\nTSB Helm 升级\nHelm TSB 卸载\n","relpermalink":"/tsb/setup/helm/","summary":"本文档解释了如何利用Helm Charts来安装TSB的不同组件。 TSB Helm Chart 管理平面安装 控制平面安装 数据平面安装 TSB Helm 升级 Helm TSB 卸载","title":"使用 Helm 安装"},{"content":"概念一章对 Cilium 和 Hubble 的所有方面进行了更深入的介绍。如果你想了解 Cilium 和 Hubble 的概要介绍，请参阅 Cilium 和 Hubble 简介。\n本章大纲 组件概览\nCilium 术语说明\n可观测性\n阅读本章 ","relpermalink":"/cilium-handbook/concepts/","summary":"概念一章对 Cilium 和 Hubble 的所有方面进行了更深入的介绍。如果你想了解 Cilium 和 Hubble 的概要介绍，请参阅 Cilium 和 Hubble 简介。 本章大纲 组件概览 Cilium 术语说明 可观测性 阅读本章","title":"Cilium 概念"},{"content":"Kubernetes 最初源于谷歌内部的 Borg，提供了面向应用的容器集群部署和管理系统。Kubernetes 的目标旨在消除编排物理 / 虚拟计算，网络和存储基础设施的负担，并使应用程序运营商和开发人员完全将重点放在以容器为中心的原语上进行自助运营。Kubernetes 也提供稳定、兼容的基础（平台），用于构建定制化的 workflows 和更高级的自动化任务。\nKubernetes 具备完善的集群管理能力，包括多层次的安全防护和准入机制、多租户应用支撑能力、透明的服务注册和服务发现机制、内建负载均衡器、故障发现和自我修复能力、服务滚动升级和在线扩容、可扩展的资源自动调度机制、多粒度的资源配额管理能力。Kubernetes 还提供完善的管理工具，涵盖开发、部署测试、运维监控等各个环节。\nBorg 简介 Borg 是谷歌内部的大规模集群管理系统，负责对谷歌内部很多核心服务的调度和管理。Borg 的目的是让用户能够不必操心资源管理的问题，让他们专注于自己的核心业务，并且做到跨多个数据中心的资源利用率最大化。\nBorg 主要由 BorgMaster、Borglet、borgcfg …","relpermalink":"/kubernetes-handbook/architecture/","summary":"Kubernetes 最初源于谷歌内部的 Borg，提供了面向应用的容器集群部署和管理系统。Kubernetes 的目标旨在消除编排物理 / 虚拟计算，网络和存储基础设施的负担，并使应用程序运营商和开发人员完全将重点放在以容器为","title":"Kubernetes 架构"},{"content":" 从客户给我们下达订单开始，一直到我们收到现金为止，我们一直都关注时间线。而且我们正在通过删除非附加值的废物来减少这个时间表。\n—— Taichi Ohno\nTaichi Ohno 被公认为精益制造之父。虽然精益制造的实践无法完全适用于软件开发领域，但它们的原则是一致的。这些原则可以指导我们很好地寻求典型的企业 IT 组织采用云原生应用架构所需的变革，并且接受作为这一转变所带来的部分的文化和组织转型。\n开始阅读 ","relpermalink":"/migrating-to-cloud-native-application-architectures/changes-needed/","summary":"从客户给我们下达订单开始，一直到我们收到现金为止，我们一直都关注时间线。而且我们正在通过删除非附加值的废物来减少这个时间表。 —— Taichi Ohno Taichi Ohno 被公认为精益制造之父。虽然精益制造的实践无法完全适用于软件开发领","title":"第二章：在变革中前行"},{"content":"现在我们已经定义了云原生应用架构，并简要介绍了企业在采用它们时必须考虑做出的变化，现在是深入研究技术细节的时候了。对每个技术细节的深入讲解已经超出了本报告的范围。本章中仅是对采用云原生应用架构后，需要做的特定工作和采用的模式的一系列简短的介绍，文中还给出了一些进一步深入了解这些方法的链接。\n开始阅读 ","relpermalink":"/migrating-to-cloud-native-application-architectures/migration-cookbook/","summary":"现在我们已经定义了云原生应用架构，并简要介绍了企业在采用它们时必须考虑做出的变化，现在是深入研究技术细节的时候了。对每个技术细节的深入讲解已经超出了本报告的范围。本章中仅是对采用云原生应用架构后，需要","title":"第三章：迁移指南"},{"content":"云原生应用由多个松散耦合的组件（称为微服务，通常以容器形式实现）组成，在需要零信任概念的无边界网络环境中运行（企业内部或云），并由来自不同地点的用户访问（例如，校园、家庭办公室等）。云原生应用不只是指在云中运行的应用。它们还指具有设计和运行时架构的一类应用，如微服务，以及用于提供所有应用服务（包括安全）的专用基础设施。将 零信任原则 纳入这类应用提供了一些技术，其中对所有受保护资源的访问是通过基于身份的保护和基于网络的保护（如微分）来强制执行的。\n由于业务原因，云原生应用程序需要敏捷和安全的更新和部署技术，以及应对网络安全事件的必要弹性。因此，它们需要一种与传统的单层或多层应用不同的应用开发、部署和运行时监控范式（统称为软件生命周期范式）。DevSecOps（开发、安全和运维）是这类应用的促进范式，因为它通过（a）持续集成、持续交付 / 持续部署（CI/CD）管道（在第 3 节中解释）等基本要素促进了敏捷和安全的开发、交付、部署和运维；（b）整个生命周期的安全测试；以及（c）运行时的持续监控，所有这些都由自动化工具支持。事实上，满足上述目标的范式最初被赋予了 DevOps 这个术语，以 …","relpermalink":"/service-mesh-devsecops/intro/","summary":"云原生应用由多个松散耦合的组件（称为微服务，通常以容器形式实现）组成，在需要零信任概念的无边界网络环境中运行（企业内部或云），并由来自不同地点的用户访问（例如，校园、家庭办公室等）。云原生应用不只是指","title":"第一章：简介"},{"content":"本节页面的内容为开发人员进行代码审查的最佳实践。这些指南可帮助您更快地完成审核并获得更高质量的结果。您不必全部阅读它们，但它们适用于每个 Google 开发人员，并且许阅读全文通常会很有帮助。\n写好 CL 描述 小型 CL 如何处理审查者的评论 另请参阅代码审查者指南，它为代码审阅者提供了详细的指导。\n","relpermalink":"/eng-practices/review/developer/","summary":"本节页面的内容为开发人员进行代码审查的最佳实践。这些指南可帮助您更快地完成审核并获得更高质量的结果。您不必全部阅读它们，但它们适用于每个 Google 开发人员，并且许阅读全文通常会很有帮助。 写好 CL 描述 小型 CL 如何处","title":"开发者指南"},{"content":"本节是基于过往经验编写的 Code Review 最佳方式建议。其中分为了很多独立的部分，共同组成完整的文档。虽然您不必阅读文档，但通读一遍会对您自己和团队很有帮助。\nCode Review 标准 Code Review 要点 查看 CL 的步骤 Code Review 速度 如何撰写 Code Review 评论 处理 Code Review 中的抵触 另请参阅代码开发者指南，该指南为正在进行 Code Review 的开发开发者提供详细指导。\n","relpermalink":"/eng-practices/review/reviewer/","summary":"本节是基于过往经验编写的 Code Review 最佳方式建议。其中分为了很多独立的部分，共同组成完整的文档。虽然您不必阅读文档，但通读一遍会对您自己和团队很有帮助。 Code Review 标准 Code Review 要点 查看 CL 的步骤 Code Review 速度 如何撰写 Code Review 评论 处理","title":"审查者指南"},{"content":"Rate limiting allows you to restrict the traffic through TSB to a predetermined limit based on traffic attributes such as source IP address and HTTP Headers.\nYou may want to consider rate limiting if you are concerned with any of the following:\nTo prevent malicious activity such as DDoS attacks. To prevent your applications and its resources (such as a database) from getting overloaded. To implement some form of business logic such as creating different API limits for different set of users. TSB …","relpermalink":"/tsb/howto/rate-limiting/","summary":"Rate limiting allows you to restrict the traffic through TSB to a predetermined limit based on traffic attributes such as source IP address and HTTP Headers.\nYou may want to consider rate limiting if you are concerned with any of the following:\nTo prevent malicious activity such as DDoS attacks. To prevent your applications and its resources (such as a database) from getting overloaded. To implement some form of business logic such as creating different API limits for different set of users. TSB supports two modes for Rate limiting: internal and external.\nInternal Rate Limiting This mode implements global rate limiting within a cluster or across multiple clusters.","title":"Rate Limiting Traffic"},{"content":" 快速开始简介\n部署应用程序\n创建租户\n创建工作区\n创建配置组\n配置权限\n入口网关\n拓扑和指标\n流量转移\n安全\n创建应用程序和 API\n","relpermalink":"/tsb/quickstart/","summary":"快速开始简介 部署应用程序 创建租户 创建工作区 创建配置组 配置权限 入口网关 拓扑和指标 流量转移 安全 创建应用程序和 API","title":"快速入门"},{"content":" Onboarding VMs with tctl\nWorkload Onboarding\n","relpermalink":"/tsb/setup/workload-onboarding/","summary":"Onboarding VMs with tctl Workload Onboarding","title":"载入工作负载"},{"content":"本章大纲 路由\nIP 地址管理（IPAM）\nIP 地址伪装\nIPv4 分片处理\n阅读本章 ","relpermalink":"/cilium-handbook/networking/","summary":"本章大纲 路由 IP 地址管理（IPAM） IP 地址伪装 IPv4 分片处理 阅读本章","title":"网络"},{"content":"本章大纲 介绍\n基于身份\n策略执行\n代理注入\n阅读本章 ","relpermalink":"/cilium-handbook/security/","summary":"本章大纲 介绍 基于身份 策略执行 代理注入 阅读本章","title":"网络安全"},{"content":"如第 1.1 节所述，参考平台是一个容器编排和管理平台。在现代应用环境中，平台是物理（裸机）或虚拟化（如虚拟机、容器）基础设施上的一个抽象层。在实施 DevSecOps 原语之前，平台只是包含了应用代码，其中包含了应用逻辑和服务网状代码，而服务网状代码又提供应用服务。本节将考虑以下内容：\n一个容器编排和资源管理平台，容纳了应用程序代码和大部分的服务网格代码 服务网格的软件架构 本章大纲 2.1 容器编排和资源管理平台\n2.2 服务网格架构\n开始阅读 ","relpermalink":"/service-mesh-devsecops/reference-platform/","summary":"如第 1.1 节所述，参考平台是一个容器编排和管理平台。在现代应用环境中，平台是物理（裸机）或虚拟化（如虚拟机、容器）基础设施上的一个抽象层。在实施 DevSecOps 原语之前，平台只是包含了应用代码，其中包含了应用逻辑和服务","title":"第二章：实施 DevSecOps 原语的参考平台"},{"content":"本章将介绍什么是云原生、云原生应用。\n阅读本章 ","relpermalink":"/cloud-native-handbook/community/","summary":"社区","title":"社区"},{"content":" Install Tetrate Service Bridge from the AWS Container Marketplace\nInstall Tetrate Service Bridge In AWS\n","relpermalink":"/tsb/setup/aws/","summary":"Install Tetrate Service Bridge from the AWS Container Marketplace\nInstall Tetrate Service Bridge In AWS","title":"AWS"},{"content":"Tetrate Service Bridge (TSB) provides authorization capabilities to authorize every HTTP request coming to Gateways and Workloads. TSB supports local authorization by using JWT claims and external authorization (ext-authz) which uses a service running externally to determine if a request should be allowed or denied.\nYou may decide to use an external authorization system if you have a separate in-house system, you want to use another authentication schema than JWT or if you want to integrate with …","relpermalink":"/tsb/howto/authorization/","summary":"Tetrate Service Bridge (TSB) provides authorization capabilities to authorize every HTTP request coming to Gateways and Workloads. TSB supports local authorization by using JWT claims and external authorization (ext-authz) which uses a service running externally to determine if a request should be allowed or denied.\nYou may decide to use an external authorization system if you have a separate in-house system, you want to use another authentication schema than JWT or if you want to integrate with a third party authorization solution such as Open Policy Agent (OPA) or PlainID.\nExt-authz can be configured in different contexts, such as Tier-1 Gateways, Ingress Gateways, and in Traffic Settings.","title":"External Authorization"},{"content":" 流量管理和迁移\n配置网关\nUsing GitOps\nRate Limiting Traffic\nExternal Authorization\nUsing WASM extensions\nUsing WAF Capabilities\nHPA using SkyWalking\nLeveraging TSB service accounts\nCreating Security Domains\n","relpermalink":"/tsb/howto/","summary":"流量管理和迁移 配置网关 Using GitOps Rate Limiting Traffic External Authorization Using WASM extensions Using WAF Capabilities HPA using SkyWalking Leveraging TSB service accounts Creating Security Domains","title":"操作任务"},{"content":"开始使用 Argo Rollouts：\n基础使用\nAmbassador\nAWS ALB\nAWS AppMesh\nIstio\nNginx\n快速开始 ","relpermalink":"/argo-rollouts/getting-started/","summary":"开始使用 Argo Rollouts： 基础使用 Ambassador AWS ALB AWS AppMesh Istio Nginx 快速开始","title":"入门"},{"content":"本章大纲 介绍\n数据包流程\neBPF Map\nIptables 用法\n阅读本章 ","relpermalink":"/cilium-handbook/ebpf/","summary":"本章大纲 介绍 数据包流程 eBPF Map Iptables 用法 阅读本章","title":"eBPF 数据路径"},{"content":"DevSecOps 在早期就将安全纳入了软件工程流程。它将安全流程和工具集成到 DevOps 的所有开发工作流程（或后面解释的管道）中，并使之自动化，从而实现无缝和连续。换句话说，它可以被看作是三个过程的组合。开发 + 安全 + 运维。\n本节讨论了 DevSecOps 的以下方面：\n组织对 DevSecOps 的准备情况 开发安全运维平台 开发安全运维的基本构件或关键原语 本章大纲 3.1 组织对 DevSecOps 的准备情况\n3.2 DevSecOps 平台\n3.3 DevSecOps 关键原语和实施任务\n开始阅读 ","relpermalink":"/service-mesh-devsecops/devsecops/","summary":"DevSecOps 在早期就将安全纳入了软件工程流程。它将安全流程和工具集成到 DevOps 的所有开发工作流程（或后面解释的管道）中，并使之自动化，从而实现无缝和连续。换句话说，它可以被看作是三个过程的组合。开发 + 安全 + 运维。 本节","title":"第三章：DevSecOps 组织准备、关键基本要素和实施"},{"content":" Overview\nTSB configuration\nExample\n","relpermalink":"/tsb/howto/wasm/","summary":"Overview\nTSB configuration\nExample","title":"Using WASM extensions"},{"content":" Alerting Guidelines\nAzure AD as the Identity Provider\nBackup and restore PostgreSQL\nChange The Administrator Password\nConfig Protection\nConfiguration Promotion\nConfigure cluster external addresses\nConfigure Log Levels\nConfigure multiple IAM token validation keys\nCustomizing TSB Kubernetes Components\nDNS Resolution at Edge\nElasticsearch Credentials\nElasticsearch privileges\nElasticsearch wipe procedure\nGateway Deletion Hold Webhook\nGitOps\nGraceful Connection Drain of istio-proxy\nIstio CA\nIstio …","relpermalink":"/tsb/operations/","summary":"Alerting Guidelines Azure AD as the Identity Provider Backup and restore PostgreSQL Change The Administrator Password Config Protection Configuration Promotion Configure cluster external addresses Configure Log Levels Configure multiple IAM token validation keys Customizing TSB Kubernetes Components DNS Resolution at Edge Elasticsearch Credentials Elasticsearch privileges Elasticsearch wipe procedure Gateway Deletion Hold Webhook GitOps Graceful Connection Drain of istio-proxy Istio CA Istio CNI Key Metrics LDAP as the Identity Provider Lower Istio Resource Consumption Move Data To A New Organization New Relic integration PostgreSQL Credentials Roles and Permissions Sidecar RED Metrics Streaming Service Logs Telemetry Architecture Tier1 Gateway in an App","title":"运维指南"},{"content":" 证书类型\n内部证书要求\n自动证书管理\n","relpermalink":"/tsb/setup/certificate/","summary":"证书类型 内部证书要求 自动证书管理","title":"证书安装"},{"content":"本指南适用于想要为其他开发人员安装和配置 Argo CD 的管理员和操作员。\n🔔 注意：请确保你已完成 入门指南。\n架构概述\nApplicationSet 控制器\n","relpermalink":"/argo-cd/operator-manual/","summary":"本指南适用于想要为其他开发人员安装和配置 Argo CD 的管理员和操作员。 🔔 注意：请确保你已完成 入门指南。 架构概述 ApplicationSet 控制器","title":"操作手册"},{"content":"Kubernetes 作为云原生应用的基础调度平台，相当于云原生的操作系统，为了便于系统的扩展，Kubernetes 中开放的以下接口，可以分别对接不同的后端，来实现自己的业务逻辑：\n容器运行时接口（CRI）：提供计算资源 容器网络接口（CNI）：提供网络资源 容器存储接口（CSI），提供存储资源 以上三种资源相当于一个分布式操作系统的最基础的几种资源类型，而 Kuberentes 是将他们粘合在一起的纽带。\n本节大纲 容器运行时接口（CRI）\n容器网络接口（CNI）\n容器存储接口（CSI）\n","relpermalink":"/kubernetes-handbook/architecture/open-interfaces/","summary":"Kubernetes 作为云原生应用的基础调度平台，相当于云原生的操作系统，为了便于系统的扩展，Kubernetes 中开放的以下接口，可以分别对接不同的后端，来实现自己的业务逻辑： 容器运行时接口（CRI）：提供计算资源 容","title":"开放接口"},{"content":"各种 CI/CD 管道都涉及到参考平台（即基于微服务的应用，有提供基础设施服务的服务网格）。虽然参考应用是基于微服务的应用，但 DevSecOps 的原语可以应用于单体应用以及既在企业内部又基于云的应用（如混合云、单一公有云和多云）。\n在第 2.1 节中，我们提到了我们参考应用环境中的五种代码类型。我们还提到，也可以为这五种代码类型中的每一种创建单独的 CI/CD 管道。这五种代码类型在参考平台组件中的位置将被讨论，然后是描述相关 CI/CD 管道的单独章节：\n参考平台中的代码类型和相关的 CI/CD 管道（4.1 节） 应用程序代码和应用服务代码的 CI/CD 管道（4.2 节） 基础设施即代码（IaC）的 CI/CD 管道（4.3 节） 策略即代码的 CI/CD 管道（4.4 节） 可观测性即代码的 CI/CD 管道（4.5 节） 所有 CI/CD 管道的实施问题，无论代码类型如何，都将在以下章节中讨论：\n确保 CI/CD 管道的安全（4.6 节） CI/CD 管道中的工作流模型（4.7 节） CI/CD 管道中的安全测试（4.8 节） 本节还将考虑 DevSecOps 的整体优 …","relpermalink":"/service-mesh-devsecops/implement/","summary":"各种 CI/CD 管道都涉及到参考平台（即基于微服务的应用，有提供基础设施服务的服务网格）。虽然参考应用是基于微服务的应用，但 DevSecOps 的原语可以应用于单体应用以及既在企业内部又基于云的应用（如混合云、单一公有云和多云）","title":"第四章：为参考平台实施 DevSecOps 原语"},{"content":" Basic troubleshooting\nCluster onboarding troubleshooting\nConfiguration status troubleshooting\nIdentify Underperforming Services\nIngress Gateway troubleshooting\nMaximum Header Size Exceed\nMultiple Transfer Encoding Chunked\nUI Metrics Troubleshooting\nUsing The Debug Container\n","relpermalink":"/tsb/troubleshooting/","summary":"Basic troubleshooting Cluster onboarding troubleshooting Configuration status troubleshooting Identify Underperforming Services Ingress Gateway troubleshooting Maximum Header Size Exceed Multiple Transfer Encoding Chunked UI Metrics Troubleshooting Using The Debug Container","title":"问题排查"},{"content":"本指南适用于已安装 Argo CD 并正在管理应用程序的开发人员。\n🔔 注意：请确保你已完成 入门指南。\n工具\n自动同步策略\n对比选项\n同步选项 环境变量\n选择性同步\n","relpermalink":"/argo-cd/user-guide/","summary":"本指南适用于已安装 Argo CD 并正在管理应用程序的开发人员。 🔔 注意：请确保你已完成 入门指南。 工具 自动同步策略 对比选项 同步选项 环境变量 选择性同步","title":"用户手册"},{"content":" 部署策略\nRollout 规范\nHPA\nVPA\n短暂元数据\n重启 Rollouts\n缩小失败的 Rollout\n回滚窗口\n反亲和性\nHelm\nKustomize\n控制器指标\n","relpermalink":"/argo-rollouts/rollout/","summary":"部署策略\nRollout 规范\nHPA\nVPA\n短暂元数据\n重启 Rollouts\n缩小失败的 Rollout\n回滚窗口\n反亲和性\nHelm\nKustomize\n控制器指标","title":"Rollout"},{"content":"本文为托管云原生应用的参考平台实施 DevSecOps 原语提供全面指导。它包括对参考平台的概述，并描述了基本的 DevSecOps 原语（即 CI/CD 管道）、其构建模块、管道的设计和执行，以及自动化在 CI/CD 管道中有效执行工作流程的作用。\n参考平台的架构除了应用代码和提供应用服务的代码外还包括用于基础设施、运行时策略和持续监测应用健康状况的功能元素，可以通过具有独立 CI/CD 管道类型的声明性代码来部署。还介绍了这些代码的运行时行为、实现高安全性的好处，以及使用风险管理工具和仪表盘指标的管道内的工件来提供持续授权操作（C-ATO）。\n","relpermalink":"/service-mesh-devsecops/summary-and-conclusion/","summary":"本文为托管云原生应用的参考平台实施 DevSecOps 原语提供全面指导。它包括对参考平台的概述，并描述了基本的 DevSecOps 原语（即 CI/CD 管道）、其构建模块、管道的设计和执行，以及自动化在 CI/CD 管道中有效执行工作流程的作用。 参考平台的架构","title":"第五章：摘要和结论"},{"content":" Getting Started\ngRPC API Guide\nInstalling httpbin\nInstalling Open Policy Agent\nInstalling sleep\nREST API Guide\ntctl\ntctl apply\ntctl collect\ntctl completion\ntctl config\ntctl delete\ntctl edit\ntctl experimental\ntctl get\ntctl install\ntctl login\ntctl ui\ntctl validate\ntctl version\ntctl whoami\nTSB CRD Reference\nWorkloadEntry Annotations\nYAML API Guide\n","relpermalink":"/tsb/reference/","summary":"Getting Started gRPC API Guide Installing httpbin Installing Open Policy Agent Installing sleep REST API Guide tctl tctl apply tctl collect tctl completion tctl config tctl delete tctl edit tctl experimental tctl get tctl install tctl login tctl ui tctl validate tctl version tctl whoami TSB CRD Reference WorkloadEntry Annotations YAML API Guide","title":"参考"},{"content":"🔔 警告：你可能不想阅读这部分文档。本手册的这一部分面向想要开发与 Argo CD 交互的第三方应用程序的人们，例如：\n聊天机器人 Slack 集成 🔔 注意：请确保你已完成 入门指南。\n","relpermalink":"/argo-cd/developer-guide/","summary":"🔔 警告：你可能不想阅读这部分文档。本手册的这一部分面向想要开发与 Argo CD 交互的第三方应用程序的人们，例如： 聊天机器人 Slack 集成 🔔 注意：请确保你已完成 入门指南。","title":"开发手册"},{"content":" 概览\nAmbassador\nAPISIX\nAWS ALB\nIstio\nNginx\n插件\nTraefik\n多提供方\n","relpermalink":"/argo-rollouts/traffic-management/","summary":"概览 Ambassador APISIX AWS ALB Istio Nginx 插件 Traefik 多提供方","title":"流量管理"},{"content":" Access Bindings\nAgent Configuration\nAPI\nAPI Access Bindings\nApplication\nApplication Access Bindings\nApplication Service\nApprovals Service\nAudit\nAuth\nAuth\nAWS Identity\nAWS Identity Matcher\nCluster Service\nClusters\nCommon Configuration Objects\nCommon Object Types\nCondition\nControl Plane\nCore types\nData Plane\nEast/West Gateway\nEgress Gateway\nGateway\nGateway Access Bindings\nGateway Common Configuration Messages\nGateway Group\nGateway Service\nHost Info\nIAM (OAuth)\nIAM (OIDC)\nInfo\nIngress Gateway …","relpermalink":"/tsb/refs/","summary":"Access Bindings\nAgent Configuration\nAPI\nAPI Access Bindings\nApplication\nApplication Access Bindings\nApplication Service\nApprovals Service\nAudit\nAuth\nAuth\nAWS Identity\nAWS Identity Matcher\nCluster Service\nClusters\nCommon Configuration Objects\nCommon Object Types\nCondition\nControl Plane\nCore types\nData Plane\nEast/West Gateway\nEgress Gateway\nGateway\nGateway Access Bindings\nGateway Common Configuration Messages\nGateway Group\nGateway Service\nHost Info\nIAM (OAuth)\nIAM (OIDC)\nInfo\nIngress Gateway\ninstall/helm/common/v1alpha1/common.proto\ninstall/helm/controlplane/v1alpha1/values.proto\nIstio Direct Mode Gateway Service\nIstio Direct Mode Security Service\nIstio Direct Mode Traffic Service\nIstio Internal Access Bindings\nIstio Internal Direct Mode Service\nIstio Internal Direct Mode Service\nIstio internal Group","title":"API"},{"content":" Service Mesh and GitOps\nTSB 常见问题解答\n","relpermalink":"/tsb/knowledge-base/","summary":"Service Mesh and GitOps TSB 常见问题解答","title":"知识库和 FAQ"},{"content":" 概览\n插件\n指标\n","relpermalink":"/argo-rollouts/analysis/","summary":"概览 插件 指标","title":"分析"},{"content":"以下列举的内容都是 Kubernetes 中的对象（Object），这些对象都可以在 YAML 文件中作为一种 API 类型来配置。\nPod Node Namespace Service Volume PersistentVolume Deployment Secret StatefulSet DaemonSet ServiceAccount ReplicationController ReplicaSet Job CronJob SecurityContext ResourceQuota LimitRange HorizontalPodAutoscaling Ingress ConfigMap Label CustomResourceDefinition Role ClusterRole 我将它们简单的分类为以下几种资源对象： …","relpermalink":"/kubernetes-handbook/objects/","summary":"以下列举的内容都是 Kubernetes 中的对象（Object），这些对象都可以在 YAML 文件中作为一种 API 类型来配置。 Pod Node Namespace Service Volume PersistentVolume Deployment Secret StatefulSet DaemonSet ServiceAccount ReplicationController ReplicaSet Job CronJob SecurityContext ResourceQuota LimitRange HorizontalPodAutoscaling Ingress ConfigMap Label CustomResourceDefinition Role ClusterRole 我将它们简单的分类为以下几种资源对象： 类别 名称 资源对象 Po","title":"Kubernetes 中的资源对象"},{"content":" 概览\n服务\n","relpermalink":"/argo-rollouts/notifications/","summary":"概览 服务","title":"通知"},{"content":" 概览\n命令\n","relpermalink":"/argo-rollouts/kubectl-plugin/","summary":"概览\n命令","title":"Kubectl plugin"},{"content":" 非版本到版本的升级\n已修订版本间的升级\n网关升级\n修订的 Istio CNI 和升级\n","relpermalink":"/tsb/setup/upgrades/","summary":"非版本到版本的升级 已修订版本间的升级 网关升级 修订的 Istio CNI 和升级","title":"控制平面升级"},{"content":"本章记录了用于在 Cilium 中配置网络策略的策略语言。安全策略可以通过以下机制指定和导入：\n使用 Kubernetes NetworkPolicy、CiliumNetworkPolicy 和 CiliumClusterwideNetworkPolicy 资源。更多细节请参见网络策略一节。在这种模式下，Kubernetes 将自动向所有代理分发策略。 通过代理的 CLI 或 API 参考直接导入到代理中。这种方法不会自动向所有代理分发策略。用户有责任在所有需要的代理中导入策略。 本章内容包括：\n策略执行模式 规则基础 三层示例 四层示例 七层示例 拒绝政策 主机策略 七层协议可视性 在策略中使用 Kubernetes 构造 端点生命周期 故障排除 阅读本章 ","relpermalink":"/cilium-handbook/policy/","summary":"本章记录了用于在 Cilium 中配置网络策略的策略语言。安全策略可以通过以下机制指定和导入： 使用 Kubernetes NetworkPolicy、CiliumNetworkPolicy 和 CiliumClusterwideNetworkPolicy 资源。更多细节请参见网络策略一节。在这种模式","title":"网络策略"},{"content":"为了管理异构和不同配置的主机，为了便于 Pod 的运维管理，Kubernetes 中提供了很多集群管理的配置和管理功能，通过 namespace 划分的空间，通过为 node 节点创建 label 和 taint 用于 pod 的调度等。\n本节大纲 Node\nNamespace\nLabel\nAnnotation\nTaint 和 Toleration（污点和容忍）\n垃圾收集\n资源调度\n服务质量等级（QoS）\n","relpermalink":"/kubernetes-handbook/cluster/","summary":"为了管理异构和不同配置的主机，为了便于 Pod 的运维管理，Kubernetes 中提供了很多集群管理的配置和管理功能，通过 namespace 划分的空间，通过为 node 节点创建 label 和 taint 用于 pod 的调度等。 本节大纲 Node Namespace Label Annotation Taint 和 Tolerat","title":"集群资源管理"},{"content":" 介绍\n入门\n用例\n安全\n","relpermalink":"/argo-cd/operator-manual/applicationset/","summary":"介绍 入门 用例 安全","title":"ApplicationSet 控制器"},{"content":"Kubernetes 中内建了很多 controller（控制器），这些相当于一个状态机，用来控制 Pod 的具体状态和行为。\n本节大纲 Deployment\nStatefulSet\nDaemonSet\nReplicationController 和 ReplicaSet\nJob\nCronJob\nIngress 控制器\nHorizontal Pod Autoscaling\n准入控制器（Admission Controller）\n","relpermalink":"/kubernetes-handbook/controllers/","summary":"Kubernetes 中内建了很多 controller（控制器），这些相当于一个状态机，用来控制 Pod 的具体状态和行为。 本节大纲 Deployment StatefulSet DaemonSet ReplicationController 和 ReplicaSet Job CronJob Ingress 控制器 Horizontal Pod Autoscaling 准入控制器（Admission Controller）","title":"控制器"},{"content":"本章大纲 介绍\n概念\n要求\n网络策略\n端点 CRD\n端点切片 CRD\nKubernetes 兼容性\n故障排除\n阅读本章 ","relpermalink":"/cilium-handbook/kubernetes/","summary":"本章大纲 介绍 概念 要求 网络策略 端点 CRD 端点切片 CRD Kubernetes 兼容性 故障排除 阅读本章","title":"Kubernetes 集成"},{"content":"应用的资源使用率通常都有高峰和低谷的时候，如何削峰填谷，提高集群的整体资源利用率，让 service 中的 Pod 个数自动调整呢？这就有赖于 Horizontal Pod Autoscaling 了，顾名思义，使 Pod 水平自动缩放。这个 Object（跟 Pod、Deployment 一样都是 API resource）也是最能体现 kubernetes 之于传统运维价值的地方，不再需要手动扩容了，终于实现自动化了，还可以自定义指标，没准未来还可以通过人工智能自动进化呢！\nHPA 属于 Kubernetes 中的 autoscaling SIG（Special Interest Group），其下有两个 feature：\nArbitrary/Custom Metrics in the Horizontal Pod Autoscaler#117 Monitoring Pipeline Metrics HPA API #118 Kubernetes 自 1.2 版本引入 HPA 机制，到 1.6 版本之前一直是通过 kubelet 来获取监控指标来判断是否需要扩缩容，1.6 版本之后 …","relpermalink":"/kubernetes-handbook/controllers/hpa/","summary":"应用的资源使用率通常都有高峰和低谷的时候，如何削峰填谷，提高集群的整体资源利用率，让 service 中的 Pod 个数自动调整呢？这就有赖于 Horizontal Pod Autoscaling 了，顾名思义，使 Pod 水平自动缩放。这个 Object（跟 Pod、Deployme","title":"Horizontal Pod Autoscaling"},{"content":"Kubernetes 中为了实现服务实例间的负载均衡和不同服务间的服务发现，创造了 Serivce 对象，同时又为从集群外部访问集群创建了 Ingress 对象。\n本节大纲 Service\n拓扑感知路由\nIngress\nGateway API\n","relpermalink":"/kubernetes-handbook/service-discovery/","summary":"Kubernetes 中为了实现服务实例间的负载均衡和不同服务间的服务发现，创造了 Serivce 对象，同时又为从集群外部访问集群创建了 Ingress 对象。 本节大纲 Service 拓扑感知路由 Ingress Gateway API","title":"服务发现与路由"},{"content":"Kubernetes 中提供了良好的多租户认证管理机制，如 RBAC、ServiceAccount 还有各种策略等。\n另外还有一个 CNCF 孵化项目 SPIFFE 旨在为所有的分布式应用提供身份支持，目前已应用在了 Envoy、Istio 等应用中。\n本节大纲 ServiceAccount\n基于角色的访问控制（RBAC）\nNetworkPolicy\nSPIFFE\nSPIRE\nSPIRE Kubernetes 工作负载注册器\nSVID 身份颁发过程\n","relpermalink":"/kubernetes-handbook/auth/","summary":"Kubernetes 中提供了良好的多租户认证管理机制，如 RBAC、ServiceAccount 还有各种策略等。 另外还有一个 CNCF 孵化项目 SPIFFE 旨在为所有的分布式应用提供身份支持，目前已应用在了 Envoy、Istio 等应用中。 本","title":"身份与权限认证"},{"content":"Kubernetes 中的网络可以说对初次接触 Kubernetes 或者没有网络方面经验的人来说可能是其中最难的部分。Kubernetes 本身并不提供网络功能，只是把网络接口开放出来，通过插件的形式实现。\n网络要解决的问题 既然 Kubernetes 中将容器的联网通过插件的方式来实现，那么该如何解决容器的联网问题呢？\n如果您在本地单台机器上运行 docker 容器的话会注意到所有容器都会处在 docker0 网桥自动分配的一个网络 IP 段内（172.17.0.1/16）。该值可以通过 docker 启动参数 --bip 来设置。这样所有本地的所有的容器都拥有了一个 IP 地址，而且还是在一个网段内彼此就可以互相通信了。\n但是 Kubernetes 管理的是集群，Kubernetes 中的网络要解决的核心问题就是每台主机的 IP 地址网段划分，以及单个容器的 IP 地址分配。概括为：\n保证每个 Pod 拥有一个集群内唯一的 IP 地址 保证不同节点的 IP 地址划分不会重复 保证跨节点的 Pod 可以互相通信 保证不同节点的 Pod 可以与跨节点的主机互相通信 为了解决该问题，出 …","relpermalink":"/kubernetes-handbook/networking/","summary":"Kubernetes 中的网络可以说对初次接触 Kubernetes 或者没有网络方面经验的人来说可能是其中最难的部分。Kubernetes 本身并不提供网络功能，只是把网络接口开放出来，通过插件的形式实现。 网络要解决的问题 既然 Kubernetes 中将容器的联网","title":"网络"},{"content":"为了管理存储，Kubernetes 提供了以下资源对象：\nSecret：用于管理敏感信息 ConfigMap：存储配置 Volume、PV、PVC、StorageClass 等：用来管理存储卷 本节将为你讲解 Kubernetes 中的存储对象。\n本节大纲 Secret\nConfigMap\nConfigMap 的热更新\nVolume\n持久化卷（Persistent Volume）\nStorage Class\n本地持久化存储\n","relpermalink":"/kubernetes-handbook/storage/","summary":"为了管理存储，Kubernetes 提供了以下资源对象： Secret：用于管理敏感信息 ConfigMap：存储配置 Volume、PV、PVC、StorageClass 等：用来管理存储卷 本节将为你讲解 Kubernetes 中","title":"存储"},{"content":"Kubernetes 是一个高度开放可扩展的架构，可以通过自定义资源类型（CRD）来定义自己的类型，还可以自己来扩展 API 服务，用户的使用方式跟 Kubernetes 的原生对象无异。\n本节大纲 使用自定义资源扩展 API\n使用 CRD 扩展 Kubernetes API\nAggregated API Server\nAPIService\n服务目录（Service Catalog）\n","relpermalink":"/kubernetes-handbook/extend/","summary":"Kubernetes 是一个高度开放可扩展的架构，可以通过自定义资源类型（CRD）来定义自己的类型，还可以自己来扩展 API 服务，用户的使用方式跟 Kubernetes 的原生对象无异。 本节大纲 使用自定义资源扩展 API 使用 CRD 扩展 Kubernetes API Aggregated API Server APIService 服务目录（S","title":"扩展集群"},{"content":"组织需要部署多个 Kubernetes 集群来为不同的业务提供隔离，增强可用性和可扩展性。\n什么是多集群？ 多集群是一种在多个 Kubernetes 集群上或跨集群部署应用的策略，目的是提高可用性、隔离性和可扩展性。多集群对于确保遵守不同的和相互冲突的法规非常重要，因为单个集群可以进行调整，以遵守特定地域或认证的法规。软件交付的速度和安全性也可以提高，单个开发团队将应用程序部署到隔离的集群中，并有选择地暴露哪些服务可用于测试和发布。\n配置多集群访问 你可以使用 kubectl config 命令配置要访问的集群，详见配置对多集群的访问。\n集群联邦 集群联邦（Federation）是指通过 Federation API 资源来统一管理多个集群的资源，如定义 Deployment 如何部署到不同集群上，及其所需的副本数等。这些集群可能位于不同的可用区、地区或者供应商。实施集群联邦一般是为了达到以下目的：\n简化管理多个集群的 Kubernetes 组件 (如 Deployment、Service 等）； 在多个集群之间分散工作负载（Pod），以提升应用（服务）的可靠性； 跨集群的资源编排，依 …","relpermalink":"/kubernetes-handbook/multi-cluster/","summary":"组织需要部署多个 Kubernetes 集群来为不同的业务提供隔离，增强可用性和可扩展性。 什么是多集群？ 多集群是一种在多个 Kubernetes 集群上或跨集群部署应用的策略，目的是提高可用性、隔离性和可扩展性。多集群对于确保遵守不同的和相互冲","title":"多集群管理"},{"content":"Kubernetes 中的各个资源对象的配置指南。\n本节大纲 配置 Pod 的 liveness 和 readiness 探针\n配置 Pod 的 Service Account\nSecret 配置\n管理 namespace 中的资源配额\n","relpermalink":"/kubernetes-handbook/config/","summary":"Kubernetes 中的各个资源对象的配置指南。 本节大纲 配置 Pod 的 liveness 和 readiness 探针 配置 Pod 的 Service Account Secret 配置 管理 namespace 中的资源配额","title":"资源对象配置"},{"content":"Kubernetes 中的 kubectl 及其他管理命令使用。\n本节大纲 Docker 用户过渡到 kubectl 命令行指南\nKubectl 命令概览\nKubectl 命令技巧大全\n使用 etcdctl 访问 Kubernetes 数据\n","relpermalink":"/kubernetes-handbook/cli/","summary":"Kubernetes 中的 kubectl 及其他管理命令使用。 本节大纲 Docker 用户过渡到 kubectl 命令行指南 Kubectl 命令概览 Kubectl 命令技巧大全 使用 etcdctl 访问 Kubernetes 数据","title":"命令使用"},{"content":"Kubernetes 支持多租户，这就需要对集群的安全性进行管理。\n本节大纲 管理集群中的 TLS\nKublet 的认证授权\nTLS Bootstrap\nIP 伪装代理\n创建用户认证授权的 kubeconfig 文件\n使用 kubeconfig 或 token 进行用户身份认证\nKubernetes 中的用户与身份认证授权\nKubernetes 集群安全性配置最佳实践\n","relpermalink":"/kubernetes-handbook/security/","summary":"Kubernetes 支持多租户，这就需要对集群的安全性进行管理。 本节大纲 管理集群中的 TLS Kublet 的认证授权 TLS Bootstrap IP 伪装代理 创建用户认证授权的 kubeconfig 文件 使用 kubeconfig 或 token 进行用户身份认证 Kubernetes 中的用户与身份认证授权 Kubernetes 集群安全性配置最佳实践","title":"集群安全性管理"},{"content":"根据用户部署和暴露服务的方式不同，有很多种方式可以用来访问 Kubernetes 集群。\n最简单也是最直接的方式是使用 kubectl 命令。 其次可以使用 kubeconfig 文件来认证授权访问 API server。 通过各种 proxy 经过端口转发访问 Kubernetes 集群中的服务 使用 Ingress，在集群外访问 Kubernetes 集群内的 service 本节大纲 访问集群\n使用 kubeconfig 文件配置跨集群认证\n通过端口转发访问集群中的应用程序\n使用 service 访问群集中的应用程序\n从外部访问 Kubernetes 中的 Pod\nLens - Kubernetes IDE\nKubernator - 更底层的 Kubernetes UI\n","relpermalink":"/kubernetes-handbook/access/","summary":"根据用户部署和暴露服务的方式不同，有很多种方式可以用来访问 Kubernetes 集群。 最简单也是最直接的方式是使用 kubectl 命令。 其次可以使用 kubeconfig 文件来认证授权访问 API server。 通过各种 proxy 经过端口转发访问 Kubernetes 集群中的服务 使用 Ing","title":"访问 Kubernetes 集群"},{"content":"理论上只要可以使用主机名做服务注册的应用都可以迁移到 Kubernetes 集群上。看到这里你可能不禁要问，为什么使用 IP 地址做服务注册发现的应用不适合迁移到 kubernetes 集群？因为这样的应用不适合自动故障恢复，因为目前 Kubernetes 中不支持固定 Pod 的 IP 地址，当 Pod 故障后自动转移到其他节点的时候该 Pod 的 IP 地址也随之变化。\n将传统应用迁移到 Kubernetes 中可能还有很长的路要走，但是直接开发云原生应用，Kubernetes 就是最佳运行时环境了。\n本节大纲 适用于 Kubernetes 的应用开发部署流程\n迁移传统应用到 Kubernetes 步骤详解——以 Hadoop YARN 为例\n使用 StatefulSet 部署有状态应用\n持续集成与交付（CI/CD）\n使用 Kustomize 配置 Kubernetes 应用\n","relpermalink":"/kubernetes-handbook/devops/","summary":"理论上只要可以使用主机名做服务注册的应用都可以迁移到 Kubernetes 集群上。看到这里你可能不禁要问，为什么使用 IP 地址做服务注册发现的应用不适合迁移到 kubernetes 集群？因为这样的应用不适合自动故障恢复，因为目前 Kubernetes 中不支持固定 Pod","title":"在 Kubernetes 中开发部署应用"},{"content":"讲解如何在原生 Kubernetes 的基础上做定制开发。\n本节大纲 SIG 和工作组\n配置 Kubernetes 开发环境\n测试 Kubernetes\nclient-go 示例\nOperator\nOperator SDK\nKubebuilder\n高级开发指南\n参与 Kubernetes 社区贡献\nMinikube\n阅读本章 ","relpermalink":"/kubernetes-handbook/develop/","summary":"讲解如何在原生 Kubernetes 的基础上做定制开发。 本节大纲 SIG 和工作组 配置 Kubernetes 开发环境 测试 Kubernetes client-go 示例 Operator Operator SDK Kubebuilder 高级开发指南 参与 Kubernetes 社区贡献 Minikube 阅读本章","title":"开发指南"},{"content":"This guide will help you to get started with Workload Onboarding in practice.\nAs part of this guide, you will:\nDeploy Istio Bookinfo example into your Kubernetes cluster Deploy ratings application on an AWS EC2 instance and onboard it into the service mesh Verify traffic between Kubernetes Pod(s) and AWS EC2 instances Deploy ratings application on an AWS Auto Scaling Group and onboard it into the service mesh This guide is intended to be an easy-to-follow demonstration of the workload onboarding …","relpermalink":"/tsb/setup/workload-onboarding/quickstart/aws-ec2/","summary":"This guide will help you to get started with Workload Onboarding in practice.\nAs part of this guide, you will:\nDeploy Istio Bookinfo example into your Kubernetes cluster Deploy ratings application on an AWS EC2 instance and onboard it into the service mesh Verify traffic between Kubernetes Pod(s) and AWS EC2 instances Deploy ratings application on an AWS Auto Scaling Group and onboard it into the service mesh This guide is intended to be an easy-to-follow demonstration of the workload onboarding capabilities.\nTo keep things simple, you are not required to configure the infrastructure the way you would do it in the case of a production deployment.","title":"Quickstart with Workloads on AWS EC2"},{"content":"This guide will help you to get started with Workload Onboarding in practice.\nAs part of this guide, you will:\nDeploy Istio Bookinfo example into an Elastic Kubernetes Service (EKS) cluster Deploy ratings application as an AWS ECS task and onboard it into the service mesh Verify traffic between Kubernetes Pod(s) and the AWS ECS task This guide is intended to be an easy-to-follow demonstration of the workload onboarding capabilities.\nTo keep things simple, you are not required to configure the …","relpermalink":"/tsb/setup/workload-onboarding/quickstart/aws-ecs/","summary":"This guide will help you to get started with Workload Onboarding in practice.\nAs part of this guide, you will:\nDeploy Istio Bookinfo example into an Elastic Kubernetes Service (EKS) cluster Deploy ratings application as an AWS ECS task and onboard it into the service mesh Verify traffic between Kubernetes Pod(s) and the AWS ECS task This guide is intended to be an easy-to-follow demonstration of the workload onboarding capabilities.\nTo keep things simple, you are not required to configure the infrastructure the way you would do it in the case of a production deployment.\nSpecifically:\nyou are not required to set up routable DNS records you are not required to use a trusted CA authority (such as Let’s Encrypt) Before proceeding, please make sure to complete the following prerequisites:","title":"Quickstart with Workloads on AWS ECS"},{"content":"This guide will help you to get started with Workload Onboarding in practice.\nAs part of this guide, you will:\nDeploy Istio Bookinfo example into your Kubernetes cluster Deploy ratings application on a VM on-premise and onboard it into the service mesh Verify traffic between Kubernetes Pod(s) and the VM on-premise This guide is intended to be an easy-to-follow demonstration of the workload onboarding capabilities.\nTo keep things simple, you are not required to configure the infrastructure the …","relpermalink":"/tsb/setup/workload-onboarding/quickstart/on-premise/","summary":"This guide will help you to get started with Workload Onboarding in practice.\nAs part of this guide, you will:\nDeploy Istio Bookinfo example into your Kubernetes cluster Deploy ratings application on a VM on-premise and onboard it into the service mesh Verify traffic between Kubernetes Pod(s) and the VM on-premise This guide is intended to be an easy-to-follow demonstration of the workload onboarding capabilities.\nTo keep things simple, you are not required to configure the infrastructure the way you would do it in the case of a production deployment.\nSpecifically:\nyou are not required to set up routable DNS records you are not required to use a trusted CA authority (such as Let’s Encrypt) Before proceeding, please make sure to complete the following prerequisites:","title":"Quickstart with Workloads on-premise"},{"content":"本节向你介绍 TSB Operator 的基本概念。你将深入了解 TSB Operator 如何管理 TSB 的整个生命周期，包括跨各个平面的安装、升级和运行时行为。\nKubernetes 知识\n如果你不熟悉 Kubernetes 命名空间、运算符、清单和自定义资源，建议你熟悉这些概念。此背景将极大地增强你对 TSB Operator 的理解以及维护 TSB 服务网格的能力。\n你可以查阅 Kubernetes 文档来了解有关 Operator 模式的更多信息。\nTSB Operator 在控制 TSB 管理、控制和数据平面组件的安装、升级和运行时行为方面发挥着关键作用。为了确保兼容性并提供平滑的升级体验，基于 Kubernetes 的 TSB 组件清单已集成到 TSB Operator 中。因此，管理、控制和数据平面组件的版本与管理它们的 TSB Operator 部署的版本相关联。TSB Operator 利用用户创建的自定义资源 (CR) 来配置和实例化这些组件。\n为了有效管理 TSB 生命周期，TSB Operator 与 tctl CLI 工具密切协作。使用 tctl ，你可以 …","relpermalink":"/tsb/concepts/operators/","summary":"本节向你介绍 TSB Operator 的基本概念。你将深入了解 TSB Operator 如何管理 TSB 的整个生命周期，包括跨各个平面的安装、升级和运行时行为。 Kubernetes 知识 如果你不熟悉 Kubernetes 命名空间、运算符、清单和自定义资源，建议你熟悉这些概念。此背景将极大地","title":"TSB Operator"},{"content":"Workload Onboarding is a TSB feature that automates onboarding workloads deployed outside of Kubernetes into the service mesh.\nFor example, you can use it to onboard workloads deployed on VMs (or perhaps VMs from auto-scaling groups) that are not part of your Kubernetes clusters.\n:::note The Workload Onboarding feature is currently an alpha feature.\nIt is quite possible that it does not support all possible deployment scenarios. Most notably, it does not yet support the use of Iptables for …","relpermalink":"/tsb/setup/workload-onboarding/guides/","summary":"Workload Onboarding is a TSB feature that automates onboarding workloads deployed outside of Kubernetes into the service mesh.\nFor example, you can use it to onboard workloads deployed on VMs (or perhaps VMs from auto-scaling groups) that are not part of your Kubernetes clusters.\n:::note The Workload Onboarding feature is currently an alpha feature.\nIt is quite possible that it does not support all possible deployment scenarios. Most notably, it does not yet support the use of Iptables for traffic redirection. You should configure your Istio sidecar and your application as necessary.\nAt the moment, this feature supports onboarding workloads from the following environments:","title":"Workload Onboarding"},{"content":"TSB comes with a rate limiting server component for every control plane cluster. By default this is disabled.\nThis section will only discuss installation procedures for the internal mode, and not for installation of external servers.\nConfiguration The rate limit server can be enabled by explicitly specifying configuration for the rateLimitServer component in the ControlPlane Operator API or Helm values and applying it to the relevant control plane clusters.\nThe rateLimitServer requires a Redis …","relpermalink":"/tsb/howto/rate-limiting/internal-rate-limiting/","summary":"TSB comes with a rate limiting server component for every control plane cluster. By default this is disabled.\nThis section will only discuss installation procedures for the internal mode, and not for installation of external servers.\nConfiguration The rate limit server can be enabled by explicitly specifying configuration for the rateLimitServer component in the ControlPlane Operator API or Helm values and applying it to the relevant control plane clusters.\nThe rateLimitServer requires a Redis backend to keep track of the rate limiting attribute counts and its details need to be included in the configuration.\nYour Control Plane operator configuration may look like the example below:","title":"Enabling the Internal Rate Limiting Server"},{"content":"This document explains how you can leverage GitOps workflows with TSB. The document assumes that GitOps is already enabled in the Management Plane cluster and/or in the application clusters.\nThe main idea behind the GitOps support in TSB is allowing:\nAdministrator teams to create TSB configuration resources directly in the Management Plane cluster. Application teams to create TSB configuration resources directly in the application clusters. Applications teams can push changes to application …","relpermalink":"/tsb/howto/gitops/gitops/","summary":"This document explains how you can leverage GitOps workflows with TSB. The document assumes that GitOps is already enabled in the Management Plane cluster and/or in the application clusters.\nThe main idea behind the GitOps support in TSB is allowing:\nAdministrator teams to create TSB configuration resources directly in the Management Plane cluster. Application teams to create TSB configuration resources directly in the application clusters. Applications teams can push changes to application configuration the same way they push changes to the applications themselves, and allows packaging together the application deployment resources and the TSB configurations, for example inside the same Helm chart.","title":"How it works"},{"content":"This document will describe what’s a WASM extension and its benefits.\nWhat’s a WASM extension ? A WASM extension is a software addon of WebAssembly which can be used to extend the Istio proxy (Envoy). These WASM extensions are executed in a sandbox environment and have limited access to the external system, and can be created using different programming languages with their SDKs. This sandbox environment provides isolation to prevent that a programming error or crash in one plugin affects other …","relpermalink":"/tsb/howto/wasm/wasm-overview/","summary":"This document will describe what’s a WASM extension and its benefits.\nWhat’s a WASM extension ? A WASM extension is a software addon of WebAssembly which can be used to extend the Istio proxy (Envoy). These WASM extensions are executed in a sandbox environment and have limited access to the external system, and can be created using different programming languages with their SDKs. This sandbox environment provides isolation to prevent that a programming error or crash in one plugin affects other plugins and security to avoid one plugin getting information from the system.\nWhat’s the benefit of WASM extensions ? Envoy can be extended using filters, and there are various builtin filters for different protocols, that can be configured to be executed as part of the networking traffic.","title":"Overview"},{"content":"Tetrate Service Bridge (TSB) provides authorization capabilities to authorize every HTTP request coming to a service from another service (“service-to-service” requests).\nTSB supports local authorization by using JWT claims and external authorization which uses a service running externally to determine if a request should be allowed or denied. External authorization can be used on both gateways and workloads (through their sidecars).\nYou may decide to use an external authorization system if you …","relpermalink":"/tsb/howto/authorization/sidecar/","summary":"Tetrate Service Bridge (TSB) provides authorization capabilities to authorize every HTTP request coming to a service from another service (“service-to-service” requests).\nTSB supports local authorization by using JWT claims and external authorization which uses a service running externally to determine if a request should be allowed or denied. External authorization can be used on both gateways and workloads (through their sidecars).\nYou may decide to use an external authorization system if you have a separate in-house system or if you want to integrate with a third party authorization solution such as Open Policy Agent (OPA) or PlainID.\nThis document describes how to configure service-to-service authorization using OPA as an example.","title":"Service to service authorization using external authorization"},{"content":"In this how-to you’ll learn how to setup traffic routing between a service running both on a VM, and a Kubernetes cluster.\nIn this guide, you’ll ✓ Install the Istio demo bookinfo application in a cluster ✓ Install the ratings service from the bookinfo application on a VM. ✓ Split traffic 80/20 between the VM, and the cluster instances of the ratings application\nBefore you get started, make sure that you: ✓ Install TSB management plane ✓ Onboarded a cluster\n✓ Install data plane operator\nFirst, …","relpermalink":"/tsb/howto/traffic/splitting-service-traffic-between-k8s-vms/","summary":"In this how-to you’ll learn how to setup traffic routing between a service running both on a VM, and a Kubernetes cluster.\nIn this guide, you’ll ✓ Install the Istio demo bookinfo application in a cluster ✓ Install the ratings service from the bookinfo application on a VM. ✓ Split traffic 80/20 between the VM, and the cluster instances of the ratings application\nBefore you get started, make sure that you: ✓ Install TSB management plane ✓ Onboarded a cluster\n✓ Install data plane operator\nFirst, start by installing bookinfo in your cluster.\nkubectl create ns bookinfo kubectl apply -f \\ https://raw.","title":"Splitting Service Traffic between K8S and VMs"},{"content":"概述 本文介绍如何使用 Helm Charts 来安装 Tetrate Service Bridge (TSB) 的不同组件。假设你的系统上已经安装了 Helm。\nTSB 为其 平面 中的每一个都提供了一个图表：\n管理平面：安装 TSB 管理平面Operator（可选择安装 MP CR 和/或密钥）。 控制平面：安装 TSB 控制平面Operator（可选择安装 MP CR 和/或密钥）。 数据平面：安装 TSB 数据平面Operator。 每个Chart都安装了相应平面的Operator。管理平面和控制平面都允许创建触发Operator的相应资源（使用 spec 属性）以部署所有 TSB 组件和/或必需的密钥（使用 secrets 属性）以使其正常运行。\n这种行为让你选择完全配置 TSB 并与 CD 流水线集成的方式。你可以使用 Helm 来：\n仅安装Operator 安装/升级平面资源（管理平面或控制平面 CR）以及Operator 安装/升级Operator和密钥 一次安装/升级它们（Operator、资源、密钥） 关于密钥，要牢记 helm install/upgrade 命令 …","relpermalink":"/tsb/setup/helm/helm/","summary":"概述 本文介绍如何使用 Helm Charts 来安装 Tetrate Service Bridge (TSB) 的不同组件。假设你的系统上已经安装了 Helm。 TSB 为其 平面 中的每一个都提供了一个图表： 管理平面：安装 TSB 管理平面Operator（可选择安装 MP CR 和/或密钥）。 控制平面","title":"TSB Helm Chart"},{"content":"在继续之前，请确保你熟悉 Istio 隔离边界 功能。\n升级前 从非版本升级到版本控制平面设置涉及启用 Istio 隔离边界功能。 启用后，可以在隔离边界内配置版本，控制平面必须升级到该版本。 按照 隔离边界安装 中提到的步骤部署具有启用隔离边界功能的控制平面。\n启用 Istio 隔离边界功能后，你需要在添加隔离边界到 ControlPlane CR 之前，将 TSB 数据平面 Operator 的规模缩小。这是为了避免 TSB 数据平面 Operator 和 TSB 控制平面 Operator 在协调相同的 TSB Ingress/Egress/Tier1Gateway 资源时发生竞争条件。\nkubectl scale --replicas=0 deployment tsb-operator-data-plane -n istio-gateway 出于同样的原因，我们还必须将 istio-operator 在 istio-gateway 命名空间中的规模缩小。\nkubectl scale --replicas=0 deployment istio-operator -n …","relpermalink":"/tsb/setup/upgrades/non-revisioned-to-revisioned/","summary":"在继续之前，请确保你熟悉 Istio 隔离边界 功能。 升级前 从非版本升级到版本控制平面设置涉及启用 Istio 隔离边界功能。 启用后，可以在隔离边界内配置版本，控制平面必须升级到该版本。 按照 隔离边界安装 中提到的步骤部署具有启用","title":"非版本到版本的升级"},{"content":"Service Mesh 架构已得到广泛采用，Tetrate 的团队由一些最早开发支持该架构的技术的工程师组成。在本节中，我们将介绍该架构、其术语、功能、特性，并重点介绍 Istio，这是为 Tetrate Service Bridge 提供支持的领先网格实现。\n什么是服务网格？ 服务网格是通过代理位于应用程序组件和网络之间的基础设施层。虽然这些组件通常是微服务，但任何工作负载（从无服务器容器到虚拟机或裸机上的传统 n 层应用程序）都可以参与网格。代理不是通过网络在组件之间进行直接通信，而是拦截并管理该通信。\n服务网格架构：控制平面和数据平面 数据平面 这些代理被称为“sidecar 代理”，因为它们与每个应用程序实例一起部署，构成了服务网格的数据平面。它们在运行时处理应用程序流量。 Tetrate Service Bridge 采用 Envoy 作为数据平面实现。 Envoy 提供了大量的安全、流量策略和遥测功能，包括：\n服务发现 弹性机制（重试、熔断、异常值检测） 客户端负载均衡 细粒度的L7流量控制 根据请求实施安全策略 基于 L7 元数据的身份验证、速率限制、策略 具有强 L7 …","relpermalink":"/tsb/concepts/service-mesh/","summary":"Service Mesh 架构已得到广泛采用，Tetrate 的团队由一些最早开发支持该架构的技术的工程师组成。在本节中，我们将介绍该架构、其术语、功能、特性，并重点介绍 Istio，这是为 Tetrate Service Bridge 提供支持的领先网格实现。 什么是","title":"服务网格简介"},{"content":"本页深入介绍了 TSB 操作员如何配置管理平面组件，并概述了 TSB 操作员管理的各种组件。\nTSB Operator 配置为监督管理平面组件的生命周期，主动监视部署的同一命名空间内的 ManagementPlane 自定义资源 (CR)。默认情况下，管理平面驻留在 tsb 命名空间中。你可以参阅管理平面安装 API 参考文档，了解有关自定义资源 API 的全面详细信息。\n组件 管理平面组件 以下是你可以使用管理平面 Operator 配置和管理的各种类型的自定义组件：\n组件 Service Deployment Cronjobs apiServer tsb tsb teamsync iamServer iam iam webUI web web frontEnvoy envoy envoy oap oap oap collector otel-collector otel-collector xcpOperator xcp-operator-central xcp-operator-central xcpCentral xcp-central central mpc mpc mpc …","relpermalink":"/tsb/concepts/operators/management-plane/","summary":"本页深入介绍了 TSB 操作员如何配置管理平面组件，并概述了 TSB 操作员管理的各种组件。 TSB Operator 配置为监督管理平面组件的生命周期，主动监视部署的同一命名空间内的 ManagementPlane 自定义资源 (CR)。默认情况下，管理平面驻留在 tsb 命名空","title":"管理平面"},{"content":"快速开始简介 欢迎使用 TSB 快速入门指南！本指南旨在引导你完成在 TSB 上加入和配置应用程序的过程。通过遵循本快速入门，你将了解如何针对各种基本场景部署应用程序并配置 TSB 及其组件。\n在本快速入门指南中，你将探索以下场景：\n部署 Istio bookinfo 示例应用程序 创建租户并连接集群 创建工作区 建立对工作区的 tctl 访问权限 创建配置组 配置权限 设置入口网关 检查服务拓扑和指标 使用 TSB 进行流量转移 在 TSB 内启用安全设置 创建应用程序并使用 OpenAPI 规范配置 API 在开始使用快速入门指南之前，请确保你：\n熟悉 TSB 概念 安装 TSB 演示环境 本指南中的每个示例将演示如何使用 tctl 命令行工具和 TSB UI 进行更改。\n在这些示例中，你将使用超级管理员权限，授予你访问所有 TSB 功能的权限。但是，请记住，对于生产用途，并非每个人都可以被授予管理员权限。出于安全考虑，不建议为每个人提供管理员访问权限。\n","relpermalink":"/tsb/quickstart/introduction/","summary":"快速开始简介 欢迎使用 TSB 快速入门指南！本指南旨在引导你完成在 TSB 上加入和配置应用程序的过程。通过遵循本快速入门，你将了解如何针对各种基本场景部署应用程序并配置 TSB 及其组件。 在本快速入门指南中，你将探索以下场","title":"快速开始简介"},{"content":"本文介绍了Tetrate Service Bridge（TSB）生态系统中统一网关的概念，解释了其重要性，并提供了详细的使用场景。\n简介 统一网关是在TSB 1.7.0中引入的关键功能，它将Tier1Gateway和IngressGateway的功能合并到一个称为Gateway的公共资源中。这种统一简化了网关管理过程，并提供了更一致的体验。\n从TSB 1.7.0开始，Tier1Gateway和IngressGateway资源将被弃用，我们强烈建议使用Gateway资源满足你的所有网关需求。前Tier1 Gateway现在将被统称为Edge Gateway。\n统一网关选项卡无缝集成到TSB UI中，使得任何网关的配置都变得容易，不管它是作为Tier 1还是Tier 2网关工作。\nTSB UI 中的 Unified Gateway 为什么需要统一网关？ 在我们的旅程早期，我们认识到我们的客户对集群特定（Tier 2）和跨云供应商（Tier1）网关有不同的需求。因此，我们开发了不同的网关解决方案来满足这些不同的需求。然而，随着我们的Gateway API的发展和客户需求变得更加复杂，我们不断 …","relpermalink":"/tsb/howto/gateway/unified-gateway/","summary":"本文介绍了Tetrate Service Bridge（TSB）生态系统中统一网关的概念，解释了其重要性，并提供了详细的使用场景。 简介 统一网关是在TSB 1.7.0中引入的关键功能，它将Tier1Gateway和Ing","title":"统一网关"},{"content":"本页面提供了开始使用Tetrate Service Bridge（TSB）安装所需的先决条件和下载说明的全面概述。\n要有效地管理TSB服务网格，需要对Kubernetes和Docker仓库操作有深入的了解。我们建议咨询它们各自的支持文档以获取额外的指导。\n先决条件 你可以安装用于生产的TSB，也可以安装用于演示配置文件以快速了解TSB。请查看以下表格中的每个要求：\n生产TSB 演示/快速入门TSB Kubernetes 集群：\nEKS 1.21 - 1.24\nGKE 1.21 - 1.24\nAKS 1.21 - 1.24（包括 Azure Stack HCI）\nOpenShift 4.7 - 4.11\nDocker UCP 3.2.5 或更高版本 ✓ ✓ Docker UCP 3.2.5或更高版本 ✓ ✓ 私有Docker注册表（HTTPS） ✓ ✓ Tetrate存储库帐户和API密钥（如果你尚未拥有此内容，请联系Tetrate） ✓ ✓ Docker引擎18.03.01或更高版本，具有对私有Docker注册表的推送访问权限 ✓ ✓ PostgreSQL 11.1或更高版本 ✓ 打 …","relpermalink":"/tsb/setup/requirements-and-download/","summary":"本页面提供了开始使用Tetrate Service Bridge（TSB）安装所需的先决条件和下载说明的全面概述。 要有效地管理TSB服务网格，需要对Kubernetes和Docker仓库操作有深入的了解。我们建议咨询","title":"先决条件和下载"},{"content":"本指南将引导你完成 TSB 演示配置文件的安装，该配置文件旨在快速概述 TSB 的功能。演示配置文件包括 PostgreSQL、Elasticsearch 和 LDAP，所有这些都在 Kubernetes 集群上进行编排。为了确保无缝体验，你的集群应包含 3-6 个节点，每个节点至少配备 4 个 vCPU 和 16 GB 内存。集群还必须建立默认存储类，并能够为 Elasticsearch 和 PostgreSQL 创建最小容量为 100 GB 的持久卷声明。\n在继续之前，请参阅 TSB 支持政策来验证与你的 Kubernetes 版本的兼容性。\n先决条件 要安装演示配置文件，请确保你已完成以下步骤：\n1. 获取 tctl 并同步镜像 首先按照下载部分中概述的步骤下载 tctl 。此外，按照同步容器镜像中所述同步所需的容器镜像。\n2. 设置 Kubernetes 集群 准备一个要安装演示配置文件的 Kubernetes 集群。创建集群的具体步骤取决于你的环境。有关创建 Kubernetes 集群的具体说明，请参阅你的环境手册。\n使用 kind 如果你使用 kind 集群进行安装，请按照 …","relpermalink":"/tsb/setup/self-managed/demo-installation/","summary":"本指南将引导你完成 TSB 演示配置文件的安装，该配置文件旨在快速概述 TSB 的功能。演示配置文件包括 PostgreSQL、Elasticsearch 和 LDAP，所有这些都在 Kubernetes 集群上进行编排。为了确保无缝体验，你的","title":"演示安装"},{"content":" 注意\n自 1.7 版本以来，TSB 支持用于 TSB 管理平面 TLS 证书、内部证书和中间 Istio CA 证书的自动证书管理。详细信息请参阅 自动证书管理。 有 4 种 TSB 运算符需要了解的证书类型：\nTSB 内部证书：用于 TSB 内部组件相互信任的证书。 应用 TLS 证书：提供给应用程序用户的证书，用于 Web 浏览器或工具。 中间 Istio CA 证书：用于签发 Istio 工作负载叶子证书的中间 CA 证书。 工作负载叶子证书：针对每个代理和网关签发的证书。 下面的图片显示了这些证书及其与 TSB 组件和你的应用程序的关系。\nTSB 内部证书 TSB 的全局控制平面 (XCP) 从管理平面分发配置到控制平面集群。XCP 由 XCP central 和 XCP edge 组成。XCP central 部署在管理平面，TSB 服务器通过名为 MPC 的组件与其交互。TSB 内部证书（图片中突出显示为绿色）用于保护 XCP central、XCP edge、MPC 组件之间的通信。TSB 使用带 TLS 的 JWT 来确保通信的安全性。在部署 TSB 之前，你需要准备 …","relpermalink":"/tsb/setup/certificate/certificate-setup/","summary":"注意 自 1.7 版本以来，TSB 支持用于 TSB 管理平面 TLS 证书、内部证书和中间 Istio CA 证书的自动证书管理。详细信息请参阅 自动证书管理。 有 4 种 TSB 运算符需要了解的证书类型： TSB 内部证书：用于 TSB 内部组件相互信任的证书。 应用 TLS 证","title":"证书类型"},{"content":"生产 Argo CD 支持多种不同的 Kubernetes 清单定义方式：\nKustomize应用程序 Helm Chart YAML/JSON/Jsonnet 清单的目录，包括Jsonnet。 任何配置为配置管理插件的自定义配置管理工具 开发 Argo CD 还支持直接上传本地清单。由于这是 GitOps 范式的反模式，因此只能出于开发目的而这样做。override需要具有权限的用户（通常是管理员）才能在本地上传清单。支持上述所有不同的 Kubernetes 部署工具。上传本地应用程序：\n$ argocd app sync APPNAME --local /path/to/dir/ ","relpermalink":"/argo-cd/user-guide/application-sources/","summary":"生产 Argo CD 支持多种不同的 Kubernetes 清单定义方式： Kustomize应用程序 Helm Chart YAML/JSON/Jsonnet 清单的目录，包括Jsonnet。 任何配置为配置管理插件的自定义配置管理工具 开发 Argo CD 还支持直接上传本地清单。由于这是 GitOps 范式的反模式，","title":"工具"},{"content":" Argo CD 架构 组件 API 服务器 API 服务器是一个 gRPC/REST 服务器，用于公开 Web UI、CLI 和 CI/CD 系统使用的 API。它具有以下职责：\n应用程序管理和状态报告 调用应用程序操作（例如同步、回滚、用户定义的操作） 存储为 K8s 机密的存储库和集群凭据管理 身份验证和身份验证委派到外部身份提供者 RBAC 执行 Git webhook 事件的侦听器/转发器 存储库服务器 存储库服务器是一个内部服务，它维护 Git 存储库的本地缓存，其中包含应用程序清单。它负责在提供以下输入时生成并返回 Kubernetes 清单：\n存储库 URL 修订版（提交、标记、分支） 应用程序路径 模板特定设置：参数、helm values.yaml 应用程序控制器 应用程序控制器是一个 Kubernetes 控制器，它不断监视运行中的应用程序，并将当前的实时状态与期望的目标状态（如 repo 中指定的）进行比较。它检测 OutOfSync 应用程序状态，并可选择采取纠正措施。它负责调用任何用户定义的生命周期事件钩子（PreSync、Sync、PostSync）\n","relpermalink":"/argo-cd/operator-manual/architecture/","summary":"Argo CD 架构 组件 API 服务器 API 服务器是一个 gRPC/REST 服务器，用于公开 Web UI、CLI 和 CI/CD 系统使用的 API。它具有以下职责： 应用程序管理和状态报告 调用应用程序操作（例如同步、回滚、用户定义的操作） 存储为 K8s 机密的存储库和集","title":"架构概述"},{"content":"什么是 Argo CD？ Argo CD 是一个基于声明式 GitOps 的 Kubernetes 应用程序交付工具。\nArgo CD UI 为什么选择 Argo CD？ 应用程序定义、配置和环境应该是声明式的，并进行版本控制。应用程序部署和生命周期管理应该是自动化的、可审计的和易于理解的。\n入门指南 快速入门 kubectl create namespace argocd kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml 请参阅我们的入门指南。我们还为其他功能提供了面向用户的文档。如果你想升级 ArgoCD，请参阅升级指南。我们还为有兴趣构建第三方集成的开发人员提供面向开发者的文档。\n工作原理 Argo CD 遵循使用 Git 存储库作为定义期望应用程序状态的真实来源的 GitOps 模式。Kubernetes 清单可以通过以下几种方式指定：\nkustomize 应用程序 helm chart jsonnet …","relpermalink":"/argo-cd/overview/","summary":"什么是 Argo CD？ Argo CD 是一个基于声明式 GitOps 的 Kubernetes 应用程序交付工具。 Argo CD UI 为什么选择 Argo CD？ 应用程序定义、配置和环境应该是声明式的，并进行版本控制。应用程序部署和生命周期管理应该是自动化的、可审计的和易于理解的","title":"Argo CD 简介"},{"content":"介绍 应用程序集控制器是一个Kubernetes 控制器，它添加了对ApplicationSet自定义资源定义 (CRD) 的支持。这个控制器/CRD 使得自动化和更大的灵活性在管理 Argo CD 应用程序跨大量的集群和在 monorepos 内成为可能，同时它使多租户 Kubernetes 集群上的自助式使用成为可能。\n应用程序集控制器与现有的 Argo CD 安装一起工作。Argo CD 是一个声明性的、GitOps 持续交付工具，允许开发人员从他们现有的 Git 工作流程中定义和控制 Kubernetes 应用程序资源的部署。\n从 Argo CD v2.3 开始，应用程序集控制器与 Argo CD 捆绑在一起。\n应用程序集控制器通过添加支持面向集群管理员的附加功能来补充 Argo CD。ApplicationSet控制器提供：\n使用单个 Kubernetes 清单定位多个 Kubernetes 集群的能力，使用 Argo CD 部署多个应用程序的能力 使用单个 Kubernetes 清单从一个或多个 Git 存储库中部署多个应用程序的能力 改进了对 monorepos 的支持： …","relpermalink":"/argo-cd/operator-manual/applicationset/overview/","summary":"介绍 应用程序集控制器是一个Kubernetes 控制器，它添加了对ApplicationSet自定义资源定义 (CRD) 的支持。这个控制器/CRD 使得自动化和更大的灵活性在管理 Argo CD 应用程序跨大量的集群和在 monorepos 内成为","title":"ApplicationSet 控制器介绍"},{"content":"Argo Rollouts 提供了多种形式的分析方法来驱动渐进式交付。本文档描述了如何实现不同形式的渐进式交付，包括分析执行的时间点、频率和发生次数。\n自定义资源定义 CRD 描述 Rollout Rollout 作为 Deployment 资源的替代品，提供了额外的蓝绿和金丝雀更新策略。这些策略可以在更新过程中创建 AnalysisRuns 和 Experiments，这些 AnalysisRuns 和 Experiments 可以推进更新，或者中止更新。 AnalysisTemplate AnalysisTemplate 是一个模板规范，定义了如何执行金丝雀分析，例如应该执行的指标、其频率以及被视为成功或失败的值。AnalysisTemplates 可以使用输入值进行参数化。 ClusterAnalysisTemplate ClusterAnalysisTemplate 类似于 AnalysisTemplate，但它不限于其命名空间。它可以被任何 Rollout 在整个集群中使用。 AnalysisRun AnalysisRun 是 AnalysisTemplate 的一个实例 …","relpermalink":"/argo-rollouts/analysis/overview/","summary":"Argo Rollouts 提供了多种形式的分析方法来驱动渐进式交付。本文档描述了如何实现不同形式的渐进式交付，包括分析执行的时间点、频率和发生次数。 自定义资源定义 CRD 描述 Rollout Rollout 作为 Deployment 资源的替代品，提供了额外的蓝绿和金丝雀更新策","title":"分析和渐进式交付"},{"content":"Kubectl 插件是一种扩展 kubectl 命令提供额外行为的方式。通常，它们用于添加新功能到 kubectl 并自动化可脚本化的工作流程来操作集群。官方文档可在 此处 找到。\nArgo Rollouts 提供了一个 Kubectl 插件来丰富 Rollouts、Experiments 和 Analysis 的体验。它提供了可视化 Argo Rollouts 资源的能力并从命令行上运行常规操作，例如 promote 或 retry。\n安装 请参阅 安装指南 了解安装插件的说明。\n用法 获取有关可用的 Argo Rollouts kubectl 插件命令的信息的最佳方法是运行 kubectl argo rollouts。插件列出了该工具可以执行的所有可用命令以及每个命令的描述。所有插件命令与 Kubernetes API 服务器交互，并使用 KubeConfig 凭据进行身份验证。由于插件利用运行命令的用户的 KubeConfig，因此插件具有这些配置的权限。\n与 kubectl 类似，该插件使用许多与 kubectl 相同的标志。例如，kubectl argo rollouts …","relpermalink":"/argo-rollouts/kubectl-plugin/overview/","summary":"Kubectl 插件是一种扩展 kubectl 命令提供额外行为的方式。通常，它们用于添加新功能到 kubectl 并自动化可脚本化的工作流程来操作集群。官方文档可在 此处 找到。 Argo Rollouts 提供了一个 Kubectl 插件来丰富 Rollouts、Experiments 和 Analysis","title":"Kubectl 插件"},{"content":"🔔 重要提示：自版本 1.1 起可用。\nArgo Rollouts 提供通知功能，由Notifications Engine支持。控制器管理员可以利用灵活的触发器和模板系统来配置终端用户请求的通知。终端用户可以通过在 Rollout 对象中添加注释来订阅配置的触发器。\n配置 触发器定义了通知应该在何时发送以及通知内容模板。默认情况下，Argo Rollouts 附带了一系列内置触发器，涵盖了 Argo Rollout 生命周期的最重要事件。触发器和模板都在argo-rollouts-notification-configmap ConfigMap 中配置。为了快速入门，你可以使用在notifications-install.yaml中定义的预配置通知模板。\n如果你正在利用 Kustomize，则建议将notifications-install.yaml作为远程资源包含在你的kustomization.yaml文件中：\napiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - …","relpermalink":"/argo-rollouts/notifications/overview/","summary":"🔔 重要提示：自版本 1.1 起可用。 Argo Rollouts 提供通知功能，由Notifications Engine支持。控制器管理员可以利用灵活的触发器和模板系统来配置终端用户请求的通知。终端用户可以通过在 Rollout 对象中添加注释来订阅","title":"通知"},{"content":"Argo Rollouts 是一个 Kubernetes 自定义资源定义 (Custom Resource Definition, CRD)，扩展了 Kubernetes Deployment Controller，它添加了渐进式交付（Progressive Delivery）和蓝绿部署（Blue Green Deployment）等交付策略。Argo Rollouts 可以与 Istio、Linkerd 等服务网格和其他流量管理工具集成。\n流量管理 流量管理是通过控制数据平面，为应用程序创建智能的路由规则。这些路由规则可以操作流量，将其引导到应用程序的不同版本，从而实现渐进式交付。这些控制规则通过确保只有一小部分用户接收新版本，从而限制了新版本的波及范围。\n实现流量管理有各种技术：\n原始百分比（例如，5% 的流量应该流向新版本，而其余的流向稳定版本） 基于头的路由（例如，将带有特定标头的请求发送到新版本） 交叉流量，其中所有流量都被复制并并行发送到新版本（但响应被忽略） Kubernetes 中的流量管理工具 核心 Kubernetes 对象没有细粒度的工具来满足所有流量管理要求。在 …","relpermalink":"/argo-rollouts/traffic-management/overview/","summary":"Argo Rollouts 是一个 Kubernetes 自定义资源定义 (Custom Resource Definition, CRD)，扩展了 Kubernetes Deployment Controller，它添加了渐进式交付（Progressive Delivery）和蓝绿部署（Blue Green Deployment）等交付策略。Argo Rollouts","title":"流量管理概览"},{"content":"本指南通过演示部署、升级、推广和终止 Rollout 来演示 Argo Rollouts 的各种概念和特性。\n要求 安装了 argo-rollouts 控制器的 Kubernetes 集群（请参阅 安装指南） 安装了带有 argo-rollouts 插件的 kubectl（请参阅 安装指南） 1. 部署 Rollout 首先，我们部署一个 Rollout 资源和一个针对该 Rollout 的 Kubernetes Service。本指南中的示例 Rollout 利用了金丝雀升级策略，该策略将 20％的流量发送到金丝雀，然后进行手动推广，最后对其余升级进行逐渐自动化的流量增加。此行为在 Rollout spec 的以下部分中描述：\nspec: replicas: 5 strategy: canary: steps: - setWeight: 20 - pause: {} - setWeight: 40 - pause: {duration: 10} - setWeight: 60 - pause: {duration: 10} - setWeight: 80 - pause: …","relpermalink":"/argo-rollouts/getting-started/basic-usage/","summary":"本指南通过演示部署、升级、推广和终止 Rollout 来演示 Argo Rollouts 的各种概念和特性。 要求 安装了 argo-rollouts 控制器的 Kubernetes 集群（请参阅 安装指南） 安装了带有 argo-rollouts 插件的 kubectl（请参阅 安装指南） 1. 部署 Rollout 首先，我们部署一个 Rollout 资源和一个针","title":"基础使用"},{"content":"什么是 Argo Rollouts？ Argo Rollouts 是一组 Kubernetes 控制器和自定义资源（CRD），为 Kubernetes 提供高级部署功能，例如蓝绿、金丝雀、金丝雀分析、实验和渐进式交付等功能。\nArgo Rollouts 可选地与 Ingress 控制器和服务网格集成，利用它们的流量整形能力在更新期间逐渐将流量转移到新版本。此外，Rollouts 可以查询和解释来自各种提供商的度量标准，以验证关键 KPI 并在更新期间驱动自动升级或回滚。\n这是一个演示视频（点击在 Youtube 上观看）：\n为什么选择 Argo Rollouts？ Kubernetes Deployment 对象支持滚动更新策略，该策略提供了一组基本的安全性保证（就绪探针）来保证更新期间的安全性。但是，滚动更新策略面临许多限制：\n对滚动更新速度的控制很少 无法控制流量流向新版本 就绪探针不适用于更深入的、压力或一次性检查 没有查询外部度量标准以验证更新的能力 可以停止进程，但无法自动中止并回滚更新 因此，在大型高容量生产环境中，滚动更新往往被认为是过于冒险的更新过程，因为它无法控制爆炸 …","relpermalink":"/argo-rollouts/overview/","summary":"什么是 Argo Rollouts？ Argo Rollouts 是一组 Kubernetes 控制器和自定义资源（CRD），为 Kubernetes 提供高级部署功能，例如蓝绿、金丝雀、金丝雀分析、实验和渐进式交付等功能。 Argo Rollouts 可选地与 Ingress 控制器和服务网格集成，利用它们的流量整形能","title":"Argo Rollouts 简介"},{"content":"蓝绿部署允许用户减少同时运行多个版本的时间。\n概述 除了管理 ReplicaSet 外，在 BlueGreenUpdate 策略期间，Rollout 控制器还将修改 Service 资源。Rollout 规范要求用户在同一命名空间中指定对活动服务的引用以及可选的预览服务。活动服务用于将常规应用程序流量发送到旧版本，而预览服务用于将流量漏斗到新版本。Rollout 控制器通过向这些服务的选择器注入 ReplicaSet 的唯一哈希来确保正确的流量路由。这允许 Rollout 定义一个活动和预览堆栈以及从预览到活动的过程。\n当 Rollout 的 .spec.template 字段发生更改时，控制器将创建新的 ReplicaSet。如果活动服务没有将流量发送到 ReplicaSet，则控制器将立即开始将流量发送到 ReplicaSet。否则，活动服务将指向旧 ReplicaSet，而 ReplicaSet 变得可用。一旦新的 ReplicaSet 变得可用，控制器将修改活动服务以指向新的 ReplicaSet。 …","relpermalink":"/argo-rollouts/rollout/deployment-strategies/bluegreen/","summary":"蓝绿部署允许用户减少同时运行多个版本的时间。 概述 除了管理 ReplicaSet 外，在 BlueGreenUpdate 策略期间，Rollout 控制器还将修改 Service 资源。Rollout 规范要求用户在同一命名空间中指定对活动服务的引用以及可选的预览服务。活动服","title":"蓝绿部署策略"},{"content":"本书介绍了服务身份的 SPIFFE 标准，以及 SPIFFE 的参考实现 SPIRE。这些项目为现代异构基础设施提供了一个统一的身份控制平面。这两个项目都是开源的，是云原生计算基金会（CNCF）的一部分。\n随着企业发展他们的应用架构以充分利用新的基础设施技术，他们的安全模式也必须不断发展。软件已经从一个盒子上的单片机发展到几十或几百个紧密联系的微服务，这些微服务可能分布在公共云或私人数据中心的数千个虚拟机上。在这个新的基础设施世界里，SPIFFE 和 SPIRE 帮助保持系统的安全。\n本书努力提炼 SPIFFE 和 SPIRE 的最重要的专家的经验，以提供对身份问题的深刻理解，帮助你解决这个问题。通过这些项目，开发和运维可以使用新的基础设施技术构建软件，同时让安全团队从昂贵和耗时的人工安全流程中解脱出来。\n关于零号乌龟 访问控制、秘密管理和身份都是相互依赖的。大规模地管理秘密需要有效的访问控制；实施访问控制需要身份；证明身份需要拥有一个秘密。保护一个秘密需要想出一些办法来保护另一个秘密，这就需要保护那个秘密，以此类推。\n这让人想起一个著名的轶事：一个女人打断了一位哲学家的讲座，告诉他世 …","relpermalink":"/spiffe/preface/","summary":"本书介绍了服务身份的 SPIFFE 标准，以及 SPIFFE 的参考实现 SPIRE。这些项目为现代异构基础设施提供了一个统一的身份控制平面。这两个项目都是开源的，是云原生计算基金会（CNCF）的一部分。 随着企业发展他们的应用架构","title":"关于本书"},{"content":"Linux 内核在网络堆栈中支持一组 BPF 钩子（hook），可用于运行 BPF 程序。Cilium 数据路径使用这些钩子来加载 BPF 程序，这些程序一起使用时会创建更高级别的网络结构。\n以下是 Cilium 使用的钩子列表和简要说明。有关每个钩子细节的更详尽的文档，请参阅 BPF 和 XDP 参考指南。\nXDP：XDP BPF 钩子位于网络驱动程序中的最早可能点，并在数据包接收时触发 BPF 程序的运行。这实现了可能的最佳数据包处理性能，因为程序在任何其他处理发生之前直接在数据包数据上运行。此钩子非常适合运行丢弃恶意或意外流量的过滤程序以及其他常见的 DDOS 保护机制。\n流量控制入口/出口：附加到流量控制（traffic control，简称 TC）入口钩子的 BPF 程序附加到网络接口，与 XDP 相同，但将在网络堆栈完成数据包的初始处理后运行。该钩子在三层网络之前运行，但可以访问与数据包关联的大部分元数据。这非常适合进行本地节点处理，例如应用三层/四层端点策略并将流量重定向到端点。对于面向网络的设备，TC 入口钩子可以与上面的 XDP 钩子耦合。完成此操作后，可以合理地假设 …","relpermalink":"/cilium-handbook/ebpf/intro/","summary":"Linux 内核在网络堆栈中支持一组 BPF 钩子（hook），可用于运行 BPF 程序。Cilium 数据路径使用这些钩子来加载 BPF 程序，这些程序一起使用时会创建更高级别的网络结构。 以下是 Cilium 使用的钩子列表和简要说明。有关每个钩子","title":"eBPF 数据路径介绍"},{"content":"Cilium 能为 Kubernetes 集群提供什么？ 在 Kubernetes 集群中运行 Cilium 时提供以下功能：\nCNI 插件支持，为 pod 连接 提供 联网。 NetworkPolicy 资源的基于身份的实现，用于隔离三层和四层网络 pod 的连接。 以 CustomResourceDefinition 形式对 NetworkPolicy 的扩展，扩展策略控制以添加： 针对以下应用协议的入口和出口执行七层策略： HTTP Kafka 对 CIDR 的出口支持以保护对外部服务的访问 强制外部无头服务自动限制为服务配置的 Kubernetes 端点集 ClusterIP 实现为 pod 到 pod 的流量提供分布式负载平衡 完全兼容现有的 kube-proxy 模型 Pod 间连接 在 Kubernetes 中，容器部署在称为 pod 的单元中，其中包括一个或多个可通过单个 IP 地址访问的容器。使用 Cilium，每个 pod 从运行 pod 的 Linux 节点的节点前缀中获取一个 IP 地址。有关其他详细信息，请参阅 IP 地址管理（IPAM）。在没有任何网络安全策 …","relpermalink":"/cilium-handbook/kubernetes/intro/","summary":"Cilium 能为 Kubernetes 集群提供什么？ 在 Kubernetes 集群中运行 Cilium 时提供以下功能： CNI 插件支持，为 pod 连接 提供 联网。 NetworkPolicy 资源的基于身份的实现，用于隔离三层和四层网络 pod 的连接。 以 CustomResourceDefinition 形式对 NetworkPolicy 的扩展，扩展策略控制以添加： 针对以下应用协议的入","title":"Kubernetes 集成介绍"},{"content":"Cilium 在多个层面上提供安全性。可以单独使用或组合使用。\n基于身份：端点之间的连接策略（三层），例如任何带有标签的端点 role=frontend 都可以连接到任何带有标签的端点 role=backend。 限制传入和传出连接的可访问端口（四层），例如带标签的端点 role=frontend只能在端口 443（https）上进行传出连接，端点role=backend 只能接受端口 443（https）上的连接。 应用程序协议级别的细粒度访问控制，以保护 HTTP 和远程过程调用（RPC）协议，例如带有标签的端点 role=frontend 只能执行 REST API 调用 GET /userdata/[0-9]+，所有其他与 role=backend API 的交互都受到限制。 ","relpermalink":"/cilium-handbook/security/intro/","summary":"Cilium 在多个层面上提供安全性。可以单独使用或组合使用。 基于身份：端点之间的连接策略（三层），例如任何带有标签的端点 role=frontend 都可以连接到任何带有标签的端点 role=backend。 限制传入和传出连接的可访问端口（","title":"介绍"},{"content":"封装 当没有提供配置时，Cilium 会自动在此模式下运行，因为它是对底层网络基础设施要求最低的模式。\n在这种模式下，所有集群节点使用基于 UDP 的封装协议 VXLAN 或 Geneve。Cilium 节点之间的所有流量都被封装。\n对网络的要求 封装依赖于正常的节点到节点的连接。这意味着如果 Cilium 节点已经可以互相到达，那么所有的路由要求都已经满足了。\n底层网络和防火墙必须允许封装数据包：\n封装方式 端口范围 / 协议 VXLAN（默认） 8472/UDP Geneve 6081/UDP 封装模式的优点 封装模式具有以下优点：\n简单\n连接集群节点的网络不需要知道 PodCIDR。集群节点可以产生多个路由或链路层域。只要集群节点可以使用 IP/UDP 相互访问，底层网络的拓扑就无关紧要。\n寻址空间\n由于不依赖于任何底层网络限制，如果相应地配置了 PodCIDR 大小，可用的寻址空间可能会更大，并且允许每个节点运行任意数量的 Pod。\n自动配置\n当与 Kubernetes 等编排系统一起运行时，集群中所有节点的列表（包括它们关联的分配前缀节点）会自动提供给每个代理。加入集群的新节 …","relpermalink":"/cilium-handbook/networking/routing/","summary":"封装 当没有提供配置时，Cilium 会自动在此模式下运行，因为它是对底层网络基础设施要求最低的模式。 在这种模式下，所有集群节点使用基于 UDP 的封装协议 VXLAN 或 Geneve。Cilium 节点之间的所有流量都被封装","title":"路由"},{"content":"本文将为你简要介绍 Cilium 和 Hubble。\n什么是 Cilium？ Cilium 是开源软件，用于透明地保护使用 Docker 和 Kubernetes 等 Linux 容器管理平台部署的应用服务之间的网络连接。\nCilium 的基础是一种新的 Linux 内核技术，称为 eBPF，它使强大的安全可视性和控制逻辑动态插入 Linux 本身。由于 eBPF 在 Linux 内核内运行，Cilium 安全策略的应用和更新无需对应用程序代码或容器配置进行任何改动。\n什么是 Hubble？ Hubble 是一个完全分布式的网络和安全可观测性平台。它建立在 Cilium 和 eBPF 之上，以完全透明的方式实现对服务的通信行为以及网络基础设施的深度可视性。\n通过建立在 Cilium 之上，Hubble 可以利用 eBPF 实现可视性。依靠 eBPF，所有的可视性都是可编程的，并允许采用一种动态的方法，最大限度地减少开销，同时按照用户的要求提供深入和详细的可视性。Hubble 的创建和专门设计是为了最好地利用这些新的 eBPF 力量。\nHubble 可以回答诸如以下问题。\n服务依赖和拓扑 …","relpermalink":"/cilium-handbook/intro/","summary":"本文将为你简要介绍 Cilium 和 Hubble。 什么是 Cilium？ Cilium 是开源软件，用于透明地保护使用 Docker 和 Kubernetes 等 Linux 容器管理平台部署的应用服务之间的网络连接。 Cilium 的基础是一种新的 Linux 内核技术，称为 eBPF，它使强大的安全可","title":"Cilium 和 Hubble 简介"},{"content":"Cilium 代理（agent）和 Cilium 网络策略的配置决定了一个端点（Endpoint）是否接受来自某个来源的流量。代理可以进入以下三种策略执行模式：\ndefault\n如果任何规则选择了一个 Endpoint 并且该规则有一个入口部分，那么该端点就会在入口处进入默认拒绝状态。如果任何规则选择了一个 Endpoint 并且该规则有一个出口部分，那么该端点就会在出口处进入默认拒绝状态。这意味着端点开始时没有任何限制，一旦有规则限制其在入口处接收流量或在出口处传输流量的能力，那么端点就会进入白名单模式，所有流量都必须明确允许。\nalways\n在 always 模式下，即使没有规则选择特定的端点，也会在所有端点上启用策略执行。如果你想配置健康实体，在启动 cilium-agent 时用 enable-policy=always 检查整个集群的连接性，你很可能想启用与健康端点的通信。\nnever\n在“never\u0026#34; 模式下，即使规则选择了特定的端点，所有端点上的策略执行也被禁用。换句话说，所有流量都允许来自任何来源（入口处）或目的地（出口处）。\n要在运行时为 Cilium 代理管理的所有 …","relpermalink":"/cilium-handbook/policy/intro/","summary":"Cilium 代理（agent）和 Cilium 网络策略的配置决定了一个端点（Endpoint）是否接受来自某个来源的流量。代理可以进入以下三种策略执行模式： default 如果任何规则选择了一个 Endpoint 并且该规则有一个入口部分，那么该端点就会","title":"网络策略模式"},{"content":"本文将为你介绍 Cilium 和 Hubble 部署中包含的组件。\nCilium 包含以下组件：\n代理 客户端 Operator CNI 插件 Hubble 包含以下组件：\n服务器 中继器 客户端 图形用户界面 eBPF 另外你还需要一个数据库来存储代理的状态。\n下图展示的 Cilium 部署的组件。\nCilium 组件示意图 Cilium 代理\nCilium 代理（cilium-agent）在集群的每个节点上运行。在高层次上，代理接受通过 Kubernetes 或 API 的配置，描述网络、服务负载均衡、网络策略、可视性和监控要求。\nCilium 代理监听来自编排系统（如 Kubernetes）的事件，以了解容器或工作负载的启动和停止时间。它管理 eBPF 程序，Linux 内核用它来控制这些容器的所有网络访问。\n客户端（CLI）\nCilium CLI 客户端（cilium）是一个命令行工具，与 Cilium 代理一起安装。它与运行在同一节点上的 Cilium 代理的 REST API 互动。CLI 允许检查本地代理的状态。它还提供工具，直接访问 eBPF map 以验证其状态。 …","relpermalink":"/cilium-handbook/concepts/overview/","summary":"本文将为你介绍 Cilium 和 Hubble 部署中包含的组件。 Cilium 包含以下组件： 代理 客户端 Operator CNI 插件 Hubble 包含以下组件： 服务器 中继器 客户端 图形用户界面 eBPF 另外你还需要一个数据库来存储代理的状态。 下图展示的 Cilium 部署的组件。 Cilium 组件示意图 Cilium 代","title":"组件概览"},{"content":"在过去的几年里，eBPF 已经从相对默默无闻变成了现代基础设施建设中最热门的技术领域之一。就我个人而言，自从看到 Thomas Graf 在 DockerCon 17 的黑带会议（Black Blet）1 上谈到 eBPF 时，我就对它的可能性感到兴奋。在云原生计算基金会（CNCF），我在技术监督委员会（TOC）的同事把 eBPF 作为我们预测 2021 年将会起飞的重点技术之一来关注。超过 2500 人报名参加了当年的 eBPF 峰会线上会议，世界上最先进的几家软件工程公司共同创建了 eBPF 基金会。显然，人们对这项技术有很大的兴趣。\n在这个简短的报告中，我希望能给你一些启示，为什么人们对 eBPF 如此兴奋，以及它在现代计算环境中提供的工具能力。你会了解到 eBPF 是什么以及为什么它如此强大。还有一些代码实例，以使这种感觉更加具象化（但如果你愿意，你可以跳过这些）。\n你将了解到在建立支持 eBPF 的工具时涉及的内容，以及为什么 eBPF 在如此短的时间内变得如普遍。\n在这份简短的报告中，难免无法了解所有的细节，但如果你想更深入地了解，我将给你一些参考信息。\n扩展的伯克利数据包 …","relpermalink":"/what-is-ebpf/introduction/","summary":"在过去的几年里，eBPF 已经从相对默默无闻变成了现代基础设施建设中最热门的技术领域之一。就我个人而言，自从看到 Thomas Graf 在 DockerCon 17 的黑带会议（Black Blet）1 上谈到 eBPF 时，我就对它的可能性感到兴奋。在云原生","title":"第一章：eBPF 简介"},{"content":"Kubernetes 是云原生时代的 POSIX 在单机时代，POSIX 是类 UNIX 系统的通用 API，而在云原生时代，Kubernetes 是云操作系统的的 POSIX，它定义了基于云的分布式系统的 API。下表将 Kubernetes 与 POSIX 进行了对比。\n对比项 Linux Kubernetes 隔离单元 进程 Pod 硬件 单机 数据中心 并发 线程 容器 资源管理 进程内存\u0026amp;CPU 内存、CPU Limit/Request 存储 文件 ConfigMap、Secret、Volume 网络 端口绑定 Service 终端 tty、pty、shell kubectl exec 网络安全 IPtables NetworkPolicy 权限 用户、文件权限 ServiceAccount、RBAC 注意\n我们不能说 Linux 就是 POSIX，只能说 Linux 是 UNIX 兼容的。 参考 Kubernetes is the POSIX of the cloud - home.robusta.dev ","relpermalink":"/cloud-native-handbook/kubernetes/what-is-kubernetes/","summary":"Kubernetes 是云原生时代的 POSIX 在单机时代，POSIX 是类 UNIX 系统的通用 API，而在云原生时代，Kubernetes 是云操作系统的的 POSIX，它定义了基于云的分布式系统的 API。下表将 Kubernetes 与 POSIX 进行了对比。 对比项 Linux Kubernetes 隔","title":"什么是 Kubernetes?"},{"content":"首先我们来阐述下将应用迁移到云原生架构的动机。\n速度 天下武功，唯快不破，市场竞争亦是如此。想象一下，能够快速创新、实验并交付软件的企业，与使用传统软件交付模式的企业，谁将在市场竞争中胜出呢？\n在传统企业中，为应用提供环境和部署新版本花费的时间通常以天、周或月来计算。这种速度严重限制了每个发行版可以承担的风险，因为修复这些错误往往跟发行一个新版本有差不多的耗时。\n互联网公司经常提到它们每天几百次发布的实践。为什么频繁发布如此重要？如果你可以每天实现几百次发布，你们就可以几乎立即从错误的版本恢复过来。如果你可以立即从错误中恢复过来，你就能够承受更多的风险。如果你可以承受更多的风险，你就可以做更疯狂的试验 —— 这些试验结果可能会成为你接下来的竞争优势。\n基于云基础设置的弹性和自服务的特性天生就适应于这种工作方式。通过调用云服务 API 来提供新的应用程序环境比基于表单的手动过程要快几个数量级。然后通过另一个 API 调用将代码部署到新的环境中。将自服务和 hook 添加到团队的 CI/CD 服务器环境中进一步加快了速度。现在，我们可以回答精益大师 Mary Poppendick 提出的问 …","relpermalink":"/migrating-to-cloud-native-application-architectures/the-rise-of-cloud-native/why-cloud-native-application-architectures/","summary":"首先我们来阐述下将应用迁移到云原生架构的动机。 速度 天下武功，唯快不破，市场竞争亦是如此。想象一下，能够快速创新、实验并交付软件的企业，与使用传统软件交付模式的企业，谁将在市场竞争中胜出呢？ 在传统企业中","title":"1.1 为何使用云原生应用架构"},{"content":"由于微服务通常是以容器的形式实现的，因此容器编排和资源管理平台被用于服务的部署、运维和维护。\n一个典型的协调和资源管理平台由各种逻辑（形成抽象层）和物理工件组成，用于部署容器。例如，在 Kubernetes 中，容器在最小的部署单元内运行，称为 Pod。一个 Pod 理论上可以承载一组容器，但通常情况下，一个 Pod 内只运行一个容器。一组 Pod 被定义在所谓的节点内，节点可以是物理机或虚拟机（VM）。一组节点构成了一个集群。通常情况下，需要单个微服务的多个实例来分配工作负载，以达到预期的性能水平。集群是一个资源池（节点），用于分配微服务的工作负载。使用的技术之一是横向扩展，即访问频率较高的微服务被分配更多的实例或分配到具有更多资源（如 CPU 和 / 或内存）的节点。\n","relpermalink":"/service-mesh-devsecops/reference-platform/container-orchestration-and-resource-management-platform/","summary":"由于微服务通常是以容器的形式实现的，因此容器编排和资源管理平台被用于服务的部署、运维和维护。 一个典型的协调和资源管理平台由各种逻辑（形成抽象层）和物理工件组成，用于部署容器。例如，在 Kubernetes 中，容器在最小的","title":"2.1 容器编排和资源管理平台"},{"content":"企业 IT 采用云原生架构所需的变革根本不是技术性的，而是企业文化和组织的变革，围绕消除造成浪费的结构、流程和活动。在本节中，我们将研究必要的文化转变。\n从信息孤岛到 DevOps 企业 IT 通常被组织成以下许多孤岛：\n软件开发 质量保证 数据库管理 系统管理 IT 运营 发布管理 项目管理 创建这些孤岛是为了让那些了解特定领域的人员来管理和指导那些执行该专业领域工作的人员。这些孤岛通常具有不同的管理层次，工具集、沟通风格、词汇表和激励结构。这些差异启发了企业 IT 目标的不同范式，以及如何实现这一目标。\n但这里面存在很多矛盾，例如开发和运维分别对软件变更持有的观念就是个经常被提起的例子。开发的任务通常被视为通过开发软件功能为组织提供额外的价值。这些功能本身就是向 IT 生态系统引入变更。所以开发的使命可以被描述为“交付变化”，而且经常根据有多少次变更来进行激励。\n相反，IT 运营的使命可以被描述为“防止变更”。IT 运营通常负责维护 IT 系统所需的可用性、弹性、性能和耐用性。因此，他们经常以维持关键绩效指标（KPI）来进行激励，例如平均故障间隔时间（MTBF）和平均恢复时 …","relpermalink":"/migrating-to-cloud-native-application-architectures/changes-needed/cultural-change/","summary":"企业 IT 采用云原生架构所需的变革根本不是技术性的，而是企业文化和组织的变革，围绕消除造成浪费的结构、流程和活动。在本节中，我们将研究必要的文化转变。 从信息孤岛到 DevOps 企业 IT 通常被组织成以下许多孤岛： 软件开发","title":"2.1 文化变革"},{"content":"在和客户讨论分解数据、服务和团队后，客户经常向我提出这样的问题，“太棒了！但是我们要怎样实现呢？”这是个好问题。如何拆分已有的单体应用并把他们迁移上云呢？\n事实证明，我已经看到了很多成功的例子，使用增量迁移这种相当可复制的模式，我现在向我所有的客户推荐这种模式。SoundCloud 和 Karma 就是公开的例子。\n本节中，我们将讲解如何一步步地将单体服务分解并将它们迁移到云上。\n新功能使用微服务形式 您可能感到很惊奇，第一步不是分解单体应用。我们假设您依然要在单体应用中构建服务。事实上，如果您没有任何新的功能来构建，那么您甚至不应该考虑这个分解。（鉴于我们的主要动机是速度，您如何维持原状还能获取速度呢？）\n团队决定，处理架构变化的最佳方法不是立即分解 Mothership 架构，而是不添加任何新的东西。我们所有的新功能以微服务形式构建…\n——Phil Calcado, SoundCloud\n所以不要继续再向单体应用中增加代码，将所有的新功能以微服务的形式构建。这是第一步就要考虑好的，因为从头开始构架一个服务比分解一个单体应用并提出服务出来容易和快速的多。\n然而有一点不可避免，就是新构 …","relpermalink":"/migrating-to-cloud-native-application-architectures/migration-cookbook/decomposition-recipes/","summary":"在和客户讨论分解数据、服务和团队后，客户经常向我提出这样的问题，“太棒了！但是我们要怎样实现呢？”这是个好问题。如何拆分已有的单体应用并把他们迁移上云呢？ 事实证明，我已经看到了很多成功的例子，使用增量","title":"3.1 分解架构"},{"content":"DevSecOps 是一种软件开发、部署和生命周期管理方法，它涉及到从整个应用程序或平台的一次大型发布转变为持续集成、持续交付和持续部署（CI/CD）方法。这种转变又要求公司的 IT 部门的结构和工作流程发生变化。最明显的变化是组织一个 DevSecOps 小组，由软件开发人员、安全专家和 IT 运维专家组成，负责应用程序（即微服务）的每一部分。这个较小的团队不仅能促进最初的敏捷开发和部署的效率和效果，还能促进后续的生命周期管理活动，如监控应用行为、开发补丁、修复错误或扩展应用。这种具有三个领域专业知识的跨职能团队的组成，构成了在组织中引入 DevSecOps 的关键成功因素。\n","relpermalink":"/service-mesh-devsecops/devsecops/organizational-preparedness-for-devsecops/","summary":"DevSecOps 是一种软件开发、部署和生命周期管理方法，它涉及到从整个应用程序或平台的一次大型发布转变为持续集成、持续交付和持续部署（CI/CD）方法。这种转变又要求公司的 IT 部门的结构和工作流程发生变化。最明显的变","title":"3.1 组织对 DevSecOps 的准备情况"},{"content":"对上述五类代码（即应用、应用服务、基础设施、策略和监控）的简要描述如下：\n应用程序代码和应用服务代码：前者包含一组特定业务事务的数据和应用逻辑，而后者包含所有服务的代码，如网络连接、负载均衡和网络弹性。 基础设施即代码（IaC）：用于提供和配置基础设施资源的代码，它以可重复和一致的方式承载 应用程序的部署。这种代码是用一种声明性语言编写的，当执行时，为正在部署的应用程序提供和配置基础设施。这种类型的代码就像在应用程序的微服务中发现的任何其他代码，只是它提供的是基础设施服务（例如，配置服务器）而不是事务服务（例如，在线零售应用程序的支付处理）。 策略即代码：描述了许多策略，包括安全策略，作为 可执行模块。一个例子是授权策略，它的代码包含了策略（如允许、拒绝等）和适用领域（如 RESTAPI 的方法，GET、PUT 等，路径等动词或工件）这段代码可以用特殊用途的策略语言（如 Rego）或常规应用中使用的语言（如 Go）编写。这段代码可能与 IaC 的配置代码有一些重合。然而，对于实施与特定于应用领域的关键安全服务相关的策略，需要一个单独的策略作为代码，驻留在参考平台的策略执行点（PEP） …","relpermalink":"/service-mesh-devsecops/implement/description-of-code-types-and-reference-platform-components/","summary":"对上述五类代码（即应用、应用服务、基础设施、策略和监控）的简要描述如下： 应用程序代码和应用服务代码：前者包含一组特定业务事务的数据和应用逻辑，而后者包含所有服务的代码，如网络连接、负载均衡和网络弹性。","title":"4.1 代码类型和参考平台组件的描述"},{"content":"还记得有一次告警电话半夜把我吵醒，发现是生产环境离线了。原来是系统瘫痪了，我们不得不要赔钱，这是我的错。\n从那一刻起，我就一直痴迷于构建坚如磐石的基础架构和基础架构管理系统，这样我就不会重蹈覆辙了。在我的职业生涯中，我为 Terraform、Kubernetes，一些编程语言和 Kops 做出过贡献，并创建了 Kubicorn。我不仅见证了系统基础架构的发展，而且我也帮助它完善。随着基础架构行业的发展，我们发现企业基础架构现在正以新的、令人兴奋的方式通过应用层管理。到目前为止，Kubernetes 是这种管理基础架构的新范例的最成熟的例子。\n我与人合著了这本书，部分地介绍了将基础架构作为云原生软件的新范例。此外，我希望鼓励基础架构工程师开始编写云原生应用程序。在这本书中，我们探讨了管理基础架构的丰富历史，并为云原生技术的未来定义了管理基础架构的模式。我们解释了基础架构由软件化 API 驱动的重要性。我们还探索了创建复杂系统的第一个基础架构组件的引导问题，并教授了扩展和测试基础架构的重要性。\n我于 2017 年加入 Heptio，担任资深布道师，并且很高兴能与行业中最聪明的系统工程师密切 …","relpermalink":"/cloud-native-infra/foreword/","summary":"还记得有一次告警电话半夜把我吵醒，发现是生产环境离线了。原来是系统瘫痪了，我们不得不要赔钱，这是我的错。 从那一刻起，我就一直痴迷于构建坚如磐石的基础架构和基础架构管理系统，这样我就不会重蹈覆辙了。在我","title":"前言"},{"content":"以下是关于本书的声明。\n许可 本出版物由 NIST 根据 2014 年《联邦信息安全现代化法案》（FISMA）（44 U.S.C. §3551 etseq）规定的法定职责编写，公共法律（P.L.）113-283。NIST 负责制定信息安全标准和准则，包括联邦信息系统的最低要求，但这些标准和准则在未经对国家安全系统行使策略权力的适当联邦官员明确批准的情况下，不得适用于这些系统。本准则与管理和预算办公室（OMB）A-130 号通知的要求一致。\n本出版物中的任何内容都不应被视为与商务部长根据法定授权对联邦机构的强制性和约束性标准和准则相抵触。这些准则也不应被解释为改变或取代商务部长、OMB 主任或任何其他联邦官员的现有权力。本出版物可由非政府组织在自愿的基础上使用，在美国不受版权限制。但是，请注明出处，NIST 将对此表示感谢。\n国家标准和技术研究所特别出版物 800-204C Natl.Inst. Stand.Technol.Spec.800-204C, 45 pages (March 2022) CODEN: NSPUE2\n本出版物可从以下网站免费获取。 …","relpermalink":"/service-mesh-devsecops/preface/","summary":"以下是关于本书的声明。 许可 本出版物由 NIST 根据 2014 年《联邦信息安全现代化法案》（FISMA）（44 U.S.C. §3551 etseq）规定的法定职责编写，公共法律（P.L.）113-283。NIST 负责制定信息安全标准","title":"声明"},{"content":"文件变更历史\n英文版\n日期 版本 描述 2021 年 8 月 1.0 首次发布 中文版\n日期 版本 描述 2021 年 8 月 8 日 1.0 首次发布 担保和认可的免责声明\n本文件中的信息和意见是 “按原样” 提供的，没有任何保证或担保。本文件以商品名称、商标、制造商或其他方式提及任何具体的商业产品、程序或服务，并不一定构成或暗示美国政府对其的认可、推荐或青睐，而且本指南不得用于广告或产品代言的目的。\n关于中文版\n中文版为 Jimmy Song 个人翻译，翻译过程中完全遵照原版，未做任何删减。其本人与本书的原作者没有任何组织或利益上的联系，翻译本书仅为交流学习之用。\n商标认可\nKubernetes 是 Linux 基金会的注册商标。 SELinux 是美国国家安全局的注册商标。 AppArmor 是 SUSE LLC 的注册商标。 Windows 和 Hyper-V 是微软公司的注册商标。 ETCD 是 CoreOS, Inc. 的注册商标。 Syslog-ng 是 One Identity Software International Designated Activity 公司的 …","relpermalink":"/kubernetes-hardening-guidance/notices-and-hitory/","summary":"文件变更历史 英文版 日期 版本 描述 2021 年 8 月 1.0 首次发布 中文版 日期 版本 描述 2021 年 8 月 8 日 1.0 首次发布 担保和认可的免责声明 本文件中的信息和意见是 “按原样” 提供的，没有任何保证或担保。本文件以","title":"通知和历史"},{"content":"Service Mesh 又译作“服务网格”，作为服务间通信的基础设施层。Buoyant 公司的 CEO Willian Morgan 在他的这篇文章 WHAT’S A SERVICE MESH? AND WHY DO I NEED ONE? 中解释了什么是 Service Mesh，为什么云原生应用需要 Service Mesh。\n服务网格是用于处理服务间通信的专用基础设施层。它负责通过包含现代云原生应用程序的复杂服务拓扑来可靠地传递请求。实际上，服务网格通常通过一组轻量级网络代理来实现，这些代理与应用程序代码一起部署，而不需要感知应用程序本身。—— Willian Morgan Buoyant CEO\n服务网格（Service Mesh）这个术语通常用于描述构成这些应用程序的微服务网络以及应用之间的交互。随着规模和复杂性的增长，服务网格越来越难以理解和管理。它的需求包括服务发现、负载均衡、故障恢复、指标收集和监控以及通常更加复杂的运维需求，例如 A/B 测试、金丝雀发布、限流、访问控制和端到端认证等。\n服务网格的特点 服务网格有如下几个特点： …","relpermalink":"/cloud-native-handbook/service-mesh/what-is-service-mesh/","summary":"Service Mesh 又译作“服务网格”，作为服务间通信的基础设施层。Buoyant 公司的 CEO Willian Morgan 在他的这篇文章 WHAT’S A SERVICE MESH? AND WHY DO I NEED ONE? 中解释了什么是 Service Mesh，为什么云原生应用需要 Service Mesh。 服务网格是用于处理服","title":"什么是服务网格？"},{"content":"云原生（Cloud Native）这个词汇由来已久，以致于何时出现已无据可考。云原生开始大规模出现在受众视线中，与 Pivotal 提出的云原生应用的理念有着莫大的关系。我们现在谈到云原生，更多的指的是一种文化，而不具象为哪些技术体系。\nPivotal 推出过 Pivotal Cloud Foundry 云原生应用平台和 Spring 开源 Java 开发框架，成为云原生应用架构中先驱者和探路者。Pivotal 是云原生应用平台第一股，2018 年在纽交所上市，2019 年底被 VMWare 以 27 亿美元收购，加入到 VMware 新的产品线 Tanzu。\nPivotal 最初的定义 早在 2015 年 Pivotal 公司的 Matt Stine 写了一本叫做 迁移到云原生应用架构 的小册子，其中探讨了云原生应用架构的几个主要特征：\n符合 12 因素应用 面向微服务架构 自服务敏捷架构 基于 API 的协作 抗脆弱性 笔者已于 2017 年翻译了本书，详见 迁移到云原生应用架构。\nCNCF 最初的定义 到了 2015 年 Google 主导成立了云原生计算基金会（CNCF）， …","relpermalink":"/cloud-native-handbook/intro/what-is-cloud-native/","summary":"云原生（Cloud Native）这个词汇由来已久，以致于何时出现已无据可考。云原生开始大规模出现在受众视线中，与 Pivotal 提出的云原生应用的理念有着莫大的关系。我们现在谈到云原生，更多的指的是一种文化，而不具","title":"什么是云原生？"},{"content":"CNCF，全称 Cloud Native Computing Foundation（云原生计算基金会），成立于 2015 年 7 月 21 日（于美国波特兰 OSCON 2015 上宣布），其最初的口号是坚持和整合开源技术来让编排容器作为微服务架构的一部分，其作为致力于云原生应用推广和普及的一支重要力量，不论您是云原生应用的开发者、管理者还是研究人员都有必要了解。\nCNCF 作为一个厂商中立的基金会，致力于 Github 上的快速成长的开源技术的推广，如 Kubernetes、Prometheus、Envoy 等，帮助开发人员更快更好的构建出色的产品。CNCF 维护了一个全景图项目，详见 GitHub。\n关于 CNCF 的使命与组织方式请参考CNCF 章程，概括的讲 CNCF 的使命包括以下三点：\n容器化包装。 通过中心编排系统的动态资源管理。 面向微服务。 CNCF 这个角色的作用是推广技术，形成社区，开源项目管理与推进生态系统健康发展。\n另外 CNCF 组织由以下部分组成：\n会员：白金、金牌、银牌、最终用户、学术和非赢利成员，不同级别的会员在治理委员会中的投票权不同。 理事会：负责 …","relpermalink":"/cloud-native-handbook/community/cncf/","summary":"CNCF，全称 Cloud Native Computing Foundation（云原生计算基金会），成立于 2015 年 7 月 21 日（于美国波特兰 OSCON 2015 上宣布），其最初的口号是坚持和整合开源技术来让编排容器作为微服务架构的一部分，其作为致力于云原生应用推","title":"云原生计算基金会（CNCF）"},{"content":"软件开发的模式又一次改变了。开源软件和公有云供应商已经从根本上改变了我们构建和部署软件的方式。有了开源软件，我们的应用不再需要从头开始编码。而通过使用公有云供应商，我们不再需要配置服务器或连接网络设备。所有这些都意味着，你可以在短短几天甚至几小时内从头开始构建和部署一个应用程序。\n但是，仅仅因为部署新的应用程序很容易，并不意味着操作和维护它们也变得更容易。随着应用程序变得更加复杂，更加异质，最重要的是，更加分布式，看清大局，以及准确地指出问题发生的地方，变得更加困难。\n但有些事情并没有改变：作为开发者和运维，我们仍然需要能够从用户的角度理解应用程序的性能，我们仍然需要对用户的事务有一个端到端的看法。我们也仍然需要衡量和说明在处理这些事务的过程中资源是如何被消耗的。也就是说，我们仍然需要可观测性。\n但是，尽管对可观测性的需求没有改变，我们实施可观测性解决方案的方式必须改变。本报告详细介绍了关于可观测性的传统思维方式对现代应用的不足：继续将可观测性作为一系列工具（尤其是作为 “三大支柱”）来实施，几乎不可能以可靠的方式运行现代应用。\nOpenTelemetry 通过提供一种综合的方法来收集 …","relpermalink":"/opentelemetry-obervability/foreword/","summary":"前言","title":"前言"},{"content":"代码审查的主要目的是确保逐步改善 Google 代码库的整体健康状况。代码审查的所有工具和流程都是为此而设计的。\n为了实现此目标，必须做出一系列权衡。\n首先，开发人员必须能够对任务进行改进。如果开发者从未向代码库提交过代码，那么代码库的改进也就无从谈起。此外，如果审核人员对代码吹毛求疵，那么开发人员以后也很难再做出改进。\n另外，审查者有责任确保随着时间的推移，CL 的质量不会使代码库的整体健康状况下降。这可能很棘手，因为通常情况下，代码库健康状况会随着时间的而下降，特别是在对团队有严格的时间要求时，团队往往会采取捷径来达成他们的目标。\n此外，审查者应对正在审核的代码负责并拥有所有权。审查者希望确保代码库保持一致、可维护及 Code Review 要点中所提及的所有其他内容。\n因此，我们将以下规则作为 Code Review 中期望的标准：\n一般来说，审核人员应该倾向于批准 CL，只要 CL 确实可以提高系统的整体代码健康状态，即使 CL 并不完美。\n这是所有 Code Review 指南中的高级原则。\n当然，也有一些限制。例如，如果 CL 添加了审查者认为系统中不需要的功能，那么即使代 …","relpermalink":"/eng-practices/review/reviewer/standard/","summary":"代码审查的主要目的是确保逐步改善 Google 代码库的整体健康状况。代码审查的所有工具和流程都是为此而设计的。 为了实现此目标，必须做出一系列权衡。 首先，开发人员必须能够对任务进行改进。如果开发者从未向代码库提交过","title":"Code Review 标准"},{"content":"CL 描述是进行了哪些更改以及为何更改的公开记录。CL 将作为版本控制系统中的永久记录，可能会在长时期内被除审查者之外的数百人阅读。\n开发者将来会根据描述搜索您的 CL。有人可能会仅凭有关联性的微弱印象，但没有更多具体细节的情况下，来查找你的改动。如果所有重要信息都在代码而不是描述中，那么会让他们更加难以找到你的 CL。\n首行 正在做什么的简短摘要。 完整的句子，使用祈使句。 后面跟一个空行。 CL 描述的第一行应该是关于这个 CL 是做什么的简短摘要，后面跟一个空白行。这是将来大多数的代码搜索者在浏览代码的版本控制历史时，最常被看到的内容，因此第一行应该提供足够的信息，以便他们不必阅读 CL 的整个描述就可以获得这个 CL 实际上是做了什么的信息。\n按照传统，CL 描述的第一行应该是一个完整的句子，就好像是一个命令（一个命令句）。例如，“Delete the FizzBuzz RPC and replace it with the new system.”而不是“Deleting the FizzBuzz RPC and replacing it with the new …","relpermalink":"/eng-practices/review/developer/cl-descriptions/","summary":"CL 描述是进行了哪些更改以及为何更改的公开记录。CL 将作为版本控制系统中的永久记录，可能会在长时期内被除审查者之外的数百人阅读。 开发者将来会根据描述搜索您的 CL。有人可能会仅凭有关联性的微弱印象，但没有","title":"写好 CL 描述"},{"content":"This document will describe how to configure Ingress Gateway external authorization using Open Policy Agent (OPA) as an example.\nBefore you get started, make sure you: ✓ Familiarize yourself with TSB concepts ✓ Install the TSB environment. You can use TSB demo for quick install ✓ Completed TSB usage quickstart. This document assumes you already created Tenant and are familiar with Workspace and Config Groups. Also you need to configure tctl to your TSB environment.\nIn this example, httpbin will …","relpermalink":"/tsb/howto/authorization/ingress-gateway/","summary":"This document will describe how to configure Ingress Gateway external authorization using Open Policy Agent (OPA) as an example.\nBefore you get started, make sure you: ✓ Familiarize yourself with TSB concepts ✓ Install the TSB environment. You can use TSB demo for quick install ✓ Completed TSB usage quickstart. This document assumes you already created Tenant and are familiar with Workspace and Config Groups. Also you need to configure tctl to your TSB environment.\nIn this example, httpbin will be used as the workload. Requests that come to Ingress GW will be checked by OPA. If the request is deemed unauthorized, then the request will be denied with a 403 (Forbidden) response.","title":"Configuring External Authorization in Ingress Gateways"},{"content":"This document explains how you can configure Flux CD with Helm and GitHub integration to deploy a TSB application to a target cluster.\n:::note This document assumes that:\nFlux version 2 CLI is installed. Helm CLI is installed. TSB is up and running, and GitOps has been enabled for the target cluster. ::: Cluster setup First, install Flux on the target cluster with a GitHub integration. To do that, use the following command under the target cluster kubernetes context.\n:::note You will need a …","relpermalink":"/tsb/howto/gitops/flux/","summary":"This document explains how you can configure Flux CD with Helm and GitHub integration to deploy a TSB application to a target cluster.\n:::note This document assumes that:\nFlux version 2 CLI is installed. Helm CLI is installed. TSB is up and running, and GitOps has been enabled for the target cluster. ::: Cluster setup First, install Flux on the target cluster with a GitHub integration. To do that, use the following command under the target cluster kubernetes context.\n:::note You will need a GitHub Personal Access Token (PAT) to input in the following command. :::\n$ flux bootstrap github \\ --owner=your-org \\ --repository=git-ops \\ --path=.","title":"Configuring Flux CD for GitOps"},{"content":"In this how-to you’ll learn how to migrate a portion of a VM “monolith” workload to a cluster and split traffic between your VM and cluster.\nIn this example, the server running on the VM will be treated as a “monolithic” application. The same steps can be followed if the VM was calling out to other VMs. You’d just have to make sure the VMs being called out to were resolvable and or reachable from your cluster.\nBefore you start: ✓ Install the TSB management plane ✓ Onboarded a cluster\n✓ Install …","relpermalink":"/tsb/howto/traffic/migrating-vm-monoliths/","summary":"In this how-to you’ll learn how to migrate a portion of a VM “monolith” workload to a cluster and split traffic between your VM and cluster.\nIn this example, the server running on the VM will be treated as a “monolithic” application. The same steps can be followed if the VM was calling out to other VMs. You’d just have to make sure the VMs being called out to were resolvable and or reachable from your cluster.\nBefore you start: ✓ Install the TSB management plane ✓ Onboarded a cluster\n✓ Install the data plane operator ✓ Provision a VM to run in a TSB workload (this guide assumes Ubuntu 20.","title":"Migrating VM Monoliths to your cluster"},{"content":"When traffic is forwarded by a gateway, it typically assumes the identity of that gateway. This default behavior simplifies access control configuration for external traffic. However, in a multi-cluster environment, more granular access control is often needed. Tetrate Service Bridge (TSB) offers the capability to preserve the original identity of a request through gateway hops, allowing for cross-cluster authentication and fine-tuned access control.\nThis documentation explains how to enable and …","relpermalink":"/tsb/howto/gateway/service-identity-propagation/","summary":"When traffic is forwarded by a gateway, it typically assumes the identity of that gateway. This default behavior simplifies access control configuration for external traffic. However, in a multi-cluster environment, more granular access control is often needed. Tetrate Service Bridge (TSB) offers the capability to preserve the original identity of a request through gateway hops, allowing for cross-cluster authentication and fine-tuned access control.\nThis documentation explains how to enable and utilize identity propagation in TSB, enabling scenarios such as propagating consumer identities to remote services, implementing detailed access control between different clusters, and applying access control rules to failover targets.","title":"Multicluster Access Control and Identity Propagation"},{"content":"In this document, we will enable a rate limit in the Ingress Gateway and show how to rate limit based on the HTTP request user-agent string.\nBefore you get started, make sure you: ✓ Familiarize yourself with TSB concepts ✓ Install the TSB environment. You can use TSB demo for quick install\n✓ Completed TSB usage quickstart. This document assumes you already created Tenant and are familiar with Workspace and Config Groups. Also you need to configure tctl to your TSB environment.\nEnable Rate …","relpermalink":"/tsb/howto/rate-limiting/ingress-gateway/","summary":"In this document, we will enable a rate limit in the Ingress Gateway and show how to rate limit based on the HTTP request user-agent string.\nBefore you get started, make sure you: ✓ Familiarize yourself with TSB concepts ✓ Install the TSB environment. You can use TSB demo for quick install\n✓ Completed TSB usage quickstart. This document assumes you already created Tenant and are familiar with Workspace and Config Groups. Also you need to configure tctl to your TSB environment.\nEnable Rate Limiting Server Read and follow the instructions on Enabling the Rate Limiting Server document.\n:::note Demo Installation If you are using the TSB demo installation, you already have rate limit service running and ready to use, and can skip this section.","title":"Rate Limiting in Ingress Gateway"},{"content":"This document will describe how the WASM extensions are defined in TSB and how are they assigned to the components on the hierarchy.\nWASM in TSB In order to control the extensions allowed in the mesh, avoid security leakages and ease the process of extension upgrades, TSB has a WASM extension catalog, where an administrator will register all the extensions that will be available to be used in the different components. This catalog will contain the description, image and execution properties for …","relpermalink":"/tsb/howto/wasm/wasm-extension/","summary":"This document will describe how the WASM extensions are defined in TSB and how are they assigned to the components on the hierarchy.\nWASM in TSB In order to control the extensions allowed in the mesh, avoid security leakages and ease the process of extension upgrades, TSB has a WASM extension catalog, where an administrator will register all the extensions that will be available to be used in the different components. This catalog will contain the description, image and execution properties for each extension. When a new version of the extension is available, changing the content of the WASM extension catalog record will propagate the update to all the assignments for that extension.","title":"TSB configuration"},{"content":"本节重点介绍组成 TSB 的架构及其含义。你将获得以下方面的知识：\n我们的可靠部署理念是 TSB 架构背后的驱动力 数据平面，由 Envoy 提供支持 由 Istio 提供支持的本地控制平面 Tetrate Service Bridge 的全局控制平面 Tetrate Service Bridge 的管理平面 拥有管理平面的重要性 Tetrate Service Bridge 中的 Envoy 扩展 读完本节后，你应该清楚地了解 TSB 架构的每个元素以及它们如何协同工作以帮助你管理环境。\n部署理念 TSB 的架构基于以故障域为中心的强大部署理念。此方法涉及识别和隔离关键系统发生故障时受影响的基础设施部分。这些故障域分为三类：\n物理故障：包括主机故障、机架故障、硬盘驱动器故障和资源短缺。 逻辑故障：逻辑故障包括错误配置、安全漏洞以及与依赖项和数据相关的问题。 数据故障：这些涉及数据库问题、错误更新、复制失败和备份问题。 为了创建可靠的系统，这些故障域被分组到孤岛中并作为独立实例进行复制。所得系统的可靠性取决于最小化副本之间的相互依赖性。\n物理故障域 在现代云环境中，需要考虑的一组物理故 …","relpermalink":"/tsb/concepts/architecture/","summary":"本节重点介绍组成 TSB 的架构及其含义。你将获得以下方面的知识： 我们的可靠部署理念是 TSB 架构背后的驱动力 数据平面，由 Envoy 提供支持 由 Istio 提供支持的本地控制平面 Tetrate Service Bridge 的全局控制平面 Tetrate Service Bridge 的管理平面 拥有管理平面的重要性","title":"TSB 架构"},{"content":"本文档提供有关 TSB 的发布节奏、不同版本的支持期以及组件版本矩阵的信息。\nTSB 发布说明 TSB 遵循基于 semver.org 的语义版本控制模型。每个版本号代表的含义如下：\nMAJOR ：因不兼容的 API 更改而增加。 MINOR ：针对新功能增加。 PATCH ：针对错误和安全修复而增加。 TSB 发布节奏和支持 TSB 定期且至少每季度发布 PATCH 更新 (1.x.y)。 每个新的 MINOR 版本都属于长期支持 (LTS) 政策。 Tetrate 为 LTS 版本提供从一般可用性 (GA) 到一般支持终止 (EoGS) 的支持，通常在 GA 后 12 个月设置。 在支持窗口期间通过 PATCH 版本提供错误和安全修复。 新功能不会向后移植到 LTS 版本。 TSB 候选版本 候选版本提供对新功能的早期访问以进行测试，但不建议用于生产使用。\n发布候选版本\n候选版本可能包含已知或未知的错误，并且不适合生产使用。 支持的版本 Tetrate 根据以下时间表提供支持和补丁：\nTSB 版本 一般可用性 一般支持结束 Kubernetes 版本 OpenShift …","relpermalink":"/tsb/release-notes-announcements/support-policy/","summary":"本文档提供有关 TSB 的发布节奏、不同版本的支持期以及组件版本矩阵的信息。 TSB 发布说明 TSB 遵循基于 semver.org 的语义版本控制模型。每个版本号代表的含义如下： MAJOR ：因不兼容的 API 更改而增加。 MINOR ：针对新功能增加。 PATCH ：针对错误和安","title":"TSB 支持策略"},{"content":"在本部分中，你将在演示 TSB 环境中部署示例应用程序 (bookinfo)。将使用 TSB UI 和 tctl 命令验证部署。\n先决条件 在继续阅读本指南之前，请确保你已完成以下步骤：\n熟悉 TSB 概念，包括工作区和组 安装 TSB 演示 TSB 演示安装负责加入集群、安装所需的操作员并为你提供必要的访问凭据。\n部署 Bookinfo 应用程序 你将使用经典的 Istio bookinfo 应用程序来测试 TSB 的功能。\n创建命名空间并部署应用程序 # Create namespace and label it for Istio injection kubectl create namespace bookinfo kubectl label namespace bookinfo istio-injection=enabled # Deploy the bookinfo application kubectl apply -n bookinfo -f …","relpermalink":"/tsb/quickstart/deploy-sample-app/","summary":"在本部分中，你将在演示 TSB 环境中部署示例应用程序 (bookinfo)。将使用 TSB UI 和 tctl 命令验证部署。 先决条件 在继续阅读本指南之前，请确保你已完成以下步骤： 熟悉 TSB 概念，包括工作区和组 安装 TSB 演示 TSB 演示安装负责","title":"部署应用程序"},{"content":"下表显示了现有的 TSB 功能及其当前阶段。该表将在每个主要或次要版本中更新。\n特征阶段定义 下表定义了 TSB 功能的成熟阶段。\n阶段 Alpha（技术预览） Beta Stable 特征 可能不包含最终版本计划的所有功能。 功能完整，但可能包含许多已知或未知的错误。 功能完整，没有已知错误。 生产用途 不应该在生产中使用。 可用于生产。 可靠，生产经过强化。 API 不保证向后兼容性。 API 是有版本的。 可靠，值得生产。 API 具有版本控制功能，并具有自动版本转换功能以实现向后兼容性。 性能 未量化或保证。 未量化或保证。 性能（延迟/规模）被量化、记录，并保证不会出现回归。 文档 缺乏文档。 Documented. 记录用例。 环境 在单一环境（仅限 EKS 或 GKE）上进行测试。 至少在两个环境上进行了测试。 （EKS、GKE、OpenShift） 在多种环境下经过良好测试。 （AKS、EKS、GKE、MKE、OpenShift） 监控 并非所有重要指标都可用。 大多数重要指标都可用。 所有重要指标均可用。 功能状态表 领域 描述 状态 API tctl UI …","relpermalink":"/tsb/release-notes-announcements/feature-status/","summary":"下表显示了现有的 TSB 功能及其当前阶段。该表将在每个主要或次要版本中更新。 特征阶段定义 下表定义了 TSB 功能的成熟阶段。 阶段 Alpha（技术预览） Beta Stable 特征 可能不包含最终版本计划的所有功能。 功能完整，但可能包含许","title":"功能状态"},{"content":"此Chart安装 TSB 管理平面Operator，还允许你使用 TSB ManagementPlane CR 安装 TSB 管理平面组件以及使其完全运行所需的所有秘密。\n在开始之前，请确保你已经查看了 Helm 安装过程。\n安装概述 创建一个 values.yaml 文件并使用所需的配置进行编辑。你可以在下面的 配置 部分中找到有关可用 Helm 配置的更多详细信息。有关 spec 部分的完整参考，请参阅 TSB ManagementPlane CR。\n使用 helm install 命令安装 TSB 管理平面。确保将 image.registry 和 version 设置为正确的注册表位置和 TSB 版本。\n等待所有 TSB 管理平面组件成功部署。你可以尝试登录 TSB UI 或使用 tctl 来验证你的安装。\n安装 要安装 TSB 管理平面，请创建一个 values.yaml 文件，包含以下内容，并根据你的需求进行编辑。\nspec: # 设置组织名称。组织名称必须小写以符合 RFC 标准。 organization: \u0026lt;organization-name\u0026gt; dataStore: …","relpermalink":"/tsb/setup/helm/managementplane/","summary":"此Chart安装 TSB 管理平面Operator，还允许你使用 TSB ManagementPlane CR 安装 TSB 管理平面组件以及使其完全运行所需的所有秘密。 在开始之前，请确保你已经查看了 Helm 安装过程。 安装概述 创建一个 values.yaml 文件并使用所需的配置进行编辑","title":"管理平面安装"},{"content":"本页面将向你展示如何在生产环境中安装Tetrate Service Bridge管理平面。\n在开始之前，请确保你已经：\n检查了要求\n检查了TSB管理平面组件\n检查了证书类型和内部证书要求\n检查了防火墙信息\n如果你正在升级以前的版本，请还要检查PostgreSQL备份和还原\n下载了Tetrate Service Bridge CLI（tctl）\n同步了Tetrate Service Bridge镜像\n管理平面Operator 为了保持安装简单，但仍允许许多自定义配置选项，我们创建了一个管理平面Operator。该Operator将在集群中运行，并根据ManagementPlane自定义资源中描述的内容引导管理平面的启动。它会监视更改并执行它们。为了帮助创建正确的自定义资源文档（CRD），我们已经添加了能力到我们的tctl客户端，用于创建基本清单，然后你可以根据你的要求进行修改。之后，你可以将清单直接应用于适当的集群，或在你的源控制操作的集群中使用。\n关于 Operator\n如果你想了解有关Operator的内部工作原理以及Operator模式的更多信息，请查看Kubernetes文档。  …","relpermalink":"/tsb/setup/self-managed/management-plane-installation/","summary":"本页面将向你展示如何在生产环境中安装Tetrate Service Bridge管理平面。 在开始之前，请确保你已经： 检查了要求 检查了TSB管理平面组件 检查了证书类型和内部证书要求 检查了防火墙信息 如果你正在升级以前的版","title":"管理平面安装"},{"content":"本页面深入介绍了 TSB Operator 如何管理控制平面组件的生命周期，并概述了你可以通过 TSB Operator 配置和管理的自定义资源。\nTSB Operator 配置为监督控制平面组件的生命周期，主动监控部署的同一命名空间内的 ControlPlane 自定义资源 (CR)。默认情况下，控制平面位于 istio-system 命名空间中。有关自定义资源 API 的详细信息，你可以参考控制平面安装 API 参考文档。\n控制平面 Operator 组件 以下是你可以使用控制平面 Operator 配置和管理的各种类型的自定义组件：\n组件 Service Deployment istio Istio-operator-metrics (istiod, vmgateway) Istio-operator (istiod vmgateway) (istio-cni-node daemonset in kube-system namespace) oap oap oap-deployment collector otel-collector otel-collector …","relpermalink":"/tsb/concepts/operators/control-plane/","summary":"本页面深入介绍了 TSB Operator 如何管理控制平面组件的生命周期，并概述了你可以通过 TSB Operator 配置和管理的自定义资源。 TSB Operator 配置为监督控制平面组件的生命周期，主动监控部署的同一命名空间内的 ControlPlane 自定义资源 (CR)。默认情况下，","title":"控制平面"},{"content":"在继续之前，请确保你了解 TSB 中的 4 种证书类型，特别是内部证书。\n注意\n请注意，此处描述的证书仅用于 TSB 组件之间的通信，因此不属于通常由 Istio 或应用程序 TLS 证书管理的工作负载证书。 提醒\n如果你在管理平面集群中安装了 cert-manager，你可以使用 tctl 自动在管理平面中安装所需的发行者和证书，并创建控制平面证书。有关更多详细信息，请参阅 管理平面安装 和 载入集群 文档。 要使用常规（非相互）TLS 进行 JWT 身份验证，XCP central 证书必须在其主体备用名称（SANs）中包含其地址。这将是 DNS 名称或 IP 地址。\n与上述 mTLS 类似，管理平面中的 XCP central 使用存储在名为 xcp-central-cert 的管理平面命名空间（默认为 tsb）中的密钥中的证书。密钥必须包含标准的 tls.crt、tls.key 和 ca.crt 字段的数据。\n以下是如果你使用 IP 地址作为 XCP central 证书的 cert-manager 资源示例。\napiVersion: cert-manager.io/v1 …","relpermalink":"/tsb/setup/certificate/certificate-requirements/","summary":"在继续之前，请确保你了解 TSB 中的 4 种证书类型，特别是内部证书。 注意 请注意，此处描述的证书仅用于 TSB 组件之间的通信，因此不属于通常由 Istio 或应用程序 TLS 证书管理的工作负载证书。 提醒 如果你在管理平面集群中安装了 ce","title":"内部证书要求"},{"content":"在本操作指南中，你将了解如何通过基于 URI 端点、标头和端口匹配流量并将其路由到目标服务的主机:端口来设置基于子集的流量路由。\n先决条件 在继续之前，请确保你已完成以下任务：\n熟悉 TSB 概念。 安装 TSB 演示环境。 创建一个租户。 创建工作区和配置组 首先，使用以下 YAML 配置创建工作区和配置组：\nhelloworld-ws-groups.yaml apiversion: api.tsb.tetrate.io/v2 kind: Workspace metadata: organization: tetrate tenant: tetrate name: helloworld-ws spec: namespaceSelector: names: - \u0026#39;*/helloworld\u0026#39; --- apiVersion: gateway.tsb.tetrate.io/v2 kind: Group metadata: organization: tetrate tenant: tetrate workspace: helloworld-ws name: helloworld-gw …","relpermalink":"/tsb/howto/gateway/subset-based-routing-using-igw-and-service-route/","summary":"在本操作指南中，你将了解如何通过基于 URI 端点、标头和端口匹配流量并将其路由到目标服务的主机:端口来设置基于子集的流量路由。 先决条件 在继续之前，请确保你已完成以下任务： 熟悉 TSB 概念。 安装 TSB 演示环境。 创建一个","title":"使用 IngressGateway 和 ServiceRoute 基于子集的流量路由"},{"content":"在继续之前，请确保你熟悉Istio 隔离边界功能。\n注意\n可以在单个隔离边界内执行已修订到已修订的控制平面升级。 升级之前 一旦启用了 Istio 隔离边界功能，边界可以用于保持服务发现隔离，并在隔离边界内升级 Istio 控制平面。对于包含单个隔离边界的ControlPlane CR 或 Helm 值：\nspec: ... components: xcp: isolationBoundaries: - name: global revisions: - name: stable istio: tsbVersion: 1.6.0 你将升级stable修订版本中的所有工作负载以使用tsbVersion: 1.6.1。\n控制平面升级策略\nTSB 支持修订到修订升级的原地和金丝雀控制平面升级。 控制平面原地升级 对于原地升级，你可以直接更新tsbVersion字段 - 保留修订name不变。\nspec: ... components: xcp: isolationBoundaries: - name: global revisions: - name: stable istio: …","relpermalink":"/tsb/setup/upgrades/revisioned-to-revisioned/","summary":"在继续之前，请确保你熟悉Istio 隔离边界功能。 注意 可以在单个隔离边界内执行已修订到已修订的控制平面升级。 升级之前 一旦启用了 Istio 隔离边界功能，边界可以用于保持服务发现隔离，并在隔离边界内升级 Istio 控制平面。","title":"已修订版本间的升级"},{"content":"在有效使用 Argo CD 之前，有必要了解该平台构建的底层技术。还有必要了解向你提供的功能以及如何使用它们。以下部分提供了一些有用的链接来建立这种理解。\n学习基础知识 浏览在线 Docker 和 Kubernetes 教程： 适合初学者的容器、虚拟机和 Docker 简介 Kubernetes 简介 教程 根据你计划如何模板化你的应用程序： Kustomize Helm 如果你要与 CI 工具集成： GitHub Actions 文档 Jenkins 用户指南 ","relpermalink":"/argo-cd/understand-the-basics/","summary":"在有效使用 Argo CD 之前，有必要了解该平台构建的底层技术。还有必要了解向你提供的功能以及如何使用它们。以下部分提供了一些有用的链接来建立这种理解。 学习基础知识 浏览在线 Docker 和 Kubernetes 教程： 适合初学者的容器、虚拟机和 Docker","title":"理解基础"},{"content":"本指南假定你已经熟悉 Argo CD 及其基本概念。有关更多信息，请参阅 Argo CD 文档。\n要求 安装 kubectl 命令行工具 有一个 kubeconfig 文件（默认位置为 ~/.kube/config）。 安装 有几种安装 ApplicationSet 控制器的选项。\nA) 将 ApplicationSet 作为 Argo CD 的一部分安装 从 Argo CD v2.3 开始，ApplicationSet 控制器已捆绑在 Argo CD 中。无需从 Argo CD 单独安装 ApplicationSet 控制器。\n有关更多信息，请参阅 Argo CD 入门指南。\nB) 将 ApplicationSet 安装到现有的 Argo CD 安装中（Argeo CD v2.3 之前） 注意: 以下说明仅适用于 Argo CD 版本 v2.3.0 之前。\nApplicationSet 控制器 必须 安装到与其所针对的 Argo CD 相同的命名空间中。\n假设 Argo CD 安装在 argocd 命名空间中，请运行以下命令：\nkubectl apply -n argocd -f …","relpermalink":"/argo-cd/operator-manual/applicationset/getting-started/","summary":"本指南假定你已经熟悉 Argo CD 及其基本概念。有关更多信息，请参阅 Argo CD 文档。 要求 安装 kubectl 命令行工具 有一个 kubeconfig 文件（默认位置为 ~/.kube/config）。 安装 有几种安装 ApplicationSet 控制器的选项。 A) 将 ApplicationSet 作为 Argo CD 的一部分安装","title":"ApplicationSet 入门"},{"content":"本教程将指导你如何配置 Argo Rollouts 与 Ambassador 配合以实现金丝雀发布。本指南中使用的所有文件都可在此存储库的 examples 目录中找到。\n要求 Kubernetes 集群 在集群中安装 Argo-Rollouts 注意\n如果使用 Ambassador Edge Stack 或 Emissary-ingress 2.0+，则需要安装 Argo-Rollouts 版本 v1.1+，并需要向 argo-rollouts 部署提供 --ambassador-api-version getambassador.io/v3alpha1。\n1. 安装和配置 Ambassador Edge Stack 如果你的集群中没有 Ambassador，可以按照 Edge Stack 文档 进行安装。\n默认情况下，Edge Stack 通过 Kubernetes 服务路由。为了获得更好的金丝雀性能，我们建议你使用端点路由。通过将以下配置保存在名为 resolver.yaml 的文件中，启用集群上的端点路由：\napiVersion: getambassador.io/v2 …","relpermalink":"/argo-rollouts/getting-started/ambassador/","summary":"本教程将指导你如何配置 Argo Rollouts 与 Ambassador 配合以实现金丝雀发布。本指南中使用的所有文件都可在此存储库的 examples 目录中找到。 要求 Kubernetes 集群 在集群中安装 Argo-Rollouts 注意 如果使用 Ambassador Edge Stack 或 Emissary-ingress 2.0+，则需要安装 Argo-Rollouts 版本 v1.1+，并需要向 argo-rollouts 部","title":"Argo Rollouts 和 Ambassador 快速开始"},{"content":"Ambassador Edge Stack 提供了你在 Kubernetes 集群边缘所需的功能（因此称为“边缘堆栈”）。这包括 API 网关、入口控制器、负载均衡器、开发人员门户、金丝雀流量路由等。它提供了一组 CRD，用户可以配置以启用不同的功能。\nArgo-Rollouts 提供了一个集成，利用了 Ambassador 的 金丝雀路由功能。这允许你的应用程序的流量在部署新版本时逐步增加。\n工作原理 Ambassador Edge Stack 提供了一个名为“Mapping”的资源，用于配置如何将流量路由到服务。通过创建具有相同 URL 前缀并指向不同服务的 2 个映射，可以实现 Ambassador 金丝雀部署。考虑以下示例：\napiVersion: getambassador.io/v2 kind: Mapping metadata: name: stable-mapping spec: prefix: /someapp rewrite: / service: someapp-stable:80 --- apiVersion: getambassador.io/v2 kind: …","relpermalink":"/argo-rollouts/traffic-management/ambassador/","summary":"Ambassador Edge Stack 提供了你在 Kubernetes 集群边缘所需的功能（因此称为“边缘堆栈”）。这包括 API 网关、入口控制器、负载均衡器、开发人员门户、金丝雀流量路由等。它提供了一组 CRD，用户可以配置以启用不同的功能。 Argo-Rollouts 提供了一个集成，","title":"Ambassador Edge Stack"},{"content":"以下描述了 Rollout 的所有可用字段：\napiVersion: argoproj.io/v1alpha1 kind: Rollout metadata: name: example-rollout-canary spec: # 期望的 Pod 数量。 # 默认为 1。 replicas: 5 analysis: # 限制存储历史上成功的分析运行和实验的数量 # 默认为 5。 successfulRunHistoryLimit: 10 # 限制存储历史上不成功的分析运行和实验的数量。 # 不成功的阶段有：\u0026#34;Error\u0026#34;，\u0026#34;Failed\u0026#34;，\u0026#34;Inconclusive\u0026#34; # 默认为 5。 unsuccessfulRunHistoryLimit: 10 # Pod 的标签选择器。被选择的 Pod 的现有副本集将受到此 Rollout 的影响。它必须与 Pod 模板的标签匹配。 selector: matchLabels: app: guestbook # WorkloadRef 包含对提供 Pod 模板（例如 Deployment）的工作负载的引用。如果使用，则不使用 Rollout 模 …","relpermalink":"/argo-rollouts/rollout/specification/","summary":"以下描述了 Rollout 的所有可用字段： apiVersion: argoproj.io/v1alpha1 kind: Rollout metadata: name: example-rollout-canary spec: # 期望的 Pod 数量。 # 默认为 1。 replicas: 5 analysis: # 限制存储历史上成功的分析运行和实验的数量 # 默认为 5。 successfulRunHistoryLimit: 10 # 限制存储历史上不成功的分析运行和实验的数量。 # 不成功的阶段有：","title":"Rollout 规范"},{"content":"控制器安装 两种安装方式：\ninstall.yaml - 标准安装方法。 kubectl create namespace argo-rollouts kubectl apply -n argo-rollouts -f https://github.com/argoproj/argo-rollouts/releases/latest/download/install.yaml 这将创建一个新的命名空间 argo-rollouts，在其中运行 Argo Rollouts 控制器。\n🔔 提示：如果你使用的是其他命名空间名称，请更新 install.yaml 集群角色绑定的服务账户命名空间名称。\n🔔 提示：在 Kubernetes v1.14 或更低版本上安装 Argo Rollouts 时，CRD 清单必须使用 --validate = false 选项进行 kubectl apply。这是由于在 v1.15 中引入的新 CRD 字段的使用，在较低的 API 服务器中默认被拒绝。\n🔔 提示：在 GKE 上，你需要授予你的账户创建新集群角色的权限：\nkubectl create …","relpermalink":"/argo-rollouts/installation/","summary":"控制器安装 两种安装方式： install.yaml - 标准安装方法。 kubectl create namespace argo-rollouts kubectl apply -n argo-rollouts -f https://github.com/argoproj/argo-rollouts/releases/latest/download/install.yaml 这将创建一个新的命名空间 argo-rollouts，在其中运行 Argo Rollouts 控制器。 🔔 提示：如果你使用的是其他命名空间名称，请更新 install.yaml 集群角色绑定的服","title":"安装 Argo Rollouts"},{"content":"🔔 重要提醒：从 v1.5 开始可用 - 状态：Alpha\nArgo Rollouts 通过第三方插件系统支持获取分析指标。这使得用户可以扩展 Rollouts 的功能，以支持不受本地支持的度量提供者。Rollouts 使用一个名为 go-plugin 的插件库来实现。你可以在此处找到示例插件：rollouts-plugin-metric-sample-prometheus\n使用指标插件 安装和使用 argo rollouts 插件有两种方法。第一种方法是将插件可执行文件挂载到 rollouts 控制器容器中。第二种方法是使用 HTTP(S) 服务器托管插件可执行文件。\n将插件可执行文件挂载到 rollouts 控制器容器中 有几种方法可以将插件可执行文件挂载到 rollouts 控制器容器中。其中一些方法将取决于你的特定基础架构。这里有几种方法：\n使用 init 容器下载插件可执行文件 使用 Kubernetes 卷挂载共享卷，例如 NFS、EBS 等。 将插件构建到 rollouts 控制器容器中 然后，你可以使用 configmap 将插件可执行文件位置指向插件。示例： …","relpermalink":"/argo-rollouts/analysis/plugins/","summary":"🔔 重要提醒：从 v1.5 开始可用 - 状态：Alpha Argo Rollouts 通过第三方插件系统支持获取分析指标。这使得用户可以扩展 Rollouts 的功能，以支持不受本地支持的度量提供者。Rollouts 使用一个名为 go-plugin 的插件库来实现。你可以在此处","title":"指标插件"},{"content":"Argo Rollouts 支持以下通知服务：\nAlertmanager Email GitHub Google Chat Grafana Mattermost NewRelic Opsgenie Overview Pagerduty Pushover Rocket.Chat Slack Teams Telegram Webex Teams Webhook ","relpermalink":"/argo-rollouts/notifications/services/","summary":"Argo Rollouts 支持以下通知服务： Alertmanager Email GitHub Google Chat Grafana Mattermost NewRelic Opsgenie Overview Pagerduty Pushover Rocket.Chat Slack Teams Telegram Webex Teams Webhook","title":"通知服务列表"},{"content":"金丝雀发布是一种部署策略，操作员将新版本的应用程序释放到生产流量的一小部分中。\n概述 由于没有关于金丝雀部署的共识标准，因此 Rollouts Controller 允许用户概述他们想要运行其金丝雀部署的方式。用户可以定义一个控制器使用的步骤列表，以在.spec.template发生更改时操作 ReplicaSets。在新的 ReplicaSet 被提升为稳定版本并且旧版本被完全缩减之前，每个步骤将被评估。\n每个步骤可以有两个字段。setWeight字段指定应发送到金丝雀的流量百分比，pause结构指示 Rollout 暂停。当控制器到达 Rollout 的pause步骤时，它将向.status.PauseConditions字段添加一个PauseCondition结构。如果pause结构中的duration字段设置，Rollout 将在等待duration字段的值之前不会进入下一个步骤。否则，Rollout 将无限期等待该 Pause 条件被删除。通过使用setWeight和pause字段，用户可以描述他们想要如何进入新版本。以下是金丝雀策略的示例。\n🔔 重要： …","relpermalink":"/argo-rollouts/rollout/deployment-strategies/canary/","summary":"金丝雀发布是一种部署策略，操作员将新版本的应用程序释放到生产流量的一小部分中。 概述 由于没有关于金丝雀部署的共识标准，因此 Rollouts Controller 允许用户概述他们想要运行其金丝雀部署的方式。用户可以定义一个控制器使用的步骤","title":"金丝雀部署"},{"content":" rollouts rollouts abort rollouts completion rollouts create rollouts create analysisrun rollouts dashboard rollouts get rollouts get experiment rollouts get rollout rollouts lint rollouts list rollouts list experiments rollouts list rollouts rollouts notifications rollouts notifications template rollouts notifications template get rollouts notifications template notify rollouts notifications trigger rollouts notifications trigger get rollouts notifications trigger run rollouts pause rollouts …","relpermalink":"/argo-rollouts/kubectl-plugin/command/","summary":"rollouts rollouts abort rollouts completion rollouts create rollouts create analysisrun rollouts dashboard rollouts get rollouts get experiment rollouts get rollout rollouts lint rollouts list rollouts list experiments rollouts list rollouts rollouts notifications rollouts notifications template rollouts notifications template get rollouts notifications template notify rollouts notifications trigger rollouts notifications trigger get rollouts notifications trigger run rollouts pause rollouts promote rollouts restart rollouts retry rollouts retry experiment rollouts retry rollout rollouts set rollouts set image rollouts status rollouts terminate rollouts terminate analysisrun rollouts terminate experiment rollouts undo rollouts version","title":"kubectl argo rollouts 命令用法"},{"content":"本章介绍了 SPIFFE 的动机和它是如何诞生的。\n压倒性的动机和需要 我们到达今天的位置，首先要经历一些成长的痛苦。\n当互联网在 1981 年首次广泛使用时，它只有 213 个不同的服务器，而安全问题几乎没有被考虑到。随着互联计算机数量的增加，安全问题仍然是一个弱点：容易被利用的漏洞导致了大规模的攻击，如莫里斯蠕虫病毒，它在 1988 年占领了互联网上的大多数 Unix 服务器，或 Slammer 蠕虫病毒，它在 2003 年在数十万台 Windows 服务器上传播。\n随着时间的推移，过去的传统周边防御模式已经不能很好地适应不断发展的计算架构和现代技术的边界。各种解决方案和技术层出不穷，以掩盖基础网络安全概念未能跟上现代化趋势而出现的越来越大的裂缝。\n那么，为什么周边模式如此普遍，怎样才能解决这些缺陷？\n多年来，我们观察到三大的趋势，突出了传统的周边模式对网络未来的限制：\n软件不再在组织控制的单个服务器上运行。自 2015 年以来，新的软件通常被构建为微服务的集合，可以单独扩展或转移到云主机供应商。如果你不能在需要安全的服务周围画出一条精确的线，就不可能在它们周围筑起一道墙。 你不能 …","relpermalink":"/spiffe/history-and-motivation-for-spiffe/","summary":"本章介绍了 SPIFFE 的动机和它是如何诞生的。 压倒性的动机和需要 我们到达今天的位置，首先要经历一些成长的痛苦。 当互联网在 1981 年首次广泛使用时，它只有 213 个不同的服务器，而安全问题几乎没有被考虑到。随着互联计算机数量","title":"SPIFFE 的历史和动机"},{"content":"标签 标签（Label）是一种通用的、灵活的和高度可扩展的方式，可以用来处理大量资源，因为我们可以用它来对事物任意分组和创建集合。每当需要描述、解决或选择某物时，它都是基于标签完成的：\n端点 被分配了从容器运行时、编排系统或其他来源派生的标签。 网络策略 根据标签选择允许通信的 端点对，策略本身也由标签标识。 什么是标签？ 标签是一对由 key 和 value 组成的字符串。可以将标签格式化为具有 key=value 的单个字符串。key 部分是强制性的，并且必须是唯一的。这通常是通过使用反向域名概念来实现的，例如 io.cilium.mykey=myvalue。value 部分是可选的，可以省略，例如 io.cilium.mykey.\n键名通常应由字符集组成 [a-z0-9-.]。\n当使用标签选择资源时，键和值都必须匹配，例如，当一个策略应该应用于所有带有标签 my.corp.foo 的端点时，标签 my.corp.foo=bar 不会与该选择器匹配。\n标签来源 标签可以来自各种来源。例如，端点 将通过本地容器运行时派生与容器关联的标签，以及与 Kubernetes 提供的 pod  …","relpermalink":"/cilium-handbook/concepts/terminology/","summary":"标签 标签（Label）是一种通用的、灵活的和高度可扩展的方式，可以用来处理大量资源，因为我们可以用它来对事物任意分组和创建集合。每当需要描述、解决或选择某物时，它都是基于标签完成的： 端点 被分配了从容器","title":"Cilium 术语说明"},{"content":"IP 地址管理（IPAM）负责分配和管理由 Cilium 管理的网络端点（容器和其他）使用的 IP 地址。Cilium 支持以下各种 IPAM 模式，以满足不同用户的需求。\n集群范围（默认） Kubernetes 主机范围 Azure IPAM AWS ENI Google Kubernetes Engine CRD 支持 集群范围 集群范围 IPAM 模式将 PodCIDR 分配给每个节点，并使用每个节点上的主机范围分配器分配 IP。因此它类似于 Kubernetes 主机范围模式。区别在于 Kubernetes 不是通过 Kubernetes v1.Node资源分配每个节点的 PodCIDR，而是 Cilium Operator 通过 v2.CiliumNode 资源管理每个节点的 PodCIDR。这种模式的优点是它不依赖于 Kubernetes 被配置为分发每个节点的 PodCIDR。\n架构 架构图 如果无法将 Kubernetes 配置为分发 PodCIDR 或需要更多控制，这将非常有用。\n在这种模式下，Cilium 代理将在启动时等待，直到 PodCIDRs …","relpermalink":"/cilium-handbook/networking/ipam/","summary":"IP 地址管理（IPAM）负责分配和管理由 Cilium 管理的网络端点（容器和其他）使用的 IP 地址。Cilium 支持以下各种 IPAM 模式，以满足不同用户的需求。 集群范围（默认） Kubernetes 主机范围 Azure IPAM AWS ENI Google Kubernetes Engine CRD 支持 集群范围 集群范围 IPAM","title":"IP 地址管理（IPAM）"},{"content":"部署 标准 Cilium Kubernetes 部署的配置包括几个 Kubernetes 资源：\nDaemonSet 资源：描述部署到每个 Kubernetes 节点的 Cilium pod。这个 pod 运行 cilium-agent 和相关的守护进程。这个 DaemonSet 的配置包括指示 Cilium docker 容器的确切版本（例如 v1.0.0）的镜像标签和传递给 cilium-agent 的命令行选项。 资源：描述传递给 cilium-agent 的ConfigMap 常用配置值，例如 kvstore 端点和凭据、启用/禁用调试模式等。 ServiceAccount、ClusterRole 和 ClusterRoleBindings 资源：当启用 Kubernetes RBAC 时，cilium-agent` 用于访问 Kubernetes API 服务器的身份和权限。 资源：如果 Secret 需要，描述用于访问 etcd kvstore 的凭据。 现有 Pod 的联网 如果在部署 Cilium 之前 pod 已经在运行 DaemonSet，这些 pod …","relpermalink":"/cilium-handbook/kubernetes/concepts/","summary":"部署 标准 Cilium Kubernetes 部署的配置包括几个 Kubernetes 资源： DaemonSet 资源：描述部署到每个 Kubernetes 节点的 Cilium pod。这个 pod 运行 cilium-agent 和相关的守护进程。这个 DaemonSet 的配置包括指示 Cilium docker 容器的确切版本（例如 v1.0.0）的镜像标签和传递给 cilium-agent 的命令行选项。","title":"概念"},{"content":"Kubernetes 等容器管理系统部署了一个网络模型，该模型为每个 pod（容器组）分配一个单独的 IP 地址。这确保了架构的简单性，避免了不必要的网络地址转换（NAT），并为每个单独的容器提供了全范围的端口号以供使用。这种模型的逻辑结果是，根据集群的大小和 pod 的总数，网络层必须管理大量的 IP 地址。\n在传统上，安全策略基于 IP 地址过滤器。下面是一个简单的例子。如果所有具有标签 role=frontend 的 pod 应该被允许发起与所有具有标签 role=backend 的 pod 的连接，那么每个运行至少一个具有标签 role=backend 的 pod 的集群节点必须安装一个相应的过滤器，允许所有 role=frontend pod 的所有 IP 地址发起与所有本地 role=backend pod 的 IP 地址的连接。所有其他的连接请求都应该被拒绝。这可能看起来像这样。如果目标地址是 10.1.1.2，那么只有当源地址是下列之一时才允许连接 [10.1.2.2,10.1.2.3,20.4.9.1]。\n每次启动或停止带有 role=frontend …","relpermalink":"/cilium-handbook/security/identity/","summary":"Kubernetes 等容器管理系统部署了一个网络模型，该模型为每个 pod（容器组）分配一个单独的 IP 地址。这确保了架构的简单性，避免了不必要的网络地址转换（NAT），并为每个单独的容器提供了全范围的端口号以供使用。这种模","title":"基于身份"},{"content":"虽然监控数据路径状态提供对数据路径状态的自省，但默认情况下它只会提供对三层/四层数据包事件的可视性。如果配置了 七层示例，则可以查看七层协议，但这需要编写每个选定端点的完整策略。为了在不配置完整策略的情况下获得对应用程序的更多可视性，Cilium 提供了一种在与 Kubernetes 一起运行时通过注解来规定可视性的方法。\n可视性信息由注解中以逗号分隔的元组列表表示：\n\u0026lt;{Traffic Direction}/{L4 Port}/{L4 Protocol}/{L7 Protocol}\u0026gt;\n例如：\n\u0026lt;Egress/53/UDP/DNS\u0026gt;,\u0026lt;Egress/80/TCP/HTTP\u0026gt; 为此，你可以在 Kubernetes YAML 中或通过命令行提供注释，例如：\nkubectl annotate pod foo -n bar io.cilium.proxy-visibility=\u0026#34;\u0026lt;Egress/53/UDP/DNS\u0026gt;,\u0026lt;Egress/80/TCP/HTTP\u0026gt;\u0026#34; Cilium 将拾取 pod 已收到这些注释，并将透明地将流量重定向到代理，以便显示 cilium monitor 流量的输出被重定向 …","relpermalink":"/cilium-handbook/policy/visibility/","summary":"虽然监控数据路径状态提供对数据路径状态的自省，但默认情况下它只会提供对三层/四层数据包事件的可视性。如果配置了 七层示例，则可以查看七层协议，但这需要编写每个选定端点的完整策略。为了在不配置完整策略的情","title":"七层可视性"},{"content":"端点到端点 首先，我们使用可选的七层出口和入口策略显示本地端点到端点的流程。随后是启用了套接字层强制的同一端点到端点流。为 TCP 流量启用套接字层实施后，启动连接的握手将遍历端点策略对象，直到 TCP 状态为 ESTABLISHED。然后在建立连接后，只需要七层策略对象。\n端点到端点的流程 端点到出口 接下来，我们使用可选的 overlay 网络显示本地端点到出口。在可选的覆盖网络中，网络流量被转发到与 overlay 网络对应的 Linux 网络接口。在默认情况下，overlay 接口名为 cilium_vxlan。与上面类似，当启用套接字层强制并使用七层代理时，我们可以避免在端点和 TCP 流量的七层策略之间运行端点策略块。如果启用，可选的 L3 加密块将加密数据包。\n端点到出口的流程 入口到端点 最后，我们还使用可选的 overlay 网络显示到本地端点的入口。与上述套接字层强制类似，可用于避免代理和端点套接字之间的一组策略遍历。如果数据包在接收时被加密，则首先将其解密，然后通过正常流程进行处理。\n入口到端点流程 这样就完成了数据路径概述。更多 BPF 细节可以在 BPF …","relpermalink":"/cilium-handbook/ebpf/lifeofapacket/","summary":"端点到端点 首先，我们使用可选的七层出口和入口策略显示本地端点到端点的流程。随后是启用了套接字层强制的同一端点到端点流。为 TCP 流量启用套接字层实施后，启动连接的握手将遍历端点策略对象，直到 TCP 状态为 ESTA","title":"数据包流程"},{"content":"现在我们将探索云原生应用架构的几个主要特征，和这些特征是如何解决我们前面提到的使用云原生应用架构的动机。\n12 因素应用 12 因素应用是一系列云原生应用架构的模式集合，最初由 Heroku 提出。这些模式可以用来说明什么样的应用才是云原生应用。它们关注速度、安全、通过声明式配置扩展、可横向扩展的无状态 / 无共享进程以及部署环境的整体松耦合。如 Cloud Foundry、Heroku 和 Amazon ElasticBeanstalk 都对部署 12 因素应用进行了专门的优化。\n在 12 因素的背景下，应用（或者叫 app）指的是独立可部署单元。组织中经常把一些互相协作的可部署单元称作一个应用。\n12 因素应用遵循以下模式：\n代码库\n每个可部署 app 在版本控制系统中都有一个独立的代码库，可以在不同的环境中部署多个实例。\n依赖\nApp 应该使用适当的工具（如 Maven、Bundler、NPM）来对依赖进行显式的声明，而不该在部署环境中隐式的实现依赖。\n配置\n配置或其他随发布环境（如部署、staging、生产）而变更的部分应当作为操作系统级的环境变量注入。\n后端服务\n后端服务，例 …","relpermalink":"/migrating-to-cloud-native-application-architectures/the-rise-of-cloud-native/defining-cloud-native-architectures/","summary":"现在我们将探索云原生应用架构的几个主要特征，和这些特征是如何解决我们前面提到的使用云原生应用架构的动机。 12 因素应用 12 因素应用是一系列云原生应用架构的模式集合，最初由 Heroku 提出。这些模式可以用来说明什么样的","title":"1.2 云原生架构的定义"},{"content":"在看了基于微服务的应用所需的各种应用服务后，考虑一下提供这些服务的服务网格的架构。服务网格由两个主要部分组成：控制平面和数据平面。\n2.2.1 控制平面 控制平面有几个组件。虽然服务网格的数据面主要由作为容器运行在与应用容器相同的 Pod 中的代理组成，但控制面组件在它们自己的 Pod、节点和相关集群中运行。以下是控制平面的 各种功能：\nEnvoy sidecar 代理的服务发现和配置 自动化的密钥和证书管理 用于策略定义和收集遥测数据的 API 服务网格组件的配置摄取 管理一个到服务网格的入站连接（入站网关） 管理来自服务网格的出站连接（出口网关） 将 sidecar 代理注入那些托管应用程序微服务容器的 Pod、节点或命名空间中 总的来说，控制平面帮助管理员用配置数据填充数据平面组件，这些数据是由控制平面的策略产生的。上述功能 3 的策略可能包括网络路由策略、负载均衡策略、蓝绿部署的策略、金丝雀部署、超时、重试和断路能力。这后三项被统称为网络基础设施服务的弹性能力的特殊名称。最后要说的是与安全相关的策略（例如，认证和授权策略、TLS 建立策略等）。这些策略规则由一个模块解析，该模块 …","relpermalink":"/service-mesh-devsecops/reference-platform/service-mesh-software-architecture/","summary":"在看了基于微服务的应用所需的各种应用服务后，考虑一下提供这些服务的服务网格的架构。服务网格由两个主要部分组成：控制平面和数据平面。 2.2.1 控制平面 控制平面有几个组件。虽然服务网格的数据面主要由作为容器运行在","title":"2.2 服务网格架构"},{"content":"在本节中，我们将探讨采用云原生应用架构的组织在创建团队时需要进行的变革。这个重组背后的理论是著名的康威定律。我们的解决方案是在长周期的产品开发中，创建一个包含了各方面专业员工的团队，而不是将他们分离在单一的团队中，例如测试人员。\n业务能力团队 设计系统的组织，最终产生的设计等同于组织之内、之间的沟通结构。\n——Melvyn Conway\n我们已经讨论了“从孤岛到 DevOps”将 IT 组织成专门的信息孤岛的做法。我们很自然地创造了这些孤岛，并把个体放到与这些孤岛一致的团队中。但是当我们需要构建一个新的软件的时候会怎么样？\n一个很常见的做法是成立一个项目团队。该团队向项目经理汇报，然后项目经理与各种孤岛合作，为项目所需的各个专业领域寻求“资源”。正如康威定律所言，这些团队将很自然地在系统中构筑起各种孤岛，我们最终得到的是联合各种孤岛相对应的孤立模块的架构：\n数据访问层 服务层 Web MVC 层 消息层 等等 这些层次中的每一层都跨越了多个业务能力领域，使得在其之上的创新和部署独立于其他业务能力的新功能变的非常困难。 …","relpermalink":"/migrating-to-cloud-native-application-architectures/changes-needed/organizational-change/","summary":"在本节中，我们将探讨采用云原生应用架构的组织在创建团队时需要进行的变革。这个重组背后的理论是著名的康威定律。我们的解决方案是在长周期的产品开发中，创建一个包含了各方面专业员工的团队，而不是将他们分离在","title":"2.2 组织变革"},{"content":"DevSecOps 是一个敏捷的、自动化的开发和部署过程，它使用称为 CI/CD 管道的原语，在自动化工具的帮助下，将软件从构建阶段带到部署阶段，最后到运行时间 / 操作阶段。这些管道是将开发者的源代码带过各个阶段的工作流程，如构建、测试、打包、交付，以及在各个阶段由测试工具支持的部署。\nDevSecOps 平台是指各种 CI/CD 管道（针对每种代码类型）运行的资源集合。至少，这个平台由以下部分组成。\n(a) 管道软件\nCI 软件 —— 从代码库中提取代码，调用构建软件，调用测试工具，并将测试后的工件存储到图像注册表中。 CD 软件 —— 拉出工件、软件包，并根据 IaC 中的计算、网络和存储资源描述，部署软件包。 (b) SDLC 软件\n构建工具（例如，IDE） 测试工具（SAST、DAST、SCA） (c) 存储库\n源代码库（如 GitHub） 容器镜像存储库或注册表 (d) 可观测性或监测工具\n日志和日志聚合工具 产生指标的工具 追踪工具（应用程序的调用顺序） 可视化工具（结合上述数据生成仪表盘 / 警报）。 在 DevSecOps 平台中，通过内置的设计功能（如零信任）和使用 …","relpermalink":"/service-mesh-devsecops/devsecops/devsecops-platform/","summary":"DevSecOps 是一个敏捷的、自动化的开发和部署过程，它使用称为 CI/CD 管道的原语，在自动化工具的帮助下，将软件从构建阶段带到部署阶段，最后到运行时间 / 操作阶段。这些管道是将开发者的源代码带过各个阶段的工作流程，如构建、","title":"3.2 DevSecOps 平台"},{"content":"当我们开始构建由微服务组成的分布式系统时，我们还会遇到在开发单体应用时通常不会遇到的非功能性要求。有时，使用物理定律就可以解决这些问题，例如一致性、延迟和网络分区问题。然而，脆弱性和易控性的问题通常可以使用相当通用的模式来解决。在本节中，我们将介绍帮助我们解决这些问题的方法。\n这些方法来自于 Spring Cloud 项目和 Netflix OSS 系列项目的组合。\n版本化和分布式配置 在“12 因素应用“中我们讨论过通过操作系统级环境变量为应用注入对应的配置，强调了这种配置管理方式的重要性。这种方式特别适合简单的系统，但是，当系统扩大后，有时我们还需要附加的配置能力：\n为调试一个生产上的问题而变更运行的应用程序日志级别 更改 message broker 中接收消息的线程数 报告所有对生产系统配置所做的更改以支持审计监管 运行中的应用切换功能开关 保护配置中的机密信息（如密码） 为了支持这些特性，我们需要配置具有以下特性的配置管理方法：\n版本控制 可审计 加密 在线刷新 Spring Cloud 项目中包含的一个可提供这些功能的配置服务器。此配置服务器通过 Git …","relpermalink":"/migrating-to-cloud-native-application-architectures/migration-cookbook/distributed-systems-recipes/","summary":"当我们开始构建由微服务组成的分布式系统时，我们还会遇到在开发单体应用时通常不会遇到的非功能性要求。有时，使用物理定律就可以解决这些问题，例如一致性、延迟和网络分区问题。然而，脆弱性和易控性的问题通常可","title":"3.2 使用分布式系统"},{"content":"应用程序代码和应用服务代码驻留在容器编排和资源管理平台中，而实现与之相关的工作流程的 CI/CD 软件通常驻留在同一平台中。应使用第 4.6 节所述的步骤对该管道进行保护，该管道控制下的应用程序代码应接受第 4.8 节所述的安全测试。此外，应用程序所在的调度平台本身应使用运行时安全工具（如 Falco）进行保护，该工具可以实时读取操作系统内核日志、容器日志和平台日志，并根据威胁检测规则引擎对其进行处理，以提醒用户注意恶意行为（例如，创建有特权的容器、未经授权的用户读取敏感文件等）。它们通常有一套默认（预定义）的规则，可以在上面添加自定义规则。在平台上安装它们，可以为集群中的每个节点启动代理，这些代理可以监控在该节点的各个 Pod 中运行的容器。这种类型的工具的优点是，它补充了现有平台的本地安全措施，如访问控制模型和 Pod 安全策略，通过实际检测它们的发生来 防止漏洞。\n","relpermalink":"/service-mesh-devsecops/implement/ci-cd-pipeline-for-application-code-and-application-services-code/","summary":"应用程序代码和应用服务代码驻留在容器编排和资源管理平台中，而实现与之相关的工作流程的 CI/CD 软件通常驻留在同一平台中。应使用第 4.6 节所述的步骤对该管道进行保护，该管道控制下的应用程序代码应接受第 4.8 节所述的安全","title":"4.2 应用程序代码和应用服务代码的 CI/CD 管道"},{"content":"作者\nCybersecurity and Infrastructure Security Agency (CISA)\nNational Security Agency (NSA) Cybersecurity Directorate Endpoint Security\n联系信息\n客户要求 / 一般网络安全问题。\n网络安全需求中心，410-854-4200，Cybersecurity_Requests@nsa.gov。\n媒体咨询 / 新闻台\n媒体关系，443-634-0721，MediaRelations@nsa.gov。\n关于事件响应资源，请联系 CISA：CISAServiceDesk@cisa.dhs.gov。\n中文版\n关于本书中文版的信息请联系 Jimmy Song：jimmysong@jimmysong.io。\n宗旨\n国家安全局和 CISA 制定本文件是为了促进其各自的网络安全，包括其制定和发布网络安全规范和缓解措施的责任。这一信息可以被广泛分享，以触达所有适当的利益相关者。\n","relpermalink":"/kubernetes-hardening-guidance/publication-information/","summary":"作者 Cybersecurity and Infrastructure Security Agency (CISA) National Security Agency (NSA) Cybersecurity Directorate Endpoint Security 联系信息 客户要求 / 一般网络安全问题。 网络安全需求中心，410-854-4200，Cybersecurity_Requests@nsa.gov。 媒体咨询 / 新闻台 媒体关系，","title":"出版信息"},{"content":"基础设施技术的历史向来引人入胜。由于基础设施的规模巨大，它已经经历了一次快速的颠覆性变革。除了计算机和互联网的早期，基础设施可谓日新月异。这些创新使基础架构更快，更可靠，更有价值。\n有些公司的人将基础设施推到了极限，他们已经找到了自动化和抽象的方法，提取基础设施更多商业价值。通过提供灵活的可用资源，他们将曾经是昂贵的成本中心转变为所需的商业公用事业。\n然而，公共事业公司很少为企业提供财务价值，这意味着基础设施往往被忽略并被视为不必要的成本。这使得投入创新或改进的时间和金钱很少。\n这样一个如此简单且令人着迷的业务栈怎么可以被轻易忽略？当基础设施出现故障时，业务显然会受到重视，那么为什么基础设施很难改善呢？\n基础设施已经达到了使消费者都感到无聊的成熟度。然而，它的潜力和新挑战又激发了实施者和工程师们新的激情。\n扩展基础设施并使用新的业务方式让来自不同行业的工程师都能找到解决方案。开源软件（OSS）和社区间的互相协作，这股力量又促使了创新的激增。\n如果管理得当，今天基础设施和应用方面的挑战将会不一样。这使得基础设施建设者和维护人员可以取得进展并开展新的有意义的工作。\n有些公司克服了诸如可扩展 …","relpermalink":"/cloud-native-infra/introduction/","summary":"基础设施技术的历史向来引人入胜。由于基础设施的规模巨大，它已经经历了一次快速的颠覆性变革。除了计算机和互联网的早期，基础设施可谓日新月异。这些创新使基础架构更快，更可靠，更有价值。 有些公司的人将基础设","title":"介绍"},{"content":"云原生应用已经发展成为一个标准化的架构，由以下部分组成。\n多个松散耦合的组件被称为微服务（通常或典型地以容器形式实现）。 一个应用服务基础设施，为用户、服务和设备提供安全通信、认证和授权等服务（例如，服务网格）。 由于安全、商业竞争力和其固有的结构（松散耦合的应用组件），这类应用需要一个不同的应用、部署和运行时监控范式 —— 统称为软件生命周期范式。DevSecOps（分别由开发、安全和运维的首字母缩写组成）是这些应用的开发、部署和运维的促进范式之一，其基本要素包括持续集成、持续交付和持续部署（CI/CD）管道。\nCI/CD 管道是将开发人员的源代码通过各个阶段的工作流程，如构建、功能测试、安全扫描漏洞、打包和部署，由带有反馈机制的自动化工具支持。在本文中，应用环境中涉及的整个源代码集被分为五种代码类型：\n应用代码，它体现了执行一个或多个业务功能的应用逻辑。 应用服务代码，用于服务，如会话建立、网络连接等。 基础设施即代码，它是以声明性代码的形式存在的计算、网络和存储资源。 策略即代码，这是运行时策略（例如，零信任），以声明性代码的形式表达。 可观测性即代码，用于持续监测应用程序的健康 …","relpermalink":"/service-mesh-devsecops/executive-summary/","summary":"云原生应用已经发展成为一个标准化的架构，由以下部分组成。 多个松散耦合的组件被称为微服务（通常或典型地以容器形式实现）。 一个应用服务基础设施，为用户、服务和设备提供安全通信、认证和授权等服务（例如，服务","title":"执行摘要"},{"content":"云原生一词已经被过度的采用，很多软件都号称是云原生，很多打着云原生旗号的会议也如雨后春笋般涌现。\n云原生本身甚至不能称为是一种架构，它首先是一种基础设施，运行在其上的应用称作云原生应用，只有符合云原生设计哲学的应用架构才叫云原生应用架构。\n云原生的设计理念 云原生系统的设计理念如下：\n面向分布式设计（Distribution）：容器、微服务、API 驱动的开发； 面向配置设计（Configuration）：一个镜像，多个环境配置； 面向韧性设计（Resistancy）：故障容忍和自愈； 面向弹性设计（Elasticity）：弹性扩展和对环境变化（负载）做出响应； 面向交付设计（Delivery）：自动拉起，缩短交付时间； 面向性能设计（Performance）：响应式，并发和资源高效利用； 面向自动化设计（Automation）：自动化的 DevOps； 面向诊断性设计（Diagnosability）：集群级别的日志、metric 和追踪； 面向安全性设计（Security）：安全端点、API Gateway、端到端加密； 以上的设计理念很多都是继承自分布式应用的设计理念。虽然有如此多 …","relpermalink":"/cloud-native-handbook/intro/cloud-native-philosophy/","summary":"云原生一词已经被过度的采用，很多软件都号称是云原生，很多打着云原生旗号的会议也如雨后春笋般涌现。 云原生本身甚至不能称为是一种架构，它首先是一种基础设施，运行在其上的应用称作云原生应用，只有符合云原生设","title":"云原生的设计哲学"},{"content":"云原生社区是由 宋净超（Jimmy Song） 于 2020 年 5 月发起的，企业中立的云原生终端用户社区。社区秉持“共识、共治、共建、共享”的原则。社区的宗旨是：连接、中立、开源。立足中国，面向世界，企业中立，关注开源，回馈开源。了解更多请访问云原生社区官网：https://cloudnative.to。\n成立背景 Software is eating the world. —— Marc Andreessen\n“软件正在吞噬这个世界”已被大家多次引用，随着云原生（Cloud Native）的崛起，我们想说的是“Cloud Native is eating the software”。随着越来越多的企业将服务迁移上云，企业原有的开发模式以及技术架构已无法适应云的应用场景，其正在被重塑，向着云原生的方向演进。\n那么什么是云原生？云原生是一系列架构、研发流程、团队文化的最佳实践组合，以此支撑更快的创新速度、极致的用户体验、稳定可靠的用户服务、高效的研发效率。开源社区与云原生的关系密不可分，正是开源社区尤其是终端用户社区的存在，极大地促进了以容器、服务网格、微服务等为代表的云原生技术的持 …","relpermalink":"/cloud-native-handbook/community/cnc/","summary":"云原生社区是由 宋净超（Jimmy Song） 于 2020 年 5 月发起的，企业中立的云原生终端用户社区。社区秉持“共识、共治、共建、共享”的原则。社区的宗旨是：连接、中立、开源。立足中国，面向世界，企业中立，关注开","title":"云原生社区（中国）"},{"content":"Istio 是一个服务网格的开源实现。Istio 支持以下功能。\n流量管理\n利用配置，我们可以控制服务间的流量。设置断路器、超时或重试都可以通过简单的配置改变来完成。\n可观测性\nIstio 通过跟踪、监控和记录让我们更好地了解你的服务，它让我们能够快速发现和修复问题。\n安全性\nIstio 可以在代理层面上管理认证、授权和通信的加密。我们可以通过快速的配置变更在各个服务中执行政策。\nIstio 组件 Istio 服务网格有两个部分：数据平面和控制平面。\n在构建分布式系统时，将组件分离成控制平面和数据平面是一种常见的模式。数据平面的组件在请求路径上，而控制平面的组件则帮助数据平面完成其工作。\nIstio 中的数据平面由 Envoy 代理组成，控制服务之间的通信。网格的控制平面部分负责管理和配置代理。\nIstio 架构 Envoy（数据平面） Envoy 是一个用 C++ 开发的高性能代理。Istio 服务网格将 Envoy 代理作为一个 sidecar 容器注入到你的应用容器旁边。然后该代理拦截该服务的所有入站和出站流量。注入的代理一起构成了服务网格的数据平面。\nEnvoy 代理也是唯一与 …","relpermalink":"/cloud-native-handbook/service-mesh/what-is-istio/","summary":"Istio 是一个服务网格的开源实现。Istio 支持以下功能。 流量管理 利用配置，我们可以控制服务间的流量。设置断路器、超时或重试都可以通过简单的配置改变来完成。 可观测性 Istio 通过跟踪、监控和记录让我们更好地了解你的","title":"什么是 Istio?"},{"content":"可观测性行业正处在巨变中。\n传统上，我们观察系统时使用的是一套筒仓式的、独立的工具，其中大部分包含结构不良（或完全非结构化）的数据。这些独立的工具也是垂直整合的。工具、协议和数据格式都属于一个特定的后端或服务，不能互换。这意味着更换或采用新的工具需要耗费时间来更换整个工具链，而不仅仅是更换后端。\n这种孤立的技术格局通常被称为可观测性的 “三大支柱”：日志、度量和（几乎没有）追踪（见图 1-1）。\n日志\n记录构成事务的各个事件。\n度量\n记录构成一个事务的事件的集合。\n追踪\n测量操作的延迟和识别事务中的性能瓶颈，或者类似的东西。传统上，许多组织并不使用分布式追踪，许多开发人员也不熟悉它。\n图 1-1：可观测性的 \u0026amp;ldquo;三大支柱\u0026amp;rdquo;。 我们用这种方法工作了很久，以致于我们不常质疑它。但正如我们将看到的，“三大支柱” 并不是一种正确的结构化的可观测性方法。事实上，这个术语只是描述了某些技术碰巧被实现的方式，它掩盖了关于如何实际使用我们的工具的几个基本事实。\n什么是事务和资源？ 在我们深入探讨不同可观测性范式的利弊之前，重要的是要定义我们所观察的是什么。我们最感兴趣的分布式系统 …","relpermalink":"/opentelemetry-obervability/history/","summary":"第 1 章：可观测性的历史","title":"第 1 章：可观测性的历史"},{"content":"注意：在考虑这些要点时，请谨记“Code Review 标准”。\n设计 审查中最重要的是 CL 的整体设计。CL 中各种代码的交互是否有意义？此变更是属于您的代码库（codebase）还是属于库（library）？它是否与您系统的其他部分很好地集成？现在是添加此功能的好时机吗？\n功能 这个 CL 是否符合开发者的意图？开发者的意图对代码的用户是否是好的？ “用户”通常都是最终用户（当他们受到变更影响时）和开发者（将来必须“使用”此代码）。\n大多数情况下，我们希望开发者能够很好地测试 CL，以便在审查时代码能够正常工作。但是，作为审查者，仍然应该考虑边缘情况，寻找并发问题，尝试像用户一样思考，并确保您单纯透过阅读方式审查时，代码没有包含任何 bug。\n当要检查 CL 的行为会对用户有重大影响时，验证 CL 的变化将变得十分重要。例如 UI 变更。当您只是阅读代码时，很难理解某些变更会如何影响用户。如果在 CL 中打 patch 或自行尝试这样的变更太不方便，您可以让开发人员为您提供功能演示。\n另一个在代码审查期间特别需要考虑功能的时机，就是如果 CL 中存在某种并行编程，理论上可能导致死 …","relpermalink":"/eng-practices/review/reviewer/looking-for/","summary":"注意：在考虑这些要点时，请谨记“Code Review 标准”。 设计 审查中最重要的是 CL 的整体设计。CL 中各种代码的交互是否有意义？此变更是属于您的代码库（codebase）还是属于库（library）？它是否与您系","title":"Code Review 要点"},{"content":"由于 eBPF 允许在 Linux 内核中运行自定义代码，在解释 eBPF 之前我需要确保你对内核的作用有所了解。然后我们将讨论为什么在修改内核行为这件事情上，eBPF 改变了游戏规则。\nLinux 内核 Linux 内核是应用程序和它们所运行的硬件之间的软件层。应用程序运行在被称为用户空间的非特权层，它不能直接访问硬件。相反，应用程序使用系统调用（syscall）接口发出请求，要求内核代表它行事。这种硬件访问可能涉及到文件的读写，发送或接收网络流量，或者只是访问内存。内核还负责协调并发进程，使许多应用程序可以同时运行。\n应用程序开发者通常不直接使用系统调用接口，因为编程语言给了我们更高级别的抽象和标准库，开发者更容易掌握这些接口。因此，很多人都不知道在程序运行时内核做了什么。如果你想了解内核调用频率，你可以使用 strace 工具来显示程序所做的所有系统调用。这里有一个例子，用 cat 从文件中读取 hello 这个词并将其写到屏幕上涉及到 100 多个系统调用：\nliz@liz-ebpf-demo-1:~$ strace -c cat liz.txt hello % time …","relpermalink":"/what-is-ebpf/changing-the-kernel-is-hard/","summary":"由于 eBPF 允许在 Linux 内核中运行自定义代码，在解释 eBPF 之前我需要确保你对内核的作用有所了解。然后我们将讨论为什么在修改内核行为这件事情上，eBPF 改变了游戏规则。 Linux 内核 Linux 内核是应用程序和它们所运行的硬件之间的软","title":"第二章：修改内核很困难"},{"content":"为什么提交小型 CL? 小且简单的 CL 是指：\n审查更快。审查者更容易抽多次五分钟时间来审查小型 CL，而不是留出 30 分钟来审查一个大型 CL。 审查得更彻底。如果是大的变更，审查者和提交者往往会因为大量细节的讨论翻来覆去而感到沮丧——有时甚至到了重要点被遗漏或丢失的程度。 不太可能引入错误。由于您进行的变更较少，您和您的审查者可以更轻松有效地推断 CL 的影响，并查看是否已引入错误。 如果被拒绝，减少浪费的工作。如果您写了一个巨大的 CL，您的评论者说整个 CL 的方向都错误了，你就浪费了很多精力和时间。 更容易合并。处理大型 CL 需要很长时间，在合并时会出现很多冲突，并且必须经常合并。 更容易设计好。打磨一个小变更的设计和代码健康状况比完善一个大变更的所有细节要容易得多。 减少对审查的阻碍。发送整体变更的自包含部分可让您在等待当前 CL 审核时继续编码。 更简单的回滚。大型 CL 更有可能触及在初始 CL 提交和回滚 CL 之间更新的文件，从而使回滚变得复杂（中间的 CL 也可能需要回滚）。 请注意，审查者可以仅凭 CL 过大而自行决定完全拒绝您的变更。通常他们会感谢您的贡 …","relpermalink":"/eng-practices/review/developer/small-cls/","summary":"为什么提交小型 CL? 小且简单的 CL 是指： 审查更快。审查者更容易抽多次五分钟时间来审查小型 CL，而不是留出 30 分钟来审查一个大型 CL。 审查得更彻底。如果是大的变更，审查者和提交者往往会因为大量细节的讨论翻来覆去","title":"小型 CL"},{"content":"This document describes how you can configure Argo CD \u0026amp; integrate Argo Rollout with TSB GitOps support and use SkyWalking as the metrics provider for canary deployment analysis and progressive delivery automation\nBefore you get started, make sure: ✓ Argo CD is installed in your cluster and Argo CD CLI is configured to connect to your Argo CD server ✓ Argo Rollout is installed in your cluster ✓ TSB is up and running, and GitOps has been enabled for the target cluster Create an Application from a …","relpermalink":"/tsb/howto/gitops/argo-rollouts/","summary":"This document describes how you can configure Argo CD \u0026 integrate Argo Rollout with TSB GitOps support and use SkyWalking as the metrics provider for canary deployment analysis and progressive delivery automation\nBefore you get started, make sure: ✓ Argo CD is installed in your cluster and Argo CD CLI is configured to connect to your Argo CD server ✓ Argo Rollout is installed in your cluster ✓ TSB is up and running, and GitOps has been enabled for the target cluster Create an Application from a Git repository Create a sample application using the below command. An example repository containing Istio’s bookinfo application and TSB configurations is available at https://github.","title":"Canary Analysis \u0026 Progressive Delivery Using Argo Rollout and SkyWalking"},{"content":"import helloWorld1YAML from ‘!!raw-loader!../../assets/howto/helloworld-1.yaml’; import helloWorldWsGroupsYAML from ‘!!raw-loader!../../assets/howto/helloworld-ws-groups.yaml’; import helloWorldIngressYAML from ‘!!raw-loader!../../assets/howto/helloworld-ingress.yaml’; import helloWorldGWYAML from ‘!!raw-loader!../../assets/howto/helloworld-gw.yaml’; import helloWorldClientLBYAML from ‘!!raw-loader!../../assets/howto/helloworld-client-lb.yaml’; import CodeBlock from ‘@theme/CodeBlock’;\nThe …","relpermalink":"/tsb/howto/traffic/load-balance/","summary":"import helloWorld1YAML from ‘!!raw-loader!../../assets/howto/helloworld-1.yaml’; import helloWorldWsGroupsYAML from ‘!!raw-loader!../../assets/howto/helloworld-ws-groups.yaml’; import helloWorldIngressYAML from ‘!!raw-loader!../../assets/howto/helloworld-ingress.yaml’; import helloWorldGWYAML from ‘!!raw-loader!../../assets/howto/helloworld-gw.yaml’; import helloWorldClientLBYAML from ‘!!raw-loader!../../assets/howto/helloworld-client-lb.yaml’; import CodeBlock from ‘@theme/CodeBlock’;\nThe following YAML file has three objects - a Workspace for the application, a Gateway group so that you can configure the application ingress, and a Traffic group that will allow you to configure the canary release process.\n{helloWorldWsGroupsYAML} Store the file as helloworld-ws-groups.yaml, and apply with tctl:\ntctl apply -f helloworld-ws-groups.yaml To deploy your application, start by creating the namespace and enable the Istio sidecar injection.\nkubectl create namespace helloworld kubectl label namespace helloworld istio-injection=enabled Then deploy your application.","title":"Client Side Load Balancing"},{"content":"Let’s try Before you get started, make sure you: ✓ Familiarize yourself with TSB concepts ✓ Install the TSB environment. You can use TSB demo for quick install ✓ Completed TSB usage quickstart. This document assumes you already created Tenant and are familiar with Workspace and Config Groups. Also you need to configure tctl to your TSB environment.\nIn this example, httpbin will be used as the workload. Requests that come to Ingress GW will add a header to the HTTP response as part of the wasm …","relpermalink":"/tsb/howto/wasm/wasm-try/","summary":"Let’s try Before you get started, make sure you: ✓ Familiarize yourself with TSB concepts ✓ Install the TSB environment. You can use TSB demo for quick install ✓ Completed TSB usage quickstart. This document assumes you already created Tenant and are familiar with Workspace and Config Groups. Also you need to configure tctl to your TSB environment.\nIn this example, httpbin will be used as the workload. Requests that come to Ingress GW will add a header to the HTTP response as part of the wasm extension execution.\nDeploy httpbin Service Follow all of the instructions in this document to create the httpbin service.","title":"Example"},{"content":"TSB supports specifying TLS or mTLS parameters for securing communication to external auth servers. This document will show you how to configure TLS validation for an external authorization server by adding a CA certificate to the authorization configuration.\nBefore you get started, make sure you: ✓ Familiarize yourself with TSB concepts ✓ Install the TSB environment. You can use TSB demo for quick install\n✓ Completed TSB usage quickstart. This document assumes you already created Tenant and are …","relpermalink":"/tsb/howto/authorization/tls-verification/","summary":"TSB supports specifying TLS or mTLS parameters for securing communication to external auth servers. This document will show you how to configure TLS validation for an external authorization server by adding a CA certificate to the authorization configuration.\nBefore you get started, make sure you: ✓ Familiarize yourself with TSB concepts ✓ Install the TSB environment. You can use TSB demo for quick install\n✓ Completed TSB usage quickstart. This document assumes you already created Tenant and are familiar with Workspace and Config Groups. Also you need to configure tctl to your TSB environment.\nThe examples in this document will build on top of “Configuring External Authorization in Ingress Gateways”.","title":"External Authz with TLS verification"},{"content":"With an EastWest gateway, any internal service can be made highly available with automated cross-cluster failover without publishing it for external access via an Ingress gateway.\nIn this guide, you’ll:\nDeploy bookinfo application in one cluster cluster-1. You’ll deploy the same bookinfo application in a second cluster cluster-2. Cause the reviews, details or ratings service to fail in cluster-1, and observe that the bookinfo application has failed. Add the reviews, details and ratings services …","relpermalink":"/tsb/howto/gateway/multi-cluster-traffic-routing-with-eastwest-gateway/","summary":"With an EastWest gateway, any internal service can be made highly available with automated cross-cluster failover without publishing it for external access via an Ingress gateway.\nIn this guide, you’ll:\nDeploy bookinfo application in one cluster cluster-1. You’ll deploy the same bookinfo application in a second cluster cluster-2. Cause the reviews, details or ratings service to fail in cluster-1, and observe that the bookinfo application has failed. Add the reviews, details and ratings services to an EastWest gateway. Repeat the failure scenario and observe that the application is not affected, and that internal traffic is routed to the services in cluster-2.","title":"Multi-cluster traffic failover with EastWest Gateways"},{"content":"TSB is capable of applying rate limits for both gateways and sidecars. In this document, we will enable rate limiting for sidecars to control quota for service to service traffic.\nBefore you get started, make sure you: ✓ Familiarize yourself with TSB concepts ✓ Install the TSB environment. You can use TSB demo for quick install\n✓ Completed TSB usage quickstart. This document assumes you already created Tenant and are familiar with Workspace and Config Groups. Also you need to configure tctl to …","relpermalink":"/tsb/howto/rate-limiting/service-to-service/","summary":"TSB is capable of applying rate limits for both gateways and sidecars. In this document, we will enable rate limiting for sidecars to control quota for service to service traffic.\nBefore you get started, make sure you: ✓ Familiarize yourself with TSB concepts ✓ Install the TSB environment. You can use TSB demo for quick install\n✓ Completed TSB usage quickstart. This document assumes you already created Tenant and are familiar with Workspace and Config Groups. Also you need to configure tctl to your TSB environment.\nEnable Rate Limiting Server Read and follow the instructions on Enabling the Rate Limiting Server document.","title":"Service to service rate limiting"},{"content":"安全性是融入 Tetrate Service Bridge (TSB) 架构每一层的一个基本方面。 TSB 的方法建立在零信任原则之上，其中安全性是网格管理环境中每个特性和功能的首要考虑因素。本节深入探讨 TSB 如何将安全视为一等公民，并提供全面的措施来保护你的应用程序、支持合规性工作并防止中断。\n在本节中，你将深入了解以下关键方面：\n租赁和抽象基础设施 访问控制策略 可审计性和日志记录 安全通信的服务身份 租赁和抽象基础设施 TSB 引入了资源的逻辑层次结构，为你的物理基础设施提供了抽象层。这种抽象实现了超越单个虚拟机 (VM) 或 Pod 的安全考虑和操作。 TSB的管理平面提供了一个结构化的框架，简化了服务配置，使其更安全、更易于管理。\nTSB 管理平面中所做的更改适用于环境中的资源集合，而不是处理特定的 VM 或 Pod。这种抽象有助于更好地组织基础设施和理解共享资源，从而最大限度地减少与共享所有权相关的潜在陷阱。\n资源层次结构 Tetrate Service Bridge 资源层次结构 TSB 层级结构的核心是组织。\n组织 每个 TSB 安装都包含一个组织，充当 TSB 范 …","relpermalink":"/tsb/concepts/security/","summary":"安全性是融入 Tetrate Service Bridge (TSB) 架构每一层的一个基本方面。 TSB 的方法建立在零信任原则之上，其中安全性是网格管理环境中每个特性和功能的首要考虑因素。本节深入探讨 TSB 如何将安全视为一等公民，并提供全面的措施来保护你的应用","title":"安全"},{"content":"在本部分中，你将了解如何使用 TSB UI 或 tctl 创建 TSB 租户。\n先决条件 在继续之前，请确保你已完成以下任务：\n熟悉 TSB 概念。 安装 TSB 演示环境。 部署 Istio Bookinfo 示例应用程序。 使用用户界面 在左侧面板的组织下，选择租户。 单击该卡以添加新租户。 输入租户 ID tetrate 。 向你的租户提供显示名称和描述。 单击添加。 使用tctl 创建以下 tenant.yaml 文件：\napiVersion: api.tsb.tetrate.io/v2 kind: Tenant metadata: organization: tetrate name: tetrate spec: displayName: Tetrate 使用 tctl 应用配置：\ntctl apply -f tenant.yaml 通过执行这些步骤，你将成功创建一个名为 tetrate 的 TSB 租户。该租户可用于组织和管理你的 TSB 环境。\n","relpermalink":"/tsb/quickstart/tenant/","summary":"在本部分中，你将了解如何使用 TSB UI 或 tctl 创建 TSB 租户。 先决条件 在继续之前，请确保你已完成以下任务： 熟悉 TSB 概念。 安装 TSB 演示环境。 部署 Istio Bookinfo 示例应用程序。 使用用户界面 在左侧面板的组织下，选择租户。 单击该卡以添加新","title":"创建租户"},{"content":"本页介绍如何将 Kubernetes 集群加入现有的 Tetrate Service Bridge（TSB）管理平面。\n在开始之前，请确保你已经完成以下操作：\n检查 要求 安装 TSB 管理平面 或 演示安装 使用 tctl 登录管理平面（tctl 连接） 检查 TSB 控制平面组件 隔离边界\nTSB 1.6 引入了隔离边界，允许你在 Kubernetes 集群内或跨多个集群中拥有多个 TSB 管理的 Istio 环境。隔离边界的一个好处是你可以执行控制平面的金丝雀升级。\n要启用隔离边界，你必须使用环境变量 ISTIO_ISOLATION_BOUNDARIES=true 更新操作员部署，并在控制平面 CR 中包含 isolationBoundaries 字段。 有关更多信息，请参阅 隔离边界。\n创建集群对象 要为集群创建正确的凭据，以便与管理平面通信，你需要使用管理平面 API 创建一个集群对象。\n根据你的需求调整以下 yaml 对象，并保存到名为 new-cluster.yaml 的文件中。\napiVersion: api.tsb.tetrate.io/v2 kind: …","relpermalink":"/tsb/setup/self-managed/onboarding-clusters/","summary":"本页介绍如何将 Kubernetes 集群加入现有的 Tetrate Service Bridge（TSB）管理平面。 在开始之前，请确保你已经完成以下操作： 检查 要求 安装 TSB 管理平面 或 演示安装 使用 tctl 登录管理平面（tctl 连接） 检查 TSB 控制平面组件 隔离边界 TSB 1.6","title":"加入集群"},{"content":"此Chart安装 TSB 控制平面Operator以将集群引入。与管理平面 Helm Chart类似，它还允许你使用TSB ControlPlane CR安装 TSB 控制平面组件，以及使其正常运行所需的所有密钥。\n在开始之前，请确保你已完成以下操作：\n检查 Helm 安装过程 已安装 TSB 管理平面 使用 tctl 登录到管理平面 安装 yq。这将用于从创建集群响应中获取 Helm 值。 隔离边界\nTSB 1.6 引入了隔离边界，允许你在 Kubernetes 集群内或跨多个集群中拥有多个 TSB 管理的 Istio 环境。隔离边界的好处之一是你可以执行控制平面的金丝雀升级。\n要启用隔离边界，你必须使用环境变量 ISTIO_ISOLATION_BOUNDARIES=true 更新Operator部署，并在控制平面 CR 中包括 isolationBoundaries 字段。 有关更多信息，请参见隔离边界。\n先决条件 在开始之前，你需要创建一个 集群对象 在 TSB 中表示你将安装 TSB 控制平面的集群。将 \u0026lt;cluster-name-in-tsb\u0026gt; …","relpermalink":"/tsb/setup/helm/controlplane/","summary":"此Chart安装 TSB 控制平面Operator以将集群引入。与管理平面 Helm Chart类似，它还允许你使用TSB ControlPlane CR安装 TSB 控制平面组件，以及使其正常运行所需的所有密钥。 在开始之前，请确保你已完成以下操作： 检","title":"控制平面安装"},{"content":"本页介绍如何利用 TSB Operator 来管理数据平面的网关配置。\nTSB Operator 配置为监督数据平面网关组件的生命周期，主动监控所有命名空间中的 IngressGateway 、 Tier1Gateway 和 EgressGateway 自定义资源 (CR)集群。默认情况下，数据平面网关组件驻留在 istio-gateway 命名空间中。你可以在数据平面安装 API 参考文档中找到有关自定义资源 API 的全面详细信息。\n数据平面 Operator 监视其创建的 Kubernetes 资源。每当它检测到监视事件（例如删除部署）时，它都会启动协调以将系统恢复到所需状态，从而有效地重新创建任何已删除的部署。\n控制平面要求\n为了让 TSB Operator 管理数据平面网关组件，同一集群中必须存在功能齐全的控制平面。这就需要有一个有效的 TSB Operator 来管理控制平面，以及有效的 ControlPlane 自定义资源 (CR)。 组件 数据平面 Operator 以下是你可以使用数据平面 Operator 配置和管理的自定义组件类型：\n组件 Service …","relpermalink":"/tsb/concepts/operators/data-plane/","summary":"本页介绍如何利用 TSB Operator 来管理数据平面的网关配置。 TSB Operator 配置为监督数据平面网关组件的生命周期，主动监控所有命名空间中的 IngressGateway 、 Tier1Gateway 和 EgressGateway 自定义资源 (CR)集群。默认情况下，数据平面网关组件驻留在 istio-gateway 命名空间中。你可以","title":"数据平面"},{"content":"在继续之前，请确保你熟悉 Istio 隔离边界 功能。\n升级方法 尽管默认情况下首选并假定使用原地网关升级，但你可以通过在 ControlPlane CR 或 Helm 值文件中设置 ENABLE_INPLACE_GATEWAY_UPGRADE 变量来控制以版本为基础的网关升级的两种方式。\nENABLE_INPLACE_GATEWAY_UPGRADE=true 是默认行为。在使用原地网关升级时，现有的网关部署将使用新的代理映像进行修补，并将继续使用相同的网关服务。这意味着你无需进行任何更改以配置网关的外部 IP。 ENABLE_INPLACE_GATEWAY_UPGRADE=false 意味着将创建一个新的网关服务和部署以进行金丝雀版本的升级，因此现在可能会有两个服务： \u0026lt;网关名称\u0026gt;/\u0026lt;网关名称\u0026gt;-old，负责处理非版本/旧版本控制平面工作负载流量。 \u0026lt;网关名称\u0026gt;-1-6-0，负责处理版本控制平面工作负载流量，将为此新创建的 \u0026lt;网关名称\u0026gt;-canary 服务分配新的外部 IP。 你可以通过使用外部负载均衡器或更新 DNS 条目来控制两个版本之间的流量。\n由于原地网关升级是默认行为，你无 …","relpermalink":"/tsb/setup/upgrades/gateway-upgrade/","summary":"在继续之前，请确保你熟悉 Istio 隔离边界 功能。 升级方法 尽管默认情况下首选并假定使用原地网关升级，但你可以通过在 ControlPlane CR 或 Helm 值文件中设置 ENABLE_INPLACE_GATEWAY_UPGRADE 变量来控制以版本为基础的网关升级的两种方式。 ENABLE_INPLACE_GATEWAY_UPGRADE=true 是默认行为。在使用原地网关升","title":"网关升级"},{"content":"TSB 支持为 TSB 组件进行自动证书管理。你可以启用 TSB 以创建自签名根 CA，用于签发证书，例如 TSB 管理平面的 TLS 证书，用于控制平面与管理平面之间的通信的 内部证书，以及应用程序集群的中间 CA 证书，Istio 在集群中将使用它们来签发应用程序工作负载的证书。\n外部根 CA\n目前，TSB 的自动证书管理不支持使用外部根 CA。将来的版本将添加对外部根 CA 的支持。 启用自动证书管理 要启用自动证书管理，你需要在 TSB 管理平面 CR 或 helm values 中设置 certIssuer 字段：\nspec: certIssuer: selfSigned: {} tsbCerts: {} clusterIntermediateCAs: {} certIssuer 字段是一个你要启用的证书颁发者的映射。目前，TSB 支持以下颁发者：\nselfSigned：这将创建一个自签名的根 CA，用于签发 TSB 组件的证书。 tsbCerts：这将为 TSB 端点提供 TSB TLS 证书，还将提供 TSB 内部证书。 clusterIntermediateCAs：这将 …","relpermalink":"/tsb/setup/certificate/automated-certificate-management/","summary":"TSB 支持为 TSB 组件进行自动证书管理。你可以启用 TSB 以创建自签名根 CA，用于签发证书，例如 TSB 管理平面的 TLS 证书，用于控制平面与管理平面之间的通信的 内部证书，以及应用程序集群的中间 CA 证书，Istio 在集群中将使用","title":"自动证书管理"},{"content":"假设你熟悉 Git、Docker、Kubernetes、持续交付和 GitOps 的核心概念。以下是 Argo CD 特有的一些概念：\n应用程序由清单定义的一组 Kubernetes 资源。这是自定义资源定义 (CRD)。 应用程序源类型使用哪个工具来构建应用程序。 目标状态应用程序的所需状态，由 Git 存储库中的文件表示。 实时状态该应用程序的实时状态。部署了哪些 pod 等。 同步状态实时状态是否与目标状态匹配。部署的应用程序是否与 Git 所说的一样？ 同步使应用程序移动到其目标状态的过程。例如，通过将更改应用到 Kubernetes 集群。 同步操作状态同步是否成功。 刷新将 Git 中的最新代码与实时状态进行比较。弄清楚有什么不同。 健康应用程序的健康状况，是否正常运行？它可以满足请求吗？ 工具从文件目录创建清单的工具。例如定制。请参阅应用程序源类型。 配置管理工具请参阅工具。 配置管理插件自定义工具。 ","relpermalink":"/argo-cd/core-concepts/","summary":"假设你熟悉 Git、Docker、Kubernetes、持续交付和 GitOps 的核心概念。以下是 Argo CD 特有的一些概念： 应用程序由清单定义的一组 Kubernetes 资源。这是自定义资源定义 (CRD)。 应用程序源类型使用哪个工具来构建应","title":"核心概念"},{"content":"使用生成器的概念，应用集控制器提供了一组强大的工具，用于自动化模板化和修改 Argo CD 应用程序。生成器从各种来源（包括 Argo CD 集群和 Git 存储库）生成模板参数数据，支持和启用新的用例。\n虽然可以将这些工具用于任何目的，但这里是应用集控制器旨在支持的一些特定用例。\n用例：集群附加组件 应用集控制器的初始设计重点是允许基础架构团队的 Kubernetes 集群管理员自动创建大量不同的 Argo CD 应用程序，跨多个集群，并将这些应用程序作为单个单元进行管理。 集群附加组件用例 就是其中一个例子。\n在 集群附加组件用例 中，管理员负责为一个或多个 Kubernetes 集群配置集群附加组件：集群附加组件是 Operator，例如 Prometheus Operator 或控制器，例如 argo-workflows 控制器（Argo 生态系统的一部分）。\n通常，这些附加组件是开发团队的应用程序所需的（例如作为多租户集群的租户，他们可能希望向 Prometheus 提供度量数据或通过 Argo Workflows 编排工作流程）。\n由于安装这些插件需要集群级别的权限，而这些 …","relpermalink":"/argo-cd/operator-manual/applicationset/use-cases/","summary":"使用生成器的概念，应用集控制器提供了一组强大的工具，用于自动化模板化和修改 Argo CD 应用程序。生成器从各种来源（包括 Argo CD 集群和 Git 存储库）生成模板参数数据，支持和启用新的用例。 虽然可以将这些工具用于任何目的，","title":"ApplicationSet 控制器用例"},{"content":"你可以使用 Apache APISIX 和 Apache APISIX Ingress Controller 来进行 Argo Rollouts 的流量管理。\n当使用 Apache APISIX Ingress Controller 作为 Ingress 时，ApisixRoute 是支持 基于权重的流量分流 的对象。\n本指南展示了如何将 ApisixRoute 与 Argo Rollouts 集成，以将其用作加权轮询负载均衡器。\n先决条件 Argo Rollouts 需要 Apache APISIX v2.15 或更新版本以及 Apache APISIX Ingress Controller v1.5.0 或更新版本。\n使用 Helm v3 安装 Apache APISIX 和 Apache APISIX Ingress Controller：\nhelm repo add apisix https://charts.apiseven.com kubectl create ns apisix helm upgrade -i apisix apisix/apisix …","relpermalink":"/argo-rollouts/traffic-management/apisix/","summary":"你可以使用 Apache APISIX 和 Apache APISIX Ingress Controller 来进行 Argo Rollouts 的流量管理。 当使用 Apache APISIX Ingress Controller 作为 Ingress 时，ApisixRoute 是支持 基于权重的流量分流 的对象。 本指南展示了如何将 ApisixRoute 与 Argo Rollouts 集成，以将其用作加权轮询负载均衡器。 先决条件 Argo Rollouts 需","title":"Apache APISIX"},{"content":"本指南介绍了 Argo Rollouts 如何与 AWS 负载均衡器控制器集成以进行流量调整。本指南以基本入门指南的概念为基础。\n要求 安装了 AWS ALB Ingress Controller 的 Kubernetes 集群 🔔 提示：请参阅负载均衡器控制器安装说明，了解如何安装 AWS 负载均衡器控制器。\n1. 部署 Rollout、Services 和 Ingress 当 AWS ALB Ingress 用作流量路由器时，Rollout canary 策略必须定义以下字段：\napiVersion: argoproj.io/v1alpha1 kind: Rollout metadata: name: rollouts-demo spec: strategy: canary: # canaryService 和 stableService 是指向 Rollout 将要修改的 Service 的引用，以便将其定向到金丝雀 ReplicaSet 和稳定 ReplicaSet（必填）。 canaryService: rollouts-demo-canary stableService: …","relpermalink":"/argo-rollouts/getting-started/alb/","summary":"本指南介绍了 Argo Rollouts 如何与 AWS 负载均衡器控制器集成以进行流量调整。本指南以基本入门指南的概念为基础。 要求 安装了 AWS ALB Ingress Controller 的 Kubernetes 集群 🔔 提示：请参阅负载均衡器控制器安装说明，了解如何安装 AWS 负载均衡器控制器。 1. 部署 R","title":"AWS Load Balancer Controller 快速开始"},{"content":"水平 Pod 自动缩放（HPA）根据观察到的 CPU 利用率或用户配置的指标自动调整 Kubernetes 资源拥有的 Pod 数量。为了实现这种行为，HPA 仅支持启用了 scale 端点的资源，该端点具有几个必需字段。scale 端点允许 HPA 了解资源的当前状态并修改资源以适当地进行扩展。Argo Rollouts 在 0.3.0 版本中添加了对 scale 端点的支持。在 HPA 修改资源后，Argo Rollouts 控制器负责在副本中协调该变化。由于 Rollout 中的策略非常不同，因此 Argo Rollouts 控制器会针对各种策略以不同的方式处理 scale 端点。下面是不同策略的行为。\n蓝绿部署 HPA 将使用从接收来自活动服务的流量的 ReplicaSet 中获取的指标来缩放 BlueGreen 策略的 Rollouts。当 HPA 更改副本计数时，Argo Rollouts 控制器将首先缩放接收来自活动服务的 ReplicaSet，然后是接收来自预览服务的 ReplicaSet。控制器将缩放接收来自预览服务的 ReplicaSet，以准备在 Rollout  …","relpermalink":"/argo-rollouts/rollout/hpa-support/","summary":"水平 Pod 自动缩放（HPA）根据观察到的 CPU 利用率或用户配置的指标自动调整 Kubernetes 资源拥有的 Pod 数量。为了实现这种行为，HPA 仅支持启用了 scale 端点的资源，该端点具有几个必需字段。scale 端点允许 HPA 了解资源的当前状态","title":"水平 Pod 自动缩放"},{"content":"Rollout Rollout 是 Kubernetes 工作负载资源，相当于 Kubernetes Deployment 对象。它旨在在需要更高级的部署或渐进式交付功能的场景中替换 Deployment 对象。Rollout 提供以下功能，而 Kubernetes Deployment 无法提供：\n蓝绿部署 金丝雀部署 与入口控制器和服务网格集成，用于高级流量路由 与度量提供程序集成，用于蓝绿和金丝雀分析 基于成功或失败的度量自动升级或回滚 渐进式交付 渐进式交付是以受控和逐步的方式发布产品更新的过程，从而降低发布的风险，通常通过耦合自动化和度量分析来驱动更新的自动升级或回滚。\n渐进式交付通常被描述为持续交付的演进，将 CI/CD 中实现的速度优势扩展到部署过程中。这是通过将新版本的曝光限制为子集用户，并观察和分析其正确行为，然后逐步增加曝光范围以涵盖更广泛的受众并持续验证正确性来实现的。\n部署策略 虽然行业已经使用一致的术语来描述各种部署策略，但这些策略的实现在各种工具之间存在差异。为了清楚地了解 Argo Rollouts 的行为，以下是 Argo Rollouts 提供的各种部 …","relpermalink":"/argo-rollouts/concepts/","summary":"Rollout Rollout 是 Kubernetes 工作负载资源，相当于 Kubernetes Deployment 对象。它旨在在需要更高级的部署或渐进式交付功能的场景中替换 Deployment 对象。Rollout 提供以下功能，而 Kubernetes Deployment 无法提供： 蓝绿部署 金丝雀部署 与入口控制器和服务网格集成，用于高级流量","title":"概念"},{"content":"Argo Rollouts 支持以下指标度量：\nPrometheus DataDog NewRelic Wavefront Job Web Kayenta CloudWatch Graphite InfluxDB Apache SkyWalking ","relpermalink":"/argo-rollouts/analysis/metrics/","summary":"Argo Rollouts 支持以下指标度量： Prometheus DataDog NewRelic Wavefront Job Web Kayenta CloudWatch Graphite InfluxDB Apache SkyWalking","title":"指标度量"},{"content":"本章从业务和技术的角度解释了在基础设施中部署 SPIFFE 和 SPIRE 的好处。\n适用于任何地方任何人 SPIFFE 和 SPIRE 旨在加强对软件组件的识别，以一种通用的方式，任何人在任何地方都可以在分布式系统中加以利用。现代基础设施的技术环境是错综复杂的。环境在硬件和软件投资的混合下变得越来越不一样。通过对系统定义、证明和维护软件身份标准化的方式来维护软件安全，无论系统部署在哪里，也无论谁来部署这些系统，都会带来许多好处。\n对于专注于提高业务便利性和回报的企业领导人来说，SPIFFE 和 SPIRE 可以大大降低与管理和签发加密身份文件（如 X.509 证书）的开销相关的成本，并通过消除开发人员对安全的服务间通信所需的身份和认证技术的了解来加速开发和部署。\n对于专注于提供强大、安全和可互操作产品的服务提供商和软件供应商来说，SPIFFE 和 SPIRE 解决了在将许多解决方案互连到最终产品时普遍存在的关键身份问题。例如，SPIFFE 可以作为一个产品的 TLS 功能和用户管理 / 认证功能的基础，一举两得。在另一个例子中，SPIFFE 可以取代管理和发行平台访问的 API 令牌 …","relpermalink":"/spiffe/benefits/","summary":"本章从业务和技术的角度解释了在基础设施中部署 SPIFFE 和 SPIRE 的好处。 适用于任何地方任何人 SPIFFE 和 SPIRE 旨在加强对软件组件的识别，以一种通用的方式，任何人在任何地方都可以在分布式系统中加以利用。现代基础设施的技术环境是错","title":"收益"},{"content":"所有 BPF Map 都是有使用容量上限的。超出限制的插入将失败，从而限制了数据路径的可扩展性。下表显示了映射的默认值。每个限制都可以在源代码中更改。如果需要，将根据要求添加配置选项。\nMap 名称 范围 默认限制 规模影响 连接跟踪 节点或端点 1M TCP/256k UDP 最大 1M 并发 TCP 连接，最大 256k 预期 UDP 应答 NAT 节点 512k 最大 512k NAT 条目 邻居表 节点 512k 最大 512k 邻居条目 端点 节点 64k 每个节点最多 64k 个本地端点 + 主机 IP IP 缓存 节点 512k 最大 256k 端点（IPv4+IPv6），最大 512k 端点（IPv4 或 IPv6）跨所有集群 负载均衡器 节点 64k 跨所有集群的所有服务的最大 64k 累积后端 策略 端点 16k 特定端点的最大允许身份 + 端口 + 协议对 16k 代理 Map 节点 512k 最大 512k 并发重定向 TCP 连接到代理 隧道 节点 64k 跨所有集群最多 32k 节点（IPv4+IPv6）或 64k 节点（IPv4 或 IPv6） IPv4  …","relpermalink":"/cilium-handbook/ebpf/maps/","summary":"所有 BPF Map 都是有使用容量上限的。超出限制的插入将失败，从而限制了数据路径的可扩展性。下表显示了映射的默认值。每个限制都可以在源代码中更改。如果需要，将根据要求添加配置选项。 Map 名称 范围 默认限制 规模影响 连接","title":"eBPF Map"},{"content":"用于 Pod 的 IPv4 地址通常是从 RFC1918 私有地址块分配的，因此不可公开路由。Cilium 会自动将离开集群的所有流量的源 IP 地址伪装成节点的 IPv4 地址，因为节点的 IP 地址已经可以在网络上路由。\nIP 地址伪装示意图 对于 IPv6 地址，只有在使用 iptables 实现模式时才会执行伪装。\n可以使用 IPv4 选项和离开主机的 IPv6 流量禁用此行为。enable-ipv4-masquerade: false``enable-ipv6-masquerade: false\n配置 设置可路由 CIDR\n默认行为是排除本地节点的 IP 分配 CIDR 内的任何目标。如果 pod IP 可在更广泛的网络中路由，则可以使用以下选项指定该网络：ipv4-native-routing-cidr: 10.0.0.0/8 ，在这种情况下，该 CIDR 内的所有目的地都 不会 被伪装。\n设置伪装接口\n请参阅实现模式以配置伪装接口。\n实现模式 基于 eBPF 基于 eBPF 的实现是最有效的实现。它需要 Linux 内核 4.19， …","relpermalink":"/cilium-handbook/networking/masquerading/","summary":"用于 Pod 的 IPv4 地址通常是从 RFC1918 私有地址块分配的，因此不可公开路由。Cilium 会自动将离开集群的所有流量的源 IP 地址伪装成节点的 IPv4 地址，因为节点的 IP 地址已经可以在网络上路由。 IP 地址伪装示意图 对于 IPv6 地址，只有在","title":"IP 地址伪装"},{"content":"所有安全策略的描述都是假设基于会话协议的有状态策略执行。这意味着策略的意图是描述允许的连接建立方向。如果策略允许A =\u0026gt; B，那么从 B 到 A 的回复数据包也会被自动允许。但是，并不自动允许 B 向 A 发起连接。如果希望得到这种结果，那么必须明确允许这两个方向。\n安全策略可以在 ingress 或 egress 处执行。对于 ingress，这意味着每个集群节点验证所有进入的数据包，并确定数据包是否被允许传输到预定的终端。相应地，对于 egress，每个集群节点验证出站数据包，并确定是否允许将数据包传输到预定目的地。\n为了在多主机集群中执行基于身份的安全，发送端点的身份被嵌入到集群节点之间传输的每个网络数据包中。然后，接收集群节点可以提取该身份，并验证一个特定的身份是否被允许与任何本地端点进行通信。\n默认安全策略 如果没有加载任何策略，默认行为是允许所有通信，除非明确启用了策略执行。一旦加载了第一条策略规则，就会自动启用策略执行，然后任何通信必须是白名单，否则相关数据包将被丢弃。\n同样，如果一个端点不受制于四层策略，则允许与所有端口进行通信。将至少一个 四层策略与一个端点相关联，将 …","relpermalink":"/cilium-handbook/security/policyenforcement/","summary":"所有安全策略的描述都是假设基于会话协议的有状态策略执行。这意味着策略的意图是描述允许的连接建立方向。如果策略允许A =\u003e B，那么从 B 到 A 的回复数据包也会被自动允许。但是，并不自动允许 B 向 A 发起连接。如果希","title":"策略执行"},{"content":"本节指定 Cilium 端点的生命周期。\nCilium 中的端点状态包括：\nrestoring：端点在 Cilium 启动之前启动，Cilium 正在恢复其网络配置。 waiting-for-identity：Cilium 正在为端点分配一个唯一的身份。 waiting-to-regenerate：端点接收到一个身份并等待（重新）生成其网络配置。 regenerating：正在（重新）生成端点的网络配置。这包括为该端点编程 eBPF。 ready：端点的网络配置已成功（重新）生成。 disconnecting：正在删除端点。 disconnected：端点已被删除。 端点状态生命周期 可以使用 cilium endpoint list 和 cilium endpoint get CLI 命令查询端点的状态。\n当端点运行时，它会在 waiting-for-identity、waiting-to-regenerate、regenerating 和 ready 状态之间转换。进入 waiting-for-identity 状态的转换表明端点改变了它的身份。 …","relpermalink":"/cilium-handbook/policy/lifecycle/","summary":"本节指定 Cilium 端点的生命周期。 Cilium 中的端点状态包括： restoring：端点在 Cilium 启动之前启动，Cilium 正在恢复其网络配置。 waiting-for-identity：Cilium 正在为端点分配一个唯一的身","title":"端点生命周期"},{"content":"集群网格将网络数据路径扩展到多个集群。它允许所有连接集群中的端点进行通信，同时提供完整的策略执行。负载均衡可通过 Kubernetes 注解获得。\n请参阅如何设置集群网格的说明。\n","relpermalink":"/cilium-handbook/clustermesh/","summary":"集群网格将网络数据路径扩展到多个集群。它允许所有连接集群中的端点进行通信，同时提供完整的策略执行。负载均衡可通过 Kubernetes 注解获得。 请参阅如何设置集群网格的说明。","title":"多集群（集群网格）"},{"content":"可观测性由 Hubble 提供，它可以以完全透明的方式深入了解服务的通信和行为以及网络基础设施。Hubble 能够在多集群（集群网格） 场景中提供节点级别、集群级别甚至跨集群的可视性。有关 Hubble 的介绍以及它与 Cilium 的关系，请阅读 Cilium 和 Hubble 简介部分。\n默认情况下，Hubble API 的范围仅限于 Cilium 代理运行的每个单独节点。换句话说，网络可视性仅提供给本地 Cilium 代理观察到的流量。在这种情况下，与 Hubble API 交互的唯一方法是使用 Hubble CLI（hubble）查询通过本地 Unix Domain Socket 提供的 Hubble API。Hubble CLI 二进制文件默认安装在 Cilium 代理 pod 上。\n部署 Hubble Relay 后，Hubble 提供完整的网络可视性。在这种情况下，Hubble Relay 服务提供了一个 Hubble API，它在 ClusterMesh 场景中涵盖整个集群甚至多个集群。可以通过将 Hubble CLI（hubble）指向 Hubble Relay 服务 …","relpermalink":"/cilium-handbook/concepts/observability/","summary":"可观测性由 Hubble 提供，它可以以完全透明的方式深入了解服务的通信和行为以及网络基础设施。Hubble 能够在多集群（集群网格） 场景中提供节点级别、集群级别甚至跨集群的可视性。有关 Hubble 的介绍以及它与 Cilium 的关系，请阅","title":"可观测性"},{"content":"Kubernetes 版本 以下列出的所有 Kubernetes 版本都经过 e2e 测试，并保证与此 Cilium 版本兼容。此处未列出的旧 Kubernetes 版本不支持 Cilium。较新的 Kubernetes 版本未列出，这取决于新版本的的向后兼容性。\n1.16 1.17 1.18 1.19 1.20 1.21 1.22 1.23 系统要求 Cilium 需要 Linux 内核 \u0026gt;= 4.9。有关所有系统要求的完整详细信息，请参阅系统要求。\n在 Kubernetes 中启用 CNI CNI（容器网络接口）是 Kubernetes 用来委托网络配置的插件层。必须在 Kubernetes 集群中启用 CNI 才能安装 Cilium。这是通过将 --network-plugin=cni 参数在所有节点上传递给 kubelet 来完成的。有关更多信息，请参阅Kubernetes CNI 网络插件文档。\n启用自动节点 CIDR 分配（推荐） Kubernetes 具有自动分配每个节点 IP CIDR 的能力。如果启用，Cilium 会自动使用此功能。这是在 Kubernetes 集群 …","relpermalink":"/cilium-handbook/kubernetes/requirements/","summary":"Kubernetes 版本 以下列出的所有 Kubernetes 版本都经过 e2e 测试，并保证与此 Cilium 版本兼容。此处未列出的旧 Kubernetes 版本不支持 Cilium。较新的 Kubernetes 版本未列出，这取决于新版本的的向后兼容性。 1.16 1.17 1.18 1.19 1.20 1.21 1.22 1.23 系统要求 Cilium 需要 Linux 内核 \u003e= 4.9。有","title":"要求"},{"content":"在这一章中，让我们来谈谈编写 eBPF 代码。我们需要考虑在内核中运行的 eBPF 程序本身，以及与之交互的用户空间代码。\n内核和用户空间代码 首先，你可以用什么编程语言来编写 eBPF 程序？\n内核接受字节码形式的 eBPF 程序 1。人工编写这种字节码是可能的，就像用汇编语言写应用程序代码一样——但对人类来说，使用一种可以被编译（即自动翻译）为字节码的高级语言通常更实用。\n由于一些原因，eBPF 程序不能用任意的高级语言编写。首先，语言编译器需要支持发出内核所期望的 eBPF 字节码格式。其次，许多编译语言都有运行时特性——例如 Go 的内存管理和垃圾回收，使它们不适合。在撰写本文时，编写 eBPF 程序的唯一选择是 C（用 clang/llvm 编译）和最新的 Rust。迄今为止，绝大多数的 eBPF 代码都是用 C 语言发布的，考虑到它是 Linux 内核的语言，这是有道理的。\n至少，用户空间的程序需要加载到内核中，并将其附加到正确的事件中。有一些实用程序，如 bpftool，可以帮助我们解决这个问题，但这些都是低级别的工具，假定你有丰富的 eBPF 知识，它们是为 eBPF  …","relpermalink":"/what-is-ebpf/ebpf-programs/","summary":"在这一章中，让我们来谈谈编写 eBPF 代码。我们需要考虑在内核中运行的 eBPF 程序本身，以及与之交互的用户空间代码。 内核和用户空间代码 首先，你可以用什么编程语言来编写 eBPF 程序？ 内核接受字节码形式的 eBPF 程序 1。人工编写这","title":"第三章：eBPF 程序"},{"content":"现在我们将一些问题转移到了云中的 DevOps 平台。\n分解单体应用 传统的 n 层单体式应用部署到云中后很难维护，因为它们经常对云基础设施提供的部署环境做出不可靠的假设，这些假设云很难提供。例如以下要求：\n可访问已挂载的共享文件系统 P2P 应用服务器集群 共享库 配置文件位于常用的配置文件目录 大多数这些假设都出于这样的事实：单体应用通常都部署在长期运行的基础设施中，并与其紧密结合。不幸的是，单体应用并不太适合弹性和短暂（非长期支持）生命周期的基础设施。\n但是即使我们可以构建一个不需要这些假设的单体应用，我们依然有一些问题需要解决：\n单体式应用的变更周期耦合，使独立业务能力无法按需部署，阻碍创新速度。 嵌入到单体应用中的服务不能独立于其他服务进行扩展，因此负载更难于优化。 新加入组织的开发人员必须适应新的团队，经常学习新的业务领域，并且一次就熟悉一个非常大的代码库。这样会增加 3-6 个月的适应时间，才能实现真正的生产力。 尝试通过堆积开发人员来扩大开发组织，增加了昂贵的沟通和协调成本。 技术栈需要长期承诺。引进新技术太过冒险，可能会对整体产生不利影响。 细心的读者会注意到，该列表 …","relpermalink":"/migrating-to-cloud-native-application-architectures/changes-needed/technical-change/","summary":"现在我们将一些问题转移到了云中的 DevOps 平台。 分解单体应用 传统的 n 层单体式应用部署到云中后很难维护，因为它们经常对云基础设施提供的部署环境做出不可靠的假设，这些假设云很难提供。例如以下要求： 可访问已挂载的共","title":"2.3 技术变革"},{"content":"所涉及的关键原语和实施任务是。\n管道和 CI/CD 管道的概念 CI/CD 管道的构建块 设计和执行 CI/CD 管道 自动化的战略 CI/CD 管道中对安全自动化工具的要求 3.3.1 管道的概念和 CI/CD 管道 DevSecOps 作为一种敏捷应用开发、部署和运维的方法论或框架，与其他方法论一样，是由 各个阶段 组成的。信息在各阶段中的顺序和流动被称为工作流，其中一些阶段可以平行执行，而其他阶段则必须遵循一个顺序。每个阶段可能需要调用一个独特的工作来执行该阶段的活动。\nDevSecOps 在流程工作流中引入的一个独特概念是 管道 的概念。有了管道，就不需要为启动 / 执行流程的每个阶段单独编写作业。相反，只有一个作业从初始阶段开始，自动触发与其他阶段有关的活动 / 任务（包括顺序的和并行的），并创建一个无错误的智能工作流程。\nDevSecOps 中的管道被称为 CI/CD 管道，这是基于它所完成的总体任务和它所包含的两个单独阶段。CD 可以表示持续交付或持续部署阶段。根据这后一个阶段，CI/CD 可以涉及以下任务：\n构建、测试、安全和交付：经过测试的修改后的代码被交付到暂存区。 …","relpermalink":"/service-mesh-devsecops/devsecops/key-primitives-and-implementation-tasks/","summary":"所涉及的关键原语和实施任务是。 管道和 CI/CD 管道的概念 CI/CD 管道的构建块 设计和执行 CI/CD 管道 自动化的战略 CI/CD 管道中对安全自动化工具的要求 3.3.1 管道的概念和 CI/CD 管道 DevSecOps 作为一种敏捷应用开发、部署和运维的方法论或框架，与其他方法","title":"3.3 DevSecOps 关键原语和实施任务"},{"content":"本章中我们讨论了两种帮助我们迁移到云原生应用架构的方法：\n分解原架构\n我们使用以下方式分解单体应用：\n所有新功能都使用微服务形式构建。 通过隔离层将微服务与单体应用集成。 通过定义有界上下文来分解服务，逐步扼杀单体架构。 使用分布式系统\n分布式系统由以下部分组成：\n版本化，分布式，通过配置服务器和管理总线刷新配置。 动态发现远端依赖。 去中心化的负载均衡策略 通过熔断器和隔板阻止级联故障 通过 API 网关集成到特定的客户端上 还有很多其他的模式，包括自动化测试、持续构建与发布管道等。欲了解更多信息，请阅读 Toby Clemson 的《Testing Strategies in a Microservice Architecture》，以及 Jez Humbl 和 David Farley（AddisonWesley）的《Continuous Delivery: Reliable Software Releases through Build, Test, and Deployment Automation》。\n","relpermalink":"/migrating-to-cloud-native-application-architectures/migration-cookbook/summary/","summary":"本章中我们讨论了两种帮助我们迁移到云原生应用架构的方法： 分解原架构 我们使用以下方式分解单体应用： 所有新功能都使用微服务形式构建。 通过隔离层将微服务与单体应用集成。 通过定义有界上下文来分解服务，逐步扼杀","title":"3.3 本章小结"},{"content":"为应用程序分配基础设施的传统方法包括最初用配置参数和持续的任务配置计算和网络资源，如补丁管理（如操作系统和库），建立符合合规法规（如数据隐私），并进行漂移（当前配置不再提供预期的操作状态）纠正。\n基础设施即代码（IaC）是一种声明式的代码，它对计算机指令进行编码，这些指令封装了通过服务的管理 API 在公共云服务或私有数据中心部署虚拟基础设施所需的 参数。换句话说，基础设施是以声明式的方式定义的，并使用用于应用程序代码的相同的源代码控制工具（如 GitOps）进行版本控制。根据特定的 IaC 工具，这种语言可以是脚本语言（如 JavaScript、Python、TypeScript 等）或专有配置语言（如 HCL），可能与标准化语言（如 JSON）兼容也可能不兼容。基本指令包括告诉系统如何配置和管理 基础设施（无论是单个计算实例还是完整的服务器，如物理服务器或虚拟机）、容器、存储、网络连接、连接拓扑和负载均衡器。在某些情况下，基础设施可能是短暂的，基础设施的寿命（无论是不可变的还是可变的）不需要继续配置管理。配置可以与应用程序代码的单个提交相联系，使用的工具可以将应用程序代码和基础设施 …","relpermalink":"/service-mesh-devsecops/implement/ci-cd-pipeline-for-infrastructure-as-code/","summary":"为应用程序分配基础设施的传统方法包括最初用配置参数和持续的任务配置计算和网络资源，如补丁管理（如操作系统和库），建立符合合规法规（如数据隐私），并进行漂移（当前配置不再提供预期的操作状态）纠正。 基础设","title":"4.3 基础设施即代码的 CI/CD 管道"},{"content":"基础架构是指支持应用程序的所有软件和硬件，包括数据中心、操作系统、部署流水线、配置管理以及支持应用程序生命周期所需的任何系统或软件。\n已经有无数的时间和金钱花在了基础架构上。通过多年来不断的技术演化和实践提炼，有些公司已经能够运行大规模的基础架构和应用程序，并且拥有卓越的敏捷性。高效运行的基础架构可以使得迭代更快，缩短投向市场的时间，从而加速业务发展。\n使用云原生基础架构是有效运行云原生应用程序的要求。如果没有正确的设计和实践来管理基础架构，即使是最好的云原生应用程序也会浪费。本书中的实践并不一定需要有巨大的基础架构规模，但如果您想从云计算中获取回报，您应该听从开创了这些模式的人的经验。\n在我们探索如何构建云中运行的应用程序的基础架构之前，我们需要了解我们是如何走到这一步。首先，我们将讨论采用云原生实践的好处。接下来，我们将看一下基础架构的简历，然后讨论下一阶段的功能，称为“云原生”，以及它与您的应用程序、运行的平台及业务之间的关系。\n在明白了这一点后，我们将向您展示解决方案及实现。\n云原生的优势 采用本书中的模式有很多好处。它们仿照谷歌、Netflix 和亚马逊这些成功的公司 ——  …","relpermalink":"/cloud-native-infra/what-is-cloud-native-infrastructure/","summary":"基础架构是指支持应用程序的所有软件和硬件，包括数据中心、操作系统、部署流水线、配置管理以及支持应用程序生命周期所需的任何系统或软件。 已经有无数的时间和金钱花在了基础架构上。通过多年来不断的技术演化和实","title":"第 1 章：什么是云原生基础架构？"},{"content":"Kubernetes® 是一个开源系统，可以自动部署、扩展和管理在容器中运行的应用程序，并且通常托管在云环境中。与传统的单体软件平台相比，使用这种类型的虚拟化基础设施可以提供一些灵活性和安全性的好处。然而，安全地管理从微服务到底层基础设施的所有方面，会引入其他的复杂性。本报告中详述的加固指导旨在帮助企业处理相关风险并享受使用这种技术的好处。\nKubernetes 中三个常见的破坏源是供应链风险、恶意威胁者和内部威胁。\n供应链风险往往是具有挑战性的，可以在容器构建周期或基础设施收购中出现。恶意威胁者可以利用 Kubernetes 架构的组件中的漏洞和错误配置，如控制平面、工作节点或容器化应用程序。内部威胁可以是管理员、用户或云服务提供商。对组织的 Kubernetes 基础设施有特殊访问权的内部人员可能会滥用这些特权。\n本指南描述了与设置和保护 Kubernetes 集群有关的安全挑战。包括避免常见错误配置的加固策略，并指导国家安全系统的系统管理员和开发人员如何部署 Kubernetes，并提供了建议的加固措施和缓解措施的配置示例。本指南详细介绍了以下缓解措施：\n扫描容器和 Pod 的漏 …","relpermalink":"/kubernetes-hardening-guidance/executive-summary/","summary":"Kubernetes® 是一个开源系统，可以自动部署、扩展和管理在容器中运行的应用程序，并且通常托管在云环境中。与传统的单体软件平台相比，使用这种类型的虚拟化基础设施可以提供一些灵活性和安全性的好处。然","title":"执行摘要"},{"content":"本文参考的是 OAM 规范中对云原生应用的定义，并做出了引申。\n云原生应用是一个相互关联但又不独立的组件（service、task、worker）的集合，这些组件与配置结合在一起并在适当的运行时实例化后，共同完成统一的功能目的。\n云原生应用模型 下图是 OAM 定义的云原生应用模型示意图，为了便于理解，图中相同颜色的部分为同一类别的对象定义。\n云原生应用模型 OAM 的规范中定义了以下对象，它们既是 OAM 规范中的基本术语也是云原生应用的基本组成。\nWorkload（工作负载）：应用程序的工作负载类型，由平台提供。 Component 组件）：定义了一个 Workload 的实例，并以基础设施中立的术语声明其运维特性。 Trait（特征）：用于将运维特性分配给组件实例。 ApplicationScope（应用作用域）：用于将组件分组成具有共同特性的松散耦合的应用。 ApplicationConfiguration（应用配置）：描述 Component 的部署、Trait 和 ApplicationScope。 关注点分离 下图是不同角色对于该模型的关注点示意图。\n云原生应用模型中的目 …","relpermalink":"/cloud-native-handbook/intro/define-cloud-native-app/","summary":"本文参考的是 OAM 规范中对云原生应用的定义，并做出了引申。 云原生应用是一个相互关联但又不独立的组件（service、task、worker）的集合，这些组件与配置结合在一起并在适当的运行时实例化后，共同完","title":"什么是云原生应用？"},{"content":"Kubernetes 一词来自希腊语，意思是“飞行员”或“舵手”。这个名字很贴切，Kubernetes 可以帮助你在波涛汹涌的容器海洋中航行。\nKubernetes 是做什么的？什么是 Docker？什么是容器编排？Kubernetes 是如何工作和扩展的？你可能还有很多其他的问题，本文将一一为你解答。\n这篇文章适合初学者，尤其是那些工作忙碌，没有办法抽出太多时间来了解 Kubernetes 和云原生的开发者们，希望本文可以帮助你进入 Kubernetes 的世界。\n简而言之，Kubernetes 提供了一个平台或工具来帮助你快速协调或扩展容器化应用，特别是在 Docker 容器。让我们深入了解一下这些概念。\n容器和容器化 那么什么是容器呢？\n要讨论容器化首先要谈到虚拟机 (VM)，顾名思义，虚拟机就是可以远程连接的虚拟服务器，比如 AWS 的 EC2 或阿里云的 ECS。\n接下来，假如你要在虚拟机上运行一个网络应用 —— 包括一个 MySQL 数据库、一个 Vue 前端和一些 Java 库，在 Ubuntu 操作系统 (OS) 上运行。你不用熟悉其中的每一个技术 —— 你只要记住，一 …","relpermalink":"/cloud-native-handbook/intro/quick-start/","summary":"Kubernetes 一词来自希腊语，意思是“飞行员”或“舵手”。这个名字很贴切，Kubernetes 可以帮助你在波涛汹涌的容器海洋中航行。 Kubernetes 是做什么的？什么是 Docker？什么是容器编排？Kubernetes 是如何工作","title":"云原生快速入门"},{"content":"你可能参加过各种云原生、服务网格相关的 meetup，在社区里看到很多人在分享和讨论 Istio，但是对于自己是否真的需要 Istio 感到踌躇，甚至因为它的复杂性而对服务网格的前景感到怀疑。那么，在你继阅读 Istio SIG 后续文章之前，请先仔细阅读本文，审视一下自己公司的现状，看看你是否有必要使用服务网格，处于 Istio 应用的哪个阶段。\n本文不是对应用服务网格的指导，而是根据社区里经常遇到的问题而整理。在使用 Istio 之前，请先考虑下以下因素：\n你的团队里有多少人？ 你的团队是否有使用 Kubernetes、Istio 的经验？ 你有多少微服务？ 这些微服务使用什么语言？ 你的运维、SRE 团队是否可以支持服务网格管理？ 你有采用开源项目的经验吗？ 你的服务都运行在哪些平台上？ 你的应用已经容器化并使用 Kubernetes 管理了吗？ 你的服务有多少是部署在虚拟机、有多少是部署到 Kubernetes 集群上，比例如何？ 你的团队有制定转移到云原生架构的计划吗？ 你想使用 Istio 的什么功能？ Istio 的稳定性是否能够满足你的需求？ 你是否可以忍受 Istio …","relpermalink":"/cloud-native-handbook/service-mesh/do-you-need-a-service-mesh/","summary":"你可能参加过各种云原生、服务网格相关的 meetup，在社区里看到很多人在分享和讨论 Istio，但是对于自己是否真的需要 Istio 感到踌躇，甚至因为它的复杂性而对服务网格的前景感到怀疑。那么，在你继阅读 Istio SIG 后续","title":"你是否需要 Istio？"},{"content":"眯着眼睛看图表并不是寻找相关性的最佳方式。目前在运维人员头脑中进行的大量工作实际上是可以自动化的。这使运维人员可以在识别问题、提出假设和验证根本原因之间迅速行动。\n为了建立更好的工具，我们需要更好的数据。遥测必须具备以下两个要求以支持高质量的自动分析：\n所有的数据点都必须用适当的索引连接在一个图上。 所有代表常见操作的数据点必须有明确的键和值。 在这一章中，我们将从一个基本的构件开始浏览现代遥测数据模型：属性。\n属性：定义键和值 最基本的数据结构是属性（attribute），定义为一个键和一个值。OpenTelemetry 的每个数据结构都包含一个属性列表。分布式系统的每个组件（HTTP 请求、SQL 客户端、无服务器函数、Kubernetes Pod）在 OpenTelemetry 规范中都被定义为一组特定的属性。这些定义被称为 OpenTelemetry 语义约定。表 2-1 显示了 HTTP 约定的部分列表。\n属性 类型 描述 示例 http.method string HTTP 请求类型 GET; POST; HEAD http.target string 在 HTTP 请求行 …","relpermalink":"/opentelemetry-obervability/the-value-of-structured-data/","summary":"第 2 章：结构化数据的价值","title":"第 2 章：结构化数据的价值"},{"content":"IT 行业正在向微服务架构和云原生解决方案发展。由于使用不同的技术开发了成百上千的微服务，这些系统可能变得复杂，难以调试。\n作为一个应用开发者，你考虑的是业务逻辑——购买产品或生成发票。然而，任何像这样的业务逻辑都会导致不同服务之间的多个服务调用。每个服务可能都有它的超时、重试逻辑和其他可能需要调整或微调的网络特定代码。\n如果在任何时候最初的请求失败了，就很难通过多个服务来追踪，准确地指出失败发生的地方，了解请求为什么失败。是网络不可靠吗？是否需要调整重试或超时？或者是业务逻辑问题或错误？\n服务可能使用不一致的跟踪和记录机制，使这种调试的复杂性增加。这些问题使你很难确定问题发生在哪里，以及如何解决。如果你是一个应用程序开发人员，而调试网络问题不属于你的核心技能，那就更是如此。\n将网络问题从应用程序堆栈中抽离出来，由另一个组件来处理网络部分，让调试网络问题变得更容易。这就是 Envoy 所做的事情。\n在每个服务实例旁边都有一个 Envoy 实例在运行。这种类型的部署也被称为 Sidecar 部署。Envoy 的另一种模式是边缘代理，用于构建 API 网关。\nEnvoy 和应用程序形成一个 …","relpermalink":"/cloud-native-handbook/service-mesh/what-is-envoy/","summary":"IT 行业正在向微服务架构和云原生解决方案发展。由于使用不同的技术开发了成百上千的微服务，这些系统可能变得复杂，难以调试。 作为一个应用开发者，你考虑的是业务逻辑——购买产品或生成发票。然而，任何像这样的业","title":"什么是 Envoy？"},{"content":"总结 现在您已经知道了 Code Review 要点，那么管理分布在多个文件中的评论的最有效方法是什么？\n变更是否有意义？它有很好的描述吗？ 首先看一下变更中最重要的部分。整体设计得好吗？ 以适当的顺序查看 CL 的其余部分。 第一步：全面了解变更 查看 CL 描述和 CL 大致上用来做什么事情。这种变更是否有意义？如果在最初不应该发生这样的变更，请立即回复，说明为什么不应该进行变更。当您拒绝这样的变更时，向开发人员建议应该做什么也是一个好主意。\n例如，您可能会说“看起来你已经完成一些不错的工作，谢谢！但实际上，我们正朝着删除您在这里修改的 FooWidget 系统的方向演进，所以我们不想对它进行任何新的修改。不过，您来重构下新的 BarWidget 类怎么样？“\n请注意，审查者不仅拒绝了当前的 CL 并提供了替代建议，而且他们保持礼貌地这样做。这种礼貌很重要，因为我们希望表明，即使不同意，我们也会相互尊重。\n如果您获得了多个您不想变更的 CL，您应该考虑重整开发团队的开发过程或外部贡献者的发布过程，以便在编写 CL 之前有更多的沟通。最好在他们完成大量工作之前说“不”，避免已经投入心 …","relpermalink":"/eng-practices/review/reviewer/navigate/","summary":"总结 现在您已经知道了 Code Review 要点，那么管理分布在多个文件中的评论的最有效方法是什么？ 变更是否有意义？它有很好的描述吗？ 首先看一下变更中最重要的部分。整体设计得好吗？ 以适当的顺序查看 CL 的其余部分。 第一步：全","title":"查看 CL 的步骤"},{"content":"当您发送 CL 进行审查时，您的审查者可能会对您的 CL 发表一些评论。以下是处理审查者评论的一些有用信息。\n不是针对您 审查的目标是保持代码库和产品的质量。当审查者对您的代码提出批评时，请将其视为在帮助您、代码库和 Google，而不是对您或您的能力的个人攻击。\n有时，审查者会感到沮丧并在评论中表达他们的挫折感。对于审查者来说，这不是一个好习惯，但作为开发人员，您应该为此做好准备。问问自己，“审查者试图与我沟通的建设性意见是什么？”然后像他们实际说的那样操作。\n永远不要愤怒地回应代码审查评论。这严重违反了专业礼仪且将永远存在于代码审查工具中。如果您太生气或恼火而无法好好的回应，那么请离开电脑一段时间，或者做一些别的事情，直到您感到平静，可以礼貌地回答。\n一般来说，如果审查者没有以建设性和礼貌的方式提供反馈，请亲自向他们解释。如果您无法亲自或通过视频通话与他们交谈，请向他们发送私人电子邮件。以友善的方式向他们解释您不喜欢的东西以及您希望他们以怎样不同的方式来做些什么。如果他们也以非建设性的方式回复此私人讨论，或者没有预期的效果，那么请酌情上报给您的经理。\n修复代码 如果审查者说他们不了 …","relpermalink":"/eng-practices/review/developer/handling-comments/","summary":"当您发送 CL 进行审查时，您的审查者可能会对您的 CL 发表一些评论。以下是处理审查者评论的一些有用信息。 不是针对您 审查的目标是保持代码库和产品的质量。当审查者对您的代码提出批评时，请将其视为在帮助您、代码库和","title":"如何处理审查者的评论"},{"content":"TSB provides authorization capability to authorize every request coming to your service from a public network. This document will describe how to configure Tier-1 Gateway authorization using Open Policy Agent (OPA) as an example.\nBefore you get started, make sure you: ✓ Familiarize yourself with TSB concepts. ✓ Completed Tier-1 Gateway routing to Tier-2 Gateway with httpbin already configured in TSB.\n✓ Created a Tenant, and understand Workspaces and Config Groups.\n✓ Configured tctl for your TSB …","relpermalink":"/tsb/howto/authorization/tier1-gateway/","summary":"TSB provides authorization capability to authorize every request coming to your service from a public network. This document will describe how to configure Tier-1 Gateway authorization using Open Policy Agent (OPA) as an example.\nBefore you get started, make sure you: ✓ Familiarize yourself with TSB concepts. ✓ Completed Tier-1 Gateway routing to Tier-2 Gateway with httpbin already configured in TSB.\n✓ Created a Tenant, and understand Workspaces and Config Groups.\n✓ Configured tctl for your TSB environment.\nThe following diagram shows the request/response flow using OPA with Tier-1 Gateways. Requests that come to Tier-1 Gateway will be checked by OPA.","title":"External Authorization in Tier-1 Gateways"},{"content":"Tier-2 gateway or IngressGateway configures a workload to act as a gateway for traffic entering the mesh. The ingress gateway also provides basic API gateway functionalities such as JWT token validation and request authorization.\nIn this guide, you’ll: ✓ Deploy bookinfo application split in two different clusters configured as Tier-2, having productpage in one and reviews, details and rating in the other.\nBefore you get started, make sure that you: ✓ You have already deployed productpage in …","relpermalink":"/tsb/howto/gateway/multi-cluster-traffic-routing-using-tier2gw/","summary":"Tier-2 gateway or IngressGateway configures a workload to act as a gateway for traffic entering the mesh. The ingress gateway also provides basic API gateway functionalities such as JWT token validation and request authorization.\nIn this guide, you’ll: ✓ Deploy bookinfo application split in two different clusters configured as Tier-2, having productpage in one and reviews, details and rating in the other.\nBefore you get started, make sure that you: ✓ You have already deployed productpage in cluster 1 and details, ratings and reviews in cluster 2. For this demo we are assuming you have bookinfo deployed and configured in TSB.","title":"Multi-cluster traffic routing using Tier-2 gateway"},{"content":"In this document, we will enable a rate limit in the Tier-1 Gateway and show how to rate limit based on the client IP address.\nBefore you get started, make sure you: ✓ Familiarize yourself with TSB concepts ✓ Install the TSB environment. You can use TSB demo for quick install\n✓ Completed TSB usage quickstart. This document assumes you already created Tenant and are familiar with Workspace and Config Groups. Also you need to configure tctl to your TSB environment.\nDeploy Tier-1 Gateway and …","relpermalink":"/tsb/howto/rate-limiting/tier1-gateway/","summary":"In this document, we will enable a rate limit in the Tier-1 Gateway and show how to rate limit based on the client IP address.\nBefore you get started, make sure you: ✓ Familiarize yourself with TSB concepts ✓ Install the TSB environment. You can use TSB demo for quick install\n✓ Completed TSB usage quickstart. This document assumes you already created Tenant and are familiar with Workspace and Config Groups. Also you need to configure tctl to your TSB environment.\nDeploy Tier-1 Gateway and Ingress Gateway Before applying any rate limits, please read Multi-cluster traffic shifting with Tier-1 Gateway and familiarize yourself with setting up multi-cluster setup using Tier-1 Gateways.","title":"Rate limiting in Tier-1 Gateway"},{"content":"This article will cover how to send traffic to an external host using HTTPS retries and timeouts.\nBefore you get started, make sure you:\n✓ Familiarize yourself with TSB concepts ✓ Install the TSB environment. You can use TSB demo for quick install\n✓ Completed TSB usage quickstart.\nUnderstanding the problem Considering an external application added to the mesh with a ServiceEntry. The application listens on HTTPS so the traffic you will be sending is expected to use simple TLS.\nThe application …","relpermalink":"/tsb/howto/traffic/external-site-https/","summary":"This article will cover how to send traffic to an external host using HTTPS retries and timeouts.\nBefore you get started, make sure you:\n✓ Familiarize yourself with TSB concepts ✓ Install the TSB environment. You can use TSB demo for quick install\n✓ Completed TSB usage quickstart.\nUnderstanding the problem Considering an external application added to the mesh with a ServiceEntry. The application listens on HTTPS so the traffic you will be sending is expected to use simple TLS.\nThe application client within the mesh will initiate an HTTP request and it will be converted to HTTPS at the sidecar to the external application host, e.","title":"Send traffic to an External Host using HTTPS"},{"content":"本页描述了从集群中卸载 TSB 的步骤。\n注意\n此过程对于所有平面（管理、控制和数据平面）都是相似的，因为你需要删除平面的任何自定义资源，然后删除Operator本身。 数据平面 由于数据平面负责在你的集群中部署入口网关，因此第一步是从你的集群中删除所有 IngressGateway 自定义资源。\nkubectl delete ingressgateways.install.tetrate.io --all --all-namespaces 这将删除集群中每个命名空间中部署的所有 IngressGateway。运行此命令后，数据平面Operator将删除每个网关的部署和相关资源。这可能需要一些时间来确保它们都成功删除。\n为确保你正常删除 istio-operator 部署，你必须按以下顺序缩放并删除数据平面Operator命名空间中的剩余对象：\nkubectl -n istio-gateway scale deployment tsb-operator-data-plane --replicas=0 kubectl -n istio-gateway delete …","relpermalink":"/tsb/setup/self-managed/uninstallation/","summary":"本页描述了从集群中卸载 TSB 的步骤。 注意 此过程对于所有平面（管理、控制和数据平面）都是相似的，因为你需要删除平面的任何自定义资源，然后删除Operator本身。 数据平面 由于数据平面负责在你的集群中部署入口","title":"TSB 卸载"},{"content":"在本节中，你将了解如何创建一个名为 bookinfo-ws 并绑定到 bookinfo 命名空间的 TSB 工作区。\n先决条件 在继续之前，请确保你已完成以下任务：\n熟悉 TSB 概念。 安装 TSB 演示环境。 部署 Istio Bookinfo 示例应用程序。 创建一个租户。 使用用户界面 在左侧面板的“租户”下，选择“工作区”。 单击该卡可添加新的工作区。 输入工作区 ID 作为 bookinfo-ws 。 为你的工作区提供显示名称和描述。 输入 demo/bookinfo 作为初始命名空间选择器。 单击添加。 如果你之前已成功启动演示应用程序，你应该会看到类似以下内容的内容：\n1 个集群 1 命名空间 4 服务 1 个工作区 使用tctl 创建以下 workspace.yaml 文件：\napiversion: api.tsb.tetrate.io/v2 kind: Workspace metadata: organization: tetrate tenant: tetrate name: bookinfo-ws spec: namespaceSelector: names: …","relpermalink":"/tsb/quickstart/workspace/","summary":"在本节中，你将了解如何创建一个名为 bookinfo-ws 并绑定到 bookinfo 命名空间的 TSB 工作区。 先决条件 在继续之前，请确保你已完成以下任务： 熟悉 TSB 概念。 安装 TSB 演示环境。 部署 Istio Bookinfo 示例应用程序。 创建一个租户。 使用用户界面 在左侧面板的“","title":"创建工作区"},{"content":"Tetrate Service Bridge (TSB) 提供强大的流量管理功能，允许有效控制其域内服务之间的流量。 TSB 简化了流量路由、分阶段部署和迁移等复杂任务，从而增强了应用程序交付的整体方法。\n本节涵盖 TSB 流量管理的基本方面：\nTSB 中的网关 TSB 中的基本应用程序流量。 TSB 使用一系列网关来管理流量路由。当流量进入你的 TSB 环境时，它会在到达其预期应用程序之前遍历各个网关。该过程涉及：\n应用程序边缘网关：也称为“边缘网关”，这种共享多租户网关有助于跨集群负载平衡。它将传入流量定向到适当的应用程序入口网关。 应用程序入口网关：称为“应用程序网关”，该网关可以在多个应用程序之间共享，也可以专用于特定应用程序。它控制流量的流动方式以及与应用程序的交互方式。应用程序入口网关归工作区所有，提供对流量的控制。 建议部署多个入口网关以隔离和控制流量。随着时间的推移，随着对网格使用的信心不断增强，可以考虑整合到共享入口网关上。\n智能流量路由 TSB 通过利用每个集群内本地控制平面的信息来确保智能流量路由。它优先考虑本地流量以获得最佳性能和可用性。 Envoy 的按请求功 …","relpermalink":"/tsb/concepts/traffic-management/","summary":"Tetrate Service Bridge (TSB) 提供强大的流量管理功能，允许有效控制其域内服务之间的流量。 TSB 简化了流量路由、分阶段部署和迁移等复杂任务，从而增强了应用程序交付的整体方法。 本节涵盖 TSB 流量管理的基本方面： TSB 中的网关 TSB 中的基本应用","title":"流量管理"},{"content":"此图表安装 TSB 数据平面Operator，用于管理 网关，如 Ingress 网关、Tier-1 网关和 Egress 网关的生命周期。\n注意\n如果你正在使用基于版本的控制平面，则不再需要数据平面Operator来管理 Istio 网关。要了解有关基于版本的控制平面的更多信息，请参阅 Istio 隔离边界 文档。 安装 要安装数据平面Operator，请运行以下 Helm 命令。确保将 \u0026lt;tsb-version\u0026gt; 和 \u0026lt;registry-location\u0026gt; 替换为正确的值。\nhelm install dp tetrate-tsb-helm/dataplane \\ --version \u0026lt;tsb-version\u0026gt; \\ --namespace istio-gateway --create-namespace \\ --set image.registry=\u0026lt;registry-location\u0026gt; 配置 镜像配置 这是一个 必填 字段。将 image.registry 设置为你的私有注册表位置，你已经同步了 TSB 镜像，将 image.tag 设置为要部署的 TSB 版本。\n名称 描述 默认 …","relpermalink":"/tsb/setup/helm/dataplane/","summary":"此图表安装 TSB 数据平面Operator，用于管理 网关，如 Ingress 网关、Tier-1 网关和 Egress 网关的生命周期。 注意 如果你正在使用基于版本的控制平面，则不再需要数据平面Operator来管理 Istio 网关。要了解有关基于版","title":"数据平面安装"},{"content":"在继续之前，请确保你熟悉Istio 隔离边界功能。\n修订的 Istio CNI 在修订的 Istio 环境中，Istio CNI 可以绑定到特定的 Istio 修订版本。考虑以下隔离边界配置，允许管理修订的 Istio 环境：\nspec: ... components: xcp: isolationBoundaries: - name: global revisions: - name: stable istio: tsbVersion: 1.6.1 一旦有了修订版本，可以按照以下所示的隔离边界配置启用 Istio CNI，并指定修订版本：\n非 OpenShift\napiVersion: install.tetrate.io/v1alpha1 kind: ControlPlane metadata: name: \u0026lt;cluster-name\u0026gt; namespace: istio-system spec: components: istio: kubeSpec: CNI: chained: true binaryDirectory: /opt/cni/bin …","relpermalink":"/tsb/setup/upgrades/cni-upgrade/","summary":"在继续之前，请确保你熟悉Istio 隔离边界功能。 修订的 Istio CNI 在修订的 Istio 环境中，Istio CNI 可以绑定到特定的 Istio 修订版本。考虑以下隔离边界配置，允许管理修订的 Istio 环境： spec: ... components: xcp: isolationBoundaries: - name: global revisions: - name: stable istio: tsbVersion: 1.6.1 一旦有了修","title":"修订的 Istio CNI 和升级"},{"content":"你可以从此存储库的最新发布页面下载最新的 Argo CD 版本，其中将包含argocd CLI。\nLinux 和 WSL ArchLinux pacman -S argocd Homebrew brew install argocd 使用 curl 下载 下载最新版本 curl -sSL -o argocd-linux-amd64 https://github.com/argoproj/argo-cd/releases/latest/download/argocd-linux-amd64 sudo install -m 555 argocd-linux-amd64 /usr/local/bin/argocd rm argocd-linux-amd64 下载具体版本 VERSION将以下命令中的替换设置\u0026lt;TAG\u0026gt;为你要下载的 Argo CD 版本：\nVERSION=\u0026lt;TAG\u0026gt; # Select desired TAG from https://github.com/argoproj/argo-cd/releases curl -sSL -o argocd-linux-amd64 …","relpermalink":"/argo-cd/cli-installation/","summary":"你可以从此存储库的最新发布页面下载最新的 Argo CD 版本，其中将包含argocd CLI。 Linux 和 WSL ArchLinux pacman -S argocd Homebrew brew install argocd 使用 curl 下载 下载最新版本 curl -sSL -o argocd-linux-amd64 https://github.com/argoproj/argo-cd/releases/latest/download/argocd-linux-amd64 sudo install -m 555 argocd-linux-amd64 /usr/local/bin/argocd rm argocd-linux-amd64 下载具体版本 VERSION将以下命令中的替换设置\u0026","title":"安装"},{"content":"要求 AWS 负载均衡器控制器 v1.1.5 或更高版本 概述 AWS 负载均衡器控制器（也称为 AWS ALB Ingress Controller）通过配置 AWS 应用程序负载均衡器（ALB）以将流量路由到一个或多个 Kubernetes 服务的 Ingress 对象，实现流量管理。ALB 通过加权目标组的概念提供了高级流量分割功能。AWS 负载均衡器控制器通过对 Ingress 对象的注解进行配置“操作”来支持此功能。\n工作原理 ALB 通过侦听器和包含操作的规则进行配置。侦听器定义客户端的流量如何进入，规则定义如何使用各种操作处理这些请求。一种操作类型允许用户将流量转发到多个目标组（每个目标组都定义为 Kubernetes 服务）。你可以在此处阅读有关 ALB 概念的更多信息。\n由 AWS 负载均衡器控制器管理的 Ingress 通过注解和规范控制 ALB 的侦听器和规则。为了在多个目标组（例如不同的 Kubernetes 服务）之间分割流量，AWS 负载均衡器控制器查看 Ingress 上的特定“操作”注 …","relpermalink":"/argo-rollouts/traffic-management/alb/","summary":"要求 AWS 负载均衡器控制器 v1.1.5 或更高版本 概述 AWS 负载均衡器控制器（也称为 AWS ALB Ingress Controller）通过配置 AWS 应用程序负载均衡器（ALB）以将流量路由到一个或多个 Kubernetes 服务的 Ingress 对象，实现流量管理。ALB 通过加权目","title":"AWS Load Balancer Controller (ALB)"},{"content":"本指南介绍了如何让 Argo Rollouts 与由 AWS App Mesh 管理的服务网格集成。本指南基于 基本入门指南 的概念构建。\n要求\n安装了 AWS App Mesh Controller for K8s 的 Kubernetes 集群 🔔 提示：请参阅 App Mesh Controler Installation instructions 以了解如何开始使用 Kubernetes 的 App Mesh。\n1. 部署 Rollout、服务和 App Mesh CRD 当 App Mesh 用作流量路由器时，Rollout canary 策略必须定义以下强制字段：\napiVersion: argoproj.io/v1alpha1 kind: Rollout metadata: name: my-rollout spec: strategy: canary: # canaryService 和 stableService 是指向 Rollout 将要修改的服务的引用，以便针对金丝雀 ReplicaSet 和稳定 ReplicaSet（均为必需）。 canaryService: …","relpermalink":"/argo-rollouts/getting-started/appmesh/","summary":"本指南介绍了如何让 Argo Rollouts 与由 AWS App Mesh 管理的服务网格集成。本指南基于 基本入门指南 的概念构建。 要求 安装了 AWS App Mesh Controller for K8s 的 Kubernetes 集群 🔔 提示：请参阅 App Mesh Controler Installation instructions 以了解如何开始使用 Kubernetes 的 App Mesh。 1. 部署 Rollout、服务","title":"AppMesh 快速开始"},{"content":"垂直 Pod 自动缩放（Vertical Pod Autoscaling，VPA）通过自动配置资源需求降低维护成本并提高集群资源利用率。\nVPA 模式 VPAs 有四种操作模式：\nAuto：VPA 在创建 pod 时分配资源请求，使用首选的更新机制更新现有 pod 的资源请求。目前，这相当于“Recreate”（见下文）。一旦在不重启（“in-place”）更新 pod 请求方面可用，它可能会成为“Auto”模式的首选更新机制。注意：此 VPA 功能是实验性的，可能会导致你的应用停机。 Recreate：VPA 在创建 pod 时分配资源请求，并在请求的资源与新建议显著不同时将其从现有 pod 中驱逐出来（如果定义了 Pod Disruption Budget，则会尊重它）。只有在需要确保每当资源请求更改时都重启 pod 时才应使用此模式。否则，请优先考虑“Auto”模式，一旦可用，该模式可以利用不重启更新。注意：此 VPA 功能是实验性的，可能会导致你的应用停机。 Initial：VPA 仅在创建 pod 时分配资源请求，从不更改它们。 Off：VPA 不会自动更改 pod 的资源要 …","relpermalink":"/argo-rollouts/rollout/vpa-support/","summary":"垂直 Pod 自动缩放（Vertical Pod Autoscaling，VPA）通过自动配置资源需求降低维护成本并提高集群资源利用率。 VPA 模式 VPAs 有四种操作模式： Auto：VPA 在创建 pod 时分配资源请求，使用首选的更新机","title":"垂直 Pod 自动缩放"},{"content":"这里是 Argo Rollouts 管理的部署中所有参与的组件的概述。\nArgo Rollouts 架构 Argo Rollouts 控制器 这是主要的控制器，它监视集群中的事件并在更改 Rollout 类型的资源时做出反应。控制器将读取所有 Rollout 的详细信息，并将集群带到与 Rollout 定义中描述的相同状态。\n请注意，Argo Rollouts 不会干涉或响应在普通的 Deployment 资源 上发生的任何更改。这意味着你可以在以其他方法部署应用程序的集群中安装 Argo Rollouts。\n要在你的集群中安装控制器并开始进行渐进式交付，请参见安装页面。\nRollout 资源 Rollout 资源是由 Argo Rollouts 引入和管理的自定义 Kubernetes 资源。它与原生 Kubernetes Deployment 资源大多兼容，但具有控制高级部署方法（如金丝雀和蓝/绿部署）的阶段、阈值和方法的额外字段。\n请注意，Argo Rollouts 控制器仅响应在 Rollout 源中发生的那些更改。它不会对普通的 Deployment 资源做任何事情。这意味 …","relpermalink":"/argo-rollouts/architecture/","summary":"这里是 Argo Rollouts 管理的部署中所有参与的组件的概述。 Argo Rollouts 架构 Argo Rollouts 控制器 这是主要的控制器，它监视集群中的事件并在更改 Rollout 类型的资源时做出反应。控制器将读取所有 Rollout 的详细信息，并将集群带到与 Rollout 定义中描述的相同状态。 请","title":"架构"},{"content":"本章解释了什么是身份，以及分配、管理和使用身份的基本知识。这些是你需要知道的概念，以便了解 SPIFFE 和 SPIRE 的工作方式。\n什么是身份？ 对于人类来说，身份是复杂的。人类是独特的个体，不能被克隆，也不能用替代的代码取代他们的思想，而且他们在一生中可能会有多种社会身份。软件服务也同样复杂。\n一个单一的程序可能会扩展到成千上万的节点，或者在构建系统推送新的更新时，一天内多次改变其代码。在这样一个快速变化的环境中，一个身份必须代表服务的特定逻辑目的（例如，客户计费数据库）和与已建立的权威或信任根（例如，my-company.example.org 或生产工作负载的发行机构）的关联。\n一旦为一个组织中的所有服务发布了身份，它们就可以被用于认证：证明一个服务是它所声称的那样。一旦服务经过相互认证，它们就可以使用身份进行授权，或控制谁可以访问这些服务，以及保密性，或保持它们相互传输的数据的秘密。虽然 SPIFFE 本身并不包括认证、授权或保密性，但它发出的身份可用于所有这些。\n为一个组织指定服务身份与设计该组织基础设施的任何其他部分类似：它密切依赖于该组织的需求。当一个服务扩大规模、改 …","relpermalink":"/spiffe/general-concepts-behind-identity/","summary":"本章解释了什么是身份，以及分配、管理和使用身份的基本知识。这些是你需要知道的概念，以便了解 SPIFFE 和 SPIRE 的工作方式。 什么是身份？ 对于人类来说，身份是复杂的。人类是独特的个体，不能被克隆，也不能用替代的代码取代","title":"身份背后的通用概念"},{"content":"根据所使用的 Linux 内核版本，eBPF 数据路径可以完全在 eBPF 中实现不同的功能集。如果某些所需功能不可用，则使用旧版 iptables 实现提供该功能。有关详细信息，请参阅 IPsec 要求。\nkube-proxy 互操作性 下图显示了 kube-proxy 安装的 iptables 规则和 Cilium 安装的 iptables 规则的集成。\n图片 下一章 ","relpermalink":"/cilium-handbook/ebpf/iptables/","summary":"根据所使用的 Linux 内核版本，eBPF 数据路径可以完全在 eBPF 中实现不同的功能集。如果某些所需功能不可用，则使用旧版 iptables 实现提供该功能。有关详细信息，请参阅 IPsec 要求。 kube-proxy 互操作性 下图显示了 kube-proxy 安装的 iptables 规则和 Cilium 安装的 iptables 规","title":"Iptables 用法"},{"content":"默认情况下，Cilium 将 eBPF 数据路径配置为执行 IP 分片跟踪，以允许不支持分片的协议（例如 UDP）通过网络透明地传输大型消息。IP 分片跟踪在 eBPF 中使用 LRU（最近最少使用）映射实现，需要 Linux 4.10 或更高版本。可以使用以下选项配置此功能：\n--enable-ipv4-fragment-tracking：启用或禁用 IPv4 分片跟踪。默认启用。 --bpf-fragments-map-max：控制使用 IP 分片的最大活动并发连接数。对于默认值，请参阅eBPF Maps。 注意\n当使用 kube-proxy 运行 Cilium 时，碎片化的 NodePort 流量可能会由于内核错误而中断，其中路由 MTU 不受转发数据包的影响。Cilium 碎片跟踪需要第一个逻辑碎片首先到达。由于内核错误，可能会在外部封装层上发生额外的碎片，从而导致数据包重新排序并导致无法跟踪碎片。\n内核错误已被 修复 并向后移植到所有维护的内核版本。如果您发现连接问题，请确保您的节点上的内核包最近已升级，然后再报告问题。\n提示\n这是一个测试版功能。如果你遇到任何问题，请提供反 …","relpermalink":"/cilium-handbook/networking/fragmentation/","summary":"默认情况下，Cilium 将 eBPF 数据路径配置为执行 IP 分片跟踪，以允许不支持分片的协议（例如 UDP）通过网络透明地传输大型消息。IP 分片跟踪在 eBPF 中使用 LRU（最近最少使用）映射实现，需要 Linux 4.10 或更高版本。可以","title":"IPv4 分片处理"},{"content":"Cilium 能够透明地将四层代理注入任何网络连接中。这被用作执行更高级别网络策略的基础（请参阅基于 DNS 和 七层示例）。\n可以注入以下代理：\nEnvoy 注入 Envoy 代理 注意\n此功能目前正处于测试阶段。 如果你有兴趣编写 Envoy 代理的 Go 扩展，请参考开发者指南。\nEnvoy 代理注入示意图 如上所述，该框架允许开发人员编写少量 Go 代码（绿色框），专注于解析新的 API 协议，并且该 Go 代码能够充分利用 Cilium 功能，包括高性能重定向 Envoy、丰富的七层感知策略语言和访问日志记录，以及通过 kTLS 对加密流量的可视性（即将推出）。总而言之，作为开发者的你只需要关心解析协议的逻辑，Cilium + Envoy + eBPF 就完成了繁重的工作。\n本指南基于假设的 r2d2 协议（参见 proxylib/r2d2/r2d2parser.go）的简单示例，该协议可用于很久以前在遥远的星系中与简单的协议机器人对话。但它也指向了 cilium/proxylib 目录中已经存在的其他真实协议，例如 Memcached 和 Cassandra。\n下一章 ","relpermalink":"/cilium-handbook/security/proxy/","summary":"Cilium 能够透明地将四层代理注入任何网络连接中。这被用作执行更高级别网络策略的基础（请参阅基于 DNS 和 七层示例）。 可以注入以下代理： Envoy 注入 Envoy 代理 注意 此功能目前正处于测试阶段。 如果你有兴趣编写 Envoy 代理的 Go 扩展，请参","title":"代理注入"},{"content":"策略规则到端点映射 确定哪些策略规则当前对端点有效，并且可以将来自 cilium endpoint list 和 cilium endpoint get 的数据与 cilium policy get 配对。cilium endpoint get 将列出适用于端点的每个规则的标签。可以传递标签列表给 cilium policy get 以显示确切的源策略。请注意，不能单独获取没有标签的规则（无标签会返回节点上的完整策略）。具有相同标签的规则将一起返回。\n在下面的示例中，其中一个 deathstar pod 的端点 id 是 568。我们可以打印应用于它的所有策略：\n$ # Get a shell on the Cilium pod $ kubectl exec -ti cilium-88k78 -n kube-system -- /bin/bash $ # print out the ingress labels $ # clean up the data $ # fetch each policy via each set of labels $ # (Note that while …","relpermalink":"/cilium-handbook/policy/troubleshooting/","summary":"策略规则到端点映射 确定哪些策略规则当前对端点有效，并且可以将来自 cilium endpoint list 和 cilium endpoint get 的数据与 cilium policy get 配对。cilium endpoint get 将列出适用于端点的每个规则的标签。可以传递标签列表给 cilium policy get 以显示确切的源策略。请注意，","title":"故障排除"},{"content":"现在你已经看到了 eBPF 编程的例子，了解到它是如何工作的。虽然基础示例使得 eBPF 看起来相对简单，但也有一些复杂的地方使得 eBPF 编程充满挑战。\n长期以来，有个问题使得编写和发布 eBPF 程序相对困难，那就是内核兼容性。\n跨内核的可移植性 eBPF 程序可以访问内核数据结构，而这些结构可能在不同的内核版本中发生变化。这些结构本身被定义在头文件中，构成了 Linux 源代码的一部分。在过去编译 eBPF 程序时，必须基于你想运行这些程序的内核兼容的头文件集。\nBCC 对可移植性的处理方法 为了解决跨内核的可移植性问题，BCC 1（BPF 编译器集合，BPF Compiler Collection）项目采取了在运行时编译 eBPF 代码的方法，在目标机器上就地进行。这意味着编译工具链需要安装到每个你想让代码运行 2 的目标机器上，而且你必须在工具启动之前等待编译完成，而且文件系统上必须有内核头文件（实际上并不总是这样）。这就引出了 BPF CO-RE。\nCO-RE CO-RE（Compile Once, Run Everyone，编译一次，到处运行）方法由以下元素组成。 …","relpermalink":"/what-is-ebpf/ebpf-complexity/","summary":"现在你已经看到了 eBPF 编程的例子，了解到它是如何工作的。虽然基础示例使得 eBPF 看起来相对简单，但也有一些复杂的地方使得 eBPF 编程充满挑战。 长期以来，有个问题使得编写和发布 eBPF 程序相对困难，那就是内核兼容性。 跨内核的","title":"第四章：eBPF 的复杂性"},{"content":"这一章将介绍 Kubernetes 的设计理念及基本概念。\nKubernetes 设计理念与分布式系统 分析和理解 Kubernetes 的设计理念可以使我们更深入地了解 Kubernetes 系统，更好地利用它管理分布式部署的云原生应用，另一方面也可以让我们借鉴其在分布式系统设计方面的经验。\n分层架构 Kubernetes 设计理念和功能其实就是一个类似 Linux 的分层架构，如下图所示\nKubernetes 分层架构示意图 核心层：Kubernetes 最核心的功能，对外提供 API 构建高层的应用，对内提供插件式应用执行环境 应用层：部署（无状态应用、有状态应用、批处理任务、集群应用等）和路由（服务发现、DNS 解析等） 管理层：系统度量（如基础设施、容器和网络的度量），自动化（如自动扩展、动态 Provision 等）以及策略管理（RBAC、Quota、PSP、NetworkPolicy 等） 接口层：kubectl 命令行工具、客户端 SDK 以及集群联邦 生态系统：在接口层之上的庞大容器集群管理调度的生态系统，可以划分为两个范畴 Kubernetes 外部：日志、监控、配 …","relpermalink":"/kubernetes-handbook/architecture/perspective/","summary":"这一章将介绍 Kubernetes 的设计理念及基本概念。 Kubernetes 设计理念与分布式系统 分析和理解 Kubernetes 的设计理念可以使我们更深入地了解 Kubernetes 系统，更好地利用它管理分布式部署的云原生应用，另一方面也可以让我们借鉴其在分布式系统设计方面的经","title":"Kubernetes 的设计理念"},{"content":"从理论上讲，DevSecOps 原语可以应用于许多应用架构，但最适合于基于微服务的架构，由于应用是由相对较小的、松散耦合的模块组成的，被称为微服务，因此允许采用敏捷开发模式。即使在基于微服务的架构中，DevSecOps 原语的实现也可以采取不同的形式，这取决于平台。在本文中，所选择的平台是一个容器编排和资源管理平台（如 Kubernetes）。该平台是物理（裸机）或虚拟化（如虚拟机、容器）基础设施上的一个抽象层。为了在本文中明确提及该平台或应用环境，它被称为 DevSecOps 原语参考平台，或简称为 参考平台。\n在描述参考平台的 DevSecOps 原语的实现之前，我们假设在部署服务网格组件方面采用了以下 尽职调查：\n用于部署和管理基于服务网格的基础设施（如网络路由）、策略执行和监控组件的安全设计模式 测试证明这些服务网格组件在应用的各个方面（如入口、出口和内部服务）的各种情况下都能按预期工作。 为参考平台实施 DevSecOps 原语所提供的指导与 (a) DevSecOps 管道中使用的工具和 (b) 提供应用服务的服务网格软件无关，尽管来自 Istio 等服务网格产品的例子被用 …","relpermalink":"/service-mesh-devsecops/intro/scope/","summary":"从理论上讲，DevSecOps 原语可以应用于许多应用架构，但最适合于基于微服务的架构，由于应用是由相对较小的、松散耦合的模块组成的，被称为微服务，因此允许采用敏捷开发模式。即使在基于微服务的架构中，D","title":"1.1 范围"},{"content":"本章中，我们讨论了希望通过软件赋予我们业务的能力并迁移到云原生应用架构的动机：\n速度\n比我们的竞争对手更快速得创新、试验并传递价值。\n安全\n在保持稳定性、可用性和持久性的同时，具有快速行动的能力。\n扩展\n根据需求变化弹性扩展。\n移动性\n客户可以随时随地通过任何设备无缝的跟我们交互。\n我们还研究了云原生应用架构的独特特征，以及如何赋予我们这些能力：\n12 因素应用\n一套优化应用设计速度，安全性和规模的模式。\n微服务\n一种架构模式，可帮助我们将部署单位与业务能力保持一致，使每个业务能够独立和自主地移动，这样一来也更快更安全。\n自服务敏捷基础设施\n云平台使开发团队能够在应用和服务抽象层面上运行，提供基础架构级速度，安全性和扩展性。\n基于 API 的协作\n将服务间交互定义为可自动验证协议的架构模式，通过简化的集成工作实现速度和安全性。\n抗脆弱性\n随着速度和规模扩大，系统的压力随之增加，系统的响应能力和安全性也随之提高。\n在下一章中，我们将探讨大多数企业为采用云原生应用架构而需要做出哪些改变。\n下一章 ","relpermalink":"/migrating-to-cloud-native-application-architectures/the-rise-of-cloud-native/summary/","summary":"本章中，我们讨论了希望通过软件赋予我们业务的能力并迁移到云原生应用架构的动机： 速度 比我们的竞争对手更快速得创新、试验并传递价值。 安全 在保持稳定性、可用性和持久性的同时，具有快速行动的能力。 扩展 根据需求","title":"1.3：本章小结"},{"content":"本章中，我们探讨了大多数企业采用云原生应用架构所需要做出的变革。从宏观总体上看是权力下放和自治：\nDevOps\n技能集中化转变为跨职能团队。\n持续交付\n发行时间表和流程的权力下放。\n自治\n决策权力下放。\n我们将这种权力下放编成两个主要的团队结构：\n业务能力团队\n自主决定设计、流程和发布时间表的跨职能团队。\n平台运营团队\n为跨职能团队提供他们所需要运行平台。\n而在技术上，我们也分散自治：\n单体应用到微服务\n将个人业务能力的控制分配给单个自主服务。\n有界上下文\n将业务领域模型的内部一致子集的控制分配到微服务。\n容器化\n将对应用包装的控制分配给业务能力团队。\n编排\n将服务集成控制分配给服务端点。\n所有这些变化造就了无数的自治单元，辅助我们以期望的创新速度安全前行。\n在最后一章中，我们将通过一组操作手册，深入研究迁移到云原生应用架构的技术细节。\n下一章 ","relpermalink":"/migrating-to-cloud-native-application-architectures/changes-needed/summary/","summary":"本章中，我们探讨了大多数企业采用云原生应用架构所需要做出的变革。从宏观总体上看是权力下放和自治： DevOps 技能集中化转变为跨职能团队。 持续交付 发行时间表和流程的权力下放。 自治 决策权力下放。 我们将这种权力下放编","title":"2.4 本章小结"},{"content":"策略即代码涉及编纂所有策略，并作为 CI/CD 管道的一部分运行，使其成为应用程序运行时的一个组成部分。策略类别的例子包括授权策略、网络策略和实现工件策略（例如，容器策略）。典型的 策略即代码软件的策略管理能力可能带有一套预定义的策略类别和策略，也支持通过提供策略模板定义新的策略类别和 相关策略。策略即代码所要求的尽职调查是，它应该提供保护，防止与应用环境（包括基础设施）相关的所有已知威胁，只有当该代码被定期扫描和更新，以应对与应用类别（如网络应用）和托管基础设施相关的威胁，才能确保这一点。下面的表 1 中给出了一些策略类别和相关策略的例子。\n表 1：策略类别和策略实例\n策略类别 策略示例 网络策略和零信任策略 - 封锁指定端口 - 指定入口主机名称 - 一般来说，所有的网络访问控制策略 实施工件策略（例如，容器策略） - 对服务器进行加固，对基础镜像进行漏洞扫描 - 确保容器不以 root 身份运行 - 阻止容器的权限升级 存储策略 - 设置持久性卷大小 - 设置持久性卷回收策略 访问控制策略 - 确保策略涵盖所有数据对象 - 确保策略涵盖管理和应用访问的所有角色 - 确保数据保护策 …","relpermalink":"/service-mesh-devsecops/implement/ci-cd-pipeline-for-policy-as-code/","summary":"策略即代码涉及编纂所有策略，并作为 CI/CD 管道的一部分运行，使其成为应用程序运行时的一个组成部分。策略类别的例子包括授权策略、网络策略和实现工件策略（例如，容器策略）。典型的 策略即代码软件的策略管理能力可能","title":"4.4 策略即代码的 CI/CD 管道"},{"content":"云原生基础架构并不适合所有人。任何架构设计都经过了一系列的权衡。您只有熟悉自己的需求才能决定哪些权衡是有益的，哪些是有害的。\n不要在不了解架构的影响和限制的情况下采用工具或设计。我们相信云原生基础架构有很多好处，但需要意识到不应该盲目的采用。我们不愿意引导大家通过错误的方式来满足需求。\n怎么知道是否应该使用云原生基础架构设计？确定云原生基础架构是否适合您，下面是一些需要了解的问题：\n您有云原生应用程序吗？(有关可从云原生基础架构中受益的应用程序功能，请参阅第 1 章) 您的工程团队是否愿意且能够编写出体现其作业功能的生产质量代码？ 您在本地或公有云是否拥有基于 API 驱动的基础架构（IaaS）？ 您的业务是否需要更快的开发迭代或非线性人员 / 系统缩放比例？ 如果您对所有这些问题都回答“yes”，那么您可能会从本书其余部分介绍的基础架构中受益。如果您对这些问题中的某个问题回答是“no”，这并不意味着您无法从某些云原生实践中受益，但是您可能需要做更多工作，然后才能从此类基础架构中充分受益。\n在业务准备好之前武断地采用云原生基础架构效果会很糟糕，因为这会强制使用一个不正确的解决方案。在没 …","relpermalink":"/cloud-native-infra/when-to-adopt-cloud-native/","summary":"云原生基础架构并不适合所有人。任何架构设计都经过了一系列的权衡。您只有熟悉自己的需求才能决定哪些权衡是有益的，哪些是有害的。 不要在不了解架构的影响和限制的情况下采用工具或设计。我们相信云原生基础架构有","title":"第 2 章：采纳云原生基础架构的时机"},{"content":"勘误 1\nPDF 原文第 4 页，kubelet 端口，默认应为 10250，而不是 10251。\n","relpermalink":"/kubernetes-hardening-guidance/corrigendum/","summary":"勘误 1 PDF 原文第 4 页，kubelet 端口，默认应为 10250，而不是 10251。","title":"勘误"},{"content":"我们在前面看到了通过客户端库来治理服务的架构图，那是我们在改造成服务网格架构前使用微服务架构通常的形式，下图是使用服务网格架构的最终形式。\n服务网格架构示意图 当然在达到这一最终形态之前我们需要将架构一步步演进，下面给出的是参考的演进路线。\nIngress 或边缘代理 如果你使用的是 Kubernetes 做容器编排调度，那么在进化到服务网格架构之前，通常会使用 Ingress Controller，做集群内外流量的反向代理，如使用 Traefik 或 Nginx Ingress Controller。\nIngress 或边缘代理架构示意图 这样只要利用 Kubernetes 的原有能力，当你的应用微服务化并容器化需要开放外部访问且只需要 L7 代理的话这种改造十分简单，但问题是无法管理服务间流量。\n路由器网格 Ingress 或者边缘代理可以处理进出集群的流量，为了应对集群内的服务间流量管理，我们可以在集群内加一个 Router 层，即路由器层，让集群内所有服务间的流量都通过该路由器。\n路由器网格架构示意图 这个架构无需对原有的单体应用和新的微服务应用做什么改造，可以很轻易的迁移进 …","relpermalink":"/cloud-native-handbook/service-mesh/service-mesh-patterns/","summary":"我们在前面看到了通过客户端库来治理服务的架构图，那是我们在改造成服务网格架构前使用微服务架构通常的形式，下图是使用服务网格架构的最终形式。 服务网格架构示意图 当然在达到这一最终形态之前我们需要将架构一步","title":"服务网格的部署模式"},{"content":"自动化开始听起来很神奇，但我们要面对现实：计算机分析不能每天告诉你系统有什么问题或为你修复它。它只能为你节省时间。\n至此，我想停止夸赞，我必须承认自动化的一些局限性。我这样做是因为围绕结合人工智能和可观测性会有相当多的炒作。这种炒作导致了惊人的论断，大意是：“人工智能异常检测和根源分析可以完全诊断问题！” 和 “人工智能操作将完全管理你的系统！”\n谨防炒作 为了摆脱此类炒作，我想明确的是，这类梦幻般的营销主张并不是我所宣称的现代可观测性将提供的。事实上，我预测许多与人工智能问题解决有关的主张大多是夸大其词。\n为什么人工智能不能解决我们的问题？一般来说，机器无法识别软件中的 “问题”，因为定义什么是 “问题” 需要一种主观的分析方式。而我们所讨论的那种现实世界的人工智能不能以任何程度的准确性进行主观决策。机器将始终缺乏足够的背景。\n例如，我们说该版本降低了性能，这里面也隐含了一个期望，就是这个版本包含了每个用户都期望的新功能。对于这个版本，性能退步是一个特征，而不是一个错误。\n虽然它们可能看起来像类似的活动，但在确定相关关系和确定根本原因之间存在着巨大的鸿沟。当你有正确的数据结构时，相关 …","relpermalink":"/opentelemetry-obervability/the-limitations-of-automated-analysis/","summary":"第 3 章：自动分析的局限性","title":"第 3 章：自动分析的局限性"},{"content":"为什么尽快进行 Code Review？ 在 Google，我们优化了开发团队共同开发产品的速度。，而不是优化单个开发者编写代码的速度。个人开发的速度很重要，它并不如整个团队的速度那么重要。\n当代码审查很慢时，会发生以下几件事：\n整个团队的速度降低了。是的，对审查没有快速响应的个人的确完成了其他工作。但是，对于团队其他人来说重要的新功能与缺陷修復将会被延迟数天、数周甚至数月，只因为每个 CL 正在等待审查和重新审查。 开发者开始抗议代码审查流程。如果审查者每隔几天只响应一次，但每次都要求对 CL 进行重大更改，那么开发者可能会变得沮丧。通常，开发者将表达对审查者过于“严格”的抱怨。如果审查者请求相同实质性更改（确实可以改善代码健康状况），但每次开发者进行更新时都会快速响应，则抱怨会逐渐消失。大多数关于代码审查流程的投诉实际上是通过加快流程来解决的。 代码健康状况可能会受到影响。如果审查速度很慢，则造成开发者提交不尽如人意的 CL 的压力会越来越大。审查太慢还会阻止代码清理、重构以及对现有 CL 的进一步改进。 Code Review 应该有多快？ 如果您没有处于重点任务的中，那么您应该 …","relpermalink":"/eng-practices/review/reviewer/speed/","summary":"为什么尽快进行 Code Review？ 在 Google，我们优化了开发团队共同开发产品的速度。，而不是优化单个开发者编写代码的速度。个人开发的速度很重要，它并不如整个团队的速度那么重要。 当代码审查很慢时，会发生","title":"Code Review 速度"},{"content":"This how-to document will show you how to configure routes to services exposing multiple ports through a single ServiceRoute config.\nScenario Consider a backend service named tcp-echo which exposes two ports, 9000 and 9001 over TCP. The service has two versions v1 and v2 and traffic splitting needs to be achieved between these two versions for both ports. In order to achieve this, a ServiceRoute with port level settings needs to be configured.\nDeploy the tcp-echo Service Deploy the tcp-echo …","relpermalink":"/tsb/howto/traffic/configure-multi-port-service-route/","summary":"This how-to document will show you how to configure routes to services exposing multiple ports through a single ServiceRoute config.\nScenario Consider a backend service named tcp-echo which exposes two ports, 9000 and 9001 over TCP. The service has two versions v1 and v2 and traffic splitting needs to be achieved between these two versions for both ports. In order to achieve this, a ServiceRoute with port level settings needs to be configured.\nDeploy the tcp-echo Service Deploy the tcp-echo application from the Istio’s samples directory into the echo namespace by installing these manifests.\nTSB configuration Deploy a Workspace and Traffic Group Apply the following configuration to create a Workspace and a Traffic Group.","title":"Configure ServiceRoute for (multi-port, multi-protocol) services"},{"content":"This document describes how to use a Tier-1 Gateway for multi cluster traffic shifting. You will create one cluster for a Tier-1 Gateway deployment and two clusters for running bookinfo applications.\nEach application cluster will have an Ingress Gateway configured to route traffic to the bookinfo application. Finally, you will configure the Tier-1 Gateway to shift the traffic from an application running on one cluster to another application on running on another cluster.\nBefore you get started, …","relpermalink":"/tsb/howto/gateway/multi-cluster-traffic-shifting/","summary":"This document describes how to use a Tier-1 Gateway for multi cluster traffic shifting. You will create one cluster for a Tier-1 Gateway deployment and two clusters for running bookinfo applications.\nEach application cluster will have an Ingress Gateway configured to route traffic to the bookinfo application. Finally, you will configure the Tier-1 Gateway to shift the traffic from an application running on one cluster to another application on running on another cluster.\nBefore you get started, make sure you: ✓ Familiarize yourself with TSB concepts ✓ Familiarize yourself with TSB management plane and cluster onboarding. Following scenarios will assume that you have already installed a TSB management plane and you have tctl configured to the correct management plane.","title":"Multi cluster traffic shifting with Tier-1 Gateway"},{"content":"TSB supports using external rate limiting servers. This document will describe how to configure Envoy rate limit service and use it as external rate limiting server in TSB’s Ingress Gateway through an example.\nBefore you get started, make sure you: ✓ Familiarize yourself with TSB concepts ✓ Install the TSB environment. You can use TSB demo for quick install\n✓ Completed TSB usage quickstart. This document assumes you already created Tenant and are familiar with Workspace and Config Groups. Also …","relpermalink":"/tsb/howto/rate-limiting/external-rate-limiting/","summary":"TSB supports using external rate limiting servers. This document will describe how to configure Envoy rate limit service and use it as external rate limiting server in TSB’s Ingress Gateway through an example.\nBefore you get started, make sure you: ✓ Familiarize yourself with TSB concepts ✓ Install the TSB environment. You can use TSB demo for quick install\n✓ Completed TSB usage quickstart. This document assumes you already created Tenant and are familiar with Workspace and Config Groups. Also you need to configure tctl to your TSB environment.\n:::note While this document will only describe how to apply rate limiting using an external server for Ingress Gateway, you can do the same for Tier-1 Gateways and service-to-service (through TSB Traffic Settings) using a similar configuration.","title":"Setting Up an External Rate Limiting Server"},{"content":"本文档解释了如何利用 Helm Chart来升级 TSB 的不同元素。本文假定 Helm 已经安装 在系统中。\n本文档仅适用于使用 Helm 创建的 TSB 实例，不适用于从基于 TCTL 的安装升级。\n在开始之前，请确保你已经：\n检查新版本的 要求 先决条件 已经 安装 Helm 已经 安装 TSB cli tctl 已经 安装 kubectl Tetrate 的镜像仓库的凭据 配置 Helm 仓库 添加仓库：\nhelm repo add tetrate-tsb-helm \u0026#39;https://charts.dl.tetrate.io/public/helm/charts/\u0026#39; helm repo update 列出可用版本：\nhelm search repo tetrate-tsb-helm -l 备份 PostgreSQL 数据库 创建 PostgreSQL 数据库的备份。\n根据你的环境，连接到数据库的确切过程可能会有所不同，请参考你环境的文档。\n升级过程 管理平面 升级管理平面Chart：\nhelm upgrade mp …","relpermalink":"/tsb/setup/helm/upgrade/","summary":"本文档解释了如何利用 Helm Chart来升级 TSB 的不同元素。本文假定 Helm 已经安装 在系统中。 本文档仅适用于使用 Helm 创建的 TSB 实例，不适用于从基于 TCTL 的安装升级。 在开始之前，请确保你已经： 检查新版本的 要求 先决条件 已经 安装","title":"TSB Helm 升级"},{"content":"本页将指导你如何使用 tctl CLI 升级 TSB，呈现不同Operator的 Kubernetes 清单，并使用 kubectl 将它们应用到集群中以进行升级。\n在开始之前，请确保你已完成以下操作：\n检查新版本的 要求 基于Operator的版本之间的升级过程相对简单。一旦Operator的 Pod 使用新的发布映像进行了更新，新生成的Operator Pod 将升级所有必要的组件以适应新版本。\n创建备份 为了确保在出现问题时可以恢复一切，请为管理平面和每个集群的本地控制平面创建备份。\n备份管理平面 备份 tctl 二进制文件 由于每个新的 tctl 二进制文件可能都附带了新的Operator和配置来部署和配置 TSB，因此你应该备份当前正在使用的 tctl 二进制文件。请在同步新映像之前执行此操作。\n将带有版本后缀的 tctl 二进制文件（例如 -1.3.0）复制到一个位置，以便在需要时快速还原旧版本。\ncp ~/.tctl/bin/tctl ~/.tctl/bin/tctl-{version} 如果你不小心丢失了二进制文件，你可以从 此网址 找到正确的版本。但强烈建议你备份当前 …","relpermalink":"/tsb/setup/self-managed/upgrade/","summary":"本页将指导你如何使用 tctl CLI 升级 TSB，呈现不同Operator的 Kubernetes 清单，并使用 kubectl 将它们应用到集群中以进行升级。 在开始之前，请确保你已完成以下操作： 检查新版本的 要求 基于Operator的版本之间的升级过程","title":"TSB 升级"},{"content":"为了配置 bookinfo 应用程序，你需要创建一个网关组、一个流量组和一个安全组。每个组都提供特定的 API 来配置服务的各个方面。\n先决条件 在继续阅读本指南之前，请确保你已完成以下步骤：\n熟悉 TSB 概念 安装TSB演示环境 部署 Istio Bookinfo 示例应用程序 创建租户 创建工作区 使用用户界面 在左侧面板的“租户”下，选择“工作区”。 单击 bookinfo-ws 工作区卡。 单击网关组按钮。 单击带有 + 图标的卡以添加新的网关组。 输入组 ID 作为 bookinfo-gw 。 为你的网关组提供显示名称和描述。 输入 */bookinfo 作为初始命名空间选择器。 将配置模式设置为 BRIDGED 。 单击添加。 从左侧面板中选择“工作区”返回到“工作区”。 对使用组 ID bookinfo-traffic 的流量组和使用组 ID bookinfo-security 的安全组重复相同的步骤。\n使用tctl 创建 groups.yaml 文件：\ngroups.yaml apiVersion: gateway.tsb.tetrate.io/v2 kind: …","relpermalink":"/tsb/quickstart/config-groups/","summary":"为了配置 bookinfo 应用程序，你需要创建一个网关组、一个流量组和一个安全组。每个组都提供特定的 API 来配置服务的各个方面。 先决条件 在继续阅读本指南之前，请确保你已完成以下步骤： 熟悉 TSB 概念 安装TSB演示环境 部署 Istio Bookinfo 示","title":"创建配置组"},{"content":"Tetrate Service Bridge (TSB) 提供强大的全局可观测性功能，可提供对整个服务网格基础设施的全面洞察。 TSB 简化了监控和了解服务运行状况和性能的过程，从而实现高效运营和故障排除。\n全局拓扑视图 TSB 的显着特征之一是它能够提供跨所有注册集群的服务网格拓扑的综合视图。这使组织能够掌握分布在各个可用区和区域的应用程序、服务和集群之间的复杂关系。全局拓扑视图允许全面了解应用程序如何在更大的基础设施环境中进行通信和交互。\n服务指标概述 TSB 提供以服务为中心的视角，使用户能够监控其应用程序的运行状况和性能，而不管底层部署详细信息或服务版本如何。此聚合视图简化了跨所有集群和区域评估应用程序整体运行状况的过程。\n此外，TSB 允许用户深入了解服务指标、单个集群甚至特定服务实例的特定方面。这种精细的可观察性使用户能够精确识别潜在问题和瓶颈，从而促进有效的故障排除和优化。\nEnvoy 指标分析 TSB 的全局可观察性还扩展到 Envoy，它是负责在服务网格内路由和管理流量的代理。用户可以访问与各个 Envoy 实例相关的详细指标，从而监控性能指标并深入了解网格内特定组件 …","relpermalink":"/tsb/concepts/observability/","summary":"Tetrate Service Bridge (TSB) 提供强大的全局可观测性功能，可提供对整个服务网格基础设施的全面洞察。 TSB 简化了监控和了解服务运行状况和性能的过程，从而实现高效运营和故障排除。 全局拓扑视图 TSB 的显着特征之一是它能够提供跨所有注册集","title":"全局可观测性"},{"content":"🔔 提示：本指南假设你对 Argo CD 所基于的工具有一定的了解。请阅读了解基础知识以了解这些工具。\n要求 安装 kubectl 命令行工具。 有一个 kubeconfig 文件（默认位置是~/.kube/config）。 CoreDNS。可以通过以下方式为 microk8s 启用microk8s enable dns \u0026amp;\u0026amp; microk8s stop \u0026amp;\u0026amp; microk8s start 1. 安装 Argo CD kubectl create namespace argocd kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml 这将创建一个新的命名空间，argocd，Argo CD 服务和应用程序资源将驻留在其中。\n🔔 警告：安装清单包括ClusterRoleBinding引用命名空间的资源argocd。如果你要将 Argo CD 安装到不同的命名空间中，请确保更新命名空间引用。\n如果你对 UI、SSO、多集群功能不感兴 …","relpermalink":"/argo-cd/getting-started/","summary":"🔔 提示：本指南假设你对 Argo CD 所基于的工具有一定的了解。请阅读了解基础知识以了解这些工具。 要求 安装 kubectl 命令行工具。 有一个 kubeconfig 文件（默认位置是~/.kube/config）。 CoreDNS。可以通过以下方式为 microk8s","title":"入门"},{"content":"本指南介绍了 Argo Rollouts 如何与Istio Service Mesh集成进行流量塑形。本指南建立在基本入门指南的概念基础上。\n要求 安装了 Istio 的 Kubernetes 集群 提示\n请参见 Istio 环境设置指南，了解如何在本地 minikube 环境中设置 Istio。\n1. 部署 Rollout、服务、Istio VirtualService 和 Istio Gateway 当使用 Istio 作为流量路由器时，Rollout 金丝雀策略必须定义以下强制字段：\napiVersion: argoproj.io/v1alpha1 kind: Rollout metadata: name: rollouts-demo spec: strategy: canary: # 引用控制器用于指向金丝雀副本集的 Service canaryService: rollouts-demo-canary # 引用控制器用于指向稳定副本集的 Service stableService: rollouts-demo-stable trafficRouting: istio: …","relpermalink":"/argo-rollouts/getting-started/istio/","summary":"本指南介绍了 Argo Rollouts 如何与Istio Service Mesh集成进行流量塑形。本指南建立在基本入门指南的概念基础上。 要求 安装了 Istio 的 Kubernetes 集群 提示 请参见 Istio 环境设置指南，了解如何在本地 minikube 环境中设置 Istio。 1. 部署 Rollou","title":"Istio"},{"content":"Istio 是一种服务网格，通过一组 CRD 提供了丰富的功能集，用于控制流向 Web 服务的流量。Istio 通过调整 Istio VirtualService 中定义的流量权重来实现此功能。使用 Argo Rollouts 时，用户可以部署包含至少一个 HTTP 路由 的 VirtualService，其中包含两个 HTTP 路由目标：一个路由目标针对金丝雀 ReplicaSet 的 pod，一个路由目标针对稳定 ReplicaSet 的 pod。Istio 提供了两种带权流量分割方法，这两种方法都可以作为 Argo Rollouts 的选项。\n主机级流量分割 子集级流量分割 主机级流量分割 使用 Argo Rollouts 和 Istio 进行流量分割的第一种方法是在两个主机名或 Kubernetes Service 之间进行分割：一个金丝雀 Service 和一个稳定 Service。这种方法类似于所有其他 Argo Rollouts mesh/ingress-controller 集成方式的工作方式（例如 ALB、SMI、Nginx）。使用此方法，用户需要部署以下资源： …","relpermalink":"/argo-rollouts/traffic-management/istio/","summary":"Istio 是一种服务网格，通过一组 CRD 提供了丰富的功能集，用于控制流向 Web 服务的流量。Istio 通过调整 Istio VirtualService 中定义的流量权重来实现此功能。使用 Argo Rollouts 时，用户可以部署包含至少一个 HTTP 路由 的 VirtualService，","title":"Istio"},{"content":"🔔 重要提示：自 v0.10.0 起可用于金丝雀发布。\n🔔 重要提示：自 v1.0 起可用于蓝绿发布。\n一个用例是，在 Rollout 标记或注释所需/稳定的 Pod 时，用用户定义的标签/注释，仅在它们是所需或稳定的集合期间，并且在 ReplicaSet 切换角色（例如从所需到稳定）时更新/删除标签。此举使 prometheus、wavefront、datadog 等查询和仪表板可以依赖一致的标签，而不是不可预测且从版本到版本不断变化的 rollouts-pod-template-hash。\n使用金丝雀策略的 Rollout 有能力使用 stableMetadata 和 canaryMetadata 字段将短暂元数据附加到稳定或金丝雀 Pod 上。\nspec: strategy: canary: stableMetadata: labels: role: stable canaryMetadata: labels: role: canary 使用蓝绿策略的 Rollout 有能力使用 activeMetadata 和 previewMetadata …","relpermalink":"/argo-rollouts/rollout/ephemeral-metadata/","summary":"🔔 重要提示：自 v0.10.0 起可用于金丝雀发布。 🔔 重要提示：自 v1.0 起可用于蓝绿发布。 一个用例是，在 Rollout 标记或注释所需/稳定的 Pod 时，用用户定义的标签/注释，仅在它们是所需或稳定的集合期间，并且在 ReplicaSet 切换角色（例如从所需到","title":"短暂元数据"},{"content":"在第三章介绍的概念基础上，本章说明了 SPIFFE 标准。解释 SPIRE 实现的组成部分以及它们是如何结合在一起的。最后，讨论威胁模型以及如果特定组件被破坏会发生什么。\n什么是 SPIFFE？ 普适安全生产身份框架（SPIFFE）是一套软件身份的开源标准。为了以一种与组织和平台无关的方式实现可互操作的软件身份，SPIFFE 定义了必要的接口和文件，以完全自动化的方式获得和验证加密身份。\n图 4.1：SPIFFE 组件。 SPIFFE ID，代表软件服务的名称（或身份）。 SPIFFE 可验证身份文件（SVID），这是一个可加密验证的文件，用于向对等者证明服务的身份。 SPIFFE Workload API，这是一个简单的节点本地 API，服务用它来获得身份，而不需要认证。 SPIFFE Trust Bundle（信任包），一种代表特定 SPIFFE 发行机构使用的公钥集合的格式。 SPIFFE Federation，这是一个简单的机制，通过它可以共享 SPIFFE Trust Bundle。 SPIFFE 不是什么 SPIFFE 旨在识别服务器、服务和其他通过计算机网络通信的非人类实 …","relpermalink":"/spiffe/introduction-to-spiffe-and-spire-concepts/","summary":"在第三章介绍的概念基础上，本章说明了 SPIFFE 标准。解释 SPIRE 实现的组成部分以及它们是如何结合在一起的。最后，讨论威胁模型以及如果特定组件被破坏会发生什么。 什么是 SPIFFE？ 普适安全生产身份框架（SPIFFE）","title":"SPIFFE 和 SPIRE 概念介绍"},{"content":"本节介绍 Kubernetes 的网络策略方面。\n命名空间 命名空间 用于在 Kubernetes 中创建虚拟集群。包括 NetworkPolicy 和 CiliumNetworkPolicy 在内的所有 Kubernetes 对象都属于一个特定的命名空间。根据定义和创建策略的方式，会自动考虑 Kubernetes 命名空间：\n作为 CiliumNetworkPolicy CRD 和 NetworkPolicy 创建和导入的网络策略适用于命名空间内，即该策略仅适用于该命名空间内的 pod。但是，可以授予对其他命名空间中的 pod 的访问权限，如下所述。 通过 API 参考 直接导入的网络策略适用于所有命名空间，除非如下所述指定命名空间选择器。 提示\n虽然有意支持通过 fromEndpoints 和 toEndpoints 中的 k8s:io.kubernetes.pod.namespace 标签指定命名空间。禁止在 endpointSelector 中指定命名空间，因为这将违反 Kubernetes 的命名空间隔离原则。endpointSelector …","relpermalink":"/cilium-handbook/policy/kubernetes/","summary":"本节介绍 Kubernetes 的网络策略方面。 命名空间 命名空间 用于在 Kubernetes 中创建虚拟集群。包括 NetworkPolicy 和 CiliumNetworkPolicy 在内的所有 Kubernetes 对象都属于一个特定的命名空间。根据定义和创建策略的方式，会自动考虑 Kubernetes 命名空间： 作为 CiliumNetworkPolicy CRD 和 NetworkPolicy 创建和导入的网络策略适","title":"Kubernetes 网络策略"},{"content":"如果你在 Kubernetes 上运行 Cilium，你可以从 Kubernetes 为你分发的策略中受益。在这种模式下，Kubernetes 负责在所有节点上分发策略，Cilium 会自动应用这些策略。三种格式可用于使用 Kubernetes 本地配置网络策略：\n在撰写本文时，标准的 NetworkPolicy 资源支持指定三层/四层入口策略，并带有标记为 beta 的有限出口支持。 扩展的 CiliumNetworkPolicy 格式可用作 CustomResourceDefinition 支持入口和出口的三层到七层层策略规范。 CiliumClusterwideNetworkPolicy 格式，它是集群范围的CustomResourceDefinition，用于指定由 Cilium 强制执行的集群范围的策略。规范与 CiliumNetworkPolicy 相同，没有指定命名空间。 Cilium 支持同时运行多个策略类型。但是，在同时使用多种策略类型时应谨慎，因为理解跨多种策略类型的完整允许流量集可能会令人困惑。如果不注意，这可能会导致意外的策略允许行为。\n网络策略 有关详细信息， …","relpermalink":"/cilium-handbook/kubernetes/policy/","summary":"如果你在 Kubernetes 上运行 Cilium，你可以从 Kubernetes 为你分发的策略中受益。在这种模式下，Kubernetes 负责在所有节点上分发策略，Cilium 会自动应用这些策略。三种格式可用于使用 Kubernetes 本地配置网络策略： 在撰写本","title":"网络策略"},{"content":"近年来，云原生应用已呈指数级增长。在本章中，我将讨论为什么 eBPF 如此适合于云原生环境。为了更具象化，我将提到 Kubernetes，但同样适用于任何容器平台。\n每台主机一个内核 要理解为什么 eBPF 在云原生世界中如此强大，你需要搞清楚一个概念：每台机器（或虚拟机）只有一个内核，所有运行在该机器上的容器都共享同一个内核 1 如 图 5-1 所示，内核了解主机上运行的所有应用代码。\n图 5-1. 同一主机上的所有容器共享一个内核 通过对内核的检测，就像我们在使用 eBPF 时做的那样，我们可以同时检测在该机器上运行的所有应用程序代码。当我们将 eBPF 程序加载到内核并将其附加到事件上时，它就会被触发，而不考虑哪个进程与该事件有关。\neBPF 与 sidecar 模式的比较 在 eBPF 之前，Kubernetes 的可观测性和安全工具大多都采用了 sidecar 模式。这种模式允许你在与应用程序相同的 pod 中，单独部署一个工具容器。这种模式的发明是一个进步，因为这意味着不再需要直接在应用程序中编写工具代码。仅仅通过部署 sidecar，工具就获得了同一 pod 中的其他容器 …","relpermalink":"/what-is-ebpf/ebpf-in-cloud-native-environments/","summary":"近年来，云原生应用已呈指数级增长。在本章中，我将讨论为什么 eBPF 如此适合于云原生环境。为了更具象化，我将提到 Kubernetes，但同样适用于任何容器平台。 每台主机一个内核 要理解为什么 eBPF 在云原生世界中如此","title":"第五章：云原生环境中的 eBPF"},{"content":"Etcd 是 Kubernetes 集群中的一个十分重要的组件，用于保存集群所有的网络配置和对象的状态信息。在后面具体的安装环境中，我们安装的 etcd 的版本是 v3.1.5，整个 Kubernetes 系统中一共有两个服务需要用到 etcd 用来协同和存储配置，分别是：\n网络插件 flannel、对于其它网络插件也需要用到 etcd 存储网络的配置信息 Kubernetes 本身，包括各种对象的状态和元信息配置 注意：flannel 操作 etcd 使用的是 v2 的 API，而 Kubernetes 操作 etcd 使用的 v3 的 API，所以在下面我们执行 etcdctl 的时候需要设置 ETCDCTL_API 环境变量，该变量默认值为 2。\n原理 Etcd 使用的是 raft 一致性算法来实现的，是一款分布式的一致性 KV 存储，主要用于共享配置和服务发现。关于 raft 一致性算法请参考 该动画演示。\n关于 Etcd 的原理解析请参考 Etcd 架构与实现解析。\n使用 Etcd 存储 Flannel 网络信息 我们在安装 Flannel …","relpermalink":"/kubernetes-handbook/architecture/etcd/","summary":"Etcd 是 Kubernetes 集群中的一个十分重要的组件，用于保存集群所有的网络配置和对象的状态信息。在后面具体的安装环境中，我们安装的 etcd 的版本是 v3.1.5，整个 Kubernetes 系统中一共有两个服务需要用到 etcd 用来协同和存储配置，分别是：","title":"Etcd 解析"},{"content":"在联邦政府的各个机构中，有几个 DevSecOps 倡议，其重点和焦点各不相同，这取决于软件和任务需求所带来的流程。尽管并不详尽，但以下是对 这些倡议 的简要概述：\nDevSecOps 管道参与构建、签入和签出一个名为 Iron Bank 的容器镜像仓库，这是一个经过国防部审查的强化容器镜像库。 空军的 Platform One，也就是实现了连续操作授权（C-ATO）概念的 DevSecOps 平台，这又简化了国防部的授权程序，以适应现代连续软件部署的速度和频率。 国家地理空间情报局（NGA）在 \u0026#34;NGA 软件之路\u0026#34; 中概述了其 DevSecOps 战略，其中为其每个软件产品规定了三个关键指标：可用性、准备时间和部署频率，以及用于实现 DevSecOps 管道的七个不同产品系列的规格，包括消息传递和工作流工具。 医疗保险和医疗补助服务中心（CMS）正在采用一种 DevSecOps 方法，其中一个重点是为软件材料清单（SBOM）奠定基 —— 这是一种正式记录，包含用于构建软件的各种组件的细节和供应链关系。制作 SBOM 的目的是为了实现持续诊断和缓解（CDM）计划下的目标。 在海军水面作 …","relpermalink":"/service-mesh-devsecops/intro/related-devsecops-initiatives/","summary":"在联邦政府的各个机构中，有几个 DevSecOps 倡议，其重点和焦点各不相同，这取决于软件和任务需求所带来的流程。尽管并不详尽，但以下是对 这些倡议 的简要概述： DevSecOps 管道参与构建、签入和签出一个名为 Iron Bank 的容器镜像仓库，这是一","title":"1.2 相关的 DevSecOps 倡议"},{"content":"可观测性即代码在应用程序的每个服务组件中部署一个监控代理，以收集三种类型的数据（在第 4.1 节中描述），将它们发送到专门的工具，将它们关联起来，进行分析，并在仪表板上显示分析后的综合数据，以呈现整个应用程序级别的情况。这种综合数据的一个例子是日志模式，它提供了一个日志数据的视图，该视图是在使用一些标准（例如，一个服务或一个事件）对日志数据进行过滤后呈现的。数据根据共同的模式（例如，基于时间戳或 IP 地址范围）被分组，以方便解释。不寻常的发生被识别出来，然后这些发现可以被用来指导和加速 进一步的调查。\n","relpermalink":"/service-mesh-devsecops/implement/ci-cd-pipeline-for-observability-as-code/","summary":"可观测性即代码在应用程序的每个服务组件中部署一个监控代理，以收集三种类型的数据（在第 4.1 节中描述），将它们发送到专门的工具，将它们关联起来，进行分析，并在仪表板上显示分析后的综合数据，以呈现整个应用程序","title":"4.5 可观测性即代码的 CI/CD 管道"},{"content":"我们在前一章中讨论了在采用云原生基础架构的前提。在部署之前，需要有由 API 驱动的基础架构（IaaS）供给。\n在本章中，我们将探讨云原生基础架构拓扑的概念，并在云中实现它们。我们将学习可以帮助运维人员控制其基础架构的常用工具和模式。\n部署基础架构的第一步应该是能够将其表述出来。传统上，可以在白板上处理，或者如果幸运的话，可以在公司 wiki 上存储的文档中处理。今天，一切都变得更加程序化，基础架构表述通常以便于应用程序解释的方式记录。无论如何表述，全面的表述基础架构的需求是不变的。\n正如人们所期望的那样，精巧的云基础架构可以从简单的设计到非常复杂的设计。无论复杂性如何，必须对基础架构的表现给予高度的重视，以确保设计的可重复性。能够清晰地传递想法更为重要。因此，明确、准确和易于理解的基础架构级资源表述势在必行。\n我们也将从制作精良的表述中获得很多好处：\n随着时间的推移，基础架构设计可以共享和版本化。 基础架构设计可以被 fork 和修改以适应特殊情况。 表述隐含的是文档。 随着本章向前推进，我们将看到基础架构表述是如何成为基础架构部署的第一步。我们将以不同的方式探索表述基础架构的能力和 …","relpermalink":"/cloud-native-infra/evolution-of-cloud-native-developments/","summary":"我们在前一章中讨论了在采用云原生基础架构的前提。在部署之前，需要有由 API 驱动的基础架构（IaaS）供给。 在本章中，我们将探讨云原生基础架构拓扑的概念，并在云中实现它们。我们将学习可以帮助运维人员控制其基","title":"第 3 章：云原生部署的演变"},{"content":"Kubernetes，经常被缩写为 “K8s”，是一个开源的容器或编排系统，用于自动部署、扩展和管理容器化应用程序。它管理着构成集群的所有元素，从应用中的每个微服务到整个集群。与单体软件平台相比，将容器化应用作为微服务使用可以提供更多的灵活性和安全优势，但也可能引入其他复杂因素。\n图 1：Kubernetes 集群组件的高层视图 本指南重点关注安全挑战，并尽可能提出适用于国家安全系统和关键基础设施管理员的加固策略。尽管本指南是针对国家安全系统和关键基础设施组织的，但也鼓励联邦和州、地方、部落和领土（SLTT）政府网络的管理员实施所提供的建议。Kubernetes 集群的安全问题可能很复杂，而且经常在利用其错误配置的潜在威胁中被滥用。以下指南提供了具体的安全配置，可以帮助建立更安全的 Kubernetes 集群。\n建议 每个部分的主要建议摘要如下：\nKubernetes Pod 安全 使用构建的容器，以非 root 用户身份运行应用程序 在可能的情况下，用不可变的文件系统运行容器 扫描容器镜像，以发现可能存在的漏洞或错误配置 使用 Pod 安全政策来执行最低水平的安全，包括： 防止有特权 …","relpermalink":"/kubernetes-hardening-guidance/introduction/","summary":"Kubernetes，经常被缩写为 “K8s”，是一个开源的容器或编排系统，用于自动部署、扩展和管理容器化应用程序。它管理着构成集群的所有元素，从应用中的每个微服务到整个集群。与","title":"简介"},{"content":"到目前为止，我们已经从数据的角度讨论了现代可观测性。但是，现代可观测性还有另一个方面，从长远来看，它可能被证明同样重要：如何对待产生数据的仪表（instrumention）。\n大多数软件系统都是用现成的部件构建的：网络框架、数据库、HTTP 客户端、代理服务器、编程语言。在大多数组织中，很少有这种软件基础设施是在内部编写的。相反，这些组件是在许多组织中共享的。最常见的是，这些共享组件是以开放源码（OSS）库的形式出现的，并具有许可权。\n由于这些开放源码软件库几乎囊括了一般系统中的所有关键功能，因此获得这些库的高质量说明对大多数可观测性系统来说至关重要。\n传统上，仪表是 “单独出售” 的。这意味着，软件库不包括产生追踪、日志或度量的仪表。相反，特定解决方案的仪表是在事后添加的，作为部署可观测系统的一部分。\n什么是特定解决方案仪表？\n在本章中，术语 “特定解决方案仪表” 是指任何旨在与特定的可观测系统一起工作的仪表，使用的是作为该特定系统的数据存储系统的产物而开发的客户端。在这些系统中，客户端和存储系统常常深深地融合在一起。因此，如果一个应用要从一个观测系统切换到另一个观测系统，通常需要进 …","relpermalink":"/opentelemetry-obervability/supporting-open-source-and-native-instrumentation/","summary":"第 4 章：支持开源和原生监测","title":"第 4 章：支持开源和原生监测"},{"content":"在这一节中，我们将解释 Envoy 的基本构建模块。\nEnvoy 配置的根被称为引导配置。它包含了一些字段，我们可以在这里提供静态或动态的资源和高级别的 Envoy 配置（例如，Envoy 实例名称、运行时配置、启用管理界面等等）。\n为了开始学习，我们将主要关注静态资源，在课程的后面，我们将介绍如何配置动态资源。\nEnvoy 输出许多统计数据，这取决于启用的组件和它们的配置。我们会在整个课程中提到不同的统计信息，在课程后面的专门模块中，我们会更多地讨论统计信息。\n下图显示了通过这些概念的请求流。\nEnvoy 构建块 这一切都从监听器开始。Envoy 暴露的监听器是命名的网络位置，可以是一个 IP 地址和一个端口，也可以是一个 Unix 域套接字路径。Envoy 通过监听器接收连接和请求。考虑一下下面的 Envoy 配置。\nstatic_resources: listeners: - name: listener_0 address: socket_address: address: 0.0.0.0 port_value: 10000 filter_chains: [{}] …","relpermalink":"/cloud-native-handbook/service-mesh/envoy-building-blocks/","summary":"在这一节中，我们将解释 Envoy 的基本构建模块。 Envoy 配置的根被称为引导配置。它包含了一些字段，我们可以在这里提供静态或动态的资源和高级别的 Envoy 配置（例如，Envoy 实例名称、运行时配置、启用管理界面等等）。 为了开","title":"Envoy 的构建模块"},{"content":"在使用 ApplicationSet 之前，了解其安全性影响非常重要。\n只有管理员可以创建/更新/删除 ApplicationSet ApplicationSet 可以在任意 Project 下创建应用程序。Argo CD 设置通常包括高权限的 Project（例如 default），往往包括管理 Argo CD 自身资源的能力（例如 RBAC ConfigMap）。\nApplicationSets 还可以快速创建任意数量的应用程序，并同样快速删除它们。\n最后，ApplicationSets 可以显示特权信息。例如，git generator 可以读取 Argo CD 命名空间中的 Secrets，并将其作为 Auth 标头发送到任意 URL（例如为 api 字段提供的 URL）。 （此功能旨在为 SCM 提供程序（如 GitHub）授权请求，但可能会被恶意用户滥用。）\n出于这些原因，只有管理员可以通过 Kubernetes RBAC 或任何其他机制获得创建、更新或删除 ApplicationSets 的权限。\n管理员必须为 ApplicationSets 的真实来源应用适当的控制 即 …","relpermalink":"/argo-cd/operator-manual/applicationset/security/","summary":"在使用 ApplicationSet 之前，了解其安全性影响非常重要。 只有管理员可以创建/更新/删除 ApplicationSet ApplicationSet 可以在任意 Project 下创建应用程序。Argo CD 设置通常包括高权限的 Project（例如 default），往往包括管理 Argo CD 自身资源的能力","title":"ApplicationSet 安全性"},{"content":"总结 保持友善。 解释你的推理。 在给出明确的指示与只指出问题并让开发人员自己决定间做好平衡。 鼓励开发人员简化代码或添加代码注释，而不仅仅是向你解释复杂性。 礼貌 一般而言，对于那些正在被您审查代码的人，除了保持有礼貌且尊重以外，重要的是还要确保您（的评论）是非常清楚且有帮助的。你并不总是必须遵循这种做法，但在说出可能令人不安或有争议的事情时你绝对应该使用它。例如：\n糟糕的示例：“为什么这里你使用了线程，显然并发并没有带来什么好处？”\n好的示例：“这里的并发模型增加了系统的复杂性，但没有任何实际的性能优势，因为没有性能优势，最好是将这些代码作为单线程处理而不是使用多线程。”\n解释为什么 关于上面的“好”示例，您会注意到的一件事是，它可以帮助开发人员理解您发表评论的原因。并不总是需要您在审查评论中包含此信息，但有时候提供更多解释，对于表明您的意图，您在遵循的最佳实践，或为您建议如何提高代码健康状况是十分恰当的。\n给予指导 一般来说，修复 CL 是开发人员的责任，而不是审查者。 您无需为开发人员详细设计解决方案或编写代码。\n但这并不意味着审查者应该没有帮助。一般来说，您应该在指出问题和提 …","relpermalink":"/eng-practices/review/reviewer/comments/","summary":"总结 保持友善。 解释你的推理。 在给出明确的指示与只指出问题并让开发人员自己决定间做好平衡。 鼓励开发人员简化代码或添加代码注释，而不仅仅是向你解释复杂性。 礼貌 一般而言，对于那些正在被您审查代码的人，除了保","title":"如何撰写 Code Review 评论"},{"content":"This how-to document will show you how to do a canary release of a new sample service. You’ll learn how to deploy and onboard a service in TSB, and how to adjust its settings to follow the canary deployment process.\n✓ You’ll create a workspace and the groups you’ll need to onboard the application\n✓ Expose the application via an application ingress gateway\n✓ Perform the canary release.\nBefore you get started make sure:\n✓ You have a TSB management plane up and running.\n✓ You have tctl configured …","relpermalink":"/tsb/howto/traffic/canary-releases/","summary":"This how-to document will show you how to do a canary release of a new sample service. You’ll learn how to deploy and onboard a service in TSB, and how to adjust its settings to follow the canary deployment process.\n✓ You’ll create a workspace and the groups you’ll need to onboard the application\n✓ Expose the application via an application ingress gateway\n✓ Perform the canary release.\nBefore you get started make sure:\n✓ You have a TSB management plane up and running.\n✓ You have tctl configured to communicate with the TSB management plane.\n✓ The cluster where you are deploying the application to is running a TSB control plane and is correctly onboarded into the TSB management plane.","title":"Canary Releases"},{"content":"In this how-to guide, you’ll add user authentication and authorization at an Ingress Gateway using Keycloak as the Identity Provider.\nBefore you get started, make sure you’ve\n✓ Installed the TSB management plane\n✓ Onboarded a cluster\n✓ Installed Keycloak with HTTPS enabled\n:::note This example will use a demo of the httpbin application that’s been tested on GKE. If you intend to follow these steps for production use, make sure you update the application information in the relevant fields with …","relpermalink":"/tsb/howto/gateway/end-user-auth-keycloak/","summary":"In this how-to guide, you’ll add user authentication and authorization at an Ingress Gateway using Keycloak as the Identity Provider.\nBefore you get started, make sure you’ve\n✓ Installed the TSB management plane\n✓ Onboarded a cluster\n✓ Installed Keycloak with HTTPS enabled\n:::note This example will use a demo of the httpbin application that’s been tested on GKE. If you intend to follow these steps for production use, make sure you update the application information in the relevant fields with your information. :::\nIn this guide, you’ll:\n✓ Add authentication and authorization to an Ingress Gateway for a demo httpbin application.","title":"End User Authentication with Keycloak"},{"content":"Once you have configured an external rate limit server, you may want to secure the traffic to the rate limit service. TSB supports specifying TLS or mTLS parameters for securing communication to external rate limit servers. This document will show you how to configure TLS validation for an external rate limit server by adding CA certificate to the rate limiting configuration.\nBefore you get started, make sure you: ✓ Familiarize yourself with TSB concepts ✓ Install the TSB environment. You can …","relpermalink":"/tsb/howto/rate-limiting/tls-validation/","summary":"Once you have configured an external rate limit server, you may want to secure the traffic to the rate limit service. TSB supports specifying TLS or mTLS parameters for securing communication to external rate limit servers. This document will show you how to configure TLS validation for an external rate limit server by adding CA certificate to the rate limiting configuration.\nBefore you get started, make sure you: ✓ Familiarize yourself with TSB concepts ✓ Install the TSB environment. You can use TSB demo for quick install\n✓ Completed TSB usage quickstart. This document assumes you already created Tenant and are familiar with Workspace and Config Groups.","title":"External Rate Limiting with TLS Verification"},{"content":"要卸载使用 Helm 安装的 TSB，你可以使用 helm uninstall 来卸载一个发布。卸载必须按照以下顺序进行：\n数据平面 控制平面 管理平面 数据平面卸载 helm uninstall dp tetrate-tsb-helm/dataplane --namespace istio-gateway 一旦 Helm 删除了与数据平面Chart的最后一个发布关联的所有资源，你将需要手动删除一些在卸载过程中创建的资源，这些资源不受 Helm 跟踪。\nkubectl delete serviceaccount tsb-helm-delete-hook --ignore-not-found kubectl delete clusterrole tsb-helm-delete-hook --ignore-not-found kubectl delete clusterrolebinding tsb-helm-delete-hook --ignore-not-found kubectl delete istiooperators.install.istio.io --all -n …","relpermalink":"/tsb/setup/helm/uninstallation/","summary":"要卸载使用 Helm 安装的 TSB，你可以使用 helm uninstall 来卸载一个发布。卸载必须按照以下顺序进行： 数据平面 控制平面 管理平面 数据平面卸载 helm uninstall dp tetrate-tsb-helm/dataplane --namespace istio-gateway 一旦 Helm 删除了与数据平面Chart的最后一个发布关联的所有资源，你将需要","title":"Helm TSB 卸载"},{"content":"Tetrate Service Bridge (TSB) 采用结构化数据流机制来确保配置更改和更新在整个服务网格基础设施中高效、准确地传播。这个复杂的过程涉及各种组件，包括管理平面、全局控制平面（XCP Central）和本地控制平面（XCP Edge），每个组件在配置生命周期中都发挥着关键作用。\nSimplified data flow from user input through TSB, XCP, to local control planes. 管理平面 TSB 中的所有配置更改均源自管理平面。用户通过各种接口与 TSB 配置交互，例如 gRPC API、TSB UI 和 tctl 命令行界面。配置更改随后会保存在数据库中，作为整个系统的事实来源。管理平面将这些更改推送到 XCP Central 以便进一步分发。\nMPC 组件\n由于遗留原因，XCP Central 通过 Kubernetes CRD 接收其配置。名为“MPC”的 shim 服务器建立到 TSB 的 API 服务器的 gRPC 流，以接收配置并将相应的 CR 推送到托管 XCP Central …","relpermalink":"/tsb/concepts/configuration-dataflow/","summary":"Tetrate Service Bridge (TSB) 采用结构化数据流机制来确保配置更改和更新在整个服务网格基础设施中高效、准确地传播。这个复杂的过程涉及各种组件，包括管理平面、全局控制平面（XCP Central）和本地控制平面（XCP Edge）","title":"配置的数据流"},{"content":"在本部分中，你将了解如何使用 TSB 中的 AccessBindings 配置访问策略来管理不同团队和用户的权限。\n先决条件 在继续之前，请确保你已完成以下任务：\n熟悉 TSB 概念。 安装 TSB 演示环境。 部署 Istio Bookinfo 示例应用程序。 创建租户和工作区。 创建配置组。 授予团队对工作区的完全访问权限 你将配置一个访问策略，授予团队对工作区的完全访问权限。团队成员将能够创建并完全管理工作区中的资源，但无法修改工作区对象本身。这将通过分配 Creator 角色来实现。\n在左侧面板的“租户”下，选择“工作区”。 单击所需的工作区以访问其详细信息页面。 单击权限选项卡。 选择“按团队”选项可查看团队列表。 找到并单击所需团队右侧的编辑图标。 选择 Creator 角色。 单击右下角的保存更改按钮。 向组的用户授予写入权限 要向组的特定用户授予写入权限，请遵循类似的过程：\n导航到组的权限选项卡。 选择“按用户”选项可查看用户列表。 找到并单击所需用户旁边的编辑图标。 选择 Writer 角色。 单击右下角的保存更改按钮。 使用tctl 你还可以通过应用 YAML 文件 …","relpermalink":"/tsb/quickstart/permissions/","summary":"在本部分中，你将了解如何使用 TSB 中的 AccessBindings 配置访问策略来管理不同团队和用户的权限。 先决条件 在继续之前，请确保你已完成以下任务： 熟悉 TSB 概念。 安装 TSB 演示环境。 部署 Istio Bookinfo 示例应用程序。 创建租户和工作区。 创建配置组。","title":"配置权限"},{"content":"Argo Rollouts Kubectl 插件可以提供本地 UI 仪表板来可视化你的 Rollouts。\n要启动它，请在包含你的 Rollouts 的命名空间中运行 kubectl argo rollouts dashboard 。然后访问 localhost:3100 查看用户界面。\n列表视图 Rollouts 列表视图 单独的 Rollout 视图 Rollouts 视图 ","relpermalink":"/argo-rollouts/dashboard/","summary":"Argo Rollouts Kubectl 插件可以提供本地 UI 仪表板来可视化你的 Rollouts。 要启动它，请在包含你的 Rollouts 的命名空间中运行 kubectl argo rollouts dashboard 。然后访问 localhost:3100 查看用户界面。 列表视图 Rollouts 列表视图 单独的 Rollout 视图 Rollouts 视图","title":"UI Dashboard"},{"content":"本指南介绍了 Argo Rollouts 如何与 NGINX Ingress Controller 集成进行流量整形。本指南基于 基本入门指南 的概念。\n要求 安装了 NGINX Ingress 控制器的 Kubernetes 集群 🔔 提示：请参阅 NGINX 环境设置指南 以了解如何使用 nginx 设置本地 minikube 环境。\n1. 部署 Rollout、服务和 Ingress 当使用 NGINX Ingress 作为流量路由器时，Rollout 金丝雀策略必须定义以下强制字段：\napiVersion: argoproj.io/v1alpha1 kind: Rollout metadata: name: rollouts-demo spec: strategy: canary: # 引用控制器将更新并指向金丝雀 ReplicaSet 的服务 canaryService: rollouts-demo-canary # 引用控制器将更新并指向稳定 ReplicaSet 的服务 stableService: rollouts-demo-stable trafficRouting: …","relpermalink":"/argo-rollouts/getting-started/nginx/","summary":"本指南介绍了 Argo Rollouts 如何与 NGINX Ingress Controller 集成进行流量整形。本指南基于 基本入门指南 的概念。 要求 安装了 NGINX Ingress 控制器的 Kubernetes 集群 🔔 提示：请参阅 NGINX 环境设置指南 以了解如何使用 nginx 设置本地 minikube 环境。 1. 部署 Rollout、服务和 Ingress 当使用","title":"Nginx Ingress 快速开始"},{"content":"Nginx Ingress Controller 允许通过一个或多个 Ingress 对象进行流量管理，以配置直接将流量路由到 Pod 的 Nginx 部署。每个 Nginx Ingress 都包含多个注释，可以修改 Nginx 部署的行为。对于应用程序不同版本之间的流量管理，Nginx Ingress 控制器提供了通过引入第二个 Ingress 对象（称为金丝雀 Ingress）进行流量拆分的功能。你可以在官方的 金丝雀注释文档页面 上阅读更多关于这些金丝雀注释的信息。金丝雀 Ingress 忽略任何其他非金丝雀 nginx 注释。取而代之，它利用来自主要 Ingress 的注释设置。\nRollout 控制器始终会在金丝雀 Ingress 上设置以下两个注释（使用你配置的或默认的 nginx.ingress.kubernetes.io 前缀）：\ncanary: true 表示这是金丝雀 Ingress canary-weight: \u0026lt;num\u0026gt; 表示将发送到金丝雀的流量百分比。如果所有流量都路由到稳定服务，则设置为 0 …","relpermalink":"/argo-rollouts/traffic-management/nginx/","summary":"Nginx Ingress Controller 允许通过一个或多个 Ingress 对象进行流量管理，以配置直接将流量路由到 Pod 的 Nginx 部署。每个 Nginx Ingress 都包含多个注释，可以修改 Nginx 部署的行为。对于应用程序不同版本之间的流量管理，Nginx Ingress 控制器提供了通过引入第二个 Ingress","title":"Nginx"},{"content":"出于各种原因，应用程序通常需要重新启动，例如出于健康目的或强制执行启动逻辑，例如重新加载修改的 Secret。在这些情况下，不希望进行整个蓝绿或金丝雀更新过程。Argo Rollouts 支持通过执行所有 Rollout 中的 Pod 的滚动重建来重启其所有 Pod 的能力，同时跳过常规的 BlueGreen 或 Canary 更新策略。\n它是如何工作的 可以通过 kubectl 插件使用 kubectl-argo-rollouts restart 命令来重新启动 Rollout：\nkubectl-argo-rollouts restart ROLLOUT 或者，如果 Rollouts 与 Argo CD 一起使用，则可以通过 Argo CD UI 或 CLI 执行捆绑的“restart”操作：\nargocd app actions run my-app restart --kind Rollout --resource-name my-rollout 这两种机制都会将 Rollout 的.spec.restartAt更新为以RFC 3339 格式的 UTC 字符串的当前时间形式（ …","relpermalink":"/argo-rollouts/rollout/restart/","summary":"出于各种原因，应用程序通常需要重新启动，例如出于健康目的或强制执行启动逻辑，例如重新加载修改的 Secret。在这些情况下，不希望进行整个蓝绿或金丝雀更新过程。Argo Rollouts 支持通过执行所有 Rollout 中的 Pod 的滚动重","title":"重启 Rollouts Pod"},{"content":"本章旨在让你为上线 SPIFFE/SPIRE 时需要做出的许多决定做好准备。\n准备人力 如果你读了前面的章节，你一定很想开始使用 SPIRE，以一种可以在许多不同类型的系统和所有组织的服务中利用的方式管理身份。然而，在你开始之前，你需要考虑，部署 SPIRE 是一个重大的基础设施变化，有可能影响到许多不同的系统。本章是关于如何开始规划 SPIRE 的部署：获得认同，以不中断的方式启用 SPIRE 支持，然后利用它来实施新的安全控制。\n组建团队并确定其他利益相关者 要部署 SPIRE，你需要确定来自安全、软件开发和 DevOps 团队的利益相关者。谁来维护 SPIRE 服务器本身？谁来部署代理？谁来编写注册条目？谁将把 SPIFFE 功能集成到应用程序中？它将如何影响现有的 CI/CD 管道？如果发生了服务中断，谁来修复它？性能要求和服务水平目标是什么？\n在这本书中，以及许多公开的博客文章和会议演讲中，都有一些成功部署 SPIRE 的组织的例子，既可以作为一种模式，也可以作为向同事宣传 SPIRE 的有用材料。\n说明你的情况并获得支持 SPIRE 跨越了几个不同的传统信息技术孤岛，因此， …","relpermalink":"/spiffe/before-you-start/","summary":"本章旨在让你为上线 SPIFFE/SPIRE 时需要做出的许多决定做好准备。 准备人力 如果你读了前面的章节，你一定很想开始使用 SPIRE，以一种可以在许多不同类型的系统和所有组织的服务中利用的方式管理身份。然而，在你开始之前，你","title":"开始前的准备"},{"content":"在 Kubernetes 中管理 pod 时，Cilium 将创建一个 CiliumEndpoint 的自定义资源定义（CRD）。每个由 Cilium 管理的 pod 都会创建一个 CiliumEndpoint，名称相同且在同一命名空间。CiliumEndpoint 对象包含的信息与 cilium endpoint get 在.status 字段下的 json 输出相同，但可以为集群中的所有 pod 获取。添加 -o json 将导出每个端点的更多信息。这包括端点的标签、安全身份和对其有效的策略。\n例如：\n$ kubectl get ciliumendpoints --all-namespaces NAMESPACE NAME AGE default app1-55d7944bdd-l7c8j 1h default app1-55d7944bdd-sn9xj 1h default app2 1h default app3 1h kube-system cilium-health-minikube 1h kube-system microscope 1h …","relpermalink":"/cilium-handbook/kubernetes/ciliumendpoint/","summary":"在 Kubernetes 中管理 pod 时，Cilium 将创建一个 CiliumEndpoint 的自定义资源定义（CRD）。每个由 Cilium 管理的 pod 都会创建一个 CiliumEndpoint，名称相同且在同一命名空间。CiliumEndpoint 对象包含的信息与 cilium endpoint","title":"端点 CRD"},{"content":"现在你已经了解了什么是 eBPF 以及它是如何工作的，我们再探索一些可能会在生产部署中使用的基于 eBPF 技术的工具。我们将举一些基于 eBPF 的开源项目的例子，这些项目提供了三方面的能力：网络、可观测性和安全。\n网络 eBPF 程序可以连接到网络接口和内核的网络堆栈的各个点。在每个点上，eBPF 程序可以丢弃数据包，将其发送到不同的目的地，甚至修改其内容。这就实现了一些非常强大的功能。让我们来看看通常用 eBPF 实现的几个网络功能。\n负载均衡 Facebook 正在大规模的使用 eBPF 的网络功能，因此你不必对 eBPF 用于网络的可扩展性有任何怀疑。他们是 BPF 的早期采用者，并在 2018 年推出了 Katran，一个开源的四层负载均衡器。\n另一个高度扩展的负载均衡器的例子是来自 Cloudflare 的 Unimog 边缘负载均衡器。通过在内核中运行，eBPF 程序可以操作网络数据包，并将其转发到适当的目的地，而不需要数据包通过网络堆栈和用户空间。\nCilium 项目作为一个 eBPF Kubernetes 网络插件更为人所知（我一会儿会讨论），但作为独立的负载均衡 …","relpermalink":"/what-is-ebpf/ebpf-tools/","summary":"现在你已经了解了什么是 eBPF 以及它是如何工作的，我们再探索一些可能会在生产部署中使用的基于 eBPF 技术的工具。我们将举一些基于 eBPF 的开源项目的例子，这些项目提供了三方面的能力：网络、可观测性和安全。 网络 eBPF 程序可以","title":"第六章：eBPF 工具"},{"content":"由于 DevSecOps 的基本要素跨越了开发（安全的构建和测试、打包）、交付 / 部署和持续监控（以确保运行期间的安全状态），本文建议的目标受众包括软件开发、运维和安全团队。\n","relpermalink":"/service-mesh-devsecops/intro/target-audience/","summary":"由于 DevSecOps 的基本要素跨越了开发（安全的构建和测试、打包）、交付 / 部署和持续监控（以确保运行期间的安全状态），本文建议的目标受众包括软件开发、运维和安全团队。","title":"1.3 目标受众"},{"content":"无论代码类型如何，CI/CD 管道都有一些共同的实施问题需要解决。确保流程安全涉及到为操作构建任务分配角色。自动化工具（例如，Git Secrets）可用于此目的。为保证 CI/CD 管道的安全，以下安全任务应被视为最低限度：\n强化托管代码和工件库的服务器。 确保用于访问存储库的凭证，如授权令牌和生成拉动请求的凭证。 控制谁可以在容器镜像注册处签入和签出，因为它们是 CI 管道产生的工件的存储处，是 CI 和 CD 管道之间的桥梁。 记录所有的代码和构建更新活动。 如果在 CI 管道中构建或测试失败 —— 向开发人员发送构建报告并停止进一步的管道任务。配置代码库自动阻止来自 CD 管道的所有拉取请求。 如果审计失败，将构建报告发送给安全团队，并停止进一步的管道任务。 确保开发人员只能访问应用程序代码，而不能访问五种管道代码类型中的任何一种。 在构建和发布过程中，在每个需要的 CI/CD 阶段签署发布工件（最好是多方签署）。 在生产发布期间，验证所有需要的签名（用多个阶段的密钥生成），以确保没有人绕过管道。 ","relpermalink":"/service-mesh-devsecops/implement/securing-the-ci-cd-pipeline/","summary":"无论代码类型如何，CI/CD 管道都有一些共同的实施问题需要解决。确保流程安全涉及到为操作构建任务分配角色。自动化工具（例如，Git Secrets）可用于此目的。为保证 CI/CD 管道的安全，以下安全任务应被视为","title":"4.6 确保 CI/CD 管道的安全"},{"content":"在前一章中，我们了解了基础架构的各种表示以及围绕其部署工具的各种方法。在本章中，我们将看看如何设计部署和管理基础架构的应用程序。在上一章中我们重点关注基础架构即软件的开放世界，有时称为基础架构即应用。\n在云原生环境中，传统的基础架构运维人员需要转变为基础架构软件工程师。与过去的其他运维角色不同，这仍然是一种新兴的做法。我们迫切需要开始探索这种模式和制定标准。\n基础架构即软件与基础架构即代码之间的根本区别在于，软件会持续运行，并会根据调节器模式创建或改变基础架构，我们将在本章后面对其进行解释。此外，基础架构即软件的新范例是，软件现在与数据存储具有更传统的关系，并公开用于定义所需状态的 API。例如，该软件可能会根据数据存储中的需要改变基础架构的表示形式，并且可以很好地管理数据存储本身！希望进行协调的状态更改通过 API 发送到软件，而不是通过运行静态代码库中的程序。\n迈向基础架构即软件的第一步是让基础架构的运维人员意识到自己是软件工程师。我们热烈欢迎您来到这个领域！先前的工具（例如配置管理）也有类似的改变基础架构运维人员的工作职能的目标， …","relpermalink":"/cloud-native-infra/designing-infrastructure-applicaitons/","summary":"在前一章中，我们了解了基础架构的各种表示以及围绕其部署工具的各种方法。在本章中，我们将看看如何设计部署和管理基础架构的应用程序。在上一章中我们重点关注基础架构即软件的开放世界，有时称为基础架构即应用。","title":"第 4 章：设计基础架构应用程序"},{"content":"Kubernetes 可以成为数据和 / 或计算能力盗窃的重要目标。虽然数据盗窃是传统上的主要动机，但寻求计算能力（通常用于加密货币挖掘）的网络行为者也被吸引到 Kubernetes 来利用其底层基础设施。除了资源盗窃，网络行为者还可能针对 Kubernetes 造成拒绝服务。下面的威胁代表了 Kubernetes 集群最可能的破坏源。\n供应链风险 - 对供应链的攻击载体是多种多样的，并且在减轻风险方面具有挑战性。供应链风险是指对手可能颠覆构成系统的任何元素的风险，包括帮助提供最终产品的产品组件、服务或人员。这可能包括用于创建和管理 Kubernetes 集群的第三方软件和供应商。供应链的潜在威胁会在多个层面上影响 Kubernetes，包括： 容器 / 应用层面 - 在 Kubernetes 中运行的应用及其第三方依赖的安全性，它们依赖于开发者的可信度和开发基础设施的防御能力。来自第三方的恶意容器或应用程序可以为网络行为者在集群中提供一个立足点。 基础设施 - 托管 Kubernetes 的底层系统有其自身的软件和硬件依赖性。系统作为工作节点或控制平面一部分的，任何潜在威胁都可能为网 …","relpermalink":"/kubernetes-hardening-guidance/threat-model/","summary":"Kubernetes 可以成为数据和 / 或计算能力盗窃的重要目标。虽然数据盗窃是传统上的主要动机，但寻求计算能力（通常用于加密货币挖掘）的网络行为者也被吸引到 Kubernetes 来利用其底层基础设施。除了资源盗窃，网络行为者还可能针对 Kubernetes 造成","title":"威胁建模"},{"content":"第 2 章描述了实现自动分析所需的数据模型，第 4 章描述了支持原生 开源仪表的额外要求，并赋予各角色（应用程序所有者、运维和响应者）自主权。这就是我们对现代可观测性的概念模型。\n在本报告的其余部分，我们描述了这个新模型的一个事实实现，即 OpenTelemetry。这一章描述了构成 OpenTelemetry 遥测管道的所有组件。后面的章节将描述稳定性保证、建议的设置以及 OpenTelemetry 现实中的部署策略。有关该项目的更多细节可以在附录中找到。\n信号 OpenTelemetry 规范被组织成不同类型的遥测，我们称之为信号（signal）。主要的信号是追踪。日志和度量是其他例子。信号是 OpenTelemetry 中最基本的设计单位。\n每一个额外的信号首先是独立开发的，然后与追踪和其他相关信号整合。这种分离允许开发新的、实验性的信号，而不影响已经变得稳定的信号的兼容性保证。\nOpenTelemetry 是一个跨领域的关注点（cross-cutting concern），它在事务通过每个库和服务时追踪其执行。为了达到这个目的，所有的信号都建立在低级别的上下文传播系统之上，该系 …","relpermalink":"/opentelemetry-obervability/architectural-overview/","summary":"第 5 章：OpenTelemetry 架构概述","title":"第 5 章：OpenTelemetry 架构概述"},{"content":"HCM 是一个网络级的过滤器，将原始字节转译成 HTTP 级别的消息和事件（例如，收到的 Header，收到的 Body 数据等）。\nHCM 过滤器还处理标准的 HTTP 功能。它支持访问记录、请求 ID 生成和跟踪、Header 操作、路由表管理和统计等功能。\n从协议的角度来看，HCM 原生支持 HTTP/1.1、WebSockets、HTTP/2 和 HTTP/3（仍在 Alpha 阶段）。\nEnvoy 代理被设计成一个 HTTP/2 复用代理，这体现在描述 Envoy 组件的术语中。\nHTTP/2 术语\n在 HTTP/2 中，流是已建立的连接中的字节的双向流动。每个流可以携带一个或多个消息（message）。消息是一个完整的帧（frame）序列，映射到一个 HTTP 请求或响应消息。最后，帧是 HTTP/2 中最小的通信单位。每个帧都包含一个帧头（frame header），它至少可以识别该帧所属的流。帧可以携带有关 HTTP Header、消息有效载荷等信息。\n无论流来自哪个连接（HTTP/1.1、HTTP/2 或 HTTP/3），Envoy …","relpermalink":"/cloud-native-handbook/service-mesh/http-conneciton-manager/","summary":"HCM 是一个网络级的过滤器，将原始字节转译成 HTTP 级别的消息和事件（例如，收到的 Header，收到的 Body 数据等）。 HCM 过滤器还处理标准的 HTTP 功能。它支持访问记录、请求 ID 生成和跟踪、Header 操作、路由表管理和统计","title":"HTTP 连接管理器介绍"},{"content":"有时开发人员会拖延（Pushback）代码审查。他们要么不同意您的建议，要么抱怨您太严格。\n谁是对的？ 当开发人员不同意您的建议时，请先花点时间考虑一下是否正确。通常，他们比你更接近代码，所以他们可能真的对它的某些方面有更好的洞察力。他们的论点有意义吗？从代码健康的角度来看它是否有意义？如果是这样，让他们知道他们是对的，把问题解决。\n但是，开发人员并不总是对的。在这种情况下，审查人应进一步解释为什么认为他们的建议是正确的。好的解释在描述对开发人员回复的理解的同时，还会解释为什么请求更改。\n特别是，当审查人员认为他们的建议会改善代码健康状况时，他们应该继续提倡更改，如果他们认为最终的代码质量改进能够证明所需的额外工作是合理的。提高代码健康状况往往只需很小的几步。\n有时需要几轮解释一个建议才能才能让对方真正理解你的用意。只要确保始终保持礼貌，让开发人员知道你有听到他们在说什么，只是你不同意该论点而已。\n沮丧的开发者 审查者有时认为，如果审查者人坚持改进，开发人员会感到不安。有时候开发人员会感到很沮丧，但这样的感觉通常只会持续很短的时间，后来他们会非常感谢您在提高代码质量方面给他们的帮助。通 …","relpermalink":"/eng-practices/review/reviewer/pushback/","summary":"有时开发人员会拖延（Pushback）代码审查。他们要么不同意您的建议，要么抱怨您太严格。 谁是对的？ 当开发人员不同意您的建议时，请先花点时间考虑一下是否正确。通常，他们比你更接近代码，所以他们可能真的","title":"处理 Code Review 中的拖延"},{"content":"In this page you’ll learn to connect to TSB with the tctl CLI and some basics of using the CLI.\nBefore you start: ✓ Install the TSB management plane (self-managed Only) ✓ Download Tetrate Service Bridge CLI (tctl)\n✓ Get your TSB’s organization name - you can find this logging in to the TSB UI or configured at installation time in the TSB ManagementPlane CR\nTSB provides a UI, but most examples in this site - and most scripting and automation - will use the tctl CLI. This document will cover how …","relpermalink":"/tsb/setup/tctl-connect/","summary":"In this page you’ll learn to connect to TSB with the tctl CLI and some basics of using the CLI.\nBefore you start: ✓ Install the TSB management plane (self-managed Only) ✓ Download Tetrate Service Bridge CLI (tctl)\n✓ Get your TSB’s organization name - you can find this logging in to the TSB UI or configured at installation time in the TSB ManagementPlane CR\nTSB provides a UI, but most examples in this site - and most scripting and automation - will use the tctl CLI. This document will cover how to get logged in so you can use the CLI, as well as steps to update your credentials.","title":"Connect to TSB with tctl"},{"content":"Egress Gateways act as a gateway for traffic exiting the mesh. Users are able to define services that are allowed to send traffic to external services through the gateway\nCurrently only HTTPS can be sent externally. However, the original outbound requests should use HTTP. These outbound HTTP requests are converted to HTTPS requests and sent to the external services. For example, a request to http://tetrate.io from the service that goes through an Egress Gateway is converted to a request to …","relpermalink":"/tsb/howto/gateway/egress-gateways/","summary":"Egress Gateways act as a gateway for traffic exiting the mesh. Users are able to define services that are allowed to send traffic to external services through the gateway\nCurrently only HTTPS can be sent externally. However, the original outbound requests should use HTTP. These outbound HTTP requests are converted to HTTPS requests and sent to the external services. For example, a request to http://tetrate.io from the service that goes through an Egress Gateway is converted to a request to https://tetrate.io, and is proxied on behalf of the originating service. Currently requests that are ultimately need to be HTTP are not supported.","title":"Controlling Access to External Services"},{"content":":::warning Technical Preview Tetrate WAF is a technical preview of a future capability of TSB. It is not recommended for production usage at this stage. :::\nThis document will describe how Web Application Firewall (WAF) capabilities can be enabled and configured in TSB.\nOverview A Web Application Firewall (WAF) inspects inbound and outbound HTTP traffic. It matches the traffic against a range of signatures in order to detect attack attempts, malformed traffic, and exfiltration of sensitive data. …","relpermalink":"/tsb/howto/waf/","summary":":::warning Technical Preview Tetrate WAF is a technical preview of a future capability of TSB. It is not recommended for production usage at this stage. :::\nThis document will describe how Web Application Firewall (WAF) capabilities can be enabled and configured in TSB.\nOverview A Web Application Firewall (WAF) inspects inbound and outbound HTTP traffic. It matches the traffic against a range of signatures in order to detect attack attempts, malformed traffic, and exfiltration of sensitive data. Suspicious traffic can be blocked, alerts can be raised, and traffic can be logged for later analysis.\nTraditional WAF solutions operate at the edge of a network, inspecting ingress and egress traffic to and from the Internet.","title":"Using WAF Capabilities"},{"content":"在本部分中，你将配置 Ingress Gateway 以允许外部流量到达 TSB 环境中的 bookinfo 应用程序。\n先决条件 在继续之前，请确保你已完成以下任务：\n熟悉 TSB 概念。 安装 TSB 演示环境。 部署 Istio Bookinfo 示例应用程序。 创建租户和工作区。 创建配置组。 配置权限。 创建入口网关对象 你将创建一个 Ingress Gateway 对象来为你的 bookinfo 应用程序启用外部流量。\n创建 ingress.yaml 创建一个名为 ingress.yaml 的文件，其中包含以下内容：\napiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: tsb-gateway-bookinfo namespace: bookinfo spec: selector: app: tsb-gateway-bookinfo servers: - port: number: 80 name: http protocol: HTTP hosts: - \u0026#34;*\u0026#34; 使用 kubectl  …","relpermalink":"/tsb/quickstart/ingress-gateway/","summary":"在本部分中，你将配置 Ingress Gateway 以允许外部流量到达 TSB 环境中的 bookinfo 应用程序。 先决条件 在继续之前，请确保你已完成以下任务： 熟悉 TSB 概念。 安装 TSB 演示环境。 部署 Istio Bookinfo 示例应用程序。 创建租户和工作区。 创建配置组。 配置权限。 创","title":"入口网关"},{"content":"🔔 提醒：自 1.5 版本起可用 - 状态：Alpha\nArgo Rollouts 支持通过第三方插件系统获取分析指标。这允许用户扩展 Rollouts 的功能以支持原生不支持的度量提供者。Rollouts 使用一个名为 go-plugin 的插件库来实现这一点。你可以在这里找到一个示例插件：rollouts-plugin-trafficrouter-sample-nginx\n使用 Traffic Router 插件 安装和使用 Argo Rollouts 插件有两种方法。第一种方法是将插件可执行文件挂载到 rollouts 控制器容器中。第二种方法是使用 HTTP（S）服务器托管插件可执行文件。\n将插件可执行文件挂载到 rollouts 控制器容器中 有几种方法可以将插件可执行文件挂载到 rollouts 控制器容器中。其中一些将取决于你的特定基础设施。这里有几种方法：\n使用 init 容器下载插件可执行文件 使用 Kubernetes 卷挂载共享卷，如 NFS、EBS 等。 将插件构建到 rollouts 控制器容器中 然后，你可以使用 configmap 将插件可执行文件位置指向 …","relpermalink":"/argo-rollouts/traffic-management/plugins/","summary":"🔔 提醒：自 1.5 版本起可用 - 状态：Alpha Argo Rollouts 支持通过第三方插件系统获取分析指标。这允许用户扩展 Rollouts 的功能以支持原生不支持的度量提供者。Rollouts 使用一个名为 go-plugin 的插件库来实现这一点。你可以在这里找到","title":"流量路由插件"},{"content":"在回滚更新时，我们可能会为所有策略缩小新的副本集。用户可以通过将 abortScaleDownDelaySeconds 设置为 0 来选择永久保留新的副本集，或者将该值调整为更大或更小的值。\n下表总结了在 Rollout 策略和 abortScaleDownDelaySeconds 的组合下的行为。请注意，abortScaleDownDelaySeconds 不适用于 argo-rollouts v1.0。 abortScaleDownDelaySeconds = nil 是默认值，这意味着在 v1.1 中，对于所有 Rollout 策略，默认情况下在 Rollout 后 30 秒内缩小新的副本集。\n策略 v1.0 行为 abortScaleDownDelaySeconds v1.1 行为 蓝绿部署 不缩小 nil 回滚后 30 秒内缩小 蓝绿部署 不缩小 0 不缩小 蓝绿部署 不缩小 N 回滚后 N 秒内缩小 基本金丝雀 回滚到稳定状态 N/A 回滚到稳定状态 带流量路由的金丝雀 立即缩小 nil 回滚后 30 秒内缩小 带流量路由的金丝雀 立即缩小 0 不缩小 带流量路由的金丝雀  …","relpermalink":"/argo-rollouts/rollout/scaledown-aborted-rs/","summary":"在回滚更新时，我们可能会为所有策略缩小新的副本集。用户可以通过将 abortScaleDownDelaySeconds 设置为 0 来选择永久保留新的副本集，或者将该值调整为更大或更小的值。 下表总结了在 Rollout 策略和 abortScaleDownDelaySeconds 的组合下的行为。请注意，abortScaleD","title":"Rollout 失败时缩小新的 ReplicaSet"},{"content":"读者将了解到 SPIRE 部署的组成部分，有哪些部署模式，以及在部署 SPIRE 时需要考虑哪些性能和安全问题。\n你的 SPIRE 部署的设计应满足你的团队和组织的技术要求。它还应包括支持可用性、可靠性、安全性、可扩展性和性能的要求。该设计将作为你的部署活动的基础。\n身份命名方案 请记住，在前面的章节中，SPIFFE ID 是一个结构化的字符串，代表一个工作负载的身份名称，正如你在第四章中看到的那样。工作负载标识符部分（URI 的路径部分）附加在信任域名（URI 的主机部分）上，可以组成关于服务所有权的含义，以表示它在什么平台上运行，谁拥有它，它的预期目的，或其他惯例。它是特意为你定义的灵活和可定制的。\n你的命名方案可能是分层的，就像文件系统的路径。也就是说，为了减少歧义，命名方案不应该以尾部的正斜杠（/）结束。下面你将看到一些不同的样例，它们遵循三种不同的约定，你可以遵循，或者如果你感到特别有灵感，也可以想出你自己的。\n直接命名服务 你可能会发现，作为软件开发生命周期的一部分，直接通过它从应用角度呈现的功能和它运行的环境来识别一个服务是很有用的。例如，管理员可能会规定，在特定环境中运 …","relpermalink":"/spiffe/designing-a-spire-deployment/","summary":"读者将了解到 SPIRE 部署的组成部分，有哪些部署模式，以及在部署 SPIRE 时需要考虑哪些性能和安全问题。 你的 SPIRE 部署的设计应满足你的团队和组织的技术要求。它还应包括支持可用性、可靠性、安全性、可扩展性和性能的要求。该设","title":"设计一个 SPIRE 部署"},{"content":"在 Kubernetes 中管理 pod 时，Cilium 将为 Cilium 管理的每个 pod 创建一个 CiliumEndpoint（CEP）的自定义资源定义（CRD）。如果启用了 enable-cilium-endpoint-slice，那么 Cilium 还会创建一个 CiliumEndpointSlice （CES）类型的 CRD，将一组具有相同安全身份的 CEP 对象分组到一个 CES 对象中，并广播 CES 对象来向其他代理传递身份，而不是通过广播 CEP 来实现。在大多数情况下，这减少了控制平面上的负载，可以使用相同的主资源维持更大规模的集群。\n例如：\n$ kubectl get ciliumendpointslices --all-namespaces NAME AGE ces-548bnpgsf-56q9f 171m ces-dy4d8x6j2-qgc2z 171m ces-f6qfylrxh-84vxm 171m ces-k29rv92f5-qb4sw 171m ces-m9gs68csm-w2qg8 171m ","relpermalink":"/cilium-handbook/kubernetes/ciliumendpointslice/","summary":"在 Kubernetes 中管理 pod 时，Cilium 将为 Cilium 管理的每个 pod 创建一个 CiliumEndpoint（CEP）的自定义资源定义（CRD）。如果启用了 enable-cilium-endpoint-slice，那么 Cilium 还会创","title":"端点切片 CRD"},{"content":"我希望这个简短的报告能让你了解 eBPF 和它的强大之处。我真正希望的是，你已经准备好尝试一些基于 eBPF 的工具！如果你想在技术方面深入研究，可以从 ebpf.io 开始，在那里你会找到更多关于技术和 ebPF 基金会的信息。对于编码实例，可以在 GitHub 上的 ebpf-beginners 仓库里找到。\n为了了解其他人是如何利用 eBPF 工具的，请参加 eBPF Summit 和 Cloud Native eBPF Day 等活动，在这些活动中，用户分享他们的成功和学习经验。还有一个活跃的 Slack 频道 ebpf.io/slack。我希望能在那里见到你！\n","relpermalink":"/what-is-ebpf/conclusion/","summary":"我希望这个简短的报告能让你了解 eBPF 和它的强大之处。我真正希望的是，你已经准备好尝试一些基于 eBPF 的工具！如果你想在技术方面深入研究，可以从 ebpf.io 开始，在那里你会找到更多关于技术和 ebPF 基金会的信息。对于编码实例，可","title":"第七章：结论"},{"content":"容器运行时接口（Container Runtime Interface），简称 CRI。CRI 中定义了 容器 和 镜像 的服务的接口，因为容器运行时与镜像的生命周期是彼此隔离的，因此需要定义两个服务。该接口使用 Protocol Buffer，基于 gRPC，在 Kubernetes v1.10 + 版本中是在 pkg/kubelet/apis/cri/runtime/v1alpha2 的 api.proto 中定义的。\nCRI 架构 Container Runtime 实现了 CRI gRPC Server，包括 RuntimeService 和 ImageService。该 gRPC Server 需要监听本地的 Unix socket，而 kubelet 则作为 gRPC Client 运行。\n启用 CRI 除非集成了 rktnetes，否则 CRI 都是被默认启用了，从 Kubernetes 1.7 版本开始，旧的预集成的 docker CRI 已经被移除。\n要想启用 CRI 只需要在 kubelet 的启动参数重传入此参 …","relpermalink":"/kubernetes-handbook/architecture/open-interfaces/cri/","summary":"容器运行时接口（Container Runtime Interface），简称 CRI。CRI 中定义了 容器 和 镜像 的服务的接口，因为容器运行时与镜像的生命周期是彼此隔离的，因此需要定义两个服务。该接口使用 Protocol Buffer，","title":"容器运行时接口（CRI）"},{"content":"由于参考平台是由容器编排和资源管理平台以及服务网格软件组成的，以下出版物为确保该平台的安全提供了指导，并为本文件的内容提供了背景信息。\nSP800-204，基于微服务的应用系统的安全策略，讨论了基于微服务的应用的特点和安全要求，以及满足这些要求的总体策略。 SP800-204A，使用服务网格构建基于微服务的安全应用，为基于微服务的应用的各种安全服务（如建立安全会话、安全监控等）提供了部署指导，这些服务使用基于独立于应用代码运行的服务代理的专用基础设施（即服务网格）。 SP800-204B，使用服务网格的基于微服务的应用的基于属性的访问控制，为在服务网格中构建满足安全要求的认证和授权框架提供了部署指导，例如：（1）通过在任何一对服务之间的通信中实现相互认证来实现零信任；（2）基于访问控制模型，如基于属性的访问控制（ABAC）模型的强大访问控制机制，可用于表达广泛的策略集，并在用户群、对象（资源）和部署环境方面可扩展。 SP800-190，应用容器安全指南，解释了与容器技术相关的安全问题，并为在规划、实施和维护容器时解决这些问题提出了实用建议。这些建议是针对容器技术架构中的每个层级提供的。 …","relpermalink":"/service-mesh-devsecops/intro/relationship-to-other-nist-guidance-documents/","summary":"由于参考平台是由容器编排和资源管理平台以及服务网格软件组成的，以下出版物为确保该平台的安全提供了指导，并为本文件的内容提供了背景信息。 SP800-204，基于微服务的应用系统的安全策略，讨论了基于微服","title":"1.4 与其他 NIST 指导文件的关系"},{"content":"下一个常见问题涉及工作流模型。所有的 CI/CD 管道都可以有两种类型的工作流程模型，这取决于作为管道一部分部署的自动化工具。\n基于推的模式 基于拉的模式 在支持基于推模式的 CI/CD 工具中，在管道的一个阶段或阶段所做的改变会触发后续阶段或阶段的改变。例如，通过一系列的编码脚本，CI 系统中的新构建会触发管道中 CD 部分的变化，从而改变部署基础设施（如 Kubernetes 集群）。使用 CI 系统作为部署变化的基础，其安全方面的缺点是有可能将凭证暴露在部署环境之外，尽管已尽最大努力确保 CI 脚本的安全，因为 CI 脚本是在部署基础设施的信任域之外运行的。由于 CD 工具拥有生产系统的 key，基于推送的模式就变得不安全了。\n在基于拉的工作流程模型中，与部署环境有关的运维（例如 Kubernetes 运维、Flux、ArgoCD）一旦观察到有新镜像被推送到注册表，就会从环境内部拉动新镜像。新镜像被从注册表中拉出，部署清单被自动更新，新镜像被部署在环境（如集群）中。因此，实际的部署基础设施状态与 Git 部署库中声明性描述的状态实现了衔接。此外，部署环境凭证（例如集群凭证）不会暴 …","relpermalink":"/service-mesh-devsecops/implement/workflow-models-in-ci-cd-pipelines/","summary":"下一个常见问题涉及工作流模型。所有的 CI/CD 管道都可以有两种类型的工作流程模型，这取决于作为管道一部分部署的自动化工具。 基于推的模式 基于拉的模式 在支持基于推模式的 CI/CD 工具中，在管道的一个阶段或阶段所做的改变会","title":"4.7 CI/CD 管道中的工作流模型"},{"content":"Pod 是 Kubernetes 中最小的可部署单元，由一个或多个容器组成。Pod 通常是网络行为者在利用容器时的初始执行环境。出于这个原因，Pod 应该被加固，以使利用更加困难，并限制成功入侵的影响。\n图 3：有 sidecar 代理作为日志容器的 Pod 组件 “非 root”容器和“无 root”容器引擎 默认情况下，许多容器服务以有特权的 root 用户身份运行，应用程序在容器内以 root 用户身份执行，尽管不需要有特权的执行。\n通过使用非 root 容器或无 root 容器引擎来防止 root 执行，可以限制容器受损的影响。这两种方法都会对运行时环境产生重大影响，因此应该对应用程序进行全面测试，以确保兼容性。\n非 root 容器：容器引擎允许容器以非 root 用户和非 root 组成员身份运行应用程序。通常情况下，这种非默认设置是在构建容器镜像的时候配置的。附录 A：非 root 应用的 Dockerfile 示例 显示了一个 Dockerfile 示例，它以非 root 用户身份运行一个应用。\n非 root 用户。另外，Kubernetes …","relpermalink":"/kubernetes-hardening-guidance/kubernetes-pod-security/","summary":"Pod 是 Kubernetes 中最小的可部署单元，由一个或多个容器组成。Pod 通常是网络行为者在利用容器时的初始执行环境。出于这个原因，Pod 应该被加固，以使利用更加困难，并限制成功入侵的影响。 图 3：有 sidecar 代理作为日志容器的 Pod","title":"Kubernetes Pod 安全"},{"content":"在构建应用程序以管理基础架构时，我们要将需要公开的 API 与要创建的应用程序等量看待。这些 API 将代表您的基础架构的抽象，而应用程序将使用 API 消费这些这些基础架构。\n务必牢牢掌握两者的重要性，和如何利用它们来创建可扩展的弹性基础架构。\n在本章中，我们将举一个虚构的云原生应用程序和 API 示例，这些应用程序和 API 会经历正常的应用程序周期。如果您想了解更多有关管理云原生应用程序的信息，请参阅第 7 章。\n设计 API 这里的 API 是指处理数据结构中的基础架构表示，而不关心如何暴露或消费这些 API。通常使用 HTTP RESTful 端点来传递数据结构，API 如何实现对本章并不重要。\n随着基础架构的不断发展，运行在基础架构之上的应用程序也要随之演变。为这些应用程序的功能将随着时间而改变，因此基础架构是也是隐性地演变。随着基础架构的不断发展，管理它的应用程序也必须发展。\n基础架构的功能、需求和发展将永无止境。如果幸运的话，云供应商的 API 将会保持稳定，不会频繁更改。作为基础架构工程师，我们需要做好准备，以适应这些需求。我们需要准备好发展我们的基础架构和运行其上的 …","relpermalink":"/cloud-native-infra/developing-infrastructure-applications/","summary":"在构建应用程序以管理基础架构时，我们要将需要公开的 API 与要创建的应用程序等量看待。这些 API 将代表您的基础架构的抽象，而应用程序将使用 API 消费这些这些基础架构。 务必牢牢掌握两者的重要性，和如何利用它们来创建可","title":"第 5 章：开发基础架构应用程序"},{"content":"OpenTelemetry 被设计成允许长期稳定性和不确定性并存的局面。在 OpenTelemetry 中，稳定性保证是在每个信号的基础上提供的。与其看版本号，不如检查你想使用的信号的稳定性等级。\n信号生命周期 图 6-1 显示了新信号是如何被添加到 OpenTelemetry 的。实验性信号仍在开发中。它们可能在任何时候改变并破坏兼容性。实验性信号的开发是以规范提案的形式开始的，它是与一组原型一起开发的。一旦实验性信号准备好在生产中使用，信号的特性就会被冻结，新信号的测试版就会以多种语言创建。测试版可能不是完整的功能，它们可能会有一些突破性的变化，但它们被认为是为早期采用者的产品反馈做好准备。一旦一个信号被认为可以被宣布为稳定版本，就会发布一个候选版本。如果候选版本能够在一段时间内保持稳定，没有问题，那么该信号的规范和测试版都被宣布为稳定。\n一旦一个信号变得稳定，它就属于 OpenTelemetry 的长期支持保障范围。OpenTelemetry 非常重视向后兼容和无缝升级。详情见以下章节。\n如果 OpenTelemetry 信号的某个组件需要退役，该组件将被标记为废弃的。被废弃的组 …","relpermalink":"/opentelemetry-obervability/stability-and-long-term-support/","summary":"第 6 章：稳定和长期支持","title":"第 6 章：稳定和长期支持"},{"content":"有时候紧急 CL 必须尽快通过 code review 过程。\n什么是紧急情况？ 紧急 CL 是这样的小更新：允许主要发布继续而不是回滚，修复显著影响用户生产的错误，处理紧迫的法律问题，关闭主要安全漏洞等。\n在紧急情况下，我们确实关心 Code Review 的整体速度，而不仅仅是响应的速度。仅在这种情况下，审查人员应该更关心审查的速度和代码的正确性（是否解决了紧急情况？）。此外（显然）这类状况的审查应该优先于所有其他 code reivew。\n但是，在紧急情况解决后，您应该再次查看紧急 CL 并进行更彻底的审查。\n什么不是紧急情况？ 需要说明的是，以下情况并非紧急情况：\n想要在本周而不是下周推出（除非有一些实际硬性截止日期，例如合作伙伴协议）。 开发人员已经在很长一段时间内完成了一项功能想要获得 CL。 审查者都在另一个时区，目前是夜间或他们已离开现场。 现在是星期五，在开发者在过周末之前获得这个 CL 会很棒。 今天因为软（非硬）截止日期，经理表示必须完成此审核并签入 CL。 回滚导致测试失败或构建破坏的 CL。 等等。\n什么是 Hard Deadline？ 硬性截止日期（Hard …","relpermalink":"/eng-practices/review/emergencies/","summary":"有时候紧急 CL 必须尽快通过 code review 过程。 什么是紧急情况？ 紧急 CL 是这样的小更新：允许主要发布继续而不是回滚，修复显著影响用户生产的错误，处理紧迫的法律问题，关闭主要安全漏洞等。 在紧急情况下，我们确实关心 Code Review 的","title":"紧急情况"},{"content":"For this scenario, you will need two clusters onboarded to configure round robin - failover between them.\nPrerequisites Before you get started, make sure you: ✓ Familiarize yourself with TSB concepts ✓ Install the TSB demo environment ✓ Create a Tenant Create workspace and gateway group The following YAML file has two objects; a Workspace for the application, and a Gateway group so that you can configure the application ingress.\n{httpBinMgmtYAML} Store as httpbin-mgmt.yaml, and apply with tctl: …","relpermalink":"/tsb/howto/gateway/distributed-ingress/","summary":"For this scenario, you will need two clusters onboarded to configure round robin - failover between them.\nPrerequisites Before you get started, make sure you: ✓ Familiarize yourself with TSB concepts ✓ Install the TSB demo environment ✓ Create a Tenant Create workspace and gateway group The following YAML file has two objects; a Workspace for the application, and a Gateway group so that you can configure the application ingress.\n{httpBinMgmtYAML} Store as httpbin-mgmt.yaml, and apply with tctl:\ntctl apply -f httpbin-mgmt.yaml Deploy httpbin The following configurations should be applied to both clusters; to deploy your application, start by creating the namespace and enable the Istio sidecar injection.","title":"Distributed Ingress Gateways"},{"content":"Apache SkyWalking Cloud on Kubernetes (SWCK) provides an external metrics adapter from which the Kubernetes Horizontal Pod Autoscaling (HPA) controller can retrieve metrics from. Users may deploy SWCK adapter into the TSB control plane in order to fetch target metrics from the Observability Analysis Platform (OAP) service.\nBefore you get started, make sure you: ✓ Familiarize yourself with TSB concepts ✓ Install the TSB demo environment ✓ Deploy the Istio Bookinfo sample app Verifying the SWCK …","relpermalink":"/tsb/howto/hpa-using-skywalking/","summary":"Apache SkyWalking Cloud on Kubernetes (SWCK) provides an external metrics adapter from which the Kubernetes Horizontal Pod Autoscaling (HPA) controller can retrieve metrics from. Users may deploy SWCK adapter into the TSB control plane in order to fetch target metrics from the Observability Analysis Platform (OAP) service.\nBefore you get started, make sure you: ✓ Familiarize yourself with TSB concepts ✓ Install the TSB demo environment ✓ Deploy the Istio Bookinfo sample app Verifying the SWCK metrics adapter The SWCK adapter is responsible for managing the OAP component, among others.\nThe adapter should have been installed when you installed the TSB demo profile.","title":"HPA using SkyWalking"},{"content":"This document describes a conservative guideline for capacity planning of Tetrate Service Bridge (TSB) in Management and Control planes.\nThese parameters apply to production installations: TSB will run with minimal resources if you are using a demo-like environment.\n:::note disclaimer The resource provisioning guidelines described in this document are very conservative.\nAlso please be aware that the resource provisioning described in this document are applicable to vertical resource scaling. …","relpermalink":"/tsb/setup/resource-planning/","summary":"This document describes a conservative guideline for capacity planning of Tetrate Service Bridge (TSB) in Management and Control planes.\nThese parameters apply to production installations: TSB will run with minimal resources if you are using a demo-like environment.\n:::note disclaimer The resource provisioning guidelines described in this document are very conservative.\nAlso please be aware that the resource provisioning described in this document are applicable to vertical resource scaling. Multiple replicas of the same TSB components do not share the load with each other, and therefore you cannot expect the combined resources from multiple components to have the same effect. Replicas of TSB components should only be used for high availability purposes only.","title":"Resource Consumption and Capacity Planning"},{"content":"在本部分中，你将检查 TSB 环境中 bookinfo 演示应用程序的服务拓扑和指标。\n先决条件 在继续之前，请确保你已完成以下任务：\n熟悉 TSB 概念。 安装 TSB 演示环境。 部署 Istio Bookinfo 示例应用程序。 创建租户和工作区。 创建配置组。 配置权限。 设置入口网关。 生成指标流量 在检查拓扑和指标之前，你需要为 bookinfo 应用程序生成流量以获得有意义的数据。使用提供的脚本生成流量：\n将以下脚本保存为 send-requests.sh ：\n#!/bin/bash while true; do curl -s https://bookinfo.tetrate.com/productpage \u0026gt; /dev/null sleep 1 done 使脚本可执行并运行它：\nchmod +x send-requests.sh ./send-requests.sh 该脚本每 1 秒向 bookinfo 产品页面发送一个请求，生成指标流量。\n查看拓扑和指标 现在，你可以在 TSB UI 中检查服务拓扑和指标。\n在左侧面板中的“租户”下，选择“仪表板”。 单击选择集群- …","relpermalink":"/tsb/quickstart/observability/","summary":"在本部分中，你将检查 TSB 环境中 bookinfo 演示应用程序的服务拓扑和指标。 先决条件 在继续之前，请确保你已完成以下任务： 熟悉 TSB 概念。 安装 TSB 演示环境。 部署 Istio Bookinfo 示例应用程序。 创建租户和工作区。 创建配置组。 配置权限。 设置入","title":"拓扑和指标"},{"content":"🔔 重要提示：自 v1.4 起支持蓝绿部署和金丝雀部署。\n默认情况下，当重新应用旧 Rollout 操作时，控制器会像处理规范更改一样处理它，并执行完整的步骤列表，并执行分析。但有两个例外：\n控制器检测到正在返回到仍在其 scaleDownDelay 范围内且已缩放的蓝绿 ReplicaSet。 控制器检测到正在返回到金丝雀的“稳定”ReplicaSet，并且升级尚未完成。 通常，当期望行为是尽快回滚时，重新运行分析和步骤对于回滚操作来说是不可取的。为了帮助实现这一点，回滚窗口功能允许用户指示在窗口内提升到 ReplicaSet 时将跳过所有步骤。\n示例：\nspec: rollbackWindow: revisions: 3 revisionHistoryLimit: 5 假设有线性修订历史记录：1、2、3、4、5（当前）。从修订版本 5 回滚到 4 或 3 将落在窗口内，因此将快速跟踪。\n","relpermalink":"/argo-rollouts/rollout/rollback/","summary":"🔔 重要提示：自 v1.4 起支持蓝绿部署和金丝雀部署。 默认情况下，当重新应用旧 Rollout 操作时，控制器会像处理规范更改一样处理它，并执行完整的步骤列表，并执行分析。但有两个例外： 控制器检测到正在返回到仍在其 scaleDownDelay 范围内且已","title":"回滚窗口"},{"content":"本章探讨了 SPIFFE 和 SPIRE 如何与环境集成。\nSPIFFE 从一开始就被设计成可插拔和可扩展的，所以将 SPIFFE 和 SPIRE 与其他软件系统集成的话题是一个广泛的话题。一个特定的集成的架构超出了本书的范围。相反，本章意在捕捉一些可能的常见集成，以及一个高层次的概述，以及进行集成工作的策略。\n使软件能够使用 SVID 在考虑如何调整软件以使用 SVID 时，有许多选项可供选择。本节介绍了其中的几个选项，以及与之相关的注意事项。\n本地 SPIFFE 支持 这种方法需要修改现有的服务，以使它们能够感知 SPIFFE。当所需的修改最小，或者可以在跨应用服务使用的通用库或框架中引入时，它是首选。对于那些对延迟敏感的数据平面服务，或希望在应用层利用身份的服务，本地集成是最好的方法。SPIFFE 提供了一些库，如用于 Go 编程语言的 GO-SPIFFE 和用于 Java 编程语言的 JAVA-SPIFFE，它们有助于开发支持 SPIFFE 的工作负载。\n当用支持 SPIFFE 库的语言构建软件时，这通常是利用 SPIFFE 最直接的方式。上面提到的 Go 和 Java 库有使 …","relpermalink":"/spiffe/integrating-with-others/","summary":"本章探讨了 SPIFFE 和 SPIRE 如何与环境集成。 SPIFFE 从一开始就被设计成可插拔和可扩展的，所以将 SPIFFE 和 SPIRE 与其他软件系统集成的话题是一个广泛的话题。一个特定的集成的架构超出了本书的范围。相反，本章意在捕捉一些可能的常见集成，","title":"与其他系统集成"},{"content":"Cilium 与多个 Kubernetes API 组兼容。有些是废弃的或测试版的，可能只在 Kubernetes 的特定版本中可用。\n所有列出的 Kubernetes 版本都经过 e2e 测试，保证与 Cilium 兼容。本表中未列出的旧版 Kubernetes 不支持 Cilium。较新的 Kubernetes 版本，虽然没有列出，但将取决于 Kubernetes 提供的后向兼容性。\nKubernetes 版本 Kubernetes NetworkPolicy API CiliumNetworkPolicy 1.16, 1.17, 1.18, 1.19, 1.20, 1.21, 1.22, 1.23 networking.k8s.io/v1 cilium.io/v2 有一个 CRD Cilium 在 Kubernetes 中使用了一个网络策略的 CRD。这个 CRD 的模式验证可能会有变化，它可以验证 Cilium Clusterwide Network Policy（CCNP）或 Cilium Network Policy（CNP）的正确性。\nCRD 本身有一个注解， …","relpermalink":"/cilium-handbook/kubernetes/compatibility/","summary":"Cilium 与多个 Kubernetes API 组兼容。有些是废弃的或测试版的，可能只在 Kubernetes 的特定版本中可用。 所有列出的 Kubernetes 版本都经过 e2e 测试，保证与 Cilium 兼容。本表中未列出的旧版 Kubernetes 不支持 Cilium。较新的 Kubernetes 版本，虽然没有列出，但将取决于 Kubernetes 提供","title":"Kubernetes 兼容性"},{"content":"容器网络接口（Container Network Interface），简称 CNI，是 CNCF 旗下的一个项目，由一组用于配置 Linux 容器的网络接口的规范和库组成，同时还包含了一些插件。CNI 仅关心容器创建时的网络分配，和当容器被删除时释放网络资源。有关详情请查看 GitHub。\nKubernetes 源码的 vendor/github.com/containernetworking/cni/libcni 目录中已经包含了 CNI 的代码，也就是说 Kubernetes 中已经内置了 CNI。\n接口定义 CNI 的接口中包括以下几个方法：\ntype CNI interface { AddNetworkList (net *NetworkConfigList, rt *RuntimeConf) (types.Result, error) DelNetworkList (net *NetworkConfigList, rt *RuntimeConf) error AddNetwork (net *NetworkConfig, rt *RuntimeConf) …","relpermalink":"/kubernetes-handbook/architecture/open-interfaces/cni/","summary":"容器网络接口（Container Network Interface），简称 CNI，是 CNCF 旗下的一个项目，由一组用于配置 Linux 容器的网络接口的规范和库组成，同时还包含了一些插件。CNI 仅关心容器创建时的网络分配，和当容器被","title":"容器网络接口（CNI）"},{"content":"本文件的结构如下：\n第二章简要介绍了参考平台，为其提供了实施 DevSecOps 原语的指导。\n第三章介绍了 DevSecOps 的基本要素（即管道），设计和执行管道的方法，以及自动化在执行中的作用。\n第四章涵盖了管道的所有方面，包括（a）所有管道需要解决的共同问题，（b）对第 1.1 节中列出的参考平台中五种代码类型的管道的描述，以及（c）DevSecOps 在整个生命周期中对整个应用环境（有五种代码类型的参考平台，因此承载着 DevSecOps 的实施）的安全保证的好处，包括 持续授权操作（C-ATO）。\n第五章提供了摘要和结论。\n下一章 ","relpermalink":"/service-mesh-devsecops/intro/organization-of-this-document/","summary":"本文件的结构如下： 第二章简要介绍了参考平台，为其提供了实施 DevSecOps 原语的指导。 第三章介绍了 DevSecOps 的基本要素（即管道），设计和执行管道的方法，以及自动化在执行中的作用。 第四章涵盖了管道的所有方面，包括（a）所有管","title":"1.5 本文件的组织"},{"content":"最后一个常见的问题是安全测试。无论代码类型是什么（例如，应用服务、Iac、Pac 或可观测性），基于微服务的基础设施的 DevSecOps 的 CI/CD 管道与服务网格应包括由自动化工具或作为服务提供的应用安全测试（AST）。这些工具会分析和测试应用程序的安全漏洞。根据 Gartner 的说法，有 四种 主要的 AST 技术：\n静态 AST（SAST）工具：分析应用程序的源码、字节码或二进制代码的安全漏洞，通常在编程和 / 或测试软件生命周期（SLC）阶段。具体来说，这项技术涉及到在提交中查看应用程序并分析其依赖关系的 技术。如果任何依赖关系包含问题或已知的安全漏洞，提交将被标记为不安全的，不允许继续部署。这也可以包括在代码中找到应该被删除的硬编码密码 / 秘密。 动态 AST（DAST）工具：在测试或运行阶段，分析应用程序的动态运行状态。它们模拟针对应用程序（通常是支持网络的应用程序、服务和 API）的攻击，分析应用程序的反应，并确定它是否有漏洞。特别是，DAST 工具比 SAST 更进一步，在 CI 工作中启动生产环境的副本，以扫描所产生的容器和 可执行文件。动态方面有助于系统捕 …","relpermalink":"/service-mesh-devsecops/implement/security-testing-common-requirement-for-ci-cd-pipelines-for-all-code-types/","summary":"最后一个常见的问题是安全测试。无论代码类型是什么（例如，应用服务、Iac、Pac 或可观测性），基于微服务的基础设施的 DevSecOps 的 CI/CD 管道与服务网格应包括由自动化工具或作为服务提供的应用安全测试（AST）。这些工","title":"4.8 安全测试——所有代码类型的 CI/CD 管道的共同要求"},{"content":"基础架构是用来支撑应用程序的。可信任的软件对于工程成功至关重要。如果每次在终端输入 ls 命令，都会发生随机动作，那么你将永远也不会相信 ls，而是去找另一种方式来列出目录中的文件。\n我们的基础架构必须值得信任。本章旨在建立信任和验证基础架构的意识形态。我们将描述的实践旨在增加对应用程序和基础架构工程的信心。\n软件测试在当今的软件工程领域非常普遍。然而，如何测试基础架构还没有很明确的最佳实践。\n这意味着本书中的所有章节中，这一节应该是最令人兴奋的！在该领域像您这样的工程师有充分的空间发挥出色的影响力。\n软件测试是一种证明软件可以正常工作的有效做法，证明软件在各种特殊情况下软件仍然可以正常运行。因此，如果我们将相同的范例应用于基础架构测试，测试目标如下：\n证明基础架构按预期运行。 证明基础架构不会失败。 证明这两种情况在各种边缘情况下都是正确的。 衡量基础架构是否有效需要我们先定义什么叫有效。现在，您应该对使用基础架构和工程代表应用程序的想法感到满意。\n定义基础架构 API 的人应该花时间构思一个可以创建有效基础架构的理智的 API。例如，如果创建了一个定义虚拟机的 API 但里面没有使 …","relpermalink":"/cloud-native-infra/testing-cloud-native-infrastructure/","summary":"基础架构是用来支撑应用程序的。可信任的软件对于工程成功至关重要。如果每次在终端输入 ls 命令，都会发生随机动作，那么你将永远也不会相信 ls，而是去找另一种方式来列出目录中的文件。 我们的基础架构必须值得信任","title":"第 6 章：测试云原生基础架构"},{"content":"集群网络是 Kubernetes 的一个核心概念。容器、Pod、服务和外部服务之间的通信必须被考虑在内。默认情况下，很少有网络策略来隔离资源，防止集群被破坏时的横向移动或升级。资源隔离和加密是限制网络行为者在集群内转移和升级的有效方法。\n关键点\n使用网络策略和防火墙来隔离资源。 确保控制平面的安全。 对流量和敏感数据（例如 Secret）进行静态加密。 命名空间 Kubernetes 命名空间是在同一集群内的多个个人、团队或应用程序之间划分集群资源的一种方式。默认情况下，命名空间不会被自动隔离。然而，命名空间确实为一个范围分配了一个标签，这可以用来通过 RBAC 和网络策略指定授权规则。除了网络隔离之外，策略可以限制存储和计算资源，以便在命名空间层面上对 Pod 进行更好的控制。\n默认有三个命名空间，它们不能被删除：\nkube-system（用于 Kubernetes 组件） kube-public（用于公共资源） default（针对用户资源） 用户 Pod 不应该放在 kube-system 或 kube-public 中，因为这些都是为集群服务保留的。可以用 YAML 文件，如  …","relpermalink":"/kubernetes-hardening-guidance/network-separation-and-hardening/","summary":"集群网络是 Kubernetes 的一个核心概念。容器、Pod、服务和外部服务之间的通信必须被考虑在内。默认情况下，很少有网络策略来隔离资源，防止集群被破坏时的横向移动或升级。资源隔离和加密是限制网络行为者在集群内转移和升","title":"网络隔离和加固"},{"content":"现在我们了解了组成 OpenTelemetry 的各个构件，我们应该如何将它们组合成一个强大的生产管道？\n答案取决于你的出发点是什么。OpenTelemetry 是模块化的，设计成可以在各种不同的规模下工作。你只需要使用相关的部分。这就是说，我们已经创建了一个建议的路线图供你遵循。\n安装 OpenTelemetry 客户端 可以单独使用 OpenTelemetry 客户端而不部署收集器。这种基本设置通常是绿地部署的充分起点，无论是测试还是初始生产。OpenTelemetry SDK 可以被配置为直接向大多数可观测性服务传输遥测数据。\n挑选一个导出器 默认情况下，OpenTelemetry 使用 OTLP 导出数据。该 SDK 提供了几种常见格式的导出器。Zipkin、Prometheus、StatsD 等。如果你使用的可观测性后端没有原生支持 OTLP，那么这些其他格式中的一种很可能会被支持。安装正确的导出器并将数据直接发送到你的后端系统。\n安装库仪表 除了 SDK，OpenTelemetry 仪表必须安装在所有 HTTP 客户端、Web 框架、数据库和应用程序的消息队列中。如果这些库 …","relpermalink":"/opentelemetry-obervability/suggested-setups-and-telemetry-pipelines/","summary":"第 7 章：建议的设置和遥测管道","title":"第 7 章：建议的设置和遥测管道"},{"content":"This how-to document will show you how to configure non-HTTP servers with TSB. After reading this document you should be familiar with the usage of the TCP section in IngressGateway and Tier1Gateway API.\nSummary Workflow is exactly the same as configuring HTTP servers in IngressGateway and Tier1Gateway. However, multi-port services are supported for non-HTTP.\nBefore you get started, make sure you: ✓ Familiarize yourself with TSB concepts ✓ Familiarize yourself with onboarding clusters ✓ Create a …","relpermalink":"/tsb/howto/gateway/configure-and-route-nonhttp-traffic/","summary":"This how-to document will show you how to configure non-HTTP servers with TSB. After reading this document you should be familiar with the usage of the TCP section in IngressGateway and Tier1Gateway API.\nSummary Workflow is exactly the same as configuring HTTP servers in IngressGateway and Tier1Gateway. However, multi-port services are supported for non-HTTP.\nBefore you get started, make sure you: ✓ Familiarize yourself with TSB concepts ✓ Familiarize yourself with onboarding clusters ✓ Create a Tenant Setup Four clusters with TSB installed - Management plane, Tier-1 and Tier-2 edge clusters. In Tier-2 clusters, deploy Tier-2 gateways in echo namespace In Tier-1 cluster, deploy Tier-1 gateway in tier1 namespace In both the gateways, the ports 8080 and 9999 should be available (for simplicity, we consider the service and the target ports to be the same).","title":"Configure and route HTTP, non-HTTP (multi-protocol) and multi-port service traffic in TSB"},{"content":"TSB service accounts can be leveraged internally within the platform to manage the cluster onboarding tctl install cluster-service-account and GitOps functionality, as well as externally for the 3rd-party systems to perform the configuration of various TSB features leveraging TSB API interface. The given article will focus on how TSB service accounts can be created and consumed by the external automation software leveraging tctl utility as a handler.\nWorking with TSB service accounts using tctl …","relpermalink":"/tsb/howto/service-accounts/","summary":"TSB service accounts can be leveraged internally within the platform to manage the cluster onboarding tctl install cluster-service-account and GitOps functionality, as well as externally for the 3rd-party systems to perform the configuration of various TSB features leveraging TSB API interface. The given article will focus on how TSB service accounts can be created and consumed by the external automation software leveraging tctl utility as a handler.\nWorking with TSB service accounts using tctl utility Most of the interactions you need for service accounts are already available in the tctl experimental service-account command:\n$ tctl x sa -h Commands to manage TSB service accounts Usage: tctl experimental service-account [command] Aliases: service-account, sa Available Commands: get Get one or multiple service accounts create Creates a new service account delete Deletes a service account gen-key Generate a new key pair for the given service account revoke-key Revoke a given key pair for the given service account token Generate a new token that can be used to authenticate to TSB Create a TSB service account using tctl experimental service-account create.","title":"Leveraging TSB service accounts"},{"content":"This page will explain in details TSB components and external dependencies that you have to provision and connect to be able to run TSB.\nBefore you continue, make sure you’ve:\n✓ Checked TSB architecture and understood the four layers of TSB: data plane (envoy proxies), local control plane (Istio), global control plane (XCP) and management plane (TSB itself).\nManagement Plane Following image shows Management Plane (MP) components\n:::note front-envoy port The default Envoy Gateway (or front-envoy) …","relpermalink":"/tsb/setup/components/","summary":"This page will explain in details TSB components and external dependencies that you have to provision and connect to be able to run TSB.\nBefore you continue, make sure you’ve:\n✓ Checked TSB architecture and understood the four layers of TSB: data plane (envoy proxies), local control plane (Istio), global control plane (XCP) and management plane (TSB itself).\nManagement Plane Following image shows Management Plane (MP) components\n:::note front-envoy port The default Envoy Gateway (or front-envoy) port is 8443 and is user configurable (e.g. changed to 443). If default port is changed then components that communicate via front-envoy need to be adjusted accordingly to match the user-defined value.","title":"TSB Components"},{"content":"在此场景中，你将学习如何使用服务路由在演示应用程序中的不同版本的评论服务之间转移流量。\n先决条件 在继续之前，请确保你已完成以下任务：\n熟悉 TSB 概念。 安装 TSB 演示环境。 部署 Istio Bookinfo 示例应用程序。 创建租户、工作区、配置组、权限、入口网关，并检查服务拓扑和指标。 仅提供 v1 服务 使用用户界面 在左侧面板的“租户”下，选择“工作区”。 在 bookinfo-ws 工作区卡上，单击“流量组”。 单击你之前创建的 bookinfo-traffic 流量组。 选择流量设置选项卡。 在流量设置下，单击服务路由。 单击“添加新…”以使用默认名称 default-serviceroute 创建新的服务路由。 将其重命名为 bookinfo-traffic-reviews 。 将服务设置为 bookinfo/reviews.bookinfo.svc.cluster.local 。 展开 bookinfo-traffic-reviews。 展开子集。 单击“添加新子集…”以创建一个名为 subset-0 的新子集。 点击 subset-0 ： 将名称设置为 v1 …","relpermalink":"/tsb/quickstart/traffic-shifting/","summary":"在此场景中，你将学习如何使用服务路由在演示应用程序中的不同版本的评论服务之间转移流量。 先决条件 在继续之前，请确保你已完成以下任务： 熟悉 TSB 概念。 安装 TSB 演示环境。 部署 Istio Bookinfo 示例应用程序。 创建租户、工作区、配","title":"流量转移"},{"content":"Tetrate Service Bridge (TSB) Tetrate Service Bridge (TSB) TSB 是一个服务网格管理平面，旨在提供对基础设施的集中控制。它映射到你的组织结构，使你能够管理资源、访问权限、网络和安全性。\n组织 组织代表管理共享基础设施的公司，由多个租户和工作区组成。\n租户 租户是组织内共享资源和访问权限的子组，例如团队或部门。\n工作空间 工作区是团队管理其命名空间的定义区域。它隔离与跨不同集群的特定团队的命名空间相关的配置。\n服务 具有唯一身份和身份验证的网络可访问目的地。\n团体 工作区中资源的逻辑分组，分类为网关、流量或安全组。\n应用 公开 API 的服务的逻辑分组，在租户的工作区中进行管理。它使开发人员能够配置服务行为和访问条件。\n应用程序接口 应用程序公开的一组端点，使开发人员能够使用 OpenAPI 文档配置行为。\n用户 与 TSB 相关的实体，包括人类和非人类实体。可以将用户组织成团队并通过 RBAC 分配访问权限。\n团队 一组用户使用 RBAC 分配对资源的访问权限。建议团队进行访问分配，增强安全性。\n架构组件 管理平面 在 TSB  …","relpermalink":"/tsb/concepts/terminology/","summary":"Tetrate Service Bridge (TSB) Tetrate Service Bridge (TSB) TSB 是一个服务网格管理平面，旨在提供对基础设施的集中控制。它映射到你的组织结构，使你能够管理资源、访问权限、网络和安全性。 组织 组织代表管理共享基础设施的公司，由多个租户和工作区组成。 租户","title":"术语"},{"content":"我删除/损坏了我的存储库，无法删除我的应用程序。 如果 Argo CD 无法生成清单，则无法删除应用程序。你需要：\n恢复/修复你的存储库。 使用cascade=false删除应用程序，然后手动删除资源。 为什么我的应用程序在成功同步后仍然处于 OutOfSync 状态？ 请查看 差异比较 文档，了解资源可能处于 OutOfSync 状态的原因，以及配置 Argo CD 忽略字段的方法 当存在不同之处时。\n为什么我的应用程序一直处于“Progressing”状态？ Argo CD 为几种标准 Kubernetes 类型提供健康状态。 Ingress、StatefulSet 和 SealedSecret 类型存在已知问题，可能会导致健康检查返回 Progressing 状态而不是 Healthy。\nIngress 如果 status.loadBalancer.ingress 列表是非空的，并且至少有一个值为 hostname 或 IP，则被视为健康状态。一些 Ingress 控制器（contour、traefik）不会更新 status.loadBalancer.ingress 字段，这会 …","relpermalink":"/argo-cd/faq/","summary":"我删除/损坏了我的存储库，无法删除我的应用程序。 如果 Argo CD 无法生成清单，则无法删除应用程序。你需要： 恢复/修复你的存储库。 使用cascade=false删除应用程序，然后手动删除资源。 为什么我的应用程序","title":"FAQ"},{"content":"你可以使用Traefik Proxy来进行 Traffic Management with Argo Rollouts。\nTraefikService是支持Traefik 作为 ingress 时加权轮询负载均衡和Traefik 作为 ingress 时流量镜像能力的对象。\n如何将 TraefikService 与 Argo Rollouts 集成，作为加权轮询负载均衡器 首先，我们需要使用 TraefikService 对象的加权轮询负载平衡能力创建 TraefikService 对象。\napiVersion: traefik.containo.us/v1alpha1 kind: TraefikService metadata: name: traefik-service spec: weighted: services: - name: stable-rollout # 为稳定应用程序版本创建的 k8s 服务名称 port: 80 - name: canary-rollout # 为新应用程序版本创建的 k8s 服务名称 port: 80 请注意，我们不指定“weight”字段。它 …","relpermalink":"/argo-rollouts/traffic-management/traefik/","summary":"你可以使用Traefik Proxy来进行 Traffic Management with Argo Rollouts。 TraefikService是支持Traefik 作为 ingress 时加权轮询负载均衡和Traefik 作为 ingress 时流量镜像能力的对象。 如何将 TraefikService 与 Argo Rollouts 集成","title":"Traefik"},{"content":"背景 根据集群的配置，蓝绿部署（或使用流量管理的金丝雀部署）可能会导致新创建的 Pod 在部署新版本后重新启动。这可能会导致问题，特别是对于无法快速启动或无法正常退出的应用程序。\n这种行为发生的原因是集群自动缩放器想要缩小创建的额外容量以支持运行在双倍容量中的部署。当节点缩小时，它所拥有的 Pod 会被删除并重新创建。这通常发生在部署具有自己的专用实例组时，因为部署对集群自动缩放器的影响更大。因此，具有大量共享节点的集群较少经历这种行为。\n例如，此处有一个正在运行的部署，它有 8 个 Pod 分布在 2 个节点上。每个节点最多可容纳 6 个 Pod：\n原 Rollout 正在运行，跨越两个节点 当部署的 spec.template 发生变化时，控制器会创建一个新的 ReplicaSet，其中包含规范更新和 Pod 总数翻倍的版本。在这种情况下，Pod 的数量增加到 16。\n由于每个节点只能容纳 6 个 Pod，所以集群自动缩放器必须将节点数增加到 3 个来容纳所有 16 个 Pod。Pod 在节点之间的分布如下所示：\nRollout 容量扩大到两倍 部署完成后，旧版本会被缩小。这会使集 …","relpermalink":"/argo-rollouts/rollout/anti-affinity/","summary":"背景 根据集群的配置，蓝绿部署（或使用流量管理的金丝雀部署）可能会导致新创建的 Pod 在部署新版本后重新启动。这可能会导致问题，特别是对于无法快速启动或无法正常退出的应用程序。 这种行为发生的原因是集群自动缩放","title":"反亲和性"},{"content":"本章解释了如何实施使用 SPIFFE 身份的授权策略。\n在 SPIFFE 的基础上建立授权 SPIFFE 专注于软件安全加密身份的发布和互操作性，但正如本书前面提到的，它并不直接解决这些身份的使用或消费问题。\nSPIFFE 经常作为一个强大的授权系统的基石，而 SPIFFE ID 本身在这个故事中扮演着重要角色。在这一节中，我们将讨论使用 SPIFFE 来建立授权的选择。\n认证与授权（AuthN Vs AuthZ） 一旦一个工作负载有了安全的加密身份，它就可以向其他服务证明其身份。向外部服务证明身份被称为认证（Authentication）。一旦通过认证，该服务就可以选择允许哪些行动。这个过程被称为授权（Authorization）。\n在一些系统中，任何被认证的实体也被授权。因为 SPIFFE 会在服务启动时自动授予其身份，所以清楚地认识到并不是每一个能够验证自己的实体都应该被授权，这一点至关重要。\n授权类型 有很多方法可以对授权进行建模。最简单的解决方案是在每个资源上附加一个授权身份的允许列表（allowlist）。然而，随着我们的探索，我们会注意到在处理生态系统的规模和复杂性时，允 …","relpermalink":"/spiffe/using-spiffe-identities-to-inform-authorization/","summary":"本章解释了如何实施使用 SPIFFE 身份的授权策略。 在 SPIFFE 的基础上建立授权 SPIFFE 专注于软件安全加密身份的发布和互操作性，但正如本书前面提到的，它并不直接解决这些身份的使用或消费问题。 SPIFFE 经常作为一个强大的授权系统的基石，","title":"使用 SPIFFE 身份通知授权"},{"content":"验证安装 检查 DaemonSet 的状态并验证所有需要的实例都处于 ready 状态：\n$ kubectl --namespace kube-system get ds NAME DESIRED CURRENT READY NODE-SELECTOR AGE cilium 1 1 0 \u0026lt;none\u0026gt; 3s 在此示例中，我们看到 1 个期望的状态，0 个 ready 状态。这表明有问题。下一步是通过在 k8s-app=cilium 标签上匹配列出所有 cilium pod，并根据每个 pod 的重启次数对列表进行排序，以便轻松识别失败的 pod：\n$ kubectl --namespace kube-system get pods --selector k8s-app=cilium \\ --sort-by=\u0026#39;.status.containerStatuses[0].restartCount\u0026#39; NAME READY STATUS RESTARTS AGE cilium-813gf 0/1 CrashLoopBackOff 2 44s cilium-813gf pod …","relpermalink":"/cilium-handbook/kubernetes/troubleshooting/","summary":"验证安装 检查 DaemonSet 的状态并验证所有需要的实例都处于 ready 状态： $ kubectl --namespace kube-system get ds NAME DESIRED CURRENT READY NODE-SELECTOR AGE cilium 1 1 0 \u003cnone\u003e 3s 在此示例中，我们看到 1 个期望的状态，0 个 ready 状态。这表明有问题。下一步是通过在 k8s-app=cilium 标签上匹配列出所有 cilium pod，并根","title":"故障排除"},{"content":"容器存储接口（Container Storage Interface），简称 CSI，CSI 试图建立一个行业标准接口的规范，借助 CSI 容器编排系统（CO）可以将任意存储系统暴露给自己的容器工作负载。有关详细信息，请查看设计方案。\ncsi 卷类型是一种 out-tree（即跟其它存储插件在同一个代码路径下，随 Kubernetes 的代码同时编译的）的 CSI 卷插件，用于 Pod 与在同一节点上运行的外部 CSI 卷驱动程序交互。部署 CSI 兼容卷驱动后，用户可以使用 csi 作为卷类型来挂载驱动提供的存储。\nCSI 持久化卷支持是在 Kubernetes v1.9 中引入的，作为一个 alpha 特性，必须由集群管理员明确启用。换句话说，集群管理员需要在 apiserver、controller-manager 和 kubelet 组件的“--feature-gates =”标志中加上“CSIPersistentVolume = true”。\nCSI 持久化卷具有以下字段可供用户指定：\ndriver：一个字符串值，指定要使用的卷驱动程序的名称。必须少于 63 个字符，并以一个 …","relpermalink":"/kubernetes-handbook/architecture/open-interfaces/csi/","summary":"容器存储接口（Container Storage Interface），简称 CSI，CSI 试图建立一个行业标准接口的规范，借助 CSI 容器编排系统（CO）可以将任意存储系统暴露给自己的容器工作负载。有关详细信息，请查看设计","title":"容器存储接口（CSI）"},{"content":"DevSecOps 的好处包括：\n各个 IT 团队之间，特别是开发人员、运维和安全团队以及其他利益相关者之间更好的沟通和协作。导致 更好的生产力。 简化软件开发、交付和部署过程 —— 由于自动化，停机时间减少，发布时间加快，基础设施和运维成本降低，效率提高。 通过实施零信任来减少攻击面，这也限制了横向移动，从而防止攻击升级。具有现代行为预防能力的持续监控进一步促进了这一点。 安全优势。通过对每个请求的验证监控、警报和反馈机制来提高安全性，因为可观测性是代码。这些将在以下段落中详细描述。具体的能力包括： a. 运行时：杀死恶意容器。 b. 反馈：由于一个错误的程序更新了代码并重新触发了管道，所以反馈到了正确的存储库。 c. 监测新的和终止的服务，并调整相关服务（如服务代理）。 d. 启用安全断言。不可绕过 —— 通过在同一空间执行的代理、安全会话、强大的认证和授权以及安全的状态转换。 启用持续授权操作（C-ATO），在本节末尾详细描述。 对每个请求的验证和上述的反馈机制将在下面进一步描述： 每个请求的验证。来自用户或客户端应用程序（服务）的每个请求都要经过验证和授权（使用 OPA 或任何 …","relpermalink":"/service-mesh-devsecops/implement/benefits-of-devsecops-primitives-to-application-security-in-the-service-mesh/","summary":"DevSecOps 的好处包括： 各个 IT 团队之间，特别是开发人员、运维和安全团队以及其他利益相关者之间更好的沟通和协作。导致 更好的生产力。 简化软件开发、交付和部署过程 —— 由于自动化，停机时间减少，发布时间加快，基础设施和","title":"4.9 DevSecOps 原语对服务网格中应用安全的好处"},{"content":"云原生应用程序依赖基础架构才能运行，反过来说，云原生基础架构也需要云原生应用程序来维持。\n对于传统基础架构，维护和升级基本上都是由人工完成。可能是在单机上手动运行服务或使用自动化工具定义基础结构和应用程序的快照。\n但是，如果基础架构可以由应用程序管理，同时基础架构又可以管理应用程序，那么基础架构工具就会成为另一种应用程序。工程师对于基础架构的责任可以用调解器模式表示，内置于该基础架构上运行的应用程序中。\n我们花了三章来说明如何构建可以管理基础架构的应用程序。本章将介绍如何在基础架构上运行云原生应用或其它任何应用。\n如前所述，保持基础架构和应用程序的简单非常重要。解决应用程序复杂性最常用的方法就是把应用程序分解成小的，易于理解的组件。通常通过创建单一职责的服务来实现，或者将代码分解为一系列事件触发的函数。\n随着小型、可部署单元的扩容成多份即使是最自动化的基础架构也可能被压垮。管理大量应用程序的唯一方法是让它们承担第 1 章中所述的功能性操作。应用程序需要在可以按规模管理之前变成原生云。\n学习完本章不会帮助您建下一个伟大的应用程序，但您将了解一些基础知识，这块可以让您的应用程序在云原生基础 …","relpermalink":"/cloud-native-infra/managing-cloud-native-applications/","summary":"云原生应用程序依赖基础架构才能运行，反过来说，云原生基础架构也需要云原生应用程序来维持。 对于传统基础架构，维护和升级基本上都是由人工完成。可能是在单机上手动运行服务或使用自动化工具定义基础结构和应用程","title":"第 7 章：管理云原生应用程序"},{"content":"认证和授权是限制访问集群资源的主要机制。如果集群配置错误，网络行为者可以扫描知名的 Kubernetes 端口，访问集群的数据库或进行 API 调用，而不需要经过认证。用户认证不是 Kubernetes 的一个内置功能。然而，有几种方法可以让管理员在集群中添加认证。\n认证 管理员必须向集群添加一个认证方法，以实现认证和授权机制。\nKubernetes 集群有两种类型的用户：服务账户和普通用户账户。服务账户代表 Pod 处理 API 请求。认证通常由 Kubernetes 通过 ServiceAccount Admission Controller 使用承载令牌自动管理。不记名令牌被安装到 Pod 中约定俗成的位置，如果令牌不安全，可能会在集群外使用。正因为如此，对 Pod Secret 的访问应该限制在那些需要使用 Kubernetes RBAC 查看的人身上。对于普通用户和管理员账户，没有自动的用户认证方法。管理员必须在集群中添加一个认证方法，以实现认证和授权机制。\nKubernetes 假设由一个独立于集群的服务来管理用户认证。Kubernetes 文档中列出了几种实现用户认证的方 …","relpermalink":"/kubernetes-hardening-guidance/authentication-and-authorization/","summary":"认证和授权是限制访问集群资源的主要机制。如果集群配置错误，网络行为者可以扫描知名的 Kubernetes 端口，访问集群的数据库或进行 API 调用，而不需要经过认证。用户认证不是 Kubernetes 的一个内置功能。然而，有几种方法可以让管理员在集","title":"认证和授权"},{"content":"上线一个新的遥测系统可能是一项复杂的工作。它需要整个工程组织的支持，不能一蹴而就。不要低估这可能会产生的问题！\n在大型组织中，通常有许多服务团队负责系统的不同部分。通常情况下，每个团队都需要付出一定的努力来使他们所管理的服务得到充分的工具化。而这些团队都有自己积压的工作，他们当然希望能够优先处理这些工作。\n不幸的是，可观测性计划在开始提供价值和证明其价值之前就会耗尽人的耐心。但通过仔细的计划和协调，这种情况是可以避免的。\n主要目标 在推广 OpenTelemetry 时，重要的是要记住，任何基于分布式追踪的可观测性系统都需要对参与事务的每个服务进行检测，以提供最大价值。如果只有部分服务被检测到，那么追踪就会被分割成小的、不相连的部分。\n这种散乱的仪表的结果是不可取的。这种情况——不一致的仪表和断裂的追踪是你想要避免的主要事情。如果追踪是断开的，运维人员仍然需要在他们的头脑中把所有的东西拼凑起来，以获得他们系统的情况。更糟糕的是，自动分析工具可以使用的数据非常有限。与人类运维人员不同，他们可以运用直觉，跳出框框来思考问题，而分析工具却只能使用他们得到的数据。由于数据有限，他们提供有用的见 …","relpermalink":"/opentelemetry-obervability/roll-out/","summary":"第 8 章：如何在组织中推广 OpenTelemetry","title":"第 8 章：如何在组织中推广 OpenTelemetry"},{"content":"An Application in TSB represents a set of logical groupings of Services that are related to each other and expose a set of APIs that implement a complete set of business logic.\nTSB can leverage OpenAPI annotations when configuring API runtime policies. In this document you will enable authorization via Open Policy Agent (OPA), as well as rate limiting through an external service. Each request will need to go through basic authorization, and for each valid user a rate limit policy will be …","relpermalink":"/tsb/howto/gateway/application-gateway-with-openapi-annotations/","summary":"An Application in TSB represents a set of logical groupings of Services that are related to each other and expose a set of APIs that implement a complete set of business logic.\nTSB can leverage OpenAPI annotations when configuring API runtime policies. In this document you will enable authorization via Open Policy Agent (OPA), as well as rate limiting through an external service. Each request will need to go through basic authorization, and for each valid user a rate limit policy will be enforced.\nBefore you get started, make sure you: ✓ Familiarize yourself with TSB concepts ✓ Familiarize yourself with Open Policy Agent (OPA) ✓ Familiarize yourself with Envoy external authorization and rate limit ✓ Install the TSB demo environment ✓ Familiarize yourself with Istio Bookinfo sample app ✓ Create a Tenant","title":"Configuring Application Gateways Using OpenAPI Annotations"},{"content":"What are Security Domains? Security Domains allow you to create configuration groupings across the TSB hierarchy from anywhere in the configuration hierarchy - Tenant, Workspace or Security Group. Think of a securityDomain as a name that can be attached to any of these TSB resources, and which can then be used in TSB rules.\nOnce a resource is identified with a securityDomain, the security domain can be used as a source or target when creating rules. This allows the Operator to establish a set of …","relpermalink":"/tsb/howto/security-domains/","summary":"What are Security Domains? Security Domains allow you to create configuration groupings across the TSB hierarchy from anywhere in the configuration hierarchy - Tenant, Workspace or Security Group. Think of a securityDomain as a name that can be attached to any of these TSB resources, and which can then be used in TSB rules.\nOnce a resource is identified with a securityDomain, the security domain can be used as a source or target when creating rules. This allows the Operator to establish a set of requirements continuously across the Tetrate Service Bridge (TSB) hierarchy as new objects are created.\nWhen should I use Security Domains?","title":"Creating Security Domains"},{"content":"If your environment has strict network policies that prevent any unauthorized communication between two namespaces, you may need to add one or more exceptions to your network policies to allow communication between the sidecars and the local Istio Control Plane, as well as between the local Istio Control Plane and the TSB management plane.\nThe following information can be used to derive the appropriate set of firewall rules.\nCommunication between TSB, Control Plane and Workloads Between Istio …","relpermalink":"/tsb/setup/firewall-information/","summary":"If your environment has strict network policies that prevent any unauthorized communication between two namespaces, you may need to add one or more exceptions to your network policies to allow communication between the sidecars and the local Istio Control Plane, as well as between the local Istio Control Plane and the TSB management plane.\nThe following information can be used to derive the appropriate set of firewall rules.\nCommunication between TSB, Control Plane and Workloads Between Istio and TSB :::note TSB Load Balancer port TSB Load Balancer (also known as front-envoy) has default port 8443. This port value is user configurable.","title":"Firewall Information"},{"content":"在此场景中，你将了解如何使用 TSB 安全设置来限制来自工作区外部的访问。这有助于通过控制服务之间的通信来增强环境的安全性。\n先决条件 在继续之前，请确保你已完成以下任务：\n熟悉 TSB 概念。 安装 TSB 演示环境。 部署 Istio Bookinfo 示例应用程序。 创建租户、工作区和配置组。 为团队和用户配置权限。 设置入口网关。 使用可观察性工具检查服务拓扑和指标。 配置流量转移。 部署 sleep 服务 首先，让我们在不属于 bookinfo 应用程序工作区的另一个命名空间中部署“睡眠”服务。这将用于测试安全设置。\n创建以下 sleep.yaml 文件：\nsleep.yaml # Copyright Istio Authors # # Licensed under the Apache License, Version 2.0 (the \u0026#34;License\u0026#34;); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at …","relpermalink":"/tsb/quickstart/security/","summary":"在此场景中，你将了解如何使用 TSB 安全设置来限制来自工作区外部的访问。这有助于通过控制服务之间的通信来增强环境的安全性。 先决条件 在继续之前，请确保你已完成以下任务： 熟悉 TSB 概念。 安装 TSB 演示环境。 部署 Istio Bookinfo 示例","title":"安全"},{"content":"Argo Rollouts 将始终响应 Rollouts 资源的更改，无论更改是如何进行的。这意味着 Argo Rollouts 与你可能用来管理部署的所有模板解决方案兼容。\nArgo Rollouts 清单可以使用 Helm 包管理器进行管理。如果你的 Helm Chart 包含 Rollout 资源，那么一旦你安装或升级 Chart，Argo Rollouts 将接管并启动渐进式交付流程。\n以下是使用 Helm 管理的 Rollout 示例：\napiVersion: argoproj.io/v1alpha1 kind: Rollout metadata: name: {{ template \u0026#34;helm-guestbook.fullname\u0026#34; . }} labels: app: {{ template \u0026#34;helm-guestbook.name\u0026#34; . }} chart: {{ template \u0026#34;helm-guestbook.chart\u0026#34; . }} release: {{ .Release.Name }} heritage: {{ .Release.Service }} spec: …","relpermalink":"/argo-rollouts/rollout/helm/","summary":"Argo Rollouts 将始终响应 Rollouts 资源的更改，无论更改是如何进行的。这意味着 Argo Rollouts 与你可能用来管理部署的所有模板解决方案兼容。 Argo Rollouts 清单可以使用 Helm 包管理器进行管理。如果你的 Helm Chart 包含 Rollout 资源，那么一旦你安装或升级 Chart，A","title":"将 Argo Rollouts 和 Helm 一起使用"},{"content":"🔔 提示：自 Argo Rollouts v1.2 起提供了多个 trafficRouting。\n多个提供程序的使用旨在涵盖一些情况，例如我们必须在南北和东西流量路由或需要使用多个提供程序的任何混合架构的情况下。\n何时可以使用多个提供程序的示例 避免在 Ingress 控制器上注入 Sidecars 这是服务网格的常见要求，通过使用多个 trafficRoutings，你可以利用南北交通转移到 NGiNX 和西东交通转移到 SMI，避免将 Ingress 控制器添加到网格中。\n避免操作 Ingress 中的主机 Header 将一些 Ingress 控制器添加到网格中的另一个常见副作用是使用这些网格主机标头将其指向网格主机名以进行路由。\n避免大爆炸 这发生在存在的机群中，其中停机时间非常短或几乎不可能。为了避免大爆炸采用，使用多个提供程序可以缓解团队如何逐步实施新技术。例如，现有机群正在使用提供程序（例如大使），已经在其推出的一部分中使用金丝雀方式进行南北方向的金丝雀测试，可以逐渐实现更多提供程序，例如 Istio，SMI 等。\n混合方案 在这种情况下，它非常类似于避免大爆炸，无论是作 …","relpermalink":"/argo-rollouts/traffic-management/mixed/","summary":"🔔 提示：自 Argo Rollouts v1.2 起提供了多个 trafficRouting。 多个提供程序的使用旨在涵盖一些情况，例如我们必须在南北和东西流量路由或需要使用多个提供程序的任何混合架构的情况下。 何时可以使用多个提供程序的示","title":"多提供方"},{"content":"什么是实验 CRD？ 实验 CRD 允许用户对一个或多个 ReplicaSet 进行短暂运行。除了运行短暂 ReplicaSet 外，实验 CRD 还可以在 ReplicaSet 旁边启动 AnalysisRuns。通常，这些 AnalysisRun 用于确认新的 ReplicaSet 是否按预期运行。\n如果设置了权重（需要流量路由）或该实验的 Service 属性，则还会生成一个服务，用于将流量路由到实验 ReplicaSet。\n实验用例 用户想要运行应用程序的两个版本以进行 Kayenta 风格的分析来启用。实验 CRD 基于实验的 spec.templates 字段创建 2 个 ReplicaSet（基线和金丝雀），并等待两者都健康。经过一段时间后，实验会缩小 ReplicaSet 的规模，用户可以开始 Kayenta 分析运行。 用户可以使用实验来启用 A/B/C 测试，通过为不同版本的应用程序启动多个实验来进行长时间测试。每个实验都有一个 PodSpec 模板，定义用户要运行的特定版本。实验允许用户同时启动多个实验，并保持每个实验的独立性。 使用不同的标签启动现有应用程序的新 …","relpermalink":"/argo-rollouts/experiment/","summary":"什么是实验 CRD？ 实验 CRD 允许用户对一个或多个 ReplicaSet 进行短暂运行。除了运行短暂 ReplicaSet 外，实验 CRD 还可以在 ReplicaSet 旁边启动 AnalysisRuns。通常，这些 AnalysisRun 用于确认新的 ReplicaSet 是否按预期运行。 如果设置了权重（需要流量路由）","title":"实验 CRD"},{"content":"本章将 SPIFFE 与其他解决类似问题的技术进行了比较。\n简介 SPIFFE 和 SPIRE 所解决的问题并不新鲜。每一个分布式系统都必须有某种形式的身份认证才是安全的。网络公钥基础设施、Kerberos/Active Directory、OAuth、秘密存储和服务网格就是例子。\n然而，这些现有的身份识别形式并不适合用于识别组织内的内部服务。网络 PKI 的实施具有挑战性，对于典型的内部部署来说也是不安全的。Kerberos，Active Directory 的认证组件，需要一个永远在线的票证授予服务器，并且没有任何同等的证明。服务网格、秘密管理器和覆盖网络都解决了服务身份的部分难题，但并不完整。SPIFFE 和 SPIRE 是目前服务身份问题的唯一完整解决方案。\n网络公钥基础设施 网络公钥基础设施（Web PKI）是广泛使用的从我们的网络浏览器连接到安全网站的方法。它利用 X.509 证书来断言用户正在连接到他们打算访问的网站。由于你可能对这种模式很熟悉，所以有理由问：为什么我们不能在我们的组织内使用 Web PKI 进行服务识别？\n在传统的 Web PKI 中，证书的发放和更新完 …","relpermalink":"/spiffe/comparing-spiffe-to-other-security-technologies/","summary":"本章将 SPIFFE 与其他解决类似问题的技术进行了比较。 简介 SPIFFE 和 SPIRE 所解决的问题并不新鲜。每一个分布式系统都必须有某种形式的身份认证才是安全的。网络公钥基础设施、Kerberos/Active Directory、O","title":"9. SPIFFE 与其他安全技术对比"},{"content":"在参考平台中，整个应用系统的运行状态或执行状态是由于基础设施代码（例如，用于服务间通信的网络路由、资源配置代码）、策略代码（例如，指定认证和授权策略的代码）和会话管理代码（例如，建立 mTLS 会话的代码、生成 JWT 令牌的代码）的执行的组合，这些代码由可观测性作为代码的执行所揭示。服务网格的可观测性代码在运行期间将基础设施、策略和会话管理代码的执行输出转发给各种监控工具，这些工具产生适用的指标和日志聚合工具以及追踪工具，这些工具又将其输出转发给集中式仪表板。作为这些工具输出的组成部分的分析，使系统管理员能够获得整个应用系统运行时状态的综合全局视图。正是通过持续监控和零信任设计功能实现的 DevSecOps 平台的运行时性能，为云原生应用提供了所有必要的安全保障。\n在 DevSecOps 管道中，实现持续 ATO 的活动是：\n检查合规的代码。可以检查以下代码是否符合《风险管理框架》的规定 (a) IaC：生成网络路线，资源配置 (b) 策略即代码：对 AuthN 和 AuthZ 策略进行编码 (c) 会话管理代码：mTLS 会话，JWT 令牌 (d) 可观测性代码\n具体的风险评估功能 …","relpermalink":"/service-mesh-devsecops/implement/leveraging-devsecops-for-continuous-authorization-to-operate-c-ato/","summary":"在参考平台中，整个应用系统的运行状态或执行状态是由于基础设施代码（例如，用于服务间通信的网络路由、资源配置代码）、策略代码（例如，指定认证和授权策略的代码）和会话管理代码（例如，建立 mTLS 会话的代码、生成","title":"4.10 利用 DevSecOps 进行持续授权操作（C-ATO）"},{"content":"我们讨论过只能使用云原生应用程序来创建基础架构。同时基础架构也负责运行这些应用程序。\n运行由应用程序配置和控制的基础架构可以轻松得扩展。我们通过学习如何通过扩展应用的方式来扩展基础架构。我们还通过学习如何保护应用程序来保护基础架构。\n在动态环境中，无法通过增加人手来管理这样的复杂性，同样也不能靠增加人手来处理策略和安全问题。\n这意味着，就像我们必须创建通过协调器模式强制执行基础架构状态的应用程序一样，我们需要创建实施安全策略的应用程序。在创建应用程序以执行的策略之前，我们需要以机器可解析的格式编写策略。\n策略即代码 由于策略没有明确定义的技术实现，所以策略难以纳入代码。它更多地关注业务如何实现而不是谁来实现。\n如何实现和谁来实现都会经常变化，但是实现方式变化更频繁且不容易被抽象化。它也是组织特定的，可能需要了解创建基础架构人员的沟通结构的具体细节。\n策略需要应用于应用程序生命周期的多个阶段。正如我们在第 7 章中所讨论的，应用程序通常有三个阶段：部署、运行和退役。\n部署阶段将在应用程序和基础架构变更发布之前先应用策略。这将包括部署规则和一致性测试。运行阶段将包括持续的遵守和执行访问控制 …","relpermalink":"/cloud-native-infra/securing-applications/","summary":"我们讨论过只能使用云原生应用程序来创建基础架构。同时基础架构也负责运行这些应用程序。 运行由应用程序配置和控制的基础架构可以轻松得扩展。我们通过学习如何通过扩展应用的方式来扩展基础架构。我们还通过学习如","title":"第 8 章：保护应用程序"},{"content":"日志记录了集群中的活动。审计日志是必要的，这不仅是为了确保服务按预期运行和配置，也是为了确保系统的安全。系统性的审计要求对安全设置进行一致和彻底的检查，以帮助识别潜在威胁。Kubernetes 能够捕获集群操作的审计日志，并监控基本的 CPU 和内存使用信息；然而，它并没有提供深入的监控或警报服务。\n关键点\n在创建时建立 Pod 基线，以便能够识别异常活动。 在主机层面、应用层面和云端（如果适用）进行日志记录。 整合现有的网络安全工具，进行综合扫描、监控、警报和分析。 设置本地日志存储，以防止在通信失败的情况下丢失。 日志 在 Kubernetes 中运行应用程序的系统管理员应该为其环境建立一个有效的日志、监控和警报系统。仅仅记录 Kubernetes 事件还不足以了解系统上发生的行动的全貌。还应在主机级、应用级和云上（如果适用）进行日志记录。而且，这些日志可以与任何外部认证和系统日志相关联，以提供整个环境所采取的行动的完整视图，供安全审计员和事件响应者使用。\n在 Kubernetes 环境中，管理员应监控 / 记录以下内容：\nAPI 请求历史 性能指标 部署情况 资源消耗 操作系统调 …","relpermalink":"/kubernetes-hardening-guidance/logging/","summary":"日志记录了集群中的活动。审计日志是必要的，这不仅是为了确保服务按预期运行和配置，也是为了确保系统的安全。系统性的审计要求对安全设置进行一致和彻底的检查，以帮助识别潜在威胁。Kubernetes 能够捕获","title":"日志审计"},{"content":"OpenTelemetry 是一个大型项目。OpenTelemetry 项目的工作被划分为特殊兴趣小组（SIG）。虽然所有的项目决策最终都是通过 GitHub issue 和 pull request 做出的，但 SIG 成员经常通过 CNCF 的官方 Slack 保持联系，而且大多数 SIG 每周都会在 Zoom 上会面一次。\n任何人都可以加入一个 SIG。要想了解更多关于当前 SIG、项目成员和项目章程的细节，请查看 GitHub 上的 OpenTelemetry 社区档案库。\n规范 OpenTelemetry 是一个规范驱动的项目。OpenTelemetry 技术委员会负责维护该规范，并通过管理规范的 backlog 来指导项目的发展。\n小的改动可以以 GitHub issue 的方式提出，随后的 pull request 直接提交给规范。但是，对规范的重大修改是通过名为 OpenTelemetry Enhancement Proposals（OTEPs）的征求意见程序进行的。\n任何人都可以提交 OTEP。OTEP 由技术委员会指定的具体审批人进行审查，这些审批人根据其专业领域进 …","relpermalink":"/opentelemetry-obervability/organization/","summary":"附录 A：OpenTelemetry 项目组织","title":"附录 A：OpenTelemetry 项目组织"},{"content":" An App Ingress is an L7 ingress that allows application developers to directly harness the power of Envoy that is available. Unlike the other types of Ingress that are available in TSB, configuring it does not require admin privileges, and exposes the features of Envoy proxy in such a way that it’s easier for application developers to define intent.\nThe App Ingress is a stripped down version of Istio using Istiod as the control plane component and Istio IngressGateway (i.e. Envoy proxy) as the …","relpermalink":"/tsb/howto/gateway/app-ingress/","summary":"An App Ingress is an L7 ingress that allows application developers to directly harness the power of Envoy that is available. Unlike the other types of Ingress that are available in TSB, configuring it does not require admin privileges, and exposes the features of Envoy proxy in such a way that it’s easier for application developers to define intent.\nThe App Ingress is a stripped down version of Istio using Istiod as the control plane component and Istio IngressGateway (i.e. Envoy proxy) as the data plane component. The App Ingress is deployed per application in the namespace owned by the application, and can only consume Istio configurations from the namespace where it is deployed in.","title":"App Ingress"},{"content":"Istio 隔离边界可以在 Kubernetes 集群内或跨多个集群中运行多个由 TSB（Tetrate Service Bridge）管理的 Istio 环境。这些 Istio 环境在服务发现和配置分发方面彼此隔离。隔离边界带来了以下几个好处：\n强大的网络隔离默认提供了在高度受管制的环境中严格且易于演示的安全性。 在集群内运行不同的 Istio 版本允许你在同一集群中支持传统和现代应用程序。 金丝雀发布提供了在测试和部署 TSB 升级时的灵活性。 安装 升级\n要从非修订的控制平面升级到修订的控制平面，请按照 非修订到修订的升级 中提到的步骤进行操作。 OpenShift\n如果你使用 OpenShift，请将以下 kubectl 命令替换为 oc。 对于全新安装，你可以按照使用 tctl 或 helm 来加入控制平面集群的标准步骤，以及以下更改进行操作：\n通过将 ISTIO_ISOLATION_BOUNDARIES 设置为 true 在 TSB 控制平面 Operator 中启用隔离边界。 在 ControlPlane CR 或控制平面 Helm 值中添加隔离边界定义。 在以下示例中， …","relpermalink":"/tsb/setup/isolation-boundaries/","summary":"Istio 隔离边界可以在 Kubernetes 集群内或跨多个集群中运行多个由 TSB（Tetrate Service Bridge）管理的 Istio 环境。这些 Istio 环境在服务发现和配置分发方面彼此隔离。隔离边界带来了以下几个好处： 强大的网络隔离默认提供了在高","title":"Istio 隔离边界"},{"content":"在本部分中，你将创建一个名为 bookinfo 的应用程序并向其附加一个 API。 API 将根据 OpenAPI 规范进行配置。\nTSB 中的应用程序是服务的逻辑分组，这些服务公开了应用程序的不同功能和用例。它们属于租户，可用于观察服务的行为并配置如何使用这些服务及其 API。应用程序公开一组 API 对象，这些对象定义可用内容和使用条件。\n创建应用程序 先决条件 在继续阅读本指南之前，请确保你已完成以下步骤：\n熟悉 TSB 概念 安装TSB演示环境 部署 Istio Bookinfo 示例应用程序 创建租户 创建工作区 配置权限 设置 Ingress 网关 检查服务拓扑和指标 配置流量转移 配置安全控制 创建应用程序 创建 application.yaml 文件：\napiVersion: application.tsb.tetrate.io/v2 kind: Application metadata: organization: tetrate tenant: tetrate name: bookinfo spec: displayName: Bookinfo …","relpermalink":"/tsb/quickstart/apps/","summary":"在本部分中，你将创建一个名为 bookinfo 的应用程序并向其附加一个 API。 API 将根据 OpenAPI 规范进行配置。 TSB 中的应用程序是服务的逻辑分组，这些服务公开了应用程序的不同功能和用例。它们属于租户，可用于观察服务的行为并配置如","title":"创建应用程序和 API"},{"content":"可以通过使用转换器配置来扩展 Kustomize 以理解 CRD 对象。使用转换器配置，可以“教授”kustomize 有关 Rollout 对象的结构，并利用 kustomize 功能，例如 ConfigMap/Secret 生成器、变量引用以及通用标签和注释。要将 Rollouts 与 kustomize 结合使用：\n下载 rollout-transform.yaml 到你的 kustomize 目录。\n在你的 kustomize configurations 部分中包含 rollout-transform.yaml：\nkind: Kustomization apiVersion: kustomize.config.k8s.io/v1beta1 configurations: - rollout-transform.yaml 展示了使用 Rollouts 中的转换器的能力的 kustomize 应用程序示例可以在这里看到。\n在 Kustomize 3.6.1 中，可以直接从远程资源引用配置： configurations: - …","relpermalink":"/argo-rollouts/rollout/kustomize/","summary":"可以通过使用转换器配置来扩展 Kustomize 以理解 CRD 对象。使用转换器配置，可以“教授”kustomize 有关 Rollout 对象的结构，并利用 kustomize 功能，例如 ConfigMap/Secret 生成器、变量引用以及通用标签和注释。要将 Rollouts 与 kustomize 结合使用： 下载 rollout-transform.yaml 到你的 kustomize 目","title":"Kustomize 集成"},{"content":"本章包括五个来自从业者的故事，他们是现实世界中部署了 SPIFFE 和 SPIRE 的企业的工程师。\nUber：用加密身份确保下一代和传统基础设施的安全 Ryan Turner，软件工程师，Uber\n在过去十年中，Uber 已经成为爆炸性增长的典型代表。随着软件服务的数量和我们运营的地理规模的增长，复杂性和风险也在增加。为了满足不断增长的需求，我们开始建立我们的下一代基础设施平台。同时，几年前，我们看到开源项目 SPIFFE 和 SPIRE 的一些早期动力。\n我们立即看到了 SPIFFE 所能带来的价值，使我们能够加强我们的下一代基础设施安全态势。我们在 Uber 上线了 SPIRE，现在正使用它在各种工作负载环境中使用可加密验证的身份建立信任。我们从一些应用服务和内部服务开始，比如一个工作流引擎，它通过访问整个平台的数据，旋转多个动态工作负载来完成特定任务。SPIRE 向我们的工作负载提供 SPIFFE 身份，跨越我们的应用周期。SPIFFE 用于验证服务，帮助我们避免可能导致生产问题的错误配置。\n使用 SPIRE 改造传统堆栈 SPIRE 现在是 Uber 的下一个基础设施的关键组 …","relpermalink":"/spiffe/practitioners-stories/","summary":"本章包括五个来自从业者的故事，他们是现实世界中部署了 SPIFFE 和 SPIRE 的企业的工程师。 Uber：用加密身份确保下一代和传统基础设施的安全 Ryan Turner，软件工程师，Uber 在过去十年中，Uber 已经成为爆炸性增长","title":"10. 从业者故事"},{"content":"该节将带领大家了解 Kubernetes 中的基本概念，尤其是作为 Kubernetes 中调度的最基本单位 Pod。\n本节中包括以下内容：\n了解 Pod 的构成 Pod 的生命周期 Pod 中容器的启动顺序模板定义 Kubernetes 中的基本组件 kube-controller-manager 就是用来控制 Pod 的状态和生命周期的，在了解各种 controller 之前我们有必要先了解下 Pod 本身和其生命周期。\n","relpermalink":"/kubernetes-handbook/architecture/pod-state-and-lifecycle/","summary":"该节将带领大家了解 Kubernetes 中的基本概念，尤其是作为 Kubernetes 中调度的最基本单位 Pod。 本节中包括以下内容： 了解 Pod 的构成 Pod 的生命周期 Pod 中容器的启动顺序模板定义 Kubernetes 中的基本组件 kube-controller-manager 就是用来控制 Pod 的状态和生命周期的，在了解各","title":"Pod 状态与生命周期管理"},{"content":"如果您认为云原生基础架构是可购买的产品或是从云供应商那购买的服务器，我们很抱歉让您失望了。如果不采用这些做法并改变您建设和维护基础架构的方式，您就不会受益。\n它不仅仅影响服务器、网络和存储。它关乎的是工程师如何管理应用程序，就像接受故障一样。\n围绕云原生实践建立的文化与传统技术和工程组织有很大不同。我们并不是解决组织文化或结构问题的专家，但如果您希望改变组织结构，我们建议您从高绩效组织中实施 DevOps 实践的角度来看待价值观和经验教训。\n一些需要探索的地方是 Netflix 的文化套餐，它促进了自由和责任感，还有亚马逊的双比萨团队，这些团队以低开销推广自治团体。云原生应用程序需要与构建它们的团队具有相同的解耦特征。康威定律很好地描述了这一点：“设计系统的架构受制于产生这些设计的组织的沟通结构。”\n在我们结束本书时，我们希望关注哪些领域是您采用云原生实践时最重要的。我们还将讨论一些预测变化的基础架构模式，以便您知道将来要寻找什么。\n关注改变的地方 如果您拥有现有的基础架构或传统数据中心，则过渡到云原生不会在一夜之间发生。如果您有足够大的基础架构或两个以上的人员管理它，试图强制实施基础 …","relpermalink":"/cloud-native-infra/implementing-cloud-native-infrasctructure/","summary":"如果您认为云原生基础架构是可购买的产品或是从云供应商那购买的服务器，我们很抱歉让您失望了。如果不采用这些做法并改变您建设和维护基础架构的方式，您就不会受益。 它不仅仅影响服务器、网络和存储。它关乎的是工","title":"第 9 章：实施云原生基础架构"},{"content":"遵循本文件中概述的加固指南是确保在 Kubernetes 协调容器上运行的应用程序安全的一个步骤。然而，安全是一个持续的过程，跟上补丁、更新和升级是至关重要的。具体的软件组件因个人配置的不同而不同，但整个系统的每一块都应尽可能保持安全。这包括更新：Kubernetes、管理程序、虚拟化软件、插件、环境运行的操作系统、服务器上运行的应用程序，以及 Kubernetes 环境中托管的任何其他软件。\n互联网安全中心（CIS）发布了保护软件安全的基准。管理员应遵守 Kubernetes 和任何其他相关系统组件的 CIS 基准。管理员应定期检查，以确保其系统的安全性符合当前安全专家对最佳实践的共识。应定期对各种系统组件进行漏洞扫描和渗透测试，主动寻找不安全的配置和零日漏洞。任何发现都应在潜在的网络行为者发现和利用它们之前及时补救。\n随着更新的部署，管理员也应该跟上从环境中删除任何不再需要的旧组件。使用托管的 Kubernetes 服务可以帮助自动升级和修补 Kubernetes、操作系统和网络协议。然而，管理员仍然必须为他们的容器化应用程序打补丁和升级。\n","relpermalink":"/kubernetes-hardening-guidance/upgrading-and-application-security-practices/","summary":"遵循本文件中概述的加固指南是确保在 Kubernetes 协调容器上运行的应用程序安全的一个步骤。然而，安全是一个持续的过程，跟上补丁、更新和升级是至关重要的。具体的软件组件因个人配置的不同而不同，但整个系统的每一块都应尽","title":"升级和应用安全实践"},{"content":"路线图很快就会过时。有关最新的路线图，请参见 OpenTelemetry 状态页面。也就是说，以下是截至目前 OpenTelemetry 的状态。\n核心组件 目前，OpenTelemetry 追踪信号已被宣布为稳定的，并且在许多语言中都有稳定的实现。\n度量信号刚刚被宣布稳定，测试版的实施将在 2022 年第一季度广泛使用。\n日志信号预计将在 2022 年第一季度宣布稳定，测试版实现预计将在 2022 年第二季度广泛使用。2021 年，Stanza 项目被捐赠给 OpenTelemetry，为 OpenTelemetry 收集器增加了高效的日志处理能力。\n用于 HTTP/RPC、数据库和消息系统的语义公约预计将在 2022 年第一季度宣布稳定。\n未来 完成上述路线图就完成了对 OpenTelemetry 核心功能的稳定性要求。这是一个巨大的里程碑，因为它打开了进一步采用 OpenTelemetry 的大门，包括与数据库、管理服务和 OSS 库的原生集成。这些集成工作大部分已经以原型和测试版支持的形式在进行。随着 OpenTelemetry 的稳定性在 2022 年上半年完成， …","relpermalink":"/opentelemetry-obervability/roadmap/","summary":"附录 B：OpenTelemetry 项目路线图","title":"附录 B：OpenTelemetry 项目路线图"},{"content":"Starting with version 1.5, TSB provides an automated way to obtain images from a remote private Docker container repository by defining imagePullSecrets in ManagementPlane and ControlPlane CRs. If imagePullSecrets is defined, the required ServiceAccounts will be patched with the credentials from the secret, allowing for secure access to the containers that are stored in the remote private repository. The following steps outline the configuration process:\nSynchronizing images TSB images are …","relpermalink":"/tsb/setup/remote-registry/","summary":"Starting with version 1.5, TSB provides an automated way to obtain images from a remote private Docker container repository by defining imagePullSecrets in ManagementPlane and ControlPlane CRs. If imagePullSecrets is defined, the required ServiceAccounts will be patched with the credentials from the secret, allowing for secure access to the containers that are stored in the remote private repository. The following steps outline the configuration process:\nSynchronizing images TSB images are located in Tetrate’s repository and only available for copying to your repository (no direct download to any environment is allowed). The first step is to transfer the images to your repository.","title":"Repository secrets"},{"content":"Argo Rollouts 控制器已经安装了Prometheus 指标，可以在 8090 端口的/metrics中获取。你可以使用这些指标查看控制器的健康状况，无论是通过仪表板还是通过其他 Prometheus 集成。\n安装和配置 Prometheus 要利用指标，你需要在 Kubernetes 集群中安装 Prometheus。如果你没有现有的 Prometheus 安装，你可以使用任何常见的方法在你的集群中安装它。流行的选项包括Prometheus Helm Chat 或 Prometheus Operator。\n一旦 Prometheus 在你的集群中运行，你需要确保它抓取 Argo Rollouts 端点。Prometheus 已经包含了针对 Kubernetes 的服务发现机制，但你需要首先进行配置。根据你的安装方法，你可能需要采取其他操作来抓取 Argo Rollouts 端点。\n例如，如果你使用了 Prometheus 的 Helm 图表，则需要使用以下注释标注你的 Argo Rollouts Controller：\nspec: template: metadata: …","relpermalink":"/argo-rollouts/rollout/controller-metrics/","summary":"Argo Rollouts 控制器已经安装了Prometheus 指标，可以在 8090 端口的/metrics中获取。你可以使用这些指标查看控制器的健康状况，无论是通过仪表板还是通过其他 Prometheus 集成。 安装和配置 Prometheus 要利用指标，你需要在 Kubernetes 集群中","title":"控制器指标"},{"content":"下面的例子是一个 Dockerfile，它以非 root 用户和非 group 成员身份运行一个应用程序。\nFROM ubuntu:latest # 升级和安装 make 工具 RUN apt update \u0026amp;\u0026amp; apt install -y make # 从一个名为 code 的文件夹中复制源代码，并使用 make 工具构建应用程序。 COPY ./code RUN make /code # 创建一个新的用户（user1）和新的组（group1）；然后切换到该用户的上下文中。 RUN useradd user1 \u0026amp;\u0026amp; groupadd group1 USER user1:group1 # 设置容器的默认入口 CMD /code/app ","relpermalink":"/kubernetes-hardening-guidance/appendix/a/","summary":"下面的例子是一个 Dockerfile，它以非 root 用户和非 group 成员身份运行一个应用程序。 FROM ubuntu:latest # 升级和安装 make 工具 RUN apt update \u0026\u0026 apt install -y make # 从一个名为 code 的文件夹中复制源代码，并使用 make 工具构建应用程序。 COPY ./code RUN make /code # 创建一","title":"附录 A：非 root 应用的 Dockerfile 示例"},{"content":"在云环境中运行时，应用程序需要具有弹性。网络通信方面特别容易出现故障。添加网络弹性的一种常见模式是创建一个导入到应用程序中的库，该库提供本附录中描述的网络弹性模式。但是，导入的库很难维护以多种语言编写的服务，且当新版本的网络库发布时，会增加应用程序测试和重新部署的负担。\n取代应用程序处理网络弹性逻辑的另一种方式是，可以将代理置于适当的位置，作为应用程序的保护和增强层。代理的优势在于避免应用程序需要额外的复杂代码，尽量减少开发人员的工作量。\n可以在连接层（物理或 SDN），应用程序或透明代理中处理网络弹性逻辑。虽然代理不是传统网络堆栈的一部分，但它们可用于透明地管理应用程序的网络弹性。\n透明代理可以在基础架构中的任何位置运行，但与应用程序的距离越近越有利。代理支持的协议还要尽可能全面，且可以代理的开放系统互连模型（OSI 模型）层。\n通过实施以下模式，代理在基础架构的弹性中扮演着积极的角色：\n负载均衡 负载切分（Load shedding） 服务发现 重试和 deadline 断路 代理也可以用来为应用程序添加功能。包括：\n安全和认证 路由（入口和出口） 洞察和监测 负载均衡 应用程序负 …","relpermalink":"/cloud-native-infra/appendix-a-patterns-for-network-resiilency/","summary":"在云环境中运行时，应用程序需要具有弹性。网络通信方面特别容易出现故障。添加网络弹性的一种常见模式是创建一个导入到应用程序中的库，该库提供本附录中描述的网络弹性模式。但是，导入的库很难维护以多种语言编写","title":"附录 A：网络弹性模式"},{"content":"本文将为您讲解 Pod 的基础概念。\n理解 Pod Pod 是 kubernetes 中你可以创建和部署的最小也是最简的单位。Pod 代表着集群中运行的进程。\nPod 中封装着应用的容器（有的情况下是好几个容器），存储、独立的网络 IP，管理容器如何运行的策略选项。Pod 代表着部署的一个单位：kubernetes 中应用的一个实例，可能由一个或者多个容器组合在一起共享资源。\nDocker 是 kubernetes 中最常用的容器运行时，但是 Pod 也支持其他容器运行时。\n在 Kubernetes 集群中 Pod 有如下两种使用方式：\n一个 Pod 中运行一个容器。“每个 Pod 中一个容器”的模式是最常见的用法；在这种使用方式中，你可以把 Pod 想象成是单个容器的封装，kuberentes 管理的是 Pod 而不是直接管理容器。 在一个 Pod 中同时运行多个容器。一个 Pod 中也可以同时封装几个需要紧密耦合互相协作的容器，它们之间共享资源。这些在同一个 Pod 中的容器可以互相协作成为一个 service 单位 —— 一个容器共享文件，另一个“sidecar”容器来更新这些文 …","relpermalink":"/kubernetes-handbook/objects/pod-overview/","summary":"本文将为您讲解 Pod 的基础概念。 理解 Pod Pod 是 kubernetes 中你可以创建和部署的最小也是最简的单位。Pod 代表着集群中运行的进程。 Pod 中封装着应用的容器（有的情况下是好几个容器），存储、独立的网络 IP，管理容器如何运行的策","title":"Pod 概览"},{"content":"Argo CD 具有在检测到 Git 中所需清单与集群中的实际状态之间存在差异时自动同步应用程序的功能。自动同步的好处是，CI/CD 管道不再需要直接访问 Argo CD API 服务器以执行部署。相反，管道将更改的清单提交并推送到跟踪 Git 存储库中。\n要配置自动同步，请运行：\nargocd app set \u0026lt;APPNAME\u0026gt; --sync-policy automated 或者，如果创建应用程序清单，则使用 automated 策略指定 syncPolicy。\nspec: syncPolicy: automated: {} 自动修整 默认情况下（作为一种安全机制），当 Argo CD 检测到资源不再在 Git 中定义时，自动同步不会删除资源。始终可以执行手动同步（并检查修整）来修整资源。也可以通过运行以下命令设置自动修整：\nargocd app set \u0026lt;APPNAME\u0026gt; --auto-prune 或通过在自动同步策略中将 prune 选项设置为 true：\nspec: syncPolicy: automated: prune: true 带有允许空值的自动修整（v1.8） 默认 …","relpermalink":"/argo-cd/user-guide/auto-sync/","summary":"Argo CD 具有在检测到 Git 中所需清单与集群中的实际状态之间存在差异时自动同步应用程序的功能。自动同步的好处是，CI/CD 管道不再需要直接访问 Argo CD API 服务器以执行部署。相反，管道将更改的清单提交并推送到跟踪 Git 存储","title":"自动同步策略"},{"content":"有两种方法可以迁移到 Rollout：\n将现有的 Deployment 资源转换为 Rollout 资源。 使用 workloadRef 字段从 Rollout 引用现有的 Deployment。 将 Deployment 转换为 Rollout 将 Deployment 转换为 Rollout 时，需要更改三个字段：\n将 apiVersion 从 apps/v1 更改为 argoproj.io/v1alpha1 将 kind 从 Deployment 更改为 Rollout 使用蓝绿或金丝雀策略替换部署策略 以下是使用金丝雀策略的 Rollout 资源示例。\napiVersion: argoproj.io/v1alpha1 # 从 apps/v1 更改而来 kind: Rollout # 从 Deployment 更改而来 metadata: name: rollouts-demo spec: selector: matchLabels: app: rollouts-demo template: metadata: labels: app: rollouts-demo spec: …","relpermalink":"/argo-rollouts/migrating/","summary":"有两种方法可以迁移到 Rollout： 将现有的 Deployment 资源转换为 Rollout 资源。 使用 workloadRef 字段从 Rollout 引用现有的 Deployment。 将 Deployment 转换为 Rollout 将 Deployment 转换为 Rollout 时，需要更改三个字段： 将 apiVersion 从 apps/v1 更改为 argoproj.io/v1alpha1 将 kind 从 Deployment 更改为 Rollout 使用蓝绿或金","title":"迁移到 Rollouts"},{"content":"Pod 是 Kubernetes 中可以创建的最小部署单元，也是 Kubernetes REST API 中的顶级资源类型。\n在 Kuberentes V1 core API 版本中的 Pod 的数据结构如下图所示：\nPod Cheatsheet 什么是 Pod？ Pod 就像是豌豆荚一样，它由一个或者多个容器组成（例如 Docker 容器），它们共享容器存储、网络和容器运行配置项。Pod 中的容器总是被同时调度，有共同的运行环境。你可以把单个 Pod 想象成是运行独立应用的“逻辑主机”—— 其中运行着一个或者多个紧密耦合的应用容器 —— 在有容器之前，这些应用都是运行在几个相同的物理机或者虚拟机上。\n尽管 kubernetes 支持多种容器运行时，但是 Docker 依然是最常用的运行时环境，我们可以使用 Docker 的术语和规则来定义 Pod。\nPod 中共享的环境包括 Linux 的 namespace、cgroup 和其他可能的隔绝环境，这一点跟 Docker 容器一致。在 Pod 的环境中，每个容器中可能还有更小的子隔离环境。\nPod 中的容器共享 IP 地址和端口号，它们 …","relpermalink":"/kubernetes-handbook/objects/pod/","summary":"Pod 是 Kubernetes 中可以创建的最小部署单元，也是 Kubernetes REST API 中的顶级资源类型。 在 Kuberentes V1 core API 版本中的 Pod 的数据结构如下图所示： Pod Cheatsheet 什么是 Pod？ Pod 就像是豌豆荚一样，它由一个或者多个容器组成（例如 Docker 容器），它们共享容器存储、网","title":"Pod 解析"},{"content":"关于使用云提供商和避免供应商锁定存在很多争议。这种辩论充满了意识心态之争。\n锁定通常是工程师和管理层关心的问题。应该将其与选择编程语言或框架一样作为应用程序的风险来权衡。编程语言和云提供商的选择是锁定的形式，工程师有责任了解风险并评估这些风险是否可以接受。\n当您选择供应商或技术时，请记住以下几点：\n锁定是不可避免的。 锁定是一种风险，但并不总是很高。 不要外包思维。 锁定是不可避免的 在技术上有两种类型的锁定：\n技术锁定\n整个开发技术栈中底层技术\n供应商锁定\n大多数情况下，作为项目的一部分而使用的服务和软件（供应商锁定还可能包括硬件和操作系统，但我们只关注服务）\n技术锁定 开发人员将选择他们熟悉的技术或为正在开发的应用程序提供最大利益的技术。这些技术可以是供应商提供的技术（例如.NET 和 Oracle 数据库）到开源软件（例如 Python 和 PostgreSQL）。\n在此级别提供的锁定通常要求符合 API 或规范，这将影响应用程序的开发。也可以选择一些替代技术，但是这通常有很高的转换成本，因为技术对应用程序的设计有很大影响。\n供应商锁定 供应商，如云提供商，是另一种不同形式的锁 …","relpermalink":"/cloud-native-infra/appendix-b-lock-in/","summary":"关于使用云提供商和避免供应商锁定存在很多争议。这种辩论充满了意识心态之争。 锁定通常是工程师和管理层关心的问题。应该将其与选择编程语言或框架一样作为应用程序的风险来权衡。编程语言和云提供商的选择是锁定的","title":"附录 B：锁定"},{"content":"下面是一个使用只读根文件系统的 Kubernetes 部署模板的例子。\napiVersion: apps/v1 kind: Deployment metadata: labels: app: web name: web spec: selector: matchLabels: app: web template: metadata: labels: app: web name: web spec: containers: - command: [\u0026#34;sleep\u0026#34;] args: [\u0026#34;999\u0026#34;] image: ubuntu:latest name: web securityContext: readOnlyRootFilesystem: true #使容器的文件系统成为只读 volumeMounts: - mountPath: /writeable/location/here #创建一个可写卷 name: volName volumes: - emptyDir: {} name: volName ","relpermalink":"/kubernetes-hardening-guidance/appendix/b/","summary":"下面是一个使用只读根文件系统的 Kubernetes 部署模板的例子。 apiVersion: apps/v1 kind: Deployment metadata: labels: app: web name: web spec: selector: matchLabels: app: web template: metadata: labels: app: web name: web spec: containers: - command: [\"sleep\"] args: [\"999\"] image: ubuntu:latest name: web securityContext: readOnlyRootFilesystem: true #使容器的文件系统成为只读 volumeMounts: - mountPath: /writeable/location/here #创建一个可写卷 name: volName volumes: - emptyDir: {} name: volName","title":"附录 B：只读文件系统的部署模板示例"},{"content":"This document will cover how to migrate a live installation of TSB using tctl and migrate to helm. The document assumes that Helm is already installed in the system.\nBefore you get started, make sure you:\n✓ Familiarize yourself with TSB concepts ✓ Install the TSB environment. You can use TSB demo for quick install\n✓ Completed TSB usage quickstart.\nPreparation of the helm charts Before hand you must be familiar with Helm. Follow the prerequisites in our guide with installing TSB with Helm. …","relpermalink":"/tsb/setup/migrate-tctl-to-helm/","summary":"This document will cover how to migrate a live installation of TSB using tctl and migrate to helm. The document assumes that Helm is already installed in the system.\nBefore you get started, make sure you:\n✓ Familiarize yourself with TSB concepts ✓ Install the TSB environment. You can use TSB demo for quick install\n✓ Completed TSB usage quickstart.\nPreparation of the helm charts Before hand you must be familiar with Helm. Follow the prerequisites in our guide with installing TSB with Helm.\nMigrate Management Plane Migrating the current installation requires only labeling and annotating the resources of the plane installation.","title":"Migration from tctl to Helm"},{"content":"本文介绍了在使用 Argo Rollouts 时的一些最佳实践、技巧和窍门。\nIngress 目标/稳定主机路由 出于各种原因，通常希望外部服务能够访问预期的 pod（即 canary/preview）或特定的稳定 pod，而不会将流量任意分配给两个版本。一些使用场景包括：\n新版本的服务可以在内部/私有环境中访问（例如进行手动验证），然后再将其外部公开。 外部 CI/CD 管道在将蓝/绿预览堆栈升级到生产环境之前运行测试。 运行比较旧版本和新版本行为的测试。 如果使用 Ingress 来将流量路由到服务，则可以添加其他主机规则到 Ingress 规则中，以便能够特别到达期望的（canary/preview）pod 或稳定的 pod。\napiVersion：networking.k8s.io/v1beta1 kind：Ingress metadata： name：guestbook spec： rules： ＃仅到达所需的Pod（也称为金丝雀/预览）的主机规则 -主机：guestbook-desired.argoproj.io http： paths： -后端： …","relpermalink":"/argo-rollouts/best-practices/","summary":"本文介绍了在使用 Argo Rollouts 时的一些最佳实践、技巧和窍门。 Ingress 目标/稳定主机路由 出于各种原因，通常希望外部服务能够访问预期的 pod（即 canary/preview）或特定的稳定 pod，而不会将流量任意分配给两个","title":"最佳实践"},{"content":"该特性在自 Kubernetes 1.6 版本推出 beta 版本。Init 容器可以在 PodSpec 中同应用程序的 containers 数组一起来指定。此前 beta 注解的值仍将保留，并覆盖 PodSpec 字段值。\n本文讲解 Init 容器的基本概念，这是一种专用的容器，在应用程序容器启动之前运行，用来包含一些应用镜像中不存在的实用工具或安装脚本。\n理解 Init 容器 Pod 能够具有多个容器，应用运行在容器里面，但是它也可能有一个或多个先于应用容器启动的 Init 容器。\nInit 容器与普通的容器非常像，除了如下两点：\nInit 容器总是运行到成功完成为止。 每个 Init 容器都必须在下一个 Init 容器启动之前成功完成。 如果 Pod 的 Init 容器失败，Kubernetes 会不断地重启该 Pod，直到 Init 容器成功为止。然而，如果 Pod 对应的 restartPolicy 为 Never，它不会重新启动。\n指定容器为 Init 容器，在 PodSpec 中添加 initContainers 字段，以 v1.Container 类型对象的 JSON …","relpermalink":"/kubernetes-handbook/objects/init-containers/","summary":"该特性在自 Kubernetes 1.6 版本推出 beta 版本。Init 容器可以在 PodSpec 中同应用程序的 containers 数组一起来指定。此前 beta 注解的值仍将保留，并覆盖 PodSpec 字段值。 本文讲解 Init 容器的基本概念，这是一种专用的容器，在应用程序容器启动之前运行，用来","title":"Init 容器"},{"content":" 以下内容最初由 CNCF 发布在 Kubernetes.io 上，并且在此获得许可。\n在 2014 年夏天，Box 对沉淀了十年的硬件和软件基础架构的痛苦，这无法与公司的需求保持一致。\n该平台为超过 5000 万用户（包括政府和大型企业如通用电气公司）管理和共享云中的内容，Box 最初是一个使用 PHP 写的具有数百万行的庞大代码，内置裸机数据中心。它已经开始将单体应用分解成微服务。 “随着我们扩展到全球各地，公有云战争正在升温，我们开始专注于如何在许多不同的环境和许多不同的云基础架构提供商之间运行我们的工作负载”，Box 联合创始人和服务架构师 Sam Ghods 说。 “迄今为止，这是一个巨大的挑战，因为所有这些不同的提供商，特别是裸机，都有非常不同的接口和与合作方式。”\n当 Ghods 参加 DockerCon 时，Box 的云原生之旅加速了。该公司已经认识到，它不能再仅仅使用裸机来运行应用程序，正在研究 Docker 容器化，使用 OpenStack 进行虚拟化以及支持公有云。\n在那次会议上，Google 宣布发布 Kubernetes 容器管理系统，Ghods 成功了。 “ …","relpermalink":"/cloud-native-infra/appendix-c-box-case-study/","summary":"以下内容最初由 CNCF 发布在 Kubernetes.io 上，并且在此获得许可。 在 2014 年夏天，Box 对沉淀了十年的硬件和软件基础架构的痛苦，这无法与公司的需求保持一致。 该平台为超过 5000 万用户（包括政府和大型企业如通用电气公司）管理和共享云","title":"附录 C Box：案例研究"},{"content":"下面是一个 Kubernetes Pod 安全策略的例子，它为集群中运行的容器执行了强大的安全要求。这个例子是基于官方的 Kubernetes 文档。我们鼓励管理员对该策略进行修改，以满足他们组织的要求。\napiVersion: policy/v1beta1 kind: PodSecurityPolicy metadata: name: restricted annotations: seccomp.security.alpha.kubernetes.io/allowedProfileNames: \u0026#39;docker/default,runtime/default\u0026#39; apparmor.security.beta.kubernetes.io/allowedProfileNames: \u0026#39;runtime/default\u0026#39; seccomp.security.alpha.kubernetes.io/defaultProfileName: \u0026#39;runtime/default\u0026#39; apparmor.security.beta.kubernetes.io/defaultProfileName: …","relpermalink":"/kubernetes-hardening-guidance/appendix/c/","summary":"下面是一个 Kubernetes Pod 安全策略的例子，它为集群中运行的容器执行了强大的安全要求。这个例子是基于官方的 Kubernetes 文档。我们鼓励管理员对该策略进行修改，以满足他们组织的要求。 apiVersion: policy/v1beta1 kind: PodSecurityPolicy metadata: name: restricted annotations: seccomp.security.alpha.kubernetes.io/allowedProfileNames: 'docker/default,runtime/default' apparmor.security.beta.kubernetes.io/allowedProfileNames: 'runtime/default' seccomp.security.alpha.kubernetes.io/defaultProfileName: 'runtime/default' apparmor.security.beta.kubernetes.io/defaultProfileName: 'runtime/default' spec: privileged: false # 需要防","title":"附录 C：Pod 安全策略示例"},{"content":"忽略无关资源 v1.1\n在某些情况下，你可能希望从应用程序的整体同步状态中排除资源。例如。如果它们是由工具生成的。这可以通过在你想要排除的资源上添加此注释来完成：\nmetadata: annotations: argocd.argoproj.io/compare-options: IgnoreExtraneous 对比选项需要修整 🔔 提示：这仅影响同步状态。如果资源的运行状况降级，那么应用程序也会降级。\nKustomize 具有允许你生成配置映射的功能（了解更多）。你可以设置 generatorOptions 添加此注释，以便你的应用保持同步：\nconfigMapGenerator: - name: my-map literals: - foo=bar generatorOptions: annotations: argocd.argoproj.io/compare-options: IgnoreExtraneous kind: Kustomization 🔔 提示：generatorOptions 向配置映射和秘密添加注释（了解更多）。\n你可能希望将其与 Prune=false 同 …","relpermalink":"/argo-cd/user-guide/compare-options/","summary":"忽略无关资源 v1.1 在某些情况下，你可能希望从应用程序的整体同步状态中排除资源。例如。如果它们是由工具生成的。这可以通过在你想要排除的资源上添加此注释来完成： metadata: annotations: argocd.argoproj.io/compare-options: IgnoreExtraneous 对比选项需要修整 🔔 提示：这仅影响同步状态","title":"对比选项"},{"content":"一般问题 Argo Rollouts 是依赖 Argo CD 或其他 Argo 项目吗？ Argo Rollouts 是一个独立的项目。虽然它与 Argo CD 和其他 Argo 项目配合使用效果很好，但它也可以单独用于渐进式交付场景。更具体地说，Argo Rollouts 不需要你也在同一集群上安装 Argo CD。\nArgo Rollouts 如何与 Argo CD 集成？ 通过 Argo CD 的 Lua 健康检查，Argo CD 可以了解 Argo Rollouts 资源的健康状况。这些健康检查了解 Argo Rollout 对象何时在进展、暂停、退化或健康。此外，Argo CD 具有基于 Lua 的资源操作，可以改变 Argo Rollouts 资源（例如取消暂停 Rollout）。\n因此，操作员可以构建自动化程序以反应 Argo Rollouts 资源的状态。例如，如果 Argo CD 创建的 Rollout 被暂停，Argo CD 会检测到并将该应用程序标记为暂停。一旦确定新版本是好的，操作员就可以使用 Argo CD 的 resume …","relpermalink":"/argo-rollouts/faq/","summary":"一般问题 Argo Rollouts 是依赖 Argo CD 或其他 Argo 项目吗？ Argo Rollouts 是一个独立的项目。虽然它与 Argo CD 和其他 Argo 项目配合使用效果很好，但它也可以单独用于渐进式交付场景。更具体地说，Argo Rollouts 不需要你也在同一集群上安装 Argo CD。 Argo Rollouts 如何","title":"FAQ"},{"content":"Pause 容器，又叫 Infra 容器，本文将探究该容器的作用与原理。\n我们知道在 kubelet 的配置中有这样一个参数：\nKUBELET_POD_INFRA_CONTAINER=--pod-infra-container-image=registry.access.redhat.com/rhel7/pod-infrastructure:latest 上面是 openshift 中的配置参数，kubernetes 中默认的配置参数是：\nKUBELET_POD_INFRA_CONTAINER=--pod-infra-container-image=gcr.io/google_containers/pause-amd64:3.0 Pause 容器，是可以自己来定义，官方使用的 gcr.io/google_containers/pause-amd64:3.0 容器的代码见 Github，使用 C 语言编写。\nPause 容器特点 镜像非常小，目前在 700KB 左右 永远处于 Pause (暂停) 状态 Pause 容器背景 像 Pod 这样一个东西，本身是一个逻辑概念。那在机器上，它究 …","relpermalink":"/kubernetes-handbook/objects/pause-container/","summary":"Pause 容器，又叫 Infra 容器，本文将探究该容器的作用与原理。 我们知道在 kubelet 的配置中有这样一个参数： KUBELET_POD_INFRA_CONTAINER=--pod-infra-container-image=registry.access.redhat.com/rhel7/pod-infrastructure:latest 上面是 openshift 中的配置参数，kubernetes 中默认的配置参数是： KUBELET_POD_INFRA_CONTAINER=--pod-infra-container-image=gcr.io/google_containers/pause-amd64:3.0 Pause 容器，是可以自己来定义，官方使用的 gcr.io/google_containers/pause-amd64:3.0 容器的代码","title":"Pause 容器"},{"content":"下面的例子是为每个团队或用户组，可以使用 kubectl 命令或 YAML 文件创建一个 Kubernetes 命名空间。应避免使用任何带有 kube 前缀的名称，因为它可能与 Kubernetes 系统保留的命名空间相冲突。\nKubectl 命令来创建一个命名空间。\nkubectl create namespace \u0026lt;insert-namespace-name-here\u0026gt; 要使用 YAML 文件创建命名空间，创建一个名为 my-namespace.yaml 的新文件，内容如下：\napiVersion: v1 kind: Namespace metadata: name: \u0026lt;insert-namespace-name-here\u0026gt; 应用命名空间，使用：\nkubectl create –f ./my-namespace.yaml 要在现有的命名空间创建新的 Pod，请切换到所需的命名空间：\nkubectl config use-context \u0026lt;insert-namespace-here\u0026gt; 应用新的 Deployment，使用：\nkubectl apply -f …","relpermalink":"/kubernetes-hardening-guidance/appendix/d/","summary":"下面的例子是为每个团队或用户组，可以使用 kubectl 命令或 YAML 文件创建一个 Kubernetes 命名空间。应避免使用任何带有 kube 前缀的名称，因为它可能与 Kubernetes 系统保留的命名空间相冲突。 Kubectl 命令来创建一个命名空间。 kubectl create namespace \u003cinsert-namespace-name-here\u003e 要使用 YAML 文件创建命名","title":"附录 D：命名空间示例"},{"content":"报告漏洞 如果你在 Argo Rollouts 中发现与安全相关的错误，我们恳请你负责任地进行披露，并给我们适当的时间来反应、分析和开发修复程序，以减轻发现的安全漏洞。\n请通过电子邮件将漏洞报告至以下地址：\ncncf-argo-security@lists.cncf.io 所有漏洞和相关信息都将得到完全保密。\n公开披露 我们将使用 GitHub 安全建议功能发布安全建议，以使我们的社区充分了解情况，并将感谢你的发现（当然，除非你愿意保持匿名）。\nInternet Bug Bounty 合作 我们很高兴地宣布，Argo 项目正在与 Hacker One 及其互联网错误赏金计划的优秀人员合作，以奖励在四个主要 Argo 项目（CD、活动、推出和工作流程）中发现安全漏洞的优秀人员然后与我们一起以负责任的方式修复和披露它们。\n如果你按照本安全政策中的规定向我们报告漏洞，我们将与你共同确定你的发现是否符合领取赏金的条件，以及如何领取赏金。\n","relpermalink":"/argo-rollouts/security/","summary":"报告漏洞 如果你在 Argo Rollouts 中发现与安全相关的错误，我们恳请你负责任地进行披露，并给我们适当的时间来反应、分析和开发修复程序，以减轻发现的安全漏洞。 请通过电子邮件将漏洞报告至以下地址： cncf-argo-security@lists.cncf.io 所有漏洞和相关信息都将","title":"Argo Rollouts 的安全策略"},{"content":"PodSecurityPolicy 类型的对象能够控制，是否可以向 Pod 发送请求，该 Pod 能够影响被应用到 Pod 和容器的 SecurityContext。\n什么是 Pod 安全策略？ Pod 安全策略 是集群级别的资源，它能够控制 Pod 运行的行为，以及它具有访问什么的能力。 PodSecurityPolicy对象定义了一组条件，指示 Pod 必须按系统所能接受的顺序运行。它们允许管理员控制如下方面：\n控制面 字段名称 已授权容器的运行 privileged 为容器添加默认的一组能力 defaultAddCapabilities 为容器去掉某些能力 requiredDropCapabilities 容器能够请求添加某些能力 allowedCapabilities 控制卷类型的使用 volumes 主机网络的使用 hostNetwork 主机端口的使用 hostPorts 主机 PID namespace 的使用 hostPID 主机 IPC namespace 的使用 hostIPC 主机路径的使用 allowedHostPaths 容器的 SELinux …","relpermalink":"/kubernetes-handbook/objects/pod-security-policy/","summary":"PodSecurityPolicy 类型的对象能够控制，是否可以向 Pod 发送请求，该 Pod 能够影响被应用到 Pod 和容器的 SecurityContext。 什么是 Pod 安全策略？ Pod 安全策略 是集群级别的资源，它能够控制 Pod 运行的行为，以及它具有访问什么的能力","title":"Pod 安全策略"},{"content":"网络策略根据使用的网络插件而不同。下面是一个网络策略的例子，参考 Kubernetes 文档将 nginx 服务的访问限制在带有标签访问的 Pod 上。\napiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: example-access-nginx namespace: prod #这可以是任何一个命名空间，或者在不使用命名空间的情况下省略。 spec: podSelector: matchLabels: app: nginx ingress: - from: - podSelector: matchLabels: access: \u0026#34;true\u0026#34; 新的 NetworkPolicy 可以通过以下方式应用：\nkubectl apply -f policy.yaml 一个默认的拒绝所有入口的策略：\napiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: deny-all-ingress spec: podSelector: {} …","relpermalink":"/kubernetes-hardening-guidance/appendix/e/","summary":"网络策略根据使用的网络插件而不同。下面是一个网络策略的例子，参考 Kubernetes 文档将 nginx 服务的访问限制在带有标签访问的 Pod 上。 apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: example-access-nginx namespace: prod #这可以是任何一个命名空间，或者在不使用命名空间的情况下省略。 spec: podSelector: matchLabels: app: nginx","title":"附录 E：网络策略示例"},{"content":"Argo CD 允许用户定制同步目标集群中所需状态的某些方面。某些同步选项可以定义为特定资源中的注释。大多数同步选项在应用程序资源 spec.syncPolicy.syncOptions 属性中配置。使用 argocd.argoproj.io/sync-options 注释配置的多个同步选项可以在注释值中使用 , 进行连接；空格将被删除。\n下面你可以找到有关每个可用同步选项的详细信息：\n无修整资源 v1.1\n你可能希望防止修整对象：\nmetadata: annotations: argocd.argoproj.io/sync-options: Prune=false 在 UI 中，Pod 将仅显示为不同步：\n同步选项无修整 同步状态面板显示跳过修整的原因：\n同步选项无修正 如果 Argo CD 期望剪切资源，则应用程序将失去同步。你可能希望与 比较选项 结合使用。\n禁用 Kubectl 验证 对于某些对象类，需要使用 --validate=false 标志使用 kubectl apply 将其应用。例如使用 RawExtension 的 Kubernetes 类型， …","relpermalink":"/argo-cd/user-guide/sync-options/","summary":"Argo CD 允许用户定制同步目标集群中所需状态的某些方面。某些同步选项可以定义为特定资源中的注释。大多数同步选项在应用程序资源 spec.syncPolicy.syncOptions 属性中配置。使用 argocd.argoproj.io/sync-options 注释配置的多个同步选项可以在注释值中使用 , 进行连接；空格将被删","title":"同步选项 "},{"content":"本文讲解的是 Kubernetes 中 Pod 的生命周期，包括生命周期的不同阶段、存活和就绪探针、重启策略等。\nPod phase Pod 的 status 字段是一个 PodStatus 对象，PodStatus 中有一个 phase 字段。\nPod 的相位（phase）是 Pod 在其生命周期中的简单宏观概述。该字段并不是对容器或 Pod 的综合汇总，也不是为了做为综合状态机。\nPod 相位的数量和含义是严格指定的。除了本文档中列举的状态外，不应该再假定 Pod 有其他的 phase 值。\n下面是 phase 可能的值：\n挂起（Pending）：Pod 已被 Kubernetes 系统接受，但有一个或者多个容器镜像尚未创建。等待时间包括调度 Pod 的时间和通过网络下载镜像的时间，这可能需要花点时间。 运行中（Running）：该 Pod 已经绑定到了一个节点上，Pod 中所有的容器都已被创建。至少有一个容器正在运行，或者正处于启动或重启状态。 成功（Succeeded）：Pod 中的所有容器都被成功终止，并且不会再重启。 失败（Failed）：Pod 中的所有容器都已终止了，并 …","relpermalink":"/kubernetes-handbook/objects/pod-lifecycle/","summary":"本文讲解的是 Kubernetes 中 Pod 的生命周期，包括生命周期的不同阶段、存活和就绪探针、重启策略等。 Pod phase Pod 的 status 字段是一个 PodStatus 对象，PodStatus 中有一个 phase 字段。 Pod 的相位（phase）是 Pod 在其生命周期中的简单宏观概述。","title":"Pod 的生命周期"},{"content":"在 Kubernetes 1.10 和更新版本中，LimitRange 支持被默认启用。下面的 YAML 文件为每个容器指定了一个 LimitRange，其中有一个默认的请求和限制，以及最小和最大的请求。\napiVersion: v1 kind: LimitRange metadata: name: cpu-min-max-demo-lr spec: limits - default: cpu: 1 defaultRequest: cpu: 0.5 max: cpu: 2 min: cpu 0.5 type: Container LimitRange 可以应用于命名空间，使用：\nkubectl apply -f \u0026lt;example-LimitRange\u0026gt;.yaml --namespace=\u0026lt;Enter-Namespace\u0026gt; 在应用了这个 LimitRange 配置的例子后，如果没有指定，命名空间中创建的所有容器都会被分配到默认的 CPU 请求和限制。命名空间中的所有容器的 CPU 请求必须大于或等于最小值，小于或等于最大 CPU 值，否则容器将不会被实例化。\n","relpermalink":"/kubernetes-hardening-guidance/appendix/f/","summary":"在 Kubernetes 1.10 和更新版本中，LimitRange 支持被默认启用。下面的 YAML 文件为每个容器指定了一个 LimitRange，其中有一个默认的请求和限制，以及最小和最大的请求。 apiVersion: v1 kind: LimitRange metadata: name: cpu-min-max-demo-lr spec: limits - default: cpu: 1 defaultRequest: cpu: 0.5 max: cpu: 2 min:","title":"附录 F：LimitRange 示例"},{"content":"Pod Hook（钩子）是由 Kubernetes 管理的 kubelet 发起的，当容器中的进程启动前或者容器中的进程终止之前运行，这是包含在容器的生命周期之中。可以同时为 Pod 中的所有容器都配置 hook。\nHook 的类型包括两种：\nexec：执行一段命令 HTTP：发送 HTTP 请求。 参考下面的配置：\napiVersion: v1 kind: Pod metadata: name: lifecycle-demo spec: containers: - name: lifecycle-demo-container image: nginx lifecycle: postStart: exec: command: [\u0026#34;/bin/sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;echo Hello from the postStart handler\u0026gt; /usr/share/message\u0026#34;] preStop: exec: command: [\u0026#34;/usr/sbin/nginx\u0026#34;,\u0026#34;-s\u0026#34;,\u0026#34;quit\u0026#34;] Kubernetes 在容器创建后立即发送 postStart 事件。但是，不能保证在调用容器的 …","relpermalink":"/kubernetes-handbook/objects/pod-hook/","summary":"Pod Hook（钩子）是由 Kubernetes 管理的 kubelet 发起的，当容器中的进程启动前或者容器中的进程终止之前运行，这是包含在容器的生命周期之中。可以同时为 Pod 中的所有容器都配置 hook。 Hook 的类型包括两种： exec：执行一段命令","title":"Pod Hook"},{"content":"通过将 YAML 文件应用于命名空间或在 Pod 的配置文件中指定要求来创建 ResourceQuota 对象，以限制命名空间内的总体资源使用。下面的例子是基于 Kubernetes 官方文档的一个命名空间的配置文件示例：\napiVersion: v1 kind: ResourceQuota metadata: name: example-cpu-mem-resourcequota spec: hard: requests.cpu: \u0026#34;1\u0026#34; requests.memory: 1Gi limits.cpu: \u0026#34;2\u0026#34; limits.memory: 2Gi 可以这样应用这个 ResourceQuota：\nkubectl apply -f example-cpu-mem-resourcequota.yaml -- namespace=\u0026lt;insert-namespace-here\u0026gt; 这个 ResourceQuota 对所选择的命名空间施加了以下限制：\n每个容器都必须有一个内存请求、内存限制、CPU 请求和 CPU 限制。 所有容器的总内存请求不应超过 1 GiB 所有容器的总内存限制不应超过 2 …","relpermalink":"/kubernetes-hardening-guidance/appendix/g/","summary":"通过将 YAML 文件应用于命名空间或在 Pod 的配置文件中指定要求来创建 ResourceQuota 对象，以限制命名空间内的总体资源使用。下面的例子是基于 Kubernetes 官方文档的一个命名空间的配置文件示例： apiVersion: v1 kind: ResourceQuota metadata: name: example-cpu-mem-resourcequota spec: hard: requests.cpu: \"1\" requests.memory: 1Gi limits.cpu: \"2\" limits.memory: 2Gi 可以这样应用","title":"附录 G：ResourceQuota 示例"},{"content":"以下环境变量可与 argocd CLI 一起使用：\n环境变量 描述 ARGOCD_SERVER 不带 https:// 前缀的 ArgoCD 服务器地址（而不是为每个命令指定 --server ） 例如：ARGOCD_SERVER=argocd.mycompany.com 如果通过 DNS 入口提供服务 ARGOCD_AUTH_TOKEN ArgoCD apiKey 以便你的 ArgoCD 用户能够进行身份验证 ARGOCD_OPTS 传递到 argocd CLI 的命令行选项，例如 ARGOCD_OPTS=\u0026#34;--grpc-web\u0026#34; ","relpermalink":"/argo-cd/user-guide/environment-variables/","summary":"以下环境变量可与 argocd CLI 一起使用： 环境变量 描述 ARGOCD_SERVER 不带 https:// 前缀的 ArgoCD 服务器地址（而不是为每个命令指定 --server ） 例如：ARGOCD_SERVER=argocd.mycompany.com 如果通过 DNS 入口提供服务 ARGOCD_AUTH_TOKEN ArgoCD apiKey 以便","title":"环境变量"},{"content":"Preset 就是预设，有时候想要让一批容器在启动的时候就注入一些信息，比如 secret、volume、volume mount 和环境变量，而又不想一个一个的改这些 Pod 的 template，这时候就可以用到 PodPreset 这个资源对象了。\n本页是关于 PodPreset 的概述，该对象用来在 Pod 创建的时候向 Pod 中注入某些特定信息。该信息可以包括 secret、volume、volume mount 和环境变量。\n理解 Pod Preset Pod Preset 是用来在 Pod 被创建的时候向其中注入额外的运行时需求的 API 资源。\n您可以使用 label selector 来指定为哪些 Pod 应用 Pod Preset。\n使用 Pod Preset 使得 pod 模板的作者可以不必为每个 Pod 明确提供所有信息。这样一来，pod 模板的作者就不需要知道关于该服务的所有细节。\n关于该背景的更多信息，请参阅 PodPreset 的设计方案。\n如何工作 Kubernetes 提供了一个准入控制器（PodPreset），当其启用时，Pod Preset 会将 …","relpermalink":"/kubernetes-handbook/objects/pod-preset/","summary":"Preset 就是预设，有时候想要让一批容器在启动的时候就注入一些信息，比如 secret、volume、volume mount 和环境变量，而又不想一个一个的改这些 Pod 的 template，这时候就可以用到 PodPreset 这个资源对象了。 本","title":"Pod Preset"},{"content":"要对秘密数据进行静态加密，下面的加密配置文件提供了一个例子，以指定所需的加密类型和加密密钥。将加密密钥存储在加密文件中只能稍微提高安全性。Secret 将被加密，但密钥将在 EncryptionConfiguration 文件中被访问。这个例子是基于 Kubernetes 的官方文档。\napiVersion: apiserver.config.k8s.io/v1 kind: EncryptionConfiguration resources: - resources: - secrets providers: - aescbc: keys: - name: key1 secret: \u0026lt;base 64 encoded secret\u0026gt; - identity: {} 要使用该加密文件进行静态加密，请在重启 API 服务器时设置 --encryption-provider-config 标志，并注明配置文件的位置。\n","relpermalink":"/kubernetes-hardening-guidance/appendix/h/","summary":"要对秘密数据进行静态加密，下面的加密配置文件提供了一个例子，以指定所需的加密类型和加密密钥。将加密密钥存储在加密文件中只能稍微提高安全性。Secret 将被加密，但密钥将在 EncryptionConfiguration 文件中被访问。这个例子是基于","title":"附录 H：加密示例"},{"content":"这篇文档适用于要构建高可用应用程序的所有者，因此他们需要了解 Pod 可能发生什么类型的中断。也适用于要执行自动集群操作的集群管理员，如升级和集群自动扩容。\n自愿中断和非自愿中断 Pod 不会消失，直到有人（人类或控制器）将其销毁，或者当出现不可避免的硬件或系统软件错误。\n我们把这些不可避免的情况称为应用的非自愿性中断。例如：\n后端节点物理机的硬件故障 集群管理员错误地删除虚拟机（实例） 云提供商或管理程序故障使虚拟机消失 内核恐慌（kernel panic） 节点由于集群网络分区而从集群中消失 由于节点资源不足而将容器逐出 除资源不足的情况外，大多数用户应该都熟悉以下这些情况；它们不是特定于 Kubernetes 的。\n我们称这些情况为”自愿中断“。包括由应用程序所有者发起的操作和由集群管理员发起的操作。典型的应用程序所有者操作包括：\n删除管理该 pod 的 Deployment 或其他控制器 更新了 Deployment 的 pod 模板导致 pod 重启 直接删除 pod（意外删除） 集群管理员操作包括：\n排空（drain）节点进行修复或升级。 从集群中排空节点以缩小集群。 从节 …","relpermalink":"/kubernetes-handbook/objects/pod-disruption-budget/","summary":"这篇文档适用于要构建高可用应用程序的所有者，因此他们需要了解 Pod 可能发生什么类型的中断。也适用于要执行自动集群操作的集群管理员，如升级和集群自动扩容。 自愿中断和非自愿中断 Pod 不会消失，直到有人（人类或控制","title":"Pod 中断与 PDB（Pod 中断预算）"},{"content":"要用密钥管理服务（KMS）提供商插件来加密 Secret，可以使用以下加密配置 YAML 文件的例子来为提供商设置属性。这个例子是基于 Kubernetes 的官方文档。\napiVersion: apiserver.config.k8s.io/v1 kind: EncryptionConfiguration resources: - resources: - secrets providers: - kms: name: myKMSPlugin endpoint: unix://tmp/socketfile.sock cachesize: 100 timeout: 3s - identity: {} 要配置 API 服务器使用 KMS 提供商，请将 --encryption-provider-config 标志与配置文件的位置一起设置，并重新启动 API 服务器。\n要从本地加密提供者切换到 KMS，请将 EncryptionConfiguration 文件中的 KMS 提供者部分添加到当前加密方法之上，如下所示。\napiVersion: …","relpermalink":"/kubernetes-hardening-guidance/appendix/i/","summary":"要用密钥管理服务（KMS）提供商插件来加密 Secret，可以使用以下加密配置 YAML 文件的例子来为提供商设置属性。这个例子是基于 Kubernetes 的官方文档。 apiVersion: apiserver.config.k8s.io/v1 kind: EncryptionConfiguration resources: - resources: - secrets providers: - kms: name: myKMSPlugin endpoint: unix://tmp/socketfile.sock cachesize: 100 timeout: 3s - identity: {} 要配置 API 服务器使用","title":"附录 I：KMS 配置实例"},{"content":"众所周知，Kubernetes 是 Google 于 2014 年 6 月基于其内部使用的 Borg 系统开源出来的容器编排调度引擎。其实从 2000 年开始，Google 就开始基于容器研发三个容器管理系统，分别是 Borg、Omega 和 Kubernetes。这篇由 Google 工程师 Brendan Burns、Brian Grant、David Oppenheimer、Eric Brewer 和 John Wilkes 几人在 2016 年发表的《Borg, Omega, and Kubernetes》论文里，阐述了 Google 从 Borg 到 Kubernetes 这个旅程中所获得知识和经验教训。\nBorg、Omega 和 Kubernetes Google 从 2000 年初就开始使用容器（Linux 容器）系统，Google 开发出来的第一个统一的容器管理系统在内部称之为“Borg”，用来管理长时间运行的生产服务和批处理服务。由于 Borg 的规模、功能的广泛性和超高的稳定性，一直到现在 Borg 在 Google 内部依然是主要的容器管理系统。\nGoogle 的 …","relpermalink":"/cloud-native-handbook/kubernetes/history/","summary":"众所周知，Kubernetes 是 Google 于 2014 年 6 月基于其内部使用的 Borg 系统开源出来的容器编排调度引擎。其实从 2000 年开始，Google 就开始基于容器研发三个容器管理系统，分别是 Borg、Omega 和 Kuberne","title":"Kubernetes 的历史"},{"content":"要创建一个 pod-reader 角色，创建一个 YAML 文件，内容如下：\napiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: namespace: your-namespace-name name: pod-reader rules: - apiGroups: [\u0026#34;\u0026#34;] # \u0026#34;\u0026#34; 表示核心 API 组 resources: [\u0026#34;pods\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;list\u0026#34;] 应用角色：\nkubectl apply --f role.yaml 要创建一个全局性的 pod-reader ClusterRole：\napiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: default # \u0026#34;namespace\u0026#34; 被省略了，因为 ClusterRoles 没有被绑定到一个命名空间上 name: global-pod-reader rules: - apiGroups: [\u0026#34;\u0026#34;] # \u0026#34;\u0026#34; 表示核心 API …","relpermalink":"/kubernetes-hardening-guidance/appendix/j/","summary":"要创建一个 pod-reader 角色，创建一个 YAML 文件，内容如下： apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: namespace: your-namespace-name name: pod-reader rules: - apiGroups: [\"\"] # \"\" 表示核心 API 组 resources: [\"pods\"] verbs: [\"get\", \"watch\", \"list\"] 应用角色： kubectl apply --f role.yaml 要创建一个全局性的 pod-reader ClusterRole： apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: default # \"namespace\" 被省略了，因为 ClusterRoles 没有被绑","title":"附录 J：pod-reader RBAC 角色"},{"content":"Node 是 Kubernetes 集群的工作节点，可以是物理机也可以是虚拟机。\nNode 的状态 Node 包括如下状态信息：\nAddress HostName：可以被 kubelet 中的 --hostname-override 参数替代。 ExternalIP：可以被集群外部路由到的 IP 地址。 InternalIP：集群内部使用的 IP，集群外部无法访问。 Condition OutOfDisk：磁盘空间不足时为 True Ready：Node controller 40 秒内没有收到 node 的状态报告为 Unknown，健康为 True，否则为 False。 MemoryPressure：当 node 有内存压力时为 True，否则为 False。 DiskPressure：当 node 有磁盘压力时为 True，否则为 False。 Capacity CPU 内存 可运行的最大 Pod 个数 Info：节点的一些版本信息，如 OS、kubernetes、docker 等 Node 管理 禁止 Pod 调度到该节点上。\nkubectl cordon \u0026lt;node\u0026gt; 驱逐该 …","relpermalink":"/kubernetes-handbook/cluster/node/","summary":"Node 是 Kubernetes 集群的工作节点，可以是物理机也可以是虚拟机。 Node 的状态 Node 包括如下状态信息： Address HostName：可以被 kubelet 中的 --hostname-override 参数替代。 ExternalIP：可以被集群外部路由到的 IP 地址。 InternalIP：集群","title":"Node"},{"content":"要创建一个 RoleBinding，需创建一个 YAML 文件，内容如下：\napiVersion: rbac.authorization.k8s.io/v1 # 这个角色绑定允许 \u0026#34;jane\u0026#34; 读取 \u0026#34;your-namespace-name\u0026#34; 的 Pod 命名空间 # 你需要在该命名空间中已经有一个名为 \u0026#34;pod-reader\u0026#34;的角色。 kind: RoleBinding metadata: name: read-pods namespace: your-namespace-name subjects: # 你可以指定一个以上的 \u0026#34;subject\u0026#34; - kind: User name: jane # \u0026#34;name\u0026#34; 是大小写敏感的 apiGroup: rbac.authorization.k8s.io roleRef: # \u0026#34;roleRef\u0026#34; 指定绑定到一个 Role/ClusterRole kind: Role # 必须是 Role 或 ClusterRole name: pod-reader # 这必须与你想绑定的 Role 或 ClusterRole …","relpermalink":"/kubernetes-hardening-guidance/appendix/k/","summary":"要创建一个 RoleBinding，需创建一个 YAML 文件，内容如下： apiVersion: rbac.authorization.k8s.io/v1 # 这个角色绑定允许 \"jane\" 读取 \"your-namespace-name\" 的 Pod 命名空间 # 你需要在该命名空间中已经有一个名为 \"pod-reader\"的角色。 kind: RoleBinding metadata: name: read-pods namespace:","title":"附录 K：RBAC RoleBinding 和 ClusterRoleBinding 示例"},{"content":"在一个 Kubernetes 集群中可以使用 namespace 创建多个“虚拟集群”，这些 namespace 之间可以完全隔离，也可以通过某种方式，让一个 namespace 中的 service 可以访问到其他的 namespace 中的服务。\n哪些情况下适合使用多个 namespace 因为 namespace 可以提供独立的命名空间，因此可以实现部分的环境隔离。当你的项目和人员众多的时候可以考虑根据项目属性，例如生产、测试、开发划分不同的 namespace。\nNamespace 使用 获取集群中有哪些 namespace\nkubectl get ns\n集群中默认会有 default 和 kube-system 这两个 namespace。\n在执行 kubectl 命令时可以使用 -n 指定操作的 namespace。\n用户的普通应用默认是在 default 下，与集群管理相关的为整个集群提供服务的应用一般部署在 kube-system 的 namespace 下，例如我们在安装 kubernetes 集群时部署的 kubedns、heapseter、EFK …","relpermalink":"/kubernetes-handbook/cluster/namespace/","summary":"在一个 Kubernetes 集群中可以使用 namespace 创建多个“虚拟集群”，这些 namespace 之间可以完全隔离，也可以通过某种方式，让一个 namespace 中的 service 可以访问到其他的 namespace 中的服务。 哪些情况下适合使用多个 namespace 因为 namespace 可以提供独立的命名空间，因此可以实现部","title":"Namespace"},{"content":"下面是一个审计策略，它以最高级别记录所有审计事件：\napiVersion: audit.k8s.io/v1 kind: Policy rules: - level: RequestResponse # 这个审计策略记录了 RequestResponse 级别的所有审计事件 这种审计策略在最高级别上记录所有事件。如果一个组织有可用的资源来存储、解析和检查大量的日志，那么在最高级别上记录所有事件是一个很好的方法，可以确保当事件发生时，所有必要的背景信息都出现在日志中。如果资源消耗和可用性是一个问题，那么可以建立更多的日志规则来降低非关键组件和常规非特权操作的日志级别，只要满足系统的审计要求。如何建立这些规则的例子可以在 Kubernetes 官方文档中找到。\n","relpermalink":"/kubernetes-hardening-guidance/appendix/l/","summary":"下面是一个审计策略，它以最高级别记录所有审计事件： apiVersion: audit.k8s.io/v1 kind: Policy rules: - level: RequestResponse # 这个审计策略记录了 RequestResponse 级别的所有审计事件 这种审计策略在最高级别上记录所有事件。如果一个组织有可用的资源来存储、解析和检查大量的日志，那","title":"附录 L：审计策略"},{"content":"选择性同步是仅同步某些资源的同步。你可以从 UI 中选择哪些资源：\n选择性同步 这样做时，请记住： 你的同步不会记录在历史记录中，因此无法回滚。 Hook 未运行。 选择性同步选项 v1.8\n打开选择性同步选项，该选项将仅同步不同步的资源。有关更多详细信息，请参阅同步选项文档。\n","relpermalink":"/argo-cd/user-guide/selective-sync/","summary":"选择性同步是仅同步某些资源的同步。你可以从 UI 中选择哪些资源： 选择性同步 这样做时，请记住： 你的同步不会记录在历史记录中，因此无法回滚。 Hook 未运行。 选择性同步选项 v1.8 打开选择性同步选项，该选项将仅同步不同步的","title":"选择性同步"},{"content":"Label 是附着到 object 上（例如 Pod）的键值对。可以在创建 object 的时候指定，也可以在 object 创建后随时指定。Labels 的值对系统本身并没有什么含义，只是对用户才有意义。\n\u0026#34;labels\u0026#34;: { \u0026#34;key1\u0026#34; : \u0026#34;value1\u0026#34;, \u0026#34;key2\u0026#34; : \u0026#34;value2\u0026#34; } Kubernetes 最终将对 labels 最终索引和反向索引用来优化查询和 watch，在 UI 和命令行中会对它们排序。不要在 label 中使用大型、非标识的结构化数据，记录这样的数据应该用 annotation。\n动机 Label 能够将组织架构映射到系统架构上（就像是康威定律），这样能够更便于微服务的管理，你可以给 object 打上如下类型的 label：\n\u0026#34;release\u0026#34; : \u0026#34;stable\u0026#34;, \u0026#34;release\u0026#34; : \u0026#34;canary\u0026#34; \u0026#34;environment\u0026#34; : \u0026#34;dev\u0026#34;, \u0026#34;environment\u0026#34; : \u0026#34;qa\u0026#34;, \u0026#34;environment\u0026#34; : \u0026#34;production\u0026#34; \u0026#34;tier\u0026#34; : \u0026#34;frontend\u0026#34;, \u0026#34;tier\u0026#34; : \u0026#34;backend\u0026#34;, …","relpermalink":"/kubernetes-handbook/cluster/label/","summary":"Label 是附着到 object 上（例如 Pod）的键值对。可以在创建 object 的时候指定，也可以在 object 创建后随时指定。Labels 的值对系统本身并没有什么含义，只是对用户才有意义。 \"labels\": { \"key1\" : \"value1\", \"key2\" : \"value2\" } Kubernetes 最终将对 labels 最终索引和反向索引用","title":"Label"},{"content":"在控制平面，用文本编辑器打开 kube-apiserver.yaml 文件。编辑 kube-apiserver 配置需要管理员权限。\nsudo vi /etc/kubernetes/manifests/kube-apiserver.yaml 在 kube-apiserver.yaml 文件中添加以下文字：\n--audit-policy-file=/etc/kubernetes/policy/audit-policy.yaml --audit-log-path=/var/log/audit.log --audit-log-maxage=1825 audit-policy-file 标志应该设置为审计策略的路径，而 audit-log-path 标志应该设置为所需的审计日志写入的安全位置。还有一些其他的标志，比如这里显示的 audit-log-maxage 标志，它规定了日志应该被保存的最大天数，还有一些标志用于指定要保留的最大审计日志文件的数量，最大的日志文件大小（兆字节）等等。启用日志记录的唯一必要标志是 audit-policy-file 和 audit-log-path 标志。其他 …","relpermalink":"/kubernetes-hardening-guidance/appendix/m/","summary":"在控制平面，用文本编辑器打开 kube-apiserver.yaml 文件。编辑 kube-apiserver 配置需要管理员权限。 sudo vi /etc/kubernetes/manifests/kube-apiserver.yaml 在 kube-apiserver.yaml 文件中添加以下文字： --audit-policy-file=/etc/kubernetes/policy/audit-policy.yaml --audit-log-path=/var/log/audit.log --audit-log-maxage=1825 audit-policy-file 标志应该设置为审计策略的路径，而 audit-log-path 标志应该设置为所需的审计日志写入的安全位置。还有一些其他的标志，比","title":"附录 M：向 kube-apiserver 提交审计策略文件的标志示例"},{"content":"Annotation，顾名思义，就是注解。Annotation 可以将 Kubernetes 资源对象关联到任意的非标识性元数据。使用客户端（如工具和库）可以检索到这些元数据。\n关联元数据到对象 Label 和 Annotation 都可以将元数据关联到 Kubernetes 资源对象。Label 主要用于选择对象，可以挑选出满足特定条件的对象。相比之下，annotation 不能用于标识及选择对象。annotation 中的元数据可多可少，可以是结构化的或非结构化的，也可以包含 label 中不允许出现的字符。\nAnnotation 和 label 一样都是 key/value 键值对映射结构：\n\u0026#34;annotations\u0026#34;: {\u0026#34;key1\u0026#34;:\u0026#34;value1\u0026#34;,\u0026#34;key2\u0026#34;:\u0026#34;value2\u0026#34;} 以下列出了一些可以记录在 annotation 中的对象信息：\n声明配置层管理的字段。使用 annotation 关联这类字段可以用于区分以下几种配置来源：客户端或服务器设置的默认值，自动生成的字段或自动生成的 auto-scaling 和 auto-sizing 系统配置的字段。\n创建信息、版 …","relpermalink":"/kubernetes-handbook/cluster/annotation/","summary":"Annotation，顾名思义，就是注解。Annotation 可以将 Kubernetes 资源对象关联到任意的非标识性元数据。使用客户端（如工具和库）可以检索到这些元数据。 关联元数据到对象 Label 和 Annotation 都可以将元数据关联到 Kubernetes 资源","title":"Annotation"},{"content":"YAML 文件示例：\napiVersion: v1 kind: Config preferences: {} clusters: - name: example-cluster cluster: server: http://127.0.0.1:8080 #web endpoint address for the log files to be sent to name: audit-webhook-service users: - name: example-users user: username: example-user password: example-password contexts: - name: example-context context: cluster: example-cluster user: example-user current-context: example-context #source: https://dev.bitolog.com/implement-audits-webhook/ 由 webhook 发送的审计事件是以 HTTP …","relpermalink":"/kubernetes-hardening-guidance/appendix/n/","summary":"YAML 文件示例： apiVersion: v1 kind: Config preferences: {} clusters: - name: example-cluster cluster: server: http://127.0.0.1:8080 #web endpoint address for the log files to be sent to name: audit-webhook-service users: - name: example-users user: username: example-user password: example-password contexts: - name: example-context context: cluster: example-cluster user: example-user current-context: example-context #source: https://dev.bitolog.com/implement-audits-webhook/ 由 webhook 发送的审计事件是以 HTTP POST 请求的形式发送的，请求体中包含 JSON 审计事件。指定的地址应该指向一个能","title":"附录 N：webhook 配置"},{"content":"Taint（污点）和 Toleration（容忍）可以作用于 node 和 pod 上，其目的是优化 pod 在集群间的调度，这跟节点亲和性类似，只不过它们作用的方式相反，具有 taint 的 node 和 pod 是互斥关系，而具有节点亲和性关系的 node 和 pod 是相吸的。另外还有可以给 node 节点设置 label，通过给 pod 设置 nodeSelector 将 pod 调度到具有匹配标签的节点上。\nTaint 和 toleration 相互配合，可以用来避免 pod 被分配到不合适的节点上。每个节点上都可以应用一个或多个 taint，这表示对于那些不能容忍这些 taint 的 pod，是不会被该节点接受的。如果将 toleration 应用于 pod 上，则表示这些 pod 可以（但不要求）被调度到具有相应 taint 的节点上。\n示例 以下分别以为 node 设置 taint 和为 pod 设置 toleration 为例。\n为 node 设置 taint 为 node1 设置 taint：\nkubectl taint nodes node1 …","relpermalink":"/kubernetes-handbook/cluster/taint-and-toleration/","summary":"Taint（污点）和 Toleration（容忍）可以作用于 node 和 pod 上，其目的是优化 pod 在集群间的调度，这跟节点亲和性类似，只不过它们作用的方式相反，具有 taint 的 node 和 pod 是互斥关系，而具有节点亲和性关系的 node 和 pod 是","title":"Taint 和 Toleration（污点和容忍）"},{"content":"Kubernetes 垃圾收集器的角色是删除指定的对象，这些对象曾经有但以后不再拥有 Owner 了。\nOwner 和 Dependent 一些 Kubernetes 对象是其它一些的 Owner。例如，一个 ReplicaSet 是一组 Pod 的 Owner。具有 Owner 的对象被称为是 Owner 的 Dependent。每个 Dependent 对象具有一个指向其所属对象的 metadata.ownerReferences 字段。\n有时，Kubernetes 会自动设置 ownerReference 的值。例如，当创建一个 ReplicaSet 时，Kubernetes 自动设置 ReplicaSet 中每个 Pod 的 ownerReference 字段值。在 1.6 版本，Kubernetes 会自动为一些对象设置 ownerReference 的值，这些对象是由 ReplicationController、ReplicaSet、StatefulSet、DaemonSet 和 Deployment 所创建或管理。\n也可以通过手动设置 ownerReference 的值， …","relpermalink":"/kubernetes-handbook/cluster/garbage-collection/","summary":"Kubernetes 垃圾收集器的角色是删除指定的对象，这些对象曾经有但以后不再拥有 Owner 了。 Owner 和 Dependent 一些 Kubernetes 对象是其它一些的 Owner。例如，一个 ReplicaSet 是一组 Pod 的 Owner。具有 Owner 的对象被称为是 Owner 的 Dependent。每个 Dependent 对象具","title":"垃圾收集"},{"content":"Deployment 为 Pod 和 ReplicaSet 提供了一个声明式定义（declarative）方法，用来替代以前的 ReplicationController 来方便的管理应用。典型的应用场景包括：\n定义 Deployment 来创建 Pod 和 ReplicaSet 滚动升级和回滚应用 扩容和缩容 暂停和继续 Deployment 比如一个简单的 nginx 应用可以定义为：\napiVersion: extensions/v1beta1 kind: Deployment metadata: name: nginx-deployment spec: replicas: 3 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.7.9 ports: - containerPort: 80 扩容：\nkubectl scale deployment nginx-deployment --replicas 10 如果集群支持 horizontal pod …","relpermalink":"/kubernetes-handbook/controllers/deployment/","summary":"Deployment 为 Pod 和 ReplicaSet 提供了一个声明式定义（declarative）方法，用来替代以前的 ReplicationController 来方便的管理应用。典型的应用场景包括： 定义 Deployment 来创建 Pod 和 ReplicaSet 滚动升级和回滚应用 扩容和缩容 暂停和继续 Deployment 比如一个简单的 nginx 应用可以定","title":"Deployment"},{"content":"StatefulSet 作为 Controller 为 Pod 提供唯一的标识。它可以保证部署和 scale 的顺序。\n使用案例参考：kubernetes contrib - statefulsets，其中包含 zookeeper 和 kakfa 的 statefulset 设置和使用说明。\nStatefulSet 是为了解决有状态服务的问题（对应 Deployments 和 ReplicaSets 是为无状态服务而设计），其应用场景包括：\n稳定的持久化存储，即 Pod 重新调度后还是能访问到相同的持久化数据，基于 PVC 来实现 稳定的网络标志，即 Pod 重新调度后其 PodName 和 HostName 不变，基于 Headless Service（即没有 Cluster IP 的 Service）来实现 有序部署，有序扩展，即 Pod 是有顺序的，在部署或者扩展的时候要依据定义的顺序依次依次进行（即从 0 到 N-1，在下一个 Pod 运行之前所有之前的 Pod 必须都是 Running 和 Ready 状态），基于 init containers 来实现 有序收缩，有序删除（ …","relpermalink":"/kubernetes-handbook/controllers/statefulset/","summary":"StatefulSet 作为 Controller 为 Pod 提供唯一的标识。它可以保证部署和 scale 的顺序。 使用案例参考：kubernetes contrib - statefulsets，其中包含 zookeeper 和 kakfa 的 statefulset 设置和使用说明。 StatefulSet 是为了解决有状态服务的问题（对应 Deployments 和 ReplicaSets 是为无状","title":"StatefulSet"},{"content":"将应用程序的功能划分为单独的进程运行在同一个最小调度单元中（例如 Kubernetes 中的 Pod）可以被视为 sidecar 模式。如下图所示，sidecar 模式允许您在应用程序旁边添加更多功能，而无需额外第三方组件配置或修改应用程序代码。\nSidecar 模式示意图 就像连接了 Sidecar 的三轮摩托车一样，在软件架构中，Sidecar 连接到父应用并且为其添加扩展或者增强功能。Sidecar 应用与主应用程序松散耦合。它可以屏蔽不同编程语言的差异，统一实现微服务的可观测性、监控、日志记录、配置、断路器等功能。\n使用 Sidecar 模式的优势 使用 sidecar 模式部署服务网格时，无需在节点上运行代理，但是集群中将运行多个相同的 sidecar 副本。在 sidecar 部署方式中，每个应用的容器旁都会部署一个伴生容器，这个容器称之为 sidecar 容器。Sidecar 接管进出应用容器的所有流量。在 Kubernetes 的 Pod 中，在原有的应用容器旁边注入一个 Sidecar 容器，两个容器共享存储、网络等资源，可以广义的将这个包含了 sidecar …","relpermalink":"/cloud-native-handbook/kubernetes/sidecar-pattern/","summary":"将应用程序的功能划分为单独的进程运行在同一个最小调度单元中（例如 Kubernetes 中的 Pod）可以被视为 sidecar 模式。如下图所示，sidecar 模式允许您在应用程序旁边添加更多功能，而无需额外第三方组件配置或修改应用程序代","title":"Sidecar 模式"},{"content":"本文将为您介绍 DaemonSet 的基本概念。\n什么是 DaemonSet？ DaemonSet 确保全部（或者一些）Node 上运行一个 Pod 的副本。当有 Node 加入集群时，也会为他们新增一个 Pod。当有 Node 从集群移除时，这些 Pod 也会被回收。删除 DaemonSet 将会删除它创建的所有 Pod。\n使用 DaemonSet 的一些典型用法：\n运行集群存储 daemon，例如在每个 Node 上运行 glusterd、ceph。 在每个 Node 上运行日志收集 daemon，例如fluentd、logstash。 在每个 Node 上运行监控 daemon，例如 Prometheus Node Exporter、collectd、Datadog 代理、New Relic 代理，或 Ganglia gmond。 一个简单的用法是，在所有的 Node 上都存在一个 DaemonSet，将被作为每种类型的 daemon 使用。 一个稍微复杂的用法可能是，对单独的每种类型的 daemon 使用多个 DaemonSet，但具有不同的标志，和/或对不同硬件类型具有不同的 …","relpermalink":"/kubernetes-handbook/controllers/daemonset/","summary":"本文将为您介绍 DaemonSet 的基本概念。 什么是 DaemonSet？ DaemonSet 确保全部（或者一些）Node 上运行一个 Pod 的副本。当有 Node 加入集群时，也会为他们新增一个 Pod。当有 Node 从集群移除时，这些 Pod 也会被回收。删除 DaemonSet 将会删除","title":"DaemonSet"},{"content":"ReplicationController 用来确保容器应用的副本数始终保持在用户定义的副本数，即如果有容器异常退出，会自动创建新的 Pod 来替代；而如果异常多出来的容器也会自动回收。\n在新版本的 Kubernetes 中建议使用 ReplicaSet 来取代 ReplicationController。ReplicaSet 跟 ReplicationController 没有本质的不同，只是名字不一样，并且 ReplicaSet 支持集合式的 selector。\n虽然 ReplicaSet 可以独立使用，但一般还是建议使用 Deployment 来自动管理 ReplicaSet，这样就无需担心跟其他机制的不兼容问题（比如 ReplicaSet 不支持 rolling-update 但 Deployment 支持）。\nReplicaSet 示例：\napiVersion: extensions/v1beta1 kind: ReplicaSet metadata: name: frontend # these labels can be applied automatically # …","relpermalink":"/kubernetes-handbook/controllers/replicaset/","summary":"ReplicationController 用来确保容器应用的副本数始终保持在用户定义的副本数，即如果有容器异常退出，会自动创建新的 Pod 来替代；而如果异常多出来的容器也会自动回收。 在新版本的 Kubernetes 中建议使用 ReplicaSet 来取代 ReplicationContro","title":"ReplicationController 和 ReplicaSet"},{"content":"Job 负责批处理任务，即仅执行一次的任务，它保证批处理任务的一个或多个 Pod 成功结束。\nJob Spec 格式 spec.template 格式同 Pod RestartPolicy 仅支持 Never 或 OnFailure 单个 Pod 时，默认 Pod 成功运行后 Job 即结束 .spec.completions 标志 Job 结束需要成功运行的 Pod 个数，默认为 1 .spec.parallelism 标志并行运行的 Pod 的个数，默认为 1 spec.activeDeadlineSeconds 标志失败 Pod 的重试最大时间，超过这个时间不会继续重试 一个简单的例子：\napiVersion: batch/v1 kind: Job metadata: name: pi spec: template: metadata: name: pi spec: containers: - name: pi image: perl command: [\u0026#34;perl\u0026#34;, \u0026#34;-Mbignum=bpi\u0026#34;, \u0026#34;-wle\u0026#34;, \u0026#34;print bpi(2000)\u0026#34;] …","relpermalink":"/kubernetes-handbook/controllers/job/","summary":"Job 负责批处理任务，即仅执行一次的任务，它保证批处理任务的一个或多个 Pod 成功结束。 Job Spec 格式 spec.template 格式同 Pod RestartPolicy 仅支持 Never 或 OnFailure 单个 Pod 时，默认 Pod 成功运行后 Job 即结束 .spec.completions 标志 Job 结束需要成功运行的 Pod 个数，默认为 1 .spec.parallelism 标志并行运行","title":"Job"},{"content":"Cron Job 管理基于时间的 Job，即：\n在给定时间点只运行一次 周期性地在给定时间点运行 一个 CronJob 对象类似于 crontab （cron table）文件中的一行。它根据指定的预定计划周期性地运行一个 Job，格式可以参考 Cron 。\n前提条件 当前使用的 Kubernetes 集群，版本 \u0026gt;= 1.8（对 CronJob）。对于先前版本的集群，版本 \u0026lt; 1.8，启动 API Server（参考 为集群开启或关闭 API 版本 获取更多信息）时，通过传递选项 --runtime-config=batch/v2alpha1=true 可以开启 batch/v2alpha1 API。\n典型的用法如下所示：\n在给定的时间点调度 Job 运行 创建周期性运行的 Job，例如：数据库备份、发送邮件。 CronJob Spec .spec.schedule：调度，必需字段，指定任务运行周期，格式同 Cron\n.spec.jobTemplate：Job 模板，必需字段，指定需要运行的任务，格式同 Job\n.spec.startingDeadlineSeconds ： …","relpermalink":"/kubernetes-handbook/controllers/cronjob/","summary":"Cron Job 管理基于时间的 Job，即： 在给定时间点只运行一次 周期性地在给定时间点运行 一个 CronJob 对象类似于 crontab （cron table）文件中的一行。它根据指定的预定计划周期性地运行一个 Job，格式可以参考 Cron 。 前提条件 当","title":"CronJob"},{"content":"为了使 Ingress 正常工作，集群中必须运行 Ingress controller。这与其他类型的控制器不同，其他类型的控制器通常作为 kube-controller-manager 二进制文件的一部分运行，在集群启动时自动启动。你需要选择最适合自己集群的 Ingress controller 或者自己实现一个。\nKubernetes 社区和众多厂商开发了大量的 Ingress Controller，你可以在 这里 找到。\n使用多个 Ingress 控制器 你可以使用 IngressClass 在集群中部署任意数量的 Ingress 控制器。请注意你的 Ingress 类资源的 .metadata.name 字段。当你创建 Ingress 时，你需要用此字段的值来设置 Ingress 对象的 ingressClassName 字段（请参考 IngressSpec v1 reference）。 ingressClassName 是之前的注解做法的替代。\n如果你不为 Ingress 指定 IngressClass，并且你的集群中只有一个 IngressClass 被标记为了集群默认，那 …","relpermalink":"/kubernetes-handbook/controllers/ingress-controller/","summary":"为了使 Ingress 正常工作，集群中必须运行 Ingress controller。这与其他类型的控制器不同，其他类型的控制器通常作为 kube-controller-manager 二进制文件的一部分运行，在集群启动时自动启动。你需要选择最适合自己集群的 Ingress controller 或者自己实现一个","title":"Ingress 控制器"},{"content":"Kubernetes 中不仅支持 CPU、内存为指标的 HPA，还支持自定义指标的 HPA，例如 QPS。\n设置自定义指标 Kubernetes 1.6\n在 Kubernetes1.6 集群中配置自定义指标的 HPA 的说明已废弃。\n在设置定义指标 HPA 之前需要先进行如下配置：\n将 heapster 的启动参数 --api-server 设置为 true\n启用 custom metric API\n将 kube-controller-manager 的启动参数中 --horizontal-pod-autoscaler-use-rest-clients 设置为 true，并指定 --master 为 API server 地址，如 --master=http://172.20.0.113:8080\n在 Kubernetes1.5 以前很容易设置，参考 1.6 以前版本的 kubernetes 中开启自定义 HPA，而在 1.6 中因为取消了原来的 annotation 方式设置 custom metric，只能通过 API server 和 kube-aggregator …","relpermalink":"/kubernetes-handbook/controllers/hpa/custom-metrics-hpa/","summary":"Kubernetes 中不仅支持 CPU、内存为指标的 HPA，还支持自定义指标的 HPA，例如 QPS。 设置自定义指标 Kubernetes 1.6 在 Kubernetes1.6 集群中配置自定义指标的 HPA 的说明已废弃。 在设置定义指标 HPA 之前需要先进行如下配置： 将 heapster 的启动参数 --api-server 设置为","title":"自定义指标 HPA"},{"content":"准入控制器（Admission Controller）位于 API Server 中，在对象被持久化之前，准入控制器拦截对 API Server 的请求，一般用来做身份验证和授权。其中包含两个特殊的控制器：MutatingAdmissionWebhook 和 ValidatingAdmissionWebhook。分别作为配置的变异和验证准入控制 webhook。\n准入控制器包括以下两种：\n变更（Mutating）准入控制：修改请求的对象 验证（Validating）准入控制：验证请求的对象 准入控制器是在 API Server 的启动参数重配置的。一个准入控制器可能属于以上两者中的一种，也可能两者都属于。当请求到达 API Server 的时候首先执行变更准入控制，然后再执行验证准入控制。\n我们在部署 Kubernetes 集群的时候都会默认开启一系列准入控制器，如果没有设置这些准入控制器的话可以说你的 Kubernetes 集群就是在裸奔，应该只有集群管理员可以修改集群的准入控制器。\n例如我会默认开启如下的准入控制器。 …","relpermalink":"/kubernetes-handbook/controllers/admission-controller/","summary":"准入控制器（Admission Controller）位于 API Server 中，在对象被持久化之前，准入控制器拦截对 API Server 的请求，一般用来做身份验证和授权。其中包含两个特殊的控制器：MutatingAdmissionW","title":"准入控制器（Admission Controller）"},{"content":"Kubernetes Pod 是有生命周期的，它们可以被创建，也可以被销毁，然而一旦被销毁生命就永远结束。通过 ReplicationController 能够动态地创建和销毁 Pod。每个 Pod 都会获取它自己的 IP 地址，即使这些 IP 地址不总是稳定可依赖的。这会导致一个问题：在 Kubernetes 集群中，如果一组 Pod（称为 backend）为其它 Pod （称为 frontend）提供服务，那么 frontend Pod 该如何发现和连接哪些 backend Pod 呢？\n关于 Service Kubernetes Service 定义了这样一种抽象：Pod 的逻辑分组，一种可以访问它们的策略 —— 通常称为微服务。这一组 Pod 能够被 Service 访问到，通常是通过 Label Selector（查看下面了解，为什么可能需要没有 selector 的 Service）实现的。\n举个例子，假设有一个用于图片处理的运行了三个副本的 backend。这些副本是可互换的 —— frontend 不需要关心它们调用了哪个 backend 副本。 …","relpermalink":"/kubernetes-handbook/service-discovery/service/","summary":"Kubernetes Pod 是有生命周期的，它们可以被创建，也可以被销毁，然而一旦被销毁生命就永远结束。通过 ReplicationController 能够动态地创建和销毁 Pod。每个 Pod 都会获取它自己的 IP 地址，即使这些 IP 地址不总是稳定可依赖的。这会导致一个问题：在 Kubernetes","title":"Service"},{"content":"拓扑感知路由指的是客户端对一个服务的访问流量，可以根据这个服务的端点拓扑，优先路由到与该客户端在同一个节点或者可用区的端点上的路由行为。\n先决条件 为了开启服务感知路由，你需要：\n开启 TopologyAwareHints 智能感知提示门控 开启 EndpointSlice 控制器 安装 kube-proxy 端点切片 我们知道 Endpoint 通常情况下是由 Service 资源自动创建和管理的，但是随着 Kubernetes 集群的规模越来越大和管理的服务越来越多，Endpoint API 的局限性变得越来越明显。 端点切片（EndpointSlices）提供了一种简单的方法来跟踪 Kubernetes 集群中的网络端点。它们为 Endpoint 提供了一种可伸缩和可拓展的替代方案，同时还可以被用到拓扑感知路由中。\nEndpointSlices 示例如下：\napiVersion: discovery.k8s.io/v1 kind: EndpointSlice metadata: name: example-hints labels: …","relpermalink":"/kubernetes-handbook/service-discovery/topology-aware-routing/","summary":"拓扑感知路由指的是客户端对一个服务的访问流量，可以根据这个服务的端点拓扑，优先路由到与该客户端在同一个节点或者可用区的端点上的路由行为。 先决条件 为了开启服务感知路由，你需要： 开启 TopologyAwareHints 智能感知提示门控 开启","title":"拓扑感知路由"},{"content":"Ingress 是从 Kubernetes 集群外部访问集群内部服务的入口，是将 Kubernetes 集群内部服务暴露到外界的几种方式之一。本文将为你详细介绍 Ingress 资源对象。\n术语\n在本篇文章中你将会看到一些在其他地方被交叉使用的术语，为了防止产生歧义，我们首先来澄清下。\n节点：Kubernetes 集群中的一台物理机或者虚拟机。 集群：位于 Internet 防火墙后的节点，这是 Kubernetes 管理的主要计算资源。 边界路由器：为集群强制执行防火墙策略的路由器。这可能是由云提供商或物理硬件管理的网关。 集群网络：一组逻辑或物理链接，可根据 Kubernetes 网络模型 实现群集内的通信。集群网络的实现包括 Overlay 模型的 flannel 和基于 SDN 的 OVS。 服务：使用标签选择器标识一组 pod 成为的 Kubernetes 服务。除非另有说明，否则服务假定在集群网络内仅可通过虚拟 IP 访问。 什么是 Ingress？ 通常情况下，service 和 pod 仅可在集群内部网络中通过 IP 地址访问。所有到达边界路由器的流量或被丢弃或被转发到 …","relpermalink":"/kubernetes-handbook/service-discovery/ingress/","summary":"Ingress 是从 Kubernetes 集群外部访问集群内部服务的入口，是将 Kubernetes 集群内部服务暴露到外界的几种方式之一。本文将为你详细介绍 Ingress 资源对象。 术语 在本篇文章中你将会看到一些在其他地方被交叉使用的术语，为了防止产生歧义，我们首先来","title":"Ingress"},{"content":" 注意\n本文根据 Gateway API 0.5.1 版本撰写。目前 Gateway API 仅用于处理 Kubernetes 集群中的南北向（入口）流量，未来也有可能处理集群内（东西向）流量。关于使用 Gateway API 处理东西向流量的提议详见此文档。 除了直接使用 Service 和 Ingress 之外，Kubernetes 社区还发起了 Gateway API 项目，它可以帮助我们将 Kubernetes 中的服务暴露到集群外。\nGateway API 是一个由 SIG-NETWORK 管理的开源项目。该项目的目标是在 Kubernetes 生态系统中发展服务网络 API。Gateway API 提供了暴露 Kubernetes 应用的资源——GatewayClass、Gateway、HTTPRoute、TCPRoute 等。\nGateway API 已经得到了大量的网关和服务网格项目支持，请在 Gateway 官方文档中查看支持状况。\n目标 Gateway API 旨在通过提供表现性的、可扩展的、面向角色的接口来改善服务网络，这些接口由许多厂商实现，并得到了业界的广泛支 …","relpermalink":"/kubernetes-handbook/service-discovery/gateway/","summary":"注意 本文根据 Gateway API 0.5.1 版本撰写。目前 Gateway API 仅用于处理 Kubernetes 集群中的南北向（入口）流量，未来也有可能处理集群内（东西向）流量。关于使用 Gateway API 处理东西向流量的提议详见此文档。 除了直接使用 Service 和 Ingress 之外，Kubernete","title":"Gateway API"},{"content":"ServiceAccount 为 Pod 中的进程提供身份信息。\n注意\n本文档描述的关于 ServiceAccount 的行为只有当你按照 Kubernetes 项目建议的方式搭建集群的情况下才有效。集群管理员可能在你的集群中进行了自定义配置，这种情况下该文档可能并不适用。 当你（真人用户）访问集群（例如使用 kubectl 命令）时，API 服务器会将你认证为一个特定的 User Account（目前通常是 admin，除非你的系统管理员自定义了集群配置）。Pod 容器中的进程也可以与 API 服务器联系。当它们在联系 API 服务器的时候，它们会被认证为一个特定的 ServiceAccount（例如default）。\n使用默认的 ServiceAccount 访问 API 服务器 当你创建 pod 的时候，如果你没有指定一个 ServiceAccount，系统会自动得在与该 pod 相同的 namespace 下为其指派一个 default ServiceAccount。如果你获取刚创建的 pod 的原始 json 或 yaml 信息（例如使用kubectl get …","relpermalink":"/kubernetes-handbook/auth/serviceaccount/","summary":"ServiceAccount 为 Pod 中的进程提供身份信息。","title":"ServiceAccount"},{"content":"注意：本文基于 Kubernetes 1.6 撰写，当时 RBAC 模式处于 beta 版本。\n基于角色的访问控制（Role-Based Access Control，即”RBAC”）使用 rbac.authorization.k8s.io API Group 实现授权决策，允许管理员通过 Kubernetes API 动态配置策略。\n要启用 RBAC，请使用 --authorization-mode=RBAC 启动 API Server。\nAPI 概述 本节将介绍 RBAC API 所定义的四种顶级类型。用户可以像使用其他 Kubernetes API 资源一样（例如通过 kubectl、API 调用等）与这些资源进行交互。例如，命令 kubectl create -f (resource).yml 可以被用于以下所有的例子，当然，读者在尝试前可能需要先阅读以下相关章节的内容。\nRole 与 ClusterRole 在 RBAC API 中，一个角色包含了一套表示一组权限的规则。权限以纯粹的累加形式累积（没有”否定”的规则）。角色可以由命名空间（namespace）内的 Role 对 …","relpermalink":"/kubernetes-handbook/auth/rbac/","summary":"注意：本文基于 Kubernetes 1.6 撰写，当时 RBAC 模式处于 beta 版本。 基于角色的访问控制（Role-Based Access Control，即”RBAC”）使用 rbac.authorization.k8s.io API Group 实现授权决策，允许管理员通过 Kubernetes API 动态配置策略。 要启用 RBAC，请使用 --authorization-mode=RBAC","title":"基于角色的访问控制（RBAC）"},{"content":"网络策略说明一组 Pod 之间是如何被允许互相通信，以及如何与其它网络 Endpoint 进行通信。 NetworkPolicy 资源使用标签来选择 Pod，并定义了一些规则，这些规则指明允许什么流量进入到选中的 Pod 上。关于 Network Policy 的详细用法请参考 Kubernetes 官网。\nNetwork Policy 的作用对象是 Pod，也可以应用到 Namespace 和集群的 Ingress、Egress 流量。Network Policy 是作用在 L3/4 层的，即限制的是对 IP 地址和端口的访问，如果需要对应用层做访问限制需要使用如 Istio 这类 Service Mesh。\n前提条件 网络策略通过网络插件来实现，所以必须使用一种支持 NetworkPolicy 的网络方案（如 calico）—— 非 Controller 创建的资源，是不起作用的。\n隔离的与未隔离的 Pod 默认 Pod 是未隔离的，它们可以从任何的源接收请求。具有一个可以选择 Pod 的网络策略后，Pod 就会变成隔离的。一旦 Namespace 中配置的网络策略能够选择一个特定 …","relpermalink":"/kubernetes-handbook/auth/network-policy/","summary":"网络策略说明一组 Pod 之间是如何被允许互相通信，以及如何与其它网络 Endpoint 进行通信。 NetworkPolicy 资源使用标签来选择 Pod，并定义了一些规则，这些规则指明允许什么流量进入到选中的 Pod 上。关于 Network Policy 的详细用法请参考 Kubernetes 官网。 Network Policy 的","title":"NetworkPolicy"},{"content":"SPIFFE，即普适安全生产身份框架（Secure Production Identity Framework for Everyone），是一套开源标准，用于在动态和异构环境中安全地进行身份识别。采用 SPIFFE 的系统无论在哪里运行，都可以轻松可靠地相互认证。\nSPIFFE 开源规范的核心是——通过简单 API 定义了一个短期的加密身份文件 SVID。然后，工作负载进行认证时可以使用该身份文件，例如建立 TLS 连接或签署和验证 JWT 令牌等。\nSPIFFE 已经在云原生应用中得到了大量的应用，尤其是在 Istio 和 Envoy 中。下面将向你介绍 SPIFFE 的一些基本概念。\n工作负载 工作负载是个单一的软件，以特定的配置部署，用于单一目的；它可能包括软件的多个运行实例，所有这些实例执行相同的任务。工作负载这个术语可以包含一系列不同的软件系统定义，包括：\n一个运行 Python 网络应用程序的网络服务器，在一个虚拟机集群上运行，前面有一个负载均衡器。 一个 MySQL 数据库的实例。 一个处理队列中项目的 worker 程序。 独立部署的系统的集合，它们一起工作，例如一个 …","relpermalink":"/kubernetes-handbook/auth/spiffe/","summary":"SPIFFE，即普适安全生产身份框架（Secure Production Identity Framework for Everyone），是一套开源标准，用于在动态和异构环境中安全地进行身份识别。采用 SPIFFE 的系统无论在哪里运行，都可以轻松可靠地相互认证。 SPIFFE 开源规范","title":"SPIFFE"},{"content":"这篇文章将向你介绍 SPIRE 的架构、基本概念及原理。\nSPIRE 是 SPIFFE API 的一个生产就绪的实现，它执行节点和工作负载认证，以便根据一组预先定义的条件，安全地向工作负载发出 SVID，并验证其他工作负载的 SVID。\nSPIRE 架构和组件 SPIRE 部署由一个 SPIRE 服务器和一个或多个 SPIRE 代理组成。服务器充当通过代理向一组工作负载发放身份的签名机构。它还维护一个工作负载身份的注册表，以及为签发这些身份而必须验证的条件。代理在本地向工作负载公开 SPIFFE 工作负载 API，必须安装在工作负载运行的每个节点上。\nSPIRE 架构图 服务器 SPIRE 服务器负责管理和发布其配置的 SPIFFE 信任域中的所有身份。它存储注册条目（指定决定特定 SPIFFE ID 应被签发的条件的选择器）和签名密钥，使用节点证明来自动验证代理的身份，并在被验证的代理请求时为工作负载创建 SVID。\nSPIRE 服务器 服务器的行为是通过一系列的插件决定的。SPIRE 包含几个插件，你可以建立额外的插件来扩展 SPIRE 以满足特定的使用情况。插件的类型包括：\n节点 …","relpermalink":"/kubernetes-handbook/auth/spire/","summary":"这篇文章将向你介绍 SPIRE 的架构、基本概念及原理。","title":"SPIRE"},{"content":"SPIRE Kubernetes 工作负载注册器实现了一个 Kubernetes ValidatingAdmissionWebhook，便于在 Kubernetes 内自动注册工作负载。\n配置 命令行配置 注册器有以下命令行选项：\n标志 描述 默认值 -config 磁盘上 HCL 配置文件的路径 k8s-workload-registrar.conf HCL 配置 配置文件是注册器所必需的。它包含 HCL 编码的配置项。\n键 类型 必需的？ 描述 默认 log_level string 必需的 日志级别（panic、 fatal、 error、 warn、 warning、 info、 debug、trace 之一） info log_path string 可选的 写入日志的磁盘路径 trust_domain string 必需的 SPIRE 服务器的信任域 agent_socket_path string 可选的 SPIRE 代理的 Unix 域套接字的路径。如果 server_address 不是 unix 域套接字地址，则为必需。 server_address string  …","relpermalink":"/kubernetes-handbook/auth/spire-k8s-workload-registar/","summary":"本文介绍了如何在 Kubernetes 中使用 SPIRE 工作负载注册器，包括工作负载注册器部署的方式，注册模式等。","title":"SPIRE Kubernetes 工作负载注册器"},{"content":"本文介绍 SPIRE 如何向工作负载颁发身份的详细步骤，叙述了从代理在节点上启动到同一节点上的工作负载收到 X.509 SVID 形式的有效身份的全过程。请注意，JWT 格式的 SVID 的处理方式是不同的。为了简单演示的目的，工作负载在 AWS EC2 上运行。\nSPIRE 服务器启动。 除非用户配置了 UpstreamAuthority 插件，否则服务器会生成自签名证书（使用自己的私钥签名的证书）；服务器将使用此证书为该服务器信任域中的所有工作负载签署 SVID。 如果是第一次启动，服务器会自动生成一个信任包（trust bundle），其内容存储在你指定的 sql 数据存储中 —— 在服务器插件：DataStore sql 的“内置插件”部分中描述。 服务器打开其注册 API，以允许你注册工作负载。 工作负载节点上的 SPIRE 代理启动。 代理执行节点证明，向服务器证明它正在运行的节点的身份。例如，在 AWS EC2 实例上运行时，它通常会通过向服务器提供 AWS 实例身份文档来执行节点证明。 代理通过 TLS 连接向服务器提供此身份证明，该 TLS 连接通过代理配置的引导程序 …","relpermalink":"/kubernetes-handbook/auth/svid/","summary":"本文介绍了为工作负载颁发 X.509 SVID 身份的详细步骤。","title":"SVID 身份颁发过程"},{"content":"如果你安装了拥有三个节点的 Kubernetes 集群，节点的状态如下所述。\n[root@node1 ~]# kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME node1 Ready \u0026lt;none\u0026gt; 2d v1.9.1 \u0026lt;none\u0026gt; CentOS Linux 7 (Core) 3.10.0-693.11.6.el7.x86_64 docker://1.12.6 node2 Ready \u0026lt;none\u0026gt; 2d v1.9.1 \u0026lt;none\u0026gt; CentOS Linux 7 (Core) 3.10.0-693.11.6.el7.x86_64 docker://1.12.6 node3 Ready \u0026lt;none\u0026gt; 2d v1.9.1 \u0026lt;none\u0026gt; CentOS Linux 7 (Core) 3.10.0-693.11.6.el7.x86_64 docker://1.12.6 当前 Kubernetes 集群中运行的所有 Pod 信息： …","relpermalink":"/kubernetes-handbook/networking/flannel/","summary":"如果你安装了拥有三个节点的 Kubernetes 集群，节点的状态如下所述。 [root@node1 ~]# kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME node1 Ready \u003cnone\u003e 2d v1.9.1 \u003cnone\u003e CentOS Linux 7 (Core) 3.10.0-693.11.6.el7.x86_64 docker://1.12.6 node2 Ready \u003cnone\u003e 2d v1.9.1 \u003cnone\u003e CentOS Linux 7 (Core) 3.10.0-693.11.6.el7.x86_64 docker://1.12.6 node3 Ready \u003cnone\u003e 2d v1.9.1 \u003cnone\u003e CentOS Linux 7 (Core) 3.10.0-693.11.6.el7.x86_64 docker://1.12.6 当前 Kubernetes 集群中运行的所有 Pod 信息： [root@node1 ~]# kubectl get pods --all-namespaces","title":"扁平网络 Flannel"},{"content":"Calico 原意为”有斑点的“，如果说一只猫为 calico cat 的话，就是说这是只花猫，也叫三色猫，所以 calico 的 logo 是只三色猫。\nCalico 概念 Calico 创建和管理一个扁平的三层网络（不需要 overlay），每个容器会分配一个可路由的 IP。由于通信时不需要解包和封包，网络性能损耗小，易于排查，且易于水平扩展。\n小规模部署时可以通过 BGP client 直接互联，大规模下可通过指定的 BGP Route Reflector 来完成，这样保证所有的数据流量都是通过 IP 路由的方式完成互联的。\nCalico 基于 iptables 还提供了丰富而灵活的网络 Policy，保证通过各个节点上的 ACL 来提供 Workload 的多租户隔离、安全组以及其他可达性限制等功能。\nCalico 架构 Calico 由以下组件组成，在部署 Calico 的时候部分组件是可选的。\nCalico API server Felix BIRD confd Dikastes CNI 插件 数据存储插件 IPAM 插件 kube-controllers Typha …","relpermalink":"/kubernetes-handbook/networking/calico/","summary":"Calico 原意为”有斑点的“，如果说一只猫为 calico cat 的话，就是说这是只花猫，也叫三色猫，所以 calico 的 logo 是只三色猫。 Calico 概念 Calico 创建和管理一个扁平的三层网络（不需要 overlay），每个容器会分配一个可路由的 IP。由于通信","title":"非 Overlay 扁平网络 Calico"},{"content":"Cilium 是一款开源软件，也是 CNCF 的孵化项目，目前已有公司提供商业化支持，还有基于 Cilium 实现的服务网格解决方案。最初它仅是作为一个 Kubernetes 网络组件。Cilium 在 1.7 版本后推出并开源了 Hubble，它是专门为网络可视化设计，能够利用 Cilium 提供的 eBPF 数据路径，获得对 Kubernetes 应用和服务的网络流量的深度可视性。这些网络流量信息可以对接 Hubble CLI、UI 工具，可以通过交互式的方式快速进行问题诊断。除了 Hubble 自身的监控工具，还可以对接主流的云原生监控体系——Prometheus 和 Grafana，实现可扩展的监控策略。\nCilium 本节将带你了解什么是 Cilium 及选择它的原因。\nCilium 是什么？ Cilium 为基于 Kubernetes 的 Linux 容器管理平台上部署的服务，透明地提供服务间的网络和 API 连接及安全。\nCilium 底层是基于 Linux 内核的新技术 eBPF，可以在 Linux 系统中动态注入强大的安全性、可视性和网络控制逻辑。Cilium …","relpermalink":"/kubernetes-handbook/networking/cilium/","summary":"Cilium 是一款开源软件，也是 CNCF 的孵化项目，目前已有公司提供商业化支持，还有基于 Cilium 实现的服务网格解决方案。最初它仅是作为一个 Kubernetes 网络组件。Cilium 在 1.7 版本后推出并开源了 Hubble，它是专门为网络可视化设计","title":"基于 eBPF 的网络 Cilium"},{"content":"Secret 解决了密码、token、密钥等敏感数据的配置问题，而不需要把这些敏感数据暴露到镜像或者 Pod Spec 中。Secret 可以以 Volume 或者环境变量的方式使用。\nSecret 有三种类型：\nService Account ：用来访问 Kubernetes API，由 Kubernetes 自动创建，并且会自动挂载到 Pod 的 /run/secrets/kubernetes.io/serviceaccount 目录中； Opaque ：base64 编码格式的 Secret，用来存储密码、密钥等； kubernetes.io/dockerconfigjson ：用来存储私有 docker registry 的认证信息。 Opaque Secret Opaque 类型的数据是一个 map 类型，要求 value 是 base64 编码格式：\n$ echo -n \u0026#34;admin\u0026#34; | base64 YWRtaW4= $ echo -n \u0026#34;1f2d1e2e67df\u0026#34; | base64 MWYyZDFlMmU2N2Rm secrets.yml\napiVersion: v1 …","relpermalink":"/kubernetes-handbook/storage/secret/","summary":"Secret 解决了密码、token、密钥等敏感数据的配置问题，而不需要把这些敏感数据暴露到镜像或者 Pod Spec 中。Secret 可以以 Volume 或者环境变量的方式使用。 Secret 有三种类型： Service Account ：用来访问 Kubernetes API，由 Kubernetes 自动创建，并且会自","title":"Secret"},{"content":"其实 ConfigMap 功能在 Kubernetes1.2 版本的时候就有了，许多应用程序会从配置文件、命令行参数或环境变量中读取配置信息。这些配置信息需要与 docker image 解耦，你总不能每修改一个配置就重做一个 image 吧？ConfigMap API 给我们提供了向容器中注入配置信息的机制，ConfigMap 可以被用来保存单个属性，也可以用来保存整个配置文件或者 JSON 二进制大对象。\nConfigMap 概览 ConfigMap API 资源用来保存 key-value pair 配置数据，这个数据可以在 pods 里使用，或者被用来为像 controller 一样的系统组件存储配置数据。虽然 ConfigMap 跟 Secrets 类似，但是 ConfigMap 更方便的处理不含敏感信息的字符串。注意：ConfigMaps 不是属性配置文件的替代品。ConfigMaps 只是作为多个 properties 文件的引用。你可以把它理解为 Linux 系统中的 /etc 目录，专门用来存储配置文件的目录。下面举个例子，使用 ConfigMap …","relpermalink":"/kubernetes-handbook/storage/configmap/","summary":"其实 ConfigMap 功能在 Kubernetes1.2 版本的时候就有了，许多应用程序会从配置文件、命令行参数或环境变量中读取配置信息。这些配置信息需要与 docker image 解耦，你总不能每修改一个配置就重做一个 image 吧？ConfigMap API 给我们提供了向容器中","title":"ConfigMap"},{"content":"ConfigMap 是用来存储配置文件的 kubernetes 资源对象，所有的配置内容都存储在 etcd 中，下文主要是探究 ConfigMap 的创建和更新流程，以及对 ConfigMap 更新后容器内挂载的内容是否同步更新的测试。\n测试示例 假设我们在 default namespace 下有一个名为 nginx-config 的 ConfigMap，可以使用 kubectl 命令来获取：\n$ kubectl get configmap nginx-config NAME DATA AGE nginx-config 1 99d 获取该 ConfigMap 的内容。\nkubectl get configmap nginx-config -o yaml apiVersion: v1 data: nginx.conf: |- worker_processes 1; events { worker_connections 1024; } http { sendfile on; server { listen 80; # a test endpoint that returns http …","relpermalink":"/kubernetes-handbook/storage/configmap-hot-update/","summary":"ConfigMap 是用来存储配置文件的 kubernetes 资源对象，所有的配置内容都存储在 etcd 中，下文主要是探究 ConfigMap 的创建和更新流程，以及对 ConfigMap 更新后容器内挂载的内容是否同步更新的测试。 测试示例 假设我们在 default namespace 下有一个名为 nginx-config 的 ConfigMa","title":"ConfigMap 的热更新"},{"content":"Volume 容器磁盘上的文件的生命周期是短暂的，这就使得在容器中运行重要应用时会出现一些问题。首先，当容器崩溃时，kubelet 会重启它，但是容器中的文件将丢失——容器以干净的状态（镜像最初的状态）重新启动。其次，在 Pod 中同时运行多个容器时，这些容器之间通常需要共享文件。Kubernetes 中的 Volume 抽象就很好的解决了这些问题。\n建议先熟悉 pod。\n背景 Docker 中也有一个 volume 的概念，尽管它稍微宽松一些，管理也很少。在 Docker 中，卷就像是磁盘或是另一个容器中的一个目录。它的生命周期不受管理，直到最近才有了 local-disk-backed 卷。Docker 现在提供了卷驱动程序，但是功能还非常有限（例如 Docker1.7 只允许每个容器使用一个卷驱动，并且无法给卷传递参数）。\n另一方面，Kubernetes 中的卷有明确的寿命——与封装它的 Pod 相同。所以，卷的生命比 Pod 中的所有容器都长，当这个容器重启时数据仍然得以保存。当然，当 Pod 不再存在时，卷也将不复存在。也许更重要的是，Kubernetes 支持多种类型的 …","relpermalink":"/kubernetes-handbook/storage/volume/","summary":"Volume 容器磁盘上的文件的生命周期是短暂的，这就使得在容器中运行重要应用时会出现一些问题。首先，当容器崩溃时，kubelet 会重启它，但是容器中的文件将丢失——容器以干净的状态（镜像最初的状态）重新启动。其","title":"Volume"},{"content":"本文档介绍了 Kubernetes 中 PersistentVolume 的当前状态。建议您在阅读本文档前先熟悉 volume。\n介绍 对于管理计算资源来说，管理存储资源明显是另一个问题。PersistentVolume 子系统为用户和管理员提供了一个 API，该 API 将如何提供存储的细节抽象了出来。为此，我们引入两个新的 API 资源：PersistentVolume 和 PersistentVolumeClaim。\nPersistentVolume（PV）是由管理员设置的存储，它是群集的一部分。就像节点是集群中的资源一样，PV 也是集群中的资源。PV 是 Volume 之类的卷插件，但具有独立于使用 PV 的 Pod 的生命周期。此 API 对象包含存储实现的细节，即 NFS、iSCSI 或特定于云供应商的存储系统。\nPersistentVolumeClaim（PVC）是用户存储的请求。它与 Pod 相似。Pod 消耗节点资源，PVC 消耗 PV 资源。Pod 可以请求特定级别的资源（CPU 和内存）。声明可以请求特定的大小和访问模式（例如，可以以读/写一次或 只读多次模式挂 …","relpermalink":"/kubernetes-handbook/storage/persistent-volume/","summary":"本文档介绍了 Kubernetes 中 PersistentVolume 的当前状态。建议您在阅读本文档前先熟悉 volume。 介绍 对于管理计算资源来说，管理存储资源明显是另一个问题。PersistentVolume 子系统为用户和管理员提供了一个 API，该 API","title":"持久化卷（Persistent Volume）"},{"content":"本文介绍了 Kubernetes 中 StorageClass 的概念。在阅读本文之前建议先熟悉 卷 和 Persistent Volume（持久卷）。\n介绍 StorageClass 为管理员提供了描述存储 “class（类）” 的方法。不同的 class 可能会映射到不同的服务质量等级或备份策略，或由群集管理员确定的任意策略。Kubernetes 本身不清楚各种 class 代表的什么。这个概念在其他存储系统中有时被称为“配置文件”。\nStorageClass 资源 StorageClass 中包含 provisioner、parameters 和 reclaimPolicy 字段，当 class 需要动态分配 PersistentVolume 时会使用到。\nStorageClass 对象的名称很重要，用户使用该类来请求一个特定的方法。当创建 StorageClass 对象时，管理员设置名称和其他参数，一旦创建了对象就不能再对其更新。\n管理员可以为没有申请绑定到特定 class 的 PVC 指定一个默认的 StorageClass ： …","relpermalink":"/kubernetes-handbook/storage/storageclass/","summary":"本文介绍了 Kubernetes 中 StorageClass 的概念。在阅读本文之前建议先熟悉 卷 和 Persistent Volume（持久卷）。 介绍 StorageClass 为管理员提供了描述存储 “class（类）” 的方法。不同的 class 可能会映射到不同的服务质量等级或备","title":"Storage Class"},{"content":"本地持久化卷允许用户通过标准 PVC 接口以简单便携的方式访问本地存储。PV 中包含系统用于将 Pod 安排到正确节点的节点亲和性信息。\n一旦配置了本地卷，外部静态配置器（provisioner）可用于帮助简化本地存储管理。请注意，本地存储配置器与大多数配置器不同，并且尚不支持动态配置。相反，它要求管理员预先配置每个节点上的本地卷，并且这些卷应该是：\nFilesystem volumeMode（默认）PV—— 将它们挂载到发现目录下。 Block volumeMode PV——在发现目录下为节点上的块设备创建一个符号链接。 配置器将通过为每个卷创建和清除 PersistentVolumes 来管理发现目录下的卷。\n配置要求 本地卷插件希望路径稳定，包括在重新启动时和添加或删除磁盘时。 静态配置器仅发现挂载点（对于文件系统模式卷）或符号链接（对于块模式卷）。对于基于目录的本地卷必须绑定到发现目录中。 版本兼容性 推荐配置器版本与 Kubernetes 版本\nProvisioner version K8s version Reason 2.1.0 1.10 Beta API …","relpermalink":"/kubernetes-handbook/storage/local-persistent-storage/","summary":"本地持久化卷允许用户通过标准 PVC 接口以简单便携的方式访问本地存储。PV 中包含系统用于将 Pod 安排到正确节点的节点亲和性信息。 一旦配置了本地卷，外部静态配置器（provisioner）可用于帮助简化本地存储管","title":"本地持久化存储"},{"content":"自定义资源是对 Kubernetes API 的扩展，Kubernetes 中的每个资源都是一个 API 对象的集合，例如我们在 YAML 文件里定义的那些 spec 都是对 Kubernetes 中的资源对象的定义，所有的自定义资源可以跟 Kubernetes 中内建的资源一样使用 kubectl 操作。\n自定义资源 Kubernetes 从 1.6 版本开始包含一个内建的资源叫做 TPR（ThirdPartyResource），可以用它来创建自定义资源，但该资源在 Kubernetes 1.7 版本开始已被 CRD（CustomResourceDefinition）取代。\n扩展 API 自定义资源实际上是为了扩展 Kubernetes 的 API，向 Kubernetes API 中增加新类型，可以使用以下三种方式：\n修改 Kubernetes 的源码，显然难度比较高，也不太合适 创建自定义 API server 并聚合到 API 中 编写自定义资源是扩展 Kubernetes API 的最简单的方式，是否编写自定义资源来扩展 API 请参考 Should I add a …","relpermalink":"/kubernetes-handbook/extend/custom-resource/","summary":"自定义资源是对 Kubernetes API 的扩展，Kubernetes 中的每个资源都是一个 API 对象的集合，例如我们在 YAML 文件里定义的那些 spec 都是对 Kubernetes 中的资源对象的定义，所有的自定义资源可以跟 Kubernetes 中内建的资源一样使用 kubectl 操作。 自定义资源","title":"使用自定义资源扩展 API"},{"content":"本文是如何创建 CRD 来扩展 Kubernetes API 的教程。CRD 是用来扩展 Kubernetes 最常用的方式，在 Service Mesh 和 Operator 中也被大量使用。因此读者如果想在 Kubernetes 上做扩展和开发的话，是十分有必要了解 CRD 的。\n在阅读本文前您需要先了解使用自定义资源扩展 API，以下内容译自 Kubernetes 官方文档，有删改，推荐阅读如何从零开始编写一个 Kubernetes CRD。\n创建 CRD（CustomResourceDefinition） 创建新的 CustomResourceDefinition（CRD）时，Kubernetes API Server 会为您指定的每个版本创建新的 RESTful 资源路径。CRD 可以是命名空间的，也可以是集群范围的，可以在 CRD scope 字段中所指定。与现有的内置对象一样，删除命名空间会删除该命名空间中的所有自定义对象。CustomResourceDefinition 本身是非命名空间的，可供所有命名空间使用。\n参考下面的 CRD， …","relpermalink":"/kubernetes-handbook/extend/crd/","summary":"本文是如何创建 CRD 来扩展 Kubernetes API 的教程。CRD 是用来扩展 Kubernetes 最常用的方式，在 Service Mesh 和 Operator 中也被大量使用。因此读者如果想在 Kubernetes 上做扩展和开发的话，是十分有必要了解 CRD 的。 在阅读本文前您需要先了解使用自定义资源扩展 API","title":"使用 CRD 扩展 Kubernetes API"},{"content":"Aggregated（聚合的）API server 是为了将原来的 API server 这个巨石（monolithic）应用给拆分成，为了方便用户开发自己的 API server 集成进来，而不用直接修改 kubernetes 官方仓库的代码，这样一来也能将 API server 解耦，方便用户使用实验特性。这些 API server 可以跟 core API server 无缝衔接，使用 kubectl 也可以管理它们。\n架构 我们需要创建一个新的组件，名为 kube-aggregator，它需要负责以下几件事：\n提供用于注册 API server 的 API 汇总所有的 API server 信息 代理所有的客户端到 API server 的请求 注意：这里说的 API server 是一组“API Server”，而不是说我们安装集群时候的那个 API server，而且这组 API server 是可以横向扩展的。\n安装配置聚合的 API server 有两种方式来启用 kube-aggregator：\n使用 test mode/single-user mode，作为一个独立 …","relpermalink":"/kubernetes-handbook/extend/aggregated-api-server/","summary":"Aggregated（聚合的）API server 是为了将原来的 API server 这个巨石（monolithic）应用给拆分成，为了方便用户开发自己的 API server 集成进来，而不用直接修改 kubernetes 官方仓库的代码，这样一来也能将 API server 解耦，方便用","title":"Aggregated API Server"},{"content":"APIService 是用来表示一个特定的 GroupVersion 的中的 server，它的结构定义位于代码 staging/src/k8s.io/kube-aggregator/pkg/apis/apiregistration/types.go 中。\n下面是一个 APIService 的示例配置：\napiVersion: apiregistration.k8s.io/v1beta1 kind: APIService metadata: name: v1alpha1.custom-metrics.metrics.k8s.io spec: insecureSkipTLSVerify: true group: custom-metrics.metrics.k8s.io groupPriorityMinimum: 1000 versionPriority: 5 service: name: api namespace: custom-metrics version: v1alpha1 APIService 详解 使用 apiregistration.k8s.io/v1beta1 …","relpermalink":"/kubernetes-handbook/extend/apiservice/","summary":"APIService 是用来表示一个特定的 GroupVersion 的中的 server，它的结构定义位于代码 staging/src/k8s.io/kube-aggregator/pkg/apis/apiregistration/types.go 中。 下面是一个 APIService 的示例配置： apiVersion: apiregistration.k8s.io/v1beta1 kind: APIService metadata: name: v1alpha1.custom-metrics.metrics.k8s.io spec: insecureSkipTLSVerify: true group: custom-metrics.metrics.k8s.io groupPriorityMinimum: 1000 versionPriority: 5 service: name: api namespace: custom-metrics version: v1alpha1 APIService 详解 使用 apiregistration.k8s.io/v1beta1 版本的 APIService，在 metadata.name 中定义该 API 的名字","title":"APIService"},{"content":"服务目录（Service Catalog）是 Kubernetes 的扩展 API，它使运行在 Kubernetes 集群中的应用程序可以轻松使用外部托管软件产品，例如由云提供商提供的数据存储服务。\n它提供列表清单、提供 (provision) 和绑定 (binding) 来自服务代理（Service Brokers）的外部托管服务，而不需要关心如何创建或管理这些服务的详细情况。\n由 Open Service Broker API 规范定义的 Service broker 是由第三方提供和维护的一组托管服务的端点 (endpoint)，该第三方可以是 AWS，GCP 或 Azure 等云提供商。\n托管服务可以是 Microsoft Azure Cloud Queue，Amazon Simple Queue Service 和 Google Cloud Pub/Sub 等，它们可以是应用可以使用的提供的各种软件。\n通过 Service Catalog，集群运营者可以浏览由 Service Broker 提供的托管服务列表，提供的托管服务实例，并与其绑定，使其可被 Kubernetes 集 …","relpermalink":"/kubernetes-handbook/extend/service-catalog/","summary":"服务目录（Service Catalog）是 Kubernetes 的扩展 API，它使运行在 Kubernetes 集群中的应用程序可以轻松使用外部托管软件产品，例如由云提供商提供的数据存储服务。 它提供列表清单、提供 (provision) 和绑定 (binding) 来自服务代理（Ser","title":"服务目录（Service Catalog）"},{"content":"2020 年初，Kubernetes 社区提议 Multi-Cluster Services API，旨在解决长久以来就存在的 Kubernetes 多集群服务管理问题。\nKubernetes 用户可能希望将他们的部署分成多个集群，但仍然保留在这些集群中运行的工作负载之间的相互依赖关系，这有很多原因。今天，集群是一个硬边界，一个服务对远程的 Kubernetes 消费者来说是不透明的，否则就可以利用元数据（如端点拓扑结构）来更好地引导流量。为了支持故障转移或在迁移过程中的临时性，用户可能希望消费分布在各集群中的服务，但今天这需要非复杂的定制解决方案。\n多集群服务 API 旨在解决这些问题。\n参考 KEP-1645: Multi-Cluster Services API - github.com ","relpermalink":"/kubernetes-handbook/multi-cluster/multi-cluster-services-api/","summary":"2020 年初，Kubernetes 社区提议 Multi-Cluster Services API，旨在解决长久以来就存在的 Kubernetes 多集群服务管理问题。 Kubernetes 用户可能希望将他们的部署分成多个集群，但仍然保留在这些集群中运行的工作负载之间的相互依赖关系，这有很多原","title":"多集群服务 API（Multi-Cluster Services API）"},{"content":" 注意\nKubefed 项目计划归档，详见 Fellow-up: discussion on archiving Kubefed。 Kubernetes 从 1.8 版本起就声称单集群最多可支持 5000 个节点和 15 万个 Pod，我相信很少有公司会部署如此庞大的一个单集群，总有很多情况下因为各种各样的原因我们可能会部署多个集群，但是有时候有想将他们统一起来管理，这时候就需要用到集群联邦（Federation）。\n为什么要使用集群联邦 Federation 使管理多个集群变得简单。它通过提供两个主要构建模块来实现：\n跨集群同步资源：Federation 提供了在多个集群中保持资源同步的能力。例如，可以保证同一个 deployment 在多个集群中存在。 跨集群服务发现：Federation 提供了自动配置 DNS 服务以及在所有集群后端上进行负载均衡的能力。例如，可以提供一个全局 VIP 或者 DNS 记录，通过它可以访问多个集群后端。 Federation 还可以提供一些其它用例：\n高可用：通过在集群间分布负载并自动配置 DNS 服务和负载均衡，federation 最大限度地减 …","relpermalink":"/kubernetes-handbook/multi-cluster/federation/","summary":"注意 Kubefed 项目计划归档，详见 Fellow-up: discussion on archiving Kubefed。 Kubernetes 从 1.8 版本起就声称单集群最多可支持 5000 个节点和 15 万个 Pod，我相信很少有公司会部署如此庞大的一个单集群，总有很多情况下因为各种各样的原因我们可能会部署多个","title":"集群联邦（Cluster Federation）"},{"content":"Kubernetes 作为一个容器编排调度引擎，资源调度是它的最基本也是最重要的功能，这一节中我们将着重讲解 Kubernetes 中是如何做资源调度的。\nKubernetes 中有一个叫做 kube-scheduler 的组件，该组件就是专门监听 kube-apiserver 中是否有还未调度到 node 上的 pod，再通过特定的算法为 pod 指定分派 node 运行。\nKubernetes 中的众多资源类型，例如 Deployment、DaemonSet、StatefulSet 等都已经定义了 Pod 运行的一些默认调度策略，但是如果我们细心的根据 node 或者 pod 的不同属性，分别为它们打上标签之后，我们将发现 Kubernetes 中的高级调度策略是多么强大。当然如果要实现动态的资源调度，即 pod 已经调度到某些节点上后，因为一些其它原因，想要让 pod 重新调度到其它节点。\n考虑以下两种情况：\n集群中有新增节点，想要让集群中的节点的资源利用率比较均衡一些，想要将一些高负载的节点上的 pod 驱逐到新增节点上，这是 kuberentes 的 scheduler 所不 …","relpermalink":"/kubernetes-handbook/cluster/scheduling/","summary":"Kubernetes 作为一个容器编排调度引擎，资源调度是它的最基本也是最重要的功能，这一节中我们将着重讲解 Kubernetes 中是如何做资源调度的。 Kubernetes 中有一个叫做 kube-scheduler 的组件，该组件就是专门监听 kube-apiserver 中是否有还未调度到 node 上的 pod，再通过特定的","title":"资源调度"},{"content":"QoS（Quality of Service），大部分译为“服务质量等级”，又译作“服务质量保证”，是作用在 Pod 上的一个配置，当 Kubernetes 创建一个 Pod 时，它就会给这个 Pod 分配一个 QoS 等级，可以是以下等级之一：\nGuaranteed：Pod 里的每个容器都必须有内存/CPU 限制和请求，而且值必须相等。 Burstable：Pod 里至少有一个容器有内存或者 CPU 请求且不满足 Guarantee 等级的要求，即内存/CPU 的值设置的不同。 BestEffort：容器必须没有任何内存或者 CPU 的限制或请求。 该配置不是通过一个配置项来配置的，而是通过配置 CPU/内存的 limits 与 requests 值的大小来确认服务质量等级的。使用 kubectl get pod -o yaml 可以看到 pod 的配置输出中有 qosClass 一项。该配置的作用是为了给资源调度提供策略支持，调度算法根据不同的服务质量等级可以确定将 pod 调度到哪些节点上。\n例如，下面这个 YAML 配置中的 Pod …","relpermalink":"/kubernetes-handbook/cluster/qos/","summary":"QoS（Quality of Service），大部分译为“服务质量等级”，又译作“服务质量保证”，是作用在 Pod 上的一个配置，当 Kubernetes 创建一个 Pod 时，它就会给这个 Pod 分配一个 QoS 等级，可以是以下等级之一： Guarant","title":"服务质量等级（QoS）"},{"content":"当你使用 Kubernetes 的时候，有没有遇到过 Pod 在启动后一会就挂掉然后又重新启动这样的恶性循环？你有没有想过 Kubernetes 是如何检测 pod 是否还存活？虽然容器已经启动，但是 Kubernetes 如何知道容器的进程是否准备好对外提供服务了呢？让我们通过 Kubernetes 官网的这篇文章 Configure Liveness and Readiness Probes，来一探究竟。\n本文将展示如何配置容器的存活和可读性探针。\nKubelet 使用 liveness probe（存活探针）来确定何时重启容器。例如，当应用程序处于运行状态但无法做进一步操作，liveness 探针将捕获到 deadlock，重启处于该状态下的容器，使应用程序在存在 bug 的情况下依然能够继续运行下去（谁的程序还没几个 bug 呢）。\nKubelet 使用 readiness probe（就绪探针）来确定容器是否已经就绪可以接受流量。只有当 Pod 中的容器都处于就绪状态时 kubelet 才会认定该 Pod 处于就绪状态。该信号的作用是控制哪些 Pod 应该作为 service …","relpermalink":"/kubernetes-handbook/config/liveness-readiness-probes/","summary":"当你使用 Kubernetes 的时候，有没有遇到过 Pod 在启动后一会就挂掉然后又重新启动这样的恶性循环？你有没有想过 Kubernetes 是如何检测 pod 是否还存活？虽然容器已经启动，但是 Kubernetes 如何知道容器的进程是否准备好对外提供服务了呢？让我们通过 Kubernetes","title":"配置 Pod 的 liveness 和 readiness 探针"},{"content":"Service account 为 Pod 中的进程提供身份信息。\n本文是关于 Service Account 的用户指南，管理指南另见 Service Account 的集群管理指南。\n注意：本文档描述的关于 Service Account 的行为只有当您按照 Kubernetes 项目建议的方式搭建起集群的情况下才有效。您的集群管理员可能在您的集群中有自定义配置，这种情况下该文档可能并不适用。\n当您（真人用户）访问集群（例如使用kubectl命令）时，apiserver 会将您认证为一个特定的 User Account（目前通常是admin，除非您的系统管理员自定义了集群配置）。Pod 容器中的进程也可以与 apiserver 联系。当它们在联系 apiserver 的时候，它们会被认证为一个特定的 Service Account（例如default）。\n使用默认的 Service Account 访问 API server 当您创建 pod 的时候，如果您没有指定一个 service account，系统会自动得在与该 pod 相同的 namespace 下为其指派一 …","relpermalink":"/kubernetes-handbook/config/service-account/","summary":"Service account 为 Pod 中的进程提供身份信息。 本文是关于 Service Account 的用户指南，管理指南另见 Service Account 的集群管理指南。 注意：本文档描述的关于 Service Account 的行为只有当您按照 Kubernetes 项目建议的方式搭建起集群的情况下才有效。您的集群管理员可能在您的集","title":"配置 Pod 的 Service Account"},{"content":"Secret 对象类型用来保存敏感信息，例如密码、OAuth 令牌和 ssh key。将这些信息放在 secret 中比放在 pod 的定义中或者 docker 镜像中来说更加安全和灵活。\nSecret 概览 Secret 是一种包含少量敏感信息例如密码、token 或 key 的对象。这样的信息可能会被放在 Pod spec 中或者镜像中；将其放在一个 secret 对象中可以更好地控制它的用途，并降低意外暴露的风险。\n用户可以创建 secret，同时系统也创建了一些 secret。\n要使用 secret，pod 需要引用 secret。Pod 可以用两种方式使用 secret：作为 volume 中的文件被挂载到 pod 中的一个或者多个容器里，或者当 kubelet 为 pod 拉取镜像时使用。\n内置 secret Service Account 使用 API 凭证自动创建和附加 secret Kubernetes 自动创建包含访问 API 凭据的 secret，并自动修改您的 pod 以使用此类型的 secret。\n如果需要，可以禁用或覆盖自动创建和使用 API 凭据。但是，如 …","relpermalink":"/kubernetes-handbook/config/secret/","summary":"Secret 对象类型用来保存敏感信息，例如密码、OAuth 令牌和 ssh key。将这些信息放在 secret 中比放在 pod 的定义中或者 docker 镜像中来说更加安全和灵活。 Secret 概览 Secret 是一种包含少量敏感信息例如密码、token 或 key 的对象。这样的信","title":"Secret 配置"},{"content":"当用多个团队或者用户共用同一个集群的时候难免会有资源竞争的情况发生，这时候就需要对不同团队或用户的资源使用配额做出限制。\n开启资源配额限制功能 目前有两种资源分配管理相关的控制策略插件 ResourceQuota 和 LimitRange。\n要启用它们只要 API Server 的启动配置的 KUBE_ADMISSION_CONTROL 参数中加入了 ResourceQuota 的设置，这样就给集群开启了资源配额限制功能，加入 LimitRange 可以用来限制一个资源申请的范围限制，参考 为 namesapce 配置默认的内存请求与限额 和 在 namespace 中配置默认的 CPU 请求与限额。\n两种控制策略的作用范围都是对于某一 namespace，ResourceQuota 用来限制 namespace 中所有的 Pod 占用的总的资源 request 和 limit，而 LimitRange 是用来设置 namespace 中 Pod 的默认的资源 request 和 limit 值。\n资源配额分为三种类型：\n计算资源配额 存储资源配额 对象数量配额 关于资源配额的详细信息 …","relpermalink":"/kubernetes-handbook/config/quota/","summary":"当用多个团队或者用户共用同一个集群的时候难免会有资源竞争的情况发生，这时候就需要对不同团队或用户的资源使用配额做出限制。 开启资源配额限制功能 目前有两种资源分配管理相关的控制策略插件 ResourceQuota 和 LimitRan","title":"管理 namespace 中的资源配额"},{"content":"对于没有使用过 Kubernetes 的 Docker 用户，如何快速掌握 kubectl 命令？\n在本文中，我们将向 docker-cli 用户介绍 Kubernetes 命令行如何与 api 进行交互。该命令行工具——kubectl，被设计成 docker-cli 用户所熟悉的样子，但是它们之间又存在一些必要的差异。该文档将向您展示每个 docker 子命令和 kubectl 与其等效的命令。\n在使用 kubernetes 集群的时候，docker 命令通常情况是不需要用到的，只有在调试程序或者容器的时候用到，我们基本上使用 kubectl 命令即可，所以在操作 kubernetes 的时候我们抛弃原先使用 docker 时的一些观念。\ndocker run 如何运行一个 nginx Deployment 并将其暴露出来？查看 kubectl run 。\n使用 docker 命令：\n$ docker run -d --restart=always -e DOMAIN=cluster --name nginx-app -p 80:80 nginx …","relpermalink":"/kubernetes-handbook/cli/docker-cli-to-kubectl/","summary":"对于没有使用过 Kubernetes 的 Docker 用户，如何快速掌握 kubectl 命令？ 在本文中，我们将向 docker-cli 用户介绍 Kubernetes 命令行如何与 api 进行交互。该命令行工具——kubectl，被设计成 docker-cli 用户所熟悉的样子，但是它们之间又存在一些必要的差异。该文档","title":"Docker 用户过渡到 kubectl 命令行指南"},{"content":"Kubernetes 提供的 kubectl 命令是与集群交互最直接的方式，v1.6 版本的 kubectl 命令参考图如下：\nkubectl cheatsheet Kubectl 的子命令主要分为 8 个类别：\n基础命令（初学者都会使用的） 基础命令（中级） 部署命令 集群管理命令 故障排查和调试命令 高级命令 设置命令 其他命令 熟悉这些命令有助于大家来操作和管理 kubernetes 集群。\n命令行提示 为了使用 kubectl 命令更加高效，我们可以选择安装一下开源软件来增加操作 kubectl 命令的快捷方式，同时为 kubectl 命令增加命令提示。\n增加 kubeclt 命令的工具（图片来自网络） kubectx：用于切换 Kubernetes context kube-ps1：为命令行终端增加$PROMPT字段 kube-shell：交互式带命令提示的 kubectl 终端 全部配置完成后的 kubectl 终端如下图所示：\n增强的 kubectl 命令 开源项目 kube-shell 可以为 kubectl 提供自动的命令提示和补全，使用起来特别方便，推荐给大家。 …","relpermalink":"/kubernetes-handbook/cli/using-kubectl/","summary":"Kubernetes 提供的 kubectl 命令是与集群交互最直接的方式，v1.6 版本的 kubectl 命令参考图如下： kubectl cheatsheet Kubectl 的子命令主要分为 8 个类别： 基础命令（初学者都会使用的） 基础命令（中级） 部署命令 集群管理命令 故障排查和调试命令 高级命令 设置命","title":"Kubectl 命令概览"},{"content":"kubectl 命令是操作 Kubernetes 集群的最直接和最高效的途径，这个 60 多 MB 大小的二进制文件，到底有啥能耐呢？\nKubectl 自动补全 $ source \u0026lt;(kubectl completion bash) # setup autocomplete in bash, bash-completion package should be installed first. $ source \u0026lt;(kubectl completion zsh) # setup autocomplete in zsh Kubectl 上下文和配置 设置 kubectl 命令交互的 kubernetes 集群并修改配置信息。参阅 使用 kubeconfig 文件进行跨集群验证 获取关于配置文件的详细信息。\n$ kubectl config view # 显示合并后的 kubeconfig 配置 # 同时使用多个 kubeconfig 文件并查看合并后的配置 $ KUBECONFIG=~/.kube/config:~/.kube/kubconfig2 kubectl config view …","relpermalink":"/kubernetes-handbook/cli/kubectl-cheatsheet/","summary":"kubectl 命令是操作 Kubernetes 集群的最直接和最高效的途径，这个 60 多 MB 大小的二进制文件，到底有啥能耐呢？ Kubectl 自动补全 $ source \u003c(kubectl completion bash) # setup autocomplete in bash, bash-completion package should be installed first. $ source \u003c(kubectl completion zsh) # setup autocomplete in zsh Kubectl 上下文和配置 设置 kubectl 命令交互的 kubernetes 集群并修改配置信息。","title":"Kubectl 命令技巧大全"},{"content":"Kubenretes1.6 中使用 etcd V3 版本的 API，使用 etcdctl 直接 ls 的话只能看到 /kube-centos 一个路径。需要在命令前加上 ETCDCTL_API=3 这个环境变量才能看到 kuberentes 在 etcd 中保存的数据。\nETCDCTL_API=3 etcdctl get /registry/namespaces/default -w=json|python -m json.tool 如果是使用 kubeadm 创建的集群，在 Kubenretes 1.11 中，etcd 默认使用 tls，这时你可以在 master 节点上使用以下命令来访问 etcd：\nETCDCTL_API=3 etcdctl --cacert=/etc/kubernetes/pki/etcd/ca.crt \\ --cert=/etc/kubernetes/pki/etcd/peer.crt \\ --key=/etc/kubernetes/pki/etcd/peer.key \\ get /registry/namespaces/default -w=json | …","relpermalink":"/kubernetes-handbook/cli/etcdctl/","summary":"Kubenretes1.6 中使用 etcd V3 版本的 API，使用 etcdctl 直接 ls 的话只能看到 /kube-centos 一个路径。需要在命令前加上 ETCDCTL_API=3 这个环境变量才能看到 kuberentes 在 etcd 中保存的数据。 ETCDCTL_API=3 etcdctl get /registry/namespaces/default -w=json|python -m json.tool 如果是使用 kubeadm 创建的集群，在 Kubenretes 1.11 中，etcd 默认使用 tls，这时你可","title":"使用 etcdctl 访问 Kubernetes 数据"},{"content":"在使用二进制文件部署 Kubernetes 集群的时候，很多人在进行到部署证书时遇到各种各样千奇百怪的问题，这一步是创建集群的基础，我们有必要详细了解一下其背后的流程和原理。\n概览 每个 Kubernetes 集群都有一个集群根证书颁发机构（CA）。集群中的组件通常使用 CA 来验证 API server 的证书，由 API 服务器验证 kubelet 客户端证书等。为了支持这一点，CA 证书包被分发到集群中的每个节点，并作为一个 secret 附加分发到默认 service account 上。或者，你的 workload 可以使用此 CA 建立信任。你的应用程序可以使用类似于 ACME 草案的协议，使用 certificates.k8s.io API 请求证书签名。\n集群中的 TLS 信任 让 Pod 中运行的应用程序信任集群根 CA 通常需要一些额外的应用程序配置。您将需要将 CA 证书包添加到 TLS 客户端或服务器信任的 CA 证书列表中。例如，您可以使用 golang TLS 配置通过解析证书链并将解析的证书添加到 tls.Config结构中的 Certificates 字 …","relpermalink":"/kubernetes-handbook/security/managing-tls-in-a-cluster/","summary":"在使用二进制文件部署 Kubernetes 集群的时候，很多人在进行到部署证书时遇到各种各样千奇百怪的问题，这一步是创建集群的基础，我们有必要详细了解一下其背后的流程和原理。 概览 每个 Kubernetes 集群都有一个集群根证书颁发机构（CA）","title":"管理集群中的 TLS"},{"content":"Kubelet 的 HTTPS 端点对外暴露了用于访问不同敏感程度数据的 API，并允许您在节点或者容器内执行不同权限级别的操作。\n本文档向您描述如何通过认证授权来访问 kubelet 的 HTTPS 端点。\nKubelet 认证 默认情况下，所有未被配置的其他身份验证方法拒绝的，对 kubelet 的 HTTPS 端点的请求将被视为匿名请求，并被授予 system:anonymous 用户名和 system:unauthenticated 组。\n如果要禁用匿名访问并发送 401 Unauthorized 的未经身份验证的请求的响应：\n启动 kubelet 时指定 --anonymous-auth=false 标志 如果要对 kubelet 的 HTTPS 端点启用 X509 客户端证书身份验证：\n启动 kubelet 时指定 --client-ca-file 标志，提供 CA bundle 以验证客户端证书 启动 apiserver 时指定 --kubelet-client-certificate 和 --kubelet-client-key 标志 参阅 apiserver …","relpermalink":"/kubernetes-handbook/security/kubelet-authentication-authorization/","summary":"Kubelet 的 HTTPS 端点对外暴露了用于访问不同敏感程度数据的 API，并允许您在节点或者容器内执行不同权限级别的操作。 本文档向您描述如何通过认证授权来访问 kubelet 的 HTTPS 端点。 Kubelet 认证 默认情况下，所有未被配置的其他身份验证方法拒","title":"Kublet 的认证授权"},{"content":"本文档介绍如何为 kubelet 设置 TLS 客户端证书引导（bootstrap）。\nKubernetes 1.4 引入了一个用于从集群级证书颁发机构（CA）请求证书的 API。此 API 的原始目的是为 kubelet 提供 TLS 客户端证书。可以在 这里 找到该提议，在 feature #43 追踪该功能的进度。\nkube-apiserver 配置 您必须提供一个 token 文件，该文件中指定了至少一个分配给 kubelet 特定 bootstrap 组的“bootstrap token”。\n该组将作为 controller manager 配置中的默认批准控制器而用于审批。随着此功能的成熟，您应该确保 token 被绑定到基于角色的访问控制（RBAC）策略上，该策略严格限制了与证书配置相关的客户端请求（使用 bootstrap token）。使用 RBAC，将 token 范围划分为组可以带来很大的灵活性（例如，当您配置完成节点后，您可以禁用特定引导组的访问）。\nToken 认证文件 Token 可以是任意的，但应该可以表示为从安全随机数生成器（ …","relpermalink":"/kubernetes-handbook/security/tls-bootstrapping/","summary":"本文档介绍如何为 kubelet 设置 TLS 客户端证书引导（bootstrap）。 Kubernetes 1.4 引入了一个用于从集群级证书颁发机构（CA）请求证书的 API。此 API 的原始目的是为 kubelet 提供 TLS 客户端证书。可以在 这里 找到该提议，在 feature #43 追踪该功","title":"TLS Bootstrap"},{"content":"本文将讲述如何配置和启用 ip-masq-agent。\n创建 ip-masq-agent 要创建 ip-masq-agent，运行下面的 kubectl 命令：\nkubectl create -f https://raw.githubusercontent.com/kubernetes-incubator/ip-masq-agent/master/ip-masq-agent.yaml 关于 ip-masq-agent 的更多信息请参考 该文档。\n在大多数情况下，默认的一套规则应该是足够的；但是，如果内置的规则不适用于您的集群，您可以创建并应用 ConfigMap 来自定义受影响的 IP 范围。例如，为了仅允许 ip-masq-agent 考虑 10.0.0.0/8，您可以在名为“config”的文件中创建以下 ConfigMap。\nnonMasqueradeCIDRs: - 10.0.0.0/8 resyncInterval: 60s 注意：重要的是，该文件被命名为 config，因为默认情况下，该文件将被用作 ip-masq-agent 查找的关键字。 …","relpermalink":"/kubernetes-handbook/security/ip-masq-agent/","summary":"本文将讲述如何配置和启用 ip-masq-agent。 创建 ip-masq-agent 要创建 ip-masq-agent，运行下面的 kubectl 命令： kubectl create -f https://raw.githubusercontent.com/kubernetes-incubator/ip-masq-agent/master/ip-masq-agent.yaml 关于 ip-masq-agent 的更多信息请参考 该文档。 在大多数情况下，默认的一套规则应该是足够的；但是，如","title":"IP 伪装代理"},{"content":"当我们安装好集群后，如果想要把 kubectl 命令交给用户使用，就不得不对用户的身份进行认证和对其权限做出限制。\n下面以创建一个 devuser 用户并将其绑定到 dev 和 test 两个 namespace 为例说明。\n创建 CA 证书和秘钥 创建 devuser-csr.json 文件\n{ \u0026#34;CN\u0026#34;: \u0026#34;devuser\u0026#34;, \u0026#34;hosts\u0026#34;: [], \u0026#34;key\u0026#34;: { \u0026#34;algo\u0026#34;: \u0026#34;rsa\u0026#34;, \u0026#34;size\u0026#34;: 2048 }, \u0026#34;names\u0026#34;: [ { \u0026#34;C\u0026#34;: \u0026#34;CN\u0026#34;, \u0026#34;ST\u0026#34;: \u0026#34;BeiJing\u0026#34;, \u0026#34;L\u0026#34;: \u0026#34;BeiJing\u0026#34;, \u0026#34;O\u0026#34;: \u0026#34;k8s\u0026#34;, \u0026#34;OU\u0026#34;: \u0026#34;System\u0026#34; } ] } 生成 CA 证书和私钥\n下面我们在 master 节点上为 devuser 创建证书和秘钥，在 /etc/kubernetes/ssl 目录下执行以下命令：\n执行该命令前请先确保该目录下已经包含如下文件：\nca-key.pem ca.pem ca-config.json devuser-csr.json $ cfssl gencert -ca=ca.pem …","relpermalink":"/kubernetes-handbook/security/kubectl-user-authentication-authorization/","summary":"当我们安装好集群后，如果想要把 kubectl 命令交给用户使用，就不得不对用户的身份进行认证和对其权限做出限制。 下面以创建一个 devuser 用户并将其绑定到 dev 和 test 两个 namespace 为例说明。 创建 CA 证书和秘钥 创建 devuser-csr.json 文件 { \"CN\": \"devuser\", \"hosts\": [], \"key\": { \"algo\": \"rsa\", \"size\": 2048","title":"创建用户认证授权的 kubeconfig 文件"},{"content":"在开启了 TLS 的集群中，每当与集群交互的时候少不了的是身份认证，使用 kubeconfig（即证书）和 token 两种认证方式是最简单也最通用的认证方式，在 dashboard 的登录功能就可以使用这两种登录功能。\n下文分两块以示例的方式来讲解两种登陆认证方式：\n为 brand 命名空间下的 brand 用户创建 kubeconfig 文件 为集群的管理员（拥有所有命名空间的 amdin 权限）创建 token 使用 kubeconfig 如何生成kubeconfig文件请参考创建用户认证授权的 kubeconfig 文件。\n注意我们生成的 kubeconfig 文件中没有 token 字段，需要手动添加该字段。\n比如我们为 brand namespace 下的 brand 用户生成了名为 brand.kubeconfig 的 kubeconfig 文件，还要再该文件中追加一行 token 的配置（如何生成 token 将在下文介绍），如下所示：\nkubeconfig 文件 对于访问 dashboard 时候的使用 kubeconfig 文件如brand.kubeconfig 必 …","relpermalink":"/kubernetes-handbook/security/auth-with-kubeconfig-or-token/","summary":"在开启了 TLS 的集群中，每当与集群交互的时候少不了的是身份认证，使用 kubeconfig（即证书）和 token 两种认证方式是最简单也最通用的认证方式，在 dashboard 的登录功能就可以使用这两种登录功能。 下文分两块以示例的方式","title":"使用 kubeconfig 或 token 进行用户身份认证"},{"content":"在安装集群的时候我们在 master 节点上生成了一堆证书、token，还在 kubelet 的配置中用到了 bootstrap token，安装各种应用时，为了能够与 API server 通信创建了各种 service account，在 Dashboard 中使用了 kubeconfig 或 token 登陆，那么这些都属于什么认证方式？如何区分用户的？我特地翻译了下这篇官方文档，想你看了之后你将找到答案。\n重点查看 bearer token 和 HTTP 认证中的 token 使用，我们已经有所应用，如 使用 kubeconfig 或 token 进行用户身份认证。\n认识 Kubernetes 中的用户 Kubernetes 集群中包含两类用户：一类是由 Kubernetes 管理的 service account，另一类是普通用户。\n普通用户被假定为由外部独立服务管理。管理员分发私钥，用户存储（如 Keystone 或 Google 帐户），甚至包含用户名和密码列表的文件。在这方面，Kubernetes 没有代表普通用户帐户的对象。无法通过 API 调用的方式向集群中添加普通 …","relpermalink":"/kubernetes-handbook/security/authentication/","summary":"在安装集群的时候我们在 master 节点上生成了一堆证书、token，还在 kubelet 的配置中用到了 bootstrap token，安装各种应用时，为了能够与 API server 通信创建了各种 service account，在 Dashboard 中使用了 kubeconfig 或 token 登陆，那么这些都属于什么认证","title":"Kubernetes 中的用户与身份认证授权"},{"content":"本文是对 Kubernetes 集群安全性管理的最佳实践。\n端口 请注意管理好以下端口。\n端口 进程 描述 4149/TCP kubelet 用于查询容器监控指标的 cAdvisor 端口 10250/TCP kubelet 访问节点的 API 端口 10255/TCP kubelet 未认证的只读端口，允许访问节点状态 10256/TCP kube-proxy kube-proxy 的健康检查服务端口 9099/TCP calico-felix calico 的健康检查服务端口（如果使用 calico/canal） 6443/TCP kube-apiserver Kubernetes API 端口 Kubernetes 安全扫描工具 kube-bench kube-bench 可以消除大约 kubernetes 集群中 95％的配置缺陷。通过应用 CIS Kubernetes Benchmark 来检查 master 节点、node 节点及其控制平面组件，从而确保集群设置了特定安全准则。在经历特定的 Kubernetes 安全问题或安全增强功能之前，这应该是第一步。\nAPI 设置 授 …","relpermalink":"/kubernetes-handbook/security/kubernetes-security-best-practice/","summary":"本文是对 Kubernetes 集群安全性管理的最佳实践。 端口 请注意管理好以下端口。 端口 进程 描述 4149/TCP kubelet 用于查询容器监控指标的 cAdvisor 端口 10250/TCP kubelet 访问节点的 API 端口 10255/TCP kubelet 未认证的只读端口，允许访问节点状态 10256/TCP kube-proxy kube-proxy 的健康检查服务端口 9099/TCP calico-felix calico 的健康","title":"Kubernetes 集群安全性配置最佳实践"},{"content":"本文列举了集中访问 Kubernetes 集群的方式。\n第一次使用 kubectl 访问 如果您是第一次访问 Kubernetes API 的话，我们建议您使用 Kubernetes 命令行工具：kubectl。\n为了访问集群，您需要知道集群的地址，并且需要有访问它的凭证。通常，如果您完成了入门指南那么这些将会自动设置，或者其他人为您部署的集群提供并给您凭证和集群地址。\n使用下面的命令检查 kubectl 已知的集群的地址和凭证：\n$ kubectl config view 直接访问 REST API Kubectl 处理对 apiserver 的定位和认证。如果您想直接访问 REST API，可以使用像 curl、wget 或浏览器这样的 http 客户端，有以下几种方式来定位和认证：\n以 proxy 模式运行 kubectl。 推荐方法。 使用已保存的 apiserver 位置信息。 使用自签名证书验证 apiserver 的身份。没有 MITM（中间人攻击）的可能。 认证到 apiserver。 将来，可能会做智能的客户端负载均衡和故障转移。 直接向 http 客户端提供位置和凭 …","relpermalink":"/kubernetes-handbook/access/methods/","summary":"本文列举了集中访问 Kubernetes 集群的方式。 第一次使用 kubectl 访问 如果您是第一次访问 Kubernetes API 的话，我们建议您使用 Kubernetes 命令行工具：kubectl。 为了访问集群，您需要知道集群的地址，并且需要有访问它的凭证。通常，如果您完成了入","title":"访问集群"},{"content":"Kubernetes 的认证方式对于不同的人来说可能有所不同。\n运行 kubelet 可能有一种认证方式（即证书）。 用户可能有不同的认证方式（即令牌）。 管理员可能具有他们为个人用户提供的证书列表。 我们可能有多个集群，并希望在同一个地方将其全部定义——这样用户就能使用自己的证书并重用相同的全局配置。 所以为了能够让用户轻松地在多个集群之间切换，对于多个用户的情况下，我们将其定义在了一个 kubeconfig 文件中。\n此文件包含一系列与昵称相关联的身份验证机制和集群连接信息。它还引入了一个（用户）认证信息元组和一个被称为上下文的与昵称相关联的集群连接信息的概念。\n如果明确指定，则允许使用多个 kubeconfig 文件。在运行时，它们与命令行中指定的覆盖选项一起加载并合并（参见下面的 规则）。\n相关讨论 http://issue.k8s.io/1755\nKubeconfig 文件的组成 Kubeconifg 文件示例 current-context: federal-context apiVersion: v1 clusters: - cluster: api-version: …","relpermalink":"/kubernetes-handbook/access/authenticate-across-clusters-kubeconfig/","summary":"Kubernetes 的认证方式对于不同的人来说可能有所不同。 运行 kubelet 可能有一种认证方式（即证书）。 用户可能有不同的认证方式（即令牌）。 管理员可能具有他们为个人用户提供的证书列表。 我们可能有多个集群，并希望在同一个地方将其","title":"使用 kubeconfig 文件配置跨集群认证"},{"content":"本页向您展示如何使用 kubectl port-forward 命令连接到运行在 Kubernetes 集群中的 Redis 服务器。这种类型的连接对于数据库调试很有帮助。\n创建一个 Pod 来运行 Redis 服务器 创建一个 Pod：\nkubectl create -f https://k8s.io/docs/tasks/access-application-cluster/redis-master.yaml 命令运行成功后将有以下输出验证该 Pod 是否已经创建：\npod \u0026#34;redis-master\u0026#34; created 检查 Pod 是否正在运行且处于就绪状态：\nkubectl get pods 当 Pod 就绪，输出显示 Running 的状态：\nNAME READY STATUS RESTARTS AGE redis-master 2/2 Running 0 41s 验证 Redis 服务器是否已在 Pod 中运行，并监听 6379 端口：\n{% raw %} kubectl get pods redis-master --template=\u0026#39;{{(index (index …","relpermalink":"/kubernetes-handbook/access/connecting-to-applications-port-forward/","summary":"本页向您展示如何使用 kubectl port-forward 命令连接到运行在 Kubernetes 集群中的 Redis 服务器。这种类型的连接对于数据库调试很有帮助。 创建一个 Pod 来运行 Redis 服务器 创建一个 Pod： kubectl create -f https://k8s.io/docs/tasks/access-application-cluster/redis-master.yaml 命令运行成功后将有以下输出验证该 Pod 是否已经创建： pod \"redis-master\" created","title":"通过端口转发访问集群中的应用程序"},{"content":"本文向您展示如何创建 Kubernetes Service 对象，外部客户端可以使用它来访问集群中运行的应用程序。该 Service 可以为具有两个运行实例的应用程序提供负载均衡。\n目的 运行 Hello World 应用程序的两个实例。 创建一个暴露 node 节点端口的 Service 对象。 使用 Service 对象访问正在运行的应用程序。 为在两个 pod 中运行的应用程序创建 service 在集群中运行 Hello World 应用程序：\nkubectl run hello-world --replicas=2 --labels=\u0026#34;run=load-balancer-example\u0026#34; --image=gcr.io/google-samples/node-hello:1.0 --port=8080 上述命令创建一个 Deployment 对象和一个相关联的 ReplicaSet 对象。该 ReplicaSet 有两个 Pod，每个 Pod 中都运行一个 Hello World 应用程序。\n显示关于该 Deployment 的信息：\nkubectl get …","relpermalink":"/kubernetes-handbook/access/service-access-application-cluster/","summary":"本文向您展示如何创建 Kubernetes Service 对象，外部客户端可以使用它来访问集群中运行的应用程序。该 Service 可以为具有两个运行实例的应用程序提供负载均衡。 目的 运行 Hello World 应用程序的两个实例。 创建一个暴露 node 节点端口的 Service 对象。 使用 Service 对","title":"使用 service 访问群集中的应用程序"},{"content":"前面几节讲到如何访问 Kubernetes 集群，本文主要讲解访问 Kubernetes 中的 Pod 和 Serivce 的几种方式，包括如下几种：\nhostNetwork hostPort NodePort LoadBalancer Ingress 说是暴露 Pod 其实跟暴露 Service 是一回事，因为 Pod 就是 Service 的后端。\nhostNetwork: true 这是一种直接定义 Pod 网络的方式。\n如果在 Pod 中使用 hostNotwork:true 配置的话，在这种 pod 中运行的应用程序可以直接看到 pod 启动的主机的网络接口。在主机的所有网络接口上都可以访问到该应用程序。以下是使用主机网络的 pod 的示例定义：\napiVersion: v1 kind: Pod metadata: name: influxdb spec: hostNetwork: true containers: - name: influxdb image: influxdb 部署该 Pod：\n$ kubectl create -f …","relpermalink":"/kubernetes-handbook/access/accessing-kubernetes-pods-from-outside-of-the-cluster/","summary":"前面几节讲到如何访问 Kubernetes 集群，本文主要讲解访问 Kubernetes 中的 Pod 和 Serivce 的几种方式，包括如下几种： hostNetwork hostPort NodePort LoadBalancer Ingress 说是暴露 Pod 其实跟暴露 Service 是一回事，因为 Pod 就是 Service 的后端。 hostNetwork: true 这是一种直接定义 Pod 网络的方式。 如果在 Pod 中使用 hostNotwork:true 配置的","title":"从外部访问 Kubernetes 中的 Pod"},{"content":"Lens 是一款开源的 Kubenretes IDE，也可以作为桌面客户端，官方网站 https://k8slens.dev，具有以下特性：\n完全开源，GitHub 地址 https://github.com/lensapp/lens 实时展示集群状态 内置 Prometheus 监控 多集群，多个 namespace 管理 原生 Kubernetes 支持 支持使用 chart 安装应用 使用 kubeconfig 登陆认证 支持多平台，Windows、Mac、Linux Visual Studio Code 友好的风格设计 Lens 界面图下图所示。\nLens Kubernetes IDE 界面 参考 Lens, Kubernetes IDE - k8slens.dev ","relpermalink":"/kubernetes-handbook/access/lens/","summary":"Lens 是一款开源的 Kubenretes IDE，也可以作为桌面客户端，官方网站 https://k8slens.dev，具有以下特性： 完全开源，GitHub 地址 https://github.com/lensapp/lens 实时展示集群状态 内置 Prometheus 监控 多集群，多个 namespace 管理 原生 Kubernetes 支持 支持使用 chart","title":"Lens - Kubernetes IDE"},{"content":"Kubernator 相较于 Kubernetes Dashboard 来说，是一个更底层的 Kubernetes UI，Dashboard 操作的都是 Kubernetes 的底层对象，而 Kubernator 是直接操作 Kubernetes 各个对象的 YAML 文件。\nKubernator 提供了一种基于目录树和关系拓扑图的方式来管理 Kubernetes 的对象的方法，用户可以在 Web 上像通过 GitHub 的网页版一样操作 Kubernetes 的对象，执行修改、拷贝等操作，详细的使用方式见 https://github.com/smpio/kubernator。\n安装 Kubernator Kubernator 的安装十分简单，可以直接使用 kubectl 命令来运行，它不依赖任何其它组件。\nkubectl create ns kubernator kubectl -n kubernator run --image=smpio/kubernator --port=80 kubernator kubectl -n kubernator expose deploy …","relpermalink":"/kubernetes-handbook/access/kubernator-kubernetes-ui/","summary":"Kubernator 相较于 Kubernetes Dashboard 来说，是一个更底层的 Kubernetes UI，Dashboard 操作的都是 Kubernetes 的底层对象，而 Kubernator 是直接操作 Kubernetes 各个对象的 YAML 文件。 Kubernator 提供了一种基于目录树和关系拓扑图的方式来管理 Kubernetes 的对象的方法，用户可以在 Web 上像通过 GitHub","title":"Kubernator - 更底层的 Kubernetes UI"},{"content":"本文讲解了如何开发容器化应用，并使用 Wercker 持续集成工具构建 docker 镜像上传到 docker 镜像仓库中，然后在本地使用 docker-compose 测试后，再使用 kompose 自动生成 kubernetes 的 yaml 文件，再将注入 Envoy sidecar 容器，集成 Istio 服务网格中的详细过程。\n整个过程如下图所示。\n流程图 为了讲解详细流程，我特意写了用 Go 语言开发的示例程序放在 GitHub 中，模拟监控流程：\nk8s-app-monitor-test：生成模拟的监控数据，在接收到 http 请求返回 json 格式的 metrics 信息 K8s-app-monitor-agent：获取监控 metrics 信息并绘图，访问该服务将获得监控图表 API 文档见 k8s-app-monitor-test 中的 api.html 文件，该文档在 API blueprint 中定义，使用 aglio 生成，打开后如图所示：\nAPI 关于服务发现 K8s-app-monitor-agent …","relpermalink":"/kubernetes-handbook/devops/deploy-applications-in-kubernetes/","summary":"本文讲解了如何开发容器化应用，并使用 Wercker 持续集成工具构建 docker 镜像上传到 docker 镜像仓库中，然后在本地使用 docker-compose 测试后，再使用 kompose 自动生成 kubernetes 的 yaml 文件，再将注入 Envoy sidecar 容器，集成 Istio 服务网格中的详细过程。 整个过程如下图所示。 流","title":"适用于 Kubernetes 的应用开发部署流程"},{"content":"本文档不是说明如何在 kubernetes 中开发和部署应用程序，如果您想要直接开发应用程序在 kubernetes 中运行可以参考 适用于 kubernetes 的应用开发部署流程。\n本文旨在说明如何将已有的应用程序尤其是传统的分布式应用程序迁移到 kubernetes 中。如果该类应用程序符合云原生应用规范（如 12 因素法则）的话，那么迁移会比较顺利，否则会遇到一些麻烦甚至是阻碍。具体请参考 迁移至云原生应用架构。\n下图是将单体应用迁移到云原生的步骤。\n将单体应用迁移到云原生 (图片来自 DevOpsDay Toronto) 接下来我们将以 Spark on YARN with kubernetes 为例来说明，该例子足够复杂也很有典型性，了解了这个例子可以帮助大家将自己的应用迁移到 kubernetes 集群上去。\n下图即整个架构的示意图，所有的进程管理和容器扩容直接使用 Makefile。\nspark on yarn with kubernetes 注意：该例子仅用来说明具体的步骤划分和复杂性，在生产环境应用还有待验证，请谨慎使用。\n术语 对于为曾接触过 kubernetes …","relpermalink":"/kubernetes-handbook/devops/migrating-hadoop-yarn-to-kubernetes/","summary":"本文档不是说明如何在 kubernetes 中开发和部署应用程序，如果您想要直接开发应用程序在 kubernetes 中运行可以参考 适用于 kubernetes 的应用开发部署流程。 本文旨在说明如何将已有的应用程序尤其是传统的分布式应用程序迁移到 kubernetes 中。如果该类应用程","title":"迁移传统应用到 Kubernetes 步骤详解——以 Hadoop YARN 为例"},{"content":"StatefulSet 这个对象是专门用来部署用状态应用的，可以为 Pod 提供稳定的身份标识，包括 hostname、启动顺序、DNS 名称等。\n下面以在 Kubernetes1.6 版本中部署 zookeeper 和 kafka 为例讲解 StatefulSet 的使用，其中 kafka 依赖于 zookeeper。\nDockerfile 和配置文件见 zookeeper 和 kafka。\n注：所有的镜像基于 CentOS 系统的 JDK 制作，为我的私人镜像，外部无法访问，yaml 中没有配置持久化存储。\n部署 Zookeeper Dockerfile 中从远程获取 zookeeper 的安装文件，然后在定义了三个脚本：\nzkGenConfig.sh：生成 zookeeper 配置文件 zkMetrics.sh：获取 zookeeper 的 metrics zkOk.sh：用来做 ReadinessProb 我们在来看下这三个脚本的执行结果。\nzkMetrics.sh 脚本实际上执行的是下面的命令：\n$ echo mntr | nc localhost …","relpermalink":"/kubernetes-handbook/devops/using-statefulset/","summary":"StatefulSet 这个对象是专门用来部署用状态应用的，可以为 Pod 提供稳定的身份标识，包括 hostname、启动顺序、DNS 名称等。 下面以在 Kubernetes1.6 版本中部署 zookeeper 和 kafka 为例讲解 StatefulSet 的使用，其中 kafka 依赖于 zookeeper。 Dockerfile 和配置文件","title":"使用 StatefulSet 部署有状态应用"},{"content":"持续集成与交付，简称 CI/CD（Continous Integration/Continous Delivery），是一种软件开发实践，旨在通过自动化软件构建、测试和部署过程，提高应用程序的交付速度和质量。它涉及多个阶段，包括代码管理、构建、测试、部署和监控。CI/CD 可以帮助开发团队更快地迭代和交付新功能，同时减少故障和错误。\n什么是持续集成与交付？ 持续集成（Continuous Integration，CI）是指开发团队通过自动化将代码的集成过程与频率增加到了一个可持续的水平。在持续集成中，开发人员经常将代码提交到共享存储库，并使用自动化构建系统（如 Jenkins、Travis CI 等）对代码进行构建、测试和部署。这有助于尽早发现和解决潜在的问题，并确保代码的稳定性和质量。\n持续交付（Continuous Delivery，CD）是在持续集成的基础上进一步扩展的概念。它强调在持续集成的基础上，通过自动化的部署流程，使软件随时处于可部署状态。持续交付的目标是实现在任何时候都能够轻松、可靠地将软件部署到生产环境中，以便快速响应需求变化或发布新功能。\nCI/CD 的主要优势包 …","relpermalink":"/kubernetes-handbook/devops/ci-cd/","summary":"持续集成与交付，简称 CI/CD（Continous Integration/Continous Delivery），是一种软件开发实践，旨在通过自动化软件构建、测试和部署过程，提高应用程序的交付速度和质量。它涉及多个阶段，包括代码管理、构建、测","title":"持续集成与交付（CI/CD）"},{"content":"Kustomize是一个开源的 Kubernetes 配置管理工具，用于对 Kubernetes 清单文件进行自定义和修改。它允许用户通过分层和声明式的方式管理和定制应用程序的配置，而无需直接修改原始的清单文件，促进了配置的复用和可维护性。\nKustomize 的主要功能包括：\n配置合并：Kustomize 允许用户通过定义基础配置和覆盖配置的方式来合并和定制 Kubernetes 清单文件。基础配置可以作为一个基准，而覆盖配置可以包含对基础配置进行修改和定制的内容。这种分层的方式使得对配置进行管理和修改更加灵活和可维护。\n声明式的配置：Kustomize 使用基于文件的声明式配置格式，使得用户可以以清晰的方式描述应用程序的配置和定制需求。用户可以定义资源的名称、标签、注释、环境变量等，并指定资源之间的关系和依赖。\n配置重用：Kustomize 支持配置的重用和共享。用户可以定义可重用的配置片段，并在多个应用程序中进行引用。这样可以避免重复的配置，提高配置的可维护性和复用性。\n多环境管理：Kustomize 支持多个环境（例如开发、测试、生产）的管理。用户可以根据不同环境的需求，为每个 …","relpermalink":"/kubernetes-handbook/devops/kustomize/","summary":"Kustomize是一个开源的 Kubernetes 配置管理工具，用于对 Kubernetes 清单文件进行自定义和修改。它允许用户通过分层和声明式的方式管理和定制应用程序的配置，而无需直接修改原始的清单文件，促进了配置的复用和可维护性。 Kustomize 的","title":"使用 Kustomize 配置 Kubernetes 应用"},{"content":"Kubernetes 的社区是以 SIG（Special Interest Group 特别兴趣小组）和工作组的形式组织起来的，每个工作组都会定期召开视频会议。\n所有的 SIG 和工作组都使用 slack 和邮件列表沟通。\nKubernetes SIG 主要 SIG 列表 api-machinery：所有 API 级别的功能，包括了 API server、API 注册和发现、通用的 API CRUD 语义，准入控制，编码 / 解码，转换，默认值，持久化层（etcd），OpenAPI，第三方资源，垃圾回收（gc）和客户端库的方方面面。 aws：如何在 AWS 上支持和使用 kubernetes。 apps：在 kubernetes 上部署和运维应用程序。关注开发者和 DevOps 在 kubernetes 上运行应用程序的体验。 architecture：维持 kubernetes 在架构设计上的一致性和原则。 auth：kubernetes 的认证授权、权限管理和安全性策略。 autoscaling：集群的自动缩放，pod 的水平和垂直自动缩放，pod 的资源初始化，pod 监控和指标 …","relpermalink":"/kubernetes-handbook/develop/sigs-and-working-group/","summary":"Kubernetes 的社区是以 SIG（Special Interest Group 特别兴趣小组）和工作组的形式组织起来的，每个工作组都会定期召开视频会议。 所有的 SIG 和工作组都使用 slack 和邮件列表沟通。 Kubernetes SIG 主要 SIG 列表 api-machinery：所有 API 级","title":"SIG 和工作组"},{"content":"我们将在 Mac 上使用 docker 环境编译 kuberentes。\n安装依赖 brew install gnu-tar Docker 环境，至少需要给容器分配 4G 内存，在低于 3G 内存的时候可能会编译失败。\n执行编译 切换目录到 kuberentes 源码的根目录下执行：\n./build/run.sh make 可以在 docker 中执行跨平台编译出二进制文件。\n需要用的的 docker 镜像：\ngcr.io/google_containers/kube-cross:v1.7.5-2 该镜像基于 Ubuntu 构建，大小 2.15G，编译环境中包含以下软件：\nGo1.7.5 etcd protobuf g++ 其他 golang 依赖包 在我自己的电脑上的整个编译过程大概要半个小时。\n编译完成的二进制文件在 /_output/local/go/bin/ 目录下。\n","relpermalink":"/kubernetes-handbook/develop/developing-environment/","summary":"我们将在 Mac 上使用 docker 环境编译 kuberentes。 安装依赖 brew install gnu-tar Docker 环境，至少需要给容器分配 4G 内存，在低于 3G 内存的时候可能会编译失败。 执行编译 切换目录到 kuberentes 源码的根目录下执行： ./build/run.sh make 可以在 docker 中执行跨平台编译出","title":"配置 Kubernetes 开发环境"},{"content":"这篇文章将指导你如何测试 Kubernetes。\n单元测试 单元测试仅依赖于源代码，是测试代码逻辑是否符合预期的最简单方法。\n运行所有的单元测试\nmake test 仅测试指定的 package\n# 单个package make test WHAT=./pkg/api # 多个packages make test WHAT=./pkg/{api,kubelet} 或者，也可以直接用 go test\ngo test -v k8s.io/kubernetes/pkg/kubelet 仅测试指定 package 的某个测试 case\n# Runs TestValidatePod in pkg/api/validation with the verbose flag set make test WHAT=./pkg/api/validation KUBE_GOFLAGS=\u0026#34;-v\u0026#34; KUBE_TEST_ARGS=\u0026#39;-run ^TestValidatePod$\u0026#39; # Runs tests that match the regex ValidatePod|ValidateConfigMap in …","relpermalink":"/kubernetes-handbook/develop/testing/","summary":"这篇文章将指导你如何测试 Kubernetes。 单元测试 单元测试仅依赖于源代码，是测试代码逻辑是否符合预期的最简单方法。 运行所有的单元测试 make test 仅测试指定的 package # 单个package make test WHAT=./pkg/api # 多个package","title":"测试 Kubernetes"},{"content":"访问 kubernetes 集群有以下几种方式：\n方式 特点 支持者 Kubernetes dashboard 直接通过 Web UI 进行操作，简单直接，可定制化程度低 官方支持 kubectl 命令行操作，功能最全，但是比较复杂，适合对其进行进一步的分装，定制功能，版本适配最好 官方支持 client-go 从 kubernetes 的代码中抽离出来的客户端包，简单易用，但需要小心区分 kubernetes 的 API 版本 官方支持 client-python python 客户端，kubernetes-incubator 官方支持 Java client fabric8 中的一部分，kubernetes 的 java 客户端 Red Hat 下面，我们基于 client-go，对 Deployment 升级镜像的步骤进行了定制，通过命令行传递一个 Deployment 的名字、应用容器名和新 image 名字的方式来升级。\nkubernetes-client-go-sample 项目的 main.go 代码如下：\npackage main import ( \u0026#34;flag\u0026#34; …","relpermalink":"/kubernetes-handbook/develop/client-go-sample/","summary":"访问 kubernetes 集群有以下几种方式： 方式 特点 支持者 Kubernetes dashboard 直接通过 Web UI 进行操作，简单直接，可定制化程度低 官方支持 kubectl 命令行操作，功能最全，但是比较复杂，适合对其进行进一步的分装，定制功能，版本适配最好 官方支持 client-go 从 kubernetes 的","title":"client-go 示例"},{"content":"Operator 最初是由 CoreOS（后被 Red Hat 收购）开发的，下面是关于 Operator 的一些基础知识：\nOperator 是用来扩展 Kubernetes API 的特定的应用程序控制器； Operator 用来创建、配置和管理复杂的有状态应用，如数据库、缓存和监控系统； Operator 基于 Kubernetes 的资源和控制器概念之上构建，但同时又包含了应用程序特定的领域知识； 创建 Operator 的关键是 CRD（自定义资源）的设计； Operator 通常作为 Deployment 资源部署在 Kubernetes 中，删掉 Operator 不会影响已使用它创建的自定义资源； Operator Hub 中罗列了目前已知的 Operator。 工作原理 Operator 是将运维人员对软件操作的知识给代码化，同时利用 Kubernetes 强大的抽象来管理大规模的软件应用。\nOperator 使用了 Kubernetes 的自定义资源扩展 API 机制，如使用 CRD（CustomResourceDefinition）来创建。Operator 通过这 …","relpermalink":"/kubernetes-handbook/develop/operator/","summary":"关于 Kubernetes Operator 的原理、用途等基础知识介绍。","title":"Operator"},{"content":"Operator SDK 由 CoreOS 开源，它是用于构建 Kubernetes 原生应用的 SDK，它提供更高级别的 API、抽象和项目脚手架。在阅读本文前请先确认您已经了解 Operator是什么。\n使用 Kubernetes 中原生的对象来部署和管理复杂的应用程序不是那么容易，尤其是要管理整个应用的生命周期、组件的扩缩容，我们之前通常是编写各种脚本，通过调用 Kubernetes 的命令行工具来管理 Kubernetes 上的应用。现在可以通过 CRD（CustomResourceDefinition）来自定义这些复杂操作，通过将运维的知识封装在自定义 API 里来减轻运维人员的负担。同时我们还可以像操作 Kubernetes 的原生资源对象一样，使用 kubectl 来操作 CRD。\n下面我们将安装和试用一下 Operator SDK。\n安装 Operator SDK $ mkdir -p $GOPATH/src/github.com/operator-framework $ cd …","relpermalink":"/kubernetes-handbook/develop/operator-sdk/","summary":"Operator SDK 由 CoreOS 开源，它是用于构建 Kubernetes 原生应用的 SDK，它提供更高级别的 API、抽象和项目脚手架。在阅读本文前请先确认您已经了解 Operator是什么。 使用 Kubernetes 中原生的对象来部署和管理复杂的应用程序不是那么容易，","title":"Operator SDK"},{"content":"Kubebuilder 是一个基于 CRD 来构建 Kubernetes API 的框架，可以使用 CRD 来构建 API、Controller 和 Admission Webhook。\n动机 目前扩展 Kubernetes 的 API 的方式有创建 CRD、使用 Operator SDK 等方式，都需要写很多的样本文件（boilerplate），使用起来十分麻烦。为了能够更方便构建 Kubernetes API 和工具，就需要一款能够事半功倍的工具，与其他 Kubernetes API 扩展方案相比，kubebuilder 更加简单易用，并获得了社区的广泛支持。\n工作流程 Kubebuilder 的工作流程如下：\n创建一个新的工程目录 创建一个或多个资源 API CRD 然后将字段添加到资源 在控制器中实现协调循环（reconcile loop），watch 额外的资源 在集群中运行测试（自动安装 CRD 并自动启动控制器） 更新引导集成测试测试新字段和业务逻辑 使用用户提供的 Dockerfile 构建和发布容器 设计哲学 Kubebuilder …","relpermalink":"/kubernetes-handbook/develop/kubebuilder/","summary":"Kubebuilder 是一个基于 CRD 来构建 Kubernetes API 的框架，可以使用 CRD 来构建 API、Controller 和 Admission Webhook。 动机 目前扩展 Kubernetes 的 API 的方式有创建 CRD、使用 Operator SDK 等方式，都需要写很多的样本文件（boilerplate），","title":"Kubebuilder"},{"content":"本页假定您已经熟悉 Kubernetes 的核心概念并可以轻松的部署自己的应用程序。在浏览了本页面及其链接的内容后，您将会更好的理解如下部分：\n可以在应用程序中使用的高级功能 扩展 Kubernetes API 的各种方法 使用高级功能部署应用 现在您知道了 Kubernetes 中提供的一组 API 对象。理解了 daemonset 和 deployment 之间的区别对于应用程序部署通常是足够的。也就是说，熟悉 Kubernetes 中其它的鲜为人知的功能也是值得的。因为这些功能有时候对于特别的用例是非常强大的。\n容器级功能 如您所知，将整个应用程序（例如容器化的 Rails 应用程序，MySQL 数据库以及所有应用程序）迁移到单个 Pod 中是一种反模式。这就是说，有一些非常有用的模式超出了容器和 Pod 之间的 1:1 的对应关系：\nSidecar 容器：虽然 Pod 中依然需要有一个主容器，你还可以添加一个副容器作为辅助（见 日志示例)。单个 Pod 中的两个容器可以通过共享卷进行通信。 Init 容器：Init 容器在 Pod 的应用容器（如主容器和 sidecar 容器） …","relpermalink":"/kubernetes-handbook/develop/advance-developer/","summary":"本页假定您已经熟悉 Kubernetes 的核心概念并可以轻松的部署自己的应用程序。在浏览了本页面及其链接的内容后，您将会更好的理解如下部分： 可以在应用程序中使用的高级功能 扩展 Kubernetes API 的各种方法 使用高级功能部署应用 现在您知道了","title":"高级开发指南"},{"content":"如果您想参与 Kubernetes 社区，请先阅读下Kubernetes Community这个 GitHub Repo 中的文档，该文档中包括社区的治理形式、社区成员资格申请、提交 Issue、查找问题和提交 PR 的指导等。\n参考 Kubernetes Community Kubernetes Developer Guide Enhencement Tracking and Backlog Kubernetes 官方网站项目 ","relpermalink":"/kubernetes-handbook/develop/contribute/","summary":"如果您想参与 Kubernetes 社区，请先阅读下Kubernetes Community这个 GitHub Repo 中的文档，该文档中包括社区的治理形式、社区成员资格申请、提交 Issue、查找问题和提交 PR 的指导等。 参考 Kubernetes Community Kubernetes Developer Guide Enhencement Tracking and Backlog Kubernetes 官","title":"参与 Kubernetes 社区贡献"},{"content":"Minikube 用于在本地运行 kubernetes 环境，用来开发和测试。\n安装 Minikube 到 GitHub 下载 minikube，我安装的是 minikube v1.11.0。\n下载完成后修改文件名为 minikube，然后 chmod +x minikube，移动到 $PATH 目录下：\nsudo mv ~/Download/minikube-darwin-adm64 /usr/local/bin/ sudo chmod +x /usr/local/bin/minikube 安装 kubectl 方式一\n参考 Install and Set Up kubectl，直接使用二进制文件安装即可。\ncurl -LO https://storage.googleapis.com/kubernetes-release/release/`curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt`/bin/darwin/amd64/kubectl …","relpermalink":"/kubernetes-handbook/develop/minikube/","summary":"Minikube 用于在本地运行 kubernetes 环境，用来开发和测试。 安装 Minikube 到 GitHub 下载 minikube，我安装的是 minikube v1.11.0。 下载完成后修改文件名为 minikube，然后 chmod +x minikube，移动到 $PATH 目录下： sudo mv ~/Download/minikube-darwin-adm64 /usr/local/bin/ sudo chmod +x /usr/local/bin/minikube 安","title":"Minikube"},{"content":"Istio 可以轻松创建具有丰富路由、负载均衡、服务间身份验证、监控等功能的已部署服务网络 - 所有这些都无需对应用程序代码进行任何更改。 Istio 致力于以最小的资源开销提供这些优势，并旨在支持具有高请求率的大型网格，同时增加最小的延迟。\nIstio 数据平面组件（Envoy 代理）处理流经系统的数据。 Istio 控制平面组件 Istiod 配置数据平面。数据平面和控制平面具有不同的性能问题。\nIstio 1.18 的性能摘要 Istio 负载测试网格由 1000 个服务和 2000 个 sidecar 组成，每秒有 70,000 个网格范围的请求。\n控制平面性能 Istiod 根据用户编写的配置文件和系统的当前状态来配置 sidecar 代理。在 Kubernetes 环境中，自定义资源定义 (CRD) 和部署构成了系统的配置和状态。 Istio 配置对象（例如 Gateway 和 VirtualService）提供用户编写的配置。为了生成代理的配置，Istiod 处理来自 Kubernetes 环境的组合配置和系统状态以及用户编写的配置。\n控制平面支持数千个服务， …","relpermalink":"/blog/performance-and-scalability/","summary":"Istio 官方公布 Istio 1.18 性能测试结果。","title":"Istio 1.18 性能测试结果"},{"content":"API Reference Packages:\ntsb.tetrate.io/v2 application.tsb.tetrate.io/v2 extension.tsb.tetrate.io/v2 gateway.tsb.tetrate.io/v2 istiointernal.tsb.tetrate.io/v2 rbac.tsb.tetrate.io/v2 security.tsb.tetrate.io/v2 traffic.tsb.tetrate.io/v2 tsb.tetrate.io/v2 Resource Types:\nCluster\nOrganization\nOrganizationSetting\nServiceAccount\nTeam\nTenant\nTenantSetting\nWorkspace\nWorkspaceSetting\nCluster ↩ Parent\nName Type Description Required apiVersion string tsb.tetrate.io/v2 true kind string Cluster true metadata …","relpermalink":"/tsb/reference/k8s-api/tsb-crds-gen/","summary":"API Reference Packages:\ntsb.tetrate.io/v2 application.tsb.tetrate.io/v2 extension.tsb.tetrate.io/v2 gateway.tsb.tetrate.io/v2 istiointernal.tsb.tetrate.io/v2 rbac.tsb.tetrate.io/v2 security.tsb.tetrate.io/v2 traffic.tsb.tetrate.io/v2 tsb.tetrate.io/v2 Resource Types:\nCluster\nOrganization\nOrganizationSetting\nServiceAccount\nTeam\nTenant\nTenantSetting\nWorkspace\nWorkspaceSetting\nCluster ↩ Parent\nName Type Description Required apiVersion string tsb.tetrate.io/v2 true kind string Cluster true metadata object Refer to the Kubernetes API documentation for the fields of the `metadata` field. true spec object A Kubernetes cluster managing both pods and VMs. false status object false Cluster.spec ↩ Parent\nA Kubernetes cluster managing both pods and VMs.\nName Type Description Required description string A description of the resource. false displayName string User friendly name for the resource. false etag string The etag for the resource.","title":""},{"content":" AccessBindings is an assignment of roles to a set of users or teams to access resources. The user or team information is obtained from an user directory (such an LDAP server or an external OIDC server) that should have been configured as part of Service Bridge installation. Note that an AccessBinding can be created or modified only by users who have SET_POLICY permission on the target resource.\nThe following example assigns the workspace-admin role to users alice, bob, and members of the t1 …","relpermalink":"/tsb/refs/tsb/rbac/v2/access-bindings/","summary":"AccessBindings is an assignment of roles to a set of users or teams to access resources. The user or team information is obtained from an user directory (such an LDAP server or an external OIDC server) that should have been configured as part of Service Bridge installation. Note that an AccessBinding can be created or modified only by users who have SET_POLICY permission on the target resource.\nThe following example assigns the workspace-admin role to users alice, bob, and members of the t1 team for the workspace w1 owned by the tenant mycompany.\nUse fully-qualified name (fqn) when specifying the target resource, as well as for the users and teams.","title":"Access Bindings"},{"content":" Agent Configuration specifies configuration of the Workload Onboarding Agent.\nIn most cases, Workload Onboarding Agent can automatically recognize the host environment, e.g. AWS EC2, which makes explicit Agent Configuration optional.\nBy default, Workload Onboarding Agent comes with the minimal configuration:\napiVersion: config.agent.onboarding.tetrate.io/v1alpha1 kind: AgentConfiguration which at runtime is interpreted as an equivalent of:\napiVersion: config.agent.onboarding.tetrate.io/v1alpha1 …","relpermalink":"/tsb/refs/onboarding/config/agent/v1alpha1/agent-configuration/","summary":"Agent Configuration specifies configuration of the Workload Onboarding Agent.\nIn most cases, Workload Onboarding Agent can automatically recognize the host environment, e.g. AWS EC2, which makes explicit Agent Configuration optional.\nBy default, Workload Onboarding Agent comes with the minimal configuration:\napiVersion: config.agent.onboarding.tetrate.io/v1alpha1 kind: AgentConfiguration which at runtime is interpreted as an equivalent of:\napiVersion: config.agent.onboarding.tetrate.io/v1alpha1 kind: AgentConfiguration host: auto: {} sidecar: istio: {} stdout: filename: /dev/stdout stderr: filename: /dev/stderr The above configuration means that Workload Onboarding Agent should infer host environment automatically, should be in control of the Istio Sidecar pre-installed on that host, should redirect standard output of the Istio Sidecar into its own output.","title":"Agent Configuration"},{"content":":::note Tetrate Service Bridge collects a large number of metrics and the relationship between those, and the threshold limits that you set will differ from environment to environment. This document outlines the generic alerting guidelines rather than providing an exhaustive list of alert configurations and thresholds, since these will differ between different environments with different workload configurations. :::\nTSB Operational Status TSB Availability The rate of successful requests to TSB …","relpermalink":"/tsb/operations/telemetry/alerting-guidelines/","summary":":::note Tetrate Service Bridge collects a large number of metrics and the relationship between those, and the threshold limits that you set will differ from environment to environment. This document outlines the generic alerting guidelines rather than providing an exhaustive list of alert configurations and thresholds, since these will differ between different environments with different workload configurations. :::\nTSB Operational Status TSB Availability The rate of successful requests to TSB API. This is an extremely user-visible signal and should be treated as such.\nEstablish the THRESHOLD value from historical metric data captured within your environment used as a baseline. A reasonable value for a first iteration would be 0.","title":"Alerting Guidelines"},{"content":" API objects define a set of servers and endpoints that expose the business logic for an Application. APIs are attached to existing Applications to configure how the features exposed by the different services that are part of the Application can be accessed.\nThe format used to define APIs is based on the OpenAPI v3 spec. Users can attach OpenAPI documents to the applications, and Service Bridge will generate all the configuration that is needed to make the APIs available. Service Bridge also …","relpermalink":"/tsb/refs/tsb/application/v2/api/","summary":"API objects define a set of servers and endpoints that expose the business logic for an Application. APIs are attached to existing Applications to configure how the features exposed by the different services that are part of the Application can be accessed.\nThe format used to define APIs is based on the OpenAPI v3 spec. Users can attach OpenAPI documents to the applications, and Service Bridge will generate all the configuration that is needed to make the APIs available. Service Bridge also provides a set of custom extensions to the OpenAPI spec that can be used to further customize the APIs in those cases where the standard OpenAPI properties are not sufficient.","title":"API"},{"content":" DEPRECATED: use Access Bindings instead.\nAPIAccessBindings is an assignment of roles to a set of users or teams to access API resources. The user or team information is obtained from an LDAP server that should have been configured as part of Service Bridge installation. Note that a APIAccessBinding can be created or modified only by users who have set_rbac permission on the API resource.\nThe following example assigns the api-admin role to users alice, bob, and members of the t1 team for the …","relpermalink":"/tsb/refs/tsb/rbac/v2/api-access-bindings/","summary":"DEPRECATED: use Access Bindings instead.\nAPIAccessBindings is an assignment of roles to a set of users or teams to access API resources. The user or team information is obtained from an LDAP server that should have been configured as part of Service Bridge installation. Note that a APIAccessBinding can be created or modified only by users who have set_rbac permission on the API resource.\nThe following example assigns the api-admin role to users alice, bob, and members of the t1 team for the APIs openapi in the application app owned by the tenant mycompany. Use fully-qualified name (fqn) when specifying user and team","title":"API Access Bindings"},{"content":" Applications are logical groupings of services that are related to each other, typically within a trusted group. A common example are three tier applications composed of a frontend, a backend and a datastore service.\nApplications are often consumed through APIs, and a single Application can expose one or more of those APIs. These APIs will define the hostnames that are exposed and the methods exposed in each hostname.\napiVersion: application.tsb.tetrate.io/v2 kind: Application metadata: name: …","relpermalink":"/tsb/refs/tsb/application/v2/application/","summary":"Applications are logical groupings of services that are related to each other, typically within a trusted group. A common example are three tier applications composed of a frontend, a backend and a datastore service.\nApplications are often consumed through APIs, and a single Application can expose one or more of those APIs. These APIs will define the hostnames that are exposed and the methods exposed in each hostname.\napiVersion: application.tsb.tetrate.io/v2 kind: Application metadata: name: three-tier organization: myorg tenant: tetrate spec: workspace: organizations/myorg/tenants/tetrate/three-tier Application An Application represents a set of logical groupings of services that are related to each other and expose a set of APIs that implement a complete set of business logic.","title":"Application"},{"content":" DEPRECATED: use Access Bindings instead.\nApplicationAccessBindings is an assignment of roles to a set of users or teams to access Application resources. The user or team information is obtained from an LDAP server that should have been configured as part of Service Bridge installation. Note that a ApplicationAccessBinding can be created or modified only by users who have SET_POLICY permission on the Application.\nThe following example assigns the application-admin role to users alice, bob, and …","relpermalink":"/tsb/refs/tsb/rbac/v2/application-access-bindings/","summary":"DEPRECATED: use Access Bindings instead.\nApplicationAccessBindings is an assignment of roles to a set of users or teams to access Application resources. The user or team information is obtained from an LDAP server that should have been configured as part of Service Bridge installation. Note that a ApplicationAccessBinding can be created or modified only by users who have SET_POLICY permission on the Application.\nThe following example assigns the application-admin role to users alice, bob, and members of the t1 team for the application app owned by the tenant mycompany. Use fully-qualified name (fqn) when specifying user and team\napiVersion: rbac.","title":"Application Access Bindings"},{"content":"import { PanelContent, PanelContentCode, } from “@theme/Panel”;\nService to manage Applications and APis\nApplications The Applications service exposes methods to manage Applications and API definitions in Service Bridge.\nCreateApplication rpc CreateApplication (tetrateio.api.tsb.application.v2.CreateApplicationRequest) returns (tetrateio.api.tsb.application.v2.Application)\nRequires CREATE\nCreates a new Application in TSB.\nGetApplication rpc GetApplication …","relpermalink":"/tsb/refs/tsb/application/v2/application-service/","summary":"import { PanelContent, PanelContentCode, } from “@theme/Panel”;\nService to manage Applications and APis\nApplications The Applications service exposes methods to manage Applications and API definitions in Service Bridge.\nCreateApplication rpc CreateApplication (tetrateio.api.tsb.application.v2.CreateApplicationRequest) returns (tetrateio.api.tsb.application.v2.Application)\nRequires CREATE\nCreates a new Application in TSB.\nGetApplication rpc GetApplication (tetrateio.api.tsb.application.v2.GetApplicationRequest) returns (tetrateio.api.tsb.application.v2.Application)\nRequires READ\nGet the details of an existing application.\nUpdateApplication rpc UpdateApplication (tetrateio.api.tsb.application.v2.Application) returns (tetrateio.api.tsb.application.v2.Application)\nRequires WRITE\nModify an existing application.\nListApplications rpc ListApplications (tetrateio.api.tsb.application.v2.ListApplicationsRequest) returns (tetrateio.api.tsb.application.v2.ListApplicationsResponse)\nList all existing applications for the given tenant.\nDeleteApplication rpc DeleteApplication (tetrateio.api.tsb.application.v2.DeleteApplicationRequest) returns (google.protobuf.Empty)\nRequires DELETE\nDelete an existing Application. Note that deleting resources in TSB is a recursive operation.","title":"Application Service"},{"content":"import { PanelContent, PanelContentCode, } from “@theme/Panel”;\nService to manage centralized approval policies.\nApprovals The Approvals service exposes methods for working with approval policies. $hide_from_yaml\nSetPolicy rpc SetPolicy (tetrateio.api.tsb.q.v2.ApprovalPolicy) returns (google.protobuf.Empty)\nRequires CreateApprovalPolicy, WriteApprovalPolicy\nSetPolicy enables authorization policy checks for the given resource and applies any provided request or approval settings. If the resource …","relpermalink":"/tsb/refs/tsb/q/v2/approvals-service/","summary":"import { PanelContent, PanelContentCode, } from “@theme/Panel”;\nService to manage centralized approval policies.\nApprovals The Approvals service exposes methods for working with approval policies. $hide_from_yaml\nSetPolicy rpc SetPolicy (tetrateio.api.tsb.q.v2.ApprovalPolicy) returns (google.protobuf.Empty)\nRequires CreateApprovalPolicy, WriteApprovalPolicy\nSetPolicy enables authorization policy checks for the given resource and applies any provided request or approval settings. If the resource has existing policies settings, they will be replaced. Once the policy is set, authorization checks will be performed for the given resource.\nGetPolicy rpc GetPolicy (tetrateio.api.tsb.q.v2.GetPolicyRequest) returns (tetrateio.api.tsb.q.v2.ApprovalPolicy)\nRequires ReadApprovalPolicy\nGetPolicy returns the approval policy for the given resource.\nQueryPolicies rpc QueryPolicies (tetrateio.api.tsb.q.v2.QueryPoliciesRequest) returns (tetrateio.api.tsb.q.v2.QueryPoliciesResponse)\nDeletePolicy rpc DeletePolicy (tetrateio.","title":"Approvals Service"},{"content":"import { PanelContent, PanelContentCode, } from “@theme/Panel”;\nAudit Log Service\nAuditService The Audit Service provides access to the Service Bridge audit log APIs.\nAll operations performed against TSB resources generate audit log events that can be queried using the Audit log APIs. Those events include information about the users that performed each action and about the actions themselves.\nThis API is integrated with the TSB permission system, and all its methods will only return audit logs …","relpermalink":"/tsb/refs/audit/v1/audit/","summary":"import { PanelContent, PanelContentCode, } from “@theme/Panel”;\nAudit Log Service\nAuditService The Audit Service provides access to the Service Bridge audit log APIs.\nAll operations performed against TSB resources generate audit log events that can be queried using the Audit log APIs. Those events include information about the users that performed each action and about the actions themselves.\nThis API is integrated with the TSB permission system, and all its methods will only return audit logs for those resources the users making the queries have permissions on.\nListAuditLogs rpc ListAuditLogs (tetrateio.api.audit.v1.ListAuditLogsRequest) returns (tetrateio.api.audit.v1.ListAuditLogsResponse)\nList audit logs. If no ‘count’ parameter has been specified, the last 25 audit logs are returned.","title":"Audit"},{"content":" Authentication and authorization configs at gateways, security group level\nAuthentication Field Description Validation Rule jwt\ntetrateio.api.tsb.auth.v2.Authentication.JWT oneof authn Authenticate an HTTP request from a JWT Token attached to it.\n–\nrules\ntetrateio.api.tsb.auth.v2.Authentication.Rules oneof authn List of rules how to authenticate an HTTP request.\n–\nJWT Field Description Validation Rule issuer\nstring REQUIRED Identifies the issuer that issued the JWT. See issuer A JWT with …","relpermalink":"/tsb/refs/tsb/auth/v2/auth/","summary":"Authentication and authorization configs at gateways, security group level\nAuthentication Field Description Validation Rule jwt\ntetrateio.api.tsb.auth.v2.Authentication.JWT oneof authn Authenticate an HTTP request from a JWT Token attached to it.\n–\nrules\ntetrateio.api.tsb.auth.v2.Authentication.Rules oneof authn List of rules how to authenticate an HTTP request.\n–\nJWT Field Description Validation Rule issuer\nstring REQUIRED Identifies the issuer that issued the JWT. See issuer A JWT with different iss claim will be rejected.\nExample: https://foobar.auth0.com Example: 1234567-compute@developer.gserviceaccount.com\nstring = { min_len: 1}\naudiences\nList of string The list of JWT audiences. that are allowed to access. A JWT containing any of these audiences will be accepted.","title":"Auth"},{"content":" Authentication and authorization configs at gateways\n","relpermalink":"/tsb/refs/tsb/gateway/v2/auth/","summary":"Authentication and authorization configs at gateways","title":"Auth"},{"content":" AwsIdentity represents an AWS-specific identity of a workload.\nE.g.,\nAWS EC2 instance identity:\npartition: aws account: \u0026#39;123456789012\u0026#39; region: ca-central-1 zone: ca-central-1b ec2: instance_id: i-1234567890abcdef0 iam_role: name: example-role AWS ECS task identity:\npartition: aws account: \u0026#39;123456789012\u0026#39; region: ca-central-1 zone: ca-central-1b ecs: task_id: 16aeded318d842bb8226e5bc678cd446 cluster: bookinfo iam_role: name: example-role AwsIdentity AwsIdentity represents an AWS-specific identity …","relpermalink":"/tsb/refs/onboarding/config/types/identity/aws/v1alpha1/aws/","summary":"AwsIdentity represents an AWS-specific identity of a workload.\nE.g.,\nAWS EC2 instance identity:\npartition: aws account: '123456789012' region: ca-central-1 zone: ca-central-1b ec2: instance_id: i-1234567890abcdef0 iam_role: name: example-role AWS ECS task identity:\npartition: aws account: '123456789012' region: ca-central-1 zone: ca-central-1b ecs: task_id: 16aeded318d842bb8226e5bc678cd446 cluster: bookinfo iam_role: name: example-role AwsIdentity AwsIdentity represents an AWS-specific identity of a workload.\nField Description Validation Rule partition\nstring REQUIRED AWS Partition.\nE.g., aws, aws-cn, aws-us-gov, etc.\nSee https://docs.aws.amazon.com/general/latest/gr/aws-arns-and-namespaces.html\nstring = { min_len: 1}\naccount\nstring REQUIRED AWS Account.\nE.g., 123456789012.\nSee https://docs.aws.amazon.com/general/latest/gr/acct-identifiers.html\nstring = { pattern: ^[0-9]{12}$}\nregion\nstring REQUIRED AWS Region.\nE.g., us-east-2, eu-west-3, cn-north-1, etc.","title":"AWS Identity"},{"content":" AwsIdentityMatcher specifies matching workloads with AWS-specific identities.\nFor example, the following configuration will match any EC2 VM instance in account 123456789012, region ca-central-1 and zone ca-central-1b:\npartitions: - aws accounts: - \u0026#39;123456789012\u0026#39; regions: - ca-central-1 zones: - ca-central-1b ec2: {} The matcher can also be used to to limit to VMs associated with a specific IAM role as shown below:\npartitions: - aws accounts: - \u0026#39;123456789012\u0026#39; regions: - ca-central-1 zones: - …","relpermalink":"/tsb/refs/onboarding/config/authorization/aws/v1alpha1/aws/","summary":"AwsIdentityMatcher specifies matching workloads with AWS-specific identities.\nFor example, the following configuration will match any EC2 VM instance in account 123456789012, region ca-central-1 and zone ca-central-1b:\npartitions: - aws accounts: - '123456789012' regions: - ca-central-1 zones: - ca-central-1b ec2: {} The matcher can also be used to to limit to VMs associated with a specific IAM role as shown below:\npartitions: - aws accounts: - '123456789012' regions: - ca-central-1 zones: - ca-central-1b ec2: iamRoleNames: - example-role The following matcher will limit to ECS instances in the bookinfo cluster and with a specific IAM role:\npartitions: - aws accounts: - '123456789012' regions: - ca-central-1 zones: - ca-central-1b ecs: clusters: - prod-cluster iamRoleNames: - example-role AwsIdentityMatcher AwsIdentityMatcher specifies matching workloads with AWS-specific identities.","title":"AWS Identity Matcher"},{"content":"This document describes the steps required to create an application in Azure to allow TSB use the cloud account for OIDC and user and group synchronization from Azure AD.\nCreate the Application Log in to the Azure portal and go to the Active Directory \u0026gt; App Registrations \u0026gt; New application registration.\nSet the application type to Web and configure the Redirect URI to point to the TSB address and the /iam/v2/oidc/callback endpoint.\nConfigure Application Secrets Once the application is created, go …","relpermalink":"/tsb/operations/users/oidc-azure/","summary":"This document describes the steps required to create an application in Azure to allow TSB use the cloud account for OIDC and user and group synchronization from Azure AD.\nCreate the Application Log in to the Azure portal and go to the Active Directory \u003e App Registrations \u003e New application registration.\nSet the application type to Web and configure the Redirect URI to point to the TSB address and the /iam/v2/oidc/callback endpoint.\nConfigure Application Secrets Once the application is created, go to the Certificates \u0026 Secrets to create a client secret to be used in TSB:\nConfigure a name and an expiration, and click Add.","title":"Azure AD as the Identity Provider"},{"content":"This document describes how to create a backup, and restore using a backup, when using PostgreSQL as TSB’s datastore. It is recommended to create a backup of your TSB datastore every 24 hours, so in case it gets corrupted, you can easily recover all the information.\nBefore you get started make sure:\n✓ You have installed and configured TSB.\n✓ You have installed and configured kubectl to access the management cluster.\n✓ You have full access to the PostgreSQL system where TSB is storing the data. …","relpermalink":"/tsb/operations/postgresql/","summary":"This document describes how to create a backup, and restore using a backup, when using PostgreSQL as TSB’s datastore. It is recommended to create a backup of your TSB datastore every 24 hours, so in case it gets corrupted, you can easily recover all the information.\nBefore you get started make sure:\n✓ You have installed and configured TSB.\n✓ You have installed and configured kubectl to access the management cluster.\n✓ You have full access to the PostgreSQL system where TSB is storing the data.\nCreate a backup of TSB configuration TSB requires PostgreSQL 11.1 or up. We will be using this 11.","title":"Backup and restore PostgreSQL"},{"content":"This document explains some possible ways to do basic troubleshooting in TSB in order to find misconfiguration issues for a given route or common causes for 50x errors.\nSystem architecture In this document the following system architecture with Tier1-Tier2 setup:\nThere are two different clusters, training-mp which contains the management plane and a control plane configured as tier1, and training-cp which is configured as tier2 and contains bookinfo and httpbin applications.\nTier1 Gateway …","relpermalink":"/tsb/troubleshooting/troubleshooting/","summary":"This document explains some possible ways to do basic troubleshooting in TSB in order to find misconfiguration issues for a given route or common causes for 50x errors.\nSystem architecture In this document the following system architecture with Tier1-Tier2 setup:\nThere are two different clusters, training-mp which contains the management plane and a control plane configured as tier1, and training-cp which is configured as tier2 and contains bookinfo and httpbin applications.\nTier1 Gateway troubleshooting When a 50x error is detected it is important to understand the error message, as it will point us to different sources.\nFor example, suppose you issued an HTTP request using curl to one of the services controlled by TSB, and you observe an error like the following:","title":"Basic troubleshooting"},{"content":"This document describes how to change the password for the TSB administrator.\nThe TSB administrator is configured locally in every TSB instance and does not belong to the corporate Identity Provider (IdP). This allows the superuser to be able to log into TSB in case of issues connecting to the Identity Provider in order to do troubleshooting and platform fixes.\nUpdate the secret Admin credentials are stored in the admin-credentials Kubernetes secret in the Management Plane namespace (tsb by …","relpermalink":"/tsb/operations/users/admin-password/","summary":"This document describes how to change the password for the TSB administrator.\nThe TSB administrator is configured locally in every TSB instance and does not belong to the corporate Identity Provider (IdP). This allows the superuser to be able to log into TSB in case of issues connecting to the Identity Provider in order to do troubleshooting and platform fixes.\nUpdate the secret Admin credentials are stored in the admin-credentials Kubernetes secret in the Management Plane namespace (tsb by default). It is securely stored as a SHA-256 hash so it cannot be reversed, and it can be modified by directly updating the secret with the SHA-256 for the desired password.","title":"Change The Administrator Password"},{"content":"This document explains most common issues when onboarding new control planes into TSB.\nConnectivity The deployment tsb-operator-control-plane needs to have connectivity with the management plane URL. Communication is performed to the front-envoy component in the tsb namespace, which is served by the envoy service.\nMake sure that the control plane can reach it and it’s not blocked by network policies, security groups or any firewall.\nTroubleshooting Once you’ve applied the necessary secrets, …","relpermalink":"/tsb/troubleshooting/cluster-onboarding/","summary":"This document explains most common issues when onboarding new control planes into TSB.\nConnectivity The deployment tsb-operator-control-plane needs to have connectivity with the management plane URL. Communication is performed to the front-envoy component in the tsb namespace, which is served by the envoy service.\nMake sure that the control plane can reach it and it’s not blocked by network policies, security groups or any firewall.\nTroubleshooting Once you’ve applied the necessary secrets, installed the control plane operator and created the control plane CR, if there’s some misconfiguration, some pods won’t be able to start. Always check for tsb-operator-control-plane logs, as it will give more information about what could be wrong.","title":"Cluster onboarding troubleshooting"},{"content":"import { PanelContent, PanelContentCode, } from “@theme/Panel”;\nService to manage clusters onboarded in TSB.\nClusters The Clusters service exposes methods to manage the registration of clusters that are managed by TSB. Before TSB can takeover networking for a given cluster, it must be onboarded in the platform. This onboarding process usually involves two steps:\nCreating the cluster object so the platform knows about it. Generate the agent tokens for the cluster, so the TSB agents installed in …","relpermalink":"/tsb/refs/tsb/v2/cluster-service/","summary":"import { PanelContent, PanelContentCode, } from “@theme/Panel”;\nService to manage clusters onboarded in TSB.\nClusters The Clusters service exposes methods to manage the registration of clusters that are managed by TSB. Before TSB can takeover networking for a given cluster, it must be onboarded in the platform. This onboarding process usually involves two steps:\nCreating the cluster object so the platform knows about it. Generate the agent tokens for the cluster, so the TSB agents installed in the actual cluster can talk to TSB. Once a cluster has been onboarded into TSB, it will start receiving configuration updates from the management plane, and the agents will keep the management updated with the status of the cluster.","title":"Cluster Service"},{"content":" Each Kubernetes cluster managed by Service Bridge should be onboarded first before configurations can be applied to the services in the cluster. Onboarding a cluster is a two step process. First, create a cluster object under the appropriate tenant. Once a cluster object is created, its status field should provide the set of join tokens that will be used by the Service Bridge agent on the cluster to talk to Service Bridge management plane. The second step is to deploy the Service Bridge agent …","relpermalink":"/tsb/refs/tsb/v2/cluster/","summary":"Each Kubernetes cluster managed by Service Bridge should be onboarded first before configurations can be applied to the services in the cluster. Onboarding a cluster is a two step process. First, create a cluster object under the appropriate tenant. Once a cluster object is created, its status field should provide the set of join tokens that will be used by the Service Bridge agent on the cluster to talk to Service Bridge management plane. The second step is to deploy the Service Bridge agent on the cluster with the join tokens and deploy Istio on the cluster. The following example creates a cluster named c1 under the tenant mycompany, indicating that the cluster is deployed on a network “vpc-01” corresponding to the AWS VPC where it resides.","title":"Clusters"},{"content":" Common configuration objects shared by the different install APIs.\nCertManagerSettings CertManagerSettings represents the settings used for the cert-manager installation. TSB supports installing and managing the lifecycle of the cert-manager installation.\nField Description Validation Rule managed\ntetrateio.api.install.common.CertManagerSettings.Managed Managed specifies whether TSB should manage the lifecycle of cert-manager.\n–\ncertManagerSpec …","relpermalink":"/tsb/refs/install/common/common-config/","summary":"Common configuration objects shared by the different install APIs.\nCertManagerSettings CertManagerSettings represents the settings used for the cert-manager installation. TSB supports installing and managing the lifecycle of the cert-manager installation.\nField Description Validation Rule managed\ntetrateio.api.install.common.CertManagerSettings.Managed Managed specifies whether TSB should manage the lifecycle of cert-manager.\n–\ncertManagerSpec\ntetrateio.api.install.common.CertManagerSettings.CertManagerSpec Configure kubernetes specific settings for cert-manager.\n–\ncertManagerWebhookSpec\ntetrateio.api.install.common.CertManagerSettings.CertManagerWebhookSpec Configure kubernetes specific settings for cert-manager-webhook.\n–\ncertManagerCaInjector\ntetrateio.api.install.common.CertManagerSettings.CertManagerCAInjector Configure kubernetes specific settings for cert-manager-cainjector.\n–\ncertManagerStartupapicheck\ntetrateio.api.install.common.CertManagerSettings.CertManagerStartupAPICheck Configure kubernetes specific settings for cert-manager-startupapicheck. DEPRECATED. Startup API Check is disabled.\n–\nCertManagerCAInjector CertManagerCAInjector represents the settings used for cert-manager CAInjector installation in the clusters.","title":"Common Configuration Objects"},{"content":" Definition of objects shared by different APIs.\nConfigGenerationMetadata ConfigGenerationMetadata allows to setup extra metadata that will be added in the final Istio generated configurations. Like new labels or annotations. Defining the config generation metadata in tenancy resources (like organization, tenant, workspace or groups) works as default values for those configs that belong to it. Defining same config generation metadata in configuration resources (like ingress gateways, service …","relpermalink":"/tsb/refs/tsb/types/v2/types/","summary":"Definition of objects shared by different APIs.\nConfigGenerationMetadata ConfigGenerationMetadata allows to setup extra metadata that will be added in the final Istio generated configurations. Like new labels or annotations. Defining the config generation metadata in tenancy resources (like organization, tenant, workspace or groups) works as default values for those configs that belong to it. Defining same config generation metadata in configuration resources (like ingress gateways, service routes, etc.) will replace the ones defined in the tenancy resources.\nField Description Validation Rule labels\nmap\u003cstring, string\u003e Set of key value paris that will be added into the metadata.labels field of the Istio generated configurations.","title":"Common Object Types"},{"content":" Condition contains details for one aspect of the current state of an API Resource.\nCondition Condition contains details for one aspect of the current state of an API Resource.\nField Description Validation Rule type\nstring REQUIRED Type of condition in CamelCase or in foo.example.com/CamelCase.\nstring = { min_len: 1}\nstatus\nstring REQUIRED Status of the condition, one of True, False, Unknown.\nstring = { in: True,False,Unknown}\nreason\nstring REQUIRED Reason contains a programmatic identifier …","relpermalink":"/tsb/refs/onboarding/config/types/core/v1alpha1/condition/","summary":"Condition contains details for one aspect of the current state of an API Resource.\nCondition Condition contains details for one aspect of the current state of an API Resource.\nField Description Validation Rule type\nstring REQUIRED Type of condition in CamelCase or in foo.example.com/CamelCase.\nstring = { min_len: 1}\nstatus\nstring REQUIRED Status of the condition, one of True, False, Unknown.\nstring = { in: True,False,Unknown}\nreason\nstring REQUIRED Reason contains a programmatic identifier indicating the reason for the condition’s last transition. Producers of specific condition types may define expected values and meanings for this field, and whether the values are considered a guaranteed API.","title":"Condition"},{"content":"Config protection is a feature that helps protect your Istio configuration from accidental changes. This allows you to configure the users who are allowed to make changes to the Istio configuration generated by TSB, protecting your Istio configuration from unintended changes. By default, a user with Kube namespace privileges would be able to create new Istio configurations or edit TSB created configurations. While user managed configurations are not altered, TSB managed configurations when …","relpermalink":"/tsb/operations/features/enable-config-protection/","summary":"Config protection is a feature that helps protect your Istio configuration from accidental changes. This allows you to configure the users who are allowed to make changes to the Istio configuration generated by TSB, protecting your Istio configuration from unintended changes. By default, a user with Kube namespace privileges would be able to create new Istio configurations or edit TSB created configurations. While user managed configurations are not altered, TSB managed configurations when changed, would be overwritten by TSB during the next sync cycle.\nThis feature is available in two variants:\nenableAuthorizedUpdateDeleteOnXcpConfigs: This allows the user to create and manage Istio configurations that are not managed by TSB.","title":"Config Protection"},{"content":":::danger Source of Truth We do not recommend the deployment architecture outlined in this document: a single instance of TSB is designed to be deployed across all of your environments (test, qa, staging, and prod). The best practice is to deploy a single TSB centrally and point all of your environments at that single TSB. Tetrate Service Bridge’s built-in controls keep your environment’s configuration isolated and safe. :::\nA few sites have deployed separate TSB instances for each environment. …","relpermalink":"/tsb/operations/configuration-promotion/","summary":":::danger Source of Truth We do not recommend the deployment architecture outlined in this document: a single instance of TSB is designed to be deployed across all of your environments (test, qa, staging, and prod). The best practice is to deploy a single TSB centrally and point all of your environments at that single TSB. Tetrate Service Bridge’s built-in controls keep your environment’s configuration isolated and safe. :::\nA few sites have deployed separate TSB instances for each environment. This guide exists for those sites to ensure they can set up a process to control configuration promotion across environments, outside TSB itself.","title":"Configuration Promotion"},{"content":"Tetrate Service Bridge’s tctl CLI lets you interact with the TSB API to apply objects’s configurations. This document describes how to use tctl to understand what’s the deployment status of a resource configuration within the system.\nResource Status TSB tracks the lifecycle of configuration changes as ResourceStatus. You can fetch them using tctl x status. Run tctl x status --help to see all the possible options.\nThere are different types of resources, depending on how their configuration status …","relpermalink":"/tsb/troubleshooting/configuration-status/","summary":"Tetrate Service Bridge’s tctl CLI lets you interact with the TSB API to apply objects’s configurations. This document describes how to use tctl to understand what’s the deployment status of a resource configuration within the system.\nResource Status TSB tracks the lifecycle of configuration changes as ResourceStatus. You can fetch them using tctl x status. Run tctl x status --help to see all the possible options.\nThere are different types of resources, depending on how their configuration status is computed.\nResource Type Configuration Status Examples Parent Aggregate the status of their children resources. workspace, trafficgroup, gatewaygroup, securitygroup Child Does not depend on other resources.","title":"Configuration status troubleshooting"},{"content":"This feature allows overriding of external addresses of onboarded clusters through IngressGateway or Tier1Gateway install CR. The provided IP addresses/hostnames will then be used to access the clusters from the outside world. Note that this feature is useful only when you have some other IP address/hostname already configured to access your kubernetes cluster from the outside world.\nData Plane To use this feature with IngressGateway, set the xcp.tetrate.io/cluster-external-addresses annotation …","relpermalink":"/tsb/operations/features/configure-cluster-external-addresses/","summary":"This feature allows overriding of external addresses of onboarded clusters through IngressGateway or Tier1Gateway install CR. The provided IP addresses/hostnames will then be used to access the clusters from the outside world. Note that this feature is useful only when you have some other IP address/hostname already configured to access your kubernetes cluster from the outside world.\nData Plane To use this feature with IngressGateway, set the xcp.tetrate.io/cluster-external-addresses annotation under kubeSpec/service in your IngressGateway install (DataPlane) CR and apply it with kubectl. You can use:\nSingle IP address Single DNS name Multiple IP addresses (comma separated) But you can’t configure multiple DNS names or combine an IP address with a DNS name.","title":"Configure cluster external addresses"},{"content":"This document describes how to adjust log levels for the different components in TSB, including platform components, Envoy sidecars and ingress gateways at runtime, as well as the procedure to view the logs.\nBefore you get started make sure:\n✓ You have installed and configured TSB properly.\n✓ You have installed and configured kubectl to access the application cluster.\nFor the example commands we assume that you have some applications deployed in a helloworld namespace.\n:::warning TSB Components …","relpermalink":"/tsb/operations/configure-log-levels/","summary":"This document describes how to adjust log levels for the different components in TSB, including platform components, Envoy sidecars and ingress gateways at runtime, as well as the procedure to view the logs.\nBefore you get started make sure:\n✓ You have installed and configured TSB properly.\n✓ You have installed and configured kubectl to access the application cluster.\nFor the example commands we assume that you have some applications deployed in a helloworld namespace.\n:::warning TSB Components Produce Lots of Logs Be careful enabling debug logging across all of TSB’s scopes for extended periods of time - TSB components produces a lot of logs!","title":"Configure Log Levels"},{"content":"This document describes how to configure the Management Plane IAM service to have multiple keys to validate JWT tokens. This can be useful when rotating the IAM signing key while still allowing access for tokens issued with the old key that has not yet expired.\nThe following example illustrates the process of migrating the main IAM signing key from using the key from the tsb-certs certificate to use a custom signing key.\nFetching the current signing key First of all you need to retrieve the …","relpermalink":"/tsb/operations/multiple-iam-keys/","summary":"This document describes how to configure the Management Plane IAM service to have multiple keys to validate JWT tokens. This can be useful when rotating the IAM signing key while still allowing access for tokens issued with the old key that has not yet expired.\nThe following example illustrates the process of migrating the main IAM signing key from using the key from the tsb-certs certificate to use a custom signing key.\nFetching the current signing key First of all you need to retrieve the configuration for the token issuer with:\nkubectl -n tsb get managementplane managementplane And find the token issuer configuration.","title":"Configure multiple IAM token validation keys"},{"content":"Launch an AWS EC2 Instance Launch an AWS EC2 instance with the following configuration:\nChoose 64-bit (x86) AMI image with Ubuntu Server (DEB) Choose a minimal Instance Type, e.g. t2.micro (1x vCPU, 1 GiB RAM) or t2.nano (1x vCPU, 0.5 GiB RAM) Choose the default VPC (for your instance to have public IP) Set Auto-assign Public IP to Enable Configure SecurityGroup to allow incoming traffic to port 9080 from 0.0.0.0/0 For the purposes of this guide, you will be creating an EC2 instance with a …","relpermalink":"/tsb/setup/workload-onboarding/quickstart/aws-ec2/configure-vm/","summary":"Launch an AWS EC2 Instance Launch an AWS EC2 instance with the following configuration:\nChoose 64-bit (x86) AMI image with Ubuntu Server (DEB) Choose a minimal Instance Type, e.g. t2.micro (1x vCPU, 1 GiB RAM) or t2.nano (1x vCPU, 0.5 GiB RAM) Choose the default VPC (for your instance to have public IP) Set Auto-assign Public IP to Enable Configure SecurityGroup to allow incoming traffic to port 9080 from 0.0.0.0/0 For the purposes of this guide, you will be creating an EC2 instance with a public IP for ease of configuration.\n:::warning This is NOT recommended for production scenarios. For production scenarios, you should do the opposite and place the Kubernetes cluster and the EC2 instances on the same network, or peered networks, and not give your VMs public IPs.","title":"Configure the VM"},{"content":"Install Bookinfo Ratings Application SSH into the VM on-premise and install the ratings application. Execute the following commands:\n# Install the latest version of trusted CA certificates sudo apt-get update -y sudo apt-get install -y ca-certificates # Add DEB repository with Node.js curl --fail --silent --location https://deb.nodesource.com/setup_14.x | sudo bash - # Install Node.js sudo apt-get install -y nodejs # Download DEB package of the Bookinfo Ratings app curl -fLO …","relpermalink":"/tsb/setup/workload-onboarding/quickstart/on-premise/configure-vm/","summary":"Install Bookinfo Ratings Application SSH into the VM on-premise and install the ratings application. Execute the following commands:\n# Install the latest version of trusted CA certificates sudo apt-get update -y sudo apt-get install -y ca-certificates # Add DEB repository with Node.js curl --fail --silent --location https://deb.nodesource.com/setup_14.x | sudo bash - # Install Node.js sudo apt-get install -y nodejs # Download DEB package of the Bookinfo Ratings app curl -fLO https://dl.cloudsmith.io/public/tetrate/onboarding-examples/raw/files/bookinfo-ratings.deb # Install DEB package sudo apt-get install -y ./bookinfo-ratings.deb # Remove downloaded file rm bookinfo-ratings.deb # Enable SystemD Unit sudo systemctl enable bookinfo-ratings # Start Bookinfo Ratings app sudo systemctl start bookinfo-ratings Verify the ratings Application Execute the following command to verify that the ratings application can now serve local requests:","title":"Configure the VM on-premise"},{"content":"You will deploy the ratings application on an AWS EC2 instance and onboard it into the service mesh.\nCreate a WorkloadGroup Execute the following command to create a WorkloadGroup:\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: networking.istio.io/v1alpha3 kind: WorkloadGroup metadata: name: ratings namespace: bookinfo labels: app: ratings spec: template: labels: app: ratings class: vm cloud: aws network: aws # (1) serviceAccount: bookinfo-ratings # (2) EOF The field spec.template.network is set to …","relpermalink":"/tsb/setup/workload-onboarding/quickstart/aws-ec2/configure-workload-onboarding/","summary":"You will deploy the ratings application on an AWS EC2 instance and onboard it into the service mesh.\nCreate a WorkloadGroup Execute the following command to create a WorkloadGroup:\ncat \u003c\u003cEOF | kubectl apply -f - apiVersion: networking.istio.io/v1alpha3 kind: WorkloadGroup metadata: name: ratings namespace: bookinfo labels: app: ratings spec: template: labels: app: ratings class: vm cloud: aws network: aws # (1) serviceAccount: bookinfo-ratings # (2) EOF The field spec.template.network is set to a non-empty value to indicate to the Istio control plane that the VM you will create later has no direct connectivity to the Kubernetes Pods.\nThe field spec.template.serviceAccount declares that the workload have the identity of the service account bookinfo-ratings within the Kubernetes cluster.","title":"Configure WorkloadGroup and Sidecar"},{"content":"You will deploy the ratings application as an AWS ECS task and onboard it into the service mesh.\nCreate a WorkloadGroup Execute the following command to create a WorkloadGroup:\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: networking.istio.io/v1alpha3 kind: WorkloadGroup metadata: name: ratings namespace: bookinfo labels: app: ratings spec: template: labels: app: ratings class: ecs cloud: aws serviceAccount: bookinfo-ratings EOF The field spec.template.serviceAccount declares that the workload have …","relpermalink":"/tsb/setup/workload-onboarding/quickstart/aws-ecs/configure-workload-onboarding/","summary":"You will deploy the ratings application as an AWS ECS task and onboard it into the service mesh.\nCreate a WorkloadGroup Execute the following command to create a WorkloadGroup:\ncat \u003c\u003cEOF | kubectl apply -f - apiVersion: networking.istio.io/v1alpha3 kind: WorkloadGroup metadata: name: ratings namespace: bookinfo labels: app: ratings spec: template: labels: app: ratings class: ecs cloud: aws serviceAccount: bookinfo-ratings EOF The field spec.template.serviceAccount declares that the workload have the identity of the service account bookinfo-ratings within the Kubernetes cluster. The service account bookinfo-ratings was created during the deployment of the Istio bookinfo example earlier\nCreate the Sidecar configuration Execute the following command to create a new sidecar configuration:","title":"Configure WorkloadGroup and Sidecar for the AWS ECS workloads"},{"content":"You will deploy the ratings application on a VM on-premise and onboard it into the service mesh.\nCreate a WorkloadGroup Execute the following command to create a WorkloadGroup:\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: networking.istio.io/v1alpha3 kind: WorkloadGroup metadata: name: ratings namespace: bookinfo labels: app: ratings spec: template: labels: app: ratings class: vm serviceAccount: bookinfo-ratings EOF The field spec.template.network is omitted to indicate to the Istio control plane …","relpermalink":"/tsb/setup/workload-onboarding/quickstart/on-premise/configure-workload-onboarding/","summary":"You will deploy the ratings application on a VM on-premise and onboard it into the service mesh.\nCreate a WorkloadGroup Execute the following command to create a WorkloadGroup:\ncat \u003c\u003cEOF | kubectl apply -f - apiVersion: networking.istio.io/v1alpha3 kind: WorkloadGroup metadata: name: ratings namespace: bookinfo labels: app: ratings spec: template: labels: app: ratings class: vm serviceAccount: bookinfo-ratings EOF The field spec.template.network is omitted to indicate to the Istio control plane that the VM on-premise has direct connectivity to the Kubernetes Pods.\nThe field spec.template.serviceAccount declares that the workload have the identity of the service account bookinfo-ratings within the Kubernetes cluster. The service account bookinfo-ratings was created during the deployment of the Istio bookinfo example earlier","title":"Configure WorkloadGroup and Sidecar for the Workload on-premise"},{"content":" ControlPlane resource exposes a set of configurations necessary to automatically install the Service Bridge control plane on a cluster. The installation API is an override API so any unset fields that aren’t required will use sensible defaults.\nPrior to creating the ControlPlane resource, a cluster needs to be created in the management plane. Control plane install scripts would create the following secrets in the Kubernetes namespace the control plane is deployed into. Make sure they exist: …","relpermalink":"/tsb/refs/install/controlplane/v1alpha1/spec/","summary":"ControlPlane resource exposes a set of configurations necessary to automatically install the Service Bridge control plane on a cluster. The installation API is an override API so any unset fields that aren’t required will use sensible defaults.\nPrior to creating the ControlPlane resource, a cluster needs to be created in the management plane. Control plane install scripts would create the following secrets in the Kubernetes namespace the control plane is deployed into. Make sure they exist:\noap-token otel-token If your Elasticsearch backend requires authentication, ensure you create the following secret:\nelastic-credentials A minimal resource must have the container registry hub, telemetryStore, and managementPlane fields set.","title":"Control Plane"},{"content":" Core types.\nNamespacedName NamespacedName specifies a namespace-scoped name.\nField Description Validation Rule namespace\nstring REQUIRED Namespace name.\nstring = { min_len: 1}\nname\nstring REQUIRED Resource name.\nstring = { min_len: 1}\n","relpermalink":"/tsb/refs/onboarding/config/types/core/v1alpha1/namespaced-name/","summary":"Core types.\nNamespacedName NamespacedName specifies a namespace-scoped name.\nField Description Validation Rule namespace\nstring REQUIRED Namespace name.\nstring = { min_len: 1}\nname\nstring REQUIRED Resource name.\nstring = { min_len: 1}","title":"Core types"},{"content":"This document describes how to customize the Kubernetes deployments for TSB components, including using overlays to perform advanced configuration of resources that are deployed by the Tetrate Service Bridge (TSB) operators, using examples.\nBackground TSB makes extensive use of the Operator pattern to deploy and configure the necessary parts in Kubernetes.\nNormally customization and fine tuning of the parameters are done through the operator, which is responsible for creating the necessary …","relpermalink":"/tsb/operations/kube-customization/","summary":"This document describes how to customize the Kubernetes deployments for TSB components, including using overlays to perform advanced configuration of resources that are deployed by the Tetrate Service Bridge (TSB) operators, using examples.\nBackground TSB makes extensive use of the Operator pattern to deploy and configure the necessary parts in Kubernetes.\nNormally customization and fine tuning of the parameters are done through the operator, which is responsible for creating the necessary resources and controlling their lifecycles.\nFor example, when you create an IngressGateway CR, a TSB operator picks up this information and deploys and/or updates the relevant resources, such as Kubernetes Service objects, by creating a manifest and applying them.","title":"Customizing TSB Kubernetes Components"},{"content":" A minimal resource should have an empty spec.\napiVersion: install.tetrate.io/v1alpha1 kind: IngressGateway metadata: name: bookinfo namespace: bookinfo spec: {} To configure infrastructure specific settings such as the service type, set the relevant field in kubeSpec. Remember that the installation API is an override API so if these fields are unset the operator will use sensible defaults. Only a subset of Kubernetes configuration is available.\napiVersion: install.tetrate.io/v1alpha1 kind: …","relpermalink":"/tsb/refs/install/dataplane/v1alpha1/spec/","summary":"A minimal resource should have an empty spec.\napiVersion: install.tetrate.io/v1alpha1 kind: IngressGateway metadata: name: bookinfo namespace: bookinfo spec: {} To configure infrastructure specific settings such as the service type, set the relevant field in kubeSpec. Remember that the installation API is an override API so if these fields are unset the operator will use sensible defaults. Only a subset of Kubernetes configuration is available.\napiVersion: install.tetrate.io/v1alpha1 kind: IngressGateway metadata: name: bookinfo namespace: bookinfo spec: kubeSpec: service: type: NodePort EgressGateway and Tier1Gateway are configured in the same manner.\nEgressGatewaySpec EgressGatewaySpec defines the desired installed state of a single egress gateway for a given namespace in Service Bridge.","title":"Data Plane"},{"content":"If you use DNS hostname when configuring cluster-external-addresses annotation for EastWest gateway, you need to enable DNS resolution at XCP edge so that DNS resolution will happen at XCP Edge.\nEnable DNS resolution at XCP edge To enable DNS resolution at XCP edge, you will need to edit xcp component in ControlPlane CR or Helm values and add an environment variable ENABLE_DNS_RESOLUTION_AT_EDGE with value true:\nspec: components: xcp: ... kubeSpec: overlays: - apiVersion: …","relpermalink":"/tsb/operations/features/edge-dns-resolution/","summary":"If you use DNS hostname when configuring cluster-external-addresses annotation for EastWest gateway, you need to enable DNS resolution at XCP edge so that DNS resolution will happen at XCP Edge.\nEnable DNS resolution at XCP edge To enable DNS resolution at XCP edge, you will need to edit xcp component in ControlPlane CR or Helm values and add an environment variable ENABLE_DNS_RESOLUTION_AT_EDGE with value true:\nspec: components: xcp: ... kubeSpec: overlays: - apiVersion: install.xcp.tetrate.io/v1alpha1 kind: EdgeXcp name: edge-xcp patches: ... - path: spec.components.edgeServer.kubeSpec.deployment.env[-1] value: name: ENABLE_DNS_RESOLUTION_AT_EDGE value: \"true\" ... Refer to Multi-cluster traffic failover with EastWest Gateways for how to enable EastWest routing .","title":"DNS Resolution at Edge"},{"content":" Configuration for east/west gateway settings\nEastWestGateway EastWestGateway is for configuring a gateway to handle east-west traffic of the services that are not exposed through Ingress or Tier1 gateways (internal services). Currently, this is restricted to specifying at Workspace level in WorkspaceSetting.\nField Description Validation Rule workloadSelector\ntetrateio.api.tsb.types.v2.WorkloadSelector REQUIRED Specify the gateway workloads (pod labels and Kubernetes namespace) under the gateway …","relpermalink":"/tsb/refs/tsb/gateway/v2/eastwest-gateway/","summary":"Configuration for east/west gateway settings\nEastWestGateway EastWestGateway is for configuring a gateway to handle east-west traffic of the services that are not exposed through Ingress or Tier1 gateways (internal services). Currently, this is restricted to specifying at Workspace level in WorkspaceSetting.\nField Description Validation Rule workloadSelector\ntetrateio.api.tsb.types.v2.WorkloadSelector REQUIRED Specify the gateway workloads (pod labels and Kubernetes namespace) under the gateway group that should be configured with this gateway. There can be only one gateway for a workload selector in a namespace.\nmessage = { required: true}\nexposedServices\nList of tetrateio.api.tsb.types.v2.ServiceSelector Exposed services is used to specify the match criteria to select specific services for internal multicluster routing (east-west routing between clusters).","title":"East/West Gateway"},{"content":" EgressGateway configures a workload to act as a gateway for traffic exiting the mesh. The egress gateway is meant to be the destination of unknown traffic within the mesh (traffic sent to non-mesh services). The gateway allows authorization control of traffic sent to it to more finely tune which services are allowed to send unknown traffic through the gateway. Only HTTP is supported at this time.\nThe following example declares an egress gateway running on pods in istio-system with the label …","relpermalink":"/tsb/refs/tsb/gateway/v2/egress-gateway/","summary":"EgressGateway configures a workload to act as a gateway for traffic exiting the mesh. The egress gateway is meant to be the destination of unknown traffic within the mesh (traffic sent to non-mesh services). The gateway allows authorization control of traffic sent to it to more finely tune which services are allowed to send unknown traffic through the gateway. Only HTTP is supported at this time.\nThe following example declares an egress gateway running on pods in istio-system with the label app=istio-egressgateway. This gateway is setup to allow traffic from anywhere in the cluster to access www.httpbin.org and from the bookinfo details app specifically, you can access any external host.","title":"Egress Gateway"},{"content":"Before you get started, you must have:\n✓ Vault 1.3.1 or newer\n✓ Vault Injector 0.3.0 or newer\n✓ Elasticsearch 6.x or 7.x with basic license or up\nSetup Vault Install Vault (it does not need to be installed in the Kubernetes cluster, but should be reachable from inside the Kubernetes cluster). The Vault Injector (agent-injector) must be installed into the cluster and configured to inject sidecars. This is automatically done by the Helm chart v0.5.0+ which installs Vault 0.12+ and …","relpermalink":"/tsb/operations/vault/elasticsearch/","summary":"Before you get started, you must have:\n✓ Vault 1.3.1 or newer\n✓ Vault Injector 0.3.0 or newer\n✓ Elasticsearch 6.x or 7.x with basic license or up\nSetup Vault Install Vault (it does not need to be installed in the Kubernetes cluster, but should be reachable from inside the Kubernetes cluster). The Vault Injector (agent-injector) must be installed into the cluster and configured to inject sidecars. This is automatically done by the Helm chart v0.5.0+ which installs Vault 0.12+ and Vault-Injector0.3.0+. The example below assumes that Vault is installed in the tsb namespace.\nFor more details, check the Vault documentation.","title":"Elasticsearch Credentials"},{"content":"If your Elasticsearch access is restricted by roles, you will need to make sure the right roles exist for TSB components.\nOAP For OAP, the necessary role permissions are described in the JSON below.\n{ \u0026#34;cluster\u0026#34;: [\u0026#34;manage_index_templates\u0026#34;, \u0026#34;monitor\u0026#34;], \u0026#34;indices\u0026#34;: [ { \u0026#34;names\u0026#34;: [\u0026#34;skywalking_*\u0026#34;], \u0026#34;privileges\u0026#34;: [\u0026#34;manage\u0026#34;, \u0026#34;read\u0026#34;, \u0026#34;write\u0026#34;], \u0026#34;allow_restricted_indices\u0026#34;: false } ], \u0026#34;applications\u0026#34;: [], \u0026#34;run_as\u0026#34;: [], \u0026#34;metadata\u0026#34;: {}, \u0026#34;transient_metadata\u0026#34;: { \u0026#34;enabled\u0026#34;: true } } You can use cURL, Kibana …","relpermalink":"/tsb/operations/elasticsearch/elasticsearch-role/","summary":"If your Elasticsearch access is restricted by roles, you will need to make sure the right roles exist for TSB components.\nOAP For OAP, the necessary role permissions are described in the JSON below.\n{ \"cluster\": [\"manage_index_templates\", \"monitor\"], \"indices\": [ { \"names\": [\"skywalking_*\"], \"privileges\": [\"manage\", \"read\", \"write\"], \"allow_restricted_indices\": false } ], \"applications\": [], \"run_as\": [], \"metadata\": {}, \"transient_metadata\": { \"enabled\": true } } You can use cURL, Kibana console or any other tool to post this to the Elasticsearch server to create the role, then you can assign the role to the user OAP will be using.","title":"Elasticsearch privileges"},{"content":"In some situations, due to data model changes in Elasticsearch indexes, it is required that you wipe existing indexes and templates so the new version of OAP can function properly.\nThe procedure below describes how to wipe such data from Elasticsearch and ensure that the OAP component will start up correctly.\n:::warning Scale down replicas Make sure to follow steps 1 and 2 before proceeding :::\n1. Scale down to 0 replicas the oap-deployment deployment in the control plane namespace.\n:::warning …","relpermalink":"/tsb/operations/elasticsearch/wipe-elastic/","summary":"In some situations, due to data model changes in Elasticsearch indexes, it is required that you wipe existing indexes and templates so the new version of OAP can function properly.\nThe procedure below describes how to wipe such data from Elasticsearch and ensure that the OAP component will start up correctly.\n:::warning Scale down replicas Make sure to follow steps 1 and 2 before proceeding :::\n1. Scale down to 0 replicas the oap-deployment deployment in the control plane namespace.\n:::warning This needs to be done in all CP clusters onboarded in TSB. :::\nkubectl -n ${CONTROL_NAMESPACE} scale deployment oap-deployment --replicas=0 2.","title":"Elasticsearch wipe procedure"},{"content":"In order to enable Workload Onboarding you need the following pieces of information:\nThe DNS name to assign the Workload Onboarding Endpoint TLS certificate for that DNS name For this example you will be using the DNS name onboarding-endpoint.example, as we do not expect you to use a routable DNS name.\nPrepare the Certificates For production purposes you will need to use a TLS certificate signed by a trust Certificate Authority (CA), such as Let’s Encrypt or an internal CA such as Vault.\nIn this …","relpermalink":"/tsb/setup/workload-onboarding/quickstart/aws-ec2/enable-workload-onboarding/","summary":"In order to enable Workload Onboarding you need the following pieces of information:\nThe DNS name to assign the Workload Onboarding Endpoint TLS certificate for that DNS name For this example you will be using the DNS name onboarding-endpoint.example, as we do not expect you to use a routable DNS name.\nPrepare the Certificates For production purposes you will need to use a TLS certificate signed by a trust Certificate Authority (CA), such as Let’s Encrypt or an internal CA such as Vault.\nIn this example you will setup an example CA which will be used throughout the rest of this guide.","title":"Enable Workload Onboarding"},{"content":" The Gateway configuration combines the functionalities of both the existing Tier1Gateway and IngressGateway, providing a unified approach for configuring a workload as a gateway in the mesh. Each server within the Gateway is configured to route requests to either destination clusters or services, but configuring one server to route requests to a destination cluster and another server to route requests to a service is not supported. To ensure consistency and compatibility, the Gateway …","relpermalink":"/tsb/refs/tsb/gateway/v2/gateway/","summary":"The Gateway configuration combines the functionalities of both the existing Tier1Gateway and IngressGateway, providing a unified approach for configuring a workload as a gateway in the mesh. Each server within the Gateway is configured to route requests to either destination clusters or services, but configuring one server to route requests to a destination cluster and another server to route requests to a service is not supported. To ensure consistency and compatibility, the Gateway configuration requires that all servers within the gateway either forward traffic to other clusters, similar to a Tier1Gateway, or route traffic to specific services, similar to an IngressGateway.","title":"Gateway"},{"content":" DEPRECATED: use Access Bindings instead.\nGatewayAccessBindings is an assignment of roles to a set of users or teams to access resources under a Gateway group. The user or team information is obtained from an LDAP server that should have been configured as part of Service Bridge installation. Note that a GatewayAccessBinding can be created or modified only by users who have SET_POLICY permission on the Gateway group.\nThe following example assigns the gateway-admin role to users alice, bob, and …","relpermalink":"/tsb/refs/tsb/rbac/v2/gateway-access-bindings/","summary":"DEPRECATED: use Access Bindings instead.\nGatewayAccessBindings is an assignment of roles to a set of users or teams to access resources under a Gateway group. The user or team information is obtained from an LDAP server that should have been configured as part of Service Bridge installation. Note that a GatewayAccessBinding can be created or modified only by users who have SET_POLICY permission on the Gateway group.\nThe following example assigns the gateway-admin role to users alice, bob, and members of the gateway-ops team for all the gateways in group g1 under workspace w1 owned by the tenant mycompany. Use fully-qualified name (fqn) when specifying user and team","title":"Gateway Access Bindings"},{"content":" Configurations used to build gateways.\nClusterDestination Field Description Validation Rule name\nstring The name of the destination cluster. Only one of name or labels must be specified.\n–\nlabels\nmap\u0026lt;string, string\u0026gt; Labels associated with the cluster. Any cluster with matching labels will be selected as a target. Only one of name or labels must be specified.\n–\nnetwork\nstring The network associated with the destination clusters. In addition to name/label selectors, only clusters matching the …","relpermalink":"/tsb/refs/tsb/gateway/v2/gateway-common/","summary":"Configurations used to build gateways.\nClusterDestination Field Description Validation Rule name\nstring The name of the destination cluster. Only one of name or labels must be specified.\n–\nlabels\nmap\u003cstring, string\u003e Labels associated with the cluster. Any cluster with matching labels will be selected as a target. Only one of name or labels must be specified.\n–\nnetwork\nstring The network associated with the destination clusters. In addition to name/label selectors, only clusters matching the selected networks will be used as a target. At least one of name/labels, and/or network must be specified.\nDeprecated: The network field is deprecated and will be removed in future releases.","title":"Gateway Common Configuration Messages"},{"content":"If a gateway is deleted (e.g. during a downscaling event), remote clusters would continue to attempt to send traffic to the gateway IP address until they received an update that the gateway’s IP address was removed. This may cause 503 errors for HTTP traffic or 000 errors for passthrough cross cluster traffic.\nSince TSB 1.6, you can delay gateway deletions by a configurable period to provide sufficient time for the gateway’s IP address removal to propagate across other clusters to avoid 503 or …","relpermalink":"/tsb/operations/features/gateway-deletion-webhook/","summary":"If a gateway is deleted (e.g. during a downscaling event), remote clusters would continue to attempt to send traffic to the gateway IP address until they received an update that the gateway’s IP address was removed. This may cause 503 errors for HTTP traffic or 000 errors for passthrough cross cluster traffic.\nSince TSB 1.6, you can delay gateway deletions by a configurable period to provide sufficient time for the gateway’s IP address removal to propagate across other clusters to avoid 503 or 000 errors. Currently this feature is disabled by default.\nEnable Gateway Deletion Hold Webhook In order to enable a gateway deletion hold webhook in your control plane, you will need to edit xcp component in ControlPlane CR or Helm values and add the following environment variables:","title":"Gateway Deletion Hold Webhook"},{"content":" Gateway Groups allow grouping the gateways in a set of namespaces owned by its parent workspace. Gateway related configurations can then be applied on the group to control the behavior of these gateways. The group can be in one of two modes: BRIDGED and DIRECT. BRIDGED mode is a minimalistic mode that allows users to quickly configure the most commonly used features in the service mesh using Tetrate specific APIs, while the DIRECT mode provides more flexibility for power users by allowing them …","relpermalink":"/tsb/refs/tsb/gateway/v2/gateway-group/","summary":"Gateway Groups allow grouping the gateways in a set of namespaces owned by its parent workspace. Gateway related configurations can then be applied on the group to control the behavior of these gateways. The group can be in one of two modes: BRIDGED and DIRECT. BRIDGED mode is a minimalistic mode that allows users to quickly configure the most commonly used features in the service mesh using Tetrate specific APIs, while the DIRECT mode provides more flexibility for power users by allowing them to configure the gateways’s traffic and security properties using a restricted subset of Istio Networking and Security APIs.","title":"Gateway Group"},{"content":"import { PanelContent, PanelContentCode, } from “@theme/Panel”;\nService to manage the configuration for Gateways.\nGateways The Gateway service provides methods to manage gateway settings in TSB.\nIt provides methods to create and manage gateway groups within a workspace, allowing to create fine-grained groupings to configure a subset of the workspace namespaces. Access policies can be assigned at group level, providing a fine-grained access control to the gateway configuration features.\nThe …","relpermalink":"/tsb/refs/tsb/gateway/v2/gateway-service/","summary":"import { PanelContent, PanelContentCode, } from “@theme/Panel”;\nService to manage the configuration for Gateways.\nGateways The Gateway service provides methods to manage gateway settings in TSB.\nIt provides methods to create and manage gateway groups within a workspace, allowing to create fine-grained groupings to configure a subset of the workspace namespaces. Access policies can be assigned at group level, providing a fine-grained access control to the gateway configuration features.\nThe Gateway service also provides methods to configure the different gateway settings that are allowed within each group.\nCreateGroup rpc CreateGroup (tetrateio.api.tsb.gateway.v2.CreateGatewayGroupRequest) returns (tetrateio.api.tsb.gateway.v2.Group)\nRequires CreateGatewayGroup\nCreate a new gateway group in the given workspace.","title":"Gateway Service"},{"content":"Tetrate Service Bridge’s command line interface (CLI) lets you interact with the TSB API allowing you for easy manipulation of objects and configurations in a programmatic, or interactive way. CLI works by submitting YAML representation of TSB or Istio objects.\nInstallation TSB CLI is a single binary which is available for Linux, MacOS and Windows.\n\u0026lt;Tabs defaultValue=“Linux” values={[ {label: ‘Linux’, value: ‘Linux’}, {label: ‘MacOS’, value: ‘MacOS’}, {label: ‘Windows’, value: ‘Windows’}, ]}\u0026gt; …","relpermalink":"/tsb/reference/cli/","summary":"Tetrate Service Bridge’s command line interface (CLI) lets you interact with the TSB API allowing you for easy manipulation of objects and configurations in a programmatic, or interactive way. CLI works by submitting YAML representation of TSB or Istio objects.\nInstallation TSB CLI is a single binary which is available for Linux, MacOS and Windows.\n\u003cTabs defaultValue=“Linux” values={[ {label: ‘Linux’, value: ‘Linux’}, {label: ‘MacOS’, value: ‘MacOS’}, {label: ‘Windows’, value: ‘Windows’}, ]}\u003e Use curl or wget to download the binary, grant permissions to execute and place it somewhere in your $PATH.\n{`mkdir -p ~/.tctl/bin curl -Lo ~/.tctl/bin/tctl https://binaries.dl.tetrate.io/public/raw/versions/linux-$(uname -m | sed s/x86_64/amd64/)-${vars.","title":"Getting Started"},{"content":"This document describes how to configure the GitOps integration for Tetrate Service Bridge (TSB). GitOps integration in TSB allows you to integrate with the lifecycle of application packaging and deployment and the different Continuous Deployment (CD) Systems.\nThis document assumes that you already have working knowledge of configuring GitOps CD systems, such as FluxCD or ArgoCD.\nHow it works Once enabled in a Management Plane cluster and/or an Application cluster, the CD System will be able to …","relpermalink":"/tsb/operations/features/configure-gitops/","summary":"This document describes how to configure the GitOps integration for Tetrate Service Bridge (TSB). GitOps integration in TSB allows you to integrate with the lifecycle of application packaging and deployment and the different Continuous Deployment (CD) Systems.\nThis document assumes that you already have working knowledge of configuring GitOps CD systems, such as FluxCD or ArgoCD.\nHow it works Once enabled in a Management Plane cluster and/or an Application cluster, the CD System will be able to apply the TSB configurations in it, which then will be pushed to the TSB Management Plane.\nEnabling GitOps The GitOps component can be configured through ManagementPlane or ControlPlane CR or Helm values for each cluster.","title":"GitOps"},{"content":"This document explains what happens when a pod which has istio-proxy sidecar enabled is deleted, particularly how the connections are treated, and how smooth you can configure the sidecar to drain the inflight connections gracefully.\n:::note This document only applies to TSB version \u0026lt;= 1.4.x. :::\nBefore you get started, make sure you: ✓ Familiarize yourself with TSB concepts ✓ Install the TSB environment. You can use TSB demo for quick install ✓ Completed TSB usage quickstart. This document …","relpermalink":"/tsb/operations/graceful-connection-drain/","summary":"This document explains what happens when a pod which has istio-proxy sidecar enabled is deleted, particularly how the connections are treated, and how smooth you can configure the sidecar to drain the inflight connections gracefully.\n:::note This document only applies to TSB version \u003c= 1.4.x. :::\nBefore you get started, make sure you: ✓ Familiarize yourself with TSB concepts ✓ Install the TSB environment. You can use TSB demo for quick install ✓ Completed TSB usage quickstart. This document assumes you already created Tenant and are familiar with Workspace and Config Groups. Also you need to configure tctl to your TSB environment ✓ Install httpbin","title":"Graceful Connection Drain of istio-proxy"},{"content":":::note Protobuf files To communicate with TSB over a gRPC connection you will need the protobuf and gRPC service definition files. Please contact your Tetrate account manager if you wish to acquire them. :::\nIn this guide you’ll see how you can use the TSB gRPC API to perform common operations on the platform. The examples in this guide use the Go bindings, as those are the ones we provide on request by default, but gRPC clients generated for other languages will work as well.\nInitializing the …","relpermalink":"/tsb/reference/grpc-api/guide/","summary":":::note Protobuf files To communicate with TSB over a gRPC connection you will need the protobuf and gRPC service definition files. Please contact your Tetrate account manager if you wish to acquire them. :::\nIn this guide you’ll see how you can use the TSB gRPC API to perform common operations on the platform. The examples in this guide use the Go bindings, as those are the ones we provide on request by default, but gRPC clients generated for other languages will work as well.\nInitializing the gRPC client Transport configuration The first thing to configure when creating the gRPC client is the connection to TSB.","title":"gRPC API Guide"},{"content":" Host Info specifies information about the host the workload is running on.\nAddress Address specifies network address.\nField Description Validation Rule ip\nstring REQUIRED IP address.\nstring = { ip: true}\ntype\ntetrateio.api.onboarding.config.types.registration.v1alpha1.AddressType REQUIRED Address type.\nenum = { not_in: 0}\nHostInfo HostInfo specifies information about the host the workload is running on.\nField Description Validation Rule addresses\nList of …","relpermalink":"/tsb/refs/onboarding/config/types/registration/v1alpha1/hostinfo/","summary":"Host Info specifies information about the host the workload is running on.\nAddress Address specifies network address.\nField Description Validation Rule ip\nstring REQUIRED IP address.\nstring = { ip: true}\ntype\ntetrateio.api.onboarding.config.types.registration.v1alpha1.AddressType REQUIRED Address type.\nenum = { not_in: 0}\nHostInfo HostInfo specifies information about the host the workload is running on.\nField Description Validation Rule addresses\nList of tetrateio.api.onboarding.config.types.registration.v1alpha1.Address REQUIRED Network addresses of the host the workload is running on.\nrepeated = { min_items: 1 items: {message:{required:true}}}\nAddressType AddressType specifies type of a network address associated with the workload.\nField Number Description UNSPECIFIED\n0\nNot specified.\nVPC","title":"Host Info"},{"content":"import { PanelContent, PanelContentCode, } from “@theme/Panel”;\nIAM APIs for authentication.\nOAuth Token rpc Token (tetrateio.api.iam.v2.GrantRequest) returns (tetrateio.api.iam.v2.GrantResponse)\nGrants tokens for a given grant type.\nThis is used by clients to obtain an access token by presenting required parameters for the requested grant type. Current only “urn:ietf:params:oauth:grant-type:device_code” is supported. When an error occurs, this will return a 4xx status code with an Error and …","relpermalink":"/tsb/refs/iam/v2/oauth-service/","summary":"import { PanelContent, PanelContentCode, } from “@theme/Panel”;\nIAM APIs for authentication.\nOAuth Token rpc Token (tetrateio.api.iam.v2.GrantRequest) returns (tetrateio.api.iam.v2.GrantResponse)\nGrants tokens for a given grant type.\nThis is used by clients to obtain an access token by presenting required parameters for the requested grant type. Current only “urn:ietf:params:oauth:grant-type:device_code” is supported. When an error occurs, this will return a 4xx status code with an Error and ErrorMessage in the response.\nDeviceCode rpc DeviceCode (tetrateio.api.iam.v2.DeviceCodeRequest) returns (tetrateio.api.iam.v2.DeviceCodeResponse)\nRequests device codes that can be used with a token grant with grant type “urn:ietf:params:oauth:grant-type:device_code”. For additional information please refer to the Device Authorization Grant RFC https://datatracker.","title":"IAM (OAuth)"},{"content":"import { PanelContent, PanelContentCode, } from “@theme/Panel”;\nIAM APIs for authentication.\nOIDC The IAM OIDC service is a service used with Open ID Connect provider integrations.\nCallback rpc Callback (tetrateio.api.iam.v2.CallbackRequest) returns (google.protobuf.Empty)\nCallback endpoint for OAuth2 Authorization Code grant flows as part of the OIDC spec.\nLogin rpc Login (tetrateio.api.iam.v2.LoginRequest) returns (google.protobuf.Empty)\nLogin endpoint to start an OIDC Authentication flow. …","relpermalink":"/tsb/refs/iam/v2/oidc-service/","summary":"import { PanelContent, PanelContentCode, } from “@theme/Panel”;\nIAM APIs for authentication.\nOIDC The IAM OIDC service is a service used with Open ID Connect provider integrations.\nCallback rpc Callback (tetrateio.api.iam.v2.CallbackRequest) returns (google.protobuf.Empty)\nCallback endpoint for OAuth2 Authorization Code grant flows as part of the OIDC spec.\nLogin rpc Login (tetrateio.api.iam.v2.LoginRequest) returns (google.protobuf.Empty)\nLogin endpoint to start an OIDC Authentication flow.\nCallbackRequest Request with parameters for an OAuth2 Authorization Code grant redirect.\nField Description Validation Rule code\nstring oneof result OAuth2 Authorization Code. When present this indicates the user authorized the request. TSB will use this code to acquire a token from the OIDC token endpoint and complete the login flow.","title":"IAM (OIDC)"},{"content":"Service performance degradations can be very difficult to understand and isolate:\nThere is far too much data to dig through to identify the cause of the performance issue The experts in the application’s behavior (the dev team) typically do not have access to the running cluster Tetrate Service Bridge provides a set of tools to:\nEnable the TSB operator to retrieve an archive of service performance data from a running cluster Enable application developers to query this data to identify the …","relpermalink":"/tsb/troubleshooting/identify-underperforming-services/","summary":"Service performance degradations can be very difficult to understand and isolate:\nThere is far too much data to dig through to identify the cause of the performance issue The experts in the application’s behavior (the dev team) typically do not have access to the running cluster Tetrate Service Bridge provides a set of tools to:\nEnable the TSB operator to retrieve an archive of service performance data from a running cluster Enable application developers to query this data to identify the slowest transactions (or those with errors) and determine the call graph associated with the slow response. Before you get started, make sure you: ✓ Familiarize yourself with TSB concepts ✓ Install the TSB demo environment ✓ Deploy the Istio Bookinfo sample app Collecting data The TSB operator can use tctl to collect the cluster state.","title":"Identify Underperforming Services"},{"content":"import { PanelContent, PanelContentCode, } from “@theme/Panel”;\nProvide information about the Service bridge platform.\nInfo The Info service provides information about the service Bridge platform.\nGetVersion rpc GetVersion (tetrateio.api.tsb.v2.GetVersionRequest) returns (tetrateio.api.tsb.v2.Version)\nGetVersion returns the version of the TSB binary\nGetCurrentUser rpc GetCurrentUser (tetrateio.api.tsb.v2.GetCurrentUserRequest) returns (tetrateio.api.tsb.v2.CurrentUser)\nGetCurrentUser returns the …","relpermalink":"/tsb/refs/tsb/v2/info/","summary":"import { PanelContent, PanelContentCode, } from “@theme/Panel”;\nProvide information about the Service bridge platform.\nInfo The Info service provides information about the service Bridge platform.\nGetVersion rpc GetVersion (tetrateio.api.tsb.v2.GetVersionRequest) returns (tetrateio.api.tsb.v2.Version)\nGetVersion returns the version of the TSB binary\nGetCurrentUser rpc GetCurrentUser (tetrateio.api.tsb.v2.GetCurrentUserRequest) returns (tetrateio.api.tsb.v2.CurrentUser)\nGetCurrentUser returns the information of the user or service account that made the request.\nCurrentUser CurrentUser contains the information of the user or service account that made the request.\nField Description Validation Rule loginName\nstring login_name is the name used in the login credentials.\n–\ntype\ntetrateio.api.tsb.v2.CurrentUser.Type The type of the current user, e.g. USER or SERVICE_ACCOUNT","title":"Info"},{"content":" DEPRECATION: The functionality provided by the IngressGateway is now provided in Gateway object, and using it is the recommended approach. The IngressGateway resource will be removed in future releases.\nIngressGateway configures a workload to act as a gateway for traffic entering the mesh. The ingress gateway also provides basic API gateway functionalities such as JWT token validation and request authorization. Gateways in privileged workspaces can route to services outside the workspace while …","relpermalink":"/tsb/refs/tsb/gateway/v2/ingress-gateway/","summary":"DEPRECATION: The functionality provided by the IngressGateway is now provided in Gateway object, and using it is the recommended approach. The IngressGateway resource will be removed in future releases.\nIngressGateway configures a workload to act as a gateway for traffic entering the mesh. The ingress gateway also provides basic API gateway functionalities such as JWT token validation and request authorization. Gateways in privileged workspaces can route to services outside the workspace while those in unprivileged workspaces can only route to services inside the workspace.\nThe following example declares an ingress gateway running on pods with app: gateway labels in the ns1 namespace.","title":"Ingress Gateway"},{"content":"Whenever we use the TSB IngressGateway or the Istio Gateway and VirtualService resources to route external traffic to our services, we might face problems with the routes that we expose. In this document, we are going to show you some of the most common failure scenarios and how to troubleshoot them.\nMissing configuration One of the first things to check is that the configuration that we created in TSB exists in the destination cluster. For instance, in this case:\n$ curl -vk …","relpermalink":"/tsb/troubleshooting/gateway-troubleshooting/","summary":"Whenever we use the TSB IngressGateway or the Istio Gateway and VirtualService resources to route external traffic to our services, we might face problems with the routes that we expose. In this document, we are going to show you some of the most common failure scenarios and how to troubleshoot them.\nMissing configuration One of the first things to check is that the configuration that we created in TSB exists in the destination cluster. For instance, in this case:\n$ curl -vk http://helloworld.tetrate.io/hello [ ... ] \u003e GET /hello HTTP/1.1 \u003e Host: helloworld.tetrate.io \u003e User-Agent: curl/7.81.0 \u003e Accept: */* \u003e * Mark bundle as not supporting multiuse \u003c HTTP/1.","title":"Ingress Gateway troubleshooting"},{"content":"This document describes how to install Tetrate Service Bridge (TSB) in your Amazon Kubernetes (EKS) cluster through the AWS Container Marketplace.\n:::note This document is intended for users who have purchased Tetrate’s AWS Container Marketplace offering. It will not work if you have not subscribed to the Tetrate Container Marketplace offering. Please contact Tetrate if you’re interested in an AWS Marketplace Private Offer. :::\nOverview of the Tetrate Operator The Tetrate Operator is a …","relpermalink":"/tsb/setup/aws/container-marketplace/","summary":"This document describes how to install Tetrate Service Bridge (TSB) in your Amazon Kubernetes (EKS) cluster through the AWS Container Marketplace.\n:::note This document is intended for users who have purchased Tetrate’s AWS Container Marketplace offering. It will not work if you have not subscribed to the Tetrate Container Marketplace offering. Please contact Tetrate if you’re interested in an AWS Marketplace Private Offer. :::\nOverview of the Tetrate Operator The Tetrate Operator is a Kubernetes Operator from Tetrate that makes it easier to install, deploy, and upgrade TSB. The AWS Container Marketplace offering for Tetrate Service Bridge installs a version of the Tetrate Operator in an EKS cluster.","title":"Install Tetrate Service Bridge from the AWS Container Marketplace"},{"content":"This document describes how to install TSB in AWS in a single VPC.\nBefore you get started, make sure you: ✓ Familiarize yourself with TSB concepts ✓ Install tctl and sync your tctl images\n✓ Install EKS CLI\n✓ Install AWS CLI\nInstalling TSB With A Single VPC In this scenario, you will need to have 3 EKS clusters, Elasticsearch, and Postgres running in your AWS account. Please follow the corresponding AWS guides for more detail on how to setup these:\nCreating an EKS cluster Getting Started with …","relpermalink":"/tsb/setup/aws/vpc/","summary":"This document describes how to install TSB in AWS in a single VPC.\nBefore you get started, make sure you: ✓ Familiarize yourself with TSB concepts ✓ Install tctl and sync your tctl images\n✓ Install EKS CLI\n✓ Install AWS CLI\nInstalling TSB With A Single VPC In this scenario, you will need to have 3 EKS clusters, Elasticsearch, and Postgres running in your AWS account. Please follow the corresponding AWS guides for more detail on how to setup these:\nCreating an EKS cluster Getting Started with Amazon OpenSearch Service (CLI reference) Creating a PostgreSQL DB instance (CLI reference) First, create the Management Plane cluster using the following command template.","title":"Install Tetrate Service Bridge In AWS"},{"content":" Image Values for the TSB operator image.\nField Description Validation Rule registry\nstring Registry used to download the operator image.\n–\ntag\nstring The tag of the operator image.\n–\nOperator Operator values for the TSB operator application.\nField Description Validation Rule deployment\ntetrateio.api.install.helm.common.v1alpha1.Operator.Deployment Values for the TSB operator deployment.\n–\nservice\ntetrateio.api.install.helm.common.v1alpha1.Operator.Service Values for the TSB operator service.\n– …","relpermalink":"/tsb/refs/install/helm/common/v1alpha1/common/","summary":"Image Values for the TSB operator image.\nField Description Validation Rule registry\nstring Registry used to download the operator image.\n–\ntag\nstring The tag of the operator image.\n–\nOperator Operator values for the TSB operator application.\nField Description Validation Rule deployment\ntetrateio.api.install.helm.common.v1alpha1.Operator.Deployment Values for the TSB operator deployment.\n–\nservice\ntetrateio.api.install.helm.common.v1alpha1.Operator.Service Values for the TSB operator service.\n–\nserviceAccount\ntetrateio.api.install.helm.common.v1alpha1.Operator.ServiceAccount Values for the TSB operator service account.\n–\nDeployment Values for the TSB operator deployment.\nField Description Validation Rule affinity\ntetrateio.api.install.kubernetes.Affinity Affinity configuration for the pod. https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity\n–\nannotations\nmap\u003cstring, string\u003e Custom collection of annotations to add to the deployment.","title":"install/helm/common/v1alpha1/common.proto"},{"content":" Secrets Secrets available in the ControlPlane installation.\nField Description Validation Rule tsb\ntetrateio.api.install.helm.controlplane.v1alpha1.Secrets.TSB Secrets to reach the TSB Management Plane.\n–\nelasticsearch\ntetrateio.api.install.helm.controlplane.v1alpha1.Secrets.ElasticSearch Secrets to reach the Elasticsearch.\n–\nxcp\ntetrateio.api.install.helm.controlplane.v1alpha1.Secrets.XCP Secrets to reach the XCP Central in the Management Plane.\n–\nclusterServiceAccount …","relpermalink":"/tsb/refs/install/helm/controlplane/v1alpha1/values/","summary":"Secrets Secrets available in the ControlPlane installation.\nField Description Validation Rule tsb\ntetrateio.api.install.helm.controlplane.v1alpha1.Secrets.TSB Secrets to reach the TSB Management Plane.\n–\nelasticsearch\ntetrateio.api.install.helm.controlplane.v1alpha1.Secrets.ElasticSearch Secrets to reach the Elasticsearch.\n–\nxcp\ntetrateio.api.install.helm.controlplane.v1alpha1.Secrets.XCP Secrets to reach the XCP Central in the Management Plane.\n–\nclusterServiceAccount\ntetrateio.api.install.helm.controlplane.v1alpha1.Secrets.ClusterServiceAccount Cluster service account used to authenticate to the Management Plane.\n–\nClusterServiceAccount Cluster service account used to authenticate to the Management Plane.\nField Description Validation Rule clusterFQN\nstring TSB FQN of the onboarded cluster resource. This will be generate tokens for all Control Plane agents.\n–\nJWK\nstring Literal JWK used to generate and sign the tokens for all the Control Plane agents.","title":"install/helm/controlplane/v1alpha1/values.proto"},{"content":"httpbin is a simple HTTP request and response service that is used for testing.\nThe httpbin service is used in many examples in the TSB documentation. This document provides the basic installation procedure for this service.\nPlease make sure to refer to each TSB documentation for specific caveats or customizations that are required for the examples to work, as this document describes the most generic installation steps.\nThe following examples assume that you have already setup TSB, and that you …","relpermalink":"/tsb/reference/samples/httpbin/","summary":"httpbin is a simple HTTP request and response service that is used for testing.\nThe httpbin service is used in many examples in the TSB documentation. This document provides the basic installation procedure for this service.\nPlease make sure to refer to each TSB documentation for specific caveats or customizations that are required for the examples to work, as this document describes the most generic installation steps.\nThe following examples assume that you have already setup TSB, and that you have onboarded Kubernetes clusters to install the httpbin workload to.\nUnless otherwise stated, the examples that use the kubectl command must be pointed to the same cluster.","title":"Installing httpbin"},{"content":"Open Policy Agent (OPA) is an open source, general-purpose policy engine that provides a high-level declarative language that lets you specify policy as code. OPA also offers simple APIs to offload policy decision-making from your software.\nThis document describes a simplified version of the configuring OPA in TSB, to accompany sections where it is used as the external authorization (ext-authz) service. In your actual application there may be differences that require tweaking.\n:::note OPA …","relpermalink":"/tsb/reference/samples/opa/","summary":"Open Policy Agent (OPA) is an open source, general-purpose policy engine that provides a high-level declarative language that lets you specify policy as code. OPA also offers simple APIs to offload policy decision-making from your software.\nThis document describes a simplified version of the configuring OPA in TSB, to accompany sections where it is used as the external authorization (ext-authz) service. In your actual application there may be differences that require tweaking.\n:::note OPA support Tetrate does not offer support for OPA. Please look elsewhere if you need support for your use case. :::\nFor more detailed explanation of the configurations described below, please refer to the official documentation.","title":"Installing Open Policy Agent"},{"content":"Sometimes it is convenient to have a workload that does nothing. In this example a container with curl installed is used as the base for the sleep service, so that testing is easier.\nThe sleep service is used in multiple examples in the TSB documentation. This document provides the basic installation procedure for this service.\nPlease make sure to refer to each TSB documentation for specific caveats or customizations that are required for the examples to work, as this document describes the most …","relpermalink":"/tsb/reference/samples/sleep-service/","summary":"Sometimes it is convenient to have a workload that does nothing. In this example a container with curl installed is used as the base for the sleep service, so that testing is easier.\nThe sleep service is used in multiple examples in the TSB documentation. This document provides the basic installation procedure for this service.\nPlease make sure to refer to each TSB documentation for specific caveats or customizations that are required for the examples to work, as this document describes the most generic installation steps.\nThe following examples assume that you have already setup TSB, and that you have onboarded Kubernetes clusters to install the sleep workload to.","title":"Installing sleep"},{"content":"In order to demonstrate how a workload deployed outside of Kubernetes integrates with the rest of the mesh, we need to have some other application(s) it could communicate with.\nFor the purposes of this guide, you need to deploy Istio Bookinfo example.\ninto your Kubernetes cluster.\nDeploy Bookinfo example Create the namespace bookinfo, and add the proper labels:\nkubectl create namespace bookinfo kubectl label namespace bookinfo istio-injection=enabled Deploy the bookinfo application:\ncat \u0026lt;\u0026lt;EOF | …","relpermalink":"/tsb/setup/workload-onboarding/quickstart/aws-ec2/bookinfo/","summary":"In order to demonstrate how a workload deployed outside of Kubernetes integrates with the rest of the mesh, we need to have some other application(s) it could communicate with.\nFor the purposes of this guide, you need to deploy Istio Bookinfo example.\ninto your Kubernetes cluster.\nDeploy Bookinfo example Create the namespace bookinfo, and add the proper labels:\nkubectl create namespace bookinfo kubectl label namespace bookinfo istio-injection=enabled Deploy the bookinfo application:\ncat \u003c\u003cEOF | kubectl apply -n bookinfo -f - apiVersion: security.istio.io/v1beta1 kind: PeerAuthentication metadata: name: default spec: mtls: mode: STRICT EOF kubectl apply -n bookinfo -f https://raw.githubusercontent.com/istio/istio/master/samples/bookinfo/platform/kube/bookinfo.yaml kubectl wait --for=condition=Available -n bookinfo deployments --all In order to send requests into the bookinfo product page from your local environment, you will need to set up port forwarding.","title":"Installing the Bookinfo Example"},{"content":"Before you get started, you must have:\n✓ Vault 1.3.1 or newer\n✓ Vault Injector 0.3.0 or newer\nSetup Vault Install Vault (it does not need to be installed in the Kubernetes cluster, but should be reachable from inside the Kubernetes cluster). The Vault Injector (agent-injector) must be installed into the cluster and configured to inject sidecars. This is automatically done by the Helm chart v0.5.0+ which installs Vault 0.12+ and Vault-Injector 0.3.0+. The example below assumes that Vault is …","relpermalink":"/tsb/operations/vault/istiod-ca/","summary":"Before you get started, you must have:\n✓ Vault 1.3.1 or newer\n✓ Vault Injector 0.3.0 or newer\nSetup Vault Install Vault (it does not need to be installed in the Kubernetes cluster, but should be reachable from inside the Kubernetes cluster). The Vault Injector (agent-injector) must be installed into the cluster and configured to inject sidecars. This is automatically done by the Helm chart v0.5.0+ which installs Vault 0.12+ and Vault-Injector 0.3.0+. The example below assumes that Vault is installed in the tsb namespace.\nFor more details, check the Vault documentation.\nhelm install --name=vault --set='server.dev.enabled=true' Port forward the vault service to local and set environment values to authenticate to the API","title":"Istio CA"},{"content":"By default, Istio injects sidecar proxies into application’s pods in order to handle the traffic for the pod. These sidecars need to be privileged containers as they need to manipulate iptables rules in the pod network namespace to be able to intercept the traffic coming in and out the pod.\nThis default behavior is not desirable from a security standpoint as it effectively grants the application pods to run with these elevated privileges. The alternative Istio provides to this is the use of a …","relpermalink":"/tsb/operations/features/istio-cni/","summary":"By default, Istio injects sidecar proxies into application’s pods in order to handle the traffic for the pod. These sidecars need to be privileged containers as they need to manipulate iptables rules in the pod network namespace to be able to intercept the traffic coming in and out the pod.\nThis default behavior is not desirable from a security standpoint as it effectively grants the application pods to run with these elevated privileges. The alternative Istio provides to this is the use of a CNI plugin that handles the pod network namespace modifications at pod creation time.\nEnable Istio CNI in control plane In order to enable the Istio CNI plugin in your control plane, you will need to edit the ControlPlane CR or Helm values to include the CNI configuration.","title":"Istio CNI"},{"content":"import { PanelContent, PanelContentCode, } from “@theme/Panel”;\nService to manage gateway settings in Istio Direct mode.\nIstioGateway The Istio Gateway service provides methods to manage gateway settings in Istio direct mode.\nThe methods in this service allow users to push Istio gateway configuration resources into TSB. All properties of the TSB resource hierarchies apply as well to these resources: grouping, access control policies in the management plane, etc.\nCreateVirtualService rpc …","relpermalink":"/tsb/refs/tsb/gateway/v2/istio-gateway-direct/","summary":"import { PanelContent, PanelContentCode, } from “@theme/Panel”;\nService to manage gateway settings in Istio Direct mode.\nIstioGateway The Istio Gateway service provides methods to manage gateway settings in Istio direct mode.\nThe methods in this service allow users to push Istio gateway configuration resources into TSB. All properties of the TSB resource hierarchies apply as well to these resources: grouping, access control policies in the management plane, etc.\nCreateVirtualService rpc CreateVirtualService (tetrateio.api.tsb.types.v2.CreateIstioObjectRequest) returns (tetrateio.api.tsb.types.v2.IstioObject)\nRequires CREATE\nCreate a new Istio VirtualService in the gateway group. Note that the VirtualService must be in one of the namespaces owned by the group.","title":"Istio Direct Mode Gateway Service"},{"content":"import { PanelContent, PanelContentCode, } from “@theme/Panel”;\nService to manage security settings in Istio Direct mode.\nIstioSecurity The Istio Security service provides methods to manage security settings in Istio direct mode.\nThe methods in this service allow users to push Istio security configuration resources into TSB. All properties of the TSB resource hierarchies apply as well to these resources: grouping, access control policies in the management plane, etc.\nCreatePeerAuthentication rpc …","relpermalink":"/tsb/refs/tsb/security/v2/istio-security-direct/","summary":"import { PanelContent, PanelContentCode, } from “@theme/Panel”;\nService to manage security settings in Istio Direct mode.\nIstioSecurity The Istio Security service provides methods to manage security settings in Istio direct mode.\nThe methods in this service allow users to push Istio security configuration resources into TSB. All properties of the TSB resource hierarchies apply as well to these resources: grouping, access control policies in the management plane, etc.\nCreatePeerAuthentication rpc CreatePeerAuthentication (tetrateio.api.tsb.types.v2.CreateIstioObjectRequest) returns (tetrateio.api.tsb.types.v2.IstioObject)\nRequires CREATE\nCreate a new Istio PeerAuthentication resource in the given group.\nGetPeerAuthentication rpc GetPeerAuthentication (tetrateio.api.tsb.types.v2.GetIstioObjectRequest) returns (tetrateio.api.tsb.types.v2.IstioObject)\nRequires READ\nGet the details of the given Istio PeerAuthentication resource.","title":"Istio Direct Mode Security Service"},{"content":"import { PanelContent, PanelContentCode, } from “@theme/Panel”;\nService to manage traffic settings in Istio Direct mode.\nIstioTraffic The Istio Traffic service provides methods to manage traffic settings in Istio direct mode.\nThe methods in this service allow users to push Istio traffic configuration resources into TSB. All properties of the TSB resource hierarchies apply as well to these resources: grouping, access control policies in the management plane, etc.\nCreateVirtualService rpc …","relpermalink":"/tsb/refs/tsb/traffic/v2/istio-traffic-direct/","summary":"import { PanelContent, PanelContentCode, } from “@theme/Panel”;\nService to manage traffic settings in Istio Direct mode.\nIstioTraffic The Istio Traffic service provides methods to manage traffic settings in Istio direct mode.\nThe methods in this service allow users to push Istio traffic configuration resources into TSB. All properties of the TSB resource hierarchies apply as well to these resources: grouping, access control policies in the management plane, etc.\nCreateVirtualService rpc CreateVirtualService (tetrateio.api.tsb.types.v2.CreateIstioObjectRequest) returns (tetrateio.api.tsb.types.v2.IstioObject)\nRequires CREATE\nCreate an Istio VirtualService in the given traffic group. Note that the VirtualService must be in one of the namespaces owned by the group.","title":"Istio Direct Mode Traffic Service"},{"content":" DEPRECATED: use Access Bindings instead.\nIstioInternalAccessBindings is an assignment of roles to a set of users or teams to access resources under a Istio internal group. The user or team information is obtained from an LDAP server that should have been configured as part of Service Bridge installation. Note that a IstioInternalAccessBinding can be created or modified only by users who have SET_POLICY permission on the Istio internal group.\nThe following example assigns the istiointernal-admin …","relpermalink":"/tsb/refs/tsb/rbac/v2/istio-internal-access-bindings/","summary":"DEPRECATED: use Access Bindings instead.\nIstioInternalAccessBindings is an assignment of roles to a set of users or teams to access resources under a Istio internal group. The user or team information is obtained from an LDAP server that should have been configured as part of Service Bridge installation. Note that a IstioInternalAccessBinding can be created or modified only by users who have SET_POLICY permission on the Istio internal group.\nThe following example assigns the istiointernal-admin role to users alice, bob, and members of the istiointernal-ops team for istio internal group g1 under workspace w1 owned by the tenant mycompany. Use fully-qualified name (fqn) when specifying user and team","title":"Istio Internal Access Bindings"},{"content":"import { PanelContent, PanelContentCode, } from “@theme/Panel”;\nIstioInternalDirect service provides methods to manage resources in Istio direct mode.\nIstioInternalDirect IstioInternalDirect service provides methods to manage resources in Istio direct mode.\nThe methods in this service allow users to push resources like Istio Envoy filters or service entries, into TSB. All properties of the TSB resource hierarchies apply as well to these resources: grouping, access control policies in the …","relpermalink":"/tsb/refs/tsb/istiointernal/v2/istio-istiointernal-direct/","summary":"import { PanelContent, PanelContentCode, } from “@theme/Panel”;\nIstioInternalDirect service provides methods to manage resources in Istio direct mode.\nIstioInternalDirect IstioInternalDirect service provides methods to manage resources in Istio direct mode.\nThe methods in this service allow users to push resources like Istio Envoy filters or service entries, into TSB. All properties of the TSB resource hierarchies apply as well to these resources: grouping, access control policies in the management plane, etc.\nCreateEnvoyFilter rpc CreateEnvoyFilter (tetrateio.api.tsb.types.v2.CreateIstioObjectRequest) returns (tetrateio.api.tsb.types.v2.IstioObject)\nRequires CREATE\nCreate an Istio EnvoyFilter in the given istio internal group. Note that the EnvoyFilter must be in one of the namespaces owned by the group.","title":"Istio Internal Direct Mode Service"},{"content":"import { PanelContent, PanelContentCode, } from “@theme/Panel”;\nIstioInternal service provides methods to manage istio internal TSB resources.\nIstioInternal IstioInternal service provides methods to manage istio internal TSB resources.\nIt provides methods to create and manage istio internal groups within a workspace, allowing to create fine-grained groupings to configure a subset of the workspace namespaces. Access policies can be assigned at group level, providing a fine-grained access control …","relpermalink":"/tsb/refs/tsb/istiointernal/v2/istiointernal-service/","summary":"import { PanelContent, PanelContentCode, } from “@theme/Panel”;\nIstioInternal service provides methods to manage istio internal TSB resources.\nIstioInternal IstioInternal service provides methods to manage istio internal TSB resources.\nIt provides methods to create and manage istio internal groups within a workspace, allowing to create fine-grained groupings to configure a subset of the workspace namespaces. Access policies can be assigned at group level, providing a fine-grained access control to the istio internal configuration features.\nCreateGroup rpc CreateGroup (tetrateio.api.tsb.istiointernal.v2.CreateIstioInternalGroupRequest) returns (tetrateio.api.tsb.istiointernal.v2.Group)\nRequires CREATE\nCreate a new Istio internal group in the given workspace.\nGroups will by default configure all the namespaces owned by their workspace, unless explicitly configured.","title":"Istio Internal Direct Mode Service"},{"content":" Istio internal groups only allow grouping DIRECT mode mesh resources in a set of namespaces owned by its parent workspace. This group is aimed for grouping resources not directly related to traffic, security, or gateway like EnvoyFilters and ServiceEntry for instance. Istio internal group is meant to group highly coupled and implementation-detailed oriented istio resources that don’t provide any BRIDGE mode guarantees or backward/forward compatibilities that other groups like traffic, security …","relpermalink":"/tsb/refs/tsb/istiointernal/v2/istio-internal-group/","summary":"Istio internal groups only allow grouping DIRECT mode mesh resources in a set of namespaces owned by its parent workspace. This group is aimed for grouping resources not directly related to traffic, security, or gateway like EnvoyFilters and ServiceEntry for instance. Istio internal group is meant to group highly coupled and implementation-detailed oriented istio resources that don’t provide any BRIDGE mode guarantees or backward/forward compatibilities that other groups like traffic, security of gateway can provide. Especially, and mainly because resources like EnvoyFilters, are highly customizable and can interfere in unpredictable ways, with any other routing, security, listeners, or filter chains among other configurations that TSB may have setup.","title":"Istio internal Group"},{"content":" Configuration affecting Istio control plane installation version and shape. Note: unlike other Istio protos, field names must use camelCase. This is asserted in tests. Without camelCase, the json tag on the Go struct will not match the user’s JSON representation. This leads to Kubernetes merge libraries, which rely on this tag, to fail. All other usages use jsonpb which does not use the json tag.\nistio.operator.v1alpha1.Affinity See k8s.io.api.core.v1.Affinity.\nField Description Validation Rule …","relpermalink":"/tsb/refs/istio.io/api/operator/v1alpha1/operator/","summary":"Configuration affecting Istio control plane installation version and shape. Note: unlike other Istio protos, field names must use camelCase. This is asserted in tests. Without camelCase, the json tag on the Go struct will not match the user’s JSON representation. This leads to Kubernetes merge libraries, which rely on this tag, to fail. All other usages use jsonpb which does not use the json tag.\nistio.operator.v1alpha1.Affinity See k8s.io.api.core.v1.Affinity.\nField Description Validation Rule nodeAffinity\nistio.operator.v1alpha1.NodeAffinity –\npodAffinity\nistio.operator.v1alpha1.PodAffinity –\npodAntiAffinity\nistio.operator.v1alpha1.PodAntiAffinity –\nistio.operator.v1alpha1.BaseComponentSpec Configuration for base component.\nField Description Validation Rule enabled\ngoogle.protobuf.BoolValue Selects whether this component is installed.\n–","title":"IstioOperator Options"},{"content":" JwtIdentity represents an JWT identity of a workload.\nE.g.,\nJWT identity of a workload:\nissuer: https://mycompany.corp subject: us-east-datacenter1-vm007 attributes: region: us-east datacenter: datacenter1 instance_name: vm007 instance_hostname: vm007.internal.corp instance_role: app-ratings JwtIdentity JwtIdentity represents an JWT identity of a workload.\nField Description Validation Rule issuer\nstring REQUIRED JWT Issuer identifier.\nThe value must be a case sensitive URL using the https …","relpermalink":"/tsb/refs/onboarding/config/types/identity/jwt/v1alpha1/jwt/","summary":"JwtIdentity represents an JWT identity of a workload.\nE.g.,\nJWT identity of a workload:\nissuer: https://mycompany.corp subject: us-east-datacenter1-vm007 attributes: region: us-east datacenter: datacenter1 instance_name: vm007 instance_hostname: vm007.internal.corp instance_role: app-ratings JwtIdentity JwtIdentity represents an JWT identity of a workload.\nField Description Validation Rule issuer\nstring REQUIRED JWT Issuer identifier.\nThe value must be a case sensitive URL using the https scheme that contains scheme, host, and optionally, port number and path components and no query or fragment components.\nE.g., https://mycompany.corp, https://accounts.google.com, https://sts.windows.net/9edbd6c9-0e5b-4cfd-afec-fdde27cdd928/, etc.\nSee https://openid.net/specs/openid-connect-core-1_0.html#IDToken\nstring = { prefix: https:// uri: true}\nsubject\nstring REQUIRED Workload identifier (JWT subject).\nA locally unique identifier within the Issuer.","title":"JWT Identity"},{"content":" JwtIdentityMatcher specifies matching workloads with JWT identities.\nFor example, the following configuration will match only those workloads that were authenticated by means of an OIDC ID Token issued by https://mycompany.corp for one of the subjects us-east-datacenter1-vm007 or us-west-datacenter2-vm008:\nissuer: \u0026#34;https://mycompany.corp\u0026#34; subjects: - \u0026#34;us-east-datacenter1-vm007\u0026#34; - \u0026#34;us-west-datacenter2-vm008\u0026#34; In those cases where an OIDC ID Token from a given issuer includes a map of fine-grained …","relpermalink":"/tsb/refs/onboarding/config/authorization/jwt/v1alpha1/jwt/","summary":"JwtIdentityMatcher specifies matching workloads with JWT identities.\nFor example, the following configuration will match only those workloads that were authenticated by means of an OIDC ID Token issued by https://mycompany.corp for one of the subjects us-east-datacenter1-vm007 or us-west-datacenter2-vm008:\nissuer: \"https://mycompany.corp\" subjects: - \"us-east-datacenter1-vm007\" - \"us-west-datacenter2-vm008\" In those cases where an OIDC ID Token from a given issuer includes a map of fine-grained attributes associated with a workload, it is possible to define rules that match those attributes.\nE.g., the following configuration will match a set workloads that were authenticated by means of an OIDC ID Token issued by https://mycompany.corp and include 1) attribute region with one of the values us-east or us-west and 2) attribute instance_role with the value app-ratings:","title":"JWT Identity Matcher"},{"content":" JwtIssuer specifies configuration associated with a JWT issuer.\nFor example,\nissuer: \u0026#34;https://mycompany.corp\u0026#34; jwksUri: \u0026#34;https://mycompany.corp/jwks.json\u0026#34; shortName: \u0026#34;mycorp\u0026#34; tokenFields: attributes: jsonPath: .custom_attributes JwtIssuer JwtIssuer specifies configuration associated with a JWT issuer.\nField Description Validation Rule issuer\nstring REQUIRED JWT Issuer identifier.\nThe value must be a case sensitive URL using the https scheme that contains scheme, host, and optionally, port number …","relpermalink":"/tsb/refs/onboarding/config/install/v1alpha1/jwt-issuer/","summary":"JwtIssuer specifies configuration associated with a JWT issuer.\nFor example,\nissuer: \"https://mycompany.corp\" jwksUri: \"https://mycompany.corp/jwks.json\" shortName: \"mycorp\" tokenFields: attributes: jsonPath: .custom_attributes JwtIssuer JwtIssuer specifies configuration associated with a JWT issuer.\nField Description Validation Rule issuer\nstring REQUIRED JWT Issuer identifier.\nThe value must be a case sensitive URL using the https scheme that contains scheme, host, and optionally, port number and path components and no query or fragment components.\nE.g., https://mycompany.corp, https://accounts.google.com, https://sts.windows.net/9edbd6c9-0e5b-4cfd-afec-fdde27cdd928/, etc.\nSee https://openid.net/specs/openid-connect-core-1_0.html#IDToken\nstring = { prefix: https:// uri: true}\njwksUri\nstring oneof jwks_source URL of the JSON Web Key Set document.\nSource of public keys the Workload Onboarding Plane should use to validate the signature of an OIDC ID Token.","title":"JWT Issuer"},{"content":" Tetrate Service Bridge collects a large number of metrics. This page is generated from dashboards ran internally at Tetrate and will be updated periodically based on best practices learned from operational experiences in Tetrate and from user deployments. Each heading represents a different dashboard, and each sub-heading is a panel on this dashboard. For this reason, you may see metrics appear multiple times.\nThe metrics described in this document build up a series of Grafana dashboards that …","relpermalink":"/tsb/operations/telemetry/key-metrics/","summary":"Tetrate Service Bridge collects a large number of metrics. This page is generated from dashboards ran internally at Tetrate and will be updated periodically based on best practices learned from operational experiences in Tetrate and from user deployments. Each heading represents a different dashboard, and each sub-heading is a panel on this dashboard. For this reason, you may see metrics appear multiple times.\nThe metrics described in this document build up a series of Grafana dashboards that can be downloaded from here, so you can import them into your Grafana setup.\nGitOps Operational Status Operational metrics to indicate Cluster GitOps health","title":"Key Metrics"},{"content":" When installing on Kubernetes, these configuration settings can be used to override the default Kubernetes configuration. Kubernetes configuration can be set on each component in the install API using the kubeSpec field.\nThe API allows for customization of every field in the rendered Kubernetes manifests. The more common configuration fields, such as resources and service type, are supported directly; and can be configured like so:\napiVersion: install.tetrate.io/v1alpha1 kind: ManagementPlane …","relpermalink":"/tsb/refs/install/kubernetes/k8s/","summary":"When installing on Kubernetes, these configuration settings can be used to override the default Kubernetes configuration. Kubernetes configuration can be set on each component in the install API using the kubeSpec field.\nThe API allows for customization of every field in the rendered Kubernetes manifests. The more common configuration fields, such as resources and service type, are supported directly; and can be configured like so:\napiVersion: install.tetrate.io/v1alpha1 kind: ManagementPlane metadata: name: managementplane spec: hub: docker.io/tetrate components: apiServer: kubeSpec: service: type: LoadBalancer deployment: resources: limits: memory: 750Mi requests: memory: 500Mi All components have a deployment and service object. Some, such as apiServer, also have a job object associated with them.","title":"Kubernetes"},{"content":"This document describes how to configure the LDAP integration for Tetrate Service Bridge (TSB). LDAP integration in TSB allows you to use LDAP as an Identity Provider for user login to TSB, as well as synchronizing users and groups from LDAP to TSB automatically.\nThis document assumes that you already have working knowledge of configuring an LDAP service, as well as how to authenticate using it.\nConfiguration LDAP can be configured through ManagementPlane CR or Helm values. Following is an …","relpermalink":"/tsb/operations/users/configuring-ldap/","summary":"This document describes how to configure the LDAP integration for Tetrate Service Bridge (TSB). LDAP integration in TSB allows you to use LDAP as an Identity Provider for user login to TSB, as well as synchronizing users and groups from LDAP to TSB automatically.\nThis document assumes that you already have working knowledge of configuring an LDAP service, as well as how to authenticate using it.\nConfiguration LDAP can be configured through ManagementPlane CR or Helm values. Following is an example of custom resource YAML that uses LDAP as TSB Identity Provider. You will need to edit the ManagementPlane CR or the Helm values and configure the relevant sections.","title":"LDAP as the Identity Provider"},{"content":"This document describes how to lower the amount of CPU and memory used by the control plane and all the gateways in the mesh by using TSB traffic settings which will generate a sidecar resource.\nPrerequisites ✓ Familiarize yourself with TSB concepts.\n✓ Install the TSB demo environment.\n✓ Create a tenant.\nPrepare the environment In this scenario we’re going to deploy three different applications; bookinfo, httpbin and helloworld. Each application will have its ingressgateway in the same namespace …","relpermalink":"/tsb/operations/lower-istio-resources/","summary":"This document describes how to lower the amount of CPU and memory used by the control plane and all the gateways in the mesh by using TSB traffic settings which will generate a sidecar resource.\nPrerequisites ✓ Familiarize yourself with TSB concepts.\n✓ Install the TSB demo environment.\n✓ Create a tenant.\nPrepare the environment In this scenario we’re going to deploy three different applications; bookinfo, httpbin and helloworld. Each application will have its ingressgateway in the same namespace where we will be receiving the traffic and forwarding it to the application.\nStart creating one namespace for each application and enable sidecar injection:","title":"Lower Istio Resource Consumption"},{"content":" ManagementPlane resource exposes a set of configurations necessary to automatically install the Service Bridge management plane on a cluster. The installation API is an override API so any unset fields that are not required will use sensible defaults.\nPrior to creating the ManagementPlane resource, verify that the following secrets exist in the namespace the management plane will be installed into:\ntsb-certs ldap-credentials custom-host-ca (if you are using TLS connection and need a custom CA …","relpermalink":"/tsb/refs/install/managementplane/v1alpha1/spec/","summary":"ManagementPlane resource exposes a set of configurations necessary to automatically install the Service Bridge management plane on a cluster. The installation API is an override API so any unset fields that are not required will use sensible defaults.\nPrior to creating the ManagementPlane resource, verify that the following secrets exist in the namespace the management plane will be installed into:\ntsb-certs ldap-credentials custom-host-ca (if you are using TLS connection and need a custom CA to connect to LDAP host) postgres-credentials (non-demo deployments) admin-credentials es-certs (if your Elasticsearch is using a self-signed certificate) elastic-credentials (if your Elasticsearch backend requires authentication) A resource containing only the container registry hub will install a demo of Service Bridge, create a default Organization and install local instances of external dependencies, such as Postgres, Elasticsearch, and LDAP server.","title":"Management Plane"},{"content":"Workload Naming Workloads onboarded into the mesh are represented by the Kubernetes resource WorkloadAutoRegistration.\nWhen a new workload is onboarded into the mesh and joins a given WorkloadGroup, the Workload Onboarding Endpoint creates a WorkloadAutoRegistration resource in the namespace of that WorkloadGroup.\nEach WorkloadAutoRegistration resource is assigned a unique name in the format:\n\u0026lt;workload-group-name\u0026gt;-\u0026lt;workload-identity\u0026gt; Where workload-identity is a unique name generated by TSB. For …","relpermalink":"/tsb/setup/workload-onboarding/guides/managing/","summary":"Workload Naming Workloads onboarded into the mesh are represented by the Kubernetes resource WorkloadAutoRegistration.\nWhen a new workload is onboarded into the mesh and joins a given WorkloadGroup, the Workload Onboarding Endpoint creates a WorkloadAutoRegistration resource in the namespace of that WorkloadGroup.\nEach WorkloadAutoRegistration resource is assigned a unique name in the format:\n\u003cworkload-group-name\u003e-\u003cworkload-identity\u003e Where workload-identity is a unique name generated by TSB. For workload running on a AWS EC2 instance, its workload-identity would be in the format\naws-\u003caws-partition\u003e-\u003caws-account\u003e-\u003caws-zone\u003e-ec2-\u003caws-ec2-instance-id\u003e Put together, a workload’s unique name may look like the following:\nratings-aws-aws-123456789012-us-east-2b-ec2-i-1234567890abcdef0 Listing Onboarded Workloads To list onboarded workloads, issue a kubectl get command for the war resource.","title":"Managing Onboarded Workloads"},{"content":"This article will cover how TSB and istio-proxy handle headers when forwarding from Istio ingress gateways or sidecars to applications.\nBefore you get started, make sure you:\n✓ Familiarize yourself with TSB concepts ✓ Install the TSB environment. You can use TSB demo for quick install\n✓ Completed TSB usage quickstart.\n✓ Install sample application httpbin.\nRequest header size in Envoy (istio-proxy) Envoy or istio-proxy can handle headers that are considerably larger. The default maximum request …","relpermalink":"/tsb/troubleshooting/maximum-header-size-exceed/","summary":"This article will cover how TSB and istio-proxy handle headers when forwarding from Istio ingress gateways or sidecars to applications.\nBefore you get started, make sure you:\n✓ Familiarize yourself with TSB concepts ✓ Install the TSB environment. You can use TSB demo for quick install\n✓ Completed TSB usage quickstart.\n✓ Install sample application httpbin.\nRequest header size in Envoy (istio-proxy) Envoy or istio-proxy can handle headers that are considerably larger. The default maximum request headers size for incoming connections is 60 KiB\nIn this case, it will not be an issue for the majority of applications, and the request headers that incoming connections will be proxied through istio-proxy.","title":"Maximum Header Size Exceed"},{"content":" A metric is a measurement about a service, captured at runtime. Logically, the moment of capturing one of these measurements is known as a metric event which consists not only of the measurement itself, but the time that it was captured and associated metadata..\nThe key aspects of a metric are the measure, the metric type, the metric origin, and the metric detect point:\nThe measure describes the type and unit of a metric event also known as measurement. The metric type is the aggregation over …","relpermalink":"/tsb/refs/tsb/observability/telemetry/v2/metric/","summary":"A metric is a measurement about a service, captured at runtime. Logically, the moment of capturing one of these measurements is known as a metric event which consists not only of the measurement itself, but the time that it was captured and associated metadata..\nThe key aspects of a metric are the measure, the metric type, the metric origin, and the metric detect point:\nThe measure describes the type and unit of a metric event also known as measurement. The metric type is the aggregation over time applied to the measurements. The metric origin tells from where the metric measurements come from.","title":"Metric"},{"content":"import { PanelContent, PanelContentCode, } from “@theme/Panel”;\nService to manage telemetry metrics.\nMetrics The Metrics service exposes methods to manage Telemetry Metrics from Telemetry Sources.\nGetMetric rpc GetMetric (tetrateio.api.tsb.observability.telemetry.v2.GetMetricRequest) returns (tetrateio.api.tsb.observability.telemetry.v2.Metric)\nGet the details of an existing telemetry metric.\nListMetrics rpc ListMetrics (tetrateio.api.tsb.observability.telemetry.v2.ListMetricsRequest) returns …","relpermalink":"/tsb/refs/tsb/observability/telemetry/v2/metric-service/","summary":"import { PanelContent, PanelContentCode, } from “@theme/Panel”;\nService to manage telemetry metrics.\nMetrics The Metrics service exposes methods to manage Telemetry Metrics from Telemetry Sources.\nGetMetric rpc GetMetric (tetrateio.api.tsb.observability.telemetry.v2.GetMetricRequest) returns (tetrateio.api.tsb.observability.telemetry.v2.Metric)\nGet the details of an existing telemetry metric.\nListMetrics rpc ListMetrics (tetrateio.api.tsb.observability.telemetry.v2.ListMetricsRequest) returns (tetrateio.api.tsb.observability.telemetry.v2.ListMetricsResponse)\nList the telemetry metrics that are available for the requested telemetry source.\nGetMetricRequest Request to retrieve a telemetry metric from a parent telemetry source resource.\nField Description Validation Rule fqn\nstring REQUIRED Fully-qualified name of the telemetry metric.\nstring = { min_len: 1}\nListMetricsRequest Request to retrieve the list of telemetry metrics from a parent telemetry source resource.","title":"Metric Service"},{"content":" This document describes how to move all configurations, users, and groups from an organization to a newly created one.\nGet Data Start by extracting all of the configurations per tenant. For each tenant, execute the following command:\ntctl get all --tenant \u0026lt;tenant\u0026gt; \u0026gt; config.yaml Once you have all the configurations in config.yaml, make sure to manually copy the various bindings (e.g. ApplicationAccessBindings, APIAccessBindings, etc) in it to a file called bindings.yaml, and remove them from …","relpermalink":"/tsb/operations/migrate-organization/","summary":"This document describes how to move all configurations, users, and groups from an organization to a newly created one.\nGet Data Start by extracting all of the configurations per tenant. For each tenant, execute the following command:\ntctl get all --tenant \u003ctenant\u003e \u003e config.yaml Once you have all the configurations in config.yaml, make sure to manually copy the various bindings (e.g. ApplicationAccessBindings, APIAccessBindings, etc) in it to a file called bindings.yaml, and remove them from config.yaml.\nThis is due to the fact that when you use the contents of config.yaml later, the fully qualified names for the bindings will not exist, which would have resulted in an error when applying the configuration.","title":"Move Data To A New Organization"},{"content":"This document describes how TSB will handle the request/response if the header has multiple transfer-encoding:chunked and also helps you in identifying whether the problem is from the source or the destination.\nWhat we recommend to resolve this issue is to make sure that there is only one transfer-encoding:chunked in both the request and the response header, otherwise Envoy will reject the request.\nBefore you get started, make sure you: ✓ Familiarize yourself with TSB concepts ✓ Install the TSB …","relpermalink":"/tsb/troubleshooting/multiple-transfer-encoding-chunked/","summary":"This document describes how TSB will handle the request/response if the header has multiple transfer-encoding:chunked and also helps you in identifying whether the problem is from the source or the destination.\nWhat we recommend to resolve this issue is to make sure that there is only one transfer-encoding:chunked in both the request and the response header, otherwise Envoy will reject the request.\nBefore you get started, make sure you: ✓ Familiarize yourself with TSB concepts ✓ Install the TSB demo environment ✓ Deploy the Istio Bookinfo sample app Note: For the response section, the application we used here deliberately generates multiple transfer-encoding:chunked headers and it’s used for only documentation purpose.","title":"Multiple Transfer Encoding Chunked"},{"content":"New Relic’s popular software analytics platform enables businesses to monitor the health and performance of their applications, servers and databases. It collects and analyzes data from various sources, including application logs, server metrics and user interactions to provide detailed insights and metrics.\nTetrate’s rich observability data integrates seamlessly with the New Relic platform. This article shows how to make telemetry data from Istio and Tetrate Service Bridge available in New …","relpermalink":"/tsb/operations/telemetry/new-relic/","summary":"New Relic’s popular software analytics platform enables businesses to monitor the health and performance of their applications, servers and databases. It collects and analyzes data from various sources, including application logs, server metrics and user interactions to provide detailed insights and metrics.\nTetrate’s rich observability data integrates seamlessly with the New Relic platform. This article shows how to make telemetry data from Istio and Tetrate Service Bridge available in New Relic. For the metrics related to the application load - please follow the New Relic article that describes Istio Data plane metrics retrieval.\n:::note The steps below are validated, however some customers might have custom New Relic setup that requires additional customization.","title":"New Relic integration"},{"content":"Overview To onboard an AWS Elastic Container Service (ECS) task you need to follow these steps:\nCreate an AWS ECS cluster Create an IAM role for the task Create a task execution IAM role Create an AWS ECS task definition with the Workload Onboarding Agent as a sidecar container Create a subnet for the tasks Create a security group Create an AWS ECS service with this task definition Create an AWS ECS cluster Create an AWS ECS cluster called bookinfo using FARGATE as the capacity provider.\naws ecs …","relpermalink":"/tsb/setup/workload-onboarding/quickstart/aws-ecs/onboard-ecs/","summary":"Overview To onboard an AWS Elastic Container Service (ECS) task you need to follow these steps:\nCreate an AWS ECS cluster Create an IAM role for the task Create a task execution IAM role Create an AWS ECS task definition with the Workload Onboarding Agent as a sidecar container Create a subnet for the tasks Create a security group Create an AWS ECS service with this task definition Create an AWS ECS cluster Create an AWS ECS cluster called bookinfo using FARGATE as the capacity provider.\naws ecs create-cluster --cluster-name bookinfo --capacity-providers FARGATE Create an IAM role for the task Create an IAM role for the task with the following trust policy.","title":"Onboard AWS ECS task"},{"content":"Start Workload Onboarding Agent Create the file /etc/onboarding-agent/onboarding.config.yaml with the following contents. Replace ONBOARDING_ENDPOINT_ADDRESS with the value that you have obtained earlier.\napiVersion: config.agent.onboarding.tetrate.io/v1alpha1 kind: OnboardingConfiguration onboardingEndpoint: host: \u0026#34;\u0026lt;ONBOARDING_ENDPOINT_ADDRESS\u0026gt;\u0026#34; transportSecurity: tls: sni: onboarding-endpoint.example # (1) workloadGroup: # (2) namespace: bookinfo name: ratings workload: labels: version: v5 # …","relpermalink":"/tsb/setup/workload-onboarding/quickstart/aws-ec2/onboard-vm/","summary":"Start Workload Onboarding Agent Create the file /etc/onboarding-agent/onboarding.config.yaml with the following contents. Replace ONBOARDING_ENDPOINT_ADDRESS with the value that you have obtained earlier.\napiVersion: config.agent.onboarding.tetrate.io/v1alpha1 kind: OnboardingConfiguration onboardingEndpoint: host: \"\u003cONBOARDING_ENDPOINT_ADDRESS\u003e\" transportSecurity: tls: sni: onboarding-endpoint.example # (1) workloadGroup: # (2) namespace: bookinfo name: ratings workload: labels: version: v5 # (3) settings: connectedOver: INTERNET # (4) This configuration instructs the Workload Onboarding Agent to connect to the Workload Onboarding Endpoint using one address, but validate the TLS certificate against the DNS name onboarding-endpoint.example (1).\nThe agent will attempt to join the WorkloadGroup you created earlier (2).\nThe extra label specified in (3) will be associated with the workload.","title":"Onboard Workload from VM"},{"content":"Start Workload Onboarding Agent Create the file /etc/onboarding-agent/onboarding.config.yaml with the following contents. Replace ONBOARDING_ENDPOINT_ADDRESS with the value that you have obtained earlier.\napiVersion: config.agent.onboarding.tetrate.io/v1alpha1 kind: OnboardingConfiguration onboardingEndpoint: host: \u0026#34;\u0026lt;ONBOARDING_ENDPOINT_ADDRESS\u0026gt;\u0026#34; transportSecurity: tls: sni: onboarding-endpoint.example # (1) workloadGroup: # (2) namespace: bookinfo name: ratings workload: labels: version: v5 # …","relpermalink":"/tsb/setup/workload-onboarding/quickstart/on-premise/onboard-vm/","summary":"Start Workload Onboarding Agent Create the file /etc/onboarding-agent/onboarding.config.yaml with the following contents. Replace ONBOARDING_ENDPOINT_ADDRESS with the value that you have obtained earlier.\napiVersion: config.agent.onboarding.tetrate.io/v1alpha1 kind: OnboardingConfiguration onboardingEndpoint: host: \"\u003cONBOARDING_ENDPOINT_ADDRESS\u003e\" transportSecurity: tls: sni: onboarding-endpoint.example # (1) workloadGroup: # (2) namespace: bookinfo name: ratings workload: labels: version: v5 # (3) This configuration instructs the Workload Onboarding Agent to connect to the Workload Onboarding Endpoint using the one address, but validate the TLS certificate against the DNS name onboarding-endpoint.example (1).\nThe agent will attempt to join the WorkloadGroup you created earlier (2).\nThe extra label specified in (3) will be associated with the workload.","title":"Onboard Workload from VM on-premise"},{"content":"To onboard a workload deployed on AWS Auto Scaling Group (ASG), you will need to perform all setup actions as part of the instance launch script instead of executing commands on the EC2 instance.\nIn a nutshell, you will need to move setup commands from previous steps into the cloud-init configuration associated with instances in the Auto Scaling Group.\nSpecifically,\nMove setup commands from the Install Bookinfo Ratings application step Move setup commands from the Install Istio sidecar step Move …","relpermalink":"/tsb/setup/workload-onboarding/quickstart/aws-ec2/onboard-asg/","summary":"To onboard a workload deployed on AWS Auto Scaling Group (ASG), you will need to perform all setup actions as part of the instance launch script instead of executing commands on the EC2 instance.\nIn a nutshell, you will need to move setup commands from previous steps into the cloud-init configuration associated with instances in the Auto Scaling Group.\nSpecifically,\nMove setup commands from the Install Bookinfo Ratings application step Move setup commands from the Install Istio sidecar step Move setup commands from the Install Workload Onboarding Agent on AWS EC2 instance step Move setup commands from the Onboard workload from AWS EC2 instance step The following configuration is a sample that has all of the steps joined together.","title":"Onboard Workload(s) from AWS Auto Scaling Group"},{"content":"This document describes the steps to onboard AWS Elastic Container Service (ECS) tasks to TSB using the Workload Onboarding feature.\nBefore you proceed, make sure that you have completed the steps described in Setting Up Workload Onboarding document. You may skip the steps around configuring the local repository and installing packages if you do not plan to onboard VMs, as the process for ECS tasks is slightly different.\nContext Every workload that gets onboarded into the mesh by Workload …","relpermalink":"/tsb/setup/workload-onboarding/guides/ecs-workloads/","summary":"This document describes the steps to onboard AWS Elastic Container Service (ECS) tasks to TSB using the Workload Onboarding feature.\nBefore you proceed, make sure that you have completed the steps described in Setting Up Workload Onboarding document. You may skip the steps around configuring the local repository and installing packages if you do not plan to onboard VMs, as the process for ECS tasks is slightly different.\nContext Every workload that gets onboarded into the mesh by Workload Onboarding must have a verifiable identity. For AWS ECS tasks, the task IAM role is used to identify which task is trying to join the mesh.","title":"Onboarding AWS ECS workloads"},{"content":" Onboarding Configuration specifies where to onboard the workload to.\nTo be able to onboard a workload into a service mesh, a user must configure Workload Onboarding Agent with the location of the Workload Onboarding Endpoint and a name of the WorkloadGroup to join.\nBy default, Workload Onboarding Agent will read Onboarding Configuration from a file /etc/onboarding-agent/onboarding.config.yaml, which must be created by the user.\nIf Onboarding Configuration file is missing or its contents is not …","relpermalink":"/tsb/refs/onboarding/config/agent/v1alpha1/onboarding-configuration/","summary":"Onboarding Configuration specifies where to onboard the workload to.\nTo be able to onboard a workload into a service mesh, a user must configure Workload Onboarding Agent with the location of the Workload Onboarding Endpoint and a name of the WorkloadGroup to join.\nBy default, Workload Onboarding Agent will read Onboarding Configuration from a file /etc/onboarding-agent/onboarding.config.yaml, which must be created by the user.\nIf Onboarding Configuration file is missing or its contents is not valid, Workload Onboarding Agent will not be able to start.\nConsider the following example of the minimal valid configuration:\napiVersion: config.agent.onboarding.tetrate.io/v1alpha1 kind: OnboardingConfiguration onboardingEndpoint: host: onboarding.","title":"Onboarding Configuration"},{"content":"This document describes the steps to onboard on-premise workloads to TSB using the Workload Onboarding feature.\nBefore you proceed, make sure that you have completed the steps described in Setting Up Workload Onboarding document.\nContext Every workload that gets onboarded into the mesh by the Workload Onboarding must have a verifiable identity.\nVMs in the cloud have a verifiable identity out-of-the-box. Such identity is provided by the respective cloud platform.\nOn-premise environments, however, …","relpermalink":"/tsb/setup/workload-onboarding/guides/on-premise-workloads/","summary":"This document describes the steps to onboard on-premise workloads to TSB using the Workload Onboarding feature.\nBefore you proceed, make sure that you have completed the steps described in Setting Up Workload Onboarding document.\nContext Every workload that gets onboarded into the mesh by the Workload Onboarding must have a verifiable identity.\nVMs in the cloud have a verifiable identity out-of-the-box. Such identity is provided by the respective cloud platform.\nOn-premise environments, however, are a black box. Whether or not your on-premise workloads have a verifiable identity depends solely on your own technology stack.\nTherefore, to be able to onboard on-premise workloads, you need to ensure they have a verifiable identity in the form of a JWT Token.","title":"Onboarding on-premise workloads"},{"content":" Onboarding Policy authorizes matching workloads to join the mesh and become a part of a WorkloadGroup.\nBy default, none of the workloads are allowed to join the mesh.\nA workload is only allowed to join the mesh if there is an OnboardingPolicy resource that explicitly authorizes that.\nFor the purposes of authorization, a workload is considered to have the identity of the host it is running on.\nE.g., workloads that run on VMs in the cloud are considered to have cloud-specific identity of that VM. …","relpermalink":"/tsb/refs/onboarding/config/authorization/v1alpha1/policy/","summary":"Onboarding Policy authorizes matching workloads to join the mesh and become a part of a WorkloadGroup.\nBy default, none of the workloads are allowed to join the mesh.\nA workload is only allowed to join the mesh if there is an OnboardingPolicy resource that explicitly authorizes that.\nFor the purposes of authorization, a workload is considered to have the identity of the host it is running on.\nE.g., workloads that run on VMs in the cloud are considered to have cloud-specific identity of that VM. In case of AWS EC2 instances, VM identity includes AWS Partition, AWS Account number, AWS Region, AWS Zone, EC2 instance id, AWS IAM Role name, etc.","title":"Onboarding Policy"},{"content":"This document describes the steps to onboard VMs to TSB using the Workload Onboarding feature.\nBefore you proceed, make sure that you have completed the steps described in Setting Up Workload Onboarding document\nOnboarding a VM Create the Workload Onboarding Agent Configuration By default, the Workload Onboarding Agent expects its configuration to be specified in a file called /etc/onboarding-agent/onboarding.config.yaml.\nCreate file /etc/onboarding-agent/onboarding.config.yaml with the …","relpermalink":"/tsb/setup/workload-onboarding/guides/onboarding/","summary":"This document describes the steps to onboard VMs to TSB using the Workload Onboarding feature.\nBefore you proceed, make sure that you have completed the steps described in Setting Up Workload Onboarding document\nOnboarding a VM Create the Workload Onboarding Agent Configuration By default, the Workload Onboarding Agent expects its configuration to be specified in a file called /etc/onboarding-agent/onboarding.config.yaml.\nCreate file /etc/onboarding-agent/onboarding.config.yaml with the following contents. Replace onboarding-endpoint-dns-name with the Workload Onboarding Endpoint to connect to, as well as workload-group-namespace and workload-group-name with the namespace and name of the Istio WorkloadGroup to join to.\napiVersion: config.agent.onboarding.tetrate.io/v1alpha1 kind: OnboardingConfiguration onboardingEndpoint: host: \u003conboarding-endpoint-dns-name\u003e workloadGroup: namespace: \u003cworkload-group-namespace\u003e name: \u003cworkload-group-name\u003e The Workload Onboarding Endpoint is assumed to be available at https://\u003conboarding-endpoint-dns-name\u003e:15443, and that it uses a TLS certificate issued for the appropriate DNS name.","title":"Onboarding VMs"},{"content":"import ratingsWorkloadEntryYAML from ‘!!raw-loader!../../assets/setup/ratings-workloadentry.yaml’; import ratingsSidecarYAML from ‘!!raw-loader!../../assets/setup/ratings-sidecar.yaml’; import CodeBlock from ‘@theme/CodeBlock’;\n:::note Bare metal servers In this guide we only call out virtual machines (VMs). If you want to onboard a workload running on a bare metal server, simply replace VM for bare metal. There is no difference in handling between them. :::\nProblem definition Istio and the …","relpermalink":"/tsb/setup/workload-onboarding/onboarding-vms/","summary":"import ratingsWorkloadEntryYAML from ‘!!raw-loader!../../assets/setup/ratings-workloadentry.yaml’; import ratingsSidecarYAML from ‘!!raw-loader!../../assets/setup/ratings-sidecar.yaml’; import CodeBlock from ‘@theme/CodeBlock’;\n:::note Bare metal servers In this guide we only call out virtual machines (VMs). If you want to onboard a workload running on a bare metal server, simply replace VM for bare metal. There is no difference in handling between them. :::\nProblem definition Istio and the underlying Kubernetes platform create a sealed ecosystem, where control plane and data plane components are tightly integrated. For example, control plane components running on each node create a mutually trusted relationship. When new pods are scheduled to run on a node, the node is a trusted entity and its critical resources, like iptables, are modified.","title":"Onboarding VMs with tctl"},{"content":" OpenAPI Extensions available to configure APIs.\nOpenAPIExtension Metadata describing an extension to the OpenAPI spec.\nField Description Validation Rule name\nstring The name of the OpenAPI extension as it should appear in the OpenAPI document. For example: x-tsb-service\n–\nappliesTo\nList of string Parts of the OpenAPI spec where this custom extension is allowed. This is a list of names of the OpenAPI elements where the extension is supported. For example: [“info”, “path”]\n–\nrequired\nbool Flag …","relpermalink":"/tsb/refs/tsb/application/v2/openapi-extensions/","summary":"OpenAPI Extensions available to configure APIs.\nOpenAPIExtension Metadata describing an extension to the OpenAPI spec.\nField Description Validation Rule name\nstring The name of the OpenAPI extension as it should appear in the OpenAPI document. For example: x-tsb-service\n–\nappliesTo\nList of string Parts of the OpenAPI spec where this custom extension is allowed. This is a list of names of the OpenAPI elements where the extension is supported. For example: [“info”, “path”]\n–\nrequired\nbool Flag that configures if the extension is mandatory for the elements where it is supported.\n–\nOpenAPIExtensions Available OpenAPI extensions to configure APi Gateway features in Service Bridge.","title":"OpenAPI Extensions"},{"content":" Organization is a root of the Service Bridge object hierarchy. Each organization is completely independent of the other with its own set of tenants, users, teams, clusters and workspaces.\nOrganizations in TSB are tied to an Identity Provider (IdP). Users and teams, representing the organizational structure, are periodically synchronized from the IdP into TSB in order to make them available for access policy configuration.\nThe following example creates an organization named myorg.\napiVersion: …","relpermalink":"/tsb/refs/tsb/v2/organization/","summary":"Organization is a root of the Service Bridge object hierarchy. Each organization is completely independent of the other with its own set of tenants, users, teams, clusters and workspaces.\nOrganizations in TSB are tied to an Identity Provider (IdP). Users and teams, representing the organizational structure, are periodically synchronized from the IdP into TSB in order to make them available for access policy configuration.\nThe following example creates an organization named myorg.\napiVersion: api.tsb.tetrate.io/v2 kind: Organization metadata: name: myorg Organization Organization is the root of the Service Bridge object hierarchy.\nField Description Validation Rule deletionProtectionEnabled\nbool When set, prevents the resource from being deleted.","title":"Organization"},{"content":" DEPRECATED: use Access Bindings instead.\nOrganizationAccessBindings is an assignment of roles to a set of users or teams to access resources under an Organization. The user or team information is obtained from an LDAP server that should have been configured as part of Service Bridge installation. Note that a OrganizationAccessBinding can be created or modified only by users who have SET_POLICY permission on the Organization.\nThe following example assigns the org-admin role to users alice, bob, …","relpermalink":"/tsb/refs/tsb/rbac/v2/organization-access-bindings/","summary":"DEPRECATED: use Access Bindings instead.\nOrganizationAccessBindings is an assignment of roles to a set of users or teams to access resources under an Organization. The user or team information is obtained from an LDAP server that should have been configured as part of Service Bridge installation. Note that a OrganizationAccessBinding can be created or modified only by users who have SET_POLICY permission on the Organization.\nThe following example assigns the org-admin role to users alice, bob, and members of the t1 team owned by the organization myorg. Use fully-qualified name (fqn) when specifying user and team\napiVersion: rbac.tsb.tetrate.io/v2 kind: OrganizationAccessBindings metadata: organization: myorg spec: allow: - role: rbac/org-admin subjects: - user: organization/myorg/users/alice - user: organization/myorg/users/bob - team: organization/myorg/teams/t1 OrganizationAccessBindings OrganizationAccessBindings assigns permissions to users of organizations.","title":"Organization Access Bindings"},{"content":" Organization Setting allows configuring global settings for the organization. Settings such as network reachability or regional failover that apply globally to the organization are configured in the Organizations Setting object.\nThis is a global object that uniquely configures the organization, and there can be only one Organization Setting object defined for each organization.\nThe following example shows how these settings can be used to describe the organization’s network reachability …","relpermalink":"/tsb/refs/tsb/v2/organization-setting/","summary":"Organization Setting allows configuring global settings for the organization. Settings such as network reachability or regional failover that apply globally to the organization are configured in the Organizations Setting object.\nThis is a global object that uniquely configures the organization, and there can be only one Organization Setting object defined for each organization.\nThe following example shows how these settings can be used to describe the organization’s network reachability settings and some regional failover configurations.\napiVersion: api.tsb.tetrate.io/v2 kind: OrganizationSetting metadata: name: org-settings organization: myorg spec: networkSettings: networkReachability: vpc01: vpc02,vpc03 regionalFailover: - from: us-east1 to: us-central1 OrganizationSetting Settings that apply globally to the entire organization.","title":"Organization Setting"},{"content":"import { PanelContent, PanelContentCode, } from “@theme/Panel”;\nService to manage Organizations in TSB\nOrganizations The Organizations service exposes methods to manage the organizations that exist in TSB. Organizations are the root of the Service Bridge object hierarchy. Each organization is completely independent of the other with its own set of tenants, users, teams, clusters and workspaces.\nGetOrganization rpc GetOrganization (tetrateio.api.tsb.v2.GetOrganizationRequest) returns …","relpermalink":"/tsb/refs/tsb/v2/organization-service/","summary":"import { PanelContent, PanelContentCode, } from “@theme/Panel”;\nService to manage Organizations in TSB\nOrganizations The Organizations service exposes methods to manage the organizations that exist in TSB. Organizations are the root of the Service Bridge object hierarchy. Each organization is completely independent of the other with its own set of tenants, users, teams, clusters and workspaces.\nGetOrganization rpc GetOrganization (tetrateio.api.tsb.v2.GetOrganizationRequest) returns (tetrateio.api.tsb.v2.Organization)\nRequires READ\nGet the details of an organization.\nSyncOrganization rpc SyncOrganization (tetrateio.api.tsb.v2.SyncOrganizationRequest) returns (tetrateio.api.tsb.v2.SyncOrganizationResponse)\nRequires CreateUser, CreateTeam, DeleteUser, DeleteTeam, WriteTeam\nSyncOrganization is used by processes that monitor the identity providers to synchronize the users and teams with the ones in TSB.","title":"Organizations Service"},{"content":"When you deploy a workload on Kubernetes, the following happens transparently:\nAn Istio sidecar is deployed next to your workload. That sidecar is configured with the workload location and other required metadata. However, when you deploy a workload outside of Kubernetes onto a standalone VM, you have to take care of that by yourself.\nThe Workload Onboarding feature solves this problem for you out of the box. Using this feature, all you need to do to onboard a workload deployed on a VM into the …","relpermalink":"/tsb/setup/workload-onboarding/guides/overview/","summary":"When you deploy a workload on Kubernetes, the following happens transparently:\nAn Istio sidecar is deployed next to your workload. That sidecar is configured with the workload location and other required metadata. However, when you deploy a workload outside of Kubernetes onto a standalone VM, you have to take care of that by yourself.\nThe Workload Onboarding feature solves this problem for you out of the box. Using this feature, all you need to do to onboard a workload deployed on a VM into the mesh is:\nInstalling an Istio sidecar on the target VM (via DEB/RPM package). Install a Workload Onboarding Agent on the target VM (also via DEB/RPM package).","title":"Overview"},{"content":" Permissions.\nPermission A permission defines an action that can be performed on a resource. By default access to resources is denied unless an explicit permission grants access to perform an operation against it.\nField Number Description INVALID\n0\nDefault value to designate no value was explicitly set for the permission.\nREAD\n1\nThe read permission grants read-only access to the resource.\nWRITE\n2\nThe write permission allows the subject to modify an existing resource.\nCREATE\n3\nThe create …","relpermalink":"/tsb/refs/tsb/rbac/v2/permissions/","summary":"Permissions.\nPermission A permission defines an action that can be performed on a resource. By default access to resources is denied unless an explicit permission grants access to perform an operation against it.\nField Number Description INVALID\n0\nDefault value to designate no value was explicitly set for the permission.\nREAD\n1\nThe read permission grants read-only access to the resource.\nWRITE\n2\nThe write permission allows the subject to modify an existing resource.\nCREATE\n3\nThe create permission allows subjects to create child resources on the resource.\nDELETE\n4\nThe delete permission grants permissions to delete the resource.","title":"Permissions"},{"content":"import { PanelContent, PanelContentCode, } from “@theme/Panel”;\nService to manage centralized approval policies.\nPermissions The Permissions service exposes methods to query permission information on existing records. $hide_from_yaml\nQueryResourcePermissions rpc QueryResourcePermissions (tetrateio.api.tsb.q.v2.QueryResourcePermissionsRequest) returns (tetrateio.api.tsb.q.v2.QueryResourcePermissionsResponse)\nQueryResourcePermission looks up permissions that are allowed for the current principal. …","relpermalink":"/tsb/refs/tsb/q/v2/permissions-service/","summary":"import { PanelContent, PanelContentCode, } from “@theme/Panel”;\nService to manage centralized approval policies.\nPermissions The Permissions service exposes methods to query permission information on existing records. $hide_from_yaml\nQueryResourcePermissions rpc QueryResourcePermissions (tetrateio.api.tsb.q.v2.QueryResourcePermissionsRequest) returns (tetrateio.api.tsb.q.v2.QueryResourcePermissionsResponse)\nQueryResourcePermission looks up permissions that are allowed for the current principal. Multiple records can be queried with a single request. Query limit is 100, multiple requests are required to lookup more than the limit.\nGetResourcePermissions rpc GetResourcePermissions (tetrateio.api.tsb.q.v2.GetResourcePermissionsRequest) returns (tetrateio.api.tsb.q.v2.GetResourcePermissionsResponse)\nGetResourcePermission looks up permissions that are allowed for the current principal. on the given resource FQN. This is similar to QueryResourcePermission but limited to a single resource FQN.","title":"Permissions Service"},{"content":" Access Policy Bindings.\nBinding A binding associates a role with a set of subjects.\nBindings are used to configure policies, where different roles can be assigned to different sets of subjects to configure a fine-grained access control to the resource protected by the policy.\nField Description Validation Rule role\nstring REQUIRED The role that defines the permissions that will be granted to the target resource.\nstring = { min_len: 1}\nsubjects\nList of tetrateio.api.tsb.rbac.v2.Subject The set of …","relpermalink":"/tsb/refs/tsb/rbac/v2/binding/","summary":"Access Policy Bindings.\nBinding A binding associates a role with a set of subjects.\nBindings are used to configure policies, where different roles can be assigned to different sets of subjects to configure a fine-grained access control to the resource protected by the policy.\nField Description Validation Rule role\nstring REQUIRED The role that defines the permissions that will be granted to the target resource.\nstring = { min_len: 1}\nsubjects\nList of tetrateio.api.tsb.rbac.v2.Subject The set of subjects that will be allowed to access the target resource with the permissions defined by the role.\n–\nSubject Subject identifies a user or a team under an organization.","title":"Policy Bindings"},{"content":"import { PanelContent, PanelContentCode, } from “@theme/Panel”;\nService to manage access control policies for TSB resources\nPolicy The Policy service provides methods to configure the access control policies for TSB resources.\nAll TSB resources have one and exactly one policy document that configures access for it. When resources are created, a default policy is attached to the resource, assigning administration privileges on the resource to the user that created it.\nGetPolicy rpc GetPolicy …","relpermalink":"/tsb/refs/tsb/rbac/v2/policy-service/","summary":"import { PanelContent, PanelContentCode, } from “@theme/Panel”;\nService to manage access control policies for TSB resources\nPolicy The Policy service provides methods to configure the access control policies for TSB resources.\nAll TSB resources have one and exactly one policy document that configures access for it. When resources are created, a default policy is attached to the resource, assigning administration privileges on the resource to the user that created it.\nGetPolicy rpc GetPolicy (tetrateio.api.tsb.rbac.v2.GetPolicyRequest) returns (tetrateio.api.tsb.rbac.v2.AccessPolicy)\nGet the access policy for the given resource.\nSetPolicy rpc SetPolicy (tetrateio.api.tsb.rbac.v2.AccessPolicy) returns (google.protobuf.Empty)\nSet the access policy for the given resource.\nGetRootPolicy rpc GetRootPolicy (tetrateio.","title":"Policy Service"},{"content":"Before you get started, you must have:\n✓ Vault 1.3.1 or newer\n✓ Vault Injector 0.3.0 or newer\nSetup Vault Install Vault (it does not need to be installed in the Kubernetes cluster, but should be reachable from inside the Kubernetes cluster). The Vault Injector (agent-injector) must be installed into the cluster and configured to inject sidecars. This is automatically done by the Helm chart v0.5.0+ which installs Vault 0.12+ and Vault-Injector 0.3.0+. The example below assumes that Vault is …","relpermalink":"/tsb/operations/vault/postgresql/","summary":"Before you get started, you must have:\n✓ Vault 1.3.1 or newer\n✓ Vault Injector 0.3.0 or newer\nSetup Vault Install Vault (it does not need to be installed in the Kubernetes cluster, but should be reachable from inside the Kubernetes cluster). The Vault Injector (agent-injector) must be installed into the cluster and configured to inject sidecars. This is automatically done by the Helm chart v0.5.0+ which installs Vault 0.12+ and Vault-Injector 0.3.0+. The example below assumes that Vault is installed in the tsb namespace.\nFor more details, check the Vault documentation.\nhelm install --name=vault --set='server.dev.enabled=true' ./vault-helm Set up the database secret engine for PostgreSQL Enable the database secrets engine in Vault.","title":"PostgreSQL Credentials"},{"content":" Services in the registry represent logically a service that can be running in different compute platforms and different locations. The same service could be running on different Kubernetes clusters at the same time, on VMS, etc. A service in the registry represents an aggregated and logical view for all those individual services, and provides high-level features such as aggregated metrics.\nPort Port exposed by a service. Registration RPC will complete the instances field by assigning the …","relpermalink":"/tsb/refs/tsb/registry/v2/service/","summary":"Services in the registry represent logically a service that can be running in different compute platforms and different locations. The same service could be running on different Kubernetes clusters at the same time, on VMS, etc. A service in the registry represents an aggregated and logical view for all those individual services, and provides high-level features such as aggregated metrics.\nPort Port exposed by a service. Registration RPC will complete the instances field by assigning the physical services FQNs.\nField Description Validation Rule number\nuint32 REQUIRED A valid non-negative integer port number.\nuint32 = { lte: 65535 gte: 1}","title":"Registered Service"},{"content":"In this guide you’ll learn how to use the TSB REST API to perform common operations on the platform. The examples in this guide use curl, because it’s a popular command used to perform HTTP requests, however, any tool that can do HTTP will work.\nAuthentication TSB has two main authentication mechanisms: basic authentication and JWT token authentication.\nBasic Auth Basic HTTP authentication is done by sending the HTTP Authorization header with the credentials encoded in the header value. The …","relpermalink":"/tsb/reference/rest-api/guide/","summary":"In this guide you’ll learn how to use the TSB REST API to perform common operations on the platform. The examples in this guide use curl, because it’s a popular command used to perform HTTP requests, however, any tool that can do HTTP will work.\nAuthentication TSB has two main authentication mechanisms: basic authentication and JWT token authentication.\nBasic Auth Basic HTTP authentication is done by sending the HTTP Authorization header with the credentials encoded in the header value. The basic format of the header is:\nAuthorization: Basic base64(username:password) For example:\nAuthorization: Basic dGVzdDoxMjPCow== JWT Token Auth JWT Token authentication is header-based, and it’s configured by setting a JWT token in the x-tetrate-token header.","title":"REST API Guide"},{"content":" Role is a named collection of permissions that can be assigned to any user or team in the system. The set of actions that can be performed by a user, such as the ability to create, delete, or update configuration will depend on the permissions associated with the user’s role. Roles are global resources that are defined once. AccessBindings in each configuration group will bind a user to a specific role defined apriori.\nTSB comes with the following predefined roles:\nRole Permissions Description …","relpermalink":"/tsb/refs/tsb/rbac/v2/role/","summary":"Role is a named collection of permissions that can be assigned to any user or team in the system. The set of actions that can be performed by a user, such as the ability to create, delete, or update configuration will depend on the permissions associated with the user’s role. Roles are global resources that are defined once. AccessBindings in each configuration group will bind a user to a specific role defined apriori.\nTSB comes with the following predefined roles:\nRole Permissions Description rbac/admin * Grants full access to the target resource and its child objects rbac/editor Read Write Create Grants read/write access to a resource and allows creating child resources rbac/creator Read Create Useful to delegate access to a resource without giving write access to the object itself.","title":"Role"},{"content":"import { PanelContent, PanelContentCode, } from “@theme/Panel”;\nService to manage access roles in Service Bridge.\nRBAC The RBAC service provides methods to manage the roles in the Service Bridge platform. It provides method to configure the roles that can be used in the management plane access control policies and their permissions.\nCreateRole rpc CreateRole (tetrateio.api.tsb.rbac.v2.CreateRoleRequest) returns (tetrateio.api.tsb.rbac.v2.Role)\nRequires CREATE\nCreate a new role.\nListRoles rpc …","relpermalink":"/tsb/refs/tsb/rbac/v2/role-service/","summary":"import { PanelContent, PanelContentCode, } from “@theme/Panel”;\nService to manage access roles in Service Bridge.\nRBAC The RBAC service provides methods to manage the roles in the Service Bridge platform. It provides method to configure the roles that can be used in the management plane access control policies and their permissions.\nCreateRole rpc CreateRole (tetrateio.api.tsb.rbac.v2.CreateRoleRequest) returns (tetrateio.api.tsb.rbac.v2.Role)\nRequires CREATE\nCreate a new role.\nListRoles rpc ListRoles (tetrateio.api.tsb.rbac.v2.ListRolesRequest) returns (tetrateio.api.tsb.rbac.v2.ListRolesResponse)\nRequires READ\nList all existing roles.\nGetRole rpc GetRole (tetrateio.api.tsb.rbac.v2.GetRoleRequest) returns (tetrateio.api.tsb.rbac.v2.Role)\nRequires READ\nGet the details of the given role.\nUpdateRole rpc UpdateRole (tetrateio.api.tsb.rbac.v2.Role) returns (tetrateio.api.tsb.rbac.v2.Role)\nRequires WRITE\nModify a role.","title":"Role Service"},{"content":"TSB offers a fine grained permissions management to control access to TSB resources. You can grant access permissions to resources such as Organizations, Tenants, Workspaces, etc. A collection of permissions can be put into Roles, which can be reused to assign permissions to the appropriate resources, for example users or groups. Once you have defined the Roles, Access Binding objects can be used to bind Roles to a set of users or teams.\nResource Model In order to understand how to work with TSB …","relpermalink":"/tsb/operations/users/roles-and-permissions/","summary":"TSB offers a fine grained permissions management to control access to TSB resources. You can grant access permissions to resources such as Organizations, Tenants, Workspaces, etc. A collection of permissions can be put into Roles, which can be reused to assign permissions to the appropriate resources, for example users or groups. Once you have defined the Roles, Access Binding objects can be used to bind Roles to a set of users or teams.\nResource Model In order to understand how to work with TSB permissions, you will need to first understand how resources are modeled in TSB.\nIn TSB resources are modeled as a hierarchical tree, with the Organization being the root for all resources.","title":"Roles and Permissions"},{"content":" DEPRECATED: use Access Bindings instead.\nSecurityAccessBindings is an assignment of roles to a set of users or teams to access resources under a Security group. The user or team information is obtained from an LDAP server that should have been configured as part of Service Bridge installation. Note that a SecurityAccessBinding can be created or modified only by users who have SET_POLICY permission on the Security group.\nThe following example assigns the security-admin role to users alice, bob, …","relpermalink":"/tsb/refs/tsb/rbac/v2/security-access-bindings/","summary":"DEPRECATED: use Access Bindings instead.\nSecurityAccessBindings is an assignment of roles to a set of users or teams to access resources under a Security group. The user or team information is obtained from an LDAP server that should have been configured as part of Service Bridge installation. Note that a SecurityAccessBinding can be created or modified only by users who have SET_POLICY permission on the Security group.\nThe following example assigns the security-admin role to users alice, bob, and members of the security-ops team for the security group g1 under workspace w1 owned by the tenant mycompany. Use fully-qualified name (fqn) when specifying user and team","title":"Security Access Bindings"},{"content":" Security Groups allow grouping the proxy workloads in a set of namespaces owned by its parent workspace. Security related configurations can then be applied on the group to control the behavior of these proxy workloads. The group can be in one of two modes: BRIDGED and DIRECT. BRIDGED mode is a minimalistic mode that allows users to quickly configure the most commonly used features in the service mesh using Tetrate specific APIs, while the DIRECT mode provides more flexibility for power users …","relpermalink":"/tsb/refs/tsb/security/v2/security-group/","summary":"Security Groups allow grouping the proxy workloads in a set of namespaces owned by its parent workspace. Security related configurations can then be applied on the group to control the behavior of these proxy workloads. The group can be in one of two modes: BRIDGED and DIRECT. BRIDGED mode is a minimalistic mode that allows users to quickly configure the most commonly used features in the service mesh using Tetrate specific APIs, while the DIRECT mode provides more flexibility for power users by allowing them to configure the proxy workload’s security properties using a restricted subset of Istio Security APIs.","title":"Security Group"},{"content":"import { PanelContent, PanelContentCode, } from “@theme/Panel”;\nService to manage security settings.\nSecurity The Security service provides methods to manage security settings in TSB.\nIt provides methods to create and manage security groups within a workspace, allowing to create fine-grained groupings to configure a subset of the workspace namespaces. Access policies can be assigned at group level, providing a fine-grained access control to the security configuration features.\nThe Security …","relpermalink":"/tsb/refs/tsb/security/v2/security-service/","summary":"import { PanelContent, PanelContentCode, } from “@theme/Panel”;\nService to manage security settings.\nSecurity The Security service provides methods to manage security settings in TSB.\nIt provides methods to create and manage security groups within a workspace, allowing to create fine-grained groupings to configure a subset of the workspace namespaces. Access policies can be assigned at group level, providing a fine-grained access control to the security configuration features.\nThe Security service also provides methods to configure the different security settings that are allowed within each group.\nCreateGroup rpc CreateGroup (tetrateio.api.tsb.security.v2.CreateSecurityGroupRequest) returns (tetrateio.api.tsb.security.v2.Group)\nRequires CREATE\nCreate a new security group in the given workspace.","title":"Security Service"},{"content":" SecuritySetting allows configuring security related properties such as TLS authentication and access control for traffic arriving at a proxy workload in a security group.\nSecurity settings can be propagated along any defined security settings in the configuration hierarchy. How security settings are propagated can be configured by specifying a PropagationStrategy.\nThe following example creates a security group for the proxy workloads in ns1, ns2 and ns3 namespaces owned by its parent workspace …","relpermalink":"/tsb/refs/tsb/security/v2/security-setting/","summary":"SecuritySetting allows configuring security related properties such as TLS authentication and access control for traffic arriving at a proxy workload in a security group.\nSecurity settings can be propagated along any defined security settings in the configuration hierarchy. How security settings are propagated can be configured by specifying a PropagationStrategy.\nThe following example creates a security group for the proxy workloads in ns1, ns2 and ns3 namespaces owned by its parent workspace w1 under tenant mycompany and defines a security setting that only allows mutual TLS authenticated traffic from other proxy workloads in the same group.\napiVersion: security.tsb.tetrate.io/v2 kind: Group metadata: name: t1 workspace: w1 tenant: mycompany organization: myorg spec: namespaceSelector: names: - \"*/ns1\" - \"*/ns2\" - \"*/ns3\" configMode: BRIDGED And the associated security settings for all proxy workloads in the group","title":"Security Setting"},{"content":"GitOps is a practice that uses Git repositories as a source of truth for application/system state. Changes to the state are performed through Pull Request (PR) and approval workflow and will be automatically applied to the system by a CD process. This is illustrated in the following image.\nThere are three core practices in GitOps:\nInfrastructure-as-Code This describes the practice of keeping all infrastructure and application configurations stored as code in Git.\nUsing Pull Requests for Changes …","relpermalink":"/tsb/knowledge-base/gitops/","summary":"GitOps is a practice that uses Git repositories as a source of truth for application/system state. Changes to the state are performed through Pull Request (PR) and approval workflow and will be automatically applied to the system by a CD process. This is illustrated in the following image.\nThere are three core practices in GitOps:\nInfrastructure-as-Code This describes the practice of keeping all infrastructure and application configurations stored as code in Git.\nUsing Pull Requests for Changes Changes are proposed on a branch, and a PR is made to merge the changes into the main branch. Using PRs allows for collaboration between operations engineers for peer review along with the development teams, security teams, and other stakeholders.","title":"Service Mesh and GitOps"},{"content":"import { PanelContent, PanelContentCode, } from “@theme/Panel”;\nService to map registered services to configuration groups.\nLookup The Lookup API allows resolving the groups that configure a particular service in the registry. It allows lookups given a service, but also reverse lookups to get all the services in the registry that are configured by a particular workspace or group.\nGroups rpc Groups (tetrateio.api.tsb.registry.v2.GroupLookupRequest) returns …","relpermalink":"/tsb/refs/tsb/registry/v2/lookup-service/","summary":"import { PanelContent, PanelContentCode, } from “@theme/Panel”;\nService to map registered services to configuration groups.\nLookup The Lookup API allows resolving the groups that configure a particular service in the registry. It allows lookups given a service, but also reverse lookups to get all the services in the registry that are configured by a particular workspace or group.\nGroups rpc Groups (tetrateio.api.tsb.registry.v2.GroupLookupRequest) returns (tetrateio.api.tsb.registry.v2.GroupLookupResponse)\nRequires ReadTrafficGroup, ReadSecurityGroup, ReadGatewayGroup, ReadIstioInternalGroup\nGet all the groups that configure the given service in the registry.\nServices rpc Services (tetrateio.api.tsb.registry.v2.ServiceLookupRequest) returns (tetrateio.api.tsb.registry.v2.ServiceLookupResponse)\nRequires ReadRegisteredService\nGet all the services in the registry that are part of the given selector.","title":"Service Registry Lookup Service"},{"content":" Service Routes can be used by service owners to configure traffic shifting across different versions of a service in a Traffic Group. The traffic to this service can originate from sidecars in the same or different traffic groups, as well as gateways.\nThe following example yaml defines a Traffic Group g1 in the namespaces ns1, ns2 and ns3, owned by its parent Workspace w1. Then it defines a Service Route for the reviews service in the ns1 namespace with two subsets: v1 and v2, where 80% of the …","relpermalink":"/tsb/refs/tsb/traffic/v2/service-route/","summary":"Service Routes can be used by service owners to configure traffic shifting across different versions of a service in a Traffic Group. The traffic to this service can originate from sidecars in the same or different traffic groups, as well as gateways.\nThe following example yaml defines a Traffic Group g1 in the namespaces ns1, ns2 and ns3, owned by its parent Workspace w1. Then it defines a Service Route for the reviews service in the ns1 namespace with two subsets: v1 and v2, where 80% of the traffic to the reviews service is sent to v1 while the remaining 20% is sent to v2.","title":"Service Route"},{"content":" ServiceSecuritySetting allows configuring security related properties such as TLS authentication and access control for traffic arriving at a particular service in a security group. These settings will replace the security group wide settings for this service.\nThe following example defines a security setting that applies to the service foo in namespace ns1 that only allows mutual TLS authenticated traffic from other proxy workloads in the same group.\napiVersion: security.tsb.tetrate.io/v2 kind: …","relpermalink":"/tsb/refs/tsb/security/v2/service-security-setting/","summary":"ServiceSecuritySetting allows configuring security related properties such as TLS authentication and access control for traffic arriving at a particular service in a security group. These settings will replace the security group wide settings for this service.\nThe following example defines a security setting that applies to the service foo in namespace ns1 that only allows mutual TLS authenticated traffic from other proxy workloads in the same group.\napiVersion: security.tsb.tetrate.io/v2 kind: ServiceSecuritySetting metadata: name: foo-auth group: sg1 workspace: w1 tenant: mycompany org: myorg spec: service: ns1/foo.ns1.svc.cluster.local settings: authentication: REQUIRED authorization: mode: GROUP The following example customizes the Extensions to enable the execution of the WasmExtensions list specified, detailing custom properties for the execution of each extension.","title":"Service Security Setting"},{"content":"import { PanelContent, PanelContentCode, } from “@theme/Panel”;\nService to manage registration of services in the TSB Service Registry.\nRegistration The service registration API allows to manage the services that exist in the catalog. It exposes methods to register and unregister individual services as well as methods to keep all the services in a given cluster in sync.\nListServices rpc ListServices (tetrateio.api.tsb.registry.v2.ListServicesRequest) returns …","relpermalink":"/tsb/refs/tsb/registry/v2/registration-service/","summary":"import { PanelContent, PanelContentCode, } from “@theme/Panel”;\nService to manage registration of services in the TSB Service Registry.\nRegistration The service registration API allows to manage the services that exist in the catalog. It exposes methods to register and unregister individual services as well as methods to keep all the services in a given cluster in sync.\nListServices rpc ListServices (tetrateio.api.tsb.registry.v2.ListServicesRequest) returns (tetrateio.api.tsb.registry.v2.ListServicesResponse)\nList the services that have been registered in an organization\nGetService rpc GetService (tetrateio.api.tsb.registry.v2.GetServiceRequest) returns (tetrateio.api.tsb.registry.v2.Service)\nRequires ReadRegisteredService\nGet the details of a registered service\nGetServiceRequest Request to retrieve a registered service.\nField Description Validation Rule fqn","title":"Servive Registry Registration Service"},{"content":"This document describes how to set up your environment so that your VMs are ready to be onboarded using the Workload Onboarding Agent.\nThe setup for Workload Onboarding consists of the following steps:\nEnable Workload Onboarding Create the WorkloadGroup Allow the workloads to join WorkloadGroup Create the Sidecar configuration Install the Workload Onboarding Agent on a VM Enable Workload Onboarding To enable Workload Onboarding in a given Kubernetes Cluster, you need to edit TSB ControlPlane …","relpermalink":"/tsb/setup/workload-onboarding/guides/setup/","summary":"This document describes how to set up your environment so that your VMs are ready to be onboarded using the Workload Onboarding Agent.\nThe setup for Workload Onboarding consists of the following steps:\nEnable Workload Onboarding Create the WorkloadGroup Allow the workloads to join WorkloadGroup Create the Sidecar configuration Install the Workload Onboarding Agent on a VM Enable Workload Onboarding To enable Workload Onboarding in a given Kubernetes Cluster, you need to edit TSB ControlPlane resource or Helm configuration as follows:\nspec: ... meshExpansion: onboarding: # (1) REQUIRED endpoint: hosts: - \u003conboarding-endpoint-dns-name\u003e # (2) REQUIRED secretName: \u003conboarding-endpoint-tls-cert\u003e # (3) REQUIRED tokenIssuer: jwt: expiration: \u003conboarding-token-expiration-time\u003e # (4) OPTIONAL localRepository: {} # (5) OPTIONAL And then:","title":"Setting Up Workload Onboarding"},{"content":":::note By default OAP in the Control Plane does not expose RED metrics. To expose RED telemetry, set environment variable SW_EXPORTER_ENABLE_OC=true when starting OAP. :::\nTSB provides a single Prometheus-compatible endpoint to expose sidecar-originated RED application metrics via the OAP service. Each control plane cluster exposes a Prometheus-scraping endpoint to query with the following command:\nkubectl port-forward -n \u0026lt;controlplane-namespace\u0026gt; svc/oap 1234:1234 \u0026amp; curl localhost:1234/metrics …","relpermalink":"/tsb/operations/telemetry/red-metrics/","summary":":::note By default OAP in the Control Plane does not expose RED metrics. To expose RED telemetry, set environment variable SW_EXPORTER_ENABLE_OC=true when starting OAP. :::\nTSB provides a single Prometheus-compatible endpoint to expose sidecar-originated RED application metrics via the OAP service. Each control plane cluster exposes a Prometheus-scraping endpoint to query with the following command:\nkubectl port-forward -n \u003ccontrolplane-namespace\u003e svc/oap 1234:1234 \u0026 curl localhost:1234/metrics Exported RED metrics include:\nRequest status codes # HELP tsb_oap_service_status_code The number of status code # TYPE tsb_oap_service_status_code counter tsb_oap_service_status_code{status=\"\u003cSTATUS|ALL\u003e\",svc=\"SERVICE_NAME\",} COUNT Request latency # HELP tsb_oap_service_latency_sum The sum of latency # TYPE tsb_oap_service_latency_sum counter tsb_oap_service_latency_sum{svc=\"SERVICE_NAME\",} SUM # HELP tsb_oap_service_latency_count The number of requests # TYPE tsb_oap_service_latency_count counter tsb_oap_service_latency_count{svc=\"SERVICE_NAME\",} COUNT ","title":"Sidecar RED Metrics"},{"content":" Source describes a set of observed resources that have a group of metrics that emit measurements at runtime. A source specifies what is being observed (which resource types: service, ingress hostnames, relation, …) and how it is being observed (with which scope of observation).\nA telemetry source can observe different types of resources in a single or aggregated way depending on the defined scope. A scope can be of type ServiceScope, IngressScope, or RelationScope, and they define the wingspan …","relpermalink":"/tsb/refs/tsb/observability/telemetry/v2/source/","summary":"Source describes a set of observed resources that have a group of metrics that emit measurements at runtime. A source specifies what is being observed (which resource types: service, ingress hostnames, relation, …) and how it is being observed (with which scope of observation).\nA telemetry source can observe different types of resources in a single or aggregated way depending on the defined scope. A scope can be of type ServiceScope, IngressScope, or RelationScope, and they define the wingspan of the telemetry source in the mesh. Each scope contains information to determine if it is a single standalone source or an aggregation of standalone sources of the same type.","title":"Source"},{"content":" Each resource in TSB is able to provide a status to let the user know it’s current integrity. Some resources, like configurations for ingress, traffic and security, are not immediately applied as soon as TSB accepts any modification from user. In these cases, the status will provide enough information to know when it is really applying to the affected workloads. This allows any user or CI/CD process to poll the status of any desired resource and proceed accordingly.\nThere are two types of …","relpermalink":"/tsb/refs/tsb/v2/status/","summary":"Each resource in TSB is able to provide a status to let the user know it’s current integrity. Some resources, like configurations for ingress, traffic and security, are not immediately applied as soon as TSB accepts any modification from user. In these cases, the status will provide enough information to know when it is really applying to the affected workloads. This allows any user or CI/CD process to poll the status of any desired resource and proceed accordingly.\nThere are two types of resources, the ones that aggregate the status of children resources and the ones that do not. Check the documentation for the different details object types for further information.","title":"Status"},{"content":"import { PanelContent, PanelContentCode, } from “@theme/Panel”;\nService to retrieve the status for TSB resources\nStatus The Status services exposes methods to retrieve the status for any resource managed by TSB.\nGetStatus rpc GetStatus (tetrateio.api.tsb.v2.GetStatusRequest) returns (tetrateio.api.tsb.v2.ResourceStatus)\nGiven a resource fully-qualified name of a resource returns its current status.\nGetStatusRequest Request to retrieve the status of a resource.\nField Description Validation Rule …","relpermalink":"/tsb/refs/tsb/v2/status-service/","summary":"import { PanelContent, PanelContentCode, } from “@theme/Panel”;\nService to retrieve the status for TSB resources\nStatus The Status services exposes methods to retrieve the status for any resource managed by TSB.\nGetStatus rpc GetStatus (tetrateio.api.tsb.v2.GetStatusRequest) returns (tetrateio.api.tsb.v2.ResourceStatus)\nGiven a resource fully-qualified name of a resource returns its current status.\nGetStatusRequest Request to retrieve the status of a resource.\nField Description Validation Rule fqn\nstring REQUIRED Fully-qualified name of the resource to retrieve the status.\nstring = { min_len: 1}","title":"Status Service"},{"content":":::warning Alpha Feature Streaming service logs is an Alpha feature and is not recommended for production usage. :::\nTSB has the feature to view service logs directly from the TSB UI. Using this feature you will be able to view near real time logs from applications and sidecars for troubleshooting.\n:::note Log Storage TSB DOES NOT store any of your logs in a storage system. Logs are streamed directly from Clusters to Management Plane. :::\nManagement Plane To enable streaming service logs in the …","relpermalink":"/tsb/operations/features/streaming-log/","summary":":::warning Alpha Feature Streaming service logs is an Alpha feature and is not recommended for production usage. :::\nTSB has the feature to view service logs directly from the TSB UI. Using this feature you will be able to view near real time logs from applications and sidecars for troubleshooting.\n:::note Log Storage TSB DOES NOT store any of your logs in a storage system. Logs are streamed directly from Clusters to Management Plane. :::\nManagement Plane To enable streaming service logs in the Management Plane, add streamingLogEnabled: true under oap components in your ManagementPlane CR or Helm values then apply.","title":"Streaming Service Logs"},{"content":"tctl tctl\nSynopsis\nTetrate Service Bridge CLI\nOptions\n-c, --config string Path to the config file to use. Can also be specified via TCTL_CONFIG env variable. This flag takes precedence over the env variable. -p, --profile string Use specific profile (default \u0026#34;default\u0026#34;) --debug Print debug messages for all requests and responses --disable-tctl-version-warn If set, disable the outdated tctl version warning. Can also be specified via TCTL_DISABLE_VERSION_WARN env variable. -h, --help help for tctl ","relpermalink":"/tsb/reference/cli/reference/tctl/","summary":"tctl tctl\nSynopsis\nTetrate Service Bridge CLI\nOptions\n-c, --config string Path to the config file to use. Can also be specified via TCTL_CONFIG env variable. This flag takes precedence over the env variable. -p, --profile string Use specific profile (default \"default\") --debug Print debug messages for all requests and responses --disable-tctl-version-warn If set, disable the outdated tctl version warning. Can also be specified via TCTL_DISABLE_VERSION_WARN env variable. -h, --help help for tctl ","title":"tctl"},{"content":"tctl apply Apply a configuration to a resource by filename or stdin\ntctl apply [flags] Examples\ntctl apply -f config.yaml Options\n-f, --file string File or directory containing configuration to apply [required] -h, --help help for apply -o, --output-type string Response output type: table, yaml, json Options inherited from parent commands\n-c, --config string Path to the config file to use. Can also be specified via TCTL_CONFIG env variable. This flag takes precedence over the env variable. …","relpermalink":"/tsb/reference/cli/reference/apply/","summary":"tctl apply Apply a configuration to a resource by filename or stdin\ntctl apply [flags] Examples\ntctl apply -f config.yaml Options\n-f, --file string File or directory containing configuration to apply [required] -h, --help help for apply -o, --output-type string Response output type: table, yaml, json Options inherited from parent commands\n-c, --config string Path to the config file to use. Can also be specified via TCTL_CONFIG env variable. This flag takes precedence over the env variable. --debug Print debug messages for all requests and responses --disable-tctl-version-warn If set, disable the outdated tctl version warning. Can also be specified via TCTL_DISABLE_VERSION_WARN env variable.","title":"tctl apply"},{"content":"tctl collect Collect the state of a Kubernetes cluster for debugging.\ntctl collect [flags] Examples\n# Collect without any obfuscation or redaction tctl collect # Collect without archiving results (useful for local debugging) tctl collect --disable-archive # Collect and redact with user-provided regex tctl collect --redact-regexes \u0026lt;regex-one\u0026gt;,\u0026lt;regex-two\u0026gt; # Collect and redact with presets tctl collect --redact-presets networking Options\n--disable-archive output files rather than tarball -h, --help …","relpermalink":"/tsb/reference/cli/reference/collect/","summary":"tctl collect Collect the state of a Kubernetes cluster for debugging.\ntctl collect [flags] Examples\n# Collect without any obfuscation or redaction tctl collect # Collect without archiving results (useful for local debugging) tctl collect --disable-archive # Collect and redact with user-provided regex tctl collect --redact-regexes \u003cregex-one\u003e,\u003cregex-two\u003e # Collect and redact with presets tctl collect --redact-presets networking Options\n--disable-archive output files rather than tarball -h, --help help for collect -o, --output-directory string the path to write the collected files under (default \"tctl-[timestamp]\") --redact-presets strings Comma-separated list of redaction presets to use in collection data obfuscation. Available presets: - \"networking\": Obfuscate any data that matches IPv4 or IPv6 addresses.","title":"tctl collect"},{"content":"tctl completion Generates tab completion scripts\ntctl completion \u0026lt;bash|zsh|fish|powershell\u0026gt; Examples\nBash: $ source \u0026lt;(tctl completion bash) # To load completions for each session, execute once: Linux: $ tctl completion bash | sudo tee -a /etc/bash_completion.d/tctl \u0026gt; /dev/null MacOS: $ tctl completion bash | sudo tee -a $(brew --prefix)/etc/bash_completion.d/tctl \u0026gt; /dev/null Zsh: # If shell completion is not already enabled in your environment you will need # to enable it. You can execute the …","relpermalink":"/tsb/reference/cli/reference/completion/","summary":"tctl completion Generates tab completion scripts\ntctl completion \u003cbash|zsh|fish|powershell\u003e Examples\nBash: $ source \u003c(tctl completion bash) # To load completions for each session, execute once: Linux: $ tctl completion bash | sudo tee -a /etc/bash_completion.d/tctl \u003e /dev/null MacOS: $ tctl completion bash | sudo tee -a $(brew --prefix)/etc/bash_completion.d/tctl \u003e /dev/null Zsh: # If shell completion is not already enabled in your environment you will need # to enable it. You can execute the following once: $ echo \"autoload -U compinit; compinit\" \u003e\u003e ~/.zshrc # To load completions for each session, execute once: $ tctl completion zsh \u003e \"${fpath[1]}/_tctl\" # You will need to start a new shell for this setup to take effect.","title":"tctl completion"},{"content":"tctl config Manages CLI configuration.\nOptions\n-h, --help help for config Options inherited from parent commands\n-c, --config string Path to the config file to use. Can also be specified via TCTL_CONFIG env variable. This flag takes precedence over the env variable. --debug Print debug messages for all requests and responses --disable-tctl-version-warn If set, disable the outdated tctl version warning. Can also be specified via TCTL_DISABLE_VERSION_WARN env variable. -p, --profile string Use …","relpermalink":"/tsb/reference/cli/reference/config/","summary":"tctl config Manages CLI configuration.\nOptions\n-h, --help help for config Options inherited from parent commands\n-c, --config string Path to the config file to use. Can also be specified via TCTL_CONFIG env variable. This flag takes precedence over the env variable. --debug Print debug messages for all requests and responses --disable-tctl-version-warn If set, disable the outdated tctl version warning. Can also be specified via TCTL_DISABLE_VERSION_WARN env variable. -p, --profile string Use specific profile (default \"default\") tctl config clusters Manages configuration of clusters\nOptions\n-h, --help help for clusters Options inherited from parent commands\n-c, --config string Path to the config file to use.","title":"tctl config"},{"content":"tctl delete Delete an object\ntctl delete [\u0026lt;apiVersion/kind\u0026gt; \u0026lt;name\u0026gt;] [flags] Examples\n# Delete a cluster using the apiVersion/Kind pattern tctl delete api.tsb.tetrate.io/v2/Cluster my-cluster # Delete a single workspace using the short form tctl delete ws my-workspace These are the available short forms: aab\tApplicationAccessBindings ab\tAccessBindings ap\tAuthorizationPolicy apiab\tAPIAccessBindings app\tApplication cs\tCluster dr\tDestinationRule ef\tEnvoyFilter eg\tEgressGateway gab …","relpermalink":"/tsb/reference/cli/reference/delete/","summary":"tctl delete Delete an object\ntctl delete [\u003capiVersion/kind\u003e \u003cname\u003e] [flags] Examples\n# Delete a cluster using the apiVersion/Kind pattern tctl delete api.tsb.tetrate.io/v2/Cluster my-cluster # Delete a single workspace using the short form tctl delete ws my-workspace These are the available short forms: aab\tApplicationAccessBindings ab\tAccessBindings ap\tAuthorizationPolicy apiab\tAPIAccessBindings app\tApplication cs\tCluster dr\tDestinationRule ef\tEnvoyFilter eg\tEgressGateway gab\tGatewayAccessBindings gg\tGatewayGroup gw\tnetworking.istio.io/v1beta1/Gateway gwt\tgateway.tsb.tetrate.io/v2/Gateway iab\tIstioInternalAccessBindings ig\tIngressGateway iig\tIstioInternalGroup oab\tOrganizationAccessBindings org\tOrganization os\tOrganizationSetting otm\tMetric ots\tSource pa\tPeerAuthentication ra\tRequestAuthentication sa\tServiceAccount sab\tSecurityAccessBindings sd\tSidecar se\tServiceEntry sg\tSecurityGroup sr\tServiceRoute ss\tSecuritySetting sss\tServiceSecuritySetting svc\tService t1\tTier1Gateway tab\tTrafficAccessBindings tg\tTrafficGroup tnab\tTenantAccessBindings tns\tTenantSetting ts\tTrafficSetting vs\tVirtualService wab\tWorkspaceAccessBindings wext\tWasmExtension wp\tWasmPlugin ws\tWorkspace wss\tWorkspaceSetting For API version and kind, please refer to: https://docs.","title":"tctl delete"},{"content":"tctl edit Edit one or multiple objects\ntctl edit \u0026lt;apiVersion/kind | kind | shortform\u0026gt; [\u0026lt;name\u0026gt;] [flags] Examples\nEdit will perform a get on the given object and launch $EDITOR (environment variable needs to be set) for editing it then apply the changes back. # Edit a workspace. tctl edit workspace foo # Edit a tenant tctl edit tenant my-department # Edit an IngressGateway tctl edit ingressgateway myIng --workspace foo --gatewaygroup bar # You can also edit lists of objects # Edit multiple gateway …","relpermalink":"/tsb/reference/cli/reference/edit/","summary":"tctl edit Edit one or multiple objects\ntctl edit \u003capiVersion/kind | kind | shortform\u003e [\u003cname\u003e] [flags] Examples\nEdit will perform a get on the given object and launch $EDITOR (environment variable needs to be set) for editing it then apply the changes back. # Edit a workspace. tctl edit workspace foo # Edit a tenant tctl edit tenant my-department # Edit an IngressGateway tctl edit ingressgateway myIng --workspace foo --gatewaygroup bar # You can also edit lists of objects # Edit multiple gateway groups at once tctl edit gatewaygroup --workspace foo --gatewaygroup bar baz # Or even all workspaces at once tctl edit workspace --tenant foo These are the available short forms: aab\tApplicationAccessBindings ab\tAccessBindings ap\tAuthorizationPolicy apiab\tAPIAccessBindings app\tApplication cs\tCluster dr\tDestinationRule ef\tEnvoyFilter eg\tEgressGateway gab\tGatewayAccessBindings gg\tGatewayGroup gw\tnetworking.","title":"tctl edit"},{"content":"tctl experimental Experimental commands that may be modified or deprecated\nOptions\n-h, --help help for experimental Options inherited from parent commands\n-c, --config string Path to the config file to use. Can also be specified via TCTL_CONFIG env variable. This flag takes precedence over the env variable. --debug Print debug messages for all requests and responses --disable-tctl-version-warn If set, disable the outdated tctl version warning. Can also be specified via TCTL_DISABLE_VERSION_WARN …","relpermalink":"/tsb/reference/cli/reference/experimental/","summary":"tctl experimental Experimental commands that may be modified or deprecated\nOptions\n-h, --help help for experimental Options inherited from parent commands\n-c, --config string Path to the config file to use. Can also be specified via TCTL_CONFIG env variable. This flag takes precedence over the env variable. --debug Print debug messages for all requests and responses --disable-tctl-version-warn If set, disable the outdated tctl version warning. Can also be specified via TCTL_DISABLE_VERSION_WARN env variable. -p, --profile string Use specific profile (default \"default\") tctl experimental app-ingress Run a Istio based Ingress Controller for your application\nSynopsis\nInstall a dedicated Ingress Controller in your environment to allow incoming/ingress traffic to be routed to your application.","title":"tctl experimental"},{"content":"tctl get Get one or multiple objects\ntctl get \u0026lt;apiVersion/kind | kind | shortform\u0026gt; [\u0026lt;name\u0026gt;] [flags] Examples\n# List tenants using the apiVersion/Kind pattern tctl get api.tsb.tetrate.io/v2/Tenant # List workspaces using the kind tctl get workspace # Get a single workspace using the short form tctl get ws my-workspace # List gateway groups of a workspace tctl get --workspace my-workspace GatewayGroup # Get the access bindings of an ingress gateway tctl get accessbindings …","relpermalink":"/tsb/reference/cli/reference/get/","summary":"tctl get Get one or multiple objects\ntctl get \u003capiVersion/kind | kind | shortform\u003e [\u003cname\u003e] [flags] Examples\n# List tenants using the apiVersion/Kind pattern tctl get api.tsb.tetrate.io/v2/Tenant # List workspaces using the kind tctl get workspace # Get a single workspace using the short form tctl get ws my-workspace # List gateway groups of a workspace tctl get --workspace my-workspace GatewayGroup # Get the access bindings of an ingress gateway tctl get accessbindings organizations/foo/tenants/foo/workspaces/foo/gatewaygroups/foo/ingressgateways/foo # Get all resources within a tenant tctl get all --tenant foo # Get all resources within a workspace tctl get all --tenant foo --workspace bar # Get all resources within a given group tctl get all --tenant foo --workspace bar --gatewaygroup baz # Get all resources within a tenant, referrencing a given FQDN tctl get all --tenant foo --fqdn some.","title":"tctl get"},{"content":"tctl install Generates install manifests and applies it to a cluster\nOptions\n-h, --help help for install Options inherited from parent commands\n-c, --config string Path to the config file to use. Can also be specified via TCTL_CONFIG env variable. This flag takes precedence over the env variable. --debug Print debug messages for all requests and responses --disable-tctl-version-warn If set, disable the outdated tctl version warning. Can also be specified via TCTL_DISABLE_VERSION_WARN env …","relpermalink":"/tsb/reference/cli/reference/install/","summary":"tctl install Generates install manifests and applies it to a cluster\nOptions\n-h, --help help for install Options inherited from parent commands\n-c, --config string Path to the config file to use. Can also be specified via TCTL_CONFIG env variable. This flag takes precedence over the env variable. --debug Print debug messages for all requests and responses --disable-tctl-version-warn If set, disable the outdated tctl version warning. Can also be specified via TCTL_DISABLE_VERSION_WARN env variable. -p, --profile string Use specific profile (default \"default\") tctl install cluster-certs Generate cluster certs for securely communicating with the management plane\ntctl install cluster-certs [flags] Examples","title":"tctl install"},{"content":"tctl login Configures the credentials for the given user\nSynopsis\nConfigures the credentials for the given user.\nThis command will exchange the given credentials for an access token that can be stored in the configuration profile. If the credentials are not provided as arguments to the login command, an interactive prompt will ask for all required information.\nOrganization and Tenant can also be configured with the following environment variables:\nTCTL_LOGIN_ORG TCTL_LOGIN_TENANT Both password …","relpermalink":"/tsb/reference/cli/reference/login/","summary":"tctl login Configures the credentials for the given user\nSynopsis\nConfigures the credentials for the given user.\nThis command will exchange the given credentials for an access token that can be stored in the configuration profile. If the credentials are not provided as arguments to the login command, an interactive prompt will ask for all required information.\nOrganization and Tenant can also be configured with the following environment variables:\nTCTL_LOGIN_ORG TCTL_LOGIN_TENANT Both password based and OpenID Connect based authentication are supported. Depending on the configured authentication server there are several different authentication flags.\nWhen password based authentication is configured, both –username and –password are required flags.","title":"tctl login"},{"content":"tctl ui Opens the TSB console in the browser\ntctl ui [flags] Examples\ntctl ui Options\n-h, --help help for ui Options inherited from parent commands\n-c, --config string Path to the config file to use. Can also be specified via TCTL_CONFIG env variable. This flag takes precedence over the env variable. --debug Print debug messages for all requests and responses --disable-tctl-version-warn If set, disable the outdated tctl version warning. Can also be specified via TCTL_DISABLE_VERSION_WARN env …","relpermalink":"/tsb/reference/cli/reference/ui/","summary":"tctl ui Opens the TSB console in the browser\ntctl ui [flags] Examples\ntctl ui Options\n-h, --help help for ui Options inherited from parent commands\n-c, --config string Path to the config file to use. Can also be specified via TCTL_CONFIG env variable. This flag takes precedence over the env variable. --debug Print debug messages for all requests and responses --disable-tctl-version-warn If set, disable the outdated tctl version warning. Can also be specified via TCTL_DISABLE_VERSION_WARN env variable. -p, --profile string Use specific profile (default \"default\") ","title":"tctl ui"},{"content":"tctl validate Offline validates a configuration from filename or stdin\ntctl validate [flags] Examples\ntctl validate -f config.yaml This syntactically validates the provided config and verifies the config of the defined selectors. Options\n-f, --file string File or directory containing configuration to validate [required] -h, --help help for validate Options inherited from parent commands\n-c, --config string Path to the config file to use. Can also be specified via TCTL_CONFIG env variable. This …","relpermalink":"/tsb/reference/cli/reference/validate/","summary":"tctl validate Offline validates a configuration from filename or stdin\ntctl validate [flags] Examples\ntctl validate -f config.yaml This syntactically validates the provided config and verifies the config of the defined selectors. Options\n-f, --file string File or directory containing configuration to validate [required] -h, --help help for validate Options inherited from parent commands\n-c, --config string Path to the config file to use. Can also be specified via TCTL_CONFIG env variable. This flag takes precedence over the env variable. --debug Print debug messages for all requests and responses --disable-tctl-version-warn If set, disable the outdated tctl version warning. Can also be specified via TCTL_DISABLE_VERSION_WARN env variable.","title":"tctl validate"},{"content":"tctl version Show the version of tctl and TSB\ntctl version [flags] Options\n--ascii Display the ASCII art for the TSB release -h, --help help for version --local-only If true, shows client version only (no server required) Options inherited from parent commands\n-c, --config string Path to the config file to use. Can also be specified via TCTL_CONFIG env variable. This flag takes precedence over the env variable. --debug Print debug messages for all requests and responses …","relpermalink":"/tsb/reference/cli/reference/version/","summary":"tctl version Show the version of tctl and TSB\ntctl version [flags] Options\n--ascii Display the ASCII art for the TSB release -h, --help help for version --local-only If true, shows client version only (no server required) Options inherited from parent commands\n-c, --config string Path to the config file to use. Can also be specified via TCTL_CONFIG env variable. This flag takes precedence over the env variable. --debug Print debug messages for all requests and responses --disable-tctl-version-warn If set, disable the outdated tctl version warning. Can also be specified via TCTL_DISABLE_VERSION_WARN env variable. -p, --profile string Use specific profile (default \"default\") ","title":"tctl version"},{"content":"tctl whoami Show the current user info\ntctl whoami [flags] Options\n-h, --help help for whoami -o, --output-type string Response output type: table, yaml, json (default \u0026#34;table\u0026#34;) Options inherited from parent commands\n-c, --config string Path to the config file to use. Can also be specified via TCTL_CONFIG env variable. This flag takes precedence over the env variable. --debug Print debug messages for all requests and responses --disable-tctl-version-warn If set, disable the outdated tctl version …","relpermalink":"/tsb/reference/cli/reference/whoami/","summary":"tctl whoami Show the current user info\ntctl whoami [flags] Options\n-h, --help help for whoami -o, --output-type string Response output type: table, yaml, json (default \"table\") Options inherited from parent commands\n-c, --config string Path to the config file to use. Can also be specified via TCTL_CONFIG env variable. This flag takes precedence over the env variable. --debug Print debug messages for all requests and responses --disable-tctl-version-warn If set, disable the outdated tctl version warning. Can also be specified via TCTL_DISABLE_VERSION_WARN env variable. -p, --profile string Use specific profile (default \"default\") ","title":"tctl whoami"},{"content":" User represents a user that has been loaded from a configured Identity Provider (IdP) that can log into the platform. Currently, users are automatically synchronized by TSB from a configured LDAP server.\nThe following example creates a user named john under the organization myorg.\napiVersion: api.tsb.tetrate.io/v2 kind: User metadata: name: john organization: myorg spec: loginName: john firstName: John lastName: Doe displayName: John Doe email: john.doe@acme.com ServiceAccount can be created to …","relpermalink":"/tsb/refs/tsb/v2/team/","summary":"User represents a user that has been loaded from a configured Identity Provider (IdP) that can log into the platform. Currently, users are automatically synchronized by TSB from a configured LDAP server.\nThe following example creates a user named john under the organization myorg.\napiVersion: api.tsb.tetrate.io/v2 kind: User metadata: name: john organization: myorg spec: loginName: john firstName: John lastName: Doe displayName: John Doe email: john.doe@acme.com ServiceAccount can be created to leverage machine authentication via JWT tokens. Each service account has a key-pair that can be used to create signed JWT tokens that can be used to authenticate to TSB.","title":"Teams and Users"},{"content":"import { PanelContent, PanelContentCode, } from “@theme/Panel”;\nService to manage Users and Teams in TSB\nTeams The Teams service provides methods to manage the Users and Teams that exist in an Organization.\nUsers and Teams are periodically synchronized from the Identity Provider (IdP) configured for the Organization, but TSB allows creating local teams to provide extended flexibility in how Users and Teams are grouped, and to provide a comprehensive way of creating more fine-grained access …","relpermalink":"/tsb/refs/tsb/v2/team-service/","summary":"import { PanelContent, PanelContentCode, } from “@theme/Panel”;\nService to manage Users and Teams in TSB\nTeams The Teams service provides methods to manage the Users and Teams that exist in an Organization.\nUsers and Teams are periodically synchronized from the Identity Provider (IdP) configured for the Organization, but TSB allows creating local teams to provide extended flexibility in how Users and Teams are grouped, and to provide a comprehensive way of creating more fine-grained access control policies.\nGetUser rpc GetUser (tetrateio.api.tsb.v2.GetUserRequest) returns (tetrateio.api.tsb.v2.User)\nRequires READ\nGet the details of an existing user.\nListUsers rpc ListUsers (tetrateio.api.tsb.v2.ListUsersRequest) returns (tetrateio.api.tsb.v2.ListUsersResponse)\nList existing users.","title":"Teams Service"},{"content":":::note This page details how to collect telemetry necessary for operating Tetrate Service Bridge, not applications managed by Tetrate Service Bridge. :::\nTetrate Service Bridge uses the Open Telemetry Collector to simplify metrics collection. A standard deployment includes one in the management plane and one alongside each onboarded control plane. Using the Collector enables Tetrate Service Bridge to simplify telemetry collection per cluster by only requiring operators to scrape a single …","relpermalink":"/tsb/operations/telemetry/telemetry-architecture/","summary":":::note This page details how to collect telemetry necessary for operating Tetrate Service Bridge, not applications managed by Tetrate Service Bridge. :::\nTetrate Service Bridge uses the Open Telemetry Collector to simplify metrics collection. A standard deployment includes one in the management plane and one alongside each onboarded control plane. Using the Collector enables Tetrate Service Bridge to simplify telemetry collection per cluster by only requiring operators to scrape a single component, rather than all components.\nManagement Plane In the management plane there is a component called the collector. It is an aggregator that exposes a single endpoint to scrape all management plane components using Prometheus.","title":"Telemetry Architecture"},{"content":"import { PanelContent, PanelContentCode, } from “@theme/Panel”;\nService to manage the Telemetry Sources.\nSources The Sources service exposes methods to manage telemetry sources from resources.\nGetSource rpc GetSource (tetrateio.api.tsb.observability.telemetry.v2.GetSourceRequest) returns (tetrateio.api.tsb.observability.telemetry.v2.Source)\nGet the details of an existing telemetry source.\nListSources rpc ListSources (tetrateio.api.tsb.observability.telemetry.v2.ListSourcesRequest) returns …","relpermalink":"/tsb/refs/tsb/observability/telemetry/v2/source-service/","summary":"import { PanelContent, PanelContentCode, } from “@theme/Panel”;\nService to manage the Telemetry Sources.\nSources The Sources service exposes methods to manage telemetry sources from resources.\nGetSource rpc GetSource (tetrateio.api.tsb.observability.telemetry.v2.GetSourceRequest) returns (tetrateio.api.tsb.observability.telemetry.v2.Source)\nGet the details of an existing telemetry source.\nListSources rpc ListSources (tetrateio.api.tsb.observability.telemetry.v2.ListSourcesRequest) returns (tetrateio.api.tsb.observability.telemetry.v2.ListSourcesResponse)\nList the telemetry sources that are available for the requested parent. It will return telemetry sources that belong to the requested parent and from all its child resources.\nGetSourceRequest Request to retrieve a Telemetry Sources from a parent resource.\nField Description Validation Rule fqn\nstring REQUIRED Fully-qualified name of the Telemetry Sources.\nTODO(marcnavarro): Add pagination information.","title":"Telemetry Source Service"},{"content":" Tenant is a self-contained entity within an organization in the Service Bridge object hierarchy. Tenants can be business units, organization units, or any logical grouping that matches a corporate structure.\nThe following example creates a tenant named mycompany in an organization named myorg.\napiVersion: api.tsb.tetrate.io/v2 kind: Tenant metadata: organization: myorg name: mycompany Tenant Tenant is a self-contained entity within an organization in the Service Bridge hierarchy.\nField …","relpermalink":"/tsb/refs/tsb/v2/tenant/","summary":"Tenant is a self-contained entity within an organization in the Service Bridge object hierarchy. Tenants can be business units, organization units, or any logical grouping that matches a corporate structure.\nThe following example creates a tenant named mycompany in an organization named myorg.\napiVersion: api.tsb.tetrate.io/v2 kind: Tenant metadata: organization: myorg name: mycompany Tenant Tenant is a self-contained entity within an organization in the Service Bridge hierarchy.\nField Description Validation Rule securityDomain\nstring Security domains can be used to group different resources under the same security domain. Although security domain is not resource itself currently, it follows a fqn format organizations/myorg/securitydomains/mysecuritydomain, and a child cannot override any ancestor’s security domain.","title":"Tenant"},{"content":" DEPRECATED: use Access Bindings instead.\nTenantAccessBindings is an assignment of roles to a set of users or teams to access resources under a Tenant. The user or team information is obtained from an LDAP server that should have been configured as part of Service Bridge installation. Note that a TenantAccessBinding can be created or modified only by users who have SET_POLICY permission on the Tenant.\nThe following example assigns the tenant-admin role to users alice, bob, and members of the t1 …","relpermalink":"/tsb/refs/tsb/rbac/v2/tenant-access-bindings/","summary":"DEPRECATED: use Access Bindings instead.\nTenantAccessBindings is an assignment of roles to a set of users or teams to access resources under a Tenant. The user or team information is obtained from an LDAP server that should have been configured as part of Service Bridge installation. Note that a TenantAccessBinding can be created or modified only by users who have SET_POLICY permission on the Tenant.\nThe following example assigns the tenant-admin role to users alice, bob, and members of the t1 team owned by the tenant mycompany. Use fully-qualified name (fqn) when specifying user and team\napiVersion: rbac.tsb.tetrate.io/v2 kind: TenantAccessBindings metadata: organization: myorg tenant: mycompany spec: allow: - role: rbac/tenant-admin subjects: - user: organization/myorg/users/alice - user: organization/myorg/users/bob - team: organization/myorg/teams/t1 TenantAccessBindings TenantAccessBindings assigns permissions to users of tenants.","title":"Tenant Access Bindings"},{"content":"import { PanelContent, PanelContentCode, } from “@theme/Panel”;\nService to manage TSB tenants.\nTenants The Tenant service can be used to manage the tenants in TSB. Tenants can be seen as organization units and line of business that have a set of resources. Every resource in TSB belongs to a tenant, and users can be assigned to tenants to get access to those resources (such as workspaces, traffic settings, etc). This service provides methods to manage the tenants that are available in the …","relpermalink":"/tsb/refs/tsb/v2/tenant-service/","summary":"import { PanelContent, PanelContentCode, } from “@theme/Panel”;\nService to manage TSB tenants.\nTenants The Tenant service can be used to manage the tenants in TSB. Tenants can be seen as organization units and line of business that have a set of resources. Every resource in TSB belongs to a tenant, and users can be assigned to tenants to get access to those resources (such as workspaces, traffic settings, etc). This service provides methods to manage the tenants that are available in the platform.\nCreateTenant rpc CreateTenant (tetrateio.api.tsb.v2.CreateTenantRequest) returns (tetrateio.api.tsb.v2.Tenant)\nRequires CREATE\nCreate a new tenant in the platform that will be the home for a set of resources.","title":"Tenant Service"},{"content":" Tenant Setting allows configuring default settings for the tenant.\nTraffic and Security settings can be defined as default for a tenant, meaning that they will be applied to all the workspaces of the tenant. These defaults settings can be overridden by creating proper WorkspaceSetting, TrafficSetting or SecuritySetting into the desired workspace or group.\napiVersion: api.tsb.tetrate.io/v2 kind: TenantSetting metadata: name: tenant-settings organization: myorg tenant: mytenant spec: …","relpermalink":"/tsb/refs/tsb/v2/tenant-setting/","summary":"Tenant Setting allows configuring default settings for the tenant.\nTraffic and Security settings can be defined as default for a tenant, meaning that they will be applied to all the workspaces of the tenant. These defaults settings can be overridden by creating proper WorkspaceSetting, TrafficSetting or SecuritySetting into the desired workspace or group.\napiVersion: api.tsb.tetrate.io/v2 kind: TenantSetting metadata: name: tenant-settings organization: myorg tenant: mytenant spec: defaultTrafficSetting: reachability: mode: WORKSPACE egress: host: bookinfo-perimeter/tsb-egress defaultSecuritySetting: authenticationSettings: trafficMode: REQUIRED authorization: mode: GROUP TenantSetting Default settings that apply to all workspaces under a tenant.\nField Description Validation Rule defaultSecuritySetting\ntetrateio.api.tsb.security.v2.SecuritySetting Security settings for all proxy workloads in this tenant.","title":"Tenant Setting"},{"content":" DEPRECATION: The functionality provided by the Tier1Gateway is now provided in Gateway object, and using it is the recommended approach. The Tier1Gateway resource will be removed in future releases.\nTier1Gateway configures a workload to act as a gateway that distributes traffic across one or more ingress gateways in other clusters.\nNOTE: Tier1 gateways cannot be used to route traffic to the same cluster. A cluster with tier1 gateway cannot have any other gateways or workloads.\nThe following …","relpermalink":"/tsb/refs/tsb/gateway/v2/tier1-gateway/","summary":"DEPRECATION: The functionality provided by the Tier1Gateway is now provided in Gateway object, and using it is the recommended approach. The Tier1Gateway resource will be removed in future releases.\nTier1Gateway configures a workload to act as a gateway that distributes traffic across one or more ingress gateways in other clusters.\nNOTE: Tier1 gateways cannot be used to route traffic to the same cluster. A cluster with tier1 gateway cannot have any other gateways or workloads.\nThe following example declares a tier1 gateway running on pods with app: gateway labels in the ns1 namespace. The gateway exposes host movieinfo.com on ports 8080, 8443 and kafka.","title":"Tier1 Gateway"},{"content":"A Tier1 gateway is used to distribute traffic across one or more ingress gateways (or Tier2 gateways) in other clusters using Istio mTLS. Prior to the 1.6 release, a Tier1 gateway required a dedicated cluster and could not be located with other gateways (e.g ingress gateways) or application workloads.\nSince TSB 1.6, you don’t need to provision a dedicated cluster to run a Tier1 gateway. You can deploy a Tier1 gateway in any of your application clusters. Currently this feature is disabled by …","relpermalink":"/tsb/operations/features/tier1-in-app-cluster/","summary":"A Tier1 gateway is used to distribute traffic across one or more ingress gateways (or Tier2 gateways) in other clusters using Istio mTLS. Prior to the 1.6 release, a Tier1 gateway required a dedicated cluster and could not be located with other gateways (e.g ingress gateways) or application workloads.\nSince TSB 1.6, you don’t need to provision a dedicated cluster to run a Tier1 gateway. You can deploy a Tier1 gateway in any of your application clusters. Currently this feature is disabled by default; it will be enabled by default in a future release.\nEnable Running Tier1 Gateway in App Cluster In order to deploy a Tier1 gateway gateway in an application cluster, you will first need to edit the xcp component in the ControlPlane CR or Helm values and add an environment variable DISABLE_TIER1_TIER2_SEPARATION with value true","title":"Tier1 Gateway in an App Cluster"},{"content":" DEPRECATED: use Access Bindings instead.\nTrafficAccessBindings is an assignment of roles to a set of users or teams to access resources under a Traffic group. The user or team information is obtained from an LDAP server that should have been configured as part of Service Bridge installation. Note that a TrafficAccessBinding can be created or modified only by users who have SET_POLICY permission on the Traffic group.\nThe following example assigns the traffic-admin role to users alice, bob, and …","relpermalink":"/tsb/refs/tsb/rbac/v2/traffic-access-bindings/","summary":"DEPRECATED: use Access Bindings instead.\nTrafficAccessBindings is an assignment of roles to a set of users or teams to access resources under a Traffic group. The user or team information is obtained from an LDAP server that should have been configured as part of Service Bridge installation. Note that a TrafficAccessBinding can be created or modified only by users who have SET_POLICY permission on the Traffic group.\nThe following example assigns the traffic-admin role to users alice, bob, and members of the traffic-ops team for traffic group g1 under workspace w1 owned by the tenant mycompany. Use fully-qualified name (fqn) when specifying user and team","title":"Traffic Access Bindings"},{"content":" Traffic Groups allow grouping the proxy workloads in a set of namespaces owned by its parent workspace. Networking and routing related configurations can then be applied on the group to control the behavior of these proxy workloads. The group can be in one of two modes: BRIDGED and DIRECT. BRIDGED mode is a minimalistic mode that allows users to quickly configure the most commonly used features in the service mesh using Tetrate specific APIs, while the DIRECT mode provides more flexibility for …","relpermalink":"/tsb/refs/tsb/traffic/v2/traffic-group/","summary":"Traffic Groups allow grouping the proxy workloads in a set of namespaces owned by its parent workspace. Networking and routing related configurations can then be applied on the group to control the behavior of these proxy workloads. The group can be in one of two modes: BRIDGED and DIRECT. BRIDGED mode is a minimalistic mode that allows users to quickly configure the most commonly used features in the service mesh using Tetrate specific APIs, while the DIRECT mode provides more flexibility for power users by allowing them to configure the proxy workload behavior using a restricted subset of Istio Networking APIs.","title":"Traffic Group"},{"content":"import { PanelContent, PanelContentCode, } from “@theme/Panel”;\nService to manage traffic settings.\nTraffic The Traffic service provides methods to manage traffic settings in TSB.\nIt provides methods to create and manage traffic groups within a workspace, allowing to create fine-grained groupings to configure a subset of the workspace namespaces. Access policies can be assigned at group level, providing a fine-grained access control to the traffic configuration features.\nThe Traffic service also …","relpermalink":"/tsb/refs/tsb/traffic/v2/traffic-service/","summary":"import { PanelContent, PanelContentCode, } from “@theme/Panel”;\nService to manage traffic settings.\nTraffic The Traffic service provides methods to manage traffic settings in TSB.\nIt provides methods to create and manage traffic groups within a workspace, allowing to create fine-grained groupings to configure a subset of the workspace namespaces. Access policies can be assigned at group level, providing a fine-grained access control to the traffic configuration features.\nThe Traffic service also provides methods to configure the different traffic settings that are allowed within each group.\nCreateGroup rpc CreateGroup (tetrateio.api.tsb.traffic.v2.CreateTrafficGroupRequest) returns (tetrateio.api.tsb.traffic.v2.Group)\nRequires CREATE\nCreate a new traffic group in the given workspace.","title":"Traffic Service"},{"content":" Traffic Settings allow configuring the behavior of the proxy workloads in a set of namespaces owned by a traffic group. Specifically, it allows configuring the dependencies of proxy workloads on namespaces outside the traffic group as well as reliability settings for outbound calls made by the proxy workloads to other services.\nThe following example creates a traffic group for the proxy workloads in ns1, ns2 and ns3 namespaces owned by its parent workspace w1 under tenant mycompany. It then …","relpermalink":"/tsb/refs/tsb/traffic/v2/traffic-setting/","summary":"Traffic Settings allow configuring the behavior of the proxy workloads in a set of namespaces owned by a traffic group. Specifically, it allows configuring the dependencies of proxy workloads on namespaces outside the traffic group as well as reliability settings for outbound calls made by the proxy workloads to other services.\nThe following example creates a traffic group for the proxy workloads in ns1, ns2 and ns3 namespaces owned by its parent workspace w1 under tenant mycompany. It then defines a traffic setting for the all workloads in these namespaces, adding a dependency on all the services in the shared db namespace, and forwarding all unknown traffic via the egress gateway in the istio-system namespace.","title":"Traffic Setting"},{"content":" Transport layer security config specifies configuration of a TLS client.\nClientTransportSecurity ClientTransportSecurity specifies transport layer security configuration.\nField Description Validation Rule tls\ntetrateio.api.onboarding.config.types.config.v1alpha1.TlsClient oneof kind TLS client configuration.\n–\nnone\ntetrateio.api.onboarding.config.types.config.v1alpha1.PlainTextClient oneof kind Plain-text client configuration.\n–\nTlsClient TlsClient specifies configuration of a TLS client.\nField …","relpermalink":"/tsb/refs/onboarding/config/types/config/v1alpha1/transport-security/","summary":"Transport layer security config specifies configuration of a TLS client.\nClientTransportSecurity ClientTransportSecurity specifies transport layer security configuration.\nField Description Validation Rule tls\ntetrateio.api.onboarding.config.types.config.v1alpha1.TlsClient oneof kind TLS client configuration.\n–\nnone\ntetrateio.api.onboarding.config.types.config.v1alpha1.PlainTextClient oneof kind Plain-text client configuration.\n–\nTlsClient TlsClient specifies configuration of a TLS client.\nField Description Validation Rule sni\nstring SNI string to present to the server during TLS handshake instead of the default value (host address).\nDefaults to empty string, in which case the default SNI value (host address) will be used.\nThis setting is meant for use in non-production scenarios, such as:\nwhen the server is not reachable by a DNS name (e.","title":"Transport layer security config"},{"content":"Workload fails to join the mesh If a new workload does not appear on the list of onboarded workloads, follow these steps.\nCheck status of the Workload Onboarding Agent Virtual Machine (VM) workloads On the host of the workload, e.g. on the VM, run:\nsystemctl status onboarding-agent You should get output similar to:\n● onboarding-agent.service - Workload Onboarding Agent Loaded: loaded (/usr/lib/systemd/system/onboarding-agent.service; enabled; vendor preset: disabled) Active: active (running) …","relpermalink":"/tsb/setup/workload-onboarding/guides/troubleshooting/","summary":"Workload fails to join the mesh If a new workload does not appear on the list of onboarded workloads, follow these steps.\nCheck status of the Workload Onboarding Agent Virtual Machine (VM) workloads On the host of the workload, e.g. on the VM, run:\nsystemctl status onboarding-agent You should get output similar to:\n● onboarding-agent.service - Workload Onboarding Agent Loaded: loaded (/usr/lib/systemd/system/onboarding-agent.service; enabled; vendor preset: disabled) Active: active (running) since Thu 2021-10-07 14:57:23 UTC; 1 minute ago # (1) Docs: https://tetrate.io/ Main PID: 3519 (bash) CGroup: /system.slice/onboarding-agent.service ├─3520 onboarding-agent --agent-config /etc/onboarding-agent/agent.config.yaml --onboarding-config /etc/onboarding-agent/onboarding.config.yaml If status of the onboarding-agent.service unit is not Active (1), double-check whether you followed onboarding instructions closely.","title":"Troubleshooting Guide"},{"content":"TSB master provide Kubernetes CRD that you can use to configure your applications. See the following links for more details:\nHow to enable GitOps feature How to use GitOps feature The full TSB Kubernetes CRDs can be downloaded here.\n","relpermalink":"/tsb/reference/k8s-api/guide/","summary":"TSB master provide Kubernetes CRD that you can use to configure your applications. See the following links for more details:\nHow to enable GitOps feature How to use GitOps feature The full TSB Kubernetes CRDs can be downloaded here.","title":"TSB CRD Reference"},{"content":"如何确定 Envoy 是否正常？ 确定 Envoy 是否正常的最佳方法是检查其健康和就绪端点（healthz）。要检查已加入的集群中应用程序的 Envoy 的 healthz 端点，你需要直接连接到应用程序的旁路 Envoy 边车。\n假设你在集群的 bookinfo 命名空间中有一个名为 details-v1-57f8794694-hc7gd 的 Pod，该 Pod 托管你的应用程序。\n使用 kubectl port-forward 建立本地机器到 Envoy 边车上的端口 15021 的端口转发：\nkubectl port-forward -n bookinfo details-v1-57f8794694-hc7gd 15021:15021 一旦上述命令成功执行，你现在应该能够将你喜爱的工具指向 URL http://localhost:15021/healthz/ready 并直接访问 Envoy 的 healthz 端点。请避免使用浏览器进行此操作，因为如果正确配置并运行，则 Envoy 代理将返回一个带有空主体的 200 OK 响应。\n例如，你可以使用 curl 以详细模式执行 …","relpermalink":"/tsb/knowledge-base/faq/","summary":"如何确定 Envoy 是否正常？ 确定 Envoy 是否正常的最佳方法是检查其健康和就绪端点（healthz）。要检查已加入的集群中应用程序的 Envoy 的 healthz 端点，你需要直接连接到应用程序的旁路 Envoy 边车。 假设你在集群的 bookinfo 命名空间中有一个名","title":"TSB 常见问题解答"},{"content":"TSB’s UI displays metrics and health of your services. However, if there are no metrics or traces displayed, then you may be facing an issue with either your services, or with TSB.\nThis guide will walk you through how to determine whether the issue is with a service, or with one of the metrics components within TSB.\nMetrics If you don’t see the metrics, use this section of the guide to troubleshoot.\nFirst, make sure that you have traffic flowing in your application. You need traffic to generate …","relpermalink":"/tsb/troubleshooting/tsb-ui-metrics/","summary":"TSB’s UI displays metrics and health of your services. However, if there are no metrics or traces displayed, then you may be facing an issue with either your services, or with TSB.\nThis guide will walk you through how to determine whether the issue is with a service, or with one of the metrics components within TSB.\nMetrics If you don’t see the metrics, use this section of the guide to troubleshoot.\nFirst, make sure that you have traffic flowing in your application. You need traffic to generate metrics.\nCheck that the time range window you’ve set in TSB is correct, and there was traffic during that period.","title":"UI Metrics Troubleshooting"},{"content":"TSB has a teamsync component that will periodically connect to your Identity Provider (IdP) and sync user and team information into TSB.\nCurrently teamsync supports LDAP and Azure AD, and will do The Right Thing for you automatically. However, if you are using another IdP, you will need to manually perform these tasks. This document will describe how to perform them.\nBefore you start, make sure that you have:\n✓ Installed TSB Management Plane ✓ Login to TSB with tctl with administrator account\n✓ …","relpermalink":"/tsb/operations/users/user-synchronization/","summary":"TSB has a teamsync component that will periodically connect to your Identity Provider (IdP) and sync user and team information into TSB.\nCurrently teamsync supports LDAP and Azure AD, and will do The Right Thing for you automatically. However, if you are using another IdP, you will need to manually perform these tasks. This document will describe how to perform them.\nBefore you start, make sure that you have:\n✓ Installed TSB Management Plane ✓ Login to TSB with tctl with administrator account\n✓ Get your TSB’s organization name - Make sure to use organization name configured at installation time in the TSB ManagementPlane CR.","title":"User Synchronization"},{"content":"import vars from “../_vars.json”;\nTetrate Service Bridge (TSB) is a complex collection of components that are interconnected using various protocols. This is probably also true for your applications deployed over the service mesh that TSB provides. In many cases you will need to check, test, and verify the network connectivity within the various TSB components to make sure that the system is working as expected.\nTo save you some time to create a debugging environment in the Kubernetes clusters, …","relpermalink":"/tsb/troubleshooting/debug-container/","summary":"import vars from “../_vars.json”;\nTetrate Service Bridge (TSB) is a complex collection of components that are interconnected using various protocols. This is probably also true for your applications deployed over the service mesh that TSB provides. In many cases you will need to check, test, and verify the network connectivity within the various TSB components to make sure that the system is working as expected.\nTo save you some time to create a debugging environment in the Kubernetes clusters, Tetrate provides a debug container that comes with most of the toolsets needed to validate the network status already installed. For example, tools such as ping, curl, gpcurl, dig, etc are already installed in this container.","title":"Using The Debug Container"},{"content":" The following example creates a security group for the sidecars in ns1, ns2 and ns3 namespaces owned by its parent workspace w1 under tenant mycompany, and a security setting that applies the WAF Settings. And the security group and security settings to which this WAF Settings is applied to.\napiVersion: security.tsb.tetrate.io/v2 kind: Group metadata: name: t1 workspace: w1 tenant: mycompany organization: myorg spec: namespaceSelector: names: - \u0026#34;*/ns1\u0026#34; - \u0026#34;*/ns2\u0026#34; - \u0026#34;*/ns3\u0026#34; configMode: BRIDGED …","relpermalink":"/tsb/refs/tsb/security/v2/waf-settings/","summary":"The following example creates a security group for the sidecars in ns1, ns2 and ns3 namespaces owned by its parent workspace w1 under tenant mycompany, and a security setting that applies the WAF Settings. And the security group and security settings to which this WAF Settings is applied to.\napiVersion: security.tsb.tetrate.io/v2 kind: Group metadata: name: t1 workspace: w1 tenant: mycompany organization: myorg spec: namespaceSelector: names: - \"*/ns1\" - \"*/ns2\" - \"*/ns3\" configMode: BRIDGED --- apiVersion: security.tsb.tetrate.io/v2 kind: SecuritySetting metadata: name: defaults group: t1 workspace: w1 tenant: mycompany organization: myorg spec: waf: rules: - Include @recommended-conf In the following examples, the security rule for blocking XSS requests is enabled on Tier1Gateway and IngressGateway respectively, with an ad-hoc debug configuration, instead of the one defined in the security rule.","title":"WAF Settings"},{"content":" The WASM extension resource allows defining custom WASM extensions that are packaged in OCI images. The resource allows specifying extension metadata that helps understand how extensions work and how they can be used. Once defined, extensions can be referenced in Ingress and Egress Gateways and Security Groups so that traffic is captured and processed by the extension accordingly. By default, extensions are globally available, but they can be assigned to specific Tenants as well to further …","relpermalink":"/tsb/refs/tsb/extension/v2/wasm-extension/","summary":"The WASM extension resource allows defining custom WASM extensions that are packaged in OCI images. The resource allows specifying extension metadata that helps understand how extensions work and how they can be used. Once defined, extensions can be referenced in Ingress and Egress Gateways and Security Groups so that traffic is captured and processed by the extension accordingly. By default, extensions are globally available, but they can be assigned to specific Tenants as well to further control and constraint where in the Organization the extensions are allowed to be used.\napiVersion: extension.tsb.tetrate.io/v2 kind: WasmExtension metadata: organization: org name: wasm-auth spec: allowedIn: - organizations/org/tenants/tenant1 url: oci://docker.","title":"WASM Extension"},{"content":"import { PanelContent, PanelContentCode, } from “@theme/Panel”;\nService to manage WASM extensions.\nWasmExtensions The WasmExtension service provides methods to manage the extensions inside an Organization. WasmExtensions are created inside TSB and assigned later to SecuritySettings and IngressGateways.\nGetWasmExtension rpc GetWasmExtension (tetrateio.api.tsb.extension.v2.GetWasmExtensionRequest) returns (tetrateio.api.tsb.extension.v2.WasmExtension)\nRequires READ\nGet a WASM extension …","relpermalink":"/tsb/refs/tsb/extension/v2/wasm-service/","summary":"import { PanelContent, PanelContentCode, } from “@theme/Panel”;\nService to manage WASM extensions.\nWasmExtensions The WasmExtension service provides methods to manage the extensions inside an Organization. WasmExtensions are created inside TSB and assigned later to SecuritySettings and IngressGateways.\nGetWasmExtension rpc GetWasmExtension (tetrateio.api.tsb.extension.v2.GetWasmExtensionRequest) returns (tetrateio.api.tsb.extension.v2.WasmExtension)\nRequires READ\nGet a WASM extension\nListWasmExtension rpc ListWasmExtension (tetrateio.api.tsb.extension.v2.ListWasmExtensionRequest) returns (tetrateio.api.tsb.extension.v2.ListWasmExtensionResponse)\nList the WASM extensions that are defined for the Organization.\nCreateWasmExtension rpc CreateWasmExtension (tetrateio.api.tsb.extension.v2.CreateWasmExtensionRequest) returns (tetrateio.api.tsb.extension.v2.WasmExtension)\nRequires CREATE\nCreates a new WasmExtension object in TSB. This is needed to let the extensions run. Once a WasmExtension has been created, it can be assigned to IngressGateway and SecuritySetting.","title":"WasmExtension Service"},{"content":" Workload Auto Registration represents a registry record of a workload onboarded into the mesh.\nWorkload Auto Registration captures essential information about the workload allowing Workload Onboarding Plane to generate boot configuration for the Istio Sidecar that will be started alongside this workload.\nWorkloadAutoRegistration resource is not supposed to be edited by the users. Instead, it gets created automatically as part of the Workload Onboarding flow.\nUsers can introspect …","relpermalink":"/tsb/refs/onboarding/config/runtime/v1alpha1/registration/","summary":"Workload Auto Registration represents a registry record of a workload onboarded into the mesh.\nWorkload Auto Registration captures essential information about the workload allowing Workload Onboarding Plane to generate boot configuration for the Istio Sidecar that will be started alongside this workload.\nWorkloadAutoRegistration resource is not supposed to be edited by the users. Instead, it gets created automatically as part of the Workload Onboarding flow.\nUsers can introspect WorkloadAutoRegistration resources for the purposes of observability and troubleshooting of Workload Onboarding.\nTo leverage k8s resource garbage collection (i.e. cascade removal),\nWorkloadAutoRegistration resource is owned by the WorkloadGroup resource the workload has joined to WorkloadAutoRegistration resource owns the Istio WorkloadEntry resource that describes the workload to the Istio Control Plane.","title":"Workload Auto Registration"},{"content":" WorkloadConfiguration specifies configuration of the workload handling.\nFor example,\nauthentication: jwt: issuers: - issuer: \u0026#34;https://mycompany.corp\u0026#34; jwksUri: \u0026#34;https://mycompany.corp/jwks.json\u0026#34; shortName: \u0026#34;mycorp\u0026#34; tokenFields: attributes: jsonPath: .custom_attributes deregistration: propagationDelay: 15s JwtAuthenticationConfiguration JwtAuthenticationConfiguration specifies configuration of the workload authentication by means of an OIDC ID Token.\nField Description Validation Rule issuers\nList …","relpermalink":"/tsb/refs/onboarding/config/install/v1alpha1/workload-configuration/","summary":"WorkloadConfiguration specifies configuration of the workload handling.\nFor example,\nauthentication: jwt: issuers: - issuer: \"https://mycompany.corp\" jwksUri: \"https://mycompany.corp/jwks.json\" shortName: \"mycorp\" tokenFields: attributes: jsonPath: .custom_attributes deregistration: propagationDelay: 15s JwtAuthenticationConfiguration JwtAuthenticationConfiguration specifies configuration of the workload authentication by means of an OIDC ID Token.\nField Description Validation Rule issuers\nList of tetrateio.api.onboarding.config.install.v1alpha1.JwtIssuer List of permitted JWT issuers.\nIf a workload authenticates itself by means of an OIDC ID Token, the issuer of that token must be present in this list, otherwise authentication attempt will be declined.\nrepeated = { items: {message:{required:true}}}\nWorkloadAuthenticationConfiguration WorkloadAuthenticationConfiguration specifies configuration of the workload authentication.\nField Description Validation Rule jwt","title":"Workload Configuration"},{"content":" WorkloadIdentity represents a platform-specific identity of a workload joining the mesh.\nE.g.,\nAWS EC2 instance identity:\naws: partition: aws account: \u0026#39;123456789012\u0026#39; region: ca-central-1 zone: ca-central-1b ec2: instance_id: i-1234567890abcdef0 iam_role: name: example-role GCP GCE instance identity:\ngcp: project_number: \u0026#39;234567890121\u0026#39; project_id: gcp-example region: us-central1 zone: us-central1-a gce: instance_id: \u0026#39;693197132356332126\u0026#39; Azure Compute instance identity:\nazure: subscription: …","relpermalink":"/tsb/refs/onboarding/config/types/identity/v1alpha1/identity/","summary":"WorkloadIdentity represents a platform-specific identity of a workload joining the mesh.\nE.g.,\nAWS EC2 instance identity:\naws: partition: aws account: '123456789012' region: ca-central-1 zone: ca-central-1b ec2: instance_id: i-1234567890abcdef0 iam_role: name: example-role GCP GCE instance identity:\ngcp: project_number: '234567890121' project_id: gcp-example region: us-central1 zone: us-central1-a gce: instance_id: '693197132356332126' Azure Compute instance identity:\nazure: subscription: 531bed28-f708-4fc5-b0c1-2c1edde46e4f resource_group: azure-example compute: instance_id: fc13d26e-d3c0-458e-b353-686d5ca19506 JWT identity:\njwt: issuer: https://mycompany.corp subject: us-east-datacenter1-vm007 attributes: region: us-east datacenter: datacenter1 instance_name: vm007 instance_hostname: vm007.internal.corp instance_role: app-ratings WorkloadIdentity WorkloadIdentity represents a platform-specific identity of a workload joining the mesh.\nField Description Validation Rule aws\ntetrateio.api.onboarding.config.types.identity.aws.v1alpha1.AwsIdentity oneof kind AWS-specific identity of a workload.","title":"Workload Identity"},{"content":" Workload Registration specifies information sent by the Workload Onboarding Agent to the Workload Onboarding Plane to register the workload in the mesh.\nAgentInfo AgentInfo specifies information about the Workload Onboarding Agent installed alongside the workload.\nField Description Validation Rule version\nstring REQUIRED Version of the Workload Onboarding Agent.\nstring = { min_len: 1}\nIstioSidecarInfo IstioInfo specifies information about the Istio Sidecar installed alongside the workload. …","relpermalink":"/tsb/refs/onboarding/config/types/registration/v1alpha1/registration/","summary":"Workload Registration specifies information sent by the Workload Onboarding Agent to the Workload Onboarding Plane to register the workload in the mesh.\nAgentInfo AgentInfo specifies information about the Workload Onboarding Agent installed alongside the workload.\nField Description Validation Rule version\nstring REQUIRED Version of the Workload Onboarding Agent.\nstring = { min_len: 1}\nIstioSidecarInfo IstioInfo specifies information about the Istio Sidecar installed alongside the workload.\nField Description Validation Rule version\nstring REQUIRED Version of the Istio Sidecar.\nstring = { min_len: 1}\nrevision\nstring Istio revision the pre-installed Istio Sidecar corresponds to.\nE.g., canary, alpha, etc.\nIf omitted, it is assumed that the pre-installed Istio Sidecar corresponds to the default Istio revision.","title":"Workload Registration"},{"content":"List of annotations on a WorkloadEntry resource supported by the tctl x sidecar-bootstrap command.\nUsage example apiVersion: networking.istio.io/v1beta1 kind: WorkloadEntry metadata: name: my-vm namespace: my-namespace annotations: sidecar-bootstrap.istio.io/ssh-user: istio-proxy sidecar-bootstrap.istio.io/proxy-config-dir: /etc/istio-proxy sidecar-bootstrap.istio.io/proxy-instance-ip: 10.0.0.1 sidecar.istio.io/logLevel: debug sidecar.istio.io/componentLogLevel: upstream:info,config:trace …","relpermalink":"/tsb/reference/cli/reference/workload-entry-annotations/","summary":"List of annotations on a WorkloadEntry resource supported by the tctl x sidecar-bootstrap command.\nUsage example apiVersion: networking.istio.io/v1beta1 kind: WorkloadEntry metadata: name: my-vm namespace: my-namespace annotations: sidecar-bootstrap.istio.io/ssh-user: istio-proxy sidecar-bootstrap.istio.io/proxy-config-dir: /etc/istio-proxy sidecar-bootstrap.istio.io/proxy-instance-ip: 10.0.0.1 sidecar.istio.io/logLevel: debug sidecar.istio.io/componentLogLevel: upstream:info,config:trace sidecar.istio.io/statsInclusionRegexps: .* # enable all Envoy metrics proxy.istio.io/config: | concurrency: 3 spec: ... Standard Istio annotations proxy.istio.io/config Overrides for the proxy configuration for this specific proxy. Available options can be found at https://istio.io/docs/reference/config/istio.mesh.v1alpha1/#ProxyConfig.\nsidecar.istio.io/interceptionMode Specifies the mode used to redirect inbound connections to Envoy (REDIRECT or TPROXY).\nsidecar.istio.io/proxyImage Specifies the Docker image to be used by the Envoy sidecar.\nsidecar.istio.io/logLevel Specifies the log level for Envoy.","title":"WorkloadEntry Annotations"},{"content":" A Workspace carves a chunk of the cluster resources owned by a tenant into an isolated configuration domain.\nThe following example claims ns1 and ns2 namespaces across all clusters owned by the tenant mycompany.\napiVersion: api.tsb.tetrate.io/v2 kind: Workspace metadata: name: w1 tenant: mycompany organization: myorg spec: namespaceSelector: names: - \u0026#34;*/ns1\u0026#34; - \u0026#34;*/ns2\u0026#34; The following example claims ns1 namespace only from the c1 cluster and claims all namespaces from the c2 cluster.\napiVersion: …","relpermalink":"/tsb/refs/tsb/v2/workspace/","summary":"A Workspace carves a chunk of the cluster resources owned by a tenant into an isolated configuration domain.\nThe following example claims ns1 and ns2 namespaces across all clusters owned by the tenant mycompany.\napiVersion: api.tsb.tetrate.io/v2 kind: Workspace metadata: name: w1 tenant: mycompany organization: myorg spec: namespaceSelector: names: - \"*/ns1\" - \"*/ns2\" The following example claims ns1 namespace only from the c1 cluster and claims all namespaces from the c2 cluster.\napiVersion: api.tsb.tetrate.io/v2 kind: Workspace metadata: name: w1 tenant: mycompany organization: myorg spec: namespaceSelector: names: - \"c1/ns1\" - \"c2/*\" Custom labels and annotations can be propagated to the final Istio translation that will be applied at the clusters.","title":"Workspace"},{"content":" DEPRECATED: use Access Bindings instead.\nWorkspaceAccessBindings is an assignment of roles to a set of users or teams to access resources under a Workspace. The user or team information is obtained from an LDAP server that should have been configured as part of Service Bridge installation. Note that a WorkspaceAccessBinding can be created or modified only by users who have SET_POLICY permission on the Workspace.\nThe following example assigns the workspace-admin role to users alice, bob, and …","relpermalink":"/tsb/refs/tsb/rbac/v2/workspace-access-bindings/","summary":"DEPRECATED: use Access Bindings instead.\nWorkspaceAccessBindings is an assignment of roles to a set of users or teams to access resources under a Workspace. The user or team information is obtained from an LDAP server that should have been configured as part of Service Bridge installation. Note that a WorkspaceAccessBinding can be created or modified only by users who have SET_POLICY permission on the Workspace.\nThe following example assigns the workspace-admin role to users alice, bob, and members of the t1 team for all workspace w1 owned by the tenant mycompany. Use fully-qualified name (fqn) when specifying user and team","title":"Workspace Access Bindings"},{"content":"import { PanelContent, PanelContentCode, } from “@theme/Panel”;\nService to manage TSB workspaces.\nWorkspaces The Workspaces service provides methods to manage the workspaces for a given tenant.\nWorkspaces are the main containers for the different configuration resources available in TSB, and provide infrastructure isolation constraints.\nCreateWorkspace rpc CreateWorkspace (tetrateio.api.tsb.v2.CreateWorkspaceRequest) returns (tetrateio.api.tsb.v2.Workspace)\nRequires CREATE\nCreate a new …","relpermalink":"/tsb/refs/tsb/v2/workspace-service/","summary":"import { PanelContent, PanelContentCode, } from “@theme/Panel”;\nService to manage TSB workspaces.\nWorkspaces The Workspaces service provides methods to manage the workspaces for a given tenant.\nWorkspaces are the main containers for the different configuration resources available in TSB, and provide infrastructure isolation constraints.\nCreateWorkspace rpc CreateWorkspace (tetrateio.api.tsb.v2.CreateWorkspaceRequest) returns (tetrateio.api.tsb.v2.Workspace)\nRequires CREATE\nCreate a new workspace. The workspace will own exclusively the namespaces configured in the namespaces selector for the workspace.\nGetWorkspace rpc GetWorkspace (tetrateio.api.tsb.v2.GetWorkspaceRequest) returns (tetrateio.api.tsb.v2.Workspace)\nRequires READ\nGet the details of an existing workspace\nUpdateWorkspace rpc UpdateWorkspace (tetrateio.api.tsb.v2.Workspace) returns (tetrateio.api.tsb.v2.Workspace)\nRequires WRITE\nModify an existing workspace\nListWorkspaces rpc ListWorkspaces (tetrateio.","title":"Workspace Service"},{"content":" Workspace Setting allows configuring the default traffic, security and east-west gateway settings for all the workloads in the namespaces owned by the workspace. Any namespace in the workspace that is not part of a traffic or security group with specific settings will use these default settings.\nThe following example sets the default security policy to accept either mutual TLS or plaintext traffic, and only accept connections at a proxy workload from services within the same namespace. The …","relpermalink":"/tsb/refs/tsb/v2/workspace-setting/","summary":"Workspace Setting allows configuring the default traffic, security and east-west gateway settings for all the workloads in the namespaces owned by the workspace. Any namespace in the workspace that is not part of a traffic or security group with specific settings will use these default settings.\nThe following example sets the default security policy to accept either mutual TLS or plaintext traffic, and only accept connections at a proxy workload from services within the same namespace. The default traffic policy allows unknown traffic from a proxy workload to be forwarded via an egress gateway tsb-egress in the perimeter namespace in the same cluster.","title":"Workspace Setting"},{"content":"In this guide you’ll learn how to use the TSB CLI (tctl) to perform common operations on the platform. You will learn how to configure the CLI to access your TSB installation and how to manage TSB resources from the command line.\nGetting started To use the YAML API you need the TSB CLI installed and configured.\nOnce you have the CLI installed and configured to talk to your TSB installation, you’ll need to configure access to the TSB platform with the tctl login command:\ntctl login You will be …","relpermalink":"/tsb/reference/yaml-api/guide/","summary":"In this guide you’ll learn how to use the TSB CLI (tctl) to perform common operations on the platform. You will learn how to configure the CLI to access your TSB installation and how to manage TSB resources from the command line.\nGetting started To use the YAML API you need the TSB CLI installed and configured.\nOnce you have the CLI installed and configured to talk to your TSB installation, you’ll need to configure access to the TSB platform with the tctl login command:\ntctl login You will be prompted for the TSB organization, which was set in the TSB install process or made available to you, the tenant, and the credentials.","title":"YAML API Guide"},{"content":"上周，Kubernetes 项目合并了一个新的 alpha 特性，使用户能够在规范中定义“sidecar containers”。这个新功能旨在帮助定义多容器 pod 中辅助容器的行为，这些容器可能有助于配置、网络、日志和度量收集等方面。\n什么是 sidecar container？ 理论上，Kubernetes 期望您在每个 pod 中运行一个容器。实际上，许多用例需要多容器 pod——例如，当您使用某些服务网格时，几乎所有的 pod 中都可能有 sidecar。\n有时，辅助容器仅用于初始化：例如为主容器配置和管理 secret。Kubernetes 已经为用户提供了定义 initContainer 的方式一段时间了。这个新功能最终为 initContainer 提供了更精细的粒度，以反映 sidecar 的特定要求，简化常见用法模式并为未来开辟了一些有趣的设计空间。\nsidecar container 特性如何工作？ 在这个新的功能门控中，sidecar containers 被定义为…\n在 pod 中比其他容器更早地启动，因为它们可能需要先初始化。这对于像服务网格这样的事情很重 …","relpermalink":"/blog/understanding-kubernetes-new-sidecar-container-feature/","summary":"Kubernetes 的新 sidecar container 特性允许用户在规范中定义辅助容器的行为，以帮助配置、网络、日志和度量收集等方面。这个新功能旨在为多容器 pod 中的 sidecar 容器提供更精细的粒度，使其能够比 initContainer 更好地反映 sidecar 的特定要求，简化常见用法模式并为未来开辟了一些有趣的设计空间。","title":"Kubernetes 将推出新 sidecar container 特性"},{"content":"前言 译者注：本文译自 Codefresh 公司发布的系列博客 Enterprise CI/CD Best Practices。\n如果你正在学习持续集成/交付/部署，你可能会发现主要有两类资源：\nCI/CD 是什么以及为什么需要它的概述。这些对于初学者很好，但不涵盖有关 Day2 操作或如何优化现有流程的任何内容。 仅涵盖 CI/CD 的特定方面（例如仅单元测试或仅部署）的详细教程，使用特定的编程语言和工具。 我们相信这两个极端之间存在差距。我们缺少一份恰当的指南，介于这两个类别之间，讨论最佳实践，但不是以抽象的方式。如果你一直想阅读有关 CI/CD 的指南，不仅解释“为什么”，还解释“如何”应用最佳实践，那么这份指南适合你。\n我们将描述所有有效的 CI/CD 工作流程的基本原理，但不仅以一般术语谈论，而且还将解释每个最佳实践背后的技术细节，更重要的是，如果你不采用它，它可能会对你产生什么影响。\n设置优先级\n一些公司试图在掌握基础知识之前跳上 DevOps 的列车。你很快会发现，CI/CD 流程中出现的一些问题通常是现有流程问题，只有当该公司试图遵循 CI/CD 流程的最佳实践时，才会 …","relpermalink":"/blog/enterprise-ci-cd-best-practices/","summary":"我们将描述所有有效的 CI/CD 工作流程的基本原理，但不仅以一般术语谈论，而且还将解释每个最佳实践背后的技术细节，更重要的是，如果你不采用它，它可能会对你产生什么影响。","title":"企业级 CI/CD 最佳实践"},{"content":"Istio 成为 CNCF 项目的毕业生。这一历史性的时刻代表着 Istio 在云原生领域的成长和成熟，标志着最广泛部署的服务网格迎来了一个令人兴奋的新篇章。Kubernetes 是 第一个获得毕业资格的项目，时间是 2018 年。今天，自它作为一个孵化项目进入 CNCF 不到一年的时间，Istio 就毕业了，成为 CNCF 历史上最快的一个。\nTetrate 是由 Istio 创始团队的成员创立的，旨在推广和扩大服务网格的应用，并自创立以来一直是 Istio 最重要的贡献者之一。我们为 Istio 及其社区的辛勤工作和奉献取得了这一里程碑式的认可而感到自豪和兴奋。\nIstio 毕业意味着什么？ CNCF 项目分为三个类别，作为项目成熟度的标志：\n沙盒。 CNCF“沙盒”是 CNCF 内新项目的入口。它为早期阶段的项目提供支持、指导和可见度，以便从 CNCF 社区中获得支持。沙盒旨在为这些项目提供一个安全和协作的环境，以便它们进行实验、创新和成熟。\n孵化。 孵化项目已经超过了开发的早期阶段，并展示了成为成熟云原生技术的潜力。Istio 凭借其强大的社区和早期采用者的不断生产使用而被接纳 …","relpermalink":"/blog/istio-service-mesh-graduates-cncf/","summary":"本文介绍了 Istio 作为 CNCF 项目的毕业生的成熟度、安全性、生产就绪、采用和生态系统、CNCF 支持和治理以及社区和企业支持。同时，介绍了 Tetrate 对 Istio 的影响，包括代码贡献、唯一的纯 OSS 企业产品、共同的专业知识、制定标准的安全领导力、社区参与、教育和培训、生态系统扩展等。Tetrate 和 Istio 的交织历史的简要时间线也被列出。最后，提供了使用 Istio 和 Tetrate Service Bridge 的方法和资源。","title":"Istio 成为最快的 CNCF 毕业项目"},{"content":"在 Kubernetes 中，同一命名空间中的任何 Pod 都可以使用其 IP 地址相互通信，无论它属于哪个部署或服务。虽然这种默认行为适用于小规模应用，但在规模扩大和复杂度增加的情况下，Pod 之间的无限通信可能会增加攻击面并导致安全漏洞。\n在集群中实施 Kubernetes 网络策略可以改善以下方面：\n安全性： 使用 Kubernetes 网络策略，你可以指定允许哪些 Pod 或服务相互通信，以及应该阻止哪些流量访问特定的资源。这样可以更容易地防止未经授权的访问敏感数据或服务。 合规性： 在医疗保健或金融服务等行业，合规性要求不可妥协。通过确保流量仅在特定的工作负载之间流动，以满足合规要求。 故障排除： 通过提供关于应该相互通信的 Pod 和服务的可见性，可以更轻松地解决网络问题，特别是在大型集群中。策略还可以帮助你确定网络问题的源，从而加快解决速度。 Kubernetes 网络策略组件 强大的网络策略包括：\n策略类型： Kubernetes 网络策略有两种类型：入口和出口。入口策略允许你控制流入 Pod 的流量，而出口策略允许你控制从 Pod 流出的流量。 …","relpermalink":"/blog/understanding-kubernetes-network-policies/","summary":"这篇文章介绍了 Kubernetes 网络策略的概念、作用和使用方法。Kubernetes 网络策略可以让你配置和执行一套规则，来控制集群内部的流量。它们可以提高安全性、符合合规性和简化故障排除。文章分析了网络策略的不同组成部分，包括选择器、入口规则和出口规则，并给出了不同的策略示例和最佳实践。文章的目标是让读者对使用 Kubernetes 网络策略来保护和管理流量有一个坚实的理解。","title":"Kubernetes 网络策略入门：概念、示例和最佳实践"},{"content":"译者注：本文译自 Sysdig 公司的网站，Sysdig 是一家提供容器安全、监控和故障排除解决方案的公司，其产品帮助用户在容器化环境中实现可观测性和安全性。这篇文章介绍了 CNAPP，CNAPP 是一个端到端的云安全解决方案，可提供实时威胁检测、简化符合性、改善 DevOps 协作、操作效率等多种好处。它通过整合安全控件、提供集中式管理和运行时洞察力等方式，增强组织的整体安全姿态。\n总览 CNAPP（容器化应用程序保护平台）是一种综合性的、全方位的安全策略，贯穿整个应用程序的生命周期（SDLC）。随着云计算的快速普及和现代应用程序的日益复杂，传统的安全措施往往无法有效地保护免受复杂的网络威胁。\nCNAPP 结合了“向左倾斜”和“向右防御”安全概念，提供了全面和强大的安全策略，确保了应用程序在整个生命周期中的保护。\n通过将安全向左移动，组织可以利用从应用程序开发过程的最开始阶段就开始的安全控制、漏洞扫描和合规性检查。\n“向右防御”概念侧重于在应用程序运行时阶段实时检测和响应安全事件。尽管在开发过程中尽最大努力保护应用程序，但漏洞可能仍然存在，或者新的威胁可能出现，因此 CNAPP 必须 …","relpermalink":"/blog/what-is-cnapp/","summary":"CNAPP 是一个端到端的云安全解决方案，可提供实时威胁检测、简化符合性、改善 DevOps 协作、操作效率等多种好处。它通过整合安全控件、提供集中式管理和运行时洞察力等方式，增强组织的整体安全姿态。","title":"什么是 CNAPP（容器化应用保护平台）?"},{"content":" 摘要：本文译自 How OpenTelemetry Works with Kubernetes。本文介绍了如何将 OpenTelemetry 与 Kubernetes 配合使用。OpenTelemetry 可以作为 Prometheus 的替代品，也可以将数据导出到各种后端，包括 Prometheus。OpenTelemetry Operator 负责部署和管理 OpenTelemetry Collector，该组件是收集、处理和导出遥测数据的中央组件。OpenTelemetry 日志提供了一种标准化的方式来收集、处理和分析分布式系统中的日志。此外，本文还介绍了 OpenTelemetry 的下一步计划，包括 Web 服务器的自动化仪器化、OpenTelemetry Profile 和 Open Agent Management Protocol。\nOpenTelemetry 的主要目标是提供一种标准的方式，使开发人员和最终用户能够从他们的应用程序和系统中创建、收集和导出遥测数据，并促进不同可观察性工具和平台之间的互操作性。\nOTEL 支持多种编程语言， …","relpermalink":"/blog/how-opentelemetry-works-with-kubernetes/","summary":"本文介绍了如何将 OpenTelemetry 与 Kubernetes 配合使用。OpenTelemetry 可以作为 Prometheus 的替代品，也可以将数据导出到各种后端，包括 Prometheus。OpenTelemetry Operator 负责部署和管理 OpenTelemetry Collector，该组件是收集、处理和导出遥测数据的中央组件。OpenTelemetry 日志提供了一种标准化的方式来收集、处理和分析分布式系统中的日志。此外，本文还介绍了 OpenTelemetry 的下一步计划，包括 Web 服务器的自动化仪器化、OpenTelemetry Profile 和 Open Agent Management Protocol。","title":"如何利用 OpenTelemetry 监控和优化 Kubernetes 的性能"},{"content":"本文译自 A Comprehensive Guide to API Gateways, Kubernetes Gateways, and Service Meshes。\n摘要：本文介绍了 API 网关、Kubernetes 网关和服务网格的综合指南。API 网关和 Kubernetes 网关解决了边缘问题和 API 抽象化，而服务网格解决了服务之间的通信挑战。文章还介绍了如何在不同的网关中配置金丝雀部署，并讨论了 Kubernetes Gateway API 的发展和服务网格接口（SMI）规范。最后，文章提供了一些关于何时使用哪种网关的建议。\n简介 本文将介绍三种技术，它们分别是 API 网关、Kubernetes 网关和 Service Mesh，以及它们之间的区别，以及如何应用它们。\nAPI 网关 API 网关是一个连接客户端和 API 的中介，它接收所有客户端请求，将它们转发到所需的 API，并将响应返回给客户端。\n它基本上是一个具有许多功能的反向代理。\n除此之外，API 网关还可以具有诸如身份验证、安全性、细粒度流量控制和监控等功能，使 API 开发人员只需专注于业务需求。\n有 …","relpermalink":"/blog/gateway-and-mesh/","summary":"本文介绍了 API 网关、Kubernetes 网关和服务网格的综合指南。API 网关和 Kubernetes 网关解决了边缘问题和 API 抽象化，而服务网格解决了服务之间的通信挑战。文章还介绍了如何在不同的网关中配置金丝雀部署，并讨论了 Kubernetes Gateway API 的发展和服务网格接口（SMI）规范。最后，文章提供了一些关于何时使用哪种网关的建议。","title":"API 网关、Kubernetes 网关和 Service Mesh 综合指南"},{"content":"本文译自 Reframing Kubernetes Observability with a Graph。\n摘要：本文介绍了将 DevOps 和 Kubernetes 视为图形的方法，以提高效率和弹性。通过将 Kubernetes 部署中的不同组件建模为图中的节点，组织可以更好地了解不同组件的交互方式以及一个区域的更改如何影响整个系统。这可以帮助组织采取更为主动、战略性的 DevOps 方法，而不仅仅是在问题出现时做出反应。\nKubernetes 可以跨多个主机部署应用程序，同时让团队将它们作为单个逻辑单元进行管理。它抽象了底层基础架构，并提供了一个用于与集群交互的统一 API，以及用于简化工作流程的自动化。它是现代开发实践的完美系统。\n但在这些以云为先的生态系统中确保效率和弹性并不容易。微服务架构使得无法跟上正在不断发生的所有软件和基础架构变化。这个问题只会因分裂的监视和可观测工具以及团队和个人之间的隔离信息而变得更加严重。\n为了跟上，组织必须以一种新的方式考虑 DevOps 和 Kubernetes - 作为一个图形。\n将 DevOps 视为图形 DevOps 通常专注于自动化和集 …","relpermalink":"/blog/reframing-kubernetes-observability-with-a-graph/","summary":"本文介绍了将 DevOps 和 Kubernetes 视为图形的方法，以提高效率和弹性。通过将 Kubernetes 部署中的不同组件建模为图中的节点，组织可以更好地了解不同组件的交互方式以及一个区域的更改如何影响整个系统。这可以帮助组织采取更为主动、战略性的 DevOps 方法，而不仅仅是在问题出现时做出反应。","title":"以图形重构 Kubernetes 可观测性"},{"content":"最近，Docker 宣布与 WasmEdge 合作，在 Docker 生态系统中支持 WebAssembly。\n本文将介绍什么是 WebAssembly，以及为什么它与 Docker 生态系统相关，并提供一些实践示例。我们假设您熟悉 Docker 工具集。我们将使用我们的 WebAssembly PHP 的端口 来演示如何构建 PHP 解释器，将其打包为 OCI 镜像的一部分，并使用 Docker 运行它。\n请注意，本文的重点是获得实践经验，而不是讨论技术细节。您可以复制以下示例，也可以只读到最后，因为我们还将提供输出。\nWebAssembly - 什么？为什么？ 这是一个非常基本的介绍。如果您已经熟悉该技术，则可以跳到动手环节。\n什么是 WebAssembly？ WebAssembly（或 Wasm）是一个开放标准，定义了一种二进制指令格式，可以从不同的源语言创建可移植的二进制可执行文件。\n这些二进制文件可以在各种环境中运行。它起源于 web，并得到所有主要浏览器的支持。\nWasm 在浏览器中是如何工作的？ 浏览器引擎集成了一个 Wasm 虚拟机，通常称为 Wasm 运行时，它可以运 …","relpermalink":"/blog/docker-without-containers/","summary":"本文介绍了如何在 Docker 中使用 WebAssembly（Wasm）来运行 PHP 应用程序。Wasm 容器比传统容器更小，提供更高级别的沙盒性能，并且具有真正的可移植性。本文还提供了一些示例，演示了如何使用 Wasm 在不同的环境中运行 WordPress。","title":"WebAssembly：无需容器就能运行 Docker！"},{"content":" 译者注：本文译自 Docker + WebAssembly: a quick intro | by Fabrizio Guglielmino | Medium，本文介绍了使用 Docker 和 WebAssembly 创建容器的过程。通过比较标准 Docker 容器和 WebAssembly 容器，作者指出 WebAssembly 容器具有性能优势、架构中立等优点，但也存在不成熟的问题。WebAssembly 容器有望彻底改变容器化应用程序的方式。\n今天，我想展示一种实用且有趣的使用 Docker 的方式：在容器中使用 WebAssembly。\n我说“实用的方式”，这就是为什么我假设您有一些经验：\nDocker（当然） Rust（实际上，只是为了理解“Hello World”） WebAssembly；只需要对其有一个基本的了解（注意：我将在讨论中交替使用 WASM 和 WebAssembly 这两个术语） 关于我即将展示的内容，简单说一下：一个 Docker 容器是一个包含运行环境的映像的运行实例。运行环境通常是一个操作系统，大多数情况下是 Linux。操作系统是运行应用程序的必要 …","relpermalink":"/blog/docker-wasm-quick-intro/","summary":"本文介绍了使用 Docker 和 WebAssembly 创建容器的过程。通过比较标准 Docker 容器和 WebAssembly 容器，作者指出 WebAssembly 容器具有性能优势、架构中立等优点，但也存在不成熟的问题。WebAssembly 容器有望彻底改变容器化应用程序的方式。","title":"用 Docker 和 WebAssembly 打造容器的新时代！"},{"content":" 译者注：本文译自 CNCF 平台白皮书，介绍了如何构建云原生计算平台以及平台可能提供的能力。这些能力包括 Web 门户、API、黄金路径模板、自动化、开发环境、可观测性、基础设施服务、数据服务、消息和事件服务、身份和密码管理服务、安全服务和工件存储。此外，该文档还介绍了与平台相关的术语，如平台、平台能力提供者、平台工程师、平台产品经理、平台团队和平台用户。\n介绍 受 DevOps 所承诺的跨职能合作的启发，平台工程正在企业中作为一种明确的合作形式出现。平台策划和呈现基础功能、框架和体验，以促进和加速应用程序开发人员、数据科学家和信息工作者等内部客户的工作。特别是在云计算领域，平台已经帮助企业实现了云计算长期承诺的价值，例如快速的产品发布、跨基础架构的可移植性、更安全和更弹性的产品以及更高的开发者生产力。\n本文旨在支持企业领导、企业架构师和平台团队领导者提倡、调查和计划云计算内部平台。我们相信平台对企业的实际价值流有重大影响，但只是间接的，因此领导共识和支持对平台团队的长期可持续性和成功至关重要。在本文中，我们将通过讨论平台的价值、如何衡量该价值以及如何实施最大化该价值的平台团队来实现 …","relpermalink":"/blog/cncf-platforms-white-paper/","summary":"本文译自 CNCF 平台白皮书，介绍了如何构建云原生计算平台以及平台可能提供的能力。这些能力包括 Web 门户、API、黄金路径模板、自动化、开发环境、可观测性、基础设施服务、数据服务、消息和事件服务、身份和密码管理服务、安全服务和工件存储。此外，该文档还介绍了与平台相关的术语，如平台、平台能力提供者、平台工程师、平台产品经理、平台团队和平台用户。","title":" CNCF 平台白皮书"},{"content":"本文译自：Reducing the Cognitive Load Associated with Observability - The New Stack\n译者注：本文讨论了降低可观测性对认知负荷的影响。在处理大量数据时，我们需要过滤和转换数据点以生成适当的信号，并依赖警报系统来进行人类干预。游戏日是测试响应能力的好机会。在团队中培养协作文化对每个人的福祉至关重要。通过实施这些策略，软件工程团队可以确保他们具备使用和有效理解可观测性信号所需的知识和技能。\n你能想象在没有现代可观测工具的情况下开发或操作分布式系统吗？我们知道可观测性是一项关键的实践，可以让我们提高系统的可靠性，减少服务停机时间，可视化使用模式，提供性能见解并促进问题解决。\n随着过去十年微服务架构和全球“shift left”的意图的广泛采用，工程师的角色——从开发人员和运维人员到 DevOps、站点可靠性工程和平台工程——发生了巨大变化。许多人被赋予更多的责任，并增加了工作量。\n什么是 Shift left？\n“Shift left” 是一种软件开发术语，它指的是在开发生命周期的早期阶段引入测试和安全性措施，以便更早地 …","relpermalink":"/blog/reducing-the-cognitive-load-associated-with-observability/","summary":"本文讨论了降低可观测性对认知负荷的影响。在处理大量数据时，我们需要过滤和转换数据点以生成适当的信号，并依赖警报系统来进行人类干预。游戏日是测试响应能力的好机会。在团队中培养协作文化对每个人的福祉至关重要。通过实施这些策略，软件工程团队可以确保他们具备使用和有效理解可观测性信号所需的知识和技能。","title":"如何降低可观测性带来的认知负荷"},{"content":"本文译自：Startup Fermyon Releases Spin 1.0 for WebAssembly Serverless Applications。\nFermyon 最近宣布推出 Spin 1.0，这是一个用于使用 WebAssembly (Wasm) 开发无服务器应用的开源开发者工具和框架。\nSpin 1.0 是其去年推出 介绍 后的首个稳定版本。在 1.0 版本中，公司增加了对新编程语言（如 JavaScript、TypeScript、Python 或 C#，除了 Rust 和 Go 之外）、连接数据库（关系型 或 Redis）、使用流行的注册表服务分发应用程序（GitHub Container Registry、Docker Hub 或 AWS ECR）、内置的 键值存储 以保持状态、在 Kubernetes 上运行应用程序以及与 HashiCorp Vault 集成以管理运行时配置等方面的支持。\n通过 Spin，该公司为创建运行 Wasm 的应用程序提供了轻松的开发体验，包括部署和安全运行它们的框架。\nFermyon 的首席技术官 Radu Matei 在一篇 博客文 …","relpermalink":"/blog/spin-wasm-ga/","summary":"Fermyon 最近宣布推出 Spin 1.0，这是一个用于使用 WebAssembly (Wasm) 开发无服务器应用的开源开发者工具和框架。","title":"初创公司 Fermyon 发布 Spin 1.0 用于 WebAssembly 无服务器应用"},{"content":"Tetrate Service Express (TSE) 是一款基于开源软件的服务连接、安全和弹性自动化解决方案，专为 Amazon EKS 设计。\n本文译自：Tetrate Service Express 介绍\n快速实现 Amazon EKS 上安全和弹性的服务网格 今天我们很高兴地宣布 Tetrate Service Express (TSE)，这是一款针对 Amazon EKS 的服务连接、安全和弹性自动化解决方案。我们基于 Istio 和 Envoy 等开源服务网格组件构建了 TSE，并针对 AWS 对 TSE 进行了简化安装、配置和操作的优化。如果您的团队正在 AWS 上进行服务网格实验，并且需要快速证明投资回报率，而无需掌握复杂的 Istio 和 AWS 基元，那么 TSE 就是适合您的选择！如果您的团队已经在单个集群上拥有了服务网格，但希望将网格扩展到多个集群甚至区域，那么 TSE 也可以帮助您。事实上，TSE 是唯一一款基于开源软件并针对 AWS 进行优化的产品，预先集成了最受欢迎的 AWS 服务，可在几分钟内让您上手。\n如果您想快速了解 Tetrate …","relpermalink":"/blog/introducing-tetrate-service-express/","summary":"Tetrate Service Express (TSE) 是一款基于开源软件的服务连接、安全和弹性自动化解决方案，专为 Amazon EKS 设计。","title":"Tetrate 推出针对 Amazon EKS 设计的服务网格解决方案 TSE"},{"content":"您是否应该让多个团队使用同一个 Kubernetes 集群？\n您是否可以安全地运行来自不信任用户的不信任工作负载？\nKubernetes 是否具备多租户功能？\n本文将探讨在运行具有多个租户的集群时面临的挑战。\n多租户可分为：\n软多租户，适用于信任您的租户 - 比如与同一家公司的团队共享集群时。 硬多租户，适用于您不信任的租户。 您还可以混合使用！\n在租户之间共享集群的基本构建块是命名空间。\n命名空间在逻辑上对资源进行分组，它们不提供任何安全机制，也不能保证所有资源都部署在同一节点上。\n命名空间中的 Pod 仍然可以与集群中的所有其他 Pod 通信，向 API 发出请求并使用它们想要的任何资源。\n默认情况下，任何用户都可以访问任何命名空间。\n那应该怎么阻止它？\n通过 RBAC，您可以限制用户和应用程序对命名空间内和命名空间中的内容所能做的事情。\n常见的操作是授予有限用户权限。\n使用 Quotas 和 LimitRanges，您可以限制命名空间中部署的资源以及可以使用的内存、CPU 等。\n如果您想限制租户对其命名空间所能做的事情，这是一个绝妙的想法。\n默认情况下，所有 Pod …","relpermalink":"/blog/multi-tenancy-in-kubernetes/","summary":"本文将探讨在运行具有多个租户的集群时面临的挑战。","title":"如何在 Kubernetes 中实现多租户隔离：命名空间、RBAC 和网络策略的应用"},{"content":" 译者注\n这篇文章探讨了在人工智能时代，大数据公司如何适应和创新。作者采访了 Alation 的联合创始人 Aaron Kalb，了解了他们的“数据目录”平台，以及他对 ChatGPT 等生成型 AI 软件的看法。Kalb 认为，生成型 AI 是一种催化剂，推动了一波新的数据智能公司的出现。他还分析了生成型 AI 的优势和挑战，以及如何利用它来提高数据质量和可信度。 就像云计算引入了一系列“大数据”解决方案一样，生成式人工智能是新一波数据智能公司的催化剂。\n还记得“大数据”这个流行语吗？它在云计算时代孕育了许多成功的公司，如 Snowflake、Databricks、DataStax、Splunk 和 Cloudera。但现在我们处于人工智能时代，据说机器学习软件现在已经达到或接近“智能”了（即使它容易 产生幻觉 ——但是，我们所有人不都是吗？）。\n因此，鉴于当前的人工智能热潮，我们是否还需要“大数据”公司来对数据进行分类和组织呢？现在 AI 不是可以为我们做到这一点吗？\n为了了解数据公司如何适应人工智能时代，我采访了 Aaron Kalb，Alation 的联合创始人之 …","relpermalink":"/blog/the-next-wave-of-big-data-companies-in-the-age-of-chatgpt/","summary":"这篇文章探讨了在人工智能时代，大数据公司如何适应和创新。作者采访了 Alation 的联合创始人 Aaron Kalb，了解了他们的“数据目录”平台，以及他对 ChatGPT 等生成型 AI 软件的看法。Kalb 认为，生成型 AI 是一种催化剂，推动了一波新的数据智能公司的出现。他还分析了生成型 AI 的优势和挑战，以及如何利用它来提高数据质量和可信度。","title":"从 Siri 到 ChatGPT：大数据公司如何迎接 AI 新浪潮"},{"content":" 如果你正在阅读这篇博客文章，我假设你已经熟悉 Kubernetes 并且知道它是一个容器编排平台。在创建新的应用程序版本时，你的容器构建过程已经很好了。Kubernetes 提供了广泛的功能，使用户能够执行复杂的部署策略。挑战在于根据你的环境，有正确和错误的使用方式。\n你组织中的平台工程师很可能非常熟悉 Kubernetes，了解将应用程序部署到集群中的正确和错误方式。挑战在于向平台的所有用户传递这些信息。\n大多数运维人员的理解很可能来自于正式培训或花费很多时间构建平台并从错误中学习。要求所有打算与平台交互的应用程序开发人员具有相同的经验是不现实的。这对组织来说在时间、精力和对产品和客户的潜在影响方面都是昂贵的，因为这些经验是从错误中学到的。\n在每个组织内，用户使用内部开发平台的策略和标准是已知的或需要遵循的。挑战在于许多组织使用文档和广泛的沟通来确保用户遵循这些标准。这在人们偏离预期标准并学习正确方法之间提供了长时间的反馈循环。\nKyverno 这就是 Kyverno 的用武之地，它是一个基于 Kubernetes 的策略引擎，提供了一种将平台管理员学到的经验编码化的方 …","relpermalink":"/blog/argo-cd-kyverno-best-practice-policies/","summary":"本文介绍了如何使用 Kyverno 和 Argo CD 来强制执行 Kubernetes 的最佳实践。Kyverno 是一个 Kubernetes 原生的策略引擎，可以用来定义和执行安全和合规的规则。Argo CD 是一个 GitOps 的持续交付解决方案，可以用来管理 Kubernetes 集群的状态。文章通过一个具体的示例，展示了如何使用 Argo CD 来部署 Kyverno 和策略，以及如何处理策略违规的情况。","title":"使用 Kyverno 和 Argo CD 实施 Kubernetes 最佳实践"},{"content":" 译者注：本文介绍了 Istio 的新的目的地导向的 waypoint 代理，它可以简化和扩展 Istio 的功能。文章介绍了 waypoint 代理的架构，部署方式，以及如何将源代理的配置转移到目的地代理，从而提高可扩展性，可调试性，一致性和安全性。文章还展示了如何使用 Istio 的策略和遥测来管理和监控 waypoint 代理。文章的来源是 Istio 官方博客。\nAmbient 将 Istio 的功能分为两个不同的层，一个安全覆盖层和一个七层流量处理层。Waypoint 代理是一个可选组件，它基于 Envoy 并为其管理的工作负载进行七层流量处理。自 2022 年首次启动 Ambient 以来，我们进行了重大更改以简化路点配置、可调试性和可扩展性。\nWaypoint 代理的架构 与 sidecar 类似，waypoint 代理也是基于 Envoy 的，由 Istio 动态配置以服务于您的应用程序。Waypoint 代理的独特之处在于它运行每个命名空间（默认）或每个服务账户。通过在应用程序 pod 之外运行，waypoint 代理可以独立于应用程序安装、升级和扩展，并降低运营成 …","relpermalink":"/blog/waypoint-proxy-made-simple/","summary":"本文介绍了 Istio 的新的目的地导向的 waypoint 代理，它可以简化和扩展 Istio 的功能。文章介绍了 waypoint 代理的架构，部署方式，以及如何将源代理的配置转移到目的地代理，从而提高可扩展性，可调试性，一致性和安全性。文章还展示了如何使用 Istio 的策略和遥测来管理和监控 waypoint 代理。文章的来源是 Istio 官方博客。","title":"Istio Ambient 模式使用 Waypoint 代理简化 Istio 部署"},{"content":" 译者注：本文介绍了如何使用 OCI 容器来运行 WebAssembly 工作负载。WebAssembly（也称为 Wasm）是一种可移植的二进制指令格式，具有可嵌入和隔离的执行环境，适用于客户端和服务器应用。WebAssembly 可以看作是一种小巧、快速、高效、安全的基于栈的虚拟机，设计用于执行不关心 CPU 或操作系统的可移植字节码。WebAssembly 最初是为 web 浏览器设计的，用来作为函数的轻量级、快速、安全、多语言的容器，但它不再局限于 web。在 web 上，WebAssembly 使用浏览器提供的现有 API。WebAssembly System Interface（WASI）是为了填补 WebAssembly 和浏览器外系统之间的空白而创建的。这使得非浏览器系统可以利用 WebAssembly 的可移植性，使 WASI 成为分发和隔离工作负载时的一个很好的选择。文章中介绍了如何配置容器运行时来从轻量级容器镜像中运行 Wasm 工作负载，并给出了一些使用示例。\nWebAssembly（也称为 Wasm）以其可嵌入和隔离的执行环境而成为一种流行的便携式二进制指令格 …","relpermalink":"/blog/wasm-containers/","summary":"本文介绍了如何使用 OCI 容器来运行 WebAssembly 工作负载。WebAssembly（也称为 Wasm）是一种可移植的二进制指令格式，具有可嵌入和隔离的执行环境，适用于客户端和服务器应用。WebAssembly 可以看作是一种小巧、快速、高效、安全的基于栈的虚拟机，设计用于执行不关心 CPU 或操作系统的可移植字节码。WebAssembly 最初是为 web 浏览器设计的，用来作为函数的轻量级、快速、安全、多语言的容器，但它不再局限于 web。在 web 上，WebAssembly 使用浏览器提供的现有 API。WebAssembly System Interface（WASI）是为了填补 WebAssembly 和浏览器外系统之间的空白而创建的。这使得非浏览器系统可以利用 WebAssembly 的可移植性，使 WASI 成为分发和隔离工作负载时的一个很好的选择。文章中介绍了如何配置容器运行时来从轻量级容器镜像中运行 Wasm 工作负载，并给出了一些使用示例。","title":"使用 OCI 容器运行 WebAssembly 工作负载"},{"content":"客户向我们询问服务网格实践中关于开放策略代理 (OPA) 和服务网格如何结合使用的问题。我们探讨了关于服务网格和 OPA 策略的最佳实践，以及它们如何相互补充的想法。为了构建讨论框架，我们使用了 NIST 的零信任架构标准。在即将发布的 NIST 标准文档特别出版物 800-207A 中，基于身份的分段是一个主要概念。最低标准包括五项策略检查，应应用于进入的每个请求系统和每个后续跃点。您可以观看我们在今年的 CloudNativeSecurityCon 上与来自 NIST 的 Ramaswami Chandramouli 进行的深入讨论的演示。\n使用服务网格实现基于身份的分割的五个策略检查：\n传输中加密 服务身份和认证 服务到服务授权 最终用户身份和身份验证 最终用户对资源的授权 简而言之，服务网格是一个专用的基础设施层，专门用于为前四项检查实施策略，并在第五项中发挥一定作用。OPA 的亮点在于第五点：最终用户对资源的授权。\nIstio 的 sidecar 代理充当微服务应用程序的安全内核。Envoy 数据平面是一个通用的策略执行点 (PEP)，可以拦截所有流量并可以在应用层应用策略。 …","relpermalink":"/blog/understanding-istio-and-open-policy-agent-opa/","summary":"本文介绍了如何将服务网格和开放策略代理（OPA）结合使用，以实现基于身份的分割的五个策略检查。服务网格为前四项检查提供了执行点，而 OPA 则在第五项中发挥作用。本文还探讨了如何将特定于业务的策略应用于请求，以及如何使用 OIDC 或专用授权基础设施来实现最终用户对资源的授权。","title":"Istio 中的外部授权过滤器：使用 OPA 实现灵活的授权策略"},{"content":" 译者注：本文译自 Fusion Auth Developer。JSON Web Token（通常缩写为 JWT）是一种通常与 OAuth2 等标准协议一起使用的令牌。本文解释了 JWT 的组成部分和工作原理。\n在我们继续之前，重要的是要注意 JWT 通常被错误地称为 JWT Tokens。在末尾添加 Token 将会使其变成 JSON Web Token Token。因此，在本文中，我们省略末尾的 Token 并简单地称之为 JWT，因为这是更正确的名称。同样地，由于 JWT 通常用作身份验证和授权过程的一部分，一些人将其称为 Authentication Tokens 或 JWT Authentication Tokens。从技术上讲，JWT 只是一个包含 Base64 编码的 JSON 的令牌。它可以用于许多不同的用例，包括身份验证和授权。因此，在本文中，我们不使用这个术语，而是讨论如何在身份验证过程中使用 JWT。\n让我们开始吧！这是一个新生成的 JWT。为清楚起见添加了换行符，但它们通常不存在。 …","relpermalink":"/blog/jwt-components-explained/","summary":"JSON Web Token（通常缩写为 JWT）是一种通常与 OAuth2 等标准协议一起使用的令牌。本文解释了 JWT 的组成部分和工作原理。","title":"JWT 组件详解"},{"content":" 译者注：本文译自 Tetrate 博客。这篇文章介绍了 wazero，一个由 Tetrate 开发的用 Go 语言编写的 WebAssembly 运行时。wazero 可以让开发者用不同的编程语言编写代码，并在安全的沙箱环境中运行。wazero 有以下几个特点：\n纯 Go，无依赖，支持跨平台和跨架构 遵循 WebAssembly 核心规范 1.0 和 2.0 支持 Go 的特性，如并发安全和上下文传递 提供了丰富的编程接口和命令行工具 性能优异，超过了其他同类运行时 WebAssembly，也称为 Wasm，是一种编译用一种编程语言（例如 C 或 Rust）编写的代码并在不同的运行时（例如 Web 浏览器或微服务）上运行它的方法。这使得它成为编写插件、扩展以及在安全沙箱环境中运行任意用户定义代码的绝佳选择。\nWebAssembly 经常被误认为是一种仅限浏览器的技术，而实际上 Wasm 是一种跨平台的二进制文件，可以由任何 WebAssembly 运行时执行。从历史上看，Go 程序员没有太多好的选择，但这种情况已经改变。\n本文介绍了 wazero，它在用 Go 编程语言编写的基础设施 …","relpermalink":"/blog/introducing-wazero-from-tetrate/","summary":"这篇文章介绍了 wazero，一个由 Tetrate 开发的用 Go 语言编写的 WebAssembly 运行时。wazero 可以让开发者用不同的编程语言编写代码，并在安全的沙箱环境中运行。","title":"Tetrate 开源项目 Wazero 简介"},{"content":"本文作者 Bilgin Ibryam 是 Diagrid 的技术产品经理，致力于开发人员生产力工具。在此之前，他曾在 Red Hat 担任顾问和架构师，同时也是 Apache 软件基金会的提交者和成员。Bilgin 还与人合著了两本关于 Kubernetes 模式和 Camel 设计模式的书。在业余时间，Bilgin 喜欢通过博客和其他方式写作和分享他的知识。\n译者注：本文的原标题是《什么是云原生绑定应用》。本文介绍了云绑定应用程序的概念，并探讨了在使用云绑定应用程序时需要考虑的几个关键因素。首先，作者解释了云绑定应用程序是指在构建应用程序时使用云提供的服务和资源。作者强调了使用云绑定应用程序可以带来很多好处，例如降低成本和提高可靠性。然而，作者也指出了在使用云绑定应用程序时需要考虑的几个关键因素，包括云供应商锁定、数据隐私和网络连接可靠性等。最后，作者提供了一些建议，帮助企业在使用云绑定应用程序时避免潜在的风险。例如，选择具有高可用性的云服务提供商，并在使用云绑定应用程序时加强数据安全措施。\n关键要点 云提供商将重点从基础设施服务转移到开发人员直接使用的应用程序优先服务，从而产生了新 …","relpermalink":"/blog/cloud-bound-applications/","summary":"本文作者 Bilgin Ibryam 是 Diagrid 的技术产品经理，致力于开发人员生产力工具。在此之前，他曾在 Red Hat 担任顾问和架构师，同时也是 Apache 软件基金会的提交者和成员。Bilgin 还与人合著了两本关于 Kubernetes 模式和 Camel 设计模式的书。在业余时间，","title":"云原生绑定应用：一种让开发者专注于业务逻辑的新架构"},{"content":" 译者注：这篇文章介绍了 Istio 的 Rust-Based Ztunnel，它是一种基于 Rust 语言的轻量级代理，用于 Istio 的 ambient mesh。在文章中，作者解释了为什么需要一种新的代理，以及 Rust 语言是如何成为最佳选择的。文章还讨论了如何使用 workload xDS 配置来管理工作负载，以及如何查看 ztunnel 日志和 L4 指标。作者表示，Rust-Based Ztunnel 显著简化了 Istio 的 ambient mesh，并提高了性能。此外，Istio ambient mesh 已经合并到了上游主干，可以通过遵循入门指南来尝试 Rust-Based Ztunnel。\nZtunnel（零信任隧道）组件是为 Istio Ambient Mesh 专门构建的每节点代理。它负责安全地连接和验证 Ambient Mesh 中的工作负载。Ztunnel 旨在专注于 Ambient Mesh 中工作负载的一小组功能，例如 mTLS、身份验证、L4 授权和遥测，而无需终止工作负载 HTTP 流量或解析工作负载 HTTP 标头。Ztunnel 确保流量高 …","relpermalink":"/blog/rust-based-ztunnel/","summary":"这篇文章介绍了 Istio 的 Rust-Based Ztunnel，它是一种基于 Rust 语言的轻量级代理，用于 Istio 的 ambient mesh。在文章中，作者解释了为什么需要一种新的代理，以及 Rust 语言是如何成为最佳选择的。文章还讨论了如何使用 workload xDS 配置来管理工作负载，以及如何查看 ztunnel 日志和 L4 指标。作者表示，Rust-Based Ztunnel 显著简化了 Istio 的 ambient mesh，并提高了性能。此外，Istio ambient mesh 已经合并到了上游主干，可以通过遵循入门指南来尝试 Rust-Based Ztunnel。","title":"Istio Ambient Mesh 中基于 Rust 的 Ztunnel 组件介绍"},{"content":" 译者注：这篇文章从多个角度探讨了 WebAssembly（Wasm）的现状和未来。首先，文章引用了 Cloud Native Computing Foundation（CNCF）的报告，指出 WebAssembly 在网页、无服务器、游戏和容器化应用中的应用越来越广泛，并预测 WebAssembly 将显著影响这些应用。其次，文章讨论了 WebAssembly 在容器、边缘计算、编程语言和无服务器应用等方面的应用。虽然 WebAssembly 已经成熟地应用于浏览器，但是在后端应用方面，如边缘设备的应用和部署，仍需要更多的工作。WASI 已经成为将 WebAssembly 扩展到浏览器之外的最佳选择，可以帮助解决在任何配置正确的 CPU 上运行 WebAssembly 运行时的复杂性。WebAssembly 和容器的应用预计将共同增长，尽管 WebAssembly 在某些用例中可以取代容器，但总体来说，两者是互补的产品。WebAssembly 的未来看起来非常光明，但是在可靠和高效地支持 WebAssembly 在浏览器之外的生产用例方面，仍有很多工作要做。 …","relpermalink":"/blog/is-webassembly-really-the-future/","summary":"这篇文章从多个角度探讨了 WebAssembly（Wasm）的现状和未来。首先，文章引用了 Cloud Native Computing Foundation（CNCF）的报告，指出 WebAssembly 在网页、无服务器、游戏和容器化应用中的应用越来越广泛，并预测 WebAssembly 将显著影响这些应用。其次，文章讨论了 WebAssembly 在容器、边缘计算、编程语言和无服务器应用等方面的应用。虽然 WebAssembly 已经成熟地应用于浏览器，但是在后端应用方面，如边缘设备的应用和部署，仍需要更多的工作。WASI 已经成为将 WebAssembly 扩展到浏览器之外的最佳选择，可以帮助解决在任何配置正确的 CPU 上运行 WebAssembly 运行时的复杂性。WebAssembly 和容器的应用预计将共同增长，尽管 WebAssembly 在某些用例中可以取代容器，但总体来说，两者是互补的产品。WebAssembly 的未来看起来非常光明，但是在可靠和高效地支持 WebAssembly 在浏览器之外的生产用例方面，仍有很多工作要做。","title":"WebAssembly 真的代表着未来吗？"},{"content":" 译者注：这篇文章介绍了如何编写云原生网络功能（CNF），即在电信领域的网络应用，它们与大多数云原生企业应用有不同的非功能性需求。CNF 需要满足高性能、高可靠性、高安全性和低延迟等指标。文章提出了一个基本的设计原则：每个容器只负责一个关注点，即一个单一的网络功能或子功能。\n本文主旨 Docker 和 Kubernetes 文档都提倡将一个应用程序或每个容器“一个问题”打包的概念。这也可以作为每个应用程序和容器运行“一种进程类型”的指南。 基于电信的云原生网络功能 (CNF) 具有低延迟、高吞吐量和弹性等特定要求，这激发了多关注点/多进程类型的容器化方法。 使用多种进程类型实现的高性能电信应用程序应该探索使用 unix 域套接字而不是 TCP 或 HTTP 进行通信，因为这可以加快容器之间的通信。 微服务的详细和简明定义 很有价值。厚微服务可以是任何利用康威定律并按产品团队边界部署代码的东西。精益微服务是那些遵循粗粒度代码部署的服务，通常在容器中，具有单一的关注点。\nCloud Native Network Functions（CNFs）是电信领域的网络应用，非功能性需求不同于大多数云 …","relpermalink":"/blog/cloud-native-network-functions-concern/","summary":"这篇文章介绍了如何编写云原生网络功能（CNF），即在电信领域的网络应用，它们与大多数云原生企业应用有不同的非功能性需求。CNF 需要满足高性能、高可靠性、高安全性和低延迟等指标。文章提出了一个基本的设计原则：每个容器只负责一个关注点，即一个单一的网络功能或子功能。","title":"云原生网络功能（CNF）应该让每个容器聚焦一个关注点"},{"content":"Envoy Gateway (EG)首次公开发布 四个月后，我们很高兴地宣布发布 版本 0.3 起。这个最新版本是几位 Tetrate 同事和整个社区其他人辛勤工作的结晶。Envoy Gateway 现在支持整个 Kubernetes Gateway API，包括实验部分——添加了一些强大的新功能，使这个免费的开源软件更接近于功能齐全的 API 网关。\nEG 的一大特点是它配置了新的网关 API，而不是旧的和非常有限的 Ingress API，或任何为了弥补 Ingress 缺陷的专有 API。虽然 EG 0.2 实现了 Gateway API 的核心部分（完全支持“基本”HTTP 路由），但 EG 0.3 在其 Gateway API 支持方面更进了一步，这可能是了解其新功能的最佳方式：\n支持更多 HTTP 功能，例如URL Rewrite、Response Header Operation 和流量镜像。这些来自 API 规范中的扩展字段。 支持路由 gRPC、UDP 和原始 TCP。这些来自 API 的实验性新部分。 请注意这些 API 扩展：我们正在努力为真实用户提供有用的功能。 …","relpermalink":"/blog/envoy-gateways-latest-v0-3-release-extends-the-kubernetes-gateway-api/","summary":"Envoy Gateway 0.3 发布，对 Kubernetes Gateway API 的支持更进一步。","title":"Envoy Gateway 0.3 发布——扩展 Kubernetes Gateway API"},{"content":"我们从最简单的基于 IDP 的 RBAC 开始，最终将基于组的 RBAC 与细粒度的权限和细粒度的资源相结合。\n授权很复杂，因为每个应用程序都必须发明自己的授权模型。但是，有一些陈旧的路径可以作为大多数应用程序的良好起点。这篇文章将描述这些模式以及 Topaz 开源项目或 Aserto 授权服务等授权平台如何帮助你实施他们。\n角色作为用户属性 最简单的授权模式将一组角色建模为用户的属性。这些角色可以在身份提供者 (IDP) 中配置，并且通常作为范围嵌入到 IDP 生成的访问令牌中。\n一些应用程序完全基于嵌入在访问令牌中的角色（或离散权限）进行授权。但这有一些缺点：\n角色/权限/范围爆炸：角色/权限越多，访问令牌中需要嵌入的范围就越多，从而导致大小问题。 IDP 和应用程序之间的耦合：每当向应用程序添加新权限时，也必须修改访问令牌中生成其他范围的代码。这通常由有权访问 IDP 的安全/身份和访问团队完成，并且它引入了工作流程的复杂性。 一旦发布，访问令牌就很难失效。只要访问令牌有效，经过身份验证的用户就拥有权限，即使他们的角色在令牌颁发后发生了变化。这反过来又会导致安全漏洞。 在这种情况 …","relpermalink":"/blog/role-based-access-control-five-common-authorization-patterns/","summary":"本文描述了五种访问控制模式以及 Topaz 开源项目或 Aserto 授权服务等授权平台如何帮助你实施他们。","title":"基于角色的访问控制：五种常见的授权模型"},{"content":"下面是我所知道的关于将 Rust 编译为 WebAssembly 的所有知识。\n前一段时间，我写了一篇如何在没有 Emscripten 的情况下将 C 编译为 WebAssembly 的博客文章，即不默认工具来简化这个过程。在 Rust 中，使 WebAssembly 变得简单的工具称为 wasm-bindgen，我们正在放弃它！同时，Rust 有点不同，因为 WebAssembly 长期以来一直是一流的目标，并且开箱即用地提供了标准库布局。\nRust 编译 WebAssembly 入门 让我们看看如何让 Rust 以尽可能少的偏离标准 Rust 工作流程的方式编译成 WebAssembly。如果你浏览互联网，许多文章和指南都会告诉你使用 cargo init --lib 创建一个 Rust 库项目，然后将 crate-type = [\u0026#34;cdylib\u0026#34;] 添加到你的 cargo.toml，如下所示：\n[package] name = \u0026#34;my_project\u0026#34; version = \u0026#34;0.1.0\u0026#34; edition = \u0026#34;2021\u0026#34; [lib] crate-type = [\u0026#34;cdylib\u0026#34;] …","relpermalink":"/blog/rust-to-wasm/","summary":"关于将 Rust 编译为 WebAssembly 的所有知识。","title":"Rust 编译 WebAssembly 指南"},{"content":" 译者评论\n本文作者观点是：不应该将自由和开源软件（FOSS）置于你的软件供应链中，而是寻找他们的供应商，因为只有能够为软件负责任的供应商存在才能作为供应链的一环。\n在过去的几年里，我们看到了很多围绕软件供应链概念的讨论。这些讨论始于 LeftPad 时代，并随着过去几年发生的各种事件而升级。这个领域所有工作的问题在于它忘记了一个基本点。\n在开始讨论这个基本点之前，我将定义供应链和一般供应商的含义，以及我们申请软件的原因。那么为什么将 FOSS（自由和开源软件）置于该定义之下的尝试被深深地误导了。\n概念 在过去的几十年里，我们看到了 FOSS 的兴起。特别是，这可以极大地增加打包为库的代码片段的重用。这要归功于围绕这个想法蓬勃发展的庞大的基础设施生态系统。今天，每一种编程语言环境中都有一个包管理器，中央存储库保存着查找库和处理它们的分发所需的元数据。\n这是可能的，因为 FOSS 许可证非常宽松，并且可以重复使用和重新混合这些库，否则会出现的大量法律和财务问题。一个现代软件项目可能有成百上千个这样的依赖项，从 OpenSSL 到测试框架或日期选择器，涵盖诸如 JSON 编码器/解码器库甚 …","relpermalink":"/blog/not-a-supplier/","summary":"在过去的几十年里，我们看到了自由和开源软件 (FOSS) 的兴起。这使得打包为库的代码段的重用大幅增长。包管理器适用于当今世界上的每一种编程语言，它们的中央存储库保存着查找库和处理它们的分发所需的元数据。","title":"我不是供应商"},{"content":"这是 服务网格最佳实践系列文章 中的第三篇，摘自 Tetrate 创始工程师 Zack Butcher 即将出版的新书 Istio in Production。\nIstio 就像一组乐高积木：它具有许多功能，可以按照您想要的任何方式进行组装。出现的结构取决于您如何组装零件。在 上一篇中，我们描述了一种运行时拓扑结构，用于构建健壮、有弹性且可靠的基础架构。在本文中，我们将描述一组网格配置，以帮助在运行时实现稳健性、弹性、可靠性和安全性。\nIstio 在其所谓的 根命名空间 中支持全局默认配置 —— 默认在 istio-system。在根命名空间中发布的配置默认适用于所有服务，但在本地命名空间中发布的任何配置都会覆盖它。因此，一些配置应该在根命名空间中发布，并且不允许在其他任何地方发布（例如用于在传输中强制加密的 PeerAuthentication 策略）。其他配置应该在每个服务自己的命名空间中编写（例如 VirtualService 控制它的弹性设置）。\n我们看到的最成功的网格采用将网格本身隐藏在另一个界面后面：例如 Helm 模板、Terraform 或更高级的解决方案， …","relpermalink":"/blog/optimize-traffic-management-and-security-with-these-service-mesh-best-practices/","summary":"本文推荐了服务网格安全性优化的一些最佳实践。","title":"服务网格安全性优化最佳实践"},{"content":"这是 服务网格最佳实践系列文章 中的第二篇，摘自 Tetrate 创始工程师 Zack Butcher 即将出版的书籍 Istio in Production。\n当涉及到在多集群的基础设施中部署服务网格时，有一些可移动的部分。这里主要想强调的是控制平面应该如何部署在应用程序附近，入口应该如何部署以促进安全性和敏捷性，如何使用 Envoy 促进跨集群负载均衡，以及网格内部如何使用证书。\n使服务网格控制平面与故障域保持一致 建议：围绕故障域部署松散耦合的控制平面以实现高可用性。\n构建高可用性系统可能具有挑战性，而且通常成本很高。我们知道的一种经过验证的技术是围绕故障域构建。故障域是当关键系统发生故障时受影响的基础架构部分。我们构建可靠系统的基本方法是将系统跨越的故障域分组为多个独立的孤岛。最终系统的整体可靠性取决于我们可以使孤岛的独立程度。实际上，总是存在一些相互依赖性，将其最小化总是成本与可用性的权衡。\n在没有耦合故障域的情况下创建隔离孤岛的最简单方法是在每个孤岛中运行关键服务的独立副本。我们可以说这些副本是筒仓的本地副本 —— 它们共享相同的故障域。在云原生架构中，Kubernetes …","relpermalink":"/blog/service-mesh-deployment-best-practices-for-security-and-high-availability/","summary":"本文强调的是控制平面应该如何部署在应用程序附近，入口应该如何部署以促进安全性和敏捷性，如何使用 Envoy 促进跨集群负载均衡，以及网格内部如何使用证书。","title":"服务网格安全性和高可用性部署最佳实践"},{"content":"本文是 Tetrate 即将出版的《Istio in Production》一书中摘录的服务网格最佳实践系列的第一篇，作者是 Tetrate 创始工程师 Zack Butcher。\n我们接到许多实施网格的企业的问题，其中之一是“我还需要哪些控制，而网格提供哪些控制？”换句话说，他们想知道网格如何适应现有的安全模型。我们发现，网格最适合作为一组安全控制的内圈，这些控制从物理网络到应用本身的每一层都被实施。\n服务网格作为通用策略执行点 我们看到网格的 Sidecar 作为通用策略执行点（NIST SP 800-204B：使用服务网格的基于属性的访问控制）。由于它拦截了所有进出应用程序的流量，Sidecar 为我们提供了一个强大的位置来实现各种策略。我们可以实现传统的安全策略，如基于应用程序标识而非网络位置的更高保证的应用程序之间的授权。但我们也可以实施之前不切实际或需要与应用程序深度参与的策略。例如，网格允许您编写以下策略：“后端可以从数据库读取（使用应用级身份进行身份验证和授权），但前提是请求具有有效的最终用户凭证并具有读取范围（使用最终用户身份进行身份验证和授权）。”\n虽然服务网格提供 …","relpermalink":"/blog/how-service-mesh-layers-microservices-security-with-traditional-security-to-move-fast-safely/","summary":"我们认为，服务网格最适合作为现有安全模型的一部分，通过在传统安全控制之上添加更细粒度的安全策略来实现。","title":"如何将服务网格作为安全模型的一部分，以分层形式将微服务安全与传统安全结合起来"},{"content":"Istio 的核心功能之一是通过管理网格中服务的身份来促进零信任网络架构。为了在网格中检索用于 mTLS 通信的有效证书，各个工作负载向 istiod 发出证书签名请求 (CSR)。Istiod 反过来验证请求并使用证书颁发机构（CA）签署 CSR 以生成证书。默认情况下，Istio 为此目的使用自己的自签名 CA，但最佳实践是通过为每个 Istio 部署创建一个中间 CA，将 Istio 集成到您现有的 PKI 中。\n如果您正在管理多个集群，这意味着颁发多个中间 CA，每个中间 CA 都应设置为在几个月或更短的时间内到期。管理这些 CA 的生命周期至关重要，因为它们必须在过期或坏事发生之前进行轮换。本文将向您展示如何简化此 CA 管理以降低风险并提高系统的整体稳定性。\n轮换 CA 时的一个关键步骤是确保实际使用新的 CA。默认情况下，Istio 仅在启动时加载其 CA。但是，Istio 可以配置为监视更改并在更新时自动重新加载其 CA。本教程取自我们与管理大量 Istio 部署的企业客户合作开发的生产手册，将展示如何配置 Istio 以自动重新加载其 CA。 …","relpermalink":"/blog/automate-istio-ca-rotation-in-production-at-scale/","summary":"本文将向您展示如何简化 Istio 中的 CA 管理以降低风险并提高系统的整体稳定性。","title":"在生产中大规模自动化 Istio CA 轮换"},{"content":"当我们与想要使用 Istio 的客户或用户交流时，这一个问题时长会出现——Istio 中的证书信任如何工作的？Istio 有自己的证书颁发机构，而我们也有自己的证书颁发机构，如何确保它们相互信任？\n简而言之，通过中间签名证书将 Istio 纳入到您现有的信任链中。\n如果您使用 Istio 作为演示或开箱即用，它将拥有自己的自签名证书 —— 它是自己的根证书。对于在多个集群中运行 Istio 的用户来说，这是一个常见的痛点：他们无意中创建了两个互不不信任的孤岛，因此没有安全通信。\n以下是如何通过让 Istio 信任您现有的 PKI 的步骤。\n简述 这是简短的版本：您应该通过为每个 Istio 部署创建一个中间签名证书来让 Istio 信任您现有的 PKI（并且每个集群应该有一个 Istio 部署）。然后你会：\n启用跨 Istio 部署的通信 允许细粒度的证书撤销，而无需同时在整个基础架构中强制使用新证书（如果这听起来像是等待发生的重大中断，那么您是对的）。 启用签名证书的轻松轮换。您需要做的就是创建一个新的中间件并使用新证书重新启动 Istio。因为它在同一个信任根中，所以一切都继续工 …","relpermalink":"/blog/istio-trust/","summary":"本文讲解了如何让 Istio 信任现有 PKI 的步骤。","title":"将 Istio 纳入信任链：使用现有 PKI 作为信任根"},{"content":"在本文中，我们将探讨如何使用 Hashicorp Vault 作为一种比使用 Kubernetes Secret 更安全的方式来存储 Istio 证书。默认情况下，Secret 使用 base64 编码存储在 etcd 中。在安全策略严格的环境中，这可能是不可接受的，因此需要额外的措施来保护它们。一种此类解决方案涉及将机密存储在外部机密存储提供程序中，例如 HashiCorp Vault。\nVault 可以托管在 Kubernetes 集群内部和外部。在本案例中，我们将探索使用托管在 Kubernetes 外部的 Vault，以便它可以同时为多个集群提供秘密。该设置也非常适合探索 Istio 的多集群功能，它需要一个共享的信任域。\n利用 vault-agent-init 容器，我们可以将证书和私钥材料注入实际的 Istio 控制平面 Pod，以便它们使用外部 CA 证书进行引导。这避免了依赖 Secret 来引导 Istio 控制平面。该技术也完全适用于入口和出口证书。\n有关如何在 Istio 中使用和管理证书的更多信息，请参见官方文档：\n身份和证书管理 插入 CA …","relpermalink":"/blog/how-to-use-hashicorp-vault-as-a-more-secure-way-to-store-istio-certificates/","summary":"本文将指导你使用 Vault 存储 Istio 的证书。","title":"如何使用 Hashicorp Vault 作为一种更安全的方式来存储 Istio 证书"},{"content":"传统的网络安全依赖于围绕可信内部网络的强大防御边界，以将不良行为者拒之门外，将敏感数据拒之门外。在日益复杂的网络环境中，维护强大的边界越来越困难。\n零信任安全正在成为企业保护其传统和现代云原生应用程序的首选方法。零信任网络架构颠覆了边界安全的假设。在零信任网络中，每个资源都在内部受到保护，就好像它暴露在开放的互联网中一样。\n为了为行业和美国联邦政府建立零信任安全指南，美国国家标准与技术研究院 (NIST) 在一系列出版物中建立了零信任安全指南，从 SP 800-207 开始，介绍一般的零信任架构及其配套SP 800-204 微服务安全标准系列。\n以下是 NIST 的核心零信任架构原则以及建议在实践中应用它们的 Kubernetes 和 Istio 参考架构。\n零信任网络的六项原则 无论网络位置如何，所有通信都应该是安全的。网络位置和可达性并不意味着信任。企业拥有或其他专用网络内部的访问请求必须满足与来自任何其他位置的通信相同的安全要求。零信任系统的一个标准是，您可以将它暴露在开放的互联网上，并且它仍然是安全的，没有未经授权的系统、数据或通信访问。 所有通信都应加密。线路上的加密可防止窃 …","relpermalink":"/blog/the-top-6-zero-trust-principles-for-kubernetes-security/","summary":"本文是 NIST 的核心零信任架构原则以及建议在实践中应用它们的 Kubernetes 和 Istio 参考架构。","title":"Kubernetes 安全的 6 大零信任原则"},{"content":"Kubernetes 是编排现代云原生工作负载的事实标准。但是，它不提供开箱即用的安全通信。这意味着每个需要实施传输中加密以对其 Kubernetes 部署采用零信任安全态势的人都需要自己解决这个问题。\n幸运的是，有很多易于理解的方法可以实现，在本文中，我们将介绍在 Kubernetes 中实现双向 TLS（mTLS）的三大最佳实践。\n什么是 mTLS，为什么对安全来说很重要？ 传输层安全性（SSL 的后继者）是部署最广泛的安全通信标准，在 HTTPS 中最为明显。TLS 非常适合在需要向客户端证明其身份的服务器之间建立既保密（防窃听）又真实（防篡改）的安全通信。但是，在双方都需要向对方证明身份的情况下（例如在 Kubernetes 应用程序中的微服务之间），TLS 是不够的。\n这就是双向 TLS (mTLS) 的用武之地。mTLS 是 TLS，但双方在建立安全通信通道之前向对方证明自己的身份。这是 Kubernetes 中安全通信所需的必要部分。mTLS 提供：\n在线加密以确保机密性和防篡改 相互的、加密的安全身份证明以确保真实性 要深入了解 mTLS 的工作原理， …","relpermalink":"/blog/top-3-mtls-best-practices-for-zero-trust-kubernetes-security/","summary":"我们将介绍在 Kubernetes 中实现双向 TLS（mTLS）的三大最佳实践。","title":"零信任 Kubernetes 安全的三大 mTLS 最佳实践"},{"content":"在这篇文章中，我们将亲身体验 Envoy Gateway 和 Gateway API。以下是逐步指导你安装 Envoy Gateway 的说明，以及通过 Envoy 代理在集群外公开 HTTP 应用程序的简单用例。\n如果你不方便运行，我在本文中包含了每个命令的输出，即使你没有 Kubernetes 集群也可以看到它是如何工作的。\n如果你是 GUI 的粉丝，在文章的最后我会附上 Tetrate 基于 Backstage 的概念验证 Envoy Gateway GUI 的屏幕截图和详细信息，以展示针对 Gateway API 构建此类东西是多么容易。\n创建 Kubernetes 集群 首先运行 Envoy Gateway 和 Kubernetes 集群。最简单、最安全的方法是使用 minikube 在本地机器上启动集群。\n$ minikube start –driver=docker --cpus=2 --memory=2g 😄 minikube v1.27.0 on Arch 22.0.0 (x86_64) ▪ KUBECONFIG=... ❗ For more information, …","relpermalink":"/blog/hands-on-with-envoy-gateway/","summary":"最近 Envoy Gateway 0.2 发布了，API 网关的生态系统迎来了新的变化。这篇文章将想你介绍 Kubernetes API 网关领域的最新进展。","title":"使用 Envoy Gateway 0.2 体验新的 Kubernetes Gateway API"},{"content":"最近 Envoy Gateway 0.2 发布了，API 网关的生态系统迎来了新的变化。这篇文章将想你介绍 Kubernetes API 网关领域的最新进展。\n如何将外部的网络请求路由到 Kubernetes 集群？你可以使用入口控制器：一组 HTTP 反向代理，将流量转接到集群中，并由 operator 来管理。也可以使用 Ambassador、Contour、Traefik 或 HAproxy 这类软件。还可以使用云提供商的解决方案，或者只是用默认的的 Nginx Ingress。或者你可能使用一个功能更全面的 API 网关，如 Tyk 或 Kong，或者在 Kubernetes Ingress 前面的另一层有一个单独的网关，如 AWS 的 API 网关，或内部的 F5，可以选择的实在太多。\n为什么我们需要一个新的入口控制器 因为很多入口控制器都有不同程度的限制：有些是基于旧的技术，如 Nginx、HAproxy，甚至是基于 Apache 建立的。这些技术的特性不适用于云原生环境，比如在配置改变时放弃已建立的连接（如果你想深入了解，Ambassador 发表了一篇比较文章）。云供应 …","relpermalink":"/blog/envoy-gateway-to-the-future/","summary":"最近 Envoy Gateway 0.2 发布了，API 网关的生态系统迎来了新的变化。这篇文章将向你介绍 Kubernetes API 网关领域的最新进展。","title":"面向未来的网关：新的 Kubernetes Gateway API 和 Envoy Gateway 0.2 介绍"},{"content":"Istio最近宣布了“Ambient Mesh” —— 一种用于 Istio 的实验性“无 sidecar”部署模型。我们最近在从服务网格中获得最大性能和弹性的背景下写了关于 sidecar 与 sidecar-less 的文章。在本文中，我们将特别介绍我们对 Ambient Mesh 的看法。\n如果你想立即开始使用可用于生产的 Istio 发行版，请尝试Tetrate Istio Distro (TID)。TID 是经过审查的 Istio 上游发行版，它易于安装、管理和升级，基于适用于 FedRAMP 环境的 FIPS 认证构建。如果你需要一种统一且一致的方式来保护和管理一组应用程序中的服务，请查看Tetrate Service Bridge (TSB)，这是我们基于 Istio 和 Envoy 构建的全面的边缘到工作负载应用程序连接平台。\n什么是 Ambient Mesh？ Ambient Mesh 是最近引入 Istio 的一种实验性新部署模型。它将 Envoy sidecar 当前执行的职责分为两个独立的组件：一个用于加密的节点级组件（称为“ztunnel”）和一个为每个服务账 …","relpermalink":"/blog/what-is-ambient-mesh/","summary":"我们对任何能够使服务网格的采用更加容易的事情都充满热情，但我们还不确定 Ambient Mesh 的部署模型能否能兑现这一承诺。","title":"什么是 Ambient Mesh？"},{"content":"我们最近发布了 Istio Ambient Mesh（译者注：笔者更倾向于将其称为 Ambient Mode，即外围模式，但译文中仍然保留了 Ambient Mesh 的叫法），它是 Istio 的无 sidecar 数据平面。如公告博客中所述，我们使用 Ambient Mesh 解决的首要问题是简化操作、更广泛的应用程序兼容性、降低基础设施成本和提高性能。在设计 ambient 数据平面时，我们希望在不牺牲安全性或功能的情况下仔细平衡运维、成本和性能方面的问题。随着 ambient 组件在应用程序 pod 之外运行，安全边界发生了变化 —— 我们相信会变得更好。在这篇博客中，我们将详细介绍这些安全性变化以及它们与 sidecar 部署模式在安全性方面的对比。\nAmbient Mesh 的分层示意图 回顾一下，Istio Ambient Mesh 引入了一个分层网格数据平面，它具有负责传输安全和路由的安全覆盖层，可以选择为需要它们的命名空间添加 L7 功能。要了解更多信息，请参阅公告博客和入门博客。安全覆盖层包括一个节点共享组件 ztunnel，它负责 L4 …","relpermalink":"/blog/ambient-security/","summary":"深入研究最近发布的 Istio Ambient Mesh（Istio 的无 sidecar 数据平面）的安全隐患。","title":"Istio 服务网格 ambient 模式安全详解"},{"content":"今天，我们很高兴地介绍 Ambient Mesh（译者注：笔者更倾向于将其称为 Ambient Mode，即外围模式），这是一种新的 Istio 数据平面模式，旨在简化操作、扩大应用兼容性并降低基础设施成本。用户可以选择将 Ambient Mesh 集成到其基础设施的网格数据平面，放弃 sidecar 代理，同时保持 Istio 的零信任安全、遥测和流量管理等核心功能。我们正在与 Istio 社区分享 Ambient Mesh 的预览版，我们正努力在未来几个月内将其推向生产就绪。\nIstio 和 Sidecar 自项目成立以来，Istio 架构的一个决定性特征是使用 sidecar—— 与应用容器一起部署的可编程代理。利用 sidecar，不需要对应用程序进行重大调整即可以享受服务网格带来的好处，省去运维的负担。\nIstio 的传统模式是将 Envoy 代理作为 sidecar 部署在工作负载的 pod 中。 虽然 sidecar 比起重构应用程序有显著的优势，但它们并没有在应用程序和 Istio 数据平面之间提供一个完美的分离。这导致了一些限制：\n侵入性：sidecar 必须通过修改 …","relpermalink":"/blog/introducing-ambient-mesh/","summary":"Ambient Mesh（外围模式）是一种新的 Istio 数据平面模式，旨在简化操作、扩大应用兼容性并降低基础设施成本。","title":"Istio 无 sidecar 代理数据平面 ambient 模式简介"},{"content":"本文译自 Cloud Native Network Functions Are Here，译者 Jimmy Song。\n主要收获 云原生网络不是另一种方式的 SDN，它以一种完全不同的方式来看待网络。 虽然 SDN 似乎是把物理网络和机器做了虚拟化，但「云原生网络功能」（Cloud-native Network Functions，下文简称 CNF）不仅仅是容器化的网络和虚拟机，它还将网络功能分割成服务，这是 CNF 与 SDN 的一个主要区别。 CNF 是 OSI 网络模型中的网络功能（越底层实现起来就越困难），这些功能是根据云原生实践实现的。 虽然 SDN 数据平面（这里指的是转发数据包）位于硬件 ASIC 上，或在传统内核网络转发的虚拟化盒子里，但 CNF 探索用户平面转发或更新的 eBPF 数据路径转发。 在云原生数据中心中，偏向于三层的解决方案，但 CNF 的一大驱动力是电信服务提供商，他们经常下降到二层的功能。 在三类云资源（计算、存储和网络）中，网络似乎最难满足云原生的非功能性需求。例如，计算弹性可以通过虚拟机、容器和编排器合理分配，并通过 CI/CD 管道进行管理。网络 …","relpermalink":"/blog/cloud-native-network-functions/","summary":"这篇文章中，我们展示了云原生网络功能将网络应用引入云原生世界的一种尝试。究竟什么是 CNF，为什么它们很重要？","title":"云原生网络功能（CNF）介绍"},{"content":"关键要点 零信任通过有选择地只允许访问用户应该被允许访问的特定资源，解决了开放网络访问的问题。 实现持续验证的关键策略是零信任网络访问 (ZTNA)。 实施零信任有助于解决 SOC（安全运营中心）或安全分析师角色中的组织技能短缺问题。 在零信任环境中，开发人员必须全面了解如何保护请求者与应用程序交互的每一步，同时考虑当前的安全上下文。 零信任框架并没有消除在每次部署后持续扫描漏洞的需要，漏洞扫描可以确保应用程序和后端系统保持保护和运作。 什么是零信任模型？ 零信任安全模型是一种设计和实施安全 IT 系统的方法。零信任背后的基本概念是“从不信任，始终验证”。这意味着用户、设备和连接在默认情况下永远不会被信任，即使它们连接到公司网络或之前已经过身份验证。\n现代 IT 环境由许多互连的组件组成，包括本地服务器、基于云的服务、移动设备、边缘位置和物联网 (IoT) 设备。依赖于保护所谓的“网络边界”的传统安全模型在这种复杂的环境中是无效的。\n攻击者可以破坏用户凭据并访问防火墙后面的本地系统。\n他们还可以访问在组织控制之外部署的基于云的或物联网资源。零信任方法在受保护资产周围建立微边 …","relpermalink":"/blog/zero-trust-developer-guide/","summary":"关于零信任，你需要了解这些知识。","title":"开发者需要了解的零信任知识"},{"content":"背景介绍 Apache SkyWalking 观察部署在服务网格中的服务的度量、日志、追踪和事件。在进行故障排除时，SkyWalking 错误分析是一个宝贵的工具，可以帮助确定错误发生的位置。然而，确定性能问题更加困难：利用预先存在的观察数据往往不可能找到性能问题的根本原因。为此，动态调试和故障排除在进行服务性能剖析时就必不可少。在这篇文章中，我们将讨论如何使用 eBPF 技术来改进 SkyWalking 中的剖析功能，并用于分析服务网格中的性能影响。\nSkyWalking 中的追踪剖析 自 SkyWalking 7.0.0 以来，Trace Profiling 通过定期对线程堆栈进行采样，让开发者知道运行哪行代码花费更多时间，从而帮助开发者发现性能问题。然而，Trace Profiling 不适合以下情况：\n线程模型：Trace Profiling 对于剖析在单线程中执行的代码最有用。它对严重依赖异步执行模式的中间件不太有用。例如，Go 中的 Goroutines 或 Kotlin Coroutines。 语言：目前，Trace Profiling 只支持 Java …","relpermalink":"/blog/pinpoint-service-mesh-critical-performance-impact-by-using-ebpf/","summary":"在这篇文章中，我们将讨论如何使用 eBPF 技术来改进 SkyWalking 中的剖析功能，并用于分析服务网格中的性能影响。","title":"使用 eBPF 准确定位服务网格的关键性能问题"},{"content":"编者的话 本文是来自 Tetrate 工程师的分享，Tetrate 的拳头产品是 Tetrate Service Bridge（下文简称 TSB），它是在开源的 Istio 和 Envoy 基础上构建的，但为其增加了管理平面。\n简介 Tetrate 的应用连接平台 Tetrate Service Bridge（TSB）提供两种网关类型，分别为一级网关（Tier-1）和二级网关（Tier-2），它们都基于 Envoy 构建，但是目的有所不同。本文将探讨这两种类型网关的功能，以及何时选用哪种网关。\n关于两级网关的简要介绍：\n一级网关（下文简称 T1）位于应用边缘，用于多集群环境。同一应用会同时托管在不同的集群上，T1 网关将对该应用的请求流量在这些集群之间路由。 二级网关（下文简称 T2）位于一个的集群边缘，用于将流量路由到该集群内由服务网格管理的服务。 两级网关释义 托管在 TSB 管理的集群中的应用部署的设计与开源的 Istio 模型非常相似。它们的结构相同，使用入口网关来路由传入的流量。T2 网关相当于 Istio 的入口网关（Ingress Gateway），在逻辑上与 Istio …","relpermalink":"/blog/designing-traffic-flow-via-tier1-and-tier2-ingress-gateways/","summary":"在 Kubernetes 中我们不能仅依靠网络层加密，还需要 mTLS 来对客户端和服务端进行双向的传输层认证。本文将聚焦于 TLS 的真实性，以及证书管理的难题，说明服务网格对于在 Kubernetes 中开启 mTLS 带来的便利。","title":"通过两级网关设计来路由服务网格流量"},{"content":"编者的话 本文翻译节选自 A Kubernetes engineer’s guide to mTLS，为了便于读者理解，笔者对原文做了一点修改 1。因为笔者最近在研究 Istio 中的身份认证及 SPIFFE，对如何在 Kubernetes 中应用 mTLS 以及为什么要使用 mTLS 产生了浓厚的兴趣，再回想起五年前手动安装 Kubernetes 时，因为给集群开启 TLS 问题而导致安装停滞不前。\n本文的主要观点是：在 Kubernetes 中我们不能仅依靠网络层加密，还需要 mTLS 来对客户端和服务端进行双向的传输层认证。本文将聚焦于 TLS 的真实性，以及证书管理的难题，说明服务网格对于在 Kubernetes 中开启 mTLS 带来的便利。\n简介 Mutual TLS（双向 TLS），或称 mTLS，是 Kubernetes 中的一个热门话题，尤其是对于那些负责为应用程序提供传输层加密的人来说。但是，你有没有考虑过，什么是 mTLS，它提供什么样的安全，为什么需要 mTLS？\n本指南我将介绍什么是 mTLS，它与常规 TLS 的关系，以及为什么它与 Kubernetes 有 …","relpermalink":"/blog/mtls-guide/","summary":"在 Kubernetes 中我们不能仅依靠网络层加密，还需要 mTLS 来对客户端和服务端进行双向的传输层认证。本文将聚焦于 TLS 的真实性，以及证书管理的难题，说明服务网格对于在 Kubernetes 中开启 mTLS 带来的便利。","title":"写给 Kubernetes 工程师的 mTLS 指南"},{"content":"编者的话 Istio 1.14 版本增加了对 SPIRE 集成的支持，这篇文章将指导你如何在 Istio 中集成 SPIRE。\nSPIRE 是 SPIFFE 规范的一个生产就绪的实现，它可以执行节点和工作负载证明，以便安全地将加密身份发给在异构环境中运行的工作负载。通过与 Envoy 的 SDS API 集成，SPIRE 可以被配置为 Istio 工作负载的加密身份来源。Istio 可以检测到一个 UNIX 域套接字的存在，该套接字在定义的套接字路径上实现了 Envoy SDS API，允许 Envoy 直接从它那里进行通信和获取身份。\n这种与 SPIRE 的集成提供了灵活的认证选项，这是默认的 Istio 身份管理所不具备的，同时利用了 Istio 强大的服务管理。例如，SPIRE 的插件架构能够提供多样化的工作负载认证选项，超越 Istio 提供的 Kubernetes 命名空间和服务账户认证。SPIRE 的节点认证将认证扩展到工作负载运行的物理或虚拟硬件上。\n关于这种 SPIRE 与 Istio 集成的快速演示，请参阅通过 Envoy 的 SDS API 将 SPIRE …","relpermalink":"/blog/istio-spire-integration/","summary":"Istio 1.14 版本增加了对 SPIRE 集成的支持，这篇文章将指导你如何在 Istio 中集成 SPIRE。","title":"如何在 Istio 中集成 SPIRE"},{"content":"前言 OpenTelemetry 追踪包含了理解分布式系统和排除故障的信息宝库 —— 但你的服务必须首先被指标化，以发射 OpenTelemetry 追踪来实现这一价值。然后，这些追踪信息需要被发送到一个可观察的后端，使你能够获得关于这些数据的任意问题的答案。可观测性是一个分析问题。\n本周早些时候，我们部分解决了这个问题，宣布在 Promscale 中普遍提供 OpenTelemetry 追踪支持，将由 SQL 驱动的可观测性带给所有开发者。随着对分析语言 ——SQL 的全面支持，我们解决了分析的问题。但我们仍然需要解决第一部分的问题：测量。\n为了让你的服务发出追踪数据，你必须手动添加 OpenTelemetry 测量工具到代码中。而且你必须针对所有服务和你使用的所有框架来做，否则你将无法看到每个请求的执行情况。你还需要部署 OpenTelemetry 收集器来接收所有新的追踪，处理它们，批处理它们，并最终将它们发送到你的可观测性后端。这需要花费大量的时间和精力。\n如果你不需要做所有这些手工工作，并且可以在几分钟内而不是几小时甚至几天内启动和运行呢？如果你还能建立一个完整的可观测性技术 …","relpermalink":"/blog/generate-and-store-opentelemetry-traces-automatically/","summary":"首先，我们将解释一下如何在 Kubernetes 自动生成和存储 OpenTelemetry 追踪，剖析 OpenTelemetry Operator 在内部的真正作用。接下来，我们将通过一个例子演示如何将其直接付诸实践。","title":"一键开启 Kubernetes 可观测性——如何自动生成和存储 OpenTelemetry 追踪"},{"content":"Envoy 基础教程已上线 Tetrate 学院，请转到 新地址 免费参与学习。\n","relpermalink":"/envoy-handbook/","summary":"免费的中文 Envoy 基础教程已上线 Tetrate 学院。","title":"Envoy 基础教程"},{"content":"Istio 基础教程已上线 Tetrate 学院，请转到 新地址 免费参与学习。\n","relpermalink":"/istio-handbook/","summary":"免费的中文 Istio 基础教程已上线 Tetrate 学院。","title":"Istio 基础教程"},{"content":"主要收获 对于现代软件系统来说，可观测性不是关于数学方程。它是关于人类如何与复杂的系统互动并试图理解它们。\n混沌工程利用了可观测性，因为它可以检测到系统稳定状态的偏差。混沌工程借助可观测性可以发现和克服系统的弱点。\n可观测性依赖于系统所发出的信号，这些信号提供了关于系统行为的原始数据。然而，可观测性不仅受限于这些信号的质量，还受限于这些信号的可视化和解释的方式。\n考虑到混沌工程、可观测性和可视化涉及到人类自我的解释，仪表盘的设计者可能会对这些解释产生偏差，这是一个事实。在这个意义上，视觉隐喻并不能保证我们以正确的方式解释这些数据。\n基于视觉隐喻的仪表盘可以提供比经典的可视化更有用的数据。然而，这两种策略都很容易产生偏差；例如，在一项研究中，大多数参与者都注意到，由于显示了糟糕的柱状图和线状图，没有在图中显示出重要的分界点，因此整体结果是有偏差的。\n自从 Netflix、Slack 和 Linkedin 等领先的技术公司采用混沌工程来抵御生产中的意外中断后，这门学科在近来已经成为主流。在这条道路上，可观测性发挥了关键作用，为工程师们带来了数据和监控的力量，他们现在有了了解自己系统的策略， …","relpermalink":"/blog/chaos-engineering-observability-visual-metaphors/","summary":"本文为可观测性引入了一个新的角色：视觉隐喻，并进行了一项研究，对比了视觉隐喻与传统表示的结果。","title":"混沌工程和视觉隐喻的可观测性"},{"content":"前言 今天，Envoy 社区宣布了一个令人兴奋的新项目：Envoy Gateway。该项目将行业领导者联合起来，精简由 Envoy 驱动的应用网关的好处。这种方法使 Envoy Gateway 能够立即为快速创新打下坚实的基础。该项目将提供一套服务来管理 Envoy 代理机群，通过易用性来推动采用，并通过定义明确的扩展机制来支持众多的用例。\n我们为什么要这样做？ Tetrate 是 Envoy Proxy 的第一贡献者（按提交量计算），也是 Envoy Gateway 指导小组的成员，其贡献者涵盖技术和管理领域。我们相信，我们强大的伙伴关系和在开源软件方面的深厚经验将有助于确保 Envoy Gateway 的成功。Tetrate 推动了 EG 计划，因为我们致力于上游项目，因为我们相信这将降低 Envoy Proxy 用户的进入门槛，也因为这与我们开发服务网格作为零信任架构基础的使命相一致。Tetrate 将大力投资建设 Envoy Gateway 的安全功能，包括支持 OAuth2 和 Let’s Encrypt 集成等 API 功能。\n对上游项目的承诺 Tetrate 从第一天起就 …","relpermalink":"/blog/the-gateway-to-a-new-frontier/","summary":"在我们看来，由于 Envoy 的设计、功能设置、安装基础和社区，它是业内最好的 API 网关。有了 Envoy Gateway，企业可以在将 Envoy 嵌入其 API 管理策略方面增加信心。","title":"Envoy API Gateway——推动网关的进一步发展"},{"content":"前言 今天，我们很高兴地宣布 Envoy Gateway 成为 Envoy 代理家族的新成员，该项目旨在大幅降低将 Envoy 作为 API 网关的使用门槛。\n历史 Envoy 在 2016 年秋天开源，令我们惊讶的是，它很快就引领了整个行业。用户被这个项目的许多不同方面所吸引，包括它的包容性社区、可扩展性、API 驱动的配置模型、强大的可观测性输出和越来越广泛的功能集。\n尽管在其早期历史中，Envoy 成为了服务网格的代名词，但它在 Lyft 的首次使用实际上是作为 API 网关 / 边缘代理，提供深入的可观测性输出，帮助 Lyft 从单体架构迁移到微服务架构。\n在过去的 5 年多时间里，我们看到 Envoy 被大量的终端用户采用，既可以作为 API 网关，也可以作为服务网格中的 sidecar 代理。同时，我们看到围绕 Envoy 出现了一个庞大的供应商生态系统，在开源和专有领域提供了大量的解决方案。Envoy 的供应商生态系统对项目的成功至关重要；如果没有对所有在 Envoy 上兼职或全职工作的员工的资助，这个项目肯定不会有今天的成就。\nEnvoy 作为许多不同的架构类型和供应商 …","relpermalink":"/blog/introducing-envoy-gateway/","summary":"今天，我们很高兴地宣布 Envoy Gateway 成为 Envoy 代理家族的新成员，该项目旨在大幅降低将 Envoy 作为 API 网关的使用门槛。","title":"开源项目 Envoy Gateway 简介"},{"content":"什么是 Wasm 插件？ 你可以使用 Wasm 插件在数据路径上添加自定义代码，轻松地扩展服务网格的功能。可以用你选择的语言编写插件。目前，有 AssemblyScript（TypeScript-ish）、C++、Rust、Zig 和 Go 语言的 Proxy-Wasm SDK。\n在这篇博文中，我们描述了如何使用 Wasm 插件来验证一个请求的有效载荷。这是 Wasm 与 Istio 的一个重要用例，也是你可以使用 Wasm 扩展 Istio 的许多方法的一个例子。您可能有兴趣阅读我们关于在 Istio 中使用 Wasm 的博文，并观看我们关于在 Istio 和 Envoy 中使用 Wasm 的免费研讨会的录音。\n何时使用 Wasm 插件？ 当你需要添加 Envoy 或 Istio 不支持的自定义功能时，你应该使用 Wasm 插件。使用 Wasm 插件来添加自定义验证、认证、日志或管理配额。\n在这个例子中，我们将构建和运行一个 Wasm 插件，验证请求 body 是 JSON，并包含两个必要的键 ——id 和 token。\n编写 Wasm 插件 这个示例使用 tinygo …","relpermalink":"/blog/validating-a-request-payload-with-wasm/","summary":"本文是一个使用 Go 语言开发 Wasm 插件验证请求负载，并将其部署到 Istio 或 Envoy 上的教程。","title":"使用 WebAssembly 验证请求负载"},{"content":"前言 Slack 有一个全球客户群，在高峰期有数百万同时连接的用户。用户之间的大部分通信涉及到向对方发送大量的微小信息。在 Slack 的大部分历史中，我们一直使用 HAProxy 作为所有传入流量的负载均衡器。今天，我们将讨论我们在使用 HAProxy 时所面临的问题，我们如何用 Envoy Proxy 来解决这些问题，迁移所涉及的步骤，以及结果是什么。让我们开始吧！\nSlack 的 Websockets 为了即时传递信息，我们使用 websocket 连接，这是一种双向的通信链接，负责让你看到 “有几个人在打字……\u0026#34;，然后是他们打的东西，速度几乎是光速的。websocket 连接被摄取到一个叫做 “wss”（WebSocket 服务）的系统中，可以通过 wss-primary.slack.com 和 wss-backup.slack.com（这不是网站，如果去访问，只会得到一个 HTTP 404）从互联网上访问。\n显示 websockets 工作原理的图表 Websocket 连接一开始是普通的 HTTPS 连接，然后客户端发出协议切换请求，将连接升级为 Websocket。 …","relpermalink":"/blog/migrating-millions-of-concurrent-websockets-to-envoy/","summary":"本文是 Slack 花半年时间从 HAProxy 迁移到 Envoy 上的经验分享。","title":"Slack 将数百万个并发的 Websockets 迁移到 Envoy 上经验分享"},{"content":"编者的话 本文作者是 Isovalent 联合创始人\u0026amp;CTO，原文标题 How eBPF will solve Service Mesh - Goodbye Sidecars，作者回顾了 Linux 内核的连接性，实现服务网格的几种模式，以及如何使用 eBPF 实现无 Sidecar 的服务网格。\n什么是服务网格？ 随着分布式应用的引入，额外的可视性、连接性和安全性要求也浮出水面。应用程序组件通过不受信任的网络跨越云和集群边界进行通信，负载均衡、弹性变得至关重要，安全必须发展到发送者和接收者都可以验证彼此的身份的模式。在分布式应用的早期，这些要求是通过直接将所需的逻辑嵌入到应用中来解决的。服务网格将这些功能从应用程序中提取出来，作为基础设施的一部分提供给所有应用程序使用，因此不再需要修改每个应用程序。\n服务网格示意图 纵观今天服务网格的功能设置，可以总结为以下几点：\n弹性连接：服务与服务之间的通信必须能够跨越边界，如云、集群和场所。通信必须是有弹性的和容错的。 L7 流量管理：负载均衡、速率限制和弹性必须是 L7 感知的（HTTP、REST、gRPC、WebSocket 等）。 基于身 …","relpermalink":"/blog/ebpf-solve-service-mesh-sidecar/","summary":"本文回顾了 Linux 内核的连接性，实现服务网格的几种模式，以及如何使用 eBPF 实现无 Sidecar 的服务网格。","title":"告别 Sidecar——使用 eBPF 解锁内核级服务网格"},{"content":"前言 Istio 1.12 中新的 WebAssembly 基础设施使其能够轻松地将额外的功能注入网格部署中。\n经过三年的努力，Istio 现在有了一个强大的扩展机制，可以将自定义和第三方 Wasm 模块添加到网格中的 sidecar。Tetrate 工程师米田武（Takeshi Yoneda）和周礼赞（Lizan Zhou）在实现这一目标方面发挥了重要作用。这篇文章将介绍 Istio 中 Wasm 的基础知识，以及为什么它很重要，然后是关于建立自己的 Wasm 插件并将其部署到网格的简短教程。\n为什么 Istio 中的 Wasm 很重要 使用 Wasm，开发人员可以更容易的扩展网格和网关。在 Tetrate，我们相信这项技术正在迅速成熟，因此我们一直在投资上游的 Istio，使配置 API、分发机制和从 Go 开始的可扩展性体验更加容易。我们认为这将使 Istio 有一个全新的方向。\n有何期待：新的插件配置 API，可靠的获取和安装机制 有一个新的顶级 API，叫做 WasmPlugin，可以让你配置要安装哪些插件，从哪里获取它们（OCI 镜像、容器本地文件或远程 HTTP 资源）， …","relpermalink":"/blog/istio-wasm-extensions-and-ecosystem/","summary":"Istio 1.12 中新的 WebAssembly 基础设施使其能够轻松地将额外的功能注入网格部署中。","title":"Istio 1.12 引入 Wasm 插件配置 API 用于扩展 Istio 生态"},{"content":"编者的话 本文译自 Istio 官方博客，博客原标题 gRPC Proxyless Service Mesh，其实是 Istio 1.11 版本中支持的实验特性，可以直接将 gRPC 服务添加到 Istio 中，而不需要再向 Pod 中注入 Envoy 代理。本文中还给出了一个 Demo 性能测试数据，这种做法可以极大的提升应用性能，降低网络延迟。\nIstio 使用一组发现 API（统称为 xDS API 来动态配置其 Envoy sidecar 代理。这些 API 的目标是成为一个 通用的数据平面 API。gRPC 项目对 xDS API 有很好的支持，也就是说你可以管理 gRPC 工作负载，而不需要同时部署 Envoy sidecar。你可以在 Megan Yahya 的 KubeCon EU 2021 演讲中了解更多关于该集成的信息。关于 gRPC 支持的最新情况，可以在他们的提案中找到，还有实现状态。\nIstio 1.11 增加了实验性支持，可以直接将 gRPC 服务添加到网格中。我们支持基本的服务发现，一些基于 VirtualService 的流量策略，以及双向 TLS。\n支 …","relpermalink":"/blog/grpc-proxyless-service-mesh/","summary":"编者的话 本文译自 Istio 官方博客，博客原标题 gRPC Proxyless Service Mesh，其实是 Istio 1.11 版本中支持的实验特性，可以直接将 gRPC 服务添加到 Istio 中，而不需要再向 Pod 中注入 Envoy 代理。本文中还给出了一个 Demo 性能测试数据，这种做法可以极大的提升应","title":"基于 gRPC 和 Istio 的无 sidecar 代理的服务网格"},{"content":"前言 今天有几个服务网格的产品和项目，承诺简化应用微服务之间的连接，同时提供额外的功能，如安全连接、可观测性和流量管理。但正如我们在过去几年中反复看到的那样，对服务网格的兴奋已经被对额外的复杂性和开销的实际担忧所抑制。让我们来探讨一下 eBPF 是如何让我们精简服务网格，使服务网格的数据平面更有效率，更容易部署。\nSidecar 问题 今天的 Kubernetes 服务网格解决方案要求你在每一个应用 pod 上添加一个代理 sidecar 容器，如 Envoy 或 Linkerd-proxy。这是正确的：即使在一个非常小的环境中，比如说有 20 个服务，每个服务运行五个 pod，分布在三个节点上，你也有 100 个代理容器。无论代理的实现多么小和有效，这种纯粹的重复都会耗费资源。\n每个代理使用的内存与它需要能够通信的服务数量有关。Pranay Singhal 写了他配置 Istio 的经验，将每个代理的消耗从 1GB 左右减少到更合理的 60-70MB。但是，即使在我们的小环境中，在三个节点上有 100 个代理，这种优化配置仍然需要每个节点 2GB 左右。\n来自 \u0026lt;a …","relpermalink":"/blog/how-ebpf-streamlines-the-service-mesh/","summary":"本文探讨一下 eBPF 是如何让我们精简服务网格，使服务网格的数据平面更有效率，更容易部署。","title":"eBPF 如何简化服务网格"},{"content":"主要收获 了解采用服务网格技术的新兴架构趋势，特别是多云、多集群和多租户模式，如何在异构基础设施（裸机、虚拟机和 Kubernetes）中部署服务网格解决方案，以及从边缘计算层到网格的应用 / 服务连接。 了解服务网格生态系统中的一些新模式，如多集群服务网格、媒体服务网格（Media Service Mesh）和混沌网格，以及经典的微服务反模式，如“死星（Death Star） “架构。 获取最新的关于在部署领域使用服务网格的创新总结，在 Pod（K8s 集群）和 VM（非 K8s 集群）之间进行快速实验、混乱工程和金丝雀部署。 探索服务网格扩展领域的创新，包括：增强身份管理，以确保微服务连接的安全性，包括自定义证书授权插件，自适应路由功能，以提高服务的可用性和可扩展性，以及增强 sidecar 代理。 了解操作方面即将出现的情况，如配置多集群功能和将 Kubernetes 工作负载连接到托管在虚拟机基础设施上的服务器，以及管理多集群服务网格中所有功能和 API 的开发者门户。 在过去的几年里，服务网格技术有了长足的发展。服务网格在各组织采用云原生技术方面发挥着重要作用。通过提供四种主 …","relpermalink":"/blog/service-mesh-ultimate-guide-e2/","summary":"本文是 InfoQ 自 2020 年 2 月发表的服务网格终极指南后的第二版，发布于 2021 年 9 月。","title":"服务网格终极指南第二版——下一代微服务开发"},{"content":"编者的话 本文译自 Envoy 代理的创始人 Matt Klein 于昨晚在个人博客上发布的文章 5 year of Envoy OSS。他在 Twitter 因为自己的程序 bug 造成重大事故而离职，后加入 Lyft，在开源 Envoy 之前几乎没有贡献和管理开源项目的经验，这篇文章分享了他个人及 Envoy 开源的心路历程，在投身开源 Envoy 还是为雇主 Lyft 效命，该如何抉择？看完本文，相信对于开源项目的维护者、创业者及投资人都会大有收获。\n前言 今天是 Envoy Proxy 开源的 5 周年。毫不夸张地说，在专业方面，过去的 5 年是一个史诗般的过山车，我的情绪介于兴奋、自豪、焦虑、尴尬、无聊、倦怠之间。我想分享一下这个项目的前传和历史，以及我在发展大型开源软件项目的过程中所学到的一些经验教训。\n前传和历史 前传 除了一些小的弯路，我在技术行业二十年的职业生涯一直专注于底层系统：嵌入式系统，操作系统，虚拟化，文件系统，以及最近的分布式系统网络。我的分布式系统网络之旅始于 2010 年初在亚马逊，我有幸帮助开发了第一批高性能计算（HPC）EC2 实例类型。我学到了大量 …","relpermalink":"/blog/envoy-oss-5-year/","summary":"开源网络代理 Envoy 的创始人 Matt Klein，在 Twitter 因为自己的程序 bug 造成重大事故而离职，后加入 Lyft，在开源 Envoy 之前几乎没有贡献和管理开源项目的经验，这篇文章分享了他个人及 Envoy 开源的心路历程，在投身开源 Envoy 还是为雇主 Lyft 效命，该如何抉择？","title":"网络代理 Envoy 开源五周年，创始人 Matt Klein 亲述开源心路历程及经验教训"},{"content":"编者的话 本文译自 Istio 官方博客 Security Best Practices。\nIstio 的安全功能提供了强大的身份、策略、透明的 TLS 加密以及认证、授权和审计（AAA）工具来保护你的服务和数据。然而，为了充分安全地利用这些功能，必须注意遵循最佳实践。建议在继续阅读之前，先回顾一下安全概述。\n双向 TLS Istio 将尽可能使用双向 TLS 对流量进行自动加密。然而，代理在默认情况下被配置为许可模式（Permissive Mode），这意味着他们将接受双向 TLS 和明文流量。\n虽然这是为了增量采用或允许来自没有 Istio sidecar 的客户端的流量的需要，但它也削弱了安全立场。建议在可能的情况下迁移到严格模式（Strict Mode），以强制使用双向 TLS。\n然而，仅靠双向 TLS 并不足以保证流量的安全，因为它只提供认证，而不是授权。这意味着，任何拥有有效证书的人仍然可以访问一个服务。\n为了完全锁定流量，建议配置授权策略。这允许创建细粒度的策略来允许或拒绝流量。例如，你可以只允许来自 app 命名空间的请求访问 hello-world 服务。 …","relpermalink":"/blog/istio-security-best-practices/","summary":"本文列举了 Istio 安全的最佳实践。","title":"Istio 安全最佳实践"},{"content":"Kubernetes Hardening Guidance National Security Agency Cybersecurity and Infrastructure Security Agency\nCybersecurity Technical Report\nNotices and history Document change history\nAugust 2021 1.0 Initial release\nDisclaimer of warranties and endorsement\nThe information and opinions contained in this document are provided “as is” and without any warranties or guarantees. Reference herein to any specific commercial products, process, or service by trade name, trademark, manufacturer, or otherwise, …","relpermalink":"/kubernetes-hardening-guidance/kubernetes-hardening-guidance-english/","summary":"Kubernetes Hardening Guidance National Security Agency Cybersecurity and Infrastructure Security Agency\nCybersecurity Technical Report\nNotices and history Document change history\nAugust 2021 1.0 Initial release\nDisclaimer of warranties and endorsement\nThe information and opinions contained in this document are provided “as is” and without any warranties or guarantees. Reference herein to any specific commercial products, process, or service by trade name, trademark, manufacturer, or otherwise, does not necessarily constitute or imply its endorsement, recommendation, or favoring by the United States Government, and this guidance shall not be used for advertising or product endorsement purposes.\nTrademark recognition\nKubernetes is a registered trademark of The Linux Foundation.","title":""},{"content":"","relpermalink":"/service-mesh-devsecops/intro/reference-platform-for-the-implementation-of-devsecops-primitives/","summary":"","title":""}]