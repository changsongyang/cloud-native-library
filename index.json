[{"authors":null,"categories":null,"content":"","date":1657011600,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1657070661,"objectID":"4177668dfb073bdc44828e933df22b61","permalink":"https://lib.jimmysong.io/author/%E5%88%98%E6%99%97/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/%E5%88%98%E6%99%97/","section":"authors","summary":"","tags":null,"title":"刘晗","type":"authors"},{"authors":null,"categories":null,"content":"","date":1657011600,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1657070661,"objectID":"ff0dc56628dae86c3b75f86be2f6ff27","permalink":"https://lib.jimmysong.io/author/%E5%90%B4%E6%99%9F/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/%E5%90%B4%E6%99%9F/","section":"authors","summary":"","tags":null,"title":"吴晟","type":"authors"},{"authors":null,"categories":null,"content":"","date":1608440400,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1657070661,"objectID":"6334034e53e76a7edeccedf4e23fafec","permalink":"https://lib.jimmysong.io/author/%E5%AE%8B%E5%87%80%E8%B6%85/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/%E5%AE%8B%E5%87%80%E8%B6%85/","section":"authors","summary":"","tags":null,"title":"宋净超","type":"authors"},{"authors":["istio"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1657070661,"objectID":"e1dcc55ed8948b6bc04d78f6ea41b0be","permalink":"https://lib.jimmysong.io/author/istio/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/istio/","section":"authors","summary":"","tags":null,"title":"Istio","type":"authors"},{"authors":["matt-klein"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1657070661,"objectID":"1724299346189377930e35df988f0127","permalink":"https://lib.jimmysong.io/author/matt-klein/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/matt-klein/","section":"authors","summary":"","tags":null,"title":"Matt Klein","type":"authors"},{"authors":["petr-mcallister"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1657070661,"objectID":"13e8bed83214de05c1e9c66ef1629a3d","permalink":"https://lib.jimmysong.io/author/petr-mcallister/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/petr-mcallister/","section":"authors","summary":"","tags":null,"title":"Petr Mcallister","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1657070661,"objectID":"6ef1c754edd87c14e514e589baf82a48","permalink":"https://lib.jimmysong.io/author/steven-landow/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/steven-landow/","section":"authors","summary":"","tags":null,"title":"Steven Landow","type":"authors"},{"authors":["varun-talwar"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1657070661,"objectID":"bcd86b9ed076f3d51cbbce8e97e78bf6","permalink":"https://lib.jimmysong.io/author/varun-talwar/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/varun-talwar/","section":"authors","summary":"","tags":null,"title":"Varun Talwar","type":"authors"},{"authors":["william-morgan"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1657070661,"objectID":"92fc141143770abac5ac6711f13bab7b","permalink":"https://lib.jimmysong.io/author/william-morgan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/william-morgan/","section":"authors","summary":"","tags":null,"title":"William Morgan","type":"authors"},{"authors":null,"categories":null,"content":"\n  注意：本书内容基于 Envoy 1.20 版本，部分命令在更新版本中可能遇到兼容性问题。   本手册梳理了 Envoy 基础知识，适用于初学者，帮你快速掌握 Envoy 代理。\n  《Envoy基础教程》封面  关于本书 Envoy 是一个开源的边缘和服务代理，专为云原生应用而设计。Envoy 与每个应用程序一起运行，通过提供网络相关的功能，如重试、超时、流量路由和镜像、TLS 终止等，以一种平台无关的方式抽象出网络。由于所有的网络流量都流经 Envoy 代理，因此很容易观察到流量和问题区域，调整性能，并准确定位延迟来源。\n本书为 Tetrate 出品的《Envoy 基础教程》的课程及实验内容，其他测试及考核证书的获取，请访问 Tetrate 学院。\n本书大纲 关于作者 宋净超（Jimmy Song），CNCF Ambassador，云原生社区创始人，个人网站 jimmysong.io。\n许可证 您可以使用署名 - 非商业性使用 - 相同方式共享 4.0 (CC BY-NC-SA 4.0) 协议共享。\n交流群 欢迎加入云原生社区微信讨论群，加入前请先填写入群申请问卷后联系 Jimmy Song 入群。\n 开始阅读   ","date":1656950400,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1657070661,"objectID":"c85f4e9c4db25d56088d2cef54dfdb81","permalink":"https://lib.jimmysong.io/hugo-in-action/","publishdate":"2022-07-05T00:00:00+08:00","relpermalink":"/hugo-in-action/","section":"hugo-in-action","summary":"静态网站","tags":null,"title":"Hugo 实战","type":"book"},{"authors":null,"categories":["网络"],"content":"\n  注：《Cilium 中文指南》除特殊说明，默认基于 Cilium 1.11 稳定版本。   《Cilium 中文指南》当前内容译自 Cilium 官方文档，节选了以下章节：\n 概念：描述了 Cilium 的组件以及部署 Cilium 的不同模式。 提供高层次的 运行一个完整的 Cilium 部署并理解其行为所需的高层次理解。 开始：快速开始使用 Cilium。 策略：详细介绍了策略语言结构和支持的格式。 内部原理：介绍了一些组件的内部细节。  其他未翻译部分主要涉及命令、API 及运维，本指南未来会加入笔者个人观点及其他内容。\n大纲   Cilium 和 Hubble 简介\n  多集群（集群网格）\n  Cilium 概念\n  网络\n  网络安全\n  eBPF 数据路径\n  网络策略\n  Kubernetes 集成\n   开始阅读   ","date":1654574400,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1657070661,"objectID":"57271c5f856915c577d9bfc3bca4ff53","permalink":"https://lib.jimmysong.io/cilium-handbook/","publishdate":"2022-06-07T12:00:00+08:00","relpermalink":"/cilium-handbook/","section":"cilium-handbook","summary":"Cilium 中文指南","tags":["Cilium"],"title":"Cilium 中文指南","type":"book"},{"authors":null,"categories":["Linux"],"content":"《什么是 eBPF —— 新一代网络、安全和可观测性工具介绍》译自 O’Reilly 发布的报告 “What is eBPF”，作者是 Liz Rice，由 JImmy Song 翻译，英文原版可以在 O’Reilly 网站上获取。\n  《什么是 eBPF》中文版封面  译者序 最近两年来关于 eBPF 的讨论在云原生社区里越来越多，尤其是当谈到 Cilium 的商业化，使用 eBPF 来优化 Istio 服务网格，甚至扬言干掉 Sidecar 时，eBPF 更是赚足了眼球。\n这本报告是由基于 Cilium 的创业公司 Isovalent 的 Liz Rice 撰写，由 O’Reilly 发布，相信可以为你揭开 eBPF 技术的神秘面纱，带你了解什么是 eBPF 还有它的强大之处。更重要的是它在云原生环境中，在服务网格、可观测性和安全中的应用。\n关于作者 Liz Rice 是云原生网络和安全专家，Isovalent 的首席开源官，是基于 eBPF 的 Cilium网络项目的创建者。她在 2019-2022 年担任 CNCF 的技术监督委员会（TOC）主席，并在 2018 年担任 KubeCon + CloudNativeCon 的联合主席。她也是 Container Security 一书的作者，由 O\u0026#39;Reilly 出版。她拥有丰富的软件开发、团队和产品管理经验，曾在网络协议和分布式系统以及数字技术领域（如 VOD、音乐和 VoIP）工作。在不写代码的时候，Liz 喜欢在天气比她的家乡伦敦好的地方骑自行车，和在 Zwift 上参加虚拟比赛。\n本书大纲   第一章：eBPF 简介\n  第二章：修改内核很困难\n  第三章：eBPF 程序\n  第四章：eBPF 的复杂性\n  第五章：云原生环境中的 eBPF\n  第六章：eBPF 工具\n  第七章：结论\n   开始阅读   ","date":1654056000,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1657070661,"objectID":"36d67d6777234055599fca124d2b8a47","permalink":"https://lib.jimmysong.io/what-is-ebpf/","publishdate":"2022-06-01T12:00:00+08:00","relpermalink":"/what-is-ebpf/","section":"what-is-ebpf","summary":"新一代网络、安全和可观测性工具简介。","tags":["eBPF"],"title":"什么是 eBPF？","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"云原生是一种行为方式和设计理念，究其本质，凡是能够提高云上资源利用率和应用交付效率的行为或方式都是云原生的。云计算的发展史就是一部云原生化的历史。Kubernetes 开启了云原生的序幕，服务网格 Istio 的出现，引领了后 Kubernetes 时代的微服务，Serverless 的兴起，使得云原生从基础设施层不断向应用架构层挺进，我们正处于一个云原生的新时代。\n  《Kubernetes 基础教程》封面  Kubernetes 是 Google 于 2014 年 6 月基于其内部使用的 Borg 系统开源出来的容器编排调度引擎，Google 将其作为初始和核心项目贡献给 CNCF（云原生计算基金会），近年来逐渐发展出了云原生生态。\nKubernetes 的目标不仅仅是一个编排系统，而是提供一个规范用以描述集群的架构，定义服务的最终状态，使系统自动地达到和维持该状态。Kubernetes 作为云原生应用的基石，相当于一个云原生操作系统，其重要性不言而喻。\n云原生技术有利于各组织在公有云、私有云和混合云等新型动态环境中，构建和运行可弹性扩展的应用。云原生的代表技术包括 容器、服务网格、微服务、不可变基础设施 和 声明式 API。这些技术能够构建容错性好、易于管理和便于观察的松耦合系统。结合可靠的自动化手段，云原生技术使工程师能够轻松地对系统作出频繁和可预测的重大变更。——CNCF（云原生计算基金会）。\n关于本书 Kubernetes Handbook 项目始于 2016 年底，开源于 2017 年 3 月，作为第一本系统介绍 Kubernetes 的中文电子书，其后经过不断完善。写作本书的过程中，笔者记录了从零开始学习和使用 Kubernetes 的历程，着重于经验总结和资料分享，亦有 Kubernetes 核心概念解析，希望能够帮助大家少走弯路，为大家介绍 Kubernetes 周边生态，如微服务、DevOps、大数据应用、服务网格、云原生应用、Serverless 等领域。\n本书大纲   Kubernetes 架构\n  资源对象\n  集群资源管理\n  控制器\n  服务发现与路由\n  身份与权限认证\n  网络\n  存储\n  扩展集群\n  多集群管理\n  资源对象配置\n  命令使用\n  集群安全性管理\n  访问集群\n  部署应用\n  开发指南\n  许可证 您可以使用署名 - 非商业性使用 - 相同方式共享 4.0 (CC BY-NC-SA 4.0) 协议共享。\n交流群 欢迎加入云原生社区微信讨论群，加入前请先填写入群申请问卷后联系 Jimmy Song 入群。\n 开始阅读   ","date":1653062400,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1657070661,"objectID":"80e116ea8343c2c2840676d1ca5d48bb","permalink":"https://lib.jimmysong.io/kubernetes-handbook/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/","section":"kubernetes-handbook","summary":"云原生应用架构实战手册","tags":["Kubernetes"],"title":"Kubernetes 基础教程","type":"book"},{"authors":null,"categories":["安全"],"content":"Kubernetes Hardening Guidance（查看英文原版 PDF） 是由美国国家安全局（NSA）于 2021 年 8 月发布的，其中文版《Kubernetes 加固指南》（或译作《Kubernetes 强化指南》），译者 Jimmy Song。\n  《Kubernetes加固指南》封面  许可证 您可以使用署名 - 非商业性使用 - 相同方式共享 4.0 (CC BY-NC-SA 4.0) 协议共享。\n交流群 欢迎加入云原生社区可观测性讨论组（微信群）参与讨论交流，加入前请先填写入群申请问卷后联系 Jimmy Song 入群。\n 开始阅读   ","date":1652889600,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1657070661,"objectID":"58218a6a687ffe5baccf7c93936e20aa","permalink":"https://lib.jimmysong.io/kubernetes-hardening-guidance/","publishdate":"2022-05-19T00:00:00+08:00","relpermalink":"/kubernetes-hardening-guidance/","section":"kubernetes-hardening-guidance","summary":"本指南译自美国国家安全局（NSA）于 2021 年 8 月发布的的 Kubernetes Hardening Guidance。","tags":["Kubernetes","安全"],"title":"Kubernetes 加固指南","type":"book"},{"authors":null,"categories":["Envoy"],"content":"\n  注意：本书内容基于 Envoy 1.20 版本，部分命令在更新版本中可能遇到兼容性问题。   本手册梳理了 Envoy 基础知识，适用于初学者，帮你快速掌握 Envoy 代理。\n  《Envoy基础教程》封面  关于本书 Envoy 是一个开源的边缘和服务代理，专为云原生应用而设计。Envoy 与每个应用程序一起运行，通过提供网络相关的功能，如重试、超时、流量路由和镜像、TLS 终止等，以一种平台无关的方式抽象出网络。由于所有的网络流量都流经 Envoy 代理，因此很容易观察到流量和问题区域，调整性能，并准确定位延迟来源。\n本书为 Tetrate 出品的《Envoy 基础教程》的课程及实验内容，其他测试及考核证书的获取，请访问 Tetrate 学院。\n本书大纲   Envoy 简介\n  HTTP 连接管理器\n  集群\n  动态配置\n  监听器子系统\n  日志\n  管理接口\n  扩展 Envoy\n  关于作者 宋净超（Jimmy Song），CNCF Ambassador，云原生社区创始人，个人网站 jimmysong.io。\n许可证 您可以使用署名 - 非商业性使用 - 相同方式共享 4.0 (CC BY-NC-SA 4.0) 协议共享。\n交流群 欢迎加入云原生社区微信讨论群，加入前请先填写入群申请问卷后联系 Jimmy Song 入群。\n 开始阅读   ","date":1652803200,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1657070661,"objectID":"10f2f42a96033280702982d0329ba73a","permalink":"https://lib.jimmysong.io/envoy-handbook/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/envoy-handbook/","section":"envoy-handbook","summary":"本手册梳理了 Envoy 基础知识，适用于初学者，帮你快速掌握 Envoy 代理。","tags":["Envoy"],"title":"Envoy 基础教程","type":"book"},{"authors":null,"categories":["Envoy"],"content":"在 Envoy 介绍章节中，你将了解 Envoy 的概况，并通过一个例子了解 Envoy 的构建模块。在本章结束时，你将通过运行 Envoy 的示例配置来了解 Envoy。\n本章大纲   什么是 Envoy？\n  Envoy 的构建模块\n   阅读本章   ","date":1652803200,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1657070661,"objectID":"c2825fc1f107df89f308e1698849459d","permalink":"https://lib.jimmysong.io/envoy-handbook/intro/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/envoy-handbook/intro/","section":"envoy-handbook","summary":"在 Envoy 介绍章节中，你将了解 Envoy 的概况，并通过一个例子了解 Envoy 的构建模块。在本章结束时，你将通过运行 Envoy 的示例配置来了解 Envoy。 本章大纲 什么是 Envoy？ Envoy 的构建模块 阅读本章","tags":["Envoy"],"title":"Envoy 简介","type":"book"},{"authors":null,"categories":["Istio"],"content":"\n  本文内容基于 Istio 1.11 或更新版本。   Istio 是由 Google、IBM、Lyft 等共同开源的 Service Mesh（服务网格）框架，于2017 年开源。\n  《Istio 基础教程》封面  关于本书 本书的主题包括：\n 服务网格概念解析 控制平面和数据平面的原理 Istio 架构详解 基于 Istio 的自定义扩展 迁移到 Istio 服务网格 构建云原生应用网络  书中部分内容来自 Tetrate 出品的 Istio 基础教程，请访问 Tetrate 学院，解锁全部教程及测试，获得 Tetrate 认证的 Istio 认证。\n本书大纲   服务网格概述\n  概念原理\n  数据平面\n  安装\n  流量管理\n  流量管理配置\n  可观测性\n  安全\n  安全配置\n  高级功能\n  问题排查\n  Istio 生态\n  实战案例\n  关于作者 宋净超（Jimmy Song），CNCF Ambassador，云原生社区创始人，个人网站 jimmysong.io。\n许可证 您可以使用署名 - 非商业性使用 - 相同方式共享 4.0 (CC BY-NC-SA 4.0) 协议共享。\n交流群 欢迎加入云原生社区微信讨论群，加入前请先填写入群申请问卷后联系 Jimmy Song 入群。\n 开始阅读   ","date":1652803200,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1657070661,"objectID":"92c1281b311cfb2840fa65f11e1b2b7e","permalink":"https://lib.jimmysong.io/istio-handbook/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/istio-handbook/","section":"istio-handbook","summary":"云原生应用网络构建指南。","tags":["Istio","Service Mesh"],"title":"Istio 基础教程","type":"book"},{"authors":null,"categories":null,"content":" 软件正在吞噬世界。\n——Mark Andreessen\n 近些年来，在一些长期由领导者支配的行业中，这些领导者的领先地位已经岌岌可危，这都是由以这些行业为核心业务的软件公司造成的。像Square、Uber、Netflix、Airbnb和特斯拉这样的公司能够持续快速增长，并且拥有傲人的市场估值，成为它们所在行业的新领导者。这些创新公司有什么共同点？\n  快速创新\n  持续可用的服务\n  弹性可扩展的Web\n  以移动为核心的用户体验\n  将软件迁移到云上是一种自演化，使用了云原生应用架构是这些公司能够如此具有破坏性的核心原因。对于云，我们指的是一个任何能够按需、自助弹性提供和释放计算、网络和存储资源的计算环境。云的定义包括公有云（例如 Amazon Web Services、Google Cloud和Microsoft Azure）和私有云（例如 VMware vSphere和 OpenStack）。\n本章中我们将探讨云原生应用架构的创新性，然后验证云原生应用架构的主要特性。\n 开始阅读   ","date":1652803200,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1657070661,"objectID":"5de5f2f69c139b8783a1d2df921c2300","permalink":"https://lib.jimmysong.io/migrating-to-cloud-native-application-architectures/the-rise-of-cloud-native/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/migrating-to-cloud-native-application-architectures/the-rise-of-cloud-native/","section":"migrating-to-cloud-native-application-architectures","summary":"软件正在吞噬世界。 ——Mark Andreessen 近些年来，在一些长期由领导者支配的行业中，这些领导者的领先地位已经岌岌可危，这都是由以这些行业为核心业务的软件公司造成的。像Square、Uber、Netflix、Ai","tags":null,"title":"第一章：云原生的崛起","type":"book"},{"authors":null,"categories":["文化"],"content":"Google 有许多通用工程实践，几乎涵盖所有语言和项目。此文档为长期积累的最佳实践，是集体经验的结晶。我们尽可能地将其公之于众，您的组织和开源项目也会从中受益。\n  《谷歌工程实践》封面  当前包含以下文档：\nGoogle 代码审查指南，实则两套指南：\n 代码审查者指南 代码开发者指南  译者序 此仓库翻译自 google/eng-practices，目前为止的主要内容为 Google 总结的如何进行 Code Review（代码审查） 指南，根据原 Github 仓库的标题判断以后会追加更多 Google 工程实践的内容。\n许可证 您可以使用署名 - 非商业性使用 - 相同方式共享 4.0 (CC BY-NC-SA 4.0) 协议共享。\n交流群 欢迎加入云原生社区微信讨论群，加入前请先填写入群申请问卷后联系 Jimmy Song 入群。\n 开始阅读   ","date":1652803200,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1657070661,"objectID":"ac520334b2cbeece15f0e5ad76ae0c0e","permalink":"https://lib.jimmysong.io/eng-practices/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/eng-practices/","section":"eng-practices","summary":"谷歌代码审查实践指南","tags":null,"title":"谷歌工程实践","type":"book"},{"authors":null,"categories":["安全"],"content":"本书译自美国国家标准标准与技术研究院（NIST）Special Publication 800-204C。\n  《利用服务网格为基于微服务的应用程序实施 DevSecOps》封面  本书大纲   声明\n  执行摘要\n  第一章：简介\n  第二章：实施 DevSecOps 原语的参考平台\n  第三章：DevSecOps 组织准备、关键基本要素和实施\n  第四章：为参考平台实施 DevSecOps 原语\n  第五章：摘要和结论\n  关于本书 作者：Ramaswamy Chandramouli\n计算机安全司信息技术实验室\n美国商务部\nGina M. Raimondo，秘书\n国家标准和技术研究所\nJames K. Olthoff，履行负责标准和技术的商务部副部长兼国家标准和技术研究所所长的非专属职能和职责\n本出版物可在：https://doi.org/10.6028/NIST.SP.800-204C 免费获取。\n 开始阅读   ","date":1652803200,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1657070661,"objectID":"c55f17d00038710de3838e3238179fd0","permalink":"https://lib.jimmysong.io/service-mesh-devsecops/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/service-mesh-devsecops/","section":"service-mesh-devsecops","summary":"本书译自美国国家标准标准与技术研究院（NIST）Special Publication 800-204C。","tags":["Service Mesh","DevSecOps"],"title":"利用服务网格为基于微服务的应用程序实施 DevSecOps","type":"book"},{"authors":null,"categories":null,"content":"本书是 Migrating to Cloud Native Application Architectures 的中文版，本书英文版发布于 2015 年 2 月，中文版由 Jimmy Song 翻译，发布于 2017 年 7 月。\n  《迁移到云原生应用架构》 图书封面  译者序 云时代的云原生应用大势已来，将传统的单体架构应用迁移到云原生架构，你准备好了吗？\n俗话说“意识决定行动”，在迁移到云原生应用之前，我们大家需要先对 Cloud Native（云原生）的概念、组织形式并对实现它的技术有一个大概的了解，这样才能指导我们的云原生架构实践。\n英文版作于2015年，其中的示例主要针对 Java 应用，实际上也适用于任何应用类型，云原生应用架构适用于异构语言的程序开发，不仅仅是针对 Java 语言的程序开发。截止到本人翻译本书时，云原生应用生态系统已经初具规模，CNCF 成员不断发展壮大，基于 Cloud Native 的创业公司不断涌现，Kubernetes 引领容器编排潮流，和 Service Mesh 技术（如 Linkerd 和 Istio） 的出现，Go 语言的兴起等为我们将应用迁移到云原生架构的提供了更多的方案选择。\n简介 当前很多企业正在采用云原生应用架构，这可以帮助其IT转型，成为市场竞争中真正敏捷的力量。 O’Reilly 的报告中定义了云原生应用架构的特性，如微服务和十二因素应用程序。\n本书中作者Matt Stine还探究了将传统的单体应用和面向服务架构（SOA）应用迁移到云原生架构所需的文化、组织和技术变革。本书中还有一个迁移手册，其中包含将单体应用程序分解为微服务，实施容错模式和执行云原生服务的自动测试的方法。\n本书中讨论的应用架构包括：\n 十二因素应用程序：云原生应用架构模式的集合 微服务：独立部署的服务，每个服务只做一件事情 自助服务的敏捷基础设施：快速，可重复和一致地提供应用环境和后台服务的平台 基于API的协作：发布和版本化的API，允许在云原生应用架构中的服务之间进行交互 抗压性：根据压力变强的系统  本章大纲   第一章：云原生的崛起\n  第二章：在变革中前行\n  第三章：迁移指南\n  关于作者 Matt Stine，Pivotal的技术产品经理，拥有15年企业IT和众多业务领域的经验。Matt 强调精益/敏捷方法、DevOps、架构模式和编程范例，他正在探究使用技术组合帮助企业IT部门能够像初创公司一样工作。\n 开始阅读   ","date":1652803200,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1657070661,"objectID":"a603c60a944fa395ef57abe67a9d1a9c","permalink":"https://lib.jimmysong.io/migrating-to-cloud-native-application-architectures/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/migrating-to-cloud-native-application-architectures/","section":"migrating-to-cloud-native-application-architectures","summary":"本书是 Migrating to Cloud Native Application Architectures 的中文版。","tags":null,"title":"迁移到云原生应用架构","type":"book"},{"authors":null,"categories":["云原生"],"content":"本书为 Cloud Native Infrastructure 中文版，作者 Justin Garrison 和 Kris Nova，英文版发行于 2017 年 11 月，已可以在网上免费获得，本书是关于创建和管理基础架构，以适用于云原生应用全生命周期管理的模式和实践。\n  《云原生基础架构》封面  阅读完这本书后，您将会有如下收获：\n 理解为什么说云原生基础架构是高效运行云原生应用所必须的 根据准则来决定您的业务何时以及是否应该采用云原生 了解部署和管理基础架构和应用程序的模式 设计测试以证明您的基础架构可以按预期工作，即使在各种边缘情况下也是如此 了解如何以策略即代码的方式保护基础架构  本书大纲   前言\n  介绍\n  第 1 章：什么是云原生基础架构？\n  第 2 章：采纳云原生基础架构的时机\n  第 3 章：云原生部署的演变\n  第 4 章：设计基础架构应用程序\n  第 5 章：开发基础架构应用程序\n  第 6 章：测试云原生基础架构\n  第 7 章：管理云原生应用程序\n  第 8 章：保护应用程序\n  第 9 章：实施云原生基础架构\n  附录 A：网络弹性模式\n  附录 B：锁定\n  附录 C Box：案例研究\n  免责声明 本书英文版版权属于 O’Reilly，中文版版权归属于机械工业出版，基于署名-非商业性使用-相同方式共享 4.0（CC BY-NC-SA 4.0）分享，本书为云原生爱好者翻译，仅可用于学习和交流目的，请勿私自印制贩卖，如有需要请购买纸质书。\n 开始阅读   ","date":1652803200,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1657070661,"objectID":"2027a42dab48d4a7a6315afb3889a6c6","permalink":"https://lib.jimmysong.io/cloud-native-infra/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/cloud-native-infra/","section":"cloud-native-infra","summary":"《云原生基础架构》，Cloud Native Infrastructure 中文版。","tags":["云原生"],"title":"云原生基础架构","type":"book"},{"authors":null,"categories":["可观测性"],"content":"本报告译自 O’Reilly 出品的 The Future of Observablity with OpeTelemetry，作者 Ted Young，译者 Jimmy Song。\n  《OpenTelemetry 可观测性的未来》封面  关于本书 本书内容包括：\n OpenTelemetry 如何满足库作者、应用程序拥有者、运维和响应者的需求 应用程序的不同角色如何围绕 OpenTelemetry 来协同和独立工作 关于在组织中采用和管理 OpenTelemetry 的实用建议  关于作者 Ted Young 是 OpenTelemetry 项目的联合创始人之一。在过去的二十年里，他设计并建立了各种大规模的分布式系统，包括可视化 FX 管道和容器调度系统。他目前在 Lightstep 公司担任开发者教育总监，住在俄勒冈州波特兰的一个小农场里。\n许可证 您可以使用署名 - 非商业性使用 - 相同方式共享 4.0 (CC BY-NC-SA 4.0) 协议共享。\n交流群 欢迎加入云原生社区可观测性讨论组（微信群）参与讨论交流，加入前请先填写入群申请问卷后联系 Jimmy Song 入群。\n 开始阅读   ","date":1651536000,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1657070661,"objectID":"97582bf10d4164c190d079eb4828b3ca","permalink":"https://lib.jimmysong.io/opentelemetry-obervability/","publishdate":"2022-05-03T00:00:00Z","relpermalink":"/opentelemetry-obervability/","section":"opentelemetry-obervability","summary":"本报告译自 O'Reilly 出品的 The Future of Observablity with OpeTelemetry，作者 Ted Young，译者 Jimmy Song。","tags":["OpenTelemetry"],"title":"OpenTelemetry 可观测性的未来","type":"book"},{"authors":null,"categories":["云原生"],"content":"欢迎来到云原生导览——一站式了解云原生技术体系。\n导览图   关于云原生\n  Kubernetes\n  服务网格\n  社区\n   开始阅读   ","date":1651536000,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1657070661,"objectID":"21e0c85bf05e46b8b242dbca3cfc0134","permalink":"https://lib.jimmysong.io/cloud-native-handbook/","publishdate":"2022-05-03T00:00:00Z","relpermalink":"/cloud-native-handbook/","section":"cloud-native-handbook","summary":"云原生技术体系导览","tags":["云原生"],"title":"云原生导览","type":"book"},{"authors":null,"categories":["云原生"],"content":"本章将介绍什么是云原生、云原生应用。\n本章大纲   什么是云原生？\n  云原生的设计哲学\n  什么是云原生应用？\n  云原生快速入门\n   阅读本章   ","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1657070661,"objectID":"5ee05dd754d9cc434268c79fe9aeeee3","permalink":"https://lib.jimmysong.io/cloud-native-handbook/intro/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/cloud-native-handbook/intro/","section":"cloud-native-handbook","summary":"本章介绍什么是云原生。","tags":["云原生"],"title":"关于云原生","type":"book"},{"authors":null,"categories":["文化"],"content":"简介 代码审查是除了代码作者之外，其他人检查代码的过程。\nGoogle 通过 Code Review 来维护代码和产品质量。\n此文档是 Google Code Review 流程和政策的规范说明。\n此页面是我们进行 Code Review 流程的概述。本指南还有另外两套文档：\n 如何进行 Code Review：针对代码审查者的详细指南。 代码开发者指南：针对 CL 开发者的的详细指南。  代码审查者应该关注哪些方面？ 代码审查时应该关注以下方面：\n 设计：代码是否经过精心设计并适合您的系统？ 功能：代码的行为是否与作者的意图相同？代码是否可以正常响应用户的行为？ 复杂度：代码能更简单吗？将来其他开发人员能轻松理解并使用此代码吗？ 测试：代码是否具有正确且设计良好的自动化测试？ 命名：开发人员是否为变量、类、方法等选择了明确的名称？ 注释：注释是否清晰有用？ 风格：代码是否遵守了风格指南？ 文档：开发人员是否同时更新了相关文档？  参阅 如何进行 Code Review 获取更多资料。\n选择最合适审查者 一般而言，您希望找到能在合理的时间内回复您的评论的最合适的审查者。\n最合适的审查者应该是能彻底了解和审查您代码的人。他们通常是代码的所有者，可能是 OWNERS 文件中的人，也可能不是。有时 CL 的不同部分可能需要不同的人审查。\n如果您找到了理想的审查者但他们又没空，那您也至少要抄送他们。\n面对面审查 如果您与有资格做代码审查的人一起结对编程了一段代码，那么该代码将被视为已审查。\n您还可以进行面对面的代码审查，审查者提问，CL 的开发人员作答。\n术语 文档中使用了 Google 内部术语，在此为外部读者澄清：\n CL：代表 “变更列表（Change List）”，表示已提交到版本控制或正在进行代码审查的自包含更改。有的组织会将其称为 “变更（change）” 或 “补丁（patch）”。 LGTM：意思是 “我觉得不错（Looks Good to Me）”。这是批准 CL 时代码审查者所说的。  参考  如何进行 Code Review：针对代码审查者的详细指南。 代码开发者指南：针对 CL 开发者的的详细指南。  ","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1657070661,"objectID":"7604ca651c03ef6c0cb7acfe4ef65b5d","permalink":"https://lib.jimmysong.io/eng-practices/review/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/eng-practices/review/","section":"eng-practices","summary":"简介 代码审查是除了代码作者之外，其他人检查代码的过程。 Google 通过 Code Review 来维护代码和产品质量。 此文档是 Google Code Review 流程和政策的规范说明。 此页面是我们进行 Code Review 流程的概述。本指南还有另外两套文档： 如何进行 Code Review","tags":null,"title":"代码审查指南","type":"book"},{"authors":null,"categories":["Envoy"],"content":"在 HCM 章节中，我们将对 HTTP 连接管理器过滤器进行扩展。我们将学习过滤器的排序，以及 HTTP 路由和匹配如何工作。我们将向你展示如何分割流量、操作 Header 信息、配置超时、实现重试、请求镜像和速率限制。\n在本章结束时，你将对 HCM 过滤器有一个很好的理解，以及如何路由和拆分 HTTP 流量，操纵 Header 等等。\n本章大纲   HCM 简介\n  HTTP 路由\n  请求匹配\n  流量分割\n  Header 操作\n  修改响应\n  生成请求 ID\n  超时\n  重试\n  请求镜像\n  速率限制\n  局部速率限制\n  全局速率限制\n  实验1：请求匹配\n  实验2：流量分割\n  实验3：Header操作\n  实验4:重试\n  实验5：局部速率限制\n  实验6：全局速率限制\n  \n   阅读本章   ","date":1652803200,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1657070661,"objectID":"1b089774cac533a634aea664bf5f811a","permalink":"https://lib.jimmysong.io/envoy-handbook/hcm/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/envoy-handbook/hcm/","section":"envoy-handbook","summary":"在 HCM 章节中，我们将对 HTTP 连接管理器过滤器进行扩展。我们将学习过滤器的排序，以及 HTTP 路由和匹配如何工作。我们将向你展示如何分割流量、操作 Header 信息、配置超时、实现重试、请求镜像和速率限制。 在本章结束时，你将对 HCM","tags":["Envoy"],"title":"HTTP 连接管理器","type":"book"},{"authors":null,"categories":["云原生"],"content":"本章将介绍什么是云原生、云原生应用。\n本章大纲   什么是 Kubernetes?\n  Kubernetes 的历史\n  Sidecar 模式\n   阅读本章   ","date":1651536000,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1657070661,"objectID":"61e94dfe61463c832219c883c5589ce6","permalink":"https://lib.jimmysong.io/cloud-native-handbook/kubernetes/","publishdate":"2022-05-03T00:00:00Z","relpermalink":"/cloud-native-handbook/kubernetes/","section":"cloud-native-handbook","summary":"关于Kubernetes。","tags":["云原生"],"title":"Kubernetes","type":"book"},{"authors":null,"categories":["云原生"],"content":"本章将介绍服务网格。\n 阅读本章   ","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1657070661,"objectID":"99e820189b2aa62b5f6325acbfc5ad4a","permalink":"https://lib.jimmysong.io/cloud-native-handbook/service-mesh/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/cloud-native-handbook/service-mesh/","section":"cloud-native-handbook","summary":"本章介绍服务网格。","tags":["云原生"],"title":"服务网格","type":"book"},{"authors":null,"categories":["网络"],"content":"概念一章对 Cilium 和 Hubble 的所有方面进行了更深入的介绍。如果你想了解 Cilium 和 Hubble 的概要介绍，请参阅 Cilium 和 Hubble 简介。\n本章大纲   组件概览\n  Cilium 术语说明\n  可观测性\n   阅读本章   ","date":1654574400,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1657070661,"objectID":"acdcbb5eee94d3cd9039aff2e4d8f48b","permalink":"https://lib.jimmysong.io/cilium-handbook/concepts/","publishdate":"2022-06-07T12:00:00+08:00","relpermalink":"/cilium-handbook/concepts/","section":"cilium-handbook","summary":"概念一章对 Cilium 和 Hubble 的所有方面进行了更深入的介绍。如果你想了解 Cilium 和 Hubble 的概要介绍，请参阅 Cilium 和 Hubble 简介。 本章大纲 组件概览 Cilium 术语说明 可观测性 阅读本章","tags":["Cilium"],"title":"Cilium 概念","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"Kubernetes 最初源于谷歌内部的 Borg，提供了面向应用的容器集群部署和管理系统。Kubernetes 的目标旨在消除编排物理 / 虚拟计算，网络和存储基础设施的负担，并使应用程序运营商和开发人员完全将重点放在以容器为中心的原语上进行自助运营。Kubernetes 也提供稳定、兼容的基础（平台），用于构建定制化的 workflows 和更高级的自动化任务。\nKubernetes 具备完善的集群管理能力，包括多层次的安全防护和准入机制、多租户应用支撑能力、透明的服务注册和服务发现机制、内建负载均衡器、故障发现和自我修复能力、服务滚动升级和在线扩容、可扩展的资源自动调度机制、多粒度的资源配额管理能力。Kubernetes 还提供完善的管理工具，涵盖开发、部署测试、运维监控等各个环节。\nBorg 简介 Borg 是谷歌内部的大规模集群管理系统，负责对谷歌内部很多核心服务的调度和管理。Borg 的目的是让用户能够不必操心资源管理的问题，让他们专注于自己的核心业务，并且做到跨多个数据中心的资源利用率最大化。\nBorg 主要由 BorgMaster、Borglet、borgcfg 和 Scheduler 组成，如下图所示\n   Borg 架构   BorgMaster 是整个集群的大脑，负责维护整个集群的状态，并将数据持久化到 Paxos 存储中； Scheduer 负责任务的调度，根据应用的特点将其调度到具体的机器上去； Borglet 负责真正运行任务（在容器中）； borgcfg 是 Borg 的命令行工具，用于跟 Borg 系统交互，一般通过一个配置文件来提交任务。  Kubernetes 架构 Kubernetes 借鉴了 Borg 的设计理念，比如 Pod、Service、Label 和单 Pod 单 IP 等。Kubernetes 的整体架构跟 Borg 非常像，如下图所示。\n   Kubernetes 架构  Kubernetes 主要由以下几个核心组件组成：\n etcd 保存了整个集群的状态； apiserver 提供了资源操作的唯一入口，并提供认证、授权、访问控制、API 注册和发现等机制； controller manager 负责维护集群的状态，比如故障检测、自动扩展、滚动更新等； scheduler 负责资源的调度，按照预定的调度策略将 Pod 调度到相应的机器上； kubelet 负责维护容器的生命周期，同时也负责 Volume（CSI）和网络（CNI）的管理； Container runtime 负责镜像管理以及 Pod 和容器的真正运行（CRI）； kube-proxy 负责为 Service 提供 cluster 内部的服务发现和负载均衡；  除了核心组件，还有一些推荐的插件，其中有的已经成为 CNCF 中的托管项目：\n CoreDNS 负责为整个集群提供 DNS 服务 Ingress Controller 为服务提供外网入口 Prometheus 提供资源监控 Dashboard 提供 GUI Federation 提供跨可用区的集群  Kubernetes 架构示意图 整体架构 下图清晰表明了 Kubernetes 的架构设计以及组件之间的通信协议。\n   Kuberentes 架构（图片来自于网络）  下面是更抽象的一个视图：\n   kubernetes 整体架构示意图  Master 架构    Kubernetes master 架构示意图  Node 架构    kubernetes node 架构示意图  分层架构 Kubernetes 设计理念和功能其实就是一个类似 Linux 的分层架构，如下图所示。\n   Kubernetes 分层架构示意图   核心层：Kubernetes 最核心的功能，对外提供 API 构建高层的应用，对内提供插件式应用执行环境 应用层：部署（无状态应用、有状态应用、批处理任务、集群应用等）和路由（服务发现、DNS 解析等）、Service Mesh（部分位于应用层） 管理层：系统度量（如基础设施、容器和网络的度量），自动化（如自动扩展、动态 Provision 等）以及策略管理（RBAC、Quota、PSP、NetworkPolicy 等）、Service Mesh（部分位于管理层） 接口层：kubectl 命令行工具、客户端 SDK 以及集群联邦 生态系统：在接口层之上的庞大容器集群管理调度的生态系统，可以划分为两个范畴  Kubernetes 外部：日志、监控、配置管理、CI/CD、Workflow、FaaS、OTS 应用、ChatOps、GitOps、SecOps 等 Kubernetes 内部：CRI、CNI、CSI、镜像仓库、Cloud Provider、集群自身的配置和管理等     关于分层架构，可以关注下 Kubernetes architectual roadmap 和 幻灯片。\n 本节大纲   Kubernetes 的设计理念\n  Etcd 解析\n  开放接口\n  Pod 状态与生命周期管理\n  参考  Borg, Omega, and Kubernetes - Lessons learned from three container-management systems over a decade Paper - Large-scale cluster management at Google with Borg KUBERNETES: AN OVERVIEW Kubernetes architectual roadmap 和 slide  ","date":1653062400,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1657070661,"objectID":"6badb69865095fc479f82d015021015c","permalink":"https://lib.jimmysong.io/kubernetes-handbook/architecture/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/architecture/","section":"kubernetes-handbook","summary":"Kubernetes 最初源于谷歌内部的 Borg，提供了面向应用的容器集群部署和管理系统。Kubernetes 的目标旨在消除编排物理 / 虚拟计算，网络和存储基础设施的负担，并使应用程序运营商和开发人员完全将重点放在以容器为","tags":["Kubernetes"],"title":"Kubernetes 架构","type":"book"},{"authors":null,"categories":null,"content":" 从客户给我们下达订单开始，一直到我们收到现金为止，我们一直都关注时间线。而且我们正在通过删除非附加值的废物来减少这个时间表。\n—— Taichi Ohno\n Taichi Ohno被公认为精益制造之父。虽然精益制造的实践无法完全适用于软件开发领域，但它们的原则是一致的。 这些原则可以指导我们很好地寻求典型的企业IT组织采用云原生应用架构所需的变革，并且接受作为这一转变所带来的部分的文化和组织转型。\n 开始阅读   ","date":1652803200,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1657070661,"objectID":"b18316be08ec2131cfa33eeeba4210f8","permalink":"https://lib.jimmysong.io/migrating-to-cloud-native-application-architectures/changes-needed/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/migrating-to-cloud-native-application-architectures/changes-needed/","section":"migrating-to-cloud-native-application-architectures","summary":"从客户给我们下达订单开始，一直到我们收到现金为止，我们一直都关注时间线。而且我们正在通过删除非附加值的废物来减少这个时间表。 —— Taichi Ohno Taichi Ohno被公认为精益制造之父。虽然精益制造的实践无法完全适用于软件","tags":null,"title":"第二章：在变革中前行","type":"book"},{"authors":null,"categories":null,"content":"现在我们已经定义了云原生应用架构，并简要介绍了企业在采用它们时必须考虑做出的变化，现在是深入研究技术细节的时候了。对每个技术细节的深入讲解已经超出了本报告的范围。本章中仅是对采用云原生应用架构后，需要做的特定工作和采用的模式的一系列简短的介绍，文中还给出了一些进一步深入了解这些方法的链接。\n 开始阅读   ","date":1652803200,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1657070661,"objectID":"79fa4a865a8f8efa678e3768da1c0375","permalink":"https://lib.jimmysong.io/migrating-to-cloud-native-application-architectures/migration-cookbook/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/migrating-to-cloud-native-application-architectures/migration-cookbook/","section":"migrating-to-cloud-native-application-architectures","summary":"现在我们已经定义了云原生应用架构，并简要介绍了企业在采用它们时必须考虑做出的变化，现在是深入研究技术细节的时候了。对每个技术细节的深入讲解已经超出了本报告的范围。本章中仅是对采用云原生应用架构后，需要","tags":null,"title":"第三章：迁移指南","type":"book"},{"authors":null,"categories":["安全"],"content":"云原生应用由多个松散耦合的组件（称为微服务，通常以容器形式实现）组成，在需要零信任概念的无边界网络环境中运行（企业内部或云），并由来自不同地点的用户访问（例如，校园、家庭办公室等）。云原生应用不只是指在云中运行的应用。它们还指具有设计和运行时架构的一类应用，如微服务，以及用于提供所有应用服务（包括安全）的专用基础设施。将 零信任原则 纳入这类应用提供了一些技术，其中对所有受保护资源的访问是通过基于身份的保护和基于网络的保护（如微分）来强制执行的。\n由于业务原因，云原生应用程序需要敏捷和安全的更新和部署技术，以及应对网络安全事件的必要弹性。因此，它们需要一种与传统的单层或多层应用不同的应用开发、部署和运行时监控范式（统称为软件生命周期范式）。DevSecOps（开发、安全和运维）是这类应用的促进范式，因为它通过（a）持续集成、持续交付 / 持续部署（CI/CD）管道（在第 3 节中解释）等基本要素促进了敏捷和安全的开发、交付、部署和运维；（b）整个生命周期的安全测试；以及（c）运行时的持续监控，所有这些都由自动化工具支持。事实上，满足上述目标的范式最初被赋予了 DevOps 这个术语，以表明它试图消除开发和运维之间的隔阂，并促进（或推动）加强合作。后来，DevSecOps 这个词是由社区的一部分人创造的，以强调安全团队在整个过程中的作用。因此，DevSecOps 这个术语表示一种文化和一套带有自动化工具的实践，以推动负责交付软件的关键利益相关者（包括开发、运维和安全组织）之间加强协作、信任、分担责任、透明度、自主性、敏捷性和自动化。DevSecOps 拥有必要的基本要素和其他构建模块，以满足云原生应用的设计目标。\n应该注意的是，整个社区对 DevSecOps 一词并无共识。如前所述，该术语主要是为了强调一个事实，即必须在软件开发生命周期的所有阶段（即构建、测试、打包、部署和运行）对安全进行测试和整合。社区中的一部分人继续使用 DevOps 这个术语，理由是没有必要定义一个新的术语，因为安全必须是任何软件生命周期过程的一个组成部分。\n本章大纲   1.1 范围\n  1.2 相关的 DevSecOps 倡议\n  1.3 目标受众\n  1.4 与其他 NIST 指导文件的关系\n  1.5 本文件的组织\n  \n   开始阅读   ","date":1652803200,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1657070661,"objectID":"7c20075273864276e68db48d6a74f4e5","permalink":"https://lib.jimmysong.io/service-mesh-devsecops/intro/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/service-mesh-devsecops/intro/","section":"service-mesh-devsecops","summary":"云原生应用由多个松散耦合的组件（称为微服务，通常以容器形式实现）组成，在需要零信任概念的无边界网络环境中运行（企业内部或云），并由来自不同地点的用户访问（例如，校园、家庭办公室等）。云原生应用不只是指","tags":["Service Mesh","DevSecOps"],"title":"第一章：简介","type":"book"},{"authors":null,"categories":["Envoy"],"content":"在本章中，我们将学习集群以及如何管理它们。在 Envoy 的集群配置部分，我们可以配置一些功能，如负载均衡、健康检查、连接池、异常点检测等。\n在本章结束时，你将了解集群和端点是如何工作的，以及如何配置负载均衡策略、异常点检测和断路。\n本章大纲   服务发现\n  主动健康检查\n  异常点检测\n  断路器\n  负载均衡\n  实验7：断路器\n   阅读本章   ","date":1652803200,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1657070661,"objectID":"296b19cc52b86725568e4f0e38a83925","permalink":"https://lib.jimmysong.io/envoy-handbook/cluster/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/envoy-handbook/cluster/","section":"envoy-handbook","summary":"在本章中，我们将学习集群以及如何管理它们。在 Envoy 的集群配置部分，我们可以配置一些功能，如负载均衡、健康检查、连接池、异常点检测等。 在本章结束时，你将了解集群和端点是如何工作的，以及如何配置负载均衡策略、","tags":["Envoy"],"title":"集群","type":"book"},{"authors":null,"categories":["文化"],"content":"本节页面的内容为开发人员进行代码审查的最佳实践。这些指南可帮助您更快地完成审核并获得更高质量的结果。您不必全部阅读它们，但它们适用于每个 Google 开发人员，并且许阅读全文通常会很有帮助。\n 写好 CL 描述 小型 CL 如何处理审查者的评论  另请参阅代码审查者指南，它为代码审阅者提供了详细的指导。\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1657070661,"objectID":"1bc0acdabd47c346b08c8e5dabfa9dd3","permalink":"https://lib.jimmysong.io/eng-practices/review/developer/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/eng-practices/review/developer/","section":"eng-practices","summary":"本节页面的内容为开发人员进行代码审查的最佳实践。这些指南可帮助您更快地完成审核并获得更高质量的结果。您不必全部阅读它们，但它们适用于每个 Google 开发人员，并且许阅读全文通常会很有帮助。 写好 CL 描述 小型 CL 如何处","tags":null,"title":"开发者指南","type":"book"},{"authors":null,"categories":["文化"],"content":"本节是基于过往经验编写的 Code Review 最佳方式建议。其中分为了很多独立的部分，共同组成完整的文档。虽然您不必阅读文档，但通读一遍会对您自己和团队很有帮助。\n Code Review 标准 Code Review 要点 查看 CL 的步骤 Code Review 速度 如何撰写 Code Review 评论 处理 Code Review 中的抵触  另请参阅代码开发者指南，该指南为正在进行 Code Review 的开发开发者提供详细指导。\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1657070661,"objectID":"58a2fff07384fdf4647c4de2f3ee82c0","permalink":"https://lib.jimmysong.io/eng-practices/review/reviewer/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/eng-practices/review/reviewer/","section":"eng-practices","summary":"本节是基于过往经验编写的 Code Review 最佳方式建议。其中分为了很多独立的部分，共同组成完整的文档。虽然您不必阅读文档，但通读一遍会对您自己和团队很有帮助。 Code Review 标准 Code Review 要点 查看 CL 的步骤 Code Review 速度 如何撰写 Code Review 评论 处理","tags":null,"title":"审查者指南","type":"book"},{"authors":null,"categories":["网络"],"content":"本章大纲   路由\n  IP 地址管理（IPAM）\n  IP 地址伪装\n  IPv4 分片处理\n   阅读本章   ","date":1655438400,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1657070661,"objectID":"562b8363e0b02790fe4ac50d26bc23da","permalink":"https://lib.jimmysong.io/cilium-handbook/networking/","publishdate":"2022-06-17T12:00:00+08:00","relpermalink":"/cilium-handbook/networking/","section":"cilium-handbook","summary":"本章大纲 路由 IP 地址管理（IPAM） IP 地址伪装 IPv4 分片处理 阅读本章","tags":["Cilium"],"title":"网络","type":"book"},{"authors":null,"categories":["网络"],"content":"本章大纲   介绍\n  基于身份\n  策略执行\n  代理注入\n   阅读本章   ","date":1655438400,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1657070661,"objectID":"7bf0eaefd278859c3c600c7eba2fd01d","permalink":"https://lib.jimmysong.io/cilium-handbook/security/","publishdate":"2022-06-17T12:00:00+08:00","relpermalink":"/cilium-handbook/security/","section":"cilium-handbook","summary":"本章大纲 介绍 基于身份 策略执行 代理注入 阅读本章","tags":["Cilium"],"title":"网络安全","type":"book"},{"authors":null,"categories":["安全"],"content":"如第 1.1 节所述，参考平台是一个容器编排和管理平台。在现代应用环境中，平台是物理（裸机）或虚拟化（如虚拟机、容器）基础设施上的一个抽象层。在实施 DevSecOps 原语之前，平台只是包含了应用代码，其中包含了应用逻辑和服务网状代码，而服务网状代码又提供应用服务。本节将考虑以下内容：\n 一个容器编排和资源管理平台，容纳了应用程序代码和大部分的服务网格代码 服务网格的软件架构  本章大纲   2.1 容器编排和资源管理平台\n  2.2 服务网格架构\n   开始阅读   ","date":1652803200,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1657070661,"objectID":"c729e106809ef8c4780a69e52e1523df","permalink":"https://lib.jimmysong.io/service-mesh-devsecops/reference-platform/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/service-mesh-devsecops/reference-platform/","section":"service-mesh-devsecops","summary":"如第 1.1 节所述，参考平台是一个容器编排和管理平台。在现代应用环境中，平台是物理（裸机）或虚拟化（如虚拟机、容器）基础设施上的一个抽象层。在实施 DevSecOps 原语之前，平台只是包含了应用代码，其中包含了应用逻辑和服务","tags":["Service Mesh","DevSecOps"],"title":"第二章：实施 DevSecOps 原语的参考平台","type":"book"},{"authors":null,"categories":["Envoy"],"content":"在本章中，我们将学习如何使用动态配置来配置 Envoy 代理。到现在为止，我们一直在使用静态配置。本章将教我们如何在运行时从文件系统或通过网络使用发现服务为单个资源提供配置。\n在本章结束时，你将了解静态和动态配置之间的区别。\n本章大纲   动态配置简介\n  来自文件系统的动态配置\n  来自控制平面的动态配置\n  实验 8：来自文件系统的动态配置\n   阅读本章   ","date":1652803200,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1657070661,"objectID":"fd749a86315e5912d690d0630631ead5","permalink":"https://lib.jimmysong.io/envoy-handbook/dynamic-config/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/envoy-handbook/dynamic-config/","section":"envoy-handbook","summary":"在本章中，我们将学习如何使用动态配置来配置 Envoy 代理。到现在为止，我们一直在使用静态配置。本章将教我们如何在运行时从文件系统或通过网络使用发现服务为单个资源提供配置。 在本章结束时，你将了解静态和动态配置之","tags":["Envoy"],"title":"动态配置","type":"book"},{"authors":null,"categories":["云原生"],"content":"本章将介绍什么是云原生、云原生应用。\n 阅读本章   ","date":1651536000,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1657070661,"objectID":"ecc35a7fba09a33544dc14edf01bc287","permalink":"https://lib.jimmysong.io/cloud-native-handbook/community/","publishdate":"2022-05-03T00:00:00Z","relpermalink":"/cloud-native-handbook/community/","section":"cloud-native-handbook","summary":"社区","tags":["云原生"],"title":"社区","type":"book"},{"authors":null,"categories":["网络"],"content":"本章大纲   介绍\n  数据包流程\n  eBPF Map\n  Iptables 用法\n   阅读本章   ","date":1655438400,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1657070661,"objectID":"73827cdd3c436ca45ed72d239bc53f5b","permalink":"https://lib.jimmysong.io/cilium-handbook/ebpf/","publishdate":"2022-06-17T12:00:00+08:00","relpermalink":"/cilium-handbook/ebpf/","section":"cilium-handbook","summary":"本章大纲 介绍 数据包流程 eBPF Map Iptables 用法 阅读本章","tags":["Cilium"],"title":"eBPF 数据路径","type":"book"},{"authors":null,"categories":["安全"],"content":"DevSecOps 在早期就将安全纳入了软件工程流程。它将安全流程和工具集成到 DevOps 的所有开发工作流程（或后面解释的管道）中，并使之自动化，从而实现无缝和连续。换句话说，它可以被看作是三个过程的组合。开发 + 安全 + 运维。\n本节讨论了 DevSecOps 的以下方面：\n 组织对 DevSecOps 的准备情况 开发安全运维平台 开发安全运维的基本构件或关键原语  本章大纲   3.1 组织对 DevSecOps 的准备情况\n  3.2 DevSecOps 平台\n  3.3 DevSecOps 关键原语和实施任务\n   开始阅读   ","date":1652803200,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1657070661,"objectID":"7b8a809be83d4168ed51285e43725921","permalink":"https://lib.jimmysong.io/service-mesh-devsecops/devsecops/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/service-mesh-devsecops/devsecops/","section":"service-mesh-devsecops","summary":"DevSecOps 在早期就将安全纳入了软件工程流程。它将安全流程和工具集成到 DevOps 的所有开发工作流程（或后面解释的管道）中，并使之自动化，从而实现无缝和连续。换句话说，它可以被看作是三个过程的组合。开发 + 安全 + 运维。 本节","tags":["Service Mesh","DevSecOps"],"title":"第三章：DevSecOps 组织准备、关键基本要素和实施","type":"book"},{"authors":null,"categories":["Envoy"],"content":"在监听器子系统章节中，我们将学习 Envoy 代理的监听器子系统。我们将介绍过滤器、过滤器链匹配、以及不同的监听器过滤器。\n在本章节结束时，你将了解监听器子系统的不同部分，并知道过滤器和过滤器链匹配是如何工作的。\n本章大纲   监听器过滤器\n  Header 操作\n  HTTP 检查器监听器过滤器\n  原始目的地监听器过滤器\n  原始源监听器过滤器\n  代理协议监听器过滤器\n  TLS 检查器监听器过滤器\n  实验 9：原始目的地过滤器\n  实验 10：TLS 检查器过滤器\n  实验 11：匹配传输和应用协议\n   阅读本章   ","date":1652803200,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1657070661,"objectID":"6e361f02528b59f11bddcac813ae4694","permalink":"https://lib.jimmysong.io/envoy-handbook/listener/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/envoy-handbook/listener/","section":"envoy-handbook","summary":"在监听器子系统章节中，我们将学习 Envoy 代理的监听器子系统。我们将介绍过滤器、过滤器链匹配、以及不同的监听器过滤器。 在本章节结束时，你将了解监听器子系统的不同部分，并知道过滤器和过滤器链匹配是如何工作的。 本","tags":["Envoy"],"title":"监听器子系统","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"Kubernetes 作为云原生应用的基础调度平台，相当于云原生的操作系统，为了便于系统的扩展，Kubernetes 中开放的以下接口，可以分别对接不同的后端，来实现自己的业务逻辑：\n 容器运行时接口（CRI）：提供计算资源 容器网络接口（CNI）：提供网络资源 容器存储接口（CSI），提供存储资源  以上三种资源相当于一个分布式操作系统的最基础的几种资源类型，而 Kuberentes 是将他们粘合在一起的纽带。\n本节大纲   容器运行时接口（CRI）\n  容器网络接口（CNI）\n  容器存储接口（CSI）\n  ","date":1653062400,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1657070661,"objectID":"dbc1b85b6924d3f8916b70a58f931757","permalink":"https://lib.jimmysong.io/kubernetes-handbook/architecture/open-interfaces/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/architecture/open-interfaces/","section":"kubernetes-handbook","summary":"Kubernetes 作为云原生应用的基础调度平台，相当于云原生的操作系统，为了便于系统的扩展，Kubernetes 中开放的以下接口，可以分别对接不同的后端，来实现自己的业务逻辑： 容器运行时接口（CRI）：提供计算资源 容","tags":["Kubernetes"],"title":"开放接口","type":"book"},{"authors":null,"categories":["安全"],"content":"各种 CI/CD 管道都涉及到参考平台（即基于微服务的应用，有提供基础设施服务的服务网格）。虽然参考应用是基于微服务的应用，但 DevSecOps 的原语可以应用于单体应用以及既在企业内部又基于云的应用（如混合云、单一公有云和多云）。\n在第 2.1 节中，我们提到了我们参考应用环境中的五种代码类型。我们还提到，也可以为这五种代码类型中的每一种创建单独的 CI/CD 管道。这五种代码类型在参考平台组件中的位置将被讨论，然后是描述相关 CI/CD 管道的单独章节：\n 参考平台中的代码类型和相关的 CI/CD 管道（4.1 节） 应用程序代码和应用服务代码的 CI/CD 管道（4.2 节） 基础设施即代码（IaC）的 CI/CD 管道（4.3 节） 策略即代码的 CI/CD 管道（4.4 节） 可观测性即代码的 CI/CD 管道（4.5 节）  所有 CI/CD 管道的实施问题，无论代码类型如何，都将在以下章节中讨论：\n 确保 CI/CD 管道的安全（4.6 节） CI/CD 管道中的工作流模型（4.7 节） CI/CD 管道中的安全测试（4.8 节）  本节还将考虑 DevSecOps 的整体优势，并在第 4.9 节和第 4.10 节分别介绍参考平台的具体优势和利用 DevSecOps 进行持续授权操作（C-ATO）的能力。\n本章大纲   4.1 代码类型和参考平台组件的描述\n  4.2 应用程序代码和应用服务代码的 CI/CD 管道\n  4.3 基础设施即代码的 CI/CD 管道\n  4.4 策略即代码的 CI/CD 管道\n  4.5 可观测性即代码的 CI/CD 管道\n  4.6 确保 CI/CD 管道的安全\n  4.7 CI/CD 管道中的工作流模型\n  4.8 安全测试——所有代码类型的 CI/CD 管道的共同要求\n  4.9 DevSecOps 原语对服务网格中应用安全的好处\n  4.10 利用 DevSecOps 进行持续授权操作（C-ATO）\n   开始阅读   ","date":1652803200,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1657070661,"objectID":"1de389d749b41ee3de9ef7ab6b1e6dc3","permalink":"https://lib.jimmysong.io/service-mesh-devsecops/implement/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/service-mesh-devsecops/implement/","section":"service-mesh-devsecops","summary":"各种 CI/CD 管道都涉及到参考平台（即基于微服务的应用，有提供基础设施服务的服务网格）。虽然参考应用是基于微服务的应用，但 DevSecOps 的原语可以应用于单体应用以及既在企业内部又基于云的应用（如混合云、单一公有云和多云）","tags":["Service Mesh","DevSecOps"],"title":"第四章：为参考平台实施 DevSecOps 原语","type":"book"},{"authors":null,"categories":["Envoy"],"content":"在日志章节中，我们将学习 Envoy 中不同类型的日志和方法。\n无论是作为流量网关还是作为服务网格中的 Sidecar，Envoy 都处于独特的地位，可以揭示你的网络中正在发生的事情。日志记录是了解系统状态的重要方式，无论是分析、审计还是故障排除。日志记录也有一个数量问题，有可能会泄露秘密。\n在本章结束时，你将了解 Envoy 中存在哪些日志选项，包括如何对日志进行结构化和过滤，以及它们可以被写到哪里。\n本章大纲   访问日志\n  配置访问记录器\n  访问日志过滤\n  Envoy 组件日志\n  实验 12：使用日志过滤器\n  实验 13：使用 gRPC 访问日志服务（ALS）记录日志\n  实验 14：将 Envoy 的日志发送到 Google Cloud Logging\n   阅读本章   ","date":1652803200,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1657070661,"objectID":"103c5755101a3deaa4f99e94bfc899e4","permalink":"https://lib.jimmysong.io/envoy-handbook/logging/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/envoy-handbook/logging/","section":"envoy-handbook","summary":"在日志章节中，我们将学习 Envoy 中不同类型的日志和方法。 无论是作为流量网关还是作为服务网格中的 Sidecar，Envoy 都处于独特的地位，可以揭示你的网络中正在发生的事情。日志记录是了解系统状态的重要方式，","tags":["Envoy"],"title":"日志","type":"book"},{"authors":null,"categories":["安全"],"content":"本文为托管云原生应用的参考平台实施 DevSecOps 原语提供全面指导。它包括对参考平台的概述，并描述了基本的 DevSecOps 原语（即 CI/CD 管道）、其构建模块、管道的设计和执行，以及自动化在 CI/CD 管道中有效执行工作流程的作用。\n参考平台的架构除了应用代码和提供应用服务的代码外还包括用于基础设施、运行时策略和持续监测应用健康状况的功能元素，可以通过具有独立 CI/CD 管道类型的声明性代码来部署。还介绍了这些代码的运行时行为、实现高安全性的好处，以及使用风险管理工具和仪表盘指标的管道内的工件来提供持续授权操作（C-ATO）。\n","date":1652803200,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1657070661,"objectID":"38cbeac134a4717a85b75678b0896988","permalink":"https://lib.jimmysong.io/service-mesh-devsecops/summary-and-conclusion/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/service-mesh-devsecops/summary-and-conclusion/","section":"service-mesh-devsecops","summary":"本文为托管云原生应用的参考平台实施 DevSecOps 原语提供全面指导。它包括对参考平台的概述，并描述了基本的 DevSecOps 原语（即 CI/CD 管道）、其构建模块、管道的设计和执行，以及自动化在 CI/CD 管道中有效执行工作流程的作用。 参考平台的架构","tags":["Service Mesh","DevSecOps"],"title":"第五章：摘要和结论","type":"book"},{"authors":null,"categories":["Envoy"],"content":"在管理接口章节中，我们将学习 Envoy 所暴露的管理接口，以及我们可以用来检索配置和统计的不同端点，以及执行其他管理任务。\n在本章结束时，你将了解如何启用管理接口以及我们可以通过它执行的不同任务。\n本章大纲   启用管理接口\n  配置转储\n  统计\n  日志\n  集群\n  监听器和监听器的排空\n  分接式过滤器\n  健康检查\n  实验15：使用 HTTP 分接式过滤器\n   阅读本章   ","date":1652803200,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1657070661,"objectID":"3b682f5af5943ff43a4c68712eed2aad","permalink":"https://lib.jimmysong.io/envoy-handbook/admin-interface/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/envoy-handbook/admin-interface/","section":"envoy-handbook","summary":"在管理接口章节中，我们将学习 Envoy 所暴露的管理接口，以及我们可以用来检索配置和统计的不同端点，以及执行其他管理任务。 在本章结束时，你将了解如何启用管理接口以及我们可以通过它执行的不同任务。 本章大纲 启用管理","tags":["Envoy"],"title":"管理接口","type":"book"},{"authors":null,"categories":["Envoy"],"content":"在本章节中，我们将了解扩展 Envoy 的不同方法。\n我们将更详细地介绍使用 Lua 和 Wasm 过滤器来扩展 Envoy 的功能。\n在本章结束时，你将了解扩展 Envoy 的不同方法以及如何使用 Lua 和 Wasm 过滤器。\n本章大纲   可扩展性概述\n  Lua 过滤器\n  WebAssembly（Wasm）\n  实验16：使用 Lua 脚本扩展 Envoy\n  实验17：使用 Wasm 和 Go 扩展 Envoy\n   阅读本章   ","date":1652803200,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1657070661,"objectID":"a9793b4ee69cc483677360c87c7294f5","permalink":"https://lib.jimmysong.io/envoy-handbook/extending-envoy/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/envoy-handbook/extending-envoy/","section":"envoy-handbook","summary":"在本章节中，我们将了解扩展 Envoy 的不同方法。 我们将更详细地介绍使用 Lua 和 Wasm 过滤器来扩展 Envoy 的功能。 在本章结束时，你将了解扩展 Envoy 的不同方法以及如何使用 Lua 和 Wasm 过滤器。 本章大纲 可扩展性概述 Lua 过滤器 WebAssemb","tags":["Envoy"],"title":"扩展 Envoy","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"以下列举的内容都是 Kubernetes 中的对象（Object），这些对象都可以在 YAML 文件中作为一种 API 类型来配置。\n Pod Node Namespace Service Volume PersistentVolume Deployment Secret StatefulSet DaemonSet ServiceAccount ReplicationController ReplicaSet Job CronJob SecurityContext ResourceQuota LimitRange HorizontalPodAutoscaling Ingress ConfigMap Label CustomResourceDefinition Role ClusterRole  我将它们简单的分类为以下几种资源对象：\n   类别 名称     资源对象 Pod、ReplicaSet、ReplicationController、Deployment、StatefulSet、DaemonSet、Job、CronJob、HorizontalPodAutoscaling、Node、Namespace、Service、Ingress、Label、CustomResourceDefinition   存储对象 Volume、PersistentVolume、Secret、ConfigMap   策略对象 SecurityContext、ResourceQuota、LimitRange   身份对象 ServiceAccount、Role、ClusterRole    理解 Kubernetes 中的对象 在 Kubernetes 系统中，Kubernetes 对象 是持久化的条目。Kubernetes 使用这些条目去表示整个集群的状态。特别地，它们描述了如下信息：\n 什么容器化应用在运行（以及在哪个 Node 上） 可以被应用使用的资源 关于应用如何表现的策略，比如重启策略、升级策略，以及容错策略  Kubernetes 对象是 “目标性记录” —— 一旦创建对象，Kubernetes 系统将持续工作以确保对象存在。通过创建对象，可以有效地告知 Kubernetes 系统，所需要的集群工作负载看起来是什么样子的，这就是 Kubernetes 集群的 期望状态。\n与 Kubernetes 对象工作 —— 是否创建、修改，或者删除 —— 需要使用 Kubernetes API。当使用 kubectl 命令行接口时，比如，CLI 会使用必要的 Kubernetes API 调用，也可以在程序中直接使用 Kubernetes API。为了实现该目标，Kubernetes 当前提供了一个 golang 客户端库 ，其它语言库（例如Python）也正在开发中。\n对象 Spec 与状态 每个 Kubernetes 对象包含两个嵌套的对象字段，它们负责管理对象的配置：对象 spec 和 对象 status。spec 必须提供，它描述了对象的 期望状态—— 希望对象所具有的特征。status 描述了对象的 实际状态，它是由 Kubernetes 系统提供和更新。在任何时刻，Kubernetes 控制平面一直处于活跃状态，管理着对象的实际状态以与我们所期望的状态相匹配。\n例如，Kubernetes Deployment 对象能够表示运行在集群中的应用。当创建 Deployment 时，可能需要设置 Deployment 的 spec，以指定该应用需要有 3 个副本在运行。Kubernetes 系统读取 Deployment spec，启动我们所期望的该应用的 3 个实例 —— 更新状态以与 spec 相匹配。如果那些实例中有失败的（一种状态变更），Kubernetes 系统通过修正来响应 spec 和状态之间的不一致 —— 这种情况，启动一个新的实例来替换。\n关于对象 spec、status 和 metadata 更多信息，查看 Kubernetes API Conventions。\n描述 Kubernetes 对象 当创建 Kubernetes 对象时，必须提供对象的 spec，用来描述该对象的期望状态，以及关于对象的一些基本信息（例如，名称）。当使用 Kubernetes API 创建对象时（或者直接创建，或者基于kubectl），API 请求必须在请求体中包含 JSON 格式的信息。更常用的是，需要在 .yaml 文件中为 kubectl 提供这些信息。 kubectl 在执行 API 请求时，将这些信息转换成 JSON 格式。\n这里有一个 .yaml 示例文件，展示了 Kubernetes Deployment 的必需字段和对象 spec：\napiVersion:apps/v1beta1kind:Deploymentmetadata:name:nginx-deploymentspec:replicas:3template:metadata:labels:app:nginxspec:containers:- name:nginximage:nginx:1.7.9ports:- containerPort:80一种创建 Deployment 的方式，类似上面使用 .yaml 文件，是使用 kubectl 命令行接口（CLI）中的 kubectl create 命令，传递 .yaml 作为参数。下面是一个示例：\n$ kubectl create -f docs/user-guide/nginx-deployment.yaml --record 输出类似如下这样：\ndeployment \u0026#34;nginx-deployment\u0026#34; created 必需字段 在想要创建的 Kubernetes 对象对应的 .yaml 文件中，需要配置如下的字段：\n apiVersion - 创建该对象所使用的 Kubernetes API 的版本 kind - 想要创建的对象的类型 metadata - 帮助识别对象唯一性的数据，包括一个 name 字符串、UID 和可选的 namespace  也需要提供对象的 spec 字段。对象 spec 的精确格式对每个 Kubernetes 对象来说是不同的，包含了特定于该对象的嵌套字段。Kubernetes API 参考能够帮助我们找到任何我们想创建的对象的 spec 格式。\n本节大纲   Pod 概览\n  Pod 解析\n  Init 容器\n  Pause 容器\n  Pod 安全策略\n  Pod 的生命周期\n  Pod Hook\n  Pod Preset\n  Pod 中断与 PDB（Pod 中断预算）\n  ","date":1653062400,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1657070661,"objectID":"95aa41669419b1382632e29343a55b5a","permalink":"https://lib.jimmysong.io/kubernetes-handbook/objects/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/objects/","section":"kubernetes-handbook","summary":"以下列举的内容都是 Kubernetes 中的对象（Object），这些对象都可以在 YAML 文件中作为一种 API 类型来配置。 Pod Node Namespace Service Volume PersistentVolume Deployment Secret StatefulSet DaemonSet ServiceAccount ReplicationController ReplicaSet Job CronJob SecurityContext ResourceQuota LimitRange HorizontalPodAutoscaling Ingress ConfigMap Label CustomResourceDefinition Role ClusterRole 我将它们简单的分类为以下几种资源对象： 类别 名称 资源对象 Po","tags":["Kubernetes"],"title":"Kubernetes 中的资源对象","type":"book"},{"authors":null,"categories":["Istio"],"content":"本章将为你介绍什么是服务网格，为什么服务网格对于云原生应用十分重要。\n本章大纲   什么是服务网格？\n  为什么要使用服务网格？\n  云原生应用网络\n   阅读本章   ","date":1652803200,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1657070661,"objectID":"79a81b97679d5df5cb1a7d6695364b15","permalink":"https://lib.jimmysong.io/istio-handbook/intro/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/istio-handbook/intro/","section":"istio-handbook","summary":"本章将为你介绍什么是服务网格，为什么服务网格对于云原生应用十分重要。 本章大纲 什么是服务网格？ 为什么要使用服务网格？ 云原生应用网络 阅读本章","tags":["Istio","Service Mesh"],"title":"服务网格概述","type":"book"},{"authors":null,"categories":["网络"],"content":"本章记录了用于在 Cilium 中配置网络策略的策略语言。安全策略可以通过以下机制指定和导入：\n 使用 Kubernetes NetworkPolicy、CiliumNetworkPolicy 和 CiliumClusterwideNetworkPolicy 资源。更多细节请参见网络策略一节。在这种模式下，Kubernetes 将自动向所有代理分发策略。 通过代理的 CLI 或 API 参考直接导入到代理中。这种方法不会自动向所有代理分发策略。用户有责任在所有需要的代理中导入策略。  本章内容包括：\n 策略执行模式 规则基础 三层示例 四层示例 七层示例 拒绝政策 主机策略 七层协议可视性 在策略中使用 Kubernetes 构造 端点生命周期 故障排除   阅读本章   ","date":1654574400,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1657070661,"objectID":"4c015e1d597404f4667d4c7fab64bc0e","permalink":"https://lib.jimmysong.io/cilium-handbook/policy/","publishdate":"2022-06-07T12:00:00+08:00","relpermalink":"/cilium-handbook/policy/","section":"cilium-handbook","summary":"本章记录了用于在 Cilium 中配置网络策略的策略语言。安全策略可以通过以下机制指定和导入： 使用 Kubernetes NetworkPolicy、CiliumNetworkPolicy 和 CiliumClusterwideNetworkPolicy 资源。更多细节请参见网络策略一节。在这种模式","tags":["Cilium"],"title":"网络策略","type":"book"},{"authors":null,"categories":["Istio"],"content":"在这一章中，你将了解：\n 服务网格的基本概念 Istio 的架构 stio 中的主要部署模式——Sidecar 模式 透明流量拦截——Istio 的核心设计思想  本章大纲   服务网格架构\n  服务网格的实现模式\n  Istio 简介\n  Istio 架构解析\n  Sidecar 模式\n  Sidecar 自动注入过程详解\n  Istio 中的透明流量劫持过程详解\n  Istio 中的 Sidecar 的流量路由详解\n   阅读本章   ","date":1652803200,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1657070661,"objectID":"fc8f2b0ef3525f2a6f9b15ae969b6814","permalink":"https://lib.jimmysong.io/istio-handbook/concepts/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/istio-handbook/concepts/","section":"istio-handbook","summary":"在这一章中，你将了解： 服务网格的基本概念 Istio 的架构 stio 中的主要部署模式——Sidecar 模式 透明流量拦截——Istio 的核心设计思想 本章大纲 服务网格架构 服务网格的实现模式 Istio 简介 Istio 架构解析 Sidecar 模式 Sidecar 自动注入过","tags":["Istio","Service Mesh"],"title":"概念原理","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"为了管理异构和不同配置的主机，为了便于 Pod 的运维管理，Kubernetes 中提供了很多集群管理的配置和管理功能，通过 namespace 划分的空间，通过为 node 节点创建 label 和 taint 用于 pod 的调度等。\n本节大纲   Node\n  Namespace\n  Label\n  Annotation\n  Taint 和 Toleration（污点和容忍）\n  垃圾收集\n  资源调度\n  服务质量等级（QoS）\n  ","date":1653062400,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1657070661,"objectID":"b6f6e6de18eaa1947184e20274bfb907","permalink":"https://lib.jimmysong.io/kubernetes-handbook/cluster/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/cluster/","section":"kubernetes-handbook","summary":"为了管理异构和不同配置的主机，为了便于 Pod 的运维管理，Kubernetes 中提供了很多集群管理的配置和管理功能，通过 namespace 划分的空间，通过为 node 节点创建 label 和 taint 用于 pod 的调度等。 本节大纲 Node Namespace Label Annotation Taint 和 Tolerat","tags":["Kubernetes"],"title":"集群资源管理","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"Kubernetes 中内建了很多 controller（控制器），这些相当于一个状态机，用来控制 Pod 的具体状态和行为。\n本节大纲   Deployment\n  StatefulSet\n  DaemonSet\n  ReplicationController 和 ReplicaSet\n  Job\n  CronJob\n  Ingress 控制器\n  Horizontal Pod Autoscaling\n  准入控制器（Admission Controller）\n  ","date":1653062400,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1657070661,"objectID":"4e1bab642923dabf1fff793fb84d1e18","permalink":"https://lib.jimmysong.io/kubernetes-handbook/controllers/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/controllers/","section":"kubernetes-handbook","summary":"Kubernetes 中内建了很多 controller（控制器），这些相当于一个状态机，用来控制 Pod 的具体状态和行为。 本节大纲 Deployment StatefulSet DaemonSet ReplicationController 和 ReplicaSet Job CronJob Ingress 控制器 Horizontal Pod Autoscaling 准入控制器（Admission Controller）","tags":["Kubernetes"],"title":"控制器","type":"book"},{"authors":null,"categories":["网络"],"content":"本章大纲   介绍\n  概念\n  要求\n  网络策略\n  端点 CRD\n  端点切片 CRD\n  Kubernetes 兼容性\n  故障排除\n   阅读本章   ","date":1655438400,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1657070661,"objectID":"865fc329655b975a5155046e35d09ace","permalink":"https://lib.jimmysong.io/cilium-handbook/kubernetes/","publishdate":"2022-06-17T12:00:00+08:00","relpermalink":"/cilium-handbook/kubernetes/","section":"cilium-handbook","summary":"本章大纲 介绍 概念 要求 网络策略 端点 CRD 端点切片 CRD Kubernetes 兼容性 故障排除 阅读本章","tags":["Cilium"],"title":"Kubernetes 集成","type":"book"},{"authors":null,"categories":["Istio"],"content":"Istio 中的数据平面默认使用的是 Envoy 代理，你也可以根据 xDS 构建其他数据平面，例如蚂蚁开源的 MOSN。\n本章大纲   Envoy 中的基本术语\n  Envoy 代理配置详解\n  xDS 协议解析\n  LDS（监听器发现服务）\n  RDS（路由发现服务）\n  CDS（集群发现服务）\n  EDS（端点发现服务）\n  SDS（秘钥发现服务）\n  ADS（聚合发现服务）\n   阅读本章   ","date":1652803200,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1657070661,"objectID":"5eb5db074391b0ac2bdccf2d024c3938","permalink":"https://lib.jimmysong.io/istio-handbook/data-plane/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/istio-handbook/data-plane/","section":"istio-handbook","summary":"Istio 中的数据平面默认使用的是 Envoy 代理，你也可以根据 xDS 构建其他数据平面，例如蚂蚁开源的 MOSN。 本章大纲 Envoy 中的基本术语 Envoy 代理配置详解 xDS 协议解析 LDS（监听器发现服务） RDS（路由发现服务） CDS（集群发现服","tags":["Istio","Service Mesh"],"title":"数据平面","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"应用的资源使用率通常都有高峰和低谷的时候，如何削峰填谷，提高集群的整体资源利用率，让 service 中的 Pod 个数自动调整呢？这就有赖于 Horizontal Pod Autoscaling 了，顾名思义，使 Pod 水平自动缩放。这个 Object（跟 Pod、Deployment 一样都是 API resource）也是最能体现 kubernetes 之于传统运维价值的地方，不再需要手动扩容了，终于实现自动化了，还可以自定义指标，没准未来还可以通过人工智能自动进化呢！\nHPA 属于 Kubernetes 中的 autoscaling SIG（Special Interest Group），其下有两个 feature：\n Arbitrary/Custom Metrics in the Horizontal Pod Autoscaler#117 Monitoring Pipeline Metrics HPA API #118  Kubernetes 自 1.2 版本引入 HPA 机制，到 1.6 版本之前一直是通过 kubelet 来获取监控指标来判断是否需要扩缩容，1.6 版本之后必须通过 API server、Heapseter 或者 kube-aggregator 来获取监控指标。\n对于 1.6 以前版本中开启自定义 HPA 请参考 Kubernetes autoscaling based on custom metrics without using a host port。\nHPA 解析 Horizontal Pod Autoscaling 仅适用于 Deployment 和 ReplicaSet，在 v1 版本中仅支持根据 Pod 的 CPU 利用率扩缩容，在 v1alpha 版本中，支持根据内存和用户自定义的 metric 扩缩容。\n如果你不想看下面的文章可以直接看下面的示例图，组件交互、组件的配置、命令示例，都画在图上了。\nHorizontal Pod Autoscaling 由 API server 和 controller 共同实现。\n   HPA 示意图  Metrics 支持 在不同版本的 API 中，HPA autoscale 时可以根据以下指标来判断：\n autoscaling/v1  CPU   autoscaling/v1alpha1  内存 自定义 metrics  kubernetes1.6 起支持自定义 metrics，但是必须在 kube-controller-manager 中配置如下两项：  --horizontal-pod-autoscaler-use-rest-clients=true --api-server 指向 kube-aggregator，也可以使用 heapster 来实现，通过在启动 heapster 的时候指定 --api-server=true。查看 kubernetes metrics     多种 metrics 组合  HPA 会根据每个 metric 的值计算出 scale 的值，并将最大的那个值作为扩容的最终结果。      使用 kubectl 管理 Horizontal Pod Autoscaling 作为 API resource 也可以像 Pod、Deployment 一样使用 kubeclt 命令管理，使用方法跟它们一样，资源名称为 hpa。\nkubectl create hpa kubectl get hpa kubectl describe hpa kubectl delete hpa 有一点不同的是，可以直接使用 kubectl autoscale 直接通过命令行的方式创建 Horizontal Pod Autoscaler。\n用法如下：\nkubectl autoscale (-f FILENAME | TYPE NAME | TYPE/NAME) [--min=MINPODS] --max=MAXPODS [--cpu-percent=CPU] [flags] [options] 举个例子：\nkubectl autoscale deployment foo --min=2 --max=5 --cpu-percent=80 为 Deployment foo 创建 一个 autoscaler，当 Pod 的 CPU 利用率达到 80% 的时候，RC 的 replica 数在 2 到 5 之间。\n注意 ：如果为 ReplicaSet 创建 HPA 的话，无法使用 rolling update，但是对于 Deployment 来说是可以的，因为 Deployment 在执行 rolling update 的时候会自动创建新的 ReplicationController。\n什么是 Horizontal Pod Autoscaling？ 利用 Horizontal Pod Autoscaling，kubernetes 能够根据监测到的 CPU 利用率（或者在 alpha 版本中支持的应用提供的 metric）自动的扩容 replication controller，deployment 和 replica set。\nHorizontal Pod Autoscaler 作为 kubernetes API resource 和 controller 的实现。Resource 确定 controller 的行为。Controller 会根据监测到用户指定的目标的 CPU 利用率周期性得调整 replication controller 或 deployment 的 replica 数量。\nHorizontal Pod Autoscaler 如何工作？ Horizontal Pod Autoscaler 由一个控制循环实现，循环周期由 controller manager 中的 --horizontal-pod-autoscaler-sync-period 标志指定（默认是 30 秒）。\n在每个周期内，controller manager 会查询 HorizontalPodAutoscaler 中定义的 metric 的资源利用率。Controller manager 从 resource metric API（每个 pod 的 resource metric）或者自定义 metric API（所有的 metric）中获取 metric。\n 每个 Pod 的 resource metric（例如 CPU），controller 通过 resource metric API 获取 HorizontalPodAutoscaler 中定义的每个 Pod 中的 metric。然后，如果设置了目标利用率，controller 计算利用的值与每个 Pod 的容器里的 resource request 值的百分比。如果设置了目标原始值，将直接使用该原始 metric 值。然后 controller 计算所有目标 Pod 的利用率或原始值（取决于所指定的目标类型）的平均值，产生一个用于缩放所需 replica 数量的比率。 请注意，如果某些 Pod 的容器没有设置相关的 resource request ，则不会定义 Pod 的 CPU 利用率，并且 Aucoscaler 也不会对该 metric 采取任何操作。 对于每个 Pod 自定义的 metric，controller 功能类似于每个 Pod 的 resource metric，只是它使用原始值而不是利用率值。 对于 object metric，获取单个度量（描述有问题的对象），并与目标值进行比较，以产生如上所述的比率。  HorizontalPodAutoscaler 控制器可以以两种不同的方式获取 metric ：直接的 Heapster 访问和 REST 客户端访问。\n当使用直接的 Heapster 访问时，HorizontalPodAutoscaler 直接通过 API 服务器的服务代理子资源查询 Heapster。需要在集群上部署 Heapster 并在 kube-system namespace 中运行。\nAutoscaler 访问相应的 replication controller，deployment 或 replica set 来缩放子资源。\nScale 是一个允许您动态设置副本数并检查其当前状态的接口。\nAPI Object Horizontal Pod Autoscaler 是 kubernetes 的 autoscaling API 组中的 API 资源。当前的稳定版本中，只支持 CPU 自动扩缩容，可以在 autoscaling/v1 API 版本中找到。\n在 alpha 版本中支持根据内存和自定义 metric 扩缩容，可以在 autoscaling/v2alpha1 中找到。autoscaling/v2alpha1 中引入的新字段在 autoscaling/v1 中是做为 annotation 而保存的。\n在 kubectl 中支持 Horizontal Pod Autoscaling Horizontal Pod Autoscaler 和其他的所有 API 资源一样，通过 kubectl 以标准的方式支持。\n我们可以使用 kubectl create 命令创建一个新的 autoscaler。\n我们可以使用 kubectl get hpa 列出所有的 autoscaler，使用 kubectl describe hpa 获取其详细信息。\n最后我们可以使用 kubectl delete hpa 删除 autoscaler。\n另外，可以使用 kubectl autoscale 命令，很轻易的就可以创建一个 Horizontal Pod Autoscaler。\n例如，执行 kubectl autoscale rc foo —min=2 —max=5 —cpu-percent=80 命令将为 replication controller foo 创建一个 autoscaler，目标的 CPU 利用率是 80%，replica 的数量介于 2 和 5 之间。\n滚动更新期间的自动扩缩容 目前在 Kubernetes 中，可以通过直接管理 replication controller 或使用 deployment 对象来执行 滚动更新，该 deployment 对象为您管理基础 replication controller。\nHorizontal Pod Autoscaler 仅支持后一种方法：Horizontal Pod Autoscaler 被绑定到 deployment 对象，它设置 deployment 对象的大小，deployment 负责设置底层 replication controller 的大小。\nHorizontal Pod Autoscaler 不能使用直接操作 replication controller 进行滚动更新，即不能将 Horizontal Pod Autoscaler 绑定到 replication controller，并进行滚动更新（例如使用 kubectl rolling-update）。\n这不行的原因是，当滚动更新创建一个新的 replication controller 时，Horizontal Pod Autoscaler 将不会绑定到新的 replication controller 上。\n支持多个 metric Kubernetes 1.6 中增加了支持基于多个 metric 的扩缩容。您可以使用 autoscaling/v2alpha1 API 版本来为 Horizontal Pod Autoscaler 指定多个 metric。然后 Horizontal Pod Autoscaler controller …","date":1653062400,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1657070661,"objectID":"9fc9399613543058461a024ad8654d29","permalink":"https://lib.jimmysong.io/kubernetes-handbook/controllers/hpa/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/controllers/hpa/","section":"kubernetes-handbook","summary":"应用的资源使用率通常都有高峰和低谷的时候，如何削峰填谷，提高集群的整体资源利用率，让 service 中的 Pod 个数自动调整呢？这就有赖于 Horizontal Pod Autoscaling 了，顾名思义，使 Pod 水平自动缩放。这个 Object（跟 Pod、Deployme","tags":["Kubernetes"],"title":"Horizontal Pod Autoscaling","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"Kubernetes 中为了实现服务实例间的负载均衡和不同服务间的服务发现，创造了 Serivce 对象，同时又为从集群外部访问集群创建了 Ingress 对象。\n本节大纲   Service\n  拓扑感知路由\n  Ingress\n  Gateway\n  ","date":1653062400,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1657070661,"objectID":"d7c054d136e4551c0111bd246cad876c","permalink":"https://lib.jimmysong.io/kubernetes-handbook/service-discovery/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/service-discovery/","section":"kubernetes-handbook","summary":"Kubernetes 中为了实现服务实例间的负载均衡和不同服务间的服务发现，创造了 Serivce 对象，同时又为从集群外部访问集群创建了 Ingress 对象。 本节大纲 Service 拓扑感知路由 Ingress Gateway","tags":["Kubernetes"],"title":"服务发现与路由","type":"book"},{"authors":null,"categories":["Istio"],"content":"有两种方法可以在单个 Kubernetes 集群上安装 Istio：\n 使用 Istioctl（istioctl） 使用 Istio Operator  在本章中，我们将使用 Istio Operator 在一个 Kubernetes 集群上安装 Istio。\n本章大纲   安装 Istio\n  GetMesh\n  Kiali\n  Bookinfo 示例\n   阅读本章   ","date":1652803200,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1657070661,"objectID":"99189028192986ccca087b8bc27d78cc","permalink":"https://lib.jimmysong.io/istio-handbook/setup/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/istio-handbook/setup/","section":"istio-handbook","summary":"有两种方法可以在单个 Kubernetes 集群上安装 Istio： 使用 Istioctl（istioctl） 使用 Istio Operator 在本章中，我们将使用 Istio Operator 在一个 Kubernetes 集群上安装 Istio。 本章大纲 安装 Istio GetMesh Kiali Bookinfo 示例 阅读本章","tags":["Istio","Service Mesh"],"title":"安装","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"Kubernetes 中提供了良好的多租户认证管理机制，如 RBAC、ServiceAccount 还有各种策略等。\n另外还有一个 CNCF 孵化项目 SPIFFE 旨在为所有的分布式应用提供身份支持，目前已应用在了 Envoy、Istio 等应用中。\n本节大纲   ServiceAccount\n  基于角色的访问控制（RBAC）\n  NetworkPolicy\n  SPIFFE\n  SPIRE\n  SPIRE Kubernetes 工作负载注册器\n  SVID 身份颁发过程\n  ","date":1653062400,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1657070661,"objectID":"ff3a41475bfc2d732d3bb4cbe453f23f","permalink":"https://lib.jimmysong.io/kubernetes-handbook/auth/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/auth/","section":"kubernetes-handbook","summary":"Kubernetes 中提供了良好的多租户认证管理机制，如 RBAC、ServiceAccount 还有各种策略等。 另外还有一个 CNCF 孵化项目 SPIFFE 旨在为所有的分布式应用提供身份支持，目前已应用在了 Envoy、Istio 等应用中。 本","tags":["Kubernetes"],"title":"身份与权限认证","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"Kubernetes 中的网络可以说对初次接触 Kubernetes 或者没有网络方面经验的人来说可能是其中最难的部分。Kubernetes 本身并不提供网络功能，只是把网络接口开放出来，通过插件的形式实现。\n网络要解决的问题 既然 Kubernetes 中将容器的联网通过插件的方式来实现，那么该如何解决容器的联网问题呢？\n如果您在本地单台机器上运行 docker 容器的话会注意到所有容器都会处在 docker0 网桥自动分配的一个网络 IP 段内（172.17.0.1/16）。该值可以通过 docker 启动参数 --bip 来设置。这样所有本地的所有的容器都拥有了一个 IP 地址，而且还是在一个网段内彼此就可以互相通信了。\n但是 Kubernetes 管理的是集群，Kubernetes 中的网络要解决的核心问题就是每台主机的 IP 地址网段划分，以及单个容器的 IP 地址分配。概括为：\n 保证每个 Pod 拥有一个集群内唯一的 IP 地址 保证不同节点的 IP 地址划分不会重复 保证跨节点的 Pod 可以互相通信 保证不同节点的 Pod 可以与跨节点的主机互相通信  为了解决该问题，出现了一系列开源的 Kubernetes 中的网络插件与方案，如：\n flannel calico contiv weave kube-router cilium canal 等等  本章将以当前最常用的 flannel、calico 和 cilium 等插件为例解析。\n本节大纲   扁平网络 Flannel\n  非 Overlay 扁平网络 Calico\n  基于 eBPF 的网络 Cilium\n  ","date":1653062400,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1657070661,"objectID":"64e53457fbb5ff70b9542fb7ef1f4462","permalink":"https://lib.jimmysong.io/kubernetes-handbook/networking/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/networking/","section":"kubernetes-handbook","summary":"Kubernetes 中的网络可以说对初次接触 Kubernetes 或者没有网络方面经验的人来说可能是其中最难的部分。Kubernetes 本身并不提供网络功能，只是把网络接口开放出来，通过插件的形式实现。 网络要解决的问题 既然 Kubernetes 中将容器的联网","tags":["Kubernetes"],"title":"网络","type":"book"},{"authors":null,"categories":["Istio"],"content":"在本章中，我们将开始使用 Istio 服务网格在服务之间进行流量路由。我们将学习如何设置一个 Ingress 资源以允许流量进入我们的集群，以及一个 Egress 资源以使流量流出集群。\n使用流量路由，我们将学习如何部署新版本的服务，并在已发布的生产版本的服务旁边运行，而不干扰生产流量。随着两个服务版本的部署，我们将逐步发布（金丝雀发布）新版本，并开始将一定比例的传入流量路由到最新版本。\n流量管理是 Isito 中的最基础功能，使用 Istio 的流量管理模型，本质上是将流量与基础设施扩容解耦，让运维人员可以通过 Pilot 指定流量遵循什么规则，而不是指定哪些 pod/VM 应该接收流量——Pilot 和智能 Envoy 代理会帮我们搞定。\n所谓流量管理是指：\n 控制服务之间的路由：通过在 VirtualService 中的规则条件匹配来设置路由，可以在服务间拆分流量。 控制路由上流量的行为：设定好路由之后，就可以在路由上指定超时和重试机制，例如超时时间、重试次数等；做错误注入、设置断路器等。可以由 VirtualService 和 DestinationRule 共同完成。 显式地向网格中注册服务：显示地引入 Service Mesh 内部或外部的服务，纳入服务网格管理。由 ServiceEntry 实现。 控制网格边缘的南北向流量：为了管理进入 Istio service mesh 的南北向入口流量，需要创建 Gateway 对象并与 VirtualService 绑定。  本章大纲   流量管理基础概念\n  Gateway\n  基本路由\n  Subset 和 DestinationRule\n  弹性\n  错误注入\n  高级路由\n  ServiceEntry\n  Sidecar\n  EnvoyFilter\n   阅读本章   ","date":1652803200,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1657070661,"objectID":"99957e88a4373afd6d712d117108ae0b","permalink":"https://lib.jimmysong.io/istio-handbook/traffic-management/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/istio-handbook/traffic-management/","section":"istio-handbook","summary":"在本章中，我们将开始使用 Istio 服务网格在服务之间进行流量路由。我们将学习如何设置一个 Ingress 资源以允许流量进入我们的集群，以及一个 Egress 资源以使流量流出集群。 使用流量路由，我们将学习如何部署新版本的服务，并在已发布","tags":["Istio","Service Mesh"],"title":"流量管理","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"为了管理存储，Kubernetes 提供了以下资源对象：\n Secret：用于管理敏感信息 ConfigMap：存储配置 Volume、PV、PVC、StorageClass 等：用来管理存储卷  本节将为你讲解 Kubernetes 中的存储对象。\n本节大纲   Secret\n  ConfigMap\n  ConfigMap 的热更新\n  Volume\n  持久化卷（Persistent Volume）\n  Storage Class\n  本地持久化存储\n  ","date":1653062400,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1657070661,"objectID":"dd60646c4133a46ddb8fb096c9bb6693","permalink":"https://lib.jimmysong.io/kubernetes-handbook/storage/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/storage/","section":"kubernetes-handbook","summary":"为了管理存储，Kubernetes 提供了以下资源对象： Secret：用于管理敏感信息 ConfigMap：存储配置 Volume、PV、PVC、StorageClass 等：用来管理存储卷 本节将为你讲解 Kubernetes 中","tags":["Kubernetes"],"title":"存储","type":"book"},{"authors":null,"categories":["Istio"],"content":"流量管理配置章节描述如何配置 Istio 服务网格中与流量管理相关的一些资源对象。\n本章大纲   Gateway\n  VirtualService\n  DestinationRule\n  WorkloadEntry\n  WorkloadGroup\n  ServiceEntry\n  Sidecar\n  EnvoyFilter\n  ProxyConfig\n   阅读本章   ","date":1652803200,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1657070661,"objectID":"96af5f978b4c26f2c23bb7d10d37d35b","permalink":"https://lib.jimmysong.io/istio-handbook/config-networking/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/istio-handbook/config-networking/","section":"istio-handbook","summary":"流量管理配置章节描述如何配置 Istio 服务网格中与流量管理相关的一些资源对象。 本章大纲 Gateway VirtualService DestinationRule WorkloadEntry WorkloadGroup ServiceEntry Sidecar EnvoyFilter ProxyConfig 阅读本章","tags":["Istio","Service Mesh"],"title":"流量管理配置","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"Kubernetes 是一个高度开放可扩展的架构，可以通过自定义资源类型（CRD）来定义自己的类型，还可以自己来扩展 API 服务，用户的使用方式跟 Kubernetes 的原生对象无异。\n本节大纲   使用自定义资源扩展 API\n  使用 CRD 扩展 Kubernetes API\n  Aggregated API Server\n  APIService\n  服务目录（Service Catalog）\n  ","date":1653062400,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1657070661,"objectID":"b49dea88098b3238136c17f11ecfdfa2","permalink":"https://lib.jimmysong.io/kubernetes-handbook/extend/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/extend/","section":"kubernetes-handbook","summary":"Kubernetes 是一个高度开放可扩展的架构，可以通过自定义资源类型（CRD）来定义自己的类型，还可以自己来扩展 API 服务，用户的使用方式跟 Kubernetes 的原生对象无异。 本节大纲 使用自定义资源扩展 API 使用 CRD 扩展 Kubernetes API Aggregated API Server APIService 服务目录（S","tags":["Kubernetes"],"title":"扩展集群","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"组织需要部署多个 Kubernetes 集群来为不同的业务提供隔离，增强可用性和可扩展性。\n什么是多集群？ 多集群是一种在多个 Kubernetes 集群上或跨集群部署应用的策略，目的是提高可用性、隔离性和可扩展性。多集群对于确保遵守不同的和相互冲突的法规非常重要，因为单个集群可以进行调整，以遵守特定地域或认证的法规。软件交付的速度和安全性也可以提高，单个开发团队将应用程序部署到隔离的集群中，并有选择地暴露哪些服务可用于测试和发布。\n配置多集群访问 你可以使用 kubectl config 命令配置要访问的集群，详见配置对多集群的访问。\n集群联邦 集群联邦（Federation）是指通过 Federation API 资源来统一管理多个集群的资源，如定义 Deployment 如何部署到不同集群上，及其所需的副本数等。这些集群可能位于不同的可用区、地区或者供应商。实施集群联邦一般是为了达到以下目的：\n 简化管理多个集群的 Kubernetes 组件 (如 Deployment、Service 等）； 在多个集群之间分散工作负载（Pod），以提升应用（服务）的可靠性； 跨集群的资源编排，依据编排策略在多个集群进行应用（服务）部署； 在不同集群中，能更快速更容易地迁移应用（服务）； 跨集群的服务发现，服务可以提供给当地存取，以降低延迟； 实践多云（Multi-cloud）或混合云（Hybird Cloud）的部署；  本节大纲   多集群服务 API（Multi-Cluster Services API）\n  集群联邦（Cluster Federation）\n  参考  Multicluster Special Interest Group - github.com 配置对多集群的访问 - kubernetes.io  ","date":1653062400,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1657070661,"objectID":"f6b9d2ae105a5551b85254d3c995a617","permalink":"https://lib.jimmysong.io/kubernetes-handbook/multi-cluster/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/multi-cluster/","section":"kubernetes-handbook","summary":"组织需要部署多个 Kubernetes 集群来为不同的业务提供隔离，增强可用性和可扩展性。 什么是多集群？ 多集群是一种在多个 Kubernetes 集群上或跨集群部署应用的策略，目的是提高可用性、隔离性和可扩展性。多集群对于确保遵守不同的和相互冲","tags":["Kubernetes"],"title":"多集群管理","type":"book"},{"authors":null,"categories":["Istio"],"content":"在本章中，我们将学习一些监控（Prometheus）、追踪（Zipkin）和数据可视化工具（Grafana）。\n什么是可观测性？ 由于采用了 sidecar 部署模式，即 Envoy 代理运行在应用实例旁边并拦截流量，这些代理也收集指标。\nEnvoy 代理收集的指标可以帮助我们获得系统状态的可视性。获得系统的这种可视性是至关重要的，因为我们需要了解正在发生的事情，并授权运维人员对应用程序进行故障排除、维护和优化。\nIstio 生成三种类型的遥测数据，为网格中的服务提供可观测性：\n 指标度量（Metric） 分布式跟踪 访问日志  指标度量 Istio 基于四个黄金信号生成指标：延迟、流量、错误和饱和度。\n延迟表示服务一个请求所需的时间。这个指标应该分成成功请求（如 HTTP 200）和失败请求（如 HTTP 500）的延迟。\n流量是衡量对系统的需求有多大，它是以系统的具体指标来衡量的。例如，每秒的 HTTP 请求，或并发会话，每秒的检索量，等等。\n错误用来衡量请求失败的比率（例如 HTTP 500）。\n饱和度衡量一个服务中最紧张的资源有多满。例如，线程池的利用率。\n这些指标是在不同的层面上收集的，首先是最细的，即 Envoy 代理层面，然后是服务层面和控制面的指标。\n代理级指标 生成指标的一个关键角色是 Envoy，它生成了一套关于所有通过代理的流量的丰富指标。使用 Envoy 生成的指标，我们可以以最低的粒度来监控服务网格，例如 Envoy 代理中的 inidivdual 监听器和集群的指标。\n作为网格运维人员，我们有能力控制在每个2工作负载实例中生成和收集哪些 Envoy 指标。\n下面是几个代理级指标的例子。\nenvoy_cluster_internal_upstream_rq{response_code_class=\u0026#34;2xx\u0026#34;,cluster_name=\u0026#34;xds-grpc\u0026#34;} 7163 envoy_cluster_upstream_rq_completed{cluster_name=\u0026#34;xds-grpc\u0026#34;} 7164 envoy_cluster_ssl_connection_error{cluster_name=\u0026#34;xds-grpc\u0026#34;} 0 envoy_cluster_lb_subsets_removed{cluster_name=\u0026#34;xds-grpc\u0026#34;} 0 envoy_cluster_internal_upstream_rq{response_code=\u0026#34;503\u0026#34;,cluster_name=\u0026#34;xds-grpc\u0026#34;} 1  注意你可以从每个 Envoy 代理实例的 /stats 端点查看代理级指标。\n 服务级指标 服务级别的指标涵盖了我们前面提到的四个黄金信号。这些指标使我们能够监控服务与服务之间的通信。此外，Istio 还提供了一组仪表盘，我们可以根据这些指标来监控服务行为。\n就像代理级别的指标一样，运营商可以自定义收集哪些服务级别的指标。\n默认情况下，Istio 的标准指标集会被导出到 Prometheus。\n下面是几个服务级指标的例子。\n控制平面度量 Istio 也会发射控制平面指标，可以帮助监控 Istio 的控制平面和行为，而不是用户服务。\n输出的控制平面指标的完整列表可以在这里找到。\n控制平面指标包括冲突的入站/出站监听器的数量、没有实例的集群数量、被拒绝或被忽略的配置等指标。\n本章大纲   Telemetry API\n  Prometheus\n  Grafana\n  Zipkin\n  Kiali\n   阅读本章   ","date":1652803200,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1657070661,"objectID":"d59534ab0ad37757da1a12b6c8d7132e","permalink":"https://lib.jimmysong.io/istio-handbook/observability/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/istio-handbook/observability/","section":"istio-handbook","summary":"在本章中，我们将学习一些监控（Prometheus）、追踪（Zipkin）和数据可视化工具（Grafana）。 什么是可观测性？ 由于采用了 sidecar 部署模式，即 Envoy 代理运行在应用实例旁边并拦截流量，这些代理也收集","tags":["Istio","Service Mesh"],"title":"可观测性","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"Kubernetes 中的各个资源对象的配置指南。\n本节大纲   配置 Pod 的 liveness 和 readiness 探针\n  配置 Pod 的 Service Account\n  Secret 配置\n  管理 namespace 中的资源配额\n  ","date":1653062400,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1657070661,"objectID":"0a16065a0db1c8c5567086f4ea5bce55","permalink":"https://lib.jimmysong.io/kubernetes-handbook/config/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/config/","section":"kubernetes-handbook","summary":"Kubernetes 中的各个资源对象的配置指南。 本节大纲 配置 Pod 的 liveness 和 readiness 探针 配置 Pod 的 Service Account Secret 配置 管理 namespace 中的资源配额","tags":["Kubernetes"],"title":"资源对象配置","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"Kubernetes 中的 kubectl 及其他管理命令使用。\n本节大纲   Docker 用户过渡到 kubectl 命令行指南\n  Kubectl 命令概览\n  Kubectl 命令技巧大全\n  使用 etcdctl 访问 Kubernetes 数据\n  ","date":1653062400,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1657070661,"objectID":"3a9e7c5b4165599e9893d3b32a66c8df","permalink":"https://lib.jimmysong.io/kubernetes-handbook/cli/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/cli/","section":"kubernetes-handbook","summary":"Kubernetes 中的 kubectl 及其他管理命令使用。 本节大纲 Docker 用户过渡到 kubectl 命令行指南 Kubectl 命令概览 Kubectl 命令技巧大全 使用 etcdctl 访问 Kubernetes 数据","tags":["Kubernetes"],"title":"命令使用","type":"book"},{"authors":null,"categories":["Istio"],"content":"在本章中，我们将学习 Istio 的安全功能。具体来说，就是认证和授权策略、安全命名和身份。\n在 Istio 中，有多个组件参与提供安全功能：\n 用于管理钥匙和证书的证书颁发机构（CA）。 Sidecar 和周边代理：实现客户端和服务器之间的安全通信，它们作为政策执行点（Policy Enforcement Point，简称PEP）工作 Envoy 代理扩展：管理遥测和审计 配置 API 服务器：分发认证、授权策略和安全命名信息  本章大纲   认证\n  对等认证和请求认证\n  mTLS\n  授权\n   阅读本章   ","date":1652803200,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1657070661,"objectID":"e3d84b03093329eb571bee3fdce533cd","permalink":"https://lib.jimmysong.io/istio-handbook/security/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/istio-handbook/security/","section":"istio-handbook","summary":"在本章中，我们将学习 Istio 的安全功能。具体来说，就是认证和授权策略、安全命名和身份。 在 Istio 中，有多个组件参与提供安全功能： 用于管理钥匙和证书的证书颁发机构（CA）。 Sidecar 和周边代理：实现客户端和服务器之间的安全","tags":["Istio","Service Mesh"],"title":"安全","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"Kubernetes 支持多租户，这就需要对集群的安全性进行管理。\n","date":1653062400,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1657070661,"objectID":"7c5c953787556ffd27321c0db1a503a4","permalink":"https://lib.jimmysong.io/kubernetes-handbook/security/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/security/","section":"kubernetes-handbook","summary":"Kubernetes 支持多租户，这就需要对集群的安全性进行管理。","tags":["Kubernetes"],"title":"集群安全性管理","type":"book"},{"authors":null,"categories":["Istio"],"content":"安全一节描述如何配置 Istio mesh 中与安全性相关的配置，其中包括：\n AuthorizationPolicy RequestAuthentication PeerAuthentication JWTRule  本章大纲   AuthorizationPolicy\n  RequestAuthentication\n  PeerAuthentication\n  JWTRule\n   阅读本章   ","date":1652803200,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1657070661,"objectID":"e2b3cd936970c5ab86532e950c209d63","permalink":"https://lib.jimmysong.io/istio-handbook/config-security/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/istio-handbook/config-security/","section":"istio-handbook","summary":"安全一节描述如何配置 Istio mesh 中与安全性相关的配置，其中包括： AuthorizationPolicy RequestAuthentication PeerAuthentication JWTRule 本章大纲 AuthorizationPolicy RequestAuthentication PeerAuthentication JWTRule 阅读本章","tags":["Istio","Service Mesh"],"title":"安全配置","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"根据用户部署和暴露服务的方式不同，有很多种方式可以用来访问 Kubernetes 集群。\n 最简单也是最直接的方式是使用 kubectl 命令。 其次可以使用 kubeconfig 文件来认证授权访问 API server。 通过各种 proxy 经过端口转发访问 Kubernetes 集群中的服务 使用 Ingress，在集群外访问 Kubernetes 集群内的 service  本节大纲   访问集群\n  使用 kubeconfig 文件配置跨集群认证\n  通过端口转发访问集群中的应用程序\n  使用 service 访问群集中的应用程序\n  从外部访问 Kubernetes 中的 Pod\n  Lens - Kubernetes IDE\n  Kubernator - 更底层的 Kubernetes UI\n  ","date":1653062400,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1657070661,"objectID":"89aac6f0d87bf9fa00cbf8ea4cad7f18","permalink":"https://lib.jimmysong.io/kubernetes-handbook/access/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/access/","section":"kubernetes-handbook","summary":"根据用户部署和暴露服务的方式不同，有很多种方式可以用来访问 Kubernetes 集群。 最简单也是最直接的方式是使用 kubectl 命令。 其次可以使用 kubeconfig 文件来认证授权访问 API server。 通过各种 proxy 经过端口转发访问 Kubernetes 集群中的服务 使用 Ing","tags":["Kubernetes"],"title":"访问 Kubernetes 集群","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"理论上只要可以使用主机名做服务注册的应用都可以迁移到 Kubernetes 集群上。看到这里你可能不禁要问，为什么使用 IP 地址做服务注册发现的应用不适合迁移到 kubernetes 集群？因为这样的应用不适合自动故障恢复，因为目前 Kubernetes 中不支持固定 Pod 的 IP 地址，当 Pod 故障后自动转移到其他节点的时候该 Pod 的 IP 地址也随之变化。\n将传统应用迁移到 Kubernetes 中可能还有很长的路要走，但是直接开发云原生应用，Kubernetes 就是最佳运行时环境了。\n本节大纲   适用于 Kubernetes 的应用开发部署流程\n  迁移传统应用到 Kubernetes 步骤详解——以 Hadoop YARN 为例\n  使用StatefulSet部署有状态应用\n  ","date":1653062400,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1657070661,"objectID":"d442ec8da38a0e97ac3e30decd93874d","permalink":"https://lib.jimmysong.io/kubernetes-handbook/devops/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/devops/","section":"kubernetes-handbook","summary":"理论上只要可以使用主机名做服务注册的应用都可以迁移到 Kubernetes 集群上。看到这里你可能不禁要问，为什么使用 IP 地址做服务注册发现的应用不适合迁移到 kubernetes 集群？因为这样的应用不适合自动故障恢复，因为目前 Kubernetes 中不支持固定 Pod","tags":["Kubernetes"],"title":"在 Kubernetes 中开发部署应用","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"讲解如何在原生 Kubernetes 的基础上做定制开发。\n本节大纲   SIG 和工作组\n  配置 Kubernetes 开发环境\n  测试 Kubernetes\n  client-go 示例\n  Operator\n  Operator SDK\n  Kubebuilder\n  高级开发指南\n  参与 Kubernetes 社区贡献\n  Minikube\n   阅读本章   ","date":1653062400,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1657070661,"objectID":"fef3110fd0d27aecf54244ec803a73ae","permalink":"https://lib.jimmysong.io/kubernetes-handbook/develop/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/develop/","section":"kubernetes-handbook","summary":"讲解如何在原生 Kubernetes 的基础上做定制开发。 本节大纲 SIG 和工作组 配置 Kubernetes 开发环境 测试 Kubernetes client-go 示例 Operator Operator SDK Kubebuilder 高级开发指南 参与 Kubernetes 社区贡献 Minikube 阅读本章","tags":["Kubernetes"],"title":"开发指南","type":"book"},{"authors":null,"categories":["Istio"],"content":"在本章中，我们将了解在多个集群上安装 Istio 的不同方法，以及如何将运行在虚拟机上的工作负载纳入服务网格。\n当决定在多集群场景下运行 Istio 时，有多种组合需要考虑。在高层次上，我们需要决定以下几点：\n 单个集群或多个集群 单个网络或多个网络 单个控制平面或多个控制平面 单个网格或多个网格  上述模式的任何组合都是可能的，然而，并非所有的模式都有意义。在本章中，我们将重点讨论涉及多个集群的场景。\n本章大纲   多集群部署\n  虚拟机负载\n   阅读本章   ","date":1652803200,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1657070661,"objectID":"9b7257e7af4164679c5001bbea6c6bb9","permalink":"https://lib.jimmysong.io/istio-handbook/advanced/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/istio-handbook/advanced/","section":"istio-handbook","summary":"在本章中，我们将了解在多个集群上安装 Istio 的不同方法，以及如何将运行在虚拟机上的工作负载纳入服务网格。 当决定在多集群场景下运行 Istio 时，有多种组合需要考虑。在高层次上，我们需要决定以下几点： 单个集群或多个集群","tags":["Istio","Service Mesh"],"title":"高级功能","type":"book"},{"authors":null,"categories":["Istio"],"content":"本章介绍了在使用 Istio 时可能遇到的问题的几种排查方法。\n本章大纲   Envoy 基础问题\n  Envoy 示例\n  调试清单\n  Istio 开发环境配置\n   阅读本章   ","date":1652803200,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1657070661,"objectID":"b2a44b9b8831dd6950a702a195be5eff","permalink":"https://lib.jimmysong.io/istio-handbook/troubleshooting/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/istio-handbook/troubleshooting/","section":"istio-handbook","summary":"本章介绍了在使用 Istio 时可能遇到的问题的几种排查方法。 本章大纲 Envoy 基础问题 Envoy 示例 调试清单 Istio 开发环境配置 阅读本章","tags":["Istio","Service Mesh"],"title":"问题排查","type":"book"},{"authors":null,"categories":["Istio"],"content":"Istio 服务网格自 2017 年 5 月开源以来，已围绕其周边诞生了诸多开源项目，这些项目有的是对 Istio 本身的扩展，有的是可以与 Istio 集成，还有的是 Istio 周边的开源工具。\n本章将分门别类为读者介绍 Istio 生态中的开源项目，以帮助读者对 Istio 的开源生态有个更直观的了解，另一方面也可以作为读者个人的选型参考。\nIstio 周边开源项目 下表中列举 Istio 生态中的开源项目，按照开源时间排序。\n   项目名称 开源时间 类别 描述 主导公司 Star 数量 与 Istio 的关系     Envoy 2016年 9 月 网络代理 云原生高性能边缘/中间服务代理 Lyft 18300 默认的数据平面   Istio 2017 年 5 月 服务网格 连接、保护、控制和观察服务。 Google 28400 控制平面   Emissary Gateway 2018 年 2 月 网关 用于微服务的 Kubernetes 原生 API 网关，基于 Envoy 构建 Ambassador 3500 可连接 Istio   APISIX 2019 年 6 月 网关 云原生 API 网关 API7 7400 可作为 Istio 的数据平面运行也可以单独作为网关   MOSN 2019 年 12 月 代理 云原生边缘网关及代理 蚂蚁 3400 可作为 Istio 数据平面   Slime 2021 年 1月 扩展 基于 Istio 的智能服务网格管理器 网易 204 为 Istio 增加一个管理平面   GetMesh 2021 年 2 月 工具 Istio 集成和命令行管理工具 Tetrate 91 实用工具，可用于 Istio 多版本管理   Aeraki 2021 年 3 月 扩展 管理 Istio 的任何七层负载 腾讯 280 扩展多协议支持   Layotto 2021 年 6 月 运行时 云原生应用运行时 蚂蚁 325 可以作为 Istio 的数据平面   Hango Gateway 2021 年 8 月 网关 基于 Envoy 和 Istio 构建的 API 网关 网易 187 可与 Istio 集成    \n   开源时间以 GitHub 仓库创建时间为准 Star 数量统计截止时间为 2021年11月11 日    本章大纲   Aeraki\n  Slime\n  Merbridge\n   阅读本章   ","date":1652803200,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1657070661,"objectID":"77ec59b7b9951623f41ab955b66b150f","permalink":"https://lib.jimmysong.io/istio-handbook/ecosystem/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/istio-handbook/ecosystem/","section":"istio-handbook","summary":"Istio 服务网格自 2017 年 5 月开源以来，已围绕其周边诞生了诸多开源项目，这些项目有的是对 Istio 本身的扩展，有的是可以与 Istio 集成，还有的是 Istio 周边的开源工具。 本章将分门别类为读者介绍 Istio 生态中的开源项目，以帮助读者对 Istio 的开","tags":["Istio","Service Mesh"],"title":"Istio 生态","type":"book"},{"authors":null,"categories":["Istio"],"content":"在本章中，我们将部署名为 Online Boutique 的微服务演示应用程序，并尝试不同的Istio功能。\nOnline Boutique 是一个云原生微服务演示应用程序。Online Boutique 由一个 10 层的微服务应用组成。该应用是一个基于 Web 的电子商务应用，用户可以浏览商品，将其添加到购物车，并购买商品。\n本章大纲   创建集群\n  部署 Online Boutique 应用\n  部署可观测性工具\n  路由流量\n  错误注入\n  弹性\n   阅读本章   ","date":1652803200,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1657070661,"objectID":"17f4b296fe262cfe036e1527bcaf58c3","permalink":"https://lib.jimmysong.io/istio-handbook/practice/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/istio-handbook/practice/","section":"istio-handbook","summary":"在本章中，我们将部署名为 Online Boutique 的微服务演示应用程序，并尝试不同的Istio功能。 Online Boutique 是一个云原生微服务演示应用程序。Online Boutique 由一个 10 层的微服务应用组成。该应用是一个基于 Web 的电子商务应用，用户可以浏","tags":["Istio","Service Mesh"],"title":"实战案例","type":"book"},{"authors":null,"categories":null,"content":"自 2017 年以来，笔者累计翻译和撰写了十几本电子书和实体书，本站中列出了所有书目，其中电子书可以直接通过导航跳转阅读。\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"zh","lastmod":1657070661,"objectID":"edca7cbb245146be752b62ec0ec546fa","permalink":"https://lib.jimmysong.io/book/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/book/","section":"book","summary":"自 2017 年以来，笔者累计翻译和撰写了十几本电子书和实体书，本站中列出了所有书目，其中电子书可以直接通过导航跳转阅读。","tags":null,"title":"书目列表","type":"book"},{"authors":null,"categories":["网络"],"content":"Linux 内核在网络堆栈中支持一组 BPF 钩子（hook），可用于运行 BPF 程序。Cilium 数据路径使用这些钩子来加载 BPF 程序，这些程序一起使用时会创建更高级别的网络结构。\n以下是 Cilium 使用的钩子列表和简要说明。有关每个钩子细节的更详尽的文档，请参阅 BPF 和 XDP 参考指南。\n  XDP：XDP BPF 钩子位于网络驱动程序中的最早可能点，并在数据包接收时触发 BPF 程序的运行。这实现了可能的最佳数据包处理性能，因为程序在任何其他处理发生之前直接在数据包数据上运行。此钩子非常适合运行丢弃恶意或意外流量的过滤程序以及其他常见的 DDOS 保护机制。\n  流量控制入口/出口：附加到流量控制（traffic control，简称 TC）入口钩子的 BPF 程序附加到网络接口，与 XDP 相同，但将在网络堆栈完成数据包的初始处理后运行。该钩子在三层网络之前运行，但可以访问与数据包关联的大部分元数据。这非常适合进行本地节点处理，例如应用三层/四层端点策略并将流量重定向到端点。对于面向网络的设备，TC 入口钩子可以与上面的 XDP 钩子耦合。完成此操作后，可以合理地假设此时的大部分流量是合法的并以主机为目的地。\n容器通常使用称为 veth 对的虚拟设备，它充当将容器连接到主机的虚拟路由。通过附加到这个 veth 对的主机端的 TC 入口钩子，Cilium 可以监控和执行所有离开容器的流量的策略。通过将 BPF 程序附加到与每个容器关联的 veth 对，并将所有网络流量路由到主机端虚拟设备，同时将另一个 BPF 程序附加到 TC 入口钩子，Cilium 可以监控所有进入或离开节点的流量并执行策略.\n  套接字操作：套接字操作钩子附加到特定的 cgroup 并在 TCP 事件上运行。Cilium 将 BPF 套接字操作程序附加到根 cgroup 并使用它来监视 TCP 状态转换，特别是 ESTABLISHED 状态转换。如果 TCP 套接字具有节点本地对等节点（可能是本地代理），则当套接字转换为 ESTABLISHED 状态时，附加套接字发送 / 接收程序。\n  套接字发送/接收：套接字发送/接收钩子（socket send/recv hook）在 TCP 套接字执行的每个发送操作上运行。此时，钩子可以检查消息并丢弃消息、将消息发送到 TCP 层或将消息重定向到另一个套接字。Cilium 使用它来加速数据路径重定向，如下所述。\n  将上述钩子与虚拟接口（cilium_host、cilium_net）、可选的 overlay 接口（cilium_vxlan）、Linux 内核加密支持和用户空间代理（Envoy）相结合，Cilium 创建了以下网络对象。\n  前置过滤器（Prefilter）：前置过滤器对象运行一个 XDP 程序并提供一组前置过滤器规则，用于过滤来自网络的流量以获得最佳性能。具体来说，数据包要么被丢弃，例如当目标不是有效的端点时，要么被堆栈处理。这可以根据需要轻松扩展以构建新的前置过滤器标准/功能。\n  端点策略：端点策略对象实现 Cilium 端点强制。使用映射来查找与身份和策略相关的数据包，该层可以很好地扩展到许多端点。根据策略，该层可能会丢弃数据包、转发到本地端点、转发到服务对象或转发到七层策略对象以获取进一步的七层规则。这是 Cilium 数据路径中的主要对象，负责将数据包映射到身份并执行三层和四层策略。\n  服务：服务对象对对象接收的每个数据包的目标 IP 和可选的目标端口执行映射查找。如果找到匹配条目，数据包将被转发到配置的三层/四层端点之一。Service 块可用于使用 TC 入口钩子在任何接口上实现独立的负载均衡器，或者可以集成到端点策略对象中。\n  三层加密：在入口时，三层加密对象标记要解密的数据包，将数据包传递给 Linux xfrm（转换）层进行解密，数据包解密后对象接收数据包，然后将其向上传递到堆栈以供其他人进一步处理对象。根据模式、直接路由或覆盖，这可能是 BPF 尾调用或将数据包传递给下一个对象的 Linux 路由堆栈。解密所需的密钥在 IPsec 标头中编码，因此在入口处我们不需要进行映射查找来查找解密密钥。\n在出口处，首先使用目标 IP 执行映射查找以确定数据包是否应加密，如果加密，则目标节点上可用的密钥。选择两个节点上可用的最新密钥，并将数据包标记为加密。然后将数据包传递到对其进行加密的 Linux xfrm 层。在接收到现在加密的数据包后，通过将其发送到 Linux 堆栈进行路由或在使用覆盖时进行直接尾调用，将其传递到下一层。\n  套接字层强制：套接字层强制使用两个钩子，套接字操作钩子和套接字发送/接收钩子来监视和附加到与 Cilium 托管端点关联的所有 TCP 套接字，包括任何 七层代理。套接字操作钩子将识别用于加速的候选套接字。这些包括所有本地节点连接（端点到端点）和任何到 Cilium 代理的连接。然后，这些已识别的连接将由套接字发送/接收钩子处理所有消息，并将使用 sockmap 快速重定向加速。快速重定向确保 Cilium 中实现的所有策略对关联的套接字 / 端点映射有效，并假设它们直接将消息发送到对等套接字。这是允许的，因为 sockmap send/recv 钩子确保消息不需要由上述任何对象处理。\n  七层策略：七层策略对象将代理流量重定向到 Cilium 用户空间代理实例。Cilium 使用 Envoy 实例作为其用户空间代理。然后，Envoy 将根据配置的七层策略转发流量或生成适当的拒绝消息。\n  这些组件相互连接，以创建 Cilium 使用的灵活高效的数据路径。下面我们展示了连接单个节点上的端点、入口到端点以及端点到出口网络设备的以下可能流程。在每种情况下，都有一个附加图表显示启用套接字层强制时可用的 TCP 加速路径。\n","date":1655438400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"19ca8e733fcdcca3590daa50a7f1e49c","permalink":"https://lib.jimmysong.io/cilium-handbook/ebpf/intro/","publishdate":"2022-06-17T12:00:00+08:00","relpermalink":"/cilium-handbook/ebpf/intro/","section":"cilium-handbook","summary":"Linux 内核在网络堆栈中支持一组 BPF 钩子（hook），可用于运行 BPF 程序。Cilium 数据路径使用这些钩子来加载 BPF 程序，这些程序一起使用时会创建更高级别的网络结构。 以下是 Cilium 使用的钩子列表和简要说明。有关每个钩子","tags":["Cilium"],"title":"eBPF 数据路径介绍","type":"book"},{"authors":null,"categories":["网络"],"content":"Cilium 能为 Kubernetes 集群提供什么？ 在 Kubernetes 集群中运行 Cilium 时提供以下功能：\n CNI 插件支持，为 pod 连接 提供 联网。 NetworkPolicy 资源的基于身份的实现，用于隔离三层和四层网络 pod 的连接。 以 CustomResourceDefinition 形式对 NetworkPolicy 的扩展，扩展策略控制以添加：  针对以下应用协议的入口和出口执行七层策略：  HTTP Kafka   对 CIDR 的出口支持以保护对外部服务的访问 强制外部无头服务自动限制为服务配置的 Kubernetes 端点集   ClusterIP 实现为 pod 到 pod 的流量提供分布式负载平衡 完全兼容现有的 kube-proxy 模型  Pod 间连接 在 Kubernetes 中，容器部署在称为 pod 的单元中，其中包括一个或多个可通过单个 IP 地址访问的容器。使用 Cilium，每个 pod 从运行 pod 的 Linux 节点的节点前缀中获取一个 IP 地址。有关其他详细信息，请参阅 IP 地址管理（IPAM）。在没有任何网络安全策略的情况下，所有的 pod 都可以互相访问。\nPod IP 地址通常位于 Kubernetes 集群本地。如果 pod 需要作为客户端访问集群外部的服务，则网络流量在离开节点时会自动伪装。\n服务负载均衡 Kubernetes 开发了服务抽象，它为用户提供了将网络流量负载平衡到不同 pod 的能力。这种抽象允许 pod 通过单个 IP 地址（一个虚拟 IP 地址）与其他 pod 联系，而无需知道所有运行该特定服务的 pod。\n如果没有 Cilium，kube-proxy 会安装在每个节点上，监视 kube-master 上的端点和服务的添加和删除，这允许它在 iptables 上应用必要的强制策略执行。因此，从 pod 接收和发送到的流量被正确地路由到为该服务服务的节点和端口。有关更多信息，您可以查看服务的 Kubernetes 用户 指南。\n在实现 ClusterIP 时，Cilium 的行为与 kube-proxy 相同，它监视服务的添加或删除，但不是在 iptables 上执行，而是更新每个节点上的 eBPF 映射条目。有关更多信息，请参阅 GItHub PR。\n延伸阅读 Kubernetes 文档包含有关 Kubernetes 网络模型和 Kubernetes 网络插件的更多背景信息 。\n","date":1655438400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"65a29792cb9e0d4efe25a2be9b44ffe8","permalink":"https://lib.jimmysong.io/cilium-handbook/kubernetes/intro/","publishdate":"2022-06-17T12:00:00+08:00","relpermalink":"/cilium-handbook/kubernetes/intro/","section":"cilium-handbook","summary":"Cilium 能为 Kubernetes 集群提供什么？ 在 Kubernetes 集群中运行 Cilium 时提供以下功能： CNI 插件支持，为 pod 连接 提供 联网。 NetworkPolicy 资源的基于身份的实现，用于隔离三层和四层网络 pod 的连接。 以 CustomResourceDefinition 形式对 NetworkPolicy 的扩展，扩展策略控制以添加： 针对以下应用协议的入","tags":["Cilium"],"title":"Kubernetes 集成介绍","type":"book"},{"authors":null,"categories":["网络"],"content":"Cilium 在多个层面上提供安全性。可以单独使用或组合使用。\n 基于身份：端点之间的连接策略（三层），例如任何带有标签的端点 role=frontend 都可以连接到任何带有标签的端点 role=backend。 限制传入和传出连接的可访问端口（四层），例如带标签的端点 role=frontend只能在端口 443（https）上进行传出连接，端点role=backend 只能接受端口 443（https）上的连接。 应用程序协议级别的细粒度访问控制，以保护 HTTP 和远程过程调用（RPC）协议，例如带有标签的端点 role=frontend 只能执行 REST API 调用 GET /userdata/[0-9]+，所有其他与 role=backend API 的交互都受到限制。  ","date":1655438400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"9d81d23e67cbd0f05da5171a5595671a","permalink":"https://lib.jimmysong.io/cilium-handbook/security/intro/","publishdate":"2022-06-17T12:00:00+08:00","relpermalink":"/cilium-handbook/security/intro/","section":"cilium-handbook","summary":"Cilium 在多个层面上提供安全性。可以单独使用或组合使用。 基于身份：端点之间的连接策略（三层），例如任何带有标签的端点 role=frontend 都可以连接到任何带有标签的端点 role=backend。 限制传入和传出连接的可访问端口（","tags":["Cilium"],"title":"介绍","type":"book"},{"authors":null,"categories":["网络"],"content":"封装 当没有提供配置时，Cilium 会自动在此模式下运行，因为它是对底层网络基础设施要求最低的模式。\n在这种模式下，所有集群节点使用基于 UDP 的封装协议 VXLAN 或 Geneve。Cilium 节点之间的所有流量都被封装。\n对网络的要求   封装依赖于正常的节点到节点的连接。这意味着如果 Cilium 节点已经可以互相到达，那么所有的路由要求都已经满足了。\n  底层网络和防火墙必须允许封装数据包：\n   封装方式 端口范围 / 协议     VXLAN（默认） 8472/UDP   Geneve 6081/UDP      封装模式的优点 封装模式具有以下优点：\n  简单\n连接集群节点的网络不需要知道 PodCIDR。集群节点可以产生多个路由或链路层域。只要集群节点可以使用 IP/UDP 相互访问，底层网络的拓扑就无关紧要。\n  寻址空间\n由于不依赖于任何底层网络限制，如果相应地配置了 PodCIDR 大小，可用的寻址空间可能会更大，并且允许每个节点运行任意数量的 Pod。\n  自动配置\n当与 Kubernetes 等编排系统一起运行时，集群中所有节点的列表（包括它们关联的分配前缀节点）会自动提供给每个代理。加入集群的新节点将自动合并到网格中。\n  身份上下文\n封装协议允许将元数据与网络数据包一起携带。Cilium 利用这种能力来传输元数据，例如源安全身份。身份传输是一种优化，旨在避免在远程节点上进行一次身份查找。\n  封装模式的缺点 封装模式有以下缺点：\n  MTU 开销\n由于添加了封装标头，可用于有效负载的有效 MTU 低于本地路由（VXLAN 的每个网络数据包 50 字节）。这会导致特定网络连接的最大吞吐率较低。这可以通过启用巨型帧（每 1500 字节 50 字节的开销对比每 9000 字节的 50 字节开销）在很大程度上得到缓解。\n  本地路由 本地路由数据路径使用 tunnel: disabled 启用并启用本机数据包转发模式。本地数据包转发模式利用 Cilium 运行的网络的路由功能，而不是执行封装。\n   本地路由模式  在本地路由模式下，Cilium 会将所有未发送到另一个本地端点的数据包委托给 Linux 内核的路由子系统。这意味着数据包将被路由，就好像本地进程会发出数据包一样。因此，连接集群节点的网络必须能够路由 PodCIDR。\n当配置本地路由时，Cilium 会在 Linux 内核中自动启用 IP 转发。\n对网络的要求 为了运行本地路由模式，连接运行 Cilium 的主机的网络必须能够使用分配给 pod 或其他工作负载的地址转发 IP 流量。\n节点上的 Linux 内核必须知道如何转发运行 Cilium 的所有节点的 pod 或其他工作负载的数据包。这可以通过两种方式实现：\n 节点本身不知道如何路由所有 pod IP，但网络上存在一个知道如何到达所有其他 pod 的路由器。在这种情况下，Linux 节点配置为包含指向此类路由器的默认路由。该模型用于云提供商网络集成。 每个单独的节点都知道所有其他节点的所有 Pod IP，并且路由被插入到 Linux 内核路由表中来表示这一点。如果所有节点共享一个 L2 网络，则可以通过启用 auto-direct-node-routes: true 选项来解决此问题。否则，必须运行额外的系统组件（例如 BGP 守护程序）来分发路由。请参阅 使用 kube-router 运行 BGP 指南，了解如何使用 kube-router 项目实现此目的。  配置 必须设置以下配置选项以在本地路由模式下运行数据路径：\n tunnel: disabled：启用本机路由模式。 ipv4-native-routing-cidr: x.x.x.x/y：设置可以执行本机路由的 CIDR。  ","date":1655438400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"237cdb4f015cd2bc200ad4b9f690c28a","permalink":"https://lib.jimmysong.io/cilium-handbook/networking/routing/","publishdate":"2022-06-17T12:00:00+08:00","relpermalink":"/cilium-handbook/networking/routing/","section":"cilium-handbook","summary":"封装 当没有提供配置时，Cilium 会自动在此模式下运行，因为它是对底层网络基础设施要求最低的模式。 在这种模式下，所有集群节点使用基于 UDP 的封装协议 VXLAN 或 Geneve。Cilium 节点之间的所有流量都被封装","tags":["Cilium"],"title":"路由","type":"book"},{"authors":null,"categories":["网络"],"content":"本文将为你简要介绍 Cilium 和 Hubble。\n什么是 Cilium？ Cilium 是开源软件，用于透明地保护使用 Docker 和 Kubernetes 等 Linux 容器管理平台部署的应用服务之间的网络连接。\nCilium 的基础是一种新的 Linux 内核技术，称为 eBPF，它使强大的安全可视性和控制逻辑动态插入 Linux 本身。由于 eBPF 在 Linux 内核内运行，Cilium 安全策略的应用和更新无需对应用程序代码或容器配置进行任何改动。\n什么是 Hubble？ Hubble 是一个完全分布式的网络和安全可观测性平台。它建立在 Cilium 和 eBPF 之上，以完全透明的方式实现对服务的通信行为以及网络基础设施的深度可视性。\n通过建立在 Cilium 之上，Hubble 可以利用 eBPF 实现可视性。依靠 eBPF，所有的可视性都是可编程的，并允许采用一种动态的方法，最大限度地减少开销，同时按照用户的要求提供深入和详细的可视性。Hubble 的创建和专门设计是为了最好地利用这些新的 eBPF 力量。\nHubble 可以回答诸如以下问题。\n服务依赖和拓扑图  哪些服务在相互通信？频率如何？服务依赖关系图是什么样子的？ 正在进行哪些 HTTP 调用？服务正在生产或消费哪些 Kafka 主题？  网络监控和警报  是否有任何网络通信失败？为什么会出现通信失败？是 DNS 的问题吗？是应用还是网络问题？通讯是在四层（TCP）还是七层（HTTP）中断的？ 在过去 5 分钟内，哪些服务遇到了 DNS 解析问题？哪些服务最近经历了 TCP 连接中断或连接超时？未回复的 TCP SYN 请求的比率是多少？  应用监控  某一服务或所有集群中的 5xx 或 4xx HTTP 响应代码的比率是多少？ 在我的集群中，HTTP 请求和响应之间的 95% 和 99% 的延迟是什么？哪些服务的性能最差？两个服务之间的延时是多少？  安全观察性  哪些服务由于网络策略而被阻止连接？哪些服务被从集群外访问过？哪些服务解析了一个特定的 DNS 名称？  为什么使用 Cilium 和 Hubble？ 现代数据中心应用程序的开发已经转向面向服务的体系结构（SOA），通常称为微服务，其中大型应用程序被分成小型独立服务，这些服务使用 HTTP 等轻量级协议通过 API 相互通信。微服务应用程序往往是高度动态的，作为持续交付的一部分部署的滚动更新期间单个容器启动或销毁，应用程序扩展 / 缩小以适应负载变化。\n这种向高度动态的微服务的转变过程，给确保微服务之间的连接方面提出了挑战和机遇。传统的 Linux 网络安全方法（例如 iptables）过滤 IP 地址和 TCP/UDP 端口，但 IP 地址经常在动态微服务环境中流失。容器的高度不稳定的生命周期导致这些方法难以与应用程序并排扩展，因为负载均衡表和访问控制列表要不断更新，可能增长成包含数十万条规则。出于安全目的，协议端口（例如，用于 HTTP 流量的 TCP 端口 80）不能再用于区分应用流量，因为该端口用于跨服务的各种消息。\n另一个挑战是提供准确的可视性，因为传统系统使用 IP 地址作为主要识别工具，其在微服务架构中的寿命被大大缩短，可能才仅仅几秒钟。\n利用 Linux eBPF，Cilium 保留了透明地插入安全可视性 + 强制执行的能力，但这种方式基于服务 /pod/ 容器标识（与传统系统中的 IP 地址识别相反），并且可以根据应用层进行过滤 （例如 HTTP）。因此，通过将安全性与寻址分离，Cilium 不仅可以在高度动态的环境中应用安全策略，而且除了提供传统的三层和四层分割之外，还可以通过在 HTTP 层运行来提供更强的安全隔离。\neBPF 的使用使得 Cilium 能够以高度可扩展的方式实现以上功能，即使对于大规模环境也不例外。\n功能概述 以下是关于 Cilium 可以提供的功能的概述。\n透明的保护 API 能够保护现代应用程序协议，如 REST/HTTP、gRPC 和 Kafka。传统防火墙在三层和四层运行，在特定端口上运行的协议要么完全受信任，要么完全被阻止。Cilium 提供了过滤各个应用程序协议请求的功能，例如：\n 允许所有带有方法 GET 和路径 /public/.* 的 HTTP 请求。拒绝所有其他请求。 允许 service1 在 Kafka topic1 主题上生产，service2 消费 topic1。拒绝所有其他 Kafka 消息。 要求 HTTP 标头 X-Token: [0-9]+ 出现在所有 REST 调用中。  详情请参考 七层协议。\n基于身份来保护服务间通信 现代分布式应用程序依赖于诸如容器之类的技术来促进敏捷性并按需扩展。这将导致在短时间内启动大量应用容器。典型的容器防火墙通过过滤源 IP 地址和目标端口来保护工作负载。这就要求不论在集群中的哪个位置启动容器时都要操作所有服务器上的防火墙。\n为了避免受到规模限制，Cilium 为共享相同安全策略的应用程序容器组分配安全标识。然后，该标识与应用程序容器发出的所有网络数据包相关联，从而允许验证接收节点处的身份。使用键值存储执行安全身份管理。\n安全访问外部服务 基于标签的安全性是集群内部访问控制的首选工具。为了保护对外部服务的访问，支持入口（ingress）和出口（egress）的传统基于 CIDR 的安全策略。这允许限制对应用程序容器的访问以及对特定 IP 范围的访问。\n简单网络 一个简单的扁平三层网络能够跨越多个集群连接所有应用程序容器。使用主机范围分配器可以简化 IP 分配。这意味着每个主机可以在主机之间没有任何协调的情况下分配 IP。\n支持以下多节点网络模型：\n  Overlay：基于封装的虚拟网络产生所有主机。目前 VXLAN 和 Geneve 已经完成，但可以启用 Linux 支持的所有封装格式。\n何时使用此模式：此模式具有最小的基础架构和集成要求。它几乎适用于任何网络基础架构，唯一的要求是主机之间可以通过 IP 连接。\n  本机路由：使用 Linux 主机的常规路由表。网络必须能够路由应用程序容器的 IP 地址。\n何时使用此模式：此模式适用于高级用户，需要了解底层网络基础结构。此模式适用于：\n 本地 IPv6 网络 与云网络路由器配合使用 如果你已经在运行路由守护进程    负载均衡 Cilium 为应用容器之间的流量和外部服务实现了分布式负载均衡，能够完全取代 kube-proxy 等组件。负载均衡是在 eBPF 中使用高效的 hashtables 实现的，允许几乎无限规模扩展。\n对于南北向的负载均衡，Cilium 的 eBPF 实现了最大的性能优化，可以连接到 XDP（eXpress Data Path），并支持直接服务器返回（DSR）以及 Maglev 一致的散列，如果负载均衡操作不在源主机上执行的话。\n对于东西向类型的负载均衡，Cilium 在 Linux 内核的套接字层（如在 TCP 连接时）执行有效的服务到后端转换，这样就可以避免在较低层的每包 NAT 操作开销。\n带宽管理 Cilium 通过高效的基于 EDT（Earlist Departure Time，最早离开时间）的速率限制来实现带宽管理，eBPF 适用于从节点流出的容器的流量。与传统的方法相比，如在 CNI 插件中使用的 HTB（Hierarchy Token Bucket，分层令牌桶）或 TBF（Token Bucket Filter，令牌桶过滤器），这可以大大减少应用的传输尾部延迟，并避免在多队列网卡下锁定。\n监控和故障排除 可视性和故障排查是任何分布式系统运行的基础。虽然我们喜欢用 tcpdump 和 ping，它们很好用，但我们努力为故障排除提供更好的工具。包括以下工具：\n 带有元数据的事件监测：当一个数据包被丢弃时，该工具不只是报告数据包的来源和目的地 IP，该工具还提供发送方和接收方的完整标签信息以及许多其他信息。 通过 Prometheus 输出指标。关键指标通过 Prometheus 导出，以便与你现有的仪表盘整合。 Hubble：一个专门为 Cilium 编写的可观测性平台。它提供服务依赖拓扑、监控和警报，以及基于流量日志的应用和安全可视性。   下一章   ","date":1654574400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"05fbea2b9cd47a7da2b47635937a6953","permalink":"https://lib.jimmysong.io/cilium-handbook/intro/","publishdate":"2022-06-07T12:00:00+08:00","relpermalink":"/cilium-handbook/intro/","section":"cilium-handbook","summary":"本文将为你简要介绍 Cilium 和 Hubble。 什么是 Cilium？ Cilium 是开源软件，用于透明地保护使用 Docker 和 Kubernetes 等 Linux 容器管理平台部署的应用服务之间的网络连接。 Cilium 的基础是一种新的 Linux 内核技术，称为 eBPF，它使强大的安全可","tags":["Cilium"],"title":"Cilium 和 Hubble 简介","type":"book"},{"authors":null,"categories":["网络"],"content":"Cilium 代理（agent）和 Cilium 网络策略的配置决定了一个端点（Endpoint）是否接受来自某个来源的流量。代理可以进入以下三种策略执行模式：\n  default\n如果任何规则选择了一个 Endpoint 并且该规则有一个入口部分，那么该端点就会在入口处进入默认拒绝状态。如果任何规则选择了一个 Endpoint 并且该规则有一个出口部分，那么该端点就会在出口处进入默认拒绝状态。这意味着端点开始时没有任何限制，一旦有规则限制其在入口处接收流量或在出口处传输流量的能力，那么端点就会进入白名单模式，所有流量都必须明确允许。\n  always\n在 always 模式下，即使没有规则选择特定的端点，也会在所有端点上启用策略执行。如果你想配置健康实体，在启动 cilium-agent 时用 enable-policy=always 检查整个集群的连接性，你很可能想启用与健康端点的通信。\n  never\n在 “never\u0026#34; 模式下，即使规则选择了特定的端点，所有端点上的策略执行也被禁用。换句话说，所有流量都允许来自任何来源（入口处）或目的地（出口处）。\n  要在运行时为 Cilium 代理管理的所有端点配置策略执行模式，请使用：\n$ cilium config PolicyEnforcement={default,always,never} 如果你想在启动时为一个特定的代理配置策略执行模式，在启动 Cilium 守护程序时提供以下标志：\n$ cilium-agent --enable-policy={default,always,never} [...] 同样，你可以通过在 Cilium DaemonSet 中加入上述参数来启用整个 Kubernetes 集群的策略执行模式：\n- name:CILIUM_ENABLE_POLICYvalue:always规则基础知识 所有策略规则都是基于白名单模式，也就是说，策略中的每条规则都允许与该规则相匹配的流量。如果存在两条规则，其中一条可以匹配更广泛的流量，那么所有匹配更广泛规则的流量都将被允许。如果两个或更多的规则之间有一个交叉点，那么与这些规则的结合点相匹配的流量将被允许。最后，如果流量不匹配任何规则，它将根据网络策略执行模式被丢弃。\n策略规则共享一个共同的基本类型，指定规则适用于哪些端点，并共享元数据以识别规则。每条规则都被分割成一个入口部分和一个出口部分。入口部分包含必须应用于进入端点的流量的规则，出口部分包含应用于来自匹配端点选择器的端点的流量的规则。可以提供入口、出口或两者。如果入口和出口都被省略，规则就没有效果。\ntype Rule struct { // EndpointSelector选择所有应该受此规则约束的端点。  // EndpointSelector和NodeSelector不能同时为空，并且  // 互相排斥。  //  // +optional  EndpointSelector EndpointSelector `json: \u0026#34;endpointSelector,omitempty\u0026#34;`。 // NodeSelector 选择所有应该受此规则约束的节点。  // EndpointSelector和NodeSelector不能同时为空，并且相互排斥的。  // 只能在CiliumClusterwideNetworkPolicies中使用。  //  // +optional  NodeSelector EndpointSelector `json: \u0026#34;nodeSelector,omitempty\u0026#34;`。 // Ingress是一个IngressRule的列表，它在Ingress时被强制执行。  // 如果省略或为空，则此规则不适用于入口处。  //  // +optional  Ingress []IngressRule `json: \u0026#34;ingress,omitempty\u0026#34;`。 // Egress是一个在出口处执行的EgressRule的列表。  // 如果省略或为空，该规则不适用于出口处。  //  // +optional  Egress []EgressRule `json: \u0026#34;egress,omitempty\u0026#34;` // Labels 是一个可选的字符串列表，可以用来  // 重新识别该规则或存储元数据。它可以根据标签来查询  // 或删除基于标签的字符串。标签并不要求是  // 唯一的，多个规则可以有重叠的或相同的标签。  //  // +optional  Labels labels.LabelArray `json: \u0026#34;labels,omitempty\u0026#34;` // Description 是一个自由格式的字符串，它可以由规则的创建者使用。  // 它可以被规则的创建者用来存储该规则目的的可读解释。  // 规则不能通过注释来识别。  //  // +optional  Description string `json: \u0026#34;description,omitempty\u0026#34;`. } endpointSelector / nodeSelector\n选择策略规则所适用的端点或节点。策略规则将被应用于所有符合选择器中指定标签的端点。\nIngress\n必须在端点入口处适用的规则列表，即适用于进入端点的所有网络数据包。\nEgress\n必须适用于端点出口的规则列表，即适用于离开端点的所有网络数据包。\nLebels\n标签是用来识别规则的。规则可以通过标签列出和删除。通过以下方式导入的策略规则 kubernetes 自动获得 io.cilium.k8s.policy.name=NAME 的标签，其中 NAME 对应的是在 NetworkPolicy 或 CiliumNetworkPolicy 资源中指定的名称。\nDescription\n描述是一个字符串，不被 Cilium 所解释。它可以用来以人类可读的形式描述规则的意图和范围。\n端点选择器 端点选择器基于 Kubernetes LabelSelector。它之所以被称为端点选择器，是因为它只适用于 Endpoint.\n节点选择器 节点选择器也是基于端点选择器的，不过它不是与端点相关的标签相匹配，而是适用于与集群中的节点相关的标签。\n节点选择器只能用在 CiliumClusterwideNetworkPolicy。请参阅 Host Policies 以了解关于节点级策略范围的详细信息。\n","date":1654574400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"d6a9a95e96cee63d6a040d848de0ea11","permalink":"https://lib.jimmysong.io/cilium-handbook/policy/intro/","publishdate":"2022-06-07T12:00:00+08:00","relpermalink":"/cilium-handbook/policy/intro/","section":"cilium-handbook","summary":"Cilium 代理（agent）和 Cilium 网络策略的配置决定了一个端点（Endpoint）是否接受来自某个来源的流量。代理可以进入以下三种策略执行模式： default 如果任何规则选择了一个 Endpoint 并且该规则有一个入口部分，那么该端点就会","tags":["Cilium"],"title":"网络策略模式","type":"book"},{"authors":null,"categories":["网络"],"content":"本文将为你介绍 Cilium 和 Hubble 部署中包含的组件。\nCilium 包含以下组件：\n 代理 客户端 Operator CNI 插件  Hubble 包含以下组件：\n 服务器 中继器 客户端 图形用户界面 eBPF  另外你还需要一个数据库来存储代理的状态。\n下图展示的 Cilium 部署的组件。\n   Cilium 组件示意图  Cilium 代理\nCilium 代理（cilium-agent）在集群的每个节点上运行。在高层次上，代理接受通过 Kubernetes 或 API 的配置，描述网络、服务负载均衡、网络策略、可视性和监控要求。\nCilium 代理监听来自编排系统（如 Kubernetes）的事件，以了解容器或工作负载的启动和停止时间。它管理 eBPF 程序，Linux 内核用它来控制这些容器的所有网络访问。\n客户端（CLI）\nCilium CLI 客户端（cilium）是一个命令行工具，与 Cilium 代理一起安装。它与运行在同一节点上的 Cilium 代理的 REST API 互动。CLI 允许检查本地代理的状态。它还提供工具，直接访问 eBPF map 以验证其状态。\nOperator\nCilium Operator 负责管理集群中的职责，这些职责在逻辑上应该是为整个集群处理一次，而不是为集群中的每个节点处理一次。Cilium Operator 不在任何转发或网络策略决定的关键路径上。如果 Operator 暂时不可用，集群一般会继续运作。然而，根据配置的不同，Operator 的可用性失败可能导致：\n IP 地址管理（IPAM）的延迟，因此，如果 Operator 需要分配新的 IP 地址，那么新工作负载的调度也会延迟 未能更新 kvstore 的心跳密钥，这将导致代理宣布 kvstore 不健康并重新启动。  CNI 插件\nCNI 插件（cilium-cni）由 Kubernetes 在一个节点上调度或终止 pod 时调用。它与节点的 Cilium API 交互，以触发必要的数据通路配置，为 pod 提供网络、负载均衡和网络策略。\nHubble 服务器\nHubble 服务器在每个节点上运行，从 Cilium 检索基于 eBPF 的可视性。它被嵌入到 Cilium 代理中，以实现高性能和低开销。它提供一个 gRPC 服务来检索流量和 Prometheus 指标。\n中继器\n中继器（hubble-relay）是一个独立的组件，它知道所有正在运行的 Hubble 服务器，并通过连接它们各自的 gRPC API 和提供代表集群中所有服务器的 API，提供集群范围内的可视性。\n客户端（CLI）\nHubble CLI（hubble）是一个命令行工具，能够连接到 hubble-relay 的 gRPC API 或本地服务器来检索流量事件。\n图形用户界面（GUI）\n图形用户界面（hubble-ui）利用基于中继的可视性，提供一个图形化的服务依赖性和连接图。\neBPF\neBPF 是一个 Linux 内核字节码解释器，最初是用来过滤网络数据包的，例如 tcpdump 和 socket 过滤器。此后，它被扩展为额外的数据结构，如 hashtable 和数组，以及额外的动作，以支持数据包的处理、转发、封装等。内核验证器确保 eBPF 程序安全运行，JIT 编译器将字节码转换为 CPU 架构的特定指令，以提高本地执行效率。eBPF 程序可以在内核的各种钩点上运行，如传入和传出数据包。\neBPF 继续发展，并在每个新的 Linux 版本中获得额外的功能。Cilium 利用 eBPF 来执行核心数据通路过滤、处理、监控和重定向，并要求 eBPF 的功能在任何 Linux 内核 4.8.0 或更新的版本中。基于 4.8.x 已经宣布终结，4.9.x 已经被提名为稳定版本，我们建议至少运行内核 4.9.17（截至本文撰写时，当前最新的稳定 Linux 内核是 4.10.x）。\nCilium 能够探测到 Linux 内核的可用功能，并在探测到时自动利用更多的最新功能。\n数据存储 Cilium 需要一个数据存储来传播代理之间的状态。它支持以下数据存储：\nKubernetes CRD（默认）\n存储任何数据和传播状态的默认选择是使用 Kubernetes 自定义资源定义（CRD）。CRD 由 Kubernetes 提供，用于集群组件通过 Kubernetes 资源表示配置和状态。\n键值存储\n在 Cilium 的默认配置中配置的 Kubernetes CRD 可以满足状态存储和传播的所有要求。键值存储可以选择作为一种优化，以提高集群的可扩展性，因为直接使用键值存储的变化通知和存储要求更有效率。\n目前支持的键值存储是：\n etcd  提示\n  可以直接利用 Kubernetes 的 etcd 集群或维护一个专门的 etcd 集群。   ","date":1654574400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"42c0c4cd3279a10545bd88daca262ae8","permalink":"https://lib.jimmysong.io/cilium-handbook/concepts/overview/","publishdate":"2022-06-07T12:00:00+08:00","relpermalink":"/cilium-handbook/concepts/overview/","section":"cilium-handbook","summary":"本文将为你介绍 Cilium 和 Hubble 部署中包含的组件。 Cilium 包含以下组件： 代理 客户端 Operator CNI 插件 Hubble 包含以下组件： 服务器 中继器 客户端 图形用户界面 eBPF 另外你还需要一个数据库来存储代理的状态。 下图展示的 Cilium 部署的组件。 Cilium 组件示意图 Cilium 代","tags":["Cilium"],"title":"组件概览","type":"book"},{"authors":null,"categories":["Linux"],"content":"在过去的几年里，eBPF 已经从相对默默无闻变成了现代基础设施建设中最热门的技术领域之一。就我个人而言，自从看到 Thomas Graf 在 DockerCon 17 的黑带会议（Black Blet）1 上谈到 eBPF 时，我就对它的可能性感到兴奋。在云原生计算基金会（CNCF），我在技术监督委员会（TOC）的同事把 eBPF 作为我们预测 2021 年将会起飞的重点技术之一来关注。超过 2500 人报名参加了当年的 eBPF 峰会线上会议，世界上最先进的几家软件工程公司共同创建了 eBPF 基金会。显然，人们对这项技术有很大的兴趣。\n在这个简短的报告中，我希望能给你一些启示，为什么人们对 eBPF 如此兴奋，以及它在现代计算环境中提供的工具能力。你会了解到 eBPF 是什么以及为什么它如此强大。还有一些代码实例，以使这种感觉更加具象化（但如果你愿意，你可以跳过这些）。\n你将了解到在建立支持 eBPF 的工具时涉及的内容，以及为什么 eBPF 在如此短的时间内变得如普遍。\n在这份简短的报告中，难免无法了解所有的细节，但如果你想更深入地了解，我将给你一些参考信息。\n扩展的伯克利数据包过滤器 让我们把缩写说出来：eBPF 代表扩展的伯克利数据包过滤器（Extended Berkeley Packet Filter）。从这个名字中，你可以看到它的根源在于过滤网络数据包，而且最初的 论文 2 是在伯克利实验室（Lawrence Berkeley National Laboratory）写的。但是（在我看来）这个名字对于传达 eBPF 的真正力量并没有很大的帮助，因为\u0026#34;扩展\u0026#34; 版本可以实现比数据包过滤多得多的功能。最近，eBPF 被用作一个独立的名称，它所包含的内容比它的缩写更多。\n那么，如果它不仅仅是包过滤，什么是 eBPF？eBPF 是一个框架，允许用户在操作系统的内核内加载和运行自定义程序。这意味着它可以扩展甚至修改内核的行为。\n当 eBPF 程序被加载到内核中时，有一个验证器确保它是安全运行的，如果无法确认，则拒绝它。一旦加载，eBPF 程序需要被附加到一个事件上，这样，每当事件发生时，程序就会被触发。\neBPF 最初是为 Linux 开发的，这也是我在本报告中重点讨论的操作系统；但值得注意的是，截至本文写作时，微软正在 为 Windows 开发 eBPF 的实现。\n现在，广泛使用的 Linux 内核都支持 “扩展” 部分，eBPF 和 BPF 这两个术语现在基本上可以互换使用。\n基于 eBPF 的工具 正如你在本报告中所看到的，动态改变内核行为的能力非常有用。传统上，如果我们想观察应用程序的行为，我们在应用程序中添加代码，以产生日志和追踪。eBPF 允许我们收集关于应用程序行为的定制信息，通过在内核中观察它，而不必修改应用程序。我们可以在这种可观测性的基础上创建 eBPF 安全工具，从内核中检测甚至防止恶意活动。我们可以用 eBPF 创建强大的、高性能的网络功能，在内核内处理网络数据包，避免昂贵的用户空间转换。\n从内核的角度来观测应用程序的概念并不新颖——这建立在较早的 Linux 功能之上，比如 perf 3，它也从内核内部收集行为和性能信息，而不需要修改被测量的应用程序。但是这些工具定义了可以收集的数据种类以及数据的格式。有了 eBPF，我们就有了更大的灵活性，因为我们可以编写完全自定义的程序，允许我们出于不同的目的建立广泛的工具。\neBPF 编程的能力异常强大，但也很复杂。对于我们大多数人来说，eBPF 的效用不是来自于自己写的程序，而是来自于使用别人创造的工具。有越来越多的项目和供应商在 eBPF 平台上创建了新一代的工具，包括可观测性、安全性、网络等。\n我将在本报告后面讨论这些更高级别的工具，但是，如果你对 Linux 命令行很熟悉，迫不及待地想看到 eBPF 的运行，推荐你从 BCC 项目开始。BCC 包含丰富的追踪工具集合；即使只是瞥一眼列表也可以领略到 eBPF 广泛的应用，包括文件操作、内存使用、CPU 统计，甚至观察在系统任意位置输入的 bash 命令。\n在下一章，我们将介绍改变内核行为的作用，为什么使用 eBPF 比直接编写内核代码更容易操作。\n参考   Thomas Graf，《Cilium——使用 BPF 和 XDP 的网络和应用安全》（DockerCon 2017 年 4 月 17-20 日） ↩︎\n Steven McCanne 和 Van Jacobson，《BSD数据包过滤器：用户级数据包捕获的新架构》（工作文件，劳伦斯伯克利国家实验室，伯克利，1992 年 12 月 19 日）。 ↩︎\n perf 是一个 Linux 子系统，用于收集性能数据。 ↩︎\n   ","date":1654056000,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"ab3e6936b1c766f7116eaaf26797a67d","permalink":"https://lib.jimmysong.io/what-is-ebpf/introduction/","publishdate":"2022-06-01T12:00:00+08:00","relpermalink":"/what-is-ebpf/introduction/","section":"what-is-ebpf","summary":"在过去的几年里，eBPF 已经从相对默默无闻变成了现代基础设施建设中最热门的技术领域之一。就我个人而言，自从看到 Thomas Graf 在 DockerCon 17 的黑带会议（Black Blet）1 上谈到 eBPF 时，我就对它的可能性感到兴奋。在云原生","tags":["eBPF"],"title":"第一章：eBPF 简介","type":"book"},{"authors":null,"categories":["云原生"],"content":"Kubernetes 是云原生时代的 POSIX 在单机时代，POSIX 是类 UNIX 系统的通用 API，而在云原生时代，Kubernetes 是云操作系统的的 POSIX，它定义了基于云的分布式系统的 API。下表将 Kubernetes 与 POSIX 进行了对比。\n   对比项 Linux Kubernetes     隔离单元 进程 Pod   硬件 单机 数据中心   并发 线程 容器   资源管理 进程内存\u0026amp;CPU 内存、CPU Limit/Request   存储 文件 ConfigMap、Secret、Volume   网络 端口绑定 Service   终端 tty、pty、shell kubectl exec   网络安全 IPtables NetworkPolicy   权限 用户、文件权限 ServiceAccount、RBAC    \n  我们不能说 Linux 就是 POSIX，只能说 Linux 是 UNIX 兼容的。   参考  Kubernetes is the POSIX of the cloud - home.robusta.dev  ","date":1653444000,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"13697af524986eeb77d09eccc118b50a","permalink":"https://lib.jimmysong.io/cloud-native-handbook/kubernetes/what-is-kubernetes/","publishdate":"2022-05-25T10:00:00+08:00","relpermalink":"/cloud-native-handbook/kubernetes/what-is-kubernetes/","section":"cloud-native-handbook","summary":"Kubernetes 是云原生时代的 POSIX 在单机时代，POSIX 是类 UNIX 系统的通用 API，而在云原生时代，Kubernetes 是云操作系统的的 POSIX，它定义了基于云的分布式系统的 API。下表将 Kubernetes 与 POSIX 进行了对比。 对比项 Linux Kubernetes 隔","tags":["云原生"],"title":"什么是 Kubernetes?","type":"book"},{"authors":null,"categories":null,"content":"首先我们来阐述下将应用迁移到云原生架构的动机。\n速度 天下武功，唯快不破，市场竞争亦是如此。想象一下，能够快速创新、实验并交付软件的企业，与使用传统软件交付模式的企业，谁将在市场竞争中胜出呢？\n在传统企业中，为应用提供环境和部署新版本花费的时间通常以天、周或月来计算。这种速度严重限制了每个发行版可以承担的风险，因为修复这些错误往往跟发行一个新版本有差不多的耗时。\n互联网公司经常提到它们每天几百次发布的实践。为什么频繁发布如此重要？如果你可以每天实现几百次发布，你们就可以几乎立即从错误的版本恢复过来。如果你可以立即从错误中恢复过来，你就能够承受更多的风险。如果你可以承受更多的风险，你就可以做更疯狂的试验 —— 这些试验结果可能会成为你接下来的竞争优势。\n基于云基础设置的弹性和自服务的特性天生就适应于这种工作方式。通过调用云服务 API 来提供新的应用程序环境比基于表单的手动过程要快几个数量级。然后通过另一个 API 调用将代码部署到新的环境中。将自服务和 hook 添加到团队的 CI/CD 服务器环境中进一步加快了速度。现在，我们可以回答精益大师 Mary Poppendick 提出的问题了 ——“如果只是改变了应用的一行代码，您的组织需要多长时间才能把应用部署到线上？“答案是几分钟或几秒钟。\n你可以大胆想象 一下，如果你也可以达到这样的速度，你的团队、你的业务可以做哪些事情呢？\n安全 光是速度快还是不够的。如果你开车是一开始就把油门踩到底，你将因此发生事故而付出惨痛的代价（有时甚至是致命的）！不同的交通方式如飞机和特快列车都会兼顾速度和安全性。云原生应用架构在快速变动的需求、稳定性、可用性和耐久性之间寻求平衡。这是可能的而且非常有必要同时实现的。\n我们前面已经提到过，云原生应用架构可以让我们迅速地从错误中恢复。我们没有谈论如何预防错误，而在企业里往往在这一点上花费了大量的时间。在追寻速度的路上，大而全的前端升级，详尽的文档，架构复核委员会和漫长的回归测试周期在一次次成为我们的绊脚石。当然，之所以这样做都是出于好意。不幸的是，所有这些做法都不能提供一致可衡量的生产缺陷改善度量。\n那么我们如何才能做到即安全又快速呢？\n可视化\n我们的架构必须为我们提供必要的工具，以便可以在发生故障时看到它。我们需要观测一切的能力，建立一个 “哪些是正常” 的概况，检测与标准情况的偏差（包括绝对值和变化率），并确定哪些组件导致了这些偏差。功能丰富的指标、监控、警报、数据可视化框架和工具是所有云原生应用架构的核心。\n故障隔离\n为了限制与故障带来的风险，我们需要限制可能受到故障影响的组件或功能的范围。如果每次亚马逊的推荐引擎挂掉后人们就不能再在亚马逊上买产品，那将是灾难性的。单体架构通常就是这种类型的故障模式。云原生应用架构通常使用微服务。通过将系统拆解为微服务，我们可以将任何一个微服务的故障范围限制在这个微服务上，但还需要结合容错才能实现这一点。\n容错\n仅仅将系统拆解为可以独立部署的微服务还是不够的；还需要防止出现错误的组件将错误传递它所依赖的组件上而造成级联故障。Mike Nygard 在他的《Release It! - Pragmatic Programmers》一书中描述了一些容错模型，最受欢迎的是断路器。软件断路器的工作原理就类似于电子断路器（保险丝）：断开它所保护的组件与故障系统之间的回路以防止级联故障。它还可以提供一个优雅的回退行为，比如回路断开的时候提供一组默认的产品推荐。我们将在 “容错” 一节详细讨论该模型。\n自动恢复\n凭借可视化、故障隔离和容错能力，我们拥有确定故障所需的工具，从故障中恢复，并在进行错误检测和故障恢复的过程中为客户提供合理的服务水平。一些故障很容易识别：它们在每次发生时呈现出相同的易于检测的模式。以服务健康检查为例，结果只有两个：健康或不健康，up 或 down。很多时候，每次遇到这样的故障时，我们都会采取相同的行动。在健康检查失败的情况下，我们通常只需重新启动或重新部署相关服务。云原生应用架构不要当应用在这些情况下无需手动干预。相反，他们会自动检测和恢复。换句话说，他们给电脑装上了寻呼机而不是人。\n弹性扩展 随着需求的增加，我们必须扩大服务能力。过去我们通过垂直扩展来处理更多的需求：购买了更强悍的服务器。我们最终实现了自己的目标，但是步伐太慢，并且产生了更多的花费。这导致了基于高峰使用预测的容量规划。我们会问” 这项服务需要多大的计算能力？” 然后购买足够的硬件来满足这个要求。很多时候我们依然会判断错误，会在如黑色星期五这类事件中打破我们的可用容量规划。但是，更多的时候，我们将会遇到数以百计的服务器，它们的 CPU 都是空闲的，这会让资源使用率指标很难看。\n创新型的公司通过以下两个开创性的举措来解决这个问题：\n 它们不再继续购买更大型的服务器，取而代之的是用大量的更便宜机器来水平扩展应用实例。这些机器更容易获得，并且能够快速部署。 通过将大型服务器虚拟化成几个较小的服务器，并向其部署多个隔离的工作负载来改善现有大型服务器的资源利用率。  随着像亚马逊 AWS 这样的公有云基础设施的出现，这两个举措融合了起来。虚拟化工作被委托给云提供商，消费者只需要关注在大量的云服务器实例横向扩展它们的应用程序实例。最近，作为应用程序部署的单元，发生了另一个转变，从虚拟机转移到了容器。\n由于公司不再需要大量启动资金来部署软件，所以向云的转变打开了更多创新之门。正在进行的维护还需要较少的资本投入，并且通过 API 进行配置不仅可以提高初始部署的速度，还可以最大限度地提高我们应对需求变化的速度。\n不幸的是，所有这些好处都带有成本。相较于垂直扩展的应用，支持水平扩展的应用程序的架构必须不同。云的弹性要求应用程序的状态短暂性。我们不仅可以快速创建新的应用实例；我们也必须能够快速、安全地处置它们。这种需求是状态管理的问题：一次性与持久性如何互相影响？在大多数垂直架构中采用的诸如聚类会话和共享文件系统的传统方法并不能很好地支持水平扩展。\n云原生应用架构的另一个标志是将状态外部化到内存数据网格、缓存和持久对象存储，同时保持应用程序实例本身基本上是无状态的。无状态应用程序可以快速创建和销毁，以及附加到外部状态管理器和脱离外部状态管理器，增强我们响应需求变化的能力。当然这也需要外部状态管理器自己来扩展。大多数云基础设施提供商已经认识到这一必要性，并提供了这类服务的健康管理。\n移动应用和客户端多样性 2014 年 1 月，美国移动设备占互联网使用量的 55％。专门针对桌面用户而开发的应用程序的时代已经过去。不过，我们必须假设用户装在口袋里到处散步的是超级计算机。这对我们的应用架构有很大的影响，因为指数级用户可以随时随地与我们的系统进行交互。\n以查看银行账户余额为例。这项任务过去是通过拨打银行的呼叫中心，前往 ATM，或者在银行的一个分支机构的向柜员请求完成的。这些客户互动模式在任何时间内，都会对银行为底层软件系统提出新需求产生极大的限制。\n迁移到网上银行导致访问量的上升，但并没有从根本上改变交互模式。您仍然必须在计算机终端上与系统进行交互，这仍然显著限制了需求。正如我的同事 Andrew Clay Shafer 经常说的那样，“我们口袋里正带着超级计算机到处游走”，我们开始对这些系统带来很大负载。现在，成千上万的客户可以随时随地与银行系统进行互动。 一位银行行政人员表示，在发薪日，客户会每隔几分钟检查一次余额。 遗留的银行系统架构根本无法满足这种需求，而云原生的应用程序体系结构却可以。\n移动平台的巨大差异也对应用架构提出了要求。客户随时都可能与多个不同供应商生产的设备，运行多个不同的操作系统平台，运行多个版本的相同操作平台以及不同类别的设备（例如手机与平板电脑）进行交互。 这不仅对移动应用程序开发人员，还对后端服务的开发人员造成了各种限制。\n移动应用程序通常必须与多个传统系统以及云原生应用架构中的多个微服务进行交互。这些服务无法设计成支持客户使用的各种各样移动平台的独特需求。强迫实现这些不同的服务，为移动应用程序开发人员上带来了负担，增加了应用访问延迟和网络访问频率，导致应用响应慢、耗电量高，最终导致用户删除您的应用程序。云原生应用架构还通过诸如 API 网关之类的设计模式来支持移动优先开发的概念，API 网关将服务聚合负担转移回服务器端。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"c571bc9c3c54ceda61eaaa6b42f777f0","permalink":"https://lib.jimmysong.io/migrating-to-cloud-native-application-architectures/the-rise-of-cloud-native/why-cloud-native-application-architectures/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/migrating-to-cloud-native-application-architectures/the-rise-of-cloud-native/why-cloud-native-application-architectures/","section":"migrating-to-cloud-native-application-architectures","summary":"首先我们来阐述下将应用迁移到云原生架构的动机。 速度 天下武功，唯快不破，市场竞争亦是如此。想象一下，能够快速创新、实验并交付软件的企业，与使用传统软件交付模式的企业，谁将在市场竞争中胜出呢？ 在传统企业中","tags":null,"title":"1.1 为何使用云原生应用架构","type":"book"},{"authors":null,"categories":["安全"],"content":"由于微服务通常是以容器的形式实现的，因此容器编排和资源管理平台被用于服务的部署、运维和维护。\n一个典型的协调和资源管理平台由各种逻辑（形成抽象层）和物理工件组成，用于部署容器。例如，在 Kubernetes 中，容器在最小的部署单元内运行，称为 Pod。一个 Pod 理论上可以承载一组容器，但通常情况下，一个 Pod 内只运行一个容器。一组 Pod 被定义在所谓的节点内，节点可以是物理机或虚拟机（VM）。一组节点构成了一个集群。通常情况下，需要单个微服务的多个实例来分配工作负载，以达到预期的性能水平。集群是一个资源池（节点），用于分配微服务的工作负载。使用的技术之一是横向扩展，即访问频率较高的微服务被分配更多的实例或分配到具有更多资源（如 CPU 和 / 或内存）的节点。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"340150dc41398680b7b9d76d21c4b85d","permalink":"https://lib.jimmysong.io/service-mesh-devsecops/reference-platform/container-orchestration-and-resource-management-platform/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/service-mesh-devsecops/reference-platform/container-orchestration-and-resource-management-platform/","section":"service-mesh-devsecops","summary":"由于微服务通常是以容器的形式实现的，因此容器编排和资源管理平台被用于服务的部署、运维和维护。 一个典型的协调和资源管理平台由各种逻辑（形成抽象层）和物理工件组成，用于部署容器。例如，在 Kubernetes 中，容器在最小的","tags":["Service Mesh","DevSecOps"],"title":"2.1 容器编排和资源管理平台","type":"book"},{"authors":null,"categories":null,"content":"企业 IT 采用云原生架构所需的变革根本不是技术性的，而是企业文化和组织的变革，围绕消除造成浪费的结构、流程和活动。 在本节中，我们将研究必要的文化转变。\n从信息孤岛到 DevOps 企业 IT 通常被组织成以下许多孤岛：\n 软件开发 质量保证 数据库管理 系统管理 IT 运营 发布管理 项目管理  创建这些孤岛是为了让那些了解特定领域的人员来管理和指导那些执行该专业领域工作的人员。 这些孤岛通常具有不同的管理层次，工具集、沟通风格、词汇表和激励结构。这些差异启发了企业 IT 目标的不同范式，以及如何实现这一目标。\n但这里面存在很多矛盾，例如开发和运维分别对软件变更持有的观念就是个经常被提起的例子。开发的任务通常被视为通过开发软件功能为组织提供额外的价值。 这些功能本身就是向 IT 生态系统引入变更。 所以开发的使命可以被描述为 “交付变化”，而且经常根据有多少次变更来进行激励。\n相反，IT 运营的使命可以被描述为 “防止变更”。 IT 运营通常负责维护 IT 系统所需的可用性、弹性、性能和耐用性。因此，他们经常以维持关键绩效指标（KPI）来进行激励，例如平均故障间隔时间（MTBF）和平均恢复时间（MTTR）。与这些措施相关的主要风险因素之一是在系统中引入任何类型的变更。 那么，不是设法将开发期望的变更安全地引入 IT 生态系统，而是通过将流程放在一起，使变更变得痛苦，从而降低了变化率。\n这些不同的范式显然导致了许多额外的工作。项目工作中的协作、沟通和简单的交接变得乏味和痛苦，最糟糕的是导致绝对混乱（甚至是危险的）。 企业 IT 通常通过创建基于单据的系统和委员会会议驱动的复杂流程来尝试 “修复” 这种情况。企业 IT 价值流在所有非增值浪费下步履瞒珊。\n像这样的环境与云原生的速度思想背道而驰。 专业的信息孤岛和流程往往是由创造安全环境的愿望所驱动。然而，他们通常提供很少的附加安全性，在某些情况下，会使事情变得更糟！\n在其核心上，DevOps 代表着这样一种思想，即将这些信息孤岛构建成共享的工具集、词汇表和沟通结构，以服务于专注于单一目标的文化：快速、安全得交付价值。 然后创建激励结构，强制和奖励领导组织朝着这一目标迈进的行为。 官僚主义和流程被信任和责任所取代。\n在这个新的世界中，开发和 IT 运营部门向共同的直接领导者汇报，并进行合作，寻找能够持续提供价值并获得期望的可用性、弹性、性能和耐久性水平的实践。今天，这些对背景敏感的做法越来越多地包括采用云原生应用架构，提供完成组织的新的共同目标所需的技术支持。\n从间断均衡到持续交付 企业经常采用敏捷流程，如 Scrum，但是只能作为开发团队内部的本地优化。\n在这个行业中，我们实际上已经成功地将个别开发团队转变为更灵活的工作方式。 我们可以这样开始项目，撰写用户故事，并执行敏捷开发的所有例程，如迭代计划会议，日常站会，回顾和客户展示 demo。 我们中的冒险者甚至可能会冒险进行工程实践，如结对编程和测试驱动开发。持续集成，这在以前是一个相当激进的概念，现在已经成为企业软件词典的标准组成部分。事实上，我已经是几个企业软件团队中的一部分，并建立了高度优化的 “故事到演示” 周期，每个开发迭代的结果在客户演示期间被热烈接受。\n但是，这些团队会遇到可怕的问题：我们什么时候可以在生产环境中看到这些功能？\n这个问题我们很难回答，因为它迫使我们考虑自己无法控制的力量：\n 我们需要多长时间才能浏览独立的质量保证流程？ 我们什么时候可以加入生产发布的行列中？ 我们可以让 IT 运营及时为我们提供生产环境吗？  在这一点上，我们意识到自己已经陷入了戴维・韦斯特哈斯（Dave Westhas）所说的 scrum 瀑布中了。 我们的团队已经开始接受敏捷原则，但我们的组织却没有。所以，不是每次迭代产生一次生产部署（这是敏捷宣言的原始出发点），代码实际上是批量参与一个更传统的下游发布周期。\n这种操作风格产生直接的后果。我们不是每次迭代都将价值交付给客户，并将有价值的反馈回到开发团队，我们继续保持 “间断均衡” 的交付方式。 间断均衡实际上丧失了敏捷交付的两个主要优点：\n 客户可能需要几周的时间才能看到软件带来的新价值。 他们认为，这种新的敏捷工作方式只是 “像往常一样”，不会增强对开发团队的信任。 因为他们没有看到可靠的交付节奏，他们回到了以前的套路将尽可能多的要求尽可能多地堆放到发布版上。 为什么？ 因为他们对软件能够很快发布没有信心，他们希望尽可能多的价值被包括在最终交付时。 开发团队可能会好几周都没有得到真正的反馈。虽然演示很棒，但任何经验丰富的开发人员都知道，只有真实用户参与到软件之中才能获得最佳反馈。这些反馈能够帮助软件修正，使团队去做正确的事情。反馈推迟后，错误的可能性只能增加，并带来昂贵的返工。  获得云原生应用架构的好处需要我们转变为持续交付。 我们拥抱端到端拥抱价值的原则，而不是 Water Scrum Fall 组织驱动的间断平衡。设想这样一个生命周期的模型是由 Mary 和 Tom Poppendieck 在《实施精益软件开发（Addison-Wesley）》一书中描述的 “概念到现金” 的想法中提出来的。 这种方法考虑了所有必要的活动，将业务想法从概念传递到创造利润的角度，并构建可以使人们和过程达到最佳目标的价值流。\n我们技术上支持这种使用连续交付的工程实践的方法，每次迭代（实际上是次每个源代码提交！）都被证明可以以自动化的方式部署。 我们构建部署流水线，可自动执行每次测试，如果该测试失败，将会阻止生产部署。 唯一剩下的决定是商业决策：现在部署可用的新功能有很好的业务意义吗？ 我们已经知道它已经如广告中的方式工作，但是我们要现在就把它们交给客户吗？ 因为部署管道是完全自动化的，所以企业能够通过点击按钮来决定是否采取行动。\n从集中治理到分散自治 Waterscrumfall 文化中的一部分已经被特别提及，因为它已经被视为云原生架构采纳的一个关键。\n企业通常采用围绕应用架构和数据管理的集中治理结构，负责维护指导方针和标准的委员会，以及批准个人设计和变更。 集中治理旨在帮助解决以下几个问题：\n 可以防止技术栈的大范围不一致，降低组织的整体维护负担。 可以防止架构选型中的大范围不一致，从而形成组织的应用程序开发的共同观点。 整个组织可以一致地处理跨部门关切，例如合规性。 数据所有权可由具有全局视野的人来决定。  之所以创造这些结构，是因为我们相信它们将有助于提高质量、降低成本或两者兼而有之。然而，这些结构很少能够帮助我们提高质量节约成本，并且进一步妨碍了云原生应用架构寻求的交付速度。 正如单体应用架构导致了限制技术创新速度的瓶颈一样，单一的治理结构同样如此。架构委员会经常只会定期召集，并且经常需要很长的等待时才能发挥工作。即使是很小的数据模型的变化 —— 可能在几分钟或几个小时内完成的更改，即将被委员会批准的变更 —— 将会把时间浪费在一个不断增长的待办事项中。\n采用云原生应用架构时通常都会与分散式治理结合起来。建立云原生应用的团队拥有他们负责交付的能力的所有方面。他们拥有和管理数据、技术栈、应用架构、每个组件设计和 API 协议并将它们交付给组织的其余部分。如果需要对某事作出决策，则由团队自主制定和执行。\n团队个体的分散自治和自主性是通过最小化、轻量级的结构进行平衡的，这些结构在可独立开发和部署的服务之间使用集成模式（例如，他们更喜欢 HTTP REST JSON API 而不是不同风格的 RPC）来实现。这些结构通常会在底层解决交叉问题，如容错。激励团队自己设法解决这些问题，然后自发组织与其他团队一起建立共同的模式和框架。随着整个组织中的最优解决方案出现，该解决方案的所有权通常被转移到云框架 / 工具团队，这可能嵌入到平台运营团队中也可能不会。当组织正在围绕对架构共识进行改革时，云框架 / 工具团队通常也将开创解决方案。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"b3ea6ec0e7126d896c3853397154aafc","permalink":"https://lib.jimmysong.io/migrating-to-cloud-native-application-architectures/changes-needed/cultural-change/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/migrating-to-cloud-native-application-architectures/changes-needed/cultural-change/","section":"migrating-to-cloud-native-application-architectures","summary":"企业 IT 采用云原生架构所需的变革根本不是技术性的，而是企业文化和组织的变革，围绕消除造成浪费的结构、流程和活动。 在本节中，我们将研究必要的文化转变。 从信息孤岛到 DevOps 企业 IT 通常被组织成以下许多孤岛： 软件开发","tags":null,"title":"2.1 文化变革","type":"book"},{"authors":null,"categories":null,"content":"在和客户讨论分解数据、服务和团队后，客户经常向我提出这样的问题，“太棒了！但是我们要怎样实现呢？” 这是个好问题。如何拆分已有的单体应用并把他们迁移上云呢？\n事实证明，我已经看到了很多成功的例子，使用增量迁移这种相当可复制的模式，我现在向我所有的客户推荐这种模式。SoundCloud 和 Karma 就是公开的例子。\n本节中，我们将讲解如何一步步地将单体服务分解并将它们迁移到云上。\n新功能使用微服务形式 您可能感到很惊奇，第一步不是分解单体应用。我们假设您依然要在单体应用中构建服务。 事实上，如果您没有任何新的功能来构建，那么您甚至不应该考虑这个分解。（鉴于我们的主要动机是速度，您如何维持原状还能获取速度呢？）\n 团队决定，处理架构变化的最佳方法不是立即分解 Mothership 架构，而是不添加任何新的东西。我们所有的新功能以微服务形式构建…\n——Phil Calcado, SoundCloud\n 所以不要继续再向单体应用中增加代码，将所有的新功能以微服务的形式构建。这是第一步就要考虑好的，因为从头开始构架一个服务比分解一个单体应用并提出服务出来容易和快速的多。\n然而有一点不可避免，就是新构建的微服务需要与已有的单体应用通信才能完成工作，这个问题怎么解决？\n隔离层  因为我们大部分的业务逻辑都是基于 Rails 的单体应用，所以我们的微服务基本也要跟它们通信。\n——Phil Calcado, SoundCloud\n Eric Evans（Addison-Wesley）的领域驱动设计（DDD）讨论了隔离层的思想。 其目的是允许两个系统的集成，而不允许一个系统的领域模型破坏另一个系统的领域模型。 当您将新功能集成到微服务中时，不希望这些新服务与整体的紧密结合，让他们深入了解整体的内部结构。 隔离层是创建 API 协议的一种方式，使得整体架构看起来像其他微服务。\nEvans 将隔离层的实施划分为三个子模块，前两个代表着经典设计模式。\n（来自 Gamma 等人，Design Patterns：Elements of Reusable Object-Oriented So ware [Addison Wesley]）：\n表现层\n表现层的目的是为了简化与单体应用接口集成的过程。单体应用设计之初很可能没有考虑这个集成，因此我们引入了表现层来解决这个问题。它没有改变单体应用的模型，这很重要，注意不要将转换和集成问题耦合到一起。\n适配器\n我们用适配器来定义 service，用来提供我们需要的新功能。它知道如何获取系统请求并使用协议将请求发送给单体应用的表层。\n转换器\n转换器的职责是在单体应用与新的微服务之间进行请求和响应的领域模型转换。\n这三个松耦合的组件解决了以下三个问题：\n 系统集成 协议转换 模型转换  剩下的是通信链路的位置。在 DDD 中，Evans 讨论了两种选择。当您无法访问或更改遗留系统时，第一，将系统的表现层设置为主要功能。我们的重点在于我们控制的整体，所以我们将倾向于 Evans 的第二个建议，适配器到表现层。使用这种替代方法，我们将表现层构筑到单体中，允许在适配器和表现层之间进行通信，因为在为此专门构建的组件之间创建连接更容易。\n最后，要注意隔离层可以促进双向通信。 正如我们新的微服务可能需要与整体进行通信以完成工作一样，反之亦然，特别是当我们进入下一阶段时。\n扼杀单体应用  在架构调整后，我们的团队可以在更加灵活的环境中自由构建新功能和增强功能。 然而，一个重要的问题仍然存在：我们如何从名为 Mothership 的单体 Rails 应用程序中提取功能？\n——Pilil Calcado，SoundCloud\n 我从 Martin Fowler 的题为 “扼杀应用” 的文章中借用了 “扼杀巨石” 的想法。 在这篇文章中，Fowler 解释了逐渐创造 “围绕旧系统边缘的新系统，让它几年来慢慢增长，直到旧系统被扼杀” 的想法。这种情况同样适用于我们。通过提取的微服务和其他隔离层的组合，我们将围绕现有单体的边缘构建一个新的云原生系统。\n两个标准帮助我们选择要提取哪些组件：\n SoundCloud 指出了第一个标准：识别单体中的有界上下文。如果您回想起我们之前讨论有限上下文，它需要一个内部一致的领域模型。我们的单体领域模型极有可能不是内部一致的。现在是开始识别子模型的时候了，里面有我们要提取的候选者。 第二个标准是优先考虑的：在众多的候选者中我们应该首先提取哪一个呢？我们可以回顾一下迁移到云原生架构的第一个原因：创新速度。什么候选微服务将最受益于创新速度？我们显然希望选择那些正在改变我们当前业务需求的服务。看看单体应用的积压。确定需要更改的代码的区域，以便提交更改的要求，然后在进行所需更改之前提取适当的有界上下文。  潜在的结束状态 我们怎么知道何时结束？下面有两个基本的结束状态：\n 单体架构已经被完全扼杀。所有的有界上下文都被提取为微服务。最后一步是确定消除不再需要隔离层的机会。 单体架构被扼杀到了这样一个点：额外服务提取的成本超过必要开发努力的回报。 单体的一些部分可能相当稳定 —— 它们几年来都没有改变，还是一直都在运行得好好的。迁移这些部分可能没有太大的价值，维持必要的隔离层与其集成的成本足够低，我们可以长期负担。  ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"a25076c4d325d118bf491f850d12aa39","permalink":"https://lib.jimmysong.io/migrating-to-cloud-native-application-architectures/migration-cookbook/decomposition-recipes/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/migrating-to-cloud-native-application-architectures/migration-cookbook/decomposition-recipes/","section":"migrating-to-cloud-native-application-architectures","summary":"在和客户讨论分解数据、服务和团队后，客户经常向我提出这样的问题，“太棒了！但是我们要怎样实现呢？” 这是个好问题。如何拆分已有的单体应用并把他们迁移上云呢？ 事实证明，我已经看到了很多成功的例子，使用增量","tags":null,"title":"3.1 分解架构","type":"book"},{"authors":null,"categories":["安全"],"content":"DevSecOps 是一种软件开发、部署和生命周期管理方法，它涉及到从整个应用程序或平台的一次大型发布转变为持续集成、持续交付和持续部署（CI/CD）方法。这种转变又要求公司的 IT 部门的结构和工作流程发生变化。最明显的变化是组织一个 DevSecOps 小组，由软件开发人员、安全专家和 IT 运维专家组成，负责应用程序（即微服务）的每一部分。这个较小的团队不仅能促进最初的敏捷开发和部署的效率和效果，还能促进后续的生命周期管理活动，如监控应用行为、开发补丁、修复错误或扩展应用。这种具有三个领域专业知识的跨职能团队的组成，构成了在组织中引入 DevSecOps 的关键成功因素。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"4d95d5e42b6c66d6dbb0709d8cf5b9b2","permalink":"https://lib.jimmysong.io/service-mesh-devsecops/devsecops/organizational-preparedness-for-devsecops/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/service-mesh-devsecops/devsecops/organizational-preparedness-for-devsecops/","section":"service-mesh-devsecops","summary":"DevSecOps 是一种软件开发、部署和生命周期管理方法，它涉及到从整个应用程序或平台的一次大型发布转变为持续集成、持续交付和持续部署（CI/CD）方法。这种转变又要求公司的 IT 部门的结构和工作流程发生变化。最明显的变","tags":["Service Mesh","DevSecOps"],"title":"3.1 组织对 DevSecOps 的准备情况","type":"book"},{"authors":null,"categories":["安全"],"content":"对上述五类代码（即应用、应用服务、基础设施、策略和监控）的简要描述如下：\n 应用程序代码和应用服务代码：前者包含一组特定业务事务的数据和应用逻辑，而后者包含所有服务的代码，如网络连接、负载均衡和网络弹性。 基础设施即代码（IaC）：用于提供和配置基础设施资源的代码，它以可重复和一致的方式承载 应用程序的部署。这种代码是用一种声明性语言编写的，当执行时，为正在部署的应用程序提供和配置基础设施。这种类型的代码就像在应用程序的微服务中发现的任何其他代码，只是它提供的是基础设施服务（例如，配置服务器）而不是事务服务（例如，在线零售应用程序的支付处理）。 策略即代码：描述了许多策略，包括安全策略，作为 可执行模块。一个例子是授权策略，它的代码包含了策略（如允许、拒绝等）和适用领域（如 RESTAPI 的方法，GET、PUT 等，路径等动词或工件）这段代码可以用特殊用途的策略语言（如 Rego）或常规应用中使用的语言（如 Go）编写。这段代码可能与 IaC 的配置代码有一些重合。然而，对于实施与特定于应用领域的关键安全服务相关的策略，需要一个单独的策略作为代码，驻留在参考平台的策略执行点（PEP）中。 可观测性即代码：推断系统内部状态的能力，并对系统内何时以及更重要的是为何发生错误提供可操作的洞察力。它是一种全栈式的可观测性，包括监测和分析，并对应用程序和承载它们的系统的整体性能提供关键的洞察力。在参考平台的背景下，可观测性即代码是指在代理中创建机构的那部分代码，并为从微服务应用中收集三种类型的数据（即日志、跟踪和遥测）创建功能。这类代码还向外部工具提供或传输数据（例如，日志聚合工具，它聚合来自单个微服务的日志数据，为瓶颈服务提供跟踪数据分析，从遥测数据生成反映应用健康状况的指标等）。以下描述了对作为代码的可观测性所实现的三种功能：   日志捕获详细的错误信息，以及用于故障排除的调试日志和堆栈跟踪。 追踪应用程序的请求，因为它们通过多个微服务来完成一项事务，以确定分布式或基于微服务的生态系统中的问题或性能瓶颈。 监测，或称度量，收集遥测从应用程序和服务中收集数据。  每种代码类型都有相关的 CI/CD 管道，并在第 4.2 到 4.5 节中进行了描述。应用服务代码、基础设施即代码、策略即代码和可观测性即代码类型之间可能存在重叠。\n托管这五种代码类型的参考平台的组成成分是：\n 业务功能组件（由几个微服务模块组成，每个模块通常作为一个容器实现），体现了应用逻辑（例如，与数据交互，执行事务等），从而形成应用代码。 基础设施组件（包含计算机、网络和存储资源），其成员可以使用基础设施即代码进行配置。 服务网格组件（通过控制面模块和服务代理的组合实现），提供应用服务，执行策略（例如，认证和授权），并包含应用服务代码和策略作为代码。 监测组件（参与确定表明应用程序健康状况的参数的模块），执行功能（例如，日志聚合、生成指标、生成仪表板的显示等）并包含作为代码的可观测性。  策略和可观测性代码类型在服务网格中的分布情况如下：\n 代理组件（入口、sidecar 和出口）。这些组件容纳了与会话建立、路由、认证和授权功能有关的编码策略。 服务网格的控制平面。这里面有一些代码，用于转发来自服务的遥测信息，并由代理发送至专门的监控工具，认证证书的生成和维护，更新代理机构中的策略，监控服务协调平台中的整体配置，以生成新的代理，并删除与停用的微服务相关的过时代理。 外部模块。这些内部模块在应用和企业层面执行专门的功能（例如，如集中授权或权利服务器、集中记录器、通过仪表板监测 / 提醒服务器状态等），并建立一个全面的应用状态视图。这些模块由来自代理或控制平面的代码调用。  ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"bd8a53cd6c58bf54fce6b09b62a6fb07","permalink":"https://lib.jimmysong.io/service-mesh-devsecops/implement/description-of-code-types-and-reference-platform-components/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/service-mesh-devsecops/implement/description-of-code-types-and-reference-platform-components/","section":"service-mesh-devsecops","summary":"对上述五类代码（即应用、应用服务、基础设施、策略和监控）的简要描述如下： 应用程序代码和应用服务代码：前者包含一组特定业务事务的数据和应用逻辑，而后者包含所有服务的代码，如网络连接、负载均衡和网络弹性。","tags":["Service Mesh","DevSecOps"],"title":"4.1 代码类型和参考平台组件的描述","type":"book"},{"authors":null,"categories":["云原生"],"content":"还记得有一次告警电话半夜把我吵醒，发现是生产环境离线了。原来是系统瘫痪了，我们不得不要赔钱，这是我的错。\n从那一刻起，我就一直痴迷于构建坚如磐石的基础架构和基础架构管理系统，这样我就不会重蹈覆辙了。在我的职业生涯中，我为 Terraform、Kubernetes，一些编程语言和 Kops 做出过贡献，并创建了 Kubicorn。我不仅见证了系统基础架构的发展，而且我也帮助它完善。随着基础架构行业的发展，我们发现企业基础架构现在正以新的、令人兴奋的方式通过应用层管理。到目前为止，Kubernetes 是这种管理基础架构的新范例的最成熟的例子。\n我与人合著了这本书，部分地介绍了将基础架构作为云原生软件的新范例。此外，我希望鼓励基础架构工程师开始编写云原生应用程序。在这本书中，我们探讨了管理基础架构的丰富历史，并为云原生技术的未来定义了管理基础架构的模式。我们解释了基础架构由软件化 API 驱动的重要性。我们还探索了创建复杂系统的第一个基础架构组件的引导问题，并教授了扩展和测试基础架构的重要性。\n我于 2017 年加入 Heptio，担任资深布道师，并且很高兴能与行业中最聪明的系统工程师密切合作。构建纯粹的开源技术对我来说一直都很重要，Heptio 也拥有这种激情。我很荣幸能在这样一个环境中工作，让我更热爱这个行业。我希望你喜欢这本书，就像 Justin 和我喜欢写这本书一样。\n——Kris Nova\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"d6f8eb92993c738fb3218bb4f982c9ef","permalink":"https://lib.jimmysong.io/cloud-native-infra/foreword/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/cloud-native-infra/foreword/","section":"cloud-native-infra","summary":"还记得有一次告警电话半夜把我吵醒，发现是生产环境离线了。原来是系统瘫痪了，我们不得不要赔钱，这是我的错。 从那一刻起，我就一直痴迷于构建坚如磐石的基础架构和基础架构管理系统，这样我就不会重蹈覆辙了。在我","tags":["云原生"],"title":"前言","type":"book"},{"authors":null,"categories":["安全"],"content":"以下是关于本书的声明。\n许可 本出版物由 NIST 根据 2014 年《联邦信息安全现代化法案》（FISMA）（44 U.S.C. §3551 etseq）规定的法定职责编写，公共法律（P.L.）113-283。NIST 负责制定信息安全标准和准则，包括联邦信息系统的最低要求，但这些标准和准则在未经对国家安全系统行使策略权力的适当联邦官员明确批准的情况下，不得适用于这些系统。本准则与管理和预算办公室（OMB）A-130 号通知的要求一致。\n本出版物中的任何内容都不应被视为与商务部长根据法定授权对联邦机构的强制性和约束性标准和准则相抵触。这些准则也不应被解释为改变或取代商务部长、OMB 主任或任何其他联邦官员的现有权力。本出版物可由非政府组织在自愿的基础上使用，在美国不受版权限制。但是，请注明出处，NIST 将对此表示感谢。\n国家标准和技术研究所特别出版物 800-204C Natl.Inst. Stand.Technol.Spec.800-204C, 45 pages (March 2022) CODEN: NSPUE2\n本出版物可从以下网站免费获取。\nhttps://doi.org/10.6028/NIST.SP.800-204C\n关于计算机系统技术的报告 美国国家标准与技术研究所（NIST）的信息技术实验室（ITL）通过为国家的测量和标准基础设施提供技术领导来促进美国经济和公共福利。ITL 开发测试、测试方法、参考数据、概念实施证明和技术分析，以推动信息技术的发展和生产性使用。ITL 的职责包括为联邦信息系统中与国家安全无关的信息制定管理、行政、技术和物理标准和准则，以实现低成本的安全和隐私。\n摘要 云原生应用已经发展成为一个标准化的架构，由多个松散耦合的组件组成，这些组件被称为微服务（通常通常以容器实现），由提供应用服务的基础设施（如服务网格）支持。这两个组件通常都被托管在一个容器调度和资源管理平台上。在这个架构中，应用环境中涉及的整套源代码可以分为五种类型：1）应用代码（体现应用逻辑）；2）应用服务代码（用于会话建立、网络连接等服务）；3）基础设施即代码（用于配置计算、网络和存储资源）；4）策略即代码（用于定义运行时策略，如以声明性代码表达的零信任）；5）可观测性即代码（用于持续监测应用运行时状态）。由于安全、商业竞争力和松散耦合的应用组件的固有结构，这类应用需要一个不同的开发、部署和运行时范式。DevSecOps（分别由开发、安全和运维的首字母缩写组成）已经被发现是这些应用的促进范式，其基本要素包括持续集成、持续交付和持续部署（CI/CD）管道。这些管道是将开发者的源代码通过各个阶段的工作流程，如构建、测试、打包、部署和运维，由带有反馈机制的自动化工具支持。本文的目的是为云原生应用的 DevSecOps 原语的实施提供指导，其架构和代码类型如上所述。本文还讨论了这种方法对高安全保障和实现持续运维授权（C-ATO）的好处。\n鸣谢 作者首先要感谢 NIST 的 David Ferraiolo，他发起了这项工作，为基于微服务的应用中服务网格的开发、部署和监控提供了有针对性的 DevSecOps 原语实施指导。衷心感谢美国空军 CSO Nicolas Chaillan 先生，感谢他详细而有见地的审查和反馈。还要感谢 Tetrate 公司的 Zack Butcher 为本文标题提供的建议。作者还对 NIST 的 Isabel Van Wyk 的详细编辑审查表示感谢。\n专利披露通知 通知：信息技术实验室（ITL）已要求专利权持有人向 ITL 披露其使用可能需要遵守本出版物的指导或要求的专利权。然而，专利持有人没有义务回应 ITL 的专利要求，ITL 也没有进行专利搜索，以确定哪些专利可能适用于本出版物。\n截至本出版物发布之日，以及在呼吁确定可能需要使用其来遵守本出版物的指导或要求的专利权利要求之后，ITL 没有发现任何此类专利权利要求。\nITL 没有作出或暗示在使用本出版物时不需要许可证以避免专利侵权。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"4b3c6d70ff0fbe8f5dcaf6190abad0e9","permalink":"https://lib.jimmysong.io/service-mesh-devsecops/preface/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/service-mesh-devsecops/preface/","section":"service-mesh-devsecops","summary":"以下是关于本书的声明。 许可 本出版物由 NIST 根据 2014 年《联邦信息安全现代化法案》（FISMA）（44 U.S.C. §3551 etseq）规定的法定职责编写，公共法律（P.L.）113-283。NIST 负责制定信息安全标准","tags":["Service Mesh","DevSecOps"],"title":"声明","type":"book"},{"authors":null,"categories":["安全"],"content":"文件变更历史\n英文版\n   日期 版本 描述     2021 年 8 月 1.0 首次发布    中文版\n   日期 版本 描述     2021 年 8 月 8 日 1.0 首次发布    担保和认可的免责声明\n本文件中的信息和意见是 “按原样” 提供的，没有任何保证或担保。本文件以商品名称、商标、制造商或其他方式提及任何具体的商业产品、程序或服务，并不一定构成或暗示美国政府对其的认可、推荐或青睐，而且本指南不得用于广告或产品代言的目的。\n关于中文版\n中文版为 Jimmy Song 个人翻译，翻译过程中完全遵照原版，未做任何删减。其本人与本书的原作者没有任何组织或利益上的联系，翻译本书仅为交流学习之用。\n商标认可\n Kubernetes 是 Linux 基金会的注册商标。 SELinux 是美国国家安全局的注册商标。 AppArmor 是 SUSE LLC 的注册商标。 Windows 和 Hyper-V 是微软公司的注册商标。 ETCD 是 CoreOS, Inc. 的注册商标。 Syslog-ng 是 One Identity Software International Designated Activity 公司的注册商标。 Prometheus 是 Linux 基金会的注册商标。 Grafana 是 Raintank, Inc.dba Grafana Labs 的注册商标。 Elasticsearch 和 ELK Stack 是 Elasticsearch B.V 的注册商标。  版权确认\n本文件中的信息、例子和数字基于 Kubernetes 作者的 Kubernetes 文档，以知识共享署名 4.0 许可方式发布。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"41195e39476d86cfb31a2cd9ed789d88","permalink":"https://lib.jimmysong.io/kubernetes-hardening-guidance/notices-and-hitory/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/kubernetes-hardening-guidance/notices-and-hitory/","section":"kubernetes-hardening-guidance","summary":"文件变更历史 英文版 日期 版本 描述 2021 年 8 月 1.0 首次发布 中文版 日期 版本 描述 2021 年 8 月 8 日 1.0 首次发布 担保和认可的免责声明 本文件中的信息和意见是 “按原样” 提供的，没有任何保证或担保。本文件以","tags":["Kubernetes","安全"],"title":"通知和历史","type":"book"},{"authors":null,"categories":["云原生"],"content":"Service Mesh 又译作 “服务网格”，作为服务间通信的基础设施层。Buoyant 公司的 CEO Willian Morgan 在他的这篇文章 WHAT’S A SERVICE MESH? AND WHY DO I NEED ONE? 中解释了什么是 Service Mesh，为什么云原生应用需要 Service Mesh。\n服务网格是用于处理服务间通信的专用基础设施层。它负责通过包含现代云原生应用程序的复杂服务拓扑来可靠地传递请求。实际上，服务网格通常通过一组轻量级网络代理来实现，这些代理与应用程序代码一起部署，而不需要感知应用程序本身。—— Willian Morgan Buoyant CEO\n服务网格（Service Mesh）这个术语通常用于描述构成这些应用程序的微服务网络以及应用之间的交互。随着规模和复杂性的增长，服务网格越来越难以理解和管理。它的需求包括服务发现、负载均衡、故障恢复、指标收集和监控以及通常更加复杂的运维需求，例如 A/B 测试、金丝雀发布、限流、访问控制和端到端认证等。\n服务网格的特点 服务网格有如下几个特点：\n 应用程序间通讯的中间层 轻量级网络代理 应用程序无感知 解耦应用程序的重试 / 超时、监控、追踪和服务发现  目前两款流行的服务网格开源软件 Linkerd 和 Istio 都可以直接在 Kubernetes 中集成，其中 Linkerd 是 CNCF 成员项目，并在 2021 年 7 月毕业。Istio 在 2018 年 7 月 31 日宣布 1.0，并在 2020 年 7 月将 商标捐献给 Open Usage Commons。\n理解服务网格 如果用一句话来解释什么是服务网格，可以将它比作是应用程序或者说微服务间的 TCP/IP，负责服务之间的网络调用、限流、熔断和监控。对于编写应用程序来说一般无须关心 TCP/IP 这一层（比如通过 HTTP 协议的 RESTful 应用），同样使用服务网格也就无须关系服务之间的那些原来是通过应用程序或者其他框架实现的事情，比如 Spring Cloud、OSS，现在只要交给服务网格就可以了。\nPhil Calçado 在他的这篇博客 Pattern: Service Mesh 中详细解释了服务网格的来龙去脉：\n 从最原始的主机之间直接使用网线相连 网络层的出现 集成到应用程序内部的控制流 分解到应用程序外部的控制流 应用程序的中集成服务发现和断路器 出现了专门用于服务发现和断路器的软件包 / 库，如 Twitter 的 Finagle 和 Facebook 的 Proxygen，这时候还是集成在应用程序内部 出现了专门用于服务发现和断路器的开源软件，如 Netflix OSS、Airbnb 的 synapse 和 nerve 最后作为微服务的中间层服务网格出现  服务网格的架构如下图所示：\n   服务网格架构图  图片来自：Pattern: Service Mesh\n服务网格作为 sidecar 运行，对应用程序来说是透明，所有应用程序间的流量都会通过它，所以对应用程序流量的控制都可以在 serivce mesh 中实现。\n服务网格如何工作？ 下面以 Istio 为例讲解服务网格如何在 Kubernetes 中工作。\n Istio 将服务请求路由到目的地址，根据中的参数判断是到生产环境、测试环境还是 staging 环境中的服务（服务可能同时部署在这三个环境中），是路由到本地环境还是公有云环境？所有的这些路由信息可以动态配置，可以是全局配置也可以为某些服务单独配置。 当 Istio 确认了目的地址后，将流量发送到相应服务发现端点，在 Kubernetes 中是 service，然后 service 会将服务转发给后端的实例。 Istio 根据它观测到最近请求的延迟时间，选择出所有应用程序的实例中响应最快的实例。 Istio 将请求发送给该实例，同时记录响应类型和延迟数据。 如果该实例挂了、不响应了或者进程不工作了，Istio 将把请求发送到其他实例上重试。 如果该实例持续返回 error，Istio 会将该实例从负载均衡池中移除，稍后再周期性得重试。 如果请求的截止时间已过，Istio 主动失败该请求，而不是再次尝试添加负载。 Istio 以 metric 和分布式追踪的形式捕获上述行为的各个方面，这些追踪信息将发送到集中 metric 系统。  为何使用服务网格？ 服务网格并没有给我们带来新功能，它是用于解决其他工具已经解决过的问题，只不过这次是在云原生的 Kubernetes 环境下的实现。\n在传统的 MVC 三层 Web 应用程序架构下，服务之间的通讯并不复杂，在应用程序内部自己管理即可，但是在现今的复杂的大型网站情况下，单体应用被分解为众多的微服务，服务之间的依赖和通讯十分复杂，出现了 Twitter 开发的 Finagle、Netflix 开发的 Hystrix 和 Google 的 Stubby 这样的 “胖客户端” 库，这些就是早期的服务网格，但是它们都近适用于特定的环境和特定的开发语言，并不能作为平台级的服务网格支持。\n在云原生架构下，容器的使用给予了异构应用程序的更多可行性，Kubernetes 增强的应用的横向扩容能力，用户可以快速的编排出复杂环境、复杂依赖关系的应用程序，同时开发者又无须过分关心应用程序的监控、扩展性、服务发现和分布式追踪这些繁琐的事情而专注于程序开发，赋予开发者更多的创造性。\n参考  Istio: A service mesh for AWS ECS - medium.com 初次了解 Istio - istio.io Application Network Functions With ESBs, API Management, and Now.. Service Mesh? - blog.christianposta.com Pattern: Service Mesh - philcalcado.com Envoy 官方文档中文版 - cloudnative.to Istio 官方文档 - istio.io  ","date":1651532400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"df1e3c2bf05a05a51e1d13325d264bac","permalink":"https://lib.jimmysong.io/cloud-native-handbook/service-mesh/what-is-service-mesh/","publishdate":"2022-05-03T00:00:00+01:00","relpermalink":"/cloud-native-handbook/service-mesh/what-is-service-mesh/","section":"cloud-native-handbook","summary":"Service Mesh 又译作 “服务网格”，作为服务间通信的基础设施层。Buoyant 公司的 CEO Willian Morgan 在他的这篇文章 WHAT’S A SERVICE MESH? AND WHY DO I NEED ONE? 中解释了什么是 Service Mesh，为什么云原生应用需要 Service Mesh。 服务网格是用于处理服","tags":["云原生"],"title":"什么是服务网格？","type":"book"},{"authors":null,"categories":["云原生"],"content":"云原生（Cloud Native）这个词汇由来已久，以致于何时出现已无据可考。云原生开始大规模出现在受众视线中，与 Pivotal 提出的云原生应用的理念有着莫大的关系。我们现在谈到云原生，更多的指的是一种文化，而不具象为哪些技术体系。\n Pivotal 推出过 Pivotal Cloud Foundry 云原生应用平台和 Spring 开源 Java 开发框架，成为云原生应用架构中先驱者和探路者。Pivotal 是云原生应用平台第一股，2018 年在纽交所上市，2019 年底被 VMWare 以 27 亿美元收购，加入到 VMware 新的产品线 Tanzu。\n Pivotal 最初的定义 早在 2015 年 Pivotal 公司的 Matt Stine 写了一本叫做 迁移到云原生应用架构 的小册子，其中探讨了云原生应用架构的几个主要特征：\n 符合 12 因素应用 面向微服务架构 自服务敏捷架构 基于 API 的协作 抗脆弱性  笔者已于 2017 年翻译了本书，详见 迁移到云原生应用架构。\nCNCF 最初的定义 到了 2015 年 Google 主导成立了云原生计算基金会（CNCF），起初 CNCF 对云原生（Cloud Native）的定义包含以下三个方面：\n 应用容器化 面向微服务架构 应用支持容器的编排调度  重定义 到了 2018 年，随着近几年来云原生生态的不断壮大，所有主流云计算供应商都加入了该基金会，且从 Cloud Native Landscape 中可以看出云原生有意蚕食原先非云原生应用的部分。CNCF 基金会中的会员以及容纳的项目越来越多，该定义已经限制了云原生生态的发展，CNCF 为云原生进行了重新定位。\n以下是 CNCF 对云原生的重新定义（中英对照）：\n Cloud native technologies empower organizations to build and run scalable applications in modern, dynamic environments such as public, private, and hybrid clouds. Containers, service meshes, microservices, immutable infrastructure, and declarative APIs exemplify this approach.\n 云原生技术有利于各组织在公有云、私有云和混合云等新型动态环境中，构建和运行可弹性扩展的应用。云原生的代表技术包括容器、服务网格、微服务、不可变基础设施和声明式 API。\n These techniques enable loosely coupled systems that are resilient, manageable, and observable. Combined with robust automation, they allow engineers to make high-impact changes frequently and predictably with minimal toil.\n 这些技术能够构建容错性好、易于管理和便于观察的松耦合系统。结合可靠的自动化手段，云原生技术使工程师能够轻松地对系统作出频繁和可预测的重大变更。\n The Cloud Native Computing Foundation seeks to drive adoption of this paradigm by fostering and sustaining an ecosystem of open source, vendor-neutral projects. We democratize state-of-the-art patterns to make these innovations accessible for everyone.\n 云原生计算基金会（CNCF）致力于培育和维护一个厂商中立的开源生态系统，来推广云原生技术。我们通过将最前沿的模式民主化，让这些创新为大众所用。\n总结 关于什么是云原生的争论还在进行中，在笔者看来云原生是一种行为方式和设计理念，究其本质，凡是能够提高云上资源利用率和应用交付效率的行为或方式都是云原生的。云计算的发展史就是一部云原生化的历史。Kubernetes 开启了云原生的序幕，服务网格 Istio 的出现，引领了后 Kubernetes 时代的微服务，serverless 的再次兴起，使得云原生从基础设施层不断向应用架构层挺进，我们正处于一个云原生的新时代。\n参考  CNCF Cloud Native Definition v1.0 - github.com 云原生关乎文化，而不是容器 - cloudnative.to  ","date":1651532400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"5047bf6f76be35dc25ed56190a88a966","permalink":"https://lib.jimmysong.io/cloud-native-handbook/intro/what-is-cloud-native/","publishdate":"2022-05-03T00:00:00+01:00","relpermalink":"/cloud-native-handbook/intro/what-is-cloud-native/","section":"cloud-native-handbook","summary":"云原生（Cloud Native）这个词汇由来已久，以致于何时出现已无据可考。云原生开始大规模出现在受众视线中，与 Pivotal 提出的云原生应用的理念有着莫大的关系。我们现在谈到云原生，更多的指的是一种文化，而不具","tags":["云原生"],"title":"什么是云原生？","type":"book"},{"authors":null,"categories":["云原生"],"content":"CNCF，全称Cloud Native Computing Foundation（云原生计算基金会），成立于 2015 年7月21日（于美国波特兰OSCON 2015上宣布），其最初的口号是坚持和整合开源技术来让编排容器作为微服务架构的一部分，其作为致力于云原生应用推广和普及的一支重要力量，不论您是云原生应用的开发者、管理者还是研究人员都有必要了解。\nCNCF作为一个厂商中立的基金会，致力于Github上的快速成长的开源技术的推广，如Kubernetes、Prometheus、Envoy等，帮助开发人员更快更好的构建出色的产品。CNCF 维护了一个全景图项目，详见 GitHub。\n关于CNCF的使命与组织方式请参考CNCF章程，概括的讲CNCF的使命包括以下三点：\n 容器化包装。 通过中心编排系统的动态资源管理。 面向微服务。  CNCF这个角色的作用是推广技术，形成社区，开源项目管理与推进生态系统健康发展。\n另外CNCF组织由以下部分组成：\n 会员：白金、金牌、银牌、最终用户、学术和非赢利成员，不同级别的会员在治理委员会中的投票权不同。 理事会：负责事务管理 TOC（技术监督委员会）：技术管理 最终用户社区：推动CNCF技术的采纳并选举最终用户技术咨询委员会 最终用户技术咨询委员会：为最终用户会议或向理事会提供咨询 营销委员会：市场推广  CNCF项目成熟度分级与毕业条件 每个CNCF项目都需要有个成熟度等级，申请成为CNCF项目的时候需要确定项目的成熟度级别。\n成熟度级别（Maturity Level）包括以下三种：\n sandbox（初级） incubating（孵化中） graduated（毕业）  是否可以成为CNCF项目需要通过Technical Oversight Committee (技术监督委员会）简称TOC，投票采取fallback策略，即回退策略，先从最高级别（graduated）开始，如果2/3多数投票通过的话则确认为该级别，如果没通过的话，则进行下一低级别的投票，如果一直到inception级别都没得到2/3多数投票通过的话，则拒绝其进入CNCF项目。\n当前所有的CNCF项目可以访问https://www.cncf.io/projects/ 。\n项目所达到相应成熟度需要满足的条件和投票机制见下图：\n   CNCF项目成熟度级别  TOC（技术监督委员会） TOC（Technical Oversight Committee）作为CNCF中的一个重要组织，它的作用是：\n 定义和维护技术视野 审批新项目加入组织，为项目设定概念架构 接受最终用户的反馈并映射到项目中 调整组件间的访问接口，协调组件之间兼容性  TOC成员通过选举产生，见选举时间表。\n参考 CNCF TOC：https://github.com/cncf/toc\nCNCF Ambassador CNCF Ambassador（CNCF 大使），人员名单详见 https://www.cncf.io/people/ambassadors/，笔者很荣幸作为第二位成为 CNCF Ambassador 的中国人。\n如何成为 CNCF Ambassador 可以通过以下方式成为 CNCF Ambassador：\n 成为 CNCF 会员或对成为某个 CNCF 的项目的贡献者 以 contributor、blogger、演讲者等身份参与 CNCF 社区项目 在社区中演讲或撰写博客 主持云原生社区 meetup  参考  AT\u0026amp;T, Box, Cisco, Cloud Foundry Foundation, CoreOS, Cycle Computing, Docker, eBay, Goldman Sachs, Google, Huawei, IBM, Intel, Joyent, Kismatic, Mesosphere, Red Hat, Switch SUPERNAP, Twitter, Univa, VMware and Weaveworks join new effort to build and maintain cloud native distributed systems - cncf.io  ","date":1651532400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"9a60e6dfc2f093b35fff57d5e8d21580","permalink":"https://lib.jimmysong.io/cloud-native-handbook/community/cncf/","publishdate":"2022-05-03T00:00:00+01:00","relpermalink":"/cloud-native-handbook/community/cncf/","section":"cloud-native-handbook","summary":"CNCF，全称Cloud Native Computing Foundation（云原生计算基金会），成立于 2015 年7月21日（于美国波特兰OSCON 2015上宣布），其最初的口号是坚持和整合开源技术来让编排容器作为微服务架构的一部分，","tags":["云原生"],"title":"云原生计算基金会（CNCF）","type":"book"},{"authors":null,"categories":["可观测性"],"content":"软件开发的模式又一次改变了。开源软件和公有云供应商已经从根本上改变了我们构建和部署软件的方式。有了开源软件，我们的应用不再需要从头开始编码。而通过使用公有云供应商，我们不再需要配置服务器或连接网络设备。所有这些都意味着，你可以在短短几天甚至几小时内从头开始构建和部署一个应用程序。\n但是，仅仅因为部署新的应用程序很容易，并不意味着操作和维护它们也变得更容易。随着应用程序变得更加复杂，更加异质，最重要的是，更加分布式，看清大局，以及准确地指出问题发生的地方，变得更加困难。\n但有些事情并没有改变：作为开发者和运维，我们仍然需要能够从用户的角度理解应用程序的性能，我们仍然需要对用户的事务有一个端到端的看法。我们也仍然需要衡量和说明在处理这些事务的过程中资源是如何被消耗的。也就是说，我们仍然需要可观测性。\n但是，尽管对可观测性的需求没有改变，我们实施可观测性解决方案的方式必须改变。本报告详细介绍了关于可观测性的传统思维方式对现代应用的不足：继续将可观测性作为一系列工具（尤其是作为 “三大支柱”）来实施，几乎不可能以可靠的方式运行现代应用。\nOpenTelemetry 通过提供一种综合的方法来收集有关应用程序行为和性能的数据，包括指标、日志和跟踪，来解决这些挑战。正如本报告所解释的，OpenTelemetry 是一种生成和收集这些数据的新方法，这种方法是为使用开源组件构建的云原生应用而设计的。\n事实上，OpenTelemetry 是专门为这些开源组件设计的。与供应商特定的仪表不同，OpenTelemetry 可以直接嵌入到开放源代码中。这意味着开源库的作者可以利用他们的专业知识来添加高质量的仪表，而不需要在他们的项目中增加任何解决方案或供应商特定的代码。\nOpenTelemetry 对应用程序所有者也有好处。在过去，遥测的产生方式与它的评估、存储和分析方式相联系。这意味着，选择使用哪种可观测性解决方案必须在开发过程的早期完成，并且在之后很难改变。OpenTelemetry（就像它的前身 OpenTracing 和 OpenCensus 一样）将你的应用程序产生遥测的方式与遥测的分析方式相分离。\n因此，当我们采用新技术时，我们往往没有考虑到该技术将如何被使用，特别是它将如何被不同角色的人使用。本报告描述了 OpenTelemetry 如何满足库作者、应用程序拥有者、运维和响应者的需求，以及它如何使这些角色中的每个人都能独立工作 —— 即自动做出关键决定并进行有效协作。\n也许最重要的是，OpenTelemetry 为应用程序所有者和操作者提供了灵活性，使他们能够选择最适合其应用程序和组织需求的可观测性解决方案，包括需要多种工具的情况。它还包括这些需求随时间变化的情况，以及需要新的工具来满足这些需求的情况，因为随着技术的成熟和组织的发展，可能会出现这种情况。\n虽然用户不需要致力于可观测性解决方案，但他们确实需要投资于生成遥测数据的一致方式。本报告展示了 OpenTelemetry 是如何被设计成范围狭窄、可扩展，而且最重要的是稳定的：如果你被要求进行这种投资，这正是你所期望的。\n与以往任何时候相比，可观测性都不能成为事后的想法。没有一个有效的可观测性解决方案的风险，也就是说，无法了解你的应用程序中正在发生的事情的风险是非常高的，而对可观测性采取错误的方法的成本会造成你的组织多年来一直在偿还的债务。无论是长时间停机，还是开发团队被无休止的供应商迁移所困扰，你的组织的成功都取决于像 OpenTelemetry 这样的集成和开放的方法。\n本报告提供了关于在你的组织中采用和管理 OpenTelemetry 的实用建议。它将使你开始走向成功的可观测性实践的道路，并释放出云原生和开源技术的许多真正的好处：你不仅能够快速建立和部署应用程序，而且能够可靠和自信地运行它们。\n—— Daniel “Spoons” Spoonhower Lightstep\n","date":1643990400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"440765fde0de973d8f49a3e80359e2e2","permalink":"https://lib.jimmysong.io/opentelemetry-obervability/foreword/","publishdate":"2022-02-05T00:00:00+08:00","relpermalink":"/opentelemetry-obervability/foreword/","section":"opentelemetry-obervability","summary":"前言","tags":["OpenTelemetry"],"title":"前言","type":"book"},{"authors":null,"categories":["文化"],"content":"代码审查的主要目的是确保逐步改善 Google 代码库的整体健康状况。代码审查的所有工具和流程都是为此而设计的。\n为了实现此目标，必须做出一系列权衡。\n首先，开发人员必须能够对任务进行改进。如果开发者从未向代码库提交过代码，那么代码库的改进也就无从谈起。此外，如果审核人员对代码吹毛求疵，那么开发人员以后也很难再做出改进。\n另外，审查者有责任确保随着时间的推移，CL 的质量不会使代码库的整体健康状况下降。这可能很棘手，因为通常情况下，代码库健康状况会随着时间的而下降，特别是在对团队有严格的时间要求时，团队往往会采取捷径来达成他们的目标。\n此外，审查者应对正在审核的代码负责并拥有所有权。审查者希望确保代码库保持一致、可维护及 Code Review 要点中所提及的所有其他内容。\n因此，我们将以下规则作为 Code Review 中期望的标准：\n一般来说，审核人员应该倾向于批准 CL，只要 CL 确实可以提高系统的整体代码健康状态，即使 CL 并不完美。\n这是所有 Code Review 指南中的高级原则。\n当然，也有一些限制。例如，如果 CL 添加了审查者认为系统中不需要的功能，那么即使代码设计良好，审查者依然可以拒绝批准它。\n此处有一个关键点就是没有“完美”的代码，只有更好的代码。审查者不该要求开发者在批准程序前仔细清理、润色 CL 每个角落。相反，审查者应该在变更的重要性与取得进展之间取得平衡。审查者不应该追求完美，而应是追求持续改进。不要因为一个 CL不是“完美的”，就将可以提高系统的可维护性、可读性和可理解性的 CL 延迟数天或数周才批准。\n审核者应该随时在可以改善的地方留下审核评论，但如果评论不是很重要，请在评论语句前加上“Nit：”之类的内容，让开发者知道这条评论是用来指出可以润色的地方，而他们可以选择是否忽略。\n注意：本文档中没有任何内容证明检查 CL 肯定会使系统的整体代码健康状况恶化。您会做这种事情应该只有在紧急情况时。\n指导 代码审查具有向开发人员传授语言、框架或通用软件设计原则新内容的重要功能。留下评论可以帮助开发人员学习新东西，这总归是很好的。分享知识是随着长年累月改善系统代码健康状况的一部分。请记住，如果您的评论纯粹是教育性的，且对于本文档中描述的标准并不重要，请在其前面添加“Nit：”或以其他方式表明作者不必在此 CL 中解决它。\n原则  基于技术事实和数据否决意见和个人偏好。 关于代码风格问题，风格指南是绝对权威。任何不在风格指南中的纯粹风格点（例如空白等）都是个人偏好的问题。代码风格应该与风格指南中的一致。如果没有以前的风格，请接受作者的风格。 软件设计方面几乎不是纯粹的风格或个人偏好问题。软件设计基于基本原则且应该权衡这些原则，而不仅仅是个人意见。有时候会有多种有效的选择。如果作者可以证明（通过数据或基于可靠的工程原理）该方法同样有效，那么审查者应该接受作者的偏好。否则，就要取决于软件设计的标准原则。 如果没有其他适用规则，则审查者可以要求作者与当前代码库中的内容保持一致，只要不恶化系统的整体代码健康状况即可。  解决冲突 如果在代码审查过程中有任何冲突，第一步应该始终是开发人员和审查者根据本文档中的 CL 开发者指南和审查者指南达成共识。\n当达成共识变得特别困难时，审阅者和开发者可以进行面对面的会议，或者有 VC 参与调停，而不仅仅是试着通过代码审查评论来解决冲突。 （但是，如果您这样做了，请确保在 CL 的评论中记录讨论结果，以供将来的读者使用。）\n如果这样还不能解决问题，那么解决该问题最常用方法是将问题升级。通常是将问题升级为更广泛的团队讨论，有一个 TL 权衡，要求维护人员对代码作出决定，或要求工程经理的帮助。 不要因为 CL 的开发者和审查者不能达成一致，就让 CL 在那里卡壳。\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"99a8b946074829942d1bb98f8d6de638","permalink":"https://lib.jimmysong.io/eng-practices/review/reviewer/standard/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/eng-practices/review/reviewer/standard/","section":"eng-practices","summary":"代码审查的主要目的是确保逐步改善 Google 代码库的整体健康状况。代码审查的所有工具和流程都是为此而设计的。 为了实现此目标，必须做出一系列权衡。 首先，开发人员必须能够对任务进行改进。如果开发者从未向代码库提交过","tags":null,"title":"Code Review 标准","type":"book"},{"authors":null,"categories":["文化"],"content":"CL 描述是进行了哪些更改以及为何更改的公开记录。CL 将作为版本控制系统中的永久记录，可能会在长时期内被除审查者之外的数百人阅读。\n开发者将来会根据描述搜索您的 CL。有人可能会仅凭有关联性的微弱印象，但没有更多具体细节的情况下，来查找你的改动。如果所有重要信息都在代码而不是描述中，那么会让他们更加难以找到你的 CL 。\n首行  正在做什么的简短摘要。 完整的句子，使用祈使句。 后面跟一个空行。  CL 描述的第一行应该是关于这个 CL 是做什么的简短摘要，后面跟一个空白行。这是将来大多数的代码搜索者在浏览代码的版本控制历史时，最常被看到的内容，因此第一行应该提供足够的信息，以便他们不必阅读 CL 的整个描述就可以获得这个 CL 实际上是做了什么的信息。\n按照传统，CL 描述的第一行应该是一个完整的句子，就好像是一个命令（一个命令句）。例如，“Delete the FizzBuzz RPC and replace it with the new system.”而不是“Deleting the FizzBuzz RPC and replacing it with the new system.“ 但是，您不必把其余的描述写成祈使句。\nBody 是信息丰富的 其余描述应该是提供信息的。可能包括对正在解决的问题的简要描述，以及为什么这是最好的方法。如果方法有任何缺点，应该提到它们。如果相关，请包括背景信息，例如错误编号，基准测试结果以及设计文档的链接。\n即使是小型 CL 也需要注意细节。在 CL 描述中提供上下文以供参照。\n糟糕的 CL 描述 “Fix bug ”是一个不充分的 CL 描述。什么 bug？你做了什么修复？其他类似的不良描述包括：\n “Fix build.” “Add patch.” “Moving code from A to B.” “Phase 1.” “Add convenience functions.” “kill weird URLs.”  其中一些是真正的 CL 描述。他们的作者可能认为自己提供了有用的信息，却没有达到 CL 描述的目的。\n好的 CL 描述 以下是一些很好的描述示例。\n功能更新  rpc：删除 RPC 服务器消息 freelist 上的大小限制。\n像 FIzzBuzz 这样的服务器有非常大的消息，并且可以从重用中受益。增大 freelist，添加一个 goroutine，缓慢释放 freelist 条目，以便空闲服务器最终释放所有 freelist 条目。\n 前几个词描述了CL实际上做了什么。其余的描述讨论了正在解决的问题，为什么这是一个很好的解决方案，以及有关具体实现的更多信息。\n重构  Construct a Task with a TimeKeeper to use its TimeStr and Now methods.\nAdd a Now method to Task, so the borglet() getter method can be removed (which was only used by OOMCandidate to call borglet’s Now method). This replaces the methods on Borglet that delegate to a TimeKeeper.\nAllowing Tasks to supply Now is a step toward eliminating the dependency on Borglet. Eventually, collaborators that depend on getting Now from the Task should be changed to use a TimeKeeper directly, but this has been an accommodation to refactoring in small steps.\nContinuing the long-range goal of refactoring the Borglet Hierarchy.\n 第一行描述了 CL 的作用以及改变。其余的描述讨论了具体的实现，CL 的背景，解决方案并不理想，以及未来的可能方向。它还解释了为什么正在进行此更改。\n需要上下文的 小 CL  Create a Python3 build rule for status.py.\nThis allows consumers who are already using this as in Python3 to depend on a rule that is next to the original status build rule instead of somewhere in their own tree. It encourages new consumers to use Python3 if they can, instead of Python2, and significantly simplifies some automated build file refactoring tools being worked on currently.\n 第一句话描述实际做了什么。其余的描述解释了为什么正在进行更改并为审查者提供了大量背景信息。\n在提交 CL 前审查描述 CL 在审查期间可能会发生重大变更。在提交 CL 之前检查 CL 描述是必要的，以确保描述仍然反映了 CL 的作用。\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"aa500f7e54b31e45a6dd638393b28812","permalink":"https://lib.jimmysong.io/eng-practices/review/developer/cl-descriptions/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/eng-practices/review/developer/cl-descriptions/","section":"eng-practices","summary":"CL 描述是进行了哪些更改以及为何更改的公开记录。CL 将作为版本控制系统中的永久记录，可能会在长时期内被除审查者之外的数百人阅读。 开发者将来会根据描述搜索您的 CL。有人可能会仅凭有关联性的微弱印象，但没有","tags":null,"title":"写好 CL 描述","type":"book"},{"authors":null,"categories":["网络"],"content":"标签 标签（Label）是一种通用的、灵活的和高度可扩展的方式，可以用来处理大量资源，因为我们可以用它来对事物任意分组和创建集合。每当需要描述、解决或选择某物时，它都是基于标签完成的：\n 端点 被分配了从容器运行时、编排系统或其他来源派生的标签。 网络策略 根据标签选择允许通信的 端点对，策略本身也由标签标识。  什么是标签？ 标签是一对由 key 和 value 组成的字符串。可以将标签格式化为具有 key=value 的单个字符串。key 部分是强制性的，并且必须是唯一的。这通常是通过使用反向域名概念来实现的，例如 io.cilium.mykey=myvalue。value 部分是可选的，可以省略，例如 io.cilium.mykey.\n键名通常应由字符集组成 [a-z0-9-.]。\n当使用标签选择资源时，键和值都必须匹配，例如，当一个策略应该应用于所有带有标签 my.corp.foo 的端点时，标签 my.corp.foo=bar 不会与该选择器匹配。\n标签来源 标签可以来自各种来源。例如，端点 将通过本地容器运行时派生与容器关联的标签，以及与 Kubernetes 提供的 pod 关联的标签。由于这两个标签命名空间彼此不知道，这可能会导致标签键冲突。\n为了解决这种潜在的冲突，Cilium 在导入标签时为所有标签键添加前缀 source: 以指示标签的来源，例如 k8s:role=frontend、container:user=joe、k8s:role=backend。这意味着当您使用 docker run [...] -l foo=bar 运行 Docker 容器时，标签 container:foo=bar 将出现在代表容器的 Cilium 端点上。类似地，以 foo: bar 标签启动的 Kubernetes pod 将由与标签关联的 Cilium 端点表示 。为每个潜在来源分配一个唯一名称。当前支持以下标签源：\n container: 对于从本地容器运行时派生的标签 k8s: 对于从 Kubernetes 派生的标签 reserved: 有关特殊保留标签，请参阅 特殊标识。 unspec: 对于未指定来源的标签  当使用标签来识别其他资源时，可以包含源以将标签匹配限制为特定类型。如果未提供源，则标签源默认为 any:，将匹配所有标签，无论其来源如何。如果提供了来源，则选择和匹配标签的来源需要匹配。\n端点 Cilium 通过分配 IP 地址使应用程序容器在网络上可用。多个应用容器可以共享同一个 IP 地址；此模型的一个典型示例是 Kubernetes Pod。所有共享公共地址的应用程序容器都在 Cilium 所指的端点中分组在一起。\n分配单独的 IP 地址允许每个端点使用整个四层端口范围。这实质上允许在同一个集群节点上运行的多个应用程序容器都绑定到众所周知的端口，例如 80 不会引起任何冲突。\nCilium 的默认行为是为每个端点分配 IPv6 和 IPv4 地址。但是，可以将此行为配置为仅使用该 --enable-ipv4=false 选项分配 IPv6 地址。如果同时分配了 IPv6 和 IPv4 地址，则任一地址都可用于到达端点。相同的行为将适用于策略规则、负载均衡等。\n身份识别 出于识别目的，Cilium 为集群节点上的所有端点分配一个内部端点 ID。端点 ID 在单个集群节点的上下文中是唯一的。\n端点元数据 端点自动从与端点关联的应用程序容器中派生元数据。然后可以使用元数据来识别端点，以实现安全 / 策略、负载均衡和路由目的。\n元数据的来源取决于使用的编排系统和容器运行时。当前支持以下元数据检索机制：\n   系统 描述     Kubernetes Pod 标签（通过 Kubernetes API）   containerd（Docker） 容器标签（通过 Docker API）    元数据以 标签 的形式附加到端点。\n以下示例启动一个带有标签的容器，该标签 app=benchmark 随后与端点相关联。标签带有前缀， container: 表示标签是从容器运行时派生的。\n$ docker run --net cilium -d -l app=benchmark tgraf/netperf aaff7190f47d071325e7af06577f672beff64ccc91d2b53c42262635c063cf1c $ cilium endpoint list ENDPOINT POLICY IDENTITY LABELS (source:key [=value]) IPv6 IPv4 STATUS ENFORCEMENT 62006 Disabled 257 container:app=benchmark f00d::a00:20f:0:f236 10.15.116.202 ready 一个端点可以有来自多个源的元数据。例如使用 containerd 作为容器运行时的 Kubernetes 集群。端点将派生 Kubernetes pod 标签（以k8s:源前缀为前缀）和容器标签（以container: 源前缀为前缀）。\n身份 所有 端点 都分配了一个身份。身份用于端点之间的基本连接。在传统的网络术语中，这运行在三层。\n身份由 标签 标识，并被赋予一个集群范围的唯一标识符。端点被分配与端点的 安全相关标签 匹配的身份，即共享同一组 安全相关标签 的所有端点将共享相同的身份。此概念允许将策略实施扩展到大量端点，因为随着应用程序的扩展，许多单独的端点通常会共享同一组安全 标签。\n什么是身份？ 端点的身份是基于与派生到 端点 的 pod 或容器关联的 标签 派生的。当一个 pod 或容器启动时，Cilium 会根据容器运行时收到的事件创建一个 端点 来代表网络上的 pod 或容器。下一步，Cilium 将解析 端点 创建的身份。每当 Pod 或容器的 标签 发生变化时，都会重新确认身份并根据需要自动修改。\n安全相关标签 在派生 身份 时，并非所有与容器或 pod 关联的 标签 都有意义。标签可用于存储元数据，例如容器启动时的时间戳。Cilium 需要知道哪些标签是有意义的，知道在推导身份时需要考虑哪些标签。为此，用户需要指定有意义标签的字符串前缀列表。标准行为是包含所有以 id 为前缀开头的标签， 例如，id.service1、id.service2、id.groupA.service44。启动代理时可以指定有意义的标签前缀列表。\n特殊身份 Cilium 管理的所有端点都将被分配一个身份。为了允许与不由 Cilium 管理的网络端点进行通信，存在特殊的身份来表示它们。特殊保留标识以 reserved: 字符串为前缀。\n   身份 数字 ID 描述     reserved:unknown 0 无法推导出身份。   reserved:host 1 本地主机。源自或指定到本地主机 IP 之一的任何流量。   reserved:world 2 集群外的任何网络端点。   reserved:unmanaged 3 不受 Cilium 管理的端点，例如在安装 Cilium 之前启动的 Kubernetes pod。   reserved:health 4 这是 Cilium 代理生成的健康检查流量。   reserved:init 5 尚未解析身份的端点被分配了初始身份。这代表了一个端点的阶段，在该阶段中，派生安全身份所需的一些元数据仍然缺失。这通常是引导阶段的情况。仅当端点的标签在创建时未知时才分配初始化标识。Docker 插件可能就是这种情况。   reserved:remote-node 6 所有远程集群主机的集合。源自或指定到任何连接集群中任何主机的 IP 之一的任何流量，而不是本地节点。   reserved:kube-apiserver 7 具有为 kube-apiserver 运行的后端的远程节点。    提示\n  Cilium 曾经在 reserved:host 身份中同时包含本地和所有远程主机。除非使用最近的默认 ConfigMap，否则这仍然是默认选项。可以通过 enable-remote-node-identity 选项启用远程节点身份。   知名身份 以下是 Cilium 自动识别的知名身份列表，Cilium 将分发安全身份，而无需联系任何外部依赖项，例如 kvstore。这样做的目的是允许引导 Cilium 并通过集群中的策略强制实现网络连接，以实现基本服务，而无需任何依赖项。\n   部署 命名空间 服务账户 集群名称 数字 ID 标签     kube-dns kube-system kube-dns cilium-cluster 102 k8s-app=kube-dns   kube-dns（EKS） kube-system kube-dns cilium-cluster 103 k8s-app=kube-dns,eks.amazonaws.com/component=kube-dns   core-dns kube-system coredns cilium-cluster 104 k8s-app=kube-dns   core-dns（EKS） kube-system coredns cilium-cluster 106 k8s-app=kube-dns,eks.amazonaws.com/component=coredns   cilium-operator cilium-namspace cilium-operator cilium-cluster 105 name=cilium-operator,io.cilium/app=operator    \n  如果 cilium-cluster 未定义该 cluster-name 选项，则默认值将设置为 default。   集群中的身份管理 身份在整个集群中都是有效的，这意味着如果在多个集群节点上启动了多个 pod 或容器，如果它们共享身份相关标签，那么它们都将解析并共享同一个身份。这需要集群节点之间的协调。\n   集群中的身份管理示意图  解析端点身份的操作是在分布式键值存储的帮助下执行的，如果之前没有看到以下值，则允许以生成新的唯一标识符的形式执行原子操作。这允许每个集群节点创建与身份相关的标签子集，然后查询键值存储以派生身份。根据之前是否查询过这组标签，要么创建一个新的身份，要么返回初始查询的身份。\n节点 Cilium 将节点称为集群的单个成员。每个节点都必须运行 cilium-agent 并且将以自主的方式运行。为了简单和规模化，在不同节点上运行的 Cilium 代理之间的状态同步保持在最低限度。它仅通过键值存储或数据包元数据发生。\n节点地址 Cilium 会自动检测节点的 IPv4 和 IPv6 地址。当 cilium-agent 启动时打印出检测到的节点地址：\nLocal node-name: worker0 Node-IPv6: f00d::ac10:14:0:1 External-Node IPv4: 172.16.0.20 Internal-Node IPv4: 10.200.28.238 ","date":1655438400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"d2f497e78b28b9c666195e8a29d74fb1","permalink":"https://lib.jimmysong.io/cilium-handbook/concepts/terminology/","publishdate":"2022-06-17T12:00:00+08:00","relpermalink":"/cilium-handbook/concepts/terminology/","section":"cilium-handbook","summary":"标签 标签（Label）是一种通用的、灵活的和高度可扩展的方式，可以用来处理大量资源，因为我们可以用它来对事物任意分组和创建集合。每当需要描述、解决或选择某物时，它都是基于标签完成的： 端点 被分配了从容器","tags":["Cilium"],"title":"Cilium 术语说明","type":"book"},{"authors":null,"categories":["网络"],"content":"IP 地址管理（IPAM）负责分配和管理由 Cilium 管理的网络端点（容器和其他）使用的 IP 地址。Cilium 支持以下各种 IPAM 模式，以满足不同用户的需求。\n 集群范围（默认） Kubernetes 主机范围 Azure IPAM AWS ENI Google Kubernetes Engine CRD 支持  集群范围 集群范围 IPAM 模式将 PodCIDR 分配给每个节点，并使用每个节点上的主机范围分配器分配 IP。因此它类似于 Kubernetes 主机范围模式。区别在于 Kubernetes 不是通过 Kubernetes v1.Node资源分配每个节点的 PodCIDR，而是 Cilium Operator 通过 v2.CiliumNode 资源管理每个节点的 PodCIDR。这种模式的优点是它不依赖于 Kubernetes 被配置为分发每个节点的 PodCIDR。\n架构    集群范围模式 IPAM 架构图  如果无法将 Kubernetes 配置为分发 PodCIDR 或需要更多控制，这将非常有用。\n在这种模式下，Cilium 代理将在启动时等待，直到 PodCIDRs 范围通过 Cilium 节点 v2.CiliumNode 对象，通过 v2.CiliumNode 中设置的资源字段为所有启用的地址族提供：\n   字段 描述     Spec.IPAM.PodCIDRs IPv4 和/或 IPv6 PodCIDR 范围    集群范围配置 有关如何在 Cilium 中启用此模式的实用教程，请参阅 由 Cilium 集群池 IPAM 支持的 CRD。\n故障排除 查找分配错误 检查 Error 字段中的 Status.Operator 字段：\nkubectl get ciliumnodes -o jsonpath=\u0026#39;{range .items[*]}{.metadata.name}{\u0026#34;\\t\u0026#34;}{.status.operator.error}{\u0026#34;\\n\u0026#34;}{end Kubernetes 主机范围 Kubernetes 主机范围 IPAM 模式启用 ipam: kubernetes 并将地址分配委托给集群中的每个单独节点。Kubernetes 分配的 IP 超出了与每个节点关联的 PodCIDR 范围。\n   Kubernetes 主机范围模式 IPAM 架构图  在这种模式下，Cilium 代理将在启动时等待，直到 PodCIDR 范围通过 Kubernetes v1.Node 对象通过以下方法之一为所有启用的地址家族提供：\n通过 v1.Node 资源字段\n   字段 描述     spec.podCIDRs IPv4 和 / 或 IPv6 PodCIDR 范围   spec.podCIDR IPv4 或 IPv6 PodCIDR 范围    提示\n  kube-controller-manager 使用 --allocate-node-cidrs 标志运行 kube-controller-manager 以指示 Kubernetes 应该分配的 PodCIDR 范围。   通过 v1.Node 注释\n   注解 描述     io.cilium.network.ipv4-pod-cidr IPv4 PodCIDR 范围   io.cilium.network.ipv6-pod-cidr IPv6 PodCIDR 范围   io.cilium.network.ipv4-cilium-host cilium 主机接口的 IPv4 地址   io.cilium.network.ipv6-cilium-host cilium 主机接口的 IPv6 地址   io.cilium.network.ipv4-health-ip cilium-health 端点的 IPv4 地址   io.cilium.network.ipv6-health-ip cilium-health 端点的 IPv6 地址    \n  基于注解的机制主要与旧的 Kubernetes 版本结合使用，这些版本尚不支持spec.podCIDRs但同时支持 IPv4 和 IPv6。   主机范围配置 存在以下 ConfigMap 选项来配置 Kubernetes 主机范围：\n ipam: kubernetes：启用 Kubernetes IPAM 模式。启用此选项将自动启用k8s-require-ipv4-pod-cidr 如何 enable-ipv4 是 true 和 k8s-require-ipv6-pod-cidr 如何 enable-ipv6 是 true。 k8s-require-ipv4-pod-cidr: true：指示 Cilium 代理等待，直到 IPv4 PodCIDR 通过 Kubernetes 节点资源可用。 k8s-require-ipv6-pod-cidr: true：指示 Cilium 代理等待，直到 IPv6 PodCIDR 通过 Kubernetes 节点资源可用。  使用 helm 之前的选项可以定义为：\n ipam: kubernetes：--set ipam.mode=kubernetes k8s-require-ipv4-pod-cidr: true：--set k8s.requireIPv4PodCIDR=true， 仅适用于 --set ipam.mode=kubernetes k8s-require-ipv6-pod-cidr: true：--set k8s.requireIPv6PodCIDR=true，仅适用于 --set ipam.mode=kubernetes  CRD 支持 CRD 支持的 IPAM 模式提供了一个可扩展的接口，以通过 Kubernetes 自定义资源定义 (CRD) 控制 IP 地址管理。这允许将 IPAM 委托给外部运营商或使其用户可配置每个节点。\n架构    启用此模式后，每个 Cilium 代理将开始监视 ciliumnodes.cilium.io名称与运行代理的 Kubernetes 节点匹配的 Kubernetes 自定义资源。\n每当更新自定义资源时，每个节点的分配池都会更新为 spec.ipam.available 字段中列出的所有地址。移除当前分配的 IP 后，该 IP 将继续使用，但在释放后无法重新分配。\n在分配池中分配 IP 后，将 IP 添加到 status.ipam.inuse 字段中。\n提示\n  节点状态更新被限制为最多每 15 秒运行一次。因此，如果同时调度多个 Pod，状态部分的更新可能会滞后。   CRD 支持配置 通过在 cilium-config ConfigMap 中设置 ipam: crd 或指定选项 --ipam=crd，可以启用 CRD 支持的 IPAM 模式。启用后，代理将等待与 Kubernetes 节点名称相匹配的 CiliumNode 自定义资源变得可用，并且至少有一个 IP 地址被列为可用。当连接性健康检查被启用时，必须有至少两个 IP 地址可用。\n在等待期间，代理将打印以下日志消息：\nWaiting for initial IP to become available in \u0026#39;\u0026lt;node-name\u0026gt;\u0026#39; custom resource 有关如何使用 Cilium 启用 CRD IPAM 模式的实用教程，请参阅 CRD 支持的 IPAM部分。\n权限 为了使自定义资源发挥作用，需要以下额外权限。使用标准 Cilium 部署工件时会自动授予这些权限：\napiVersion:rbac.authorization.k8s.io/v1kind:ClusterRolemetadata:name:ciliumrules:- apiGroups:- cilium.ioresources:- ciliumnodes- ciliumnodes/statusverbs:- \u0026#39;*\u0026#39;CRD 定义 CilumNode 自定义资源以标准 Kubernetes 资源为模型，分为一个 spec 和 status 部分：\ntype CiliumNode struct { [...] // Spec is the specification of the node  Spec NodeSpec `json:\u0026#34;spec\u0026#34;` // Status it the status of the node  Status NodeStatus `json:\u0026#34;status\u0026#34;` } IPAM 规范 该 spec 部分嵌入了一个 IPAM 特定字段，该字段允许定义节点可用于分配的所有 IP 的列表：\n// AllocationMap is a map of allocated IPs indexed by IP type AllocationMap map[string]AllocationIP // NodeSpec is the configuration specific to a node type NodeSpec struct { // [...]  // IPAM is the address management specification. This section can be  // populated by a user or it can be automatically populated by an IPAM  // operator  //  // +optional  IPAM IPAMSpec `json:\u0026#34;ipam,omitempty\u0026#34;` } // IPAMSpec is the IPAM specification of the node type IPAMSpec struct { // Pool is the list of IPs available to the node for allocation. When  // an IP is used, the IP will remain on this list but will be added to  // Status.IPAM.InUse  //  // +optional  Pool AllocationMap `json:\u0026#34;pool,omitempty\u0026#34;` } // AllocationIP is an IP available for allocation or already allocated type AllocationIP struct { // Owner is the owner of the IP, this field is set if the IP has been  // allocated. It will be set to the pod name or another identifier  // representing the usage of the IP  //  // The owner field is left blank for an entry in Spec.IPAM.Pool  // and filled out as the IP is used and also added to  // Status.IPAM.InUse.  //  // +optional  Owner string `json:\u0026#34;owner,omitempty\u0026#34;` // Resource is set for both available and allocated IPs, it represents  // what resource the IP is associated with, e.g. in combination with  // AWS ENI, this will refer to the ID …","date":1655438400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"2ab52cffaca15277c293d32dd50d62a3","permalink":"https://lib.jimmysong.io/cilium-handbook/networking/ipam/","publishdate":"2022-06-17T12:00:00+08:00","relpermalink":"/cilium-handbook/networking/ipam/","section":"cilium-handbook","summary":"IP 地址管理（IPAM）负责分配和管理由 Cilium 管理的网络端点（容器和其他）使用的 IP 地址。Cilium 支持以下各种 IPAM 模式，以满足不同用户的需求。 集群范围（默认） Kubernetes 主机范围 Azure IPAM AWS ENI Google Kubernetes Engine CRD 支持 集群范围 集群范围 IPAM","tags":["Cilium"],"title":"IP 地址管理（IPAM）","type":"book"},{"authors":null,"categories":["网络"],"content":"部署 标准 Cilium Kubernetes 部署的配置包括几个 Kubernetes 资源：\n DaemonSet  资源：描述部署到每个 Kubernetes 节点的 Cilium pod 。这个 pod 运行 cilium-agent 和相关的守护进程。这个 DaemonSet 的配置包括指示 Cilium docker 容器的确切版本（例如 v1.0.0）的镜像标签和传递给 cilium-agent 的命令行选项。 资源：描述传递给 cilium-agent 的ConfigMap 常用配置值，例如 kvstore 端点和凭据、启用/禁用调试模式等。 ServiceAccount、ClusterRole 和 ClusterRoleBindings 资源：当启用 Kubernetes RBAC 时，cilium-agent` 用于访问 Kubernetes API 服务器的身份和权限。 资源：如果 Secret 需要，描述用于访问 etcd kvstore 的凭据。  现有 Pod 的联网 如果在部署 Cilium 之前 pod 已经在运行 DaemonSet，这些 pod 仍将根据 CNI 配置使用以前的网络插件连接。一个典型的例子是默认运行在 kube-system 命名空间中的 kube-dns 服务。\n改变这种现有 pod 的网络的一个简单方法是依靠 Kubernetes 在 Deployment 中的 pod 被删除时自动重新启动的事实，所以我们可以简单地删除原来的 kube-dns pod，紧接着启动的替换 pod 将由 Cilium 管理网络。在生产部署中，这个步骤可以作为 kube-dns pod 的滚动更新来执行，以避免 DNS 服务的停机。\n$ kubectl --namespace kube-system delete pods -l k8s-app=kube-dns pod \u0026#34;kube-dns-268032401-t57r2\u0026#34; deleted 运行 kubectl get pods 将显示 Kubernetes 启动了一组新的 kube-dns pod，同时终止了旧的 pod：\n$ kubectl --namespace kube-system get pods NAME READY STATUS RESTARTS AGE cilium-5074s 1/1 Running 0 58m kube-addon-manager-minikube 1/1 Running 0 59m kube-dns-268032401-j0vml 3/3 Running 0 9s kube-dns-268032401-t57r2 3/3 Terminating 0 57m 默认允许本地主机的入口流量 Kubernetes 具有通过存活探针和就绪探针向用户指示其应用程序当前运行状况的功能。为了让 kubelet 对每个 pod 运行这些健康检查，默认情况下，Cilium 将始终允许从本地主机到每个 pod 的所有入口流量。\n","date":1655438400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"18b5d46ed67c0da360999da6d0ba6525","permalink":"https://lib.jimmysong.io/cilium-handbook/kubernetes/concepts/","publishdate":"2022-06-17T12:00:00+08:00","relpermalink":"/cilium-handbook/kubernetes/concepts/","section":"cilium-handbook","summary":"部署 标准 Cilium Kubernetes 部署的配置包括几个 Kubernetes 资源： DaemonSet 资源：描述部署到每个 Kubernetes 节点的 Cilium pod 。这个 pod 运行 cilium-agent 和相关的守护进程。这个 DaemonSet 的配置包括指示 Cilium docker 容器的确切版本（例如 v1.0.0）的镜像标签和传递给 cilium-agent 的命令行选项。 资源","tags":["Cilium"],"title":"概念","type":"book"},{"authors":null,"categories":["网络"],"content":"Kubernetes 等容器管理系统部署了一个网络模型，该模型为每个 pod（容器组）分配一个单独的 IP 地址。这确保了架构的简单性，避免了不必要的网络地址转换 （NAT），并为每个单独的容器提供了全范围的端口号以供使用。这种模型的逻辑结果是，根据集群的大小和 pod 的总数，网络层必须管理大量的 IP 地址。\n在传统上，安全策略基于 IP 地址过滤器。下面是一个简单的例子。如果所有具有标签 role=frontend 的 pod 应该被允许发起与所有具有标签 role=backend 的 pod 的连接，那么每个运行至少一个具有标签 role=backend 的 pod 的集群节点必须安装一个相应的过滤器，允许所有 role=frontend pod 的所有 IP 地址发起与所有本地 role=backend pod 的 IP 地址的连接。所有其他的连接请求都应该被拒绝。这可能看起来像这样。如果目标地址是 10.1.1.2，那么只有当源地址是下列之一时才允许连接 [10.1.2.2,10.1.2.3,20.4.9.1]。\n每次启动或停止带有 role=frontend 或 role=backend 标签的新 pod 时，必须更新运行任何此类 pod 的每个集群节点的规则，从允许的 IP 地址列表中添加或删除相应的 IP 地址。在大型分布式应用中，这可能意味着每秒多次更新数以千计的集群节点，这取决于部署的 pod 的流失率。更糟糕的是，新的 role=frontend pod 的启动必须推迟到所有运行 role=backend pod 的服务器都被更新了新的安全规则之后，否则来自新 pod 的连接尝试可能会被错误地放弃。这使得它难以有效地扩展。\n为了避免这些可能限制扩展性和灵活性的复杂情况，Cilium 将安全与网络寻址分开。相反，安全是基于 pod 的身份，它是通过标签得出的。这个身份可以在 pod 之间共享。这意味着，当第一个 role=frontend pod 启动时，Cilium 会给该 pod 分配一个身份，然后允许它与 role=backend pod 的身份发起连接。随后启动额外的 role=frontend pod 只需要通过键值存储来解决这个身份，不需要在任何承载 role=backend pod 的集群节点上执行任何操作。一个新的 pod 的启动必须只延迟到 pod 的身份被解决，这比更新所有其他集群节点上的安全规则要简单得多。\n   基于身份的安全示意图  ","date":1655438400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"bdc8ad4c7b2e7a5a7ac5f38bd3239d98","permalink":"https://lib.jimmysong.io/cilium-handbook/security/identity/","publishdate":"2022-06-17T12:00:00+08:00","relpermalink":"/cilium-handbook/security/identity/","section":"cilium-handbook","summary":"Kubernetes 等容器管理系统部署了一个网络模型，该模型为每个 pod（容器组）分配一个单独的 IP 地址。这确保了架构的简单性，避免了不必要的网络地址转换 （NAT），并为每个单独的容器提供了全范围的端口号以供使用。这种模","tags":["Cilium"],"title":"基于身份","type":"book"},{"authors":null,"categories":["网络"],"content":"虽然监控数据路径状态提供对数据路径状态的自省，但默认情况下它只会提供对三层/四层数据包事件的可视性。如果配置了 七层示例，则可以查看七层协议，但这需要编写每个选定端点的完整策略。 为了在不配置完整策略的情况下获得对应用程序的更多可视性，Cilium 提供了一种在与 Kubernetes 一起运行时通过注解来规定可视性的方法。\n可视性信息由注解中以逗号分隔的元组列表表示：\n\u0026lt;{Traffic Direction}/{L4 Port}/{L4 Protocol}/{L7 Protocol}\u0026gt;\n例如：\n\u0026lt;Egress/53/UDP/DNS\u0026gt;,\u0026lt;Egress/80/TCP/HTTP\u0026gt;  为此，你可以在 Kubernetes YAML 中或通过命令行提供注释，例如：\nkubectl annotate pod foo -n bar io.cilium.proxy-visibility=\u0026#34;\u0026lt;Egress/53/UDP/DNS\u0026gt;,\u0026lt;Egress/80/TCP/HTTP\u0026gt;\u0026#34; Cilium 将拾取 pod 已收到这些注释，并将透明地将流量重定向到代理，以便显示 cilium monitor 流量的输出被重定向到代理，例如：\n-\u0026gt; Request http from 1474 ([k8s:id=app2 k8s:io.kubernetes.pod.namespace=default k8s:appSecond=true k8s:io.cilium.k8s.policy.cluster=default k8s:io.cilium.k8s.policy.serviceaccount=app2-account k8s:zgroup=testapp]) to 244 ([k8s:io.cilium.k8s.policy.cluster=default k8s:io.cilium.k8s.policy.serviceaccount=app1-account k8s:io.kubernetes.pod.namespace=default k8s:zgroup=testapp k8s:id=app1]), identity 30162-\u0026gt;42462, verdict Forwarded GET http://app1-service/ =\u0026gt; 0 -\u0026gt; Response http to 1474 ([k8s:zgroup=testapp k8s:id=app2 k8s:io.kubernetes.pod.namespace=default k8s:appSecond=true k8s:io.cilium.k8s.policy.cluster=default k8s:io.cilium.k8s.policy.serviceaccount=app2-account]) from 244 ([k8s:io.cilium.k8s.policy.serviceaccount=app1-account k8s:io.kubernetes.pod.namespace=default k8s:zgroup=testapp k8s:id=app1 k8s:io.cilium.k8s.policy.cluster=default]), identity 30162-\u0026gt;42462, verdict Forwarded GET http://app1-service/ =\u0026gt; 200  您可以通过检查该 pod 的 Cilium 端点来检查可视性策略的状态，例如：\n$ kubectl get cep -n kube-system NAME ENDPOINT ID IDENTITY ID INGRESS ENFORCEMENT EGRESS ENFORCEMENT VISIBILITY POLICY ENDPOINT STATE IPV4 IPV6 coredns-7d7f5b7685-wvzwb 1959 104 false false ready 10.16.75.193 f00d::a10:0:0:2c77 $ $ kubectl annotate pod -n kube-system coredns-7d7f5b7685-wvzwb io.cilium.proxy-visibility=\u0026#34;\u0026lt;Egress/53/UDP/DNS\u0026gt;,\u0026lt;Egress/80/TCP/HTTP\u0026gt;\u0026#34; --overwrite pod/coredns-7d7f5b7685-wvzwb annotated $ $ kubectl get cep -n kube-system NAME ENDPOINT ID IDENTITY ID INGRESS ENFORCEMENT EGRESS ENFORCEMENT VISIBILITY POLICY ENDPOINT STATE IPV4 IPV6 coredns-7d7f5b7685-wvzwb 1959 104 false false OK ready 10.16.75.193 f00d::a10:0:0:2c7 故障排查 如果七层可视性未出现在 cilium monitor 或 Hubble 组件中，则值得仔细检查：\n  没有在注解中指定的方向应用强制策略 CiliumEndpoint 中的 “可视性策略” 列显示 OK。如果为空，则未配置注解；如果显示错误，则可视性注解存在问题。   以下示例故意错误配置注解，以证明当可视性注解无法实现时，pod 的 CiliumEndpoint 会出现错误：\n$ kubectl annotate pod -n kube-system coredns-7d7f5b7685-wvzwb io.cilium.proxy-visibility=\u0026#34;\u0026lt;Ingress/53/UDP/DNS\u0026gt;,\u0026lt;Egress/80/TCP/HTTP\u0026gt;\u0026#34; pod/coredns-7d7f5b7685-wvzwb annotated $ $ kubectl get cep -n kube-system NAME ENDPOINT ID IDENTITY ID INGRESS ENFORCEMENT EGRESS ENFORCEMENT VISIBILITY POLICY ENDPOINT STATE IPV4 IPV6 coredns-7d7f5b7685-wvzwb 1959 104 false false dns not allowed with direction Ingress ready 10.16.75.193 f00d::a10:0:0:2c77 限制  如果导入的规则选择了被注解的 pod，则可视性注解将不适用。 DNS 可视性仅在 egress 上可用。 不支持 Proxylib 解析器，包括 Kafka。要获得对这些协议的可视性，你必须创建一个允许所有七层流量的网络策略，方法是遵循 七层示例（Kafka）或 Envoy proxylib 扩展指南。此限制见 GitHub Issue 14072。  ","date":1655438400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"0d4f7798c95eec1bfd1d97dfc28baa94","permalink":"https://lib.jimmysong.io/cilium-handbook/policy/visibility/","publishdate":"2022-06-17T12:00:00+08:00","relpermalink":"/cilium-handbook/policy/visibility/","section":"cilium-handbook","summary":"虽然监控数据路径状态提供对数据路径状态的自省，但默认情况下它只会提供对三层/四层数据包事件的可视性。如果配置了 七层示例，则可以查看七层协议，但这需要编写每个选定端点的完整策略。 为了在不配置完整策略的情","tags":["Cilium"],"title":"七层可视性","type":"book"},{"authors":null,"categories":["网络"],"content":"端点到端点 首先，我们使用可选的七层出口和入口策略显示本地端点到端点的流程。随后是启用了套接字层强制的同一端点到端点流。为 TCP 流量启用套接字层实施后，启动连接的握手将遍历端点策略对象，直到 TCP 状态为 ESTABLISHED。然后在建立连接后，只需要七层策略对象。\n   端点到端点的流程  端点到出口 接下来，我们使用可选的 overlay 网络显示本地端点到出口。在可选的覆盖网络中，网络流量被转发到与 overlay 网络对应的 Linux 网络接口。在默认情况下，overlay 接口名为 cilium_vxlan。与上面类似，当启用套接字层强制并使用七层代理时，我们可以避免在端点和 TCP 流量的七层策略之间运行端点策略块。如果启用，可选的 L3 加密块将加密数据包。\n   端点到出口的流程  入口到端点 最后，我们还使用可选的 overlay 网络显示到本地端点的入口。与上述套接字层强制类似，可用于避免代理和端点套接字之间的一组策略遍历。如果数据包在接收时被加密，则首先将其解密，然后通过正常流程进行处理。\n   入口到端点流程  这样就完成了数据路径概述。更多 BPF 细节可以在 BPF 和 XDP 参考指南中找到。\n","date":1655438400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"f0ad28d10173073377a8778b9de7a58c","permalink":"https://lib.jimmysong.io/cilium-handbook/ebpf/lifeofapacket/","publishdate":"2022-06-17T12:00:00+08:00","relpermalink":"/cilium-handbook/ebpf/lifeofapacket/","section":"cilium-handbook","summary":"端点到端点 首先，我们使用可选的七层出口和入口策略显示本地端点到端点的流程。随后是启用了套接字层强制的同一端点到端点流。为 TCP 流量启用套接字层实施后，启动连接的握手将遍历端点策略对象，直到 TCP 状态为 ESTA","tags":["Cilium"],"title":"数据包流程","type":"book"},{"authors":null,"categories":null,"content":"现在我们将探索云原生应用架构的几个主要特征，和这些特征是如何解决我们前面提到的使用云原生应用架构的动机。\n12 因素应用 12 因素应用是一系列云原生应用架构的模式集合，最初由 Heroku 提出。这些模式可以用来说明什么样的应用才是云原生应用。它们关注速度、安全、通过声明式配置扩展、可横向扩展的无状态 / 无共享进程以及部署环境的整体松耦合。如 Cloud Foundry、Heroku 和 Amazon ElasticBeanstalk 都对部署 12 因素应用进行了专门的优化。\n在 12 因素的背景下，应用（或者叫 app）指的是独立可部署单元。组织中经常把一些互相协作的可部署单元称作一个应用。\n12 因素应用遵循以下模式：\n代码库\n每个可部署 app 在版本控制系统中都有一个独立的代码库，可以在不同的环境中部署多个实例。\n依赖\nApp 应该使用适当的工具（如 Maven、Bundler、NPM）来对依赖进行显式的声明，而不该在部署环境中隐式的实现依赖。\n配置\n配置或其他随发布环境（如部署、staging、生产）而变更的部分应当作为操作系统级的环境变量注入。\n后端服务\n后端服务，例如数据库、消息代理应视为附加资源，并在所有环境中同等看待。\n编译、发布、运行\n构建一个可部署的 app 组件并将它与配置绑定，根据这个组件 / 配置的组合来启动一个或者多个进程，这两个阶段是严格分离的。\n进程\n该 app 执行一个或者多个无状态进程（例如 master/work），它们之间不需要共享任何东西。任何需要的状态都置于后端服务（例如 cache、对象存储等）。\n端口绑定\n该应用程序是独立的，并通过端口绑定（包括 HTTP）导出任何 / 所有服务。\n并发\n并发通常通过水平扩展应用程序进程来实现（尽管如果需要的话进程也可以通过内部管理的线程多路复用来实现）。\n可任意处置性\n通过快速迅速启动和优雅的终止进程，可以最大程度上的实现鲁棒性。这些方面允许快速弹性缩放、部署更改和从崩溃中恢复。\n开发 / 生产平等\n通过保持开发、staging 和生产环境尽可能的相同来实现持续交付和部署。\n日志\n不管理日志文件，将日志视为事件流，允许执行环境通过集中式服务收集、聚合、索引和分析事件。\n管理进程\n行政或管理类任务（如数据库迁移），应该在与 app 长期运行的相同的环境中一次性完成。\n这些特性很适合快速部署应用程序，因为它们不需要对将要部署的环境做任何假定。不对环境假设能够允许底层云平台使用简单而一致的机制，轻松实现自动化，快速配置新环境，并部署应用。以这种方式，十二因素应用模式能够帮我们优化应用的部署速度。\n这些特性也很好地适用于突发需求，或者低成本地 “丢弃” 应用程序。应用程序环境本身是 100％一次性的，因为任何应用程序状态，无论是内存还是持久性，都被提取到后端服务。这允许应用程序以易于自动化的非常简单和弹性的方式进行伸缩。在大多数情况下，底层平台只需将现有环境复制到所需的数目并启动进程。缩容是通过暂停正在运行的进程和删除环境来完成，无需设法地实现备份或以其他方式保存这些环境的状态。就这样，12 因素应用模式帮助我们实现规模优化。\n最后，应用程序的可处理性使得底层平台能够非常快速地从故障事件中恢复。\n此外，将日志作为事件流处理能够极大程度上的增强应用程序运行时底层行为的可视性。\n强制环境之间的等同、配置机制的一致性和后端服务管理使云平台能够为应用程序运行时架构的各个方面提供丰富的可视性。以这种方式，十二因素应用模式能够优化安全性。\n微服务 微服务将单体业务系统分解为多个 “仅做好一件事” 的可独立部署的服务。这件事通常代表某项业务能力，或者最小可提供业务价值的 “原子 “服务单元。\n微服务架构通过以下几种方式为速度、安全、可扩展性赋能：\n 当我们将业务领域分解为可独立部署的有限能力的环境的同时，也将相关的变更周期解耦。只要变更限于单一有限的环境，并且服务继续履行其现有合约，那么这些更改可以独立于与其他业务来进行开展和部署。结果是实现了更频繁和快速的部署，从而实现了持续的价值流动。 通过扩展部署组织本身可以加快部署。由于沟通和协调的开销，添加更多的人，往往会使软件构建变得更加苦难。 弗雷德・布鲁克斯（Fred Brooks，人月神话作者）很多年前就教导我们，在软件项目的晚期增加更多的人力将会时软件项目更加延期。 然而，我们可以通过在有限的环境中构建更多的沙箱，而不是将所有的开发者都放在同一个沙箱中。 由于学习业务领域和现有代码的认知负担减少，并建立了与较小团队的关系，因此我们添加到每个沙箱的新开发人员可以更快速地提高并变得更高效。 可以加快采用新技术的步伐。大型单体应用架构通常与对技术堆栈的长期保证有关。这些保证的存在是为了减轻采用新技术的风险。采用了错误的技术在单体架构中的代价会更高，因为这些错误可能会影响整个企业架构。如果我们可以在单个整体的范围内采用新技术，将隔离并最大限度地降低风险，就像隔离和最小运行时故障的风险一样。 微服务提供独立、高效的服务扩展。单体架构也可以扩展，但要求我们扩展所有组件，而不仅仅是那些负载较重的组件。当且仅当相关联的负载需要它时，微服务才会被缩放。  自服务敏捷架构 使用云原生应用架构的团队通常负责其应用的部署和持续运营。云原生应用的成功采纳者已经为团队提供了自服务平台。\n正如我们创建业务能力团队为每个有界的环境构建微服务一样，我们还创建了一个能力小组，负责提供一个部署和运行这些微服务的平台。\n这些平台中最大好处是为消费者提供主要的抽象层。通过基础架构即服务（IAAS），我们要求 API 创建虚拟服务器实例、网络和存储，然后应用各种形式的配置管理和自动化，以使我们的应用程序和支持服务能够运行。现在这种允许我们自定义应用和支持服务的平台正在不断涌现。\n应用程序代码简单地以预构建的工件（可能是作为持续交付管道的一部分生成的）或 Git 远程的原始源代码的形式 “推送”。 然后，平台构建应用程序工件，构建应用程序环境，部署应用程序，并启动必要的进程。 团队不必考虑他们的代码在哪里运行或如何到达那里，这些对用户都是透明得，因为平台会关注这些。\n这样的模型同样适合于后端服务。需要数据库？ 消息队列或邮件服务器？ 只需要求平台来配合您的需求。平台现在支持各种 SQL/NoSQL 数据存储、消息队列、搜索引擎、缓存和其他重要的后端服务。这些服务实例然后可以 “绑定” 到您的应用程序，必要的凭据会自动注入到应用程序的环境中以供其使用。从而消除了大量凌乱而易出错的定制自动化。\n这些平台还经常提供广泛的额外操作能力：\n 应用程序实例的自动化和按需扩展 应用健康管理 请求到或跨应用程序实例间的动态路由和负载均衡 日志和指标的聚合  这种工具的组合确保了能力团队能够根据敏捷原则开发和运行服务，从而实现速度，安全性和规模化。\n基于 API 的协作 在云原生应用架构中，服务之间的唯一互动模式是通过已发布和版本化的 API。这些 API 通常是具有 JSON 序列化的 HTTP REST 风格，但也可以是其他协议和序列化格式。\n只要有需要，在不会破坏任何现有的 API 协议的前提下，团队就可以部署新的功能，而不需要与其他团队进行同步。自助服务基础设施平台的主要交互模式也是通过 API，就像其他业务服务一样。供给、缩放和维护应用程序基础设施的方式不是通过提交单据，而是将这些请求提交给提供该服务的 API。\n通过消费者驱动的协议，可以在服务间交互的双方验证协议的合规性。服务消费者不能访问其依赖关系的私有实现细节，或者直接访问其依赖关系的数据存储。实际上，只允许有一个服务能够直接访问任何数据存储。这种强制解耦直接支持云原生的速度目标。\n抗脆弱性 Nassim Taleb 在他的 Antifragile（Random House）一书中介绍了抗脆弱性的概念。如果脆弱性是受到压力源的弱化或破坏的质量系统，那么与之相反呢？许多人会以稳健性或弹性作出回应 —— 在遭受压力时不会被破坏或变弱。然而，Taleb 引入了与脆弱性相反的抗脆弱性概念，或者在受到压力源时变得更强的质量系统。什么系统会这样工作？联想下人体免疫系统，当接触病原体时，其免疫力变强，隔离时较弱。我们可以像这样建立架构吗？云原生架构的采用者们已经设法构建它们了。Netflix Simian Army 项目就是个例子，其中著名的子模块 “混沌猴”，它将随机故障注入到生产组件中，目的是识别和消除架构中的缺陷。通过明确地寻求应用架构中的弱点，注入故障并强制进行修复，架构自然会随着时间的推移而更大程度地收敛。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"d1414a6339679ec839bb2b29681f240d","permalink":"https://lib.jimmysong.io/migrating-to-cloud-native-application-architectures/the-rise-of-cloud-native/defining-cloud-native-architectures/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/migrating-to-cloud-native-application-architectures/the-rise-of-cloud-native/defining-cloud-native-architectures/","section":"migrating-to-cloud-native-application-architectures","summary":"现在我们将探索云原生应用架构的几个主要特征，和这些特征是如何解决我们前面提到的使用云原生应用架构的动机。 12 因素应用 12 因素应用是一系列云原生应用架构的模式集合，最初由 Heroku 提出。这些模式可以用来说明什么样的","tags":null,"title":"1.2 云原生架构的定义","type":"book"},{"authors":null,"categories":["安全"],"content":"在看了基于微服务的应用所需的各种应用服务后，考虑一下提供这些服务的服务网格的架构。服务网格由两个主要部分组成：控制平面和数据平面。\n2.2.1 控制平面 控制平面有几个组件。虽然服务网格的数据面主要由作为容器运行在与应用容器相同的 Pod 中的代理组成，但控制面组件在它们自己的 Pod、节点和相关集群中运行。以下是控制平面的 各种功能：\n Envoy sidecar 代理的服务发现和配置 自动化的密钥和证书管理 用于策略定义和收集遥测数据的 API 服务网格组件的配置摄取 管理一个到服务网格的入站连接（入站网关） 管理来自服务网格的出站连接（出口网关） 将 sidecar 代理注入那些托管应用程序微服务容器的 Pod、节点或命名空间中  总的来说，控制平面帮助管理员用配置数据填充数据平面组件，这些数据是由控制平面的策略产生的。上述功能 3 的策略可能包括网络路由策略、负载均衡策略、蓝绿部署的策略、金丝雀部署、超时、重试和断路能力。这后三项被统称为网络基础设施服务的弹性能力的特殊名称。最后要说的是与安全相关的策略（例如，认证和授权策略、TLS 建立策略等）。这些策略规则由一个模块解析，该模块将其转换为配置参数，供执行这些策略的数据平面代理中的可执行程序使用。\n2.2.2 数据平面 数据平面组件执行三种不同的功能：\n 安全的网络功能 策略执行功能 可观测性功能  执行上述三种功能的数据平面的主要组件被称为 sidecar 代理。这个七层代理运行在与它执行代理功能的微服务相同的网络命名空间（在这个平台上，就是同一个 Pod）。每个微服务都有一个代理，以确保来自微服务的请求不会绕过其相关的代理，每个代理都作为容器运行在与应用微服务相同的 Pod 中。两个容器都有相同的 IP 地址，并共享相同的 IP 表规则。这使得代理完全控制了 Pod，并 处理所有通过它的流量。\n第一类功能（安全网络）包括与微服务之间的实际路由或消息通信有关的所有功能。属于这一类的功能是服务发现、建立安全（TLS）会话、为每个微服务及其相关请求建立网络路径和路由规则、验证每个请求（来自服务或用户）以及授权请求。\n以建立双向 TLS 会话为例，发起通信会话的代理将与服务网格的控制平面中的模块进行交互，以检查是否需要通过链加密流量并与后端或目标 Pod 建立双向 TLS。使用双向 TLS 启用这个功能需要每个 Pod 有一个证书（即有效的凭证）。由于一个规模较大的微服务应用程序（由许多微服务组成）可能需要数百个 Pod（即使没有通过多个实例对单个微服务进行横向扩展），这可能涉及到管理数百个生命周期短暂的证书。这反过来又要求每个微服务有一个强大的身份，服务网格要有一个访问管理器、一个证书存储和一个证书验证能力。此外，为了支持认证策略，还需要识别和认证两个通信的 Pod 的机制。\n其他类型的代理包括拦截客户端调用到应用程序的第一个入口点（第一个被调用的微服务）的 入口代理 和处理微服务对驻扎在平台集群外的应用模块的请求的出口代理。\n数据平面执行的第二类功能是通过代理中的配置参数执行控制平面中定义的策略（策略执行服务）。一个例子是使用作为微服务请求一部分的 JWT 令牌中的信息来验证调用服务。另一个例子是使用驻留在代理本身的代码或通过连接到外部授权服务，为每个请求执行访问控制策略。\n服务代理与应用服务容器联合执行的第三类功能是收集遥测数据，这有助于监测服务的健康和状态，将与服务相关的日志传输到控制平面中的日志聚合模块，并将必要的数据附加到应用请求头，以方便追踪与特定应用事务相关的所有请求。应用响应由代理机构以返回代码、响应描述或检索数据的形式传达给其相关的调用服务。\n服务网格是容器编排平台感知的，与 API 服务器互动，该服务器为安装在各种平台工件（如 Pod、节点、命名空间）中的应用服务提供一个窗口，监测它是否有新的微服务，并自动将 sidecar 容器注入包含这些新微服务的 Pod。一旦服务网格插入 sidecar 代理容器，运维和安全团队就可以对流量执行策略，并帮助保护和运维应用程序。这些团队还可以配置对微服务应用的监控，而不干扰应用的运作。\n基础设施、策略执行和可观测性服务的配置可以使用作为 DevSecOps 管道一部分的声明性代码来自动化。虽然开发团队应该全面了解其代码部署的安全和管理细节，但上述服务的自动化为他们提供了更多的时间来集中精力进行高效的开发范式，如代码模块化和结构化。\n 下一章   ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"f6b1fcef9d8d5d47367748738bcf7b81","permalink":"https://lib.jimmysong.io/service-mesh-devsecops/reference-platform/service-mesh-software-architecture/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/service-mesh-devsecops/reference-platform/service-mesh-software-architecture/","section":"service-mesh-devsecops","summary":"在看了基于微服务的应用所需的各种应用服务后，考虑一下提供这些服务的服务网格的架构。服务网格由两个主要部分组成：控制平面和数据平面。 2.2.1 控制平面 控制平面有几个组件。虽然服务网格的数据面主要由作为容器运行在","tags":["Service Mesh","DevSecOps"],"title":"2.2 服务网格架构","type":"book"},{"authors":null,"categories":null,"content":"在本节中，我们将探讨采用云原生应用架构的组织在创建团队时需要进行的变革。这个重组背后的理论是著名的康威定律。我们的解决方案是在长周期的产品开发中，创建一个包含了各方面专业员工的团队，而不是将他们分离在单一的团队中，例如测试人员。\n业务能力团队  设计系统的组织，最终产生的设计等同于组织之内、之间的沟通结构。\n——Melvyn Conway\n 我们已经讨论了 “从孤岛到 DevOps” 将 IT 组织成专门的信息孤岛的做法。我们很自然地创造了这些孤岛，并把个体放到与这些孤岛一致的团队中。但是当我们需要构建一个新的软件的时候会怎么样？\n一个很常见的做法是成立一个项目团队。该团队向项目经理汇报，然后项目经理与各种孤岛合作，为项目所需的各个专业领域寻求 “资源”。正如康威定律所言，这些团队将很自然地在系统中构筑起各种孤岛，我们最终得到的是联合各种孤岛相对应的孤立模块的架构：\n 数据访问层 服务层 Web MVC 层 消息层 等等  这些层次中的每一层都跨越了多个业务能力领域，使得在其之上的创新和部署独立于其他业务能力的新功能变的非常困难。\n寻求迁移到将业务能力分离的微服务等云原生架构的公司经常采用 Thoughtworks 称之为的 “逆康威定律”。他们没有建立一个与其组织结构图相匹配的架构，而是决定了他们想要的架构，并重组组织以匹配该架构。如果你这样做的话，根据康威定律，您所期望的架构终将出现。\n因此，作为转向 DevOps 文化的一部分，我们组织了跨职能、业务能力的团队，开发的是产品而不再是项目。开发产品需要长期的付出，直到它们不再为企业提供价值为止。（直到你的代码不再运行在生产上为止！）构建、测试、交付和运营提供业务能力的服务所需的所有角色都存在于一个团队中，该团队不会向组织的其他部分交接代码。这些团队通常被组织为 “双比萨队”，意思是如果不能用两个比萨饼喂饱，那就意味着团队规模太大了。\n那么剩下的就是确定要创建的团队。如果我们遵循逆康威定律，我们将从组织的领域模型开始，并寻求可以封装在有限环境中的业务能力。一旦我们确定了这些能力，我们就可以创建为这些业务能力的整个生命周期负责的团队。 业务能力团队掌握其应用程序从开发到运营的整个生命周期。\n平台运营团队 业务能力团队需要依赖于我们前面提到的” 自助敏捷基础架构 “。\n事实上，我们可以这样来描述一种特殊的业务能力 —— 开发、部署和运营业务的能力。这种能力应该是平台运营团队所具有的。\n平台运营团队运营自助敏捷基础架构平台，并交付给业务能力团队使用。该团队通常包括传统的系统、网络和存储管理员角色。如果公司正在运营云平台，该团队也将拥有管理数据中心的团队或与他们紧密合作，并了解提供基础架构平台所需的硬件能力。\nIT 运营传统上通过各种基于单据的系统与客户进行互动。由于基于平台操作流来运行自助服务平台，因此必须提供不同形式的交互方式。正如业务能力团队之间通过定义好的 API 协议相互协作一样，平台运营团队也为该平台提供了 API 协议。业务能力团队不再需要排队申请应用环境和数据服务，而是采用更精简的方式构建按需申请环境和服务的自动化发布管道。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"28ce72da1a56827461bd2b9c115c3c2a","permalink":"https://lib.jimmysong.io/migrating-to-cloud-native-application-architectures/changes-needed/organizational-change/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/migrating-to-cloud-native-application-architectures/changes-needed/organizational-change/","section":"migrating-to-cloud-native-application-architectures","summary":"在本节中，我们将探讨采用云原生应用架构的组织在创建团队时需要进行的变革。这个重组背后的理论是著名的康威定律。我们的解决方案是在长周期的产品开发中，创建一个包含了各方面专业员工的团队，而不是将他们分离在","tags":null,"title":"2.2 组织变革","type":"book"},{"authors":null,"categories":["安全"],"content":"DevSecOps 是一个敏捷的、自动化的开发和部署过程，它使用称为 CI/CD 管道的原语，在自动化工具的帮助下，将软件从构建阶段带到部署阶段，最后到运行时间 / 操作阶段。这些管道是将开发者的源代码带过各个阶段的工作流程，如构建、测试、打包、交付，以及在各个阶段由测试工具支持的部署。\nDevSecOps 平台是指各种 CI/CD 管道（针对每种代码类型）运行的资源集合。至少，这个平台由以下部分组成。\n(a) 管道软件\n CI 软件 —— 从代码库中提取代码，调用构建软件，调用测试工具，并将测试后的工件存储到图像注册表中。 CD 软件 —— 拉出工件、软件包，并根据 IaC 中的计算、网络和存储资源描述，部署软件包。  (b) SDLC 软件\n 构建工具（例如，IDE） 测试工具（SAST、DAST、SCA）  (c) 存储库\n 源代码库（如 GitHub） 容器镜像存储库或注册表  (d) 可观测性或监测工具\n 日志和日志聚合工具 产生指标的工具 追踪工具（应用程序的调用顺序） 可视化工具（结合上述数据生成仪表盘 / 警报）。  在 DevSecOps 平台中，通过内置的设计功能（如零信任）和使用一套全面的安全测试工具，如静态应用安全工具（SAST）、动态安全测试工具（DAST）和软件组成分析（SCA）工具进行测试，在构建和部署阶段提供安全保证。此外，在运行时 / 操作阶段，还通过持续的行为检测 / 预防工具提供安全保证，其中一些工具甚至可能使用人工智能（AI）和机器学习（ML）等复杂技术。因此，DevSecOps 平台不仅在构建和部署阶段运行，而且在运行时 / 操作阶段也运行。\n在一些 DevSecOps 平台中，执行应用程序安全分析的安全工具（例如 SAST、DAST 和 SCA），例如通过在后台的有效扫描来识别漏洞和错误，可以与集成开发环境（IDE）和其他 DevOps 工具紧密集成。这一功能的存在，使得这些工具对开发者来说是透明的，避免了他们为运行这些工具而 调用单独的 API。根据集成开发环境、所执行的任务或工具所消耗的资源的不同，工具也可以与集成开发环境分开执行。\n3.2.1 DevSecOps 平台的可交付成果 SAST、DAST 和 SCA 工具的使用可能不仅限于测试应用程序代码。DevSecOps 可能包括将这些工具用于其他代码类型，如 IaC，因为 IaC 定义了应用程序的部署架构，因此是自动评估和补救安全设计差距的关键途径。\n总之，DevSecOps 平台可以提供以下内容：\n 通过在与应用环境中所有代码类型相关的管道内纳入充分的测试 / 检查，提供安全保证。安全性不是被归入一个单独的任务或阶段。 DevSecOps 平台也在运行时（生产中）运行，通过协助执行零信任原则，并通过持续监控，随后的警报和纠正机制，提供实时的安全保证，从而实现持续授权操作（C-ATO）的认证。  ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"c75dec1ac8aa75d719488cda32d37a60","permalink":"https://lib.jimmysong.io/service-mesh-devsecops/devsecops/devsecops-platform/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/service-mesh-devsecops/devsecops/devsecops-platform/","section":"service-mesh-devsecops","summary":"DevSecOps 是一个敏捷的、自动化的开发和部署过程，它使用称为 CI/CD 管道的原语，在自动化工具的帮助下，将软件从构建阶段带到部署阶段，最后到运行时间 / 操作阶段。这些管道是将开发者的源代码带过各个阶段的工作流程，如构建、","tags":["Service Mesh","DevSecOps"],"title":"3.2 DevSecOps 平台","type":"book"},{"authors":null,"categories":null,"content":"当我们开始构建由微服务组成的分布式系统时，我们还会遇到在开发单体应用时通常不会遇到的非功能性要求。有时，使用物理定律就可以解决这些问题，例如一致性、延迟和网络分区问题。然而，脆弱性和易控性的问题通常可以使用相当通用的模式来解决。在本节中，我们将介绍帮助我们解决这些问题的方法。\n这些方法来自于 Spring Cloud 项目和 Netflix OSS 系列项目的组合。\n版本化和分布式配置 在 “12 因素应用 “中我们讨论过通过操作系统级环境变量为应用注入对应的配置，强调了这种配置管理方式的重要性。这种方式特别适合简单的系统，但是，当系统扩大后，有时我们还需要附加的配置能力：\n 为调试一个生产上的问题而变更运行的应用程序日志级别 更改 message broker 中接收消息的线程数 报告所有对生产系统配置所做的更改以支持审计监管 运行中的应用切换功能开关 保护配置中的机密信息（如密码）  为了支持这些特性，我们需要配置具有以下特性的配置管理方法：\n 版本控制 可审计 加密 在线刷新  Spring Cloud 项目中包含的一个可提供这些功能的配置服务器。此配置服务器通过 Git 本地仓库支持的 REST API 呈现了应用程序及应用程序配置文件（例如，可用开 / 关切换的一组配置作为一组，如 “deployment” 和 “staging” 配置）（图 3 -1）。\n   Spring Cloud Config Server  例 3-1 是示例配置服务器的默认配置文件：\n   Example 3-1    该配置中指定了后端 Git 仓库中的 application.yml 文件。\n  greeting 当前被设置为 ohai。\n  例 3-1 中的配置是自动生成的，无需手动编码。我们可以看到，通过检查它的 /env 端点（例 3-2），greeting 的值被分发到 Spring 应用中。\n   Example 3-2   该应用接收到来自配置服务器的 greeting 的值：ohai。  现在我们就可以无需重启客户端应用就可以更新 greeting 的值。该功能由 Spring Cloud 项目中的一个名为 Spring Cloud Bus 的组件提供。该项目将分布式系统的节点与轻量级消息代理进行链接，然后可以用于广播状态更改，如我们所需的配置更改（图 3-2）。该项目将分布式系统的节点与轻量级消息代理进行链接，然后可以用于广播状态更改，如我们所需的配置更改（图 3-2）。\n只需通过对参与总线的任何应用程序的 /bus/refresh 端点执行 HTTP POST（这显然应该进行适当的安全性保护），指示总线上的所有应用程序使用配置服务器中的最新的可用值刷新其配置。\n   Figure 3-2  服务注册发现 当我们创建分布式系统时，代码的依赖不再是一个方法调用。相反，消费它们必须通过网络调用。我们该如何布线，才能使组合系统中的所有微服务彼此通信？\n云中的（图 3-3）的同样架构模式是有一个前端（应用程序）和后端（业务）服务。后端服务往往不能直接从互联网访问，而是通过前端服务访问。服务注册提供的所有服务的列表， 使它们可以通过一个客户端库到达前端服务（路由和负载均衡），客户端库执行负载均衡和路由到后端服务。\n   Figure 3-3  在使用服务定位器和依赖注入模式的各种形式之前，我们已经解决了这个问题，面向服务的架构长期以来一直使用各种形式的服务注册表。我们将采用类似的解决方案，利用 Eureka，这是一个 Netflix OSS 项目，可用于定位服务，以实现中间层服务的负载平衡和故障转移。为了使用 Netflix OSS 服务，Spring Cloud Netflix 项目提供了基于注释的配置模型，这大大简化了开发人员在开发 Spring 应用程序时对 Eureka 的心力耗费。\n在例 3-3 中，只需简单得在代码中添加 @EnableDiscoveryClient 注释，应用程序就可以进行服务注册和发现。\n   Example 3-3   @EnableDiscoveryClient 开启应用程序的服务注册发现。  该应用程序就能够通过利用 DiscoveryClient 与它的依赖组件通信。例 3-4 是应用程序查找名为 PRODUCER 的注册服务的一个实例，获得其 URL，然后利用 Spring 的 RestTemplate 与之通信。\n   Example 3-4   开启的 DiscoveryClient 通过 Spring 注入。 getNextServerFromEureka 方法使用 round-robin 算法提供服务实例的位置。  路由和负载均衡 基本的 round-robin 负载平衡在许多情况下是有效的，但云环境中的分布式系统通常需要更高级的路由和负载均衡行为。这些通常由各种外部集中式负载均衡解决方案提供。然而，这种解决方案通常不具有足够的信息或上下文，以便在给定的应用程序尝试与其依赖进行通信时做出最佳选择。此外，如果这种外部解决方案故障，这些故障可以跨越整个架构。\n云原生的解决方案通常将路由和负载均衡的职责放在客户端。Ribbon Netflix OSS 项目就是其中的一种。（图 3-4）\n   Figure 3-4  Ribbon 提供一组丰富的功能集：\n  多种内建的负载均衡规则：\n Round-robin 轮询负载均衡 平均加权响应时间负载均衡 随机负载均衡 可用性过滤负载均衡（避免跳闸线路和高并发链接数） 自定义负载均衡插件系统    与服务发现解决方案的可拔插集成（包括 Eureka）\n  云原生智能，例如可用区亲和性和不健康区规避\n  内建的故障恢复能力\n  跟 Eureka 一样，Spring Cloud Netflix 项目也大大简化了 Spring 应用程序开发人员使用 Ribbon 的心力耗费。开发人员可以注入一个 LoadBalancerClient 的实例，然后使用它来解析应用程序依赖关系的一个实例（例 3-5），而不是注入 DiscoveryClient 的实例（用于直接从 Eureka 中消费）。\n   Example 3-5-1     Example 3-5-2   由 Spring 注入的 LoadBalancerClient。 choose 方法使用当前负载均衡算法提供了服务的一个示例地址。  Spring Cloud Netflix 通过创建可以注入到 Bean 中的 Ribbon-enabled 的 RestTemplate bean 来进一步简化 Ribbon 的配置。RestTemplate 的这个实例被配置为使用 Ribbon（示例 3-6）自动将实例的逻辑服务名称解析为 instanceURI。\n   Example 3-6   注入的是 RestTemplate 而不是 LoadBalancerClient。 注入的 RestTemplate 自动将 http://producer 解析为实际的服务实例的 URI。  容错 分布式系统比起单体架构来说有更多潜在的故障模式。由于传入系统中的每一个请求都可能触及几十甚至上百个不同的微服务，因此这些依赖中的某些故障实质上是不可避免的。\n 如果不进行容错，30 个依赖，每个都是 99.99% 的正常运行时间，每个月将导致 2 个小时的停机时间（99.99%^30=99.7% 的正常运行时间 = 2 小时以上的停机时间）。\n——Ben Christensen，Netflix 工程师\n 如何避免这类故障导致级联故障，给我们的系统可用性数据带来负面影响？Mike Nygard 在他的 Pragmatic Programmers 中提出了几个可以觉得该问题的几个模式，包括：\n熔断器\n当服务的依赖被确定为不健康时，使用熔断器来阻绝该服务与其依赖的远程调用，就像电路熔断器可以防止电力使用过度，防止房子被烧毁一样。熔断器实现为状态机（图 3-5）。当其处于关闭状态时，服务调用将直接传递给依赖关系。如果任何一个调用失败，则计入这次失败。当故障计数在指定时间内达到指定的阈值时，熔断器进入打开状态。在熔断器为打开状态时，所有调用都会失败。在预定时间段之后，线路转变为 “半开” 状态。在这种状态下，调用再次尝试远程依赖组件。成功的调用将熔断器转换回关闭状态，而失败的调用将熔断器返回到打开状态。\n   Figure 3-5  隔板\n隔板将服务分区，以便限制错误影响的区域，并防止整个服务由于某个区域中的故障而失败。这些分区就像将船舶划分成多个水密舱室一样，使用隔板将不同的舱室分区。这可以防止当船只受损时造成整艘船沉没（例如，当被鱼雷击中时）。软件系统中可以用许多方式利用隔板。简单地将系统分为微服务是我们的第一道防线。将应用程序进程分区为 Linux 容器，以便使用单个进程无法接管整个计算机。另一个例子是将并行工作划分为不同的线程池。\nNetflix 的 Hystrix 应用了这些和更多的模式，并提供了强大的容错功能。为了包含熔断器的代码，Hystrix 允许代码被包含到 HystrixCommand 对象中。\n   Example 3-7   run 方法中封装了熔断器  Spring Cloud Netflix 通过在 Spring Boot 应用程序中添加 @EnableCircuitBreaker 注解来启用 Hystrix 运行时组件。然后通过另一组注解，使得基于 Spring 和 Hystrix 的编程与我们先前描述的集成一样简单（例 3-8）。\n   Example 3-8   使用 @HystrixCommand 注解的方法封装了一个熔断器。 当线路处于打开或者半开状态时，注解中引用的 getProducerFallback 方法，提供了一个优雅的回调操作。  Hystrix 相较于其他熔断器来说是独一无二的，因为它还通过在其自己的线程池中操作每个熔断器来提供隔板。它还收集了许多关于熔断器状态的有用指标，其中包括：\n  流量\n  请求率\n  错误百分比\n  主机报告\n  延迟百分点\n  成功、失败和拒绝\n  这些 metric 会被发送到事件流中，然后被 Netflix OSS 项目中的另一个叫做 Turbine 的组聚合。每个单独的和聚合后的 metric 流都可以在强大的 Hystrix Dashboard（图 3-6）中以可视化的方式呈现，该页面提供了很好的分布式系统总体健康状态的可视化效果。\n   Figure 3-6  API 网关 / 边缘服务 在 “移动应用和客户端多样性” 中我们探讨过服务器端聚合与微服务生态系统。为什么有这个必要？\n延迟\n移动设备通常运行在比我们家用设备更低速的网络上。即使是在家用或企业网络上， 为了满足单个应用屏幕的需求，需要连接数十（或者上百）个微服务，这样的延迟也将变得不可接受。很明显，应用程序需要使用并发的方式来访问这些服务。在服务端一次性捕获和实行这些并发模式，会比在每一个设备平台上做相同的事情，来得更廉价、更不容易出错。\n延迟的另一个来源是响应数据的大小。在 Web 服务开发领域，近年来一直趋向于 “返回一切可能有用的数据” 的做法，这将导致响应的返回数据越来愈大，远远超出了单一的移动设备屏幕的需求。移动设备开发者更倾向于通过仅检索必要的信息而忽略其他不重要的信息，来减少等待时间。\n往返通信\n即使网速不成问题，与大量的微服务通信依然会给移动应用开发者造成困扰。移动设备的电池消耗主要是因为网络开销造成的。移动应用开发者尽可能通过最少的服务端调用来减少网络的开销，并提供预期的用户体验。\n设备多样性\n移动设备生态系统中设备多样性是十分巨大的。企业必须应对不断增长的客户群体差异，包括如下这些：\n  制造商\n  设备类型\n  形式因素\n  设备尺寸\n  编程语言\n  操作系统\n  运行时环境\n  并发模型\n  支持的网络协议\n  这种多样性甚至扩大到超出了移 …","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"eb550d80c782554214a8ec4f7700ab5b","permalink":"https://lib.jimmysong.io/migrating-to-cloud-native-application-architectures/migration-cookbook/distributed-systems-recipes/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/migrating-to-cloud-native-application-architectures/migration-cookbook/distributed-systems-recipes/","section":"migrating-to-cloud-native-application-architectures","summary":"当我们开始构建由微服务组成的分布式系统时，我们还会遇到在开发单体应用时通常不会遇到的非功能性要求。有时，使用物理定律就可以解决这些问题，例如一致性、延迟和网络分区问题。然而，脆弱性和易控性的问题通常可","tags":null,"title":"3.2 使用分布式系统","type":"book"},{"authors":null,"categories":["安全"],"content":"应用程序代码和应用服务代码驻留在容器编排和资源管理平台中，而实现与之相关的工作流程的 CI/CD 软件通常驻留在同一平台中。应使用第 4.6 节所述的步骤对该管道进行保护，该管道控制下的应用程序代码应接受第 4.8 节所述的安全测试。此外，应用程序所在的调度平台本身应使用运行时安全工具（如 Falco）进行保护，该工具可以实时读取操作系统内核日志、容器日志和平台日志，并根据威胁检测规则引擎对其进行处理，以提醒用户注意恶意行为（例如，创建有特权的容器、未经授权的用户读取敏感文件等）。它们通常有一套默认（预定义）的规则，可以在上面添加自定义规则。在平台上安装它们，可以为集群中的每个节点启动代理，这些代理可以监控在该节点的各个 Pod 中运行的容器。这种类型的工具的优点是，它补充了现有平台的本地安全措施，如访问控制模型和 Pod 安全策略，通过实际检测它们的发生来 防止漏洞。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"207f16f7aabf87c773bb1c1ea9c9186f","permalink":"https://lib.jimmysong.io/service-mesh-devsecops/implement/ci-cd-pipeline-for-application-code-and-application-services-code/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/service-mesh-devsecops/implement/ci-cd-pipeline-for-application-code-and-application-services-code/","section":"service-mesh-devsecops","summary":"应用程序代码和应用服务代码驻留在容器编排和资源管理平台中，而实现与之相关的工作流程的 CI/CD 软件通常驻留在同一平台中。应使用第 4.6 节所述的步骤对该管道进行保护，该管道控制下的应用程序代码应接受第 4.8 节所述的安全","tags":["Service Mesh","DevSecOps"],"title":"4.2 应用程序代码和应用服务代码的 CI/CD 管道","type":"book"},{"authors":null,"categories":["Envoy"],"content":"HCM 是一个网络级的过滤器，将原始字节转译成 HTTP 级别的消息和事件（例如，收到的 Header，收到的 Body 数据等）。\nHCM 过滤器还处理标准的 HTTP 功能。它支持访问记录、请求 ID 生成和跟踪、Header 操作、路由表管理和统计等功能。\n从协议的角度来看，HCM 原生支持 HTTP/1.1、WebSockets、HTTP/2 和 HTTP/3（仍在 Alpha 阶段）。\nEnvoy 代理被设计成一个 HTTP/2 复用代理，这体现在描述 Envoy 组件的术语中。\nHTTP/2 术语\n在 HTTP/2 中，流是已建立的连接中的字节的双向流动。每个流可以携带一个或多个消息（message）。消息是一个完整的帧（frame）序列，映射到一个 HTTP 请求或响应消息。最后，帧是 HTTP/2 中最小的通信单位。每个帧都包含一个帧头（frame header），它至少可以识别该帧所属的流。帧可以携带有关 HTTP Header、消息有效载荷等信息。\n无论流来自哪个连接（HTTP/1.1、HTTP/2 或 HTTP/3），Envoy 都使用一个叫做 编解码 API（codec API） 的功能，将不同的线程协议翻译成流、请求、响应等协议无关模型。协议无关的模型意味着大多数 Envoy 代码不需要理解每个协议的具体内容。\nHTTP 过滤器 在 HCM 中，Envoy 支持一系列的 HTTP 过滤器。与监听器级别的过滤器不同，这些过滤器对 HTTP 级别的消息进行操作，而不知道底层协议（HTTP/1.1、HTTP/2 等）或复用能力。\n有三种类型的 HTTP 过滤器。\n 解码器（Decoder）：当 HCM 对请求流的部分进行解码时调用。 编码器（Encoder）：当 HCM 对响应流的部分进行编码时调用。 解码器 / 编码器（Decoder/Encoder）：在两个路径上调用，解码和编码  下图解释了 Envoy 如何在请求和响应路径上调用不同的过滤器类型。\n   请求响应路径及 HTTP 过滤器  像网络过滤器一样，单个的 HTTP 过滤器可以停止或继续执行后续的过滤器，并在单个请求流的范围内相互分享状态。\n数据共享 在高层次上，我们可以把过滤器之间的数据共享分成静态和动态。\n静态包含 Envoy 加载配置时的任何不可变的数据集，它被分成三个部分。\n1. 元数据\nEnvoy 的配置，如监听器、路由或集群，都包含一个metadata数据字段，存储键 / 值对。元数据允许我们存储特定过滤器的配置。这些值不能改变，并在所有请求 / 连接中共享。例如，元数据值在集群中使用子集选择器时被使用。\n2. 类型化的元数据\n类型化元数据不需要为每个流或请求将元数据转换为类型化的类对象，而是允许过滤器为特定的键注册一个一次性的转换逻辑。来自 xDS 的元数据在配置加载时被转换为类对象，过滤器可以在运行时请求类型化的版本，而不需要每次都转换。\n3. HTTP 每路过滤器配置\n与适用于所有虚拟主机的全局配置相比，我们还可以指定每个虚拟主机或路由的配置。每个路由的配置被嵌入到路由表中，可以在 typed_per_filter_config 字段下指定。\n另一种分享数据的方式是使用动态状态。动态状态会在每个连接或 HTTP 流中产生，并且它可以被产生它的过滤器改变。名为 StreamInfo 的对象提供了一种从 map 上存储和检索类型对象的方法。\n过滤器顺序 指定 HTTP 过滤器的顺序很重要。考虑一下下面的 HTTP 过滤器链。\nhttp_filters:- filter_1- filter_2- filter_3一般来说，链中的最后一个过滤器通常是路由器过滤器。假设所有的过滤器都是解码器 / 编码器过滤器，HCM 在请求路径上调用它们的顺序是filter_1、filter_2、filter_3。\n在响应路径上，Envoy 只调用编码器过滤器，但顺序相反。由于这三个过滤器都是解码器 / 编码器过滤器，所以在响应路径上的顺序是 filter_3、filter_2、filter_1。\n内置 HTTP 过滤器 Envoy 已经内置了几个 HTTP 过滤器，如 CORS、CSRF、健康检查、JWT 认证等。你可以在这里找到 HTTP 过滤器的完整列表。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"b76cdbd18617a4bb4428f64dabcd72e9","permalink":"https://lib.jimmysong.io/envoy-handbook/hcm/introduction/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/envoy-handbook/hcm/introduction/","section":"envoy-handbook","summary":"HCM 是一个网络级的过滤器，将原始字节转译成 HTTP 级别的消息和事件（例如，收到的 Header，收到的 Body 数据等）。 HCM 过滤器还处理标准的 HTTP 功能。它支持访问记录、请求 ID 生成和跟踪、Header 操作、路由表管理和统计","tags":["Envoy"],"title":"HTTP 连接管理器（HCM）简介","type":"book"},{"authors":null,"categories":["Envoy"],"content":"前面提到的路由器过滤器（envoy.filters.http.router）就是实现 HTTP 转发的。路由器过滤器几乎被用于所有的 HTTP 代理方案中。路由器过滤器的主要工作是查看路由表，并对请求进行相应的路由（转发和重定向）。\n路由器使用传入请求的信息（例如，host  或 authority 头），并通过虚拟主机和路由规则将其与上游集群相匹配。\n所有配置的 HTTP 过滤器都使用包含路由表的路由配置（route_config）。尽管路由表的主要消费者将是路由器过滤器，但其他过滤器如果想根据请求的目的地做出任何决定，也可以访问它。\n一组虚拟主机构成了路由配置。每个虚拟主机都有一个逻辑名称，一组可以根据请求头被路由到它的域，以及一组指定如何匹配请求并指出下一步要做什么的路由。\nEnvoy 还支持路由级别的优先级路由。每个优先级都有其连接池和断路设置。目前支持的两个优先级是 DEFAULT 和 HIGH。如果我们没有明确提供优先级，则默认为 DEFAULT。\n这里有一个片段，显示了一个路由配置的例子。\nroute_config:name:my_route_config# 用于统计的名称，与路由无关virtual_hosts:- name:bar_vhostdomains:[\u0026#34;bar.io\u0026#34;]routes:- match:prefix:\u0026#34;/\u0026#34;route:priority:HIGHcluster:bar_io- name:foo_vhostdomains:[\u0026#34;foo.io\u0026#34;]routes:- match:prefix:\u0026#34;/\u0026#34;route:cluster:foo_io- match:prefix:\u0026#34;/api\u0026#34;route:cluster:foo_io_api当一个 HTTP 请求进来时，虚拟主机、域名和路由匹配依次发生。\n host或authority头被匹配到每个虚拟主机的domains字段中指定的值。例如，如果主机头被设置为 foo.io，则虚拟主机 foo_vhost 匹配。 接下来会检查匹配的虚拟主机内routs下的条目。如果发现匹配，就不做进一步检查，而是选择一个集群。例如，如果我们匹配了 foo.io 虚拟主机，并且请求前缀是 /api，那么集群 foo_io_api 就被选中。 如果提供，虚拟主机中的每个虚拟集群（virtual_clusters）都会被检查是否匹配。如果有匹配的，就使用一个虚拟集群，而不再进行进一步的虚拟集群检查。   虚拟集群是一种指定针对特定端点的重组词匹配规则的方式，并明确为匹配的请求生成统计信息。\n 虚拟主机的顺序以及每个主机内的路由都很重要。考虑下面的路由配置。\nroute_config:virtual_hosts:- name:hello_vhostdomains:[\u0026#34;hello.io\u0026#34;]routes:- match:prefix:\u0026#34;/api\u0026#34;route:cluster:hello_io_api- match:prefix:\u0026#34;/api/v1\u0026#34;route:cluster:hello_io_api_v1如果我们发送以下请求，哪个路由 / 集群被选中？\ncurl hello.io/api/v1 第一个设置集群 hello_io_api的路由被匹配。这是因为匹配是按照前缀的顺序进行评估的。然而，我们可能错误地期望前缀为 /api/v1 的路由被匹配。为了解决这个问题，我们可以调换路由的顺序，或者使用不同的匹配规则。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"4629630f7fb75d52ba360b1536a1f162","permalink":"https://lib.jimmysong.io/envoy-handbook/hcm/http-routing/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/envoy-handbook/hcm/http-routing/","section":"envoy-handbook","summary":"前面提到的路由器过滤器（envoy.filters.http.router）就是实现 HTTP 转发的。路由器过滤器几乎被用于所有的 HTTP 代理方案中。路由器过滤器的主要工作是查看路由表，并对请求进行相应的路由（转发","tags":["Envoy"],"title":"HTTP 路由","type":"book"},{"authors":null,"categories":["安全"],"content":"作者\nCybersecurity and Infrastructure Security Agency (CISA)\nNational Security Agency (NSA) Cybersecurity Directorate Endpoint Security\n联系信息\n客户要求 / 一般网络安全问题。\n网络安全需求中心，410-854-4200，Cybersecurity_Requests@nsa.gov。\n媒体咨询 / 新闻台\n媒体关系，443-634-0721，MediaRelations@nsa.gov。\n关于事件响应资源，请联系 CISA：CISAServiceDesk@cisa.dhs.gov。\n中文版\n关于本书中文版的信息请联系 Jimmy Song：jimmysong@jimmysong.io。\n宗旨\n国家安全局和 CISA 制定本文件是为了促进其各自的网络安全，包括其制定和发布网络安全规范和缓解措施的责任。这一信息可以被广泛分享，以触达所有适当的利益相关者。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"0e827effb7fe83fe8cfc65cdb9b42e99","permalink":"https://lib.jimmysong.io/kubernetes-hardening-guidance/publication-information/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/kubernetes-hardening-guidance/publication-information/","section":"kubernetes-hardening-guidance","summary":"作者 Cybersecurity and Infrastructure Security Agency (CISA) National Security Agency (NSA) Cybersecurity Directorate Endpoint Security 联系信息 客户要求 / 一般网络安全问题。 网络安全需求中心，410-854-4200，Cybersecurity_Requests@nsa.gov。 媒体咨询 / 新闻台 媒体关系，","tags":["Kubernetes","安全"],"title":"出版信息","type":"book"},{"authors":null,"categories":["云原生"],"content":"基础设施技术的历史向来引人入胜。由于基础设施的规模巨大，它已经经历了一次快速的颠覆性变革。除了计算机和互联网的早期，基础设施可谓日新月异。这些创新使基础架构更快，更可靠，更有价值。\n有些公司的人将基础设施推到了极限，他们已经找到了自动化和抽象的方法，提取基础设施更多商业价值。通过提供灵活的可用资源，他们将曾经是昂贵的成本中心转变为所需的商业公用事业。\n然而，公共事业公司很少为企业提供财务价值，这意味着基础设施往往被忽略并被视为不必要的成本。这使得投入创新或改进的时间和金钱很少。\n这样一个如此简单且令人着迷的业务栈怎么可以被轻易忽略？当基础设施出现故障时，业务显然会受到重视，那么为什么基础设施很难改善呢？\n基础设施已经达到了使消费者都感到无聊的成熟度。然而，它的潜力和新挑战又激发了实施者和工程师们新的激情。\n扩展基础设施并使用新的业务方式让来自不同行业的工程师都能找到解决方案。开源软件（OSS）和社区间的互相协作，这股力量又促使了创新的激增。\n如果管理得当，今天基础设施和应用方面的挑战将会不一样。这使得基础设施建设者和维护人员可以取得进展并开展新的有意义的工作。\n有些公司克服了诸如可扩展性、可靠性和灵活性等挑战。他们创建并封装了一些项目，封装了可供他人遵循的模式。这些模式有时很容易被实现者发现，但在其他情况下，它们不太明显。\n在本书中，我们将分享来自云原生技术前沿的公司的经验教训，使您能够有效解决可靠运行可伸缩应用程序的问题。现代商业发展非常迅速。本书中的模式将使您的基础设施能够跟上业务的速度和敏捷性需求。更重要的是，我们会让您自行决定何时采用这些模式。\n这些模式中有很多都已经在开源项目中得到了体现。其中一些项目由云原生计算基金会（CNCF）维护。这些项目和基金会并不是模式的唯一体现，但忽视它们会让你失去理智。以它们为例，但要自己进行尽职调查，以审核您所采用的每个解决方案。\n我们将向您展示云原生基础设施的益处以及可扩展系统和应用程序的基本模式。我们将向您展示如何测试您的基础设施，以及如何创建一个可以适应您需求的灵活的基础设施。您将了一些重要方面和未来发展。\n这本书可以激励你继续前进并自由分享你在社区中学到的知识。\n谁应该读这本书 如果您是开发基础设施或基础设施管理工具的工程师，那么本书就是为您准备的。本书将帮助您了解创建旨在在云环境中运行的基础设施的模式、流程和实践。通过了解应该怎么做，您可以更好地了解应用程序的作用，以及应该何时构建基础设施或使用云服务。\n应用程序工程师从本书中还可以发现哪些服务应该是其应用程序的一部分，哪些服务应该由基础设施提供。通过本书了解应用开发者和管理基础设施的工程师应该共同承担的责任。\n希望提升技能并系统地在设计基础设施和维护云网关基础设施方面发挥更大作用的系统管理员也可以从本书中学到很多。\n你是否在公有云中运行所有的基础设施？本书将帮助您了解何时使用云服务以及何时构建自己的抽象或服务。\n运行在数据中心还是本地云？我们将概述现代应用对基础设施的期望，并将帮助您了解利用当前投资的必要服务。\n这本书不是一本教程，除了给出实现示例之外，我们没有指出特定的产品。对于经理、董事和高管来说，这可能太过技术性，但可能会有所帮助，具体取决于该角色的参与和技术专长。\n最重要的是，如果您想了解基础设施如何影响业务，请阅读本书，以及如何创建经证实可为具备全球互联网运营规模的企业服务的基础设施。即使您的应用程序不需要扩展到这种规模，如果您的基础设施是使用此处描述的模式构建的，并且考虑到灵活性和可操作性，这本书仍然值得一读。\n为什么我们写了这本书 我们希望通过专注于模式和实践而不是特定产品和供应商来帮助您了解。当前存在太多的解决方案而人们不了解它们本身到底要解决什么问题。\n我们相信通过云原生应用程序管理云原生基础设施的好处，并且我们希望所有人都具有这种意识。\n我们希望回馈社区，推动行业向前发展。我们发现这样做的最好方式是解释业务和基础设施之间的关系，阐明问题并解释发现它们的工程师和组织所做的解决方案。\n以不涉及产品的方式解释模式并不总是很容易，但了解产品存在的原因很重要。我们经常使用产品作为模式的例子，但只有当它们会帮助您提供解决方案的实施示例时才会提到。\n如果没有无数人数万小时的自愿编写代码，帮助他人以及投资社区，我们就不会走到这里。我们非常感谢那些帮助我们了解这些模式的人们，我们希望能够回馈并帮助下一代工程师。这本书就是我们表达谢意的方式。\n浏览本书 本书的组织结构如下：\n 第 1 章介绍云原生基础设施是什么以及我们如何走到当前这一步。 第 2 章可以帮助您决定是否以及何时采用后面章节中预先描述的模式。 第 3 章和第 4 章展示了应该如何部署基础设施以及如何编写应用程序来管理它。 第 5 章将教你如何从测试开始就设计可靠的基础设施。 第 6 章和第 7 章展示了如何管理基础设施和应用程序。 第 8 章总结并提供了一些有关未来发展的见解。  如果你像我们一样，不会从前到后完整看完本书。以下是关于本书主题的一些建议：\n 如果您是一位专注于创建和维护基础设施的工程师，您应该至少阅读第 3 章至第 6 章。 应用程序开发人员可以专注于第 4、5 和 7 章关于将基础架构工具开发为云原生应用程序。 所有不构建云原生基础设施的人都将从第 1、2、8 章中受益匪浅。  在线资源 您应该通过访问 CNCF 网站熟悉云原生计算基金会（CNCF）及其托管项目。本书中的许多项目都被用作示例。\n您还可以通过查看 CNCF 景观项目（参见图 P-1），了解项目这些项目的全局视图。\n云原生应用程序是从 Heroku 的 12 因素的定义开始的。我们会解释它们之间的相似之处，但你应该熟悉下 12 因素是什么（参见 http://12factor.net）。\n还有许多关于 DevOps 的书籍、文章和演讲。尽管本书不关注 DevOps 实践，但是在实现云原生基础设施时，如果没有 DevOps 规定的工具、实践和文化，将很难实现。\n   图 P-1. CNCF Landscape  致谢 Justin Garrison\n感谢 Beth、Logan、我的朋友、家人以及在此过程中支持我们的同事。感谢那些帮助我们的社区和社区领袖以及给予宝贵反馈的评论者。感谢 Kris 让这本书变得更好，感谢读者花点时间阅读本书并提高你的技能。\nKris Nova\n感谢 Allison、Bryan、Charlie、Justin、Kjersti、Meghann 和 Patrick 为我写这本书所作出的帮助。我爱你们，永远感激你们为我所做的一切。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"40c2928b0fc571caeaa8ab3c0a8d8e56","permalink":"https://lib.jimmysong.io/cloud-native-infra/introduction/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/cloud-native-infra/introduction/","section":"cloud-native-infra","summary":"基础设施技术的历史向来引人入胜。由于基础设施的规模巨大，它已经经历了一次快速的颠覆性变革。除了计算机和互联网的早期，基础设施可谓日新月异。这些创新使基础架构更快，更可靠，更有价值。 有些公司的人将基础设","tags":["云原生"],"title":"介绍","type":"book"},{"authors":null,"categories":["Envoy"],"content":"IT 行业正在向微服务架构和云原生解决方案发展。由于使用不同的技术开发了成百上千的微服务，这些系统可能变得复杂，难以调试。\n作为一个应用开发者，你考虑的是业务逻辑——购买产品或生成发票。然而，任何像这样的业务逻辑都会导致不同服务之间的多个服务调用。每个服务可能都有它的超时、重试逻辑和其他可能需要调整或微调的网络特定代码。\n如果在任何时候最初的请求失败了，就很难通过多个服务来追踪，准确地指出失败发生的地方，了解请求为什么失败。是网络不可靠吗？是否需要调整重试或超时？或者是业务逻辑问题或错误？\n服务可能使用不一致的跟踪和记录机制，使这种调试的复杂性增加。这些问题使你很难确定问题发生在哪里，以及如何解决。如果你是一个应用程序开发人员，而调试网络问题不属于你的核心技能，那就更是如此。\n将网络问题从应用程序堆栈中抽离出来，由另一个组件来处理网络部分，让调试网络问题变得更容易。这就是 Envoy 所做的事情。\n在每个服务实例旁边都有一个 Envoy 实例在运行。这种类型的部署也被称为 Sidecar 部署。Envoy 的另一种模式是边缘代理，用于构建 API 网关。\nEnvoy 和应用程序形成一个原子实体，但仍然是独立的进程。应用程序处理业务逻辑，而 Envoy 则处理网络问题。\n在发生故障的情况下，分离关注点可以更容易确定故障是来自应用程序还是网络。\n为了帮助网络调试，Envoy 提供了以下高级功能。\n进程外架构 Envoy 是一个独立的进程，旨在与每个应用程序一起运行 —— 也就是我们前面提到的 Sidecar 部署模式。集中配置的 Envoy 的集合形成了一个透明的服务网格。\n路由和其他网络功能的责任被推给了 Envoy。应用程序向一个虚拟地址（localhost）而不是真实地址（如公共 IP 地址或主机名）发送请求，不知道网络拓扑结构。应用程序不再承担路由的责任，因为该任务被委托给一个外部进程。\n与其让应用程序管理其网络配置，不如在 Envoy 层面上独立于应用程序管理网络配置。在一个组织中，这可以使应用程序开发人员解放出来，专注于应用程序的业务逻辑。\nEnvoy 适用于任何编程语言。你可以用 Go、Java、C++ 或其他任何语言编写你的应用程序，而 Envoy 可以在它们之间架起桥梁。Envoy 的行为是相同的，无论应用程序的编程语言或它们运行的操作系统是什么。\nEnvoy 还可以在整个基础设施中透明地进行部署和升级。这与为每个单独的应用程序部署库升级相比，后者可能是非常痛苦和耗时的。\n进程外架构是有益的，因为它使我们在不同的编程语言 / 应用堆栈中保持一致，我们可以免费获得独立的应用生命周期和所有的 Envoy 网络功能，而不必在每个应用中单独解决这些问题。\nL3/L4 过滤器结构 Envoy 是一个 L3/L4 网络代理，根据 IP 地址和 TCP 或 UDP 端口进行决策。它具有一个可插拔的过滤器链，可以编写你的过滤器来执行不同的 TCP/UDP 任务。\n过滤器链（Filter Chain） 的想法借鉴了 Linux shell，即一个操作的输出被输送到另一个操作中。例如：\nls -l | grep \u0026#34;Envoy*.cc\u0026#34; | wc -l Envoy 可以通过堆叠所需的过滤器来构建逻辑和行为，形成一个过滤器链。许多过滤器已经存在，并支持诸如原始 TCP 代理、UDP 代理、HTTP 代理、TLS 客户端认证等任务。Envoy 也是可扩展的，我们可以编写我们的过滤器。\nL7 过滤器结构 Envoy 支持一个额外的 HTTP L7 过滤器层。我们可以在 HTTP 连接管理子系统中插入 HTTP 过滤器，执行不同的任务，如缓冲、速率限制、路由 / 转发等。\n一流的 HTTP/2 支持 Envoy 同时支持 HTTP/1.1 和 HTTP/2，并且可以作为一个透明的 HTTP/1.1 到 HTTP/2 的双向代理进行操作。这意味着任何 HTTP/1.1 和 HTTP/2 客户端和目标服务器的组合都可以被桥接起来。即使你的传统应用没有通过 HTTP/2 进行通信，如果你把它们部署在 Envoy 代理旁边，它们最终也会通过 HTTP/2 进行通信。\n推荐在所有的服务间配置的 Envoy 使用 HTTP/2，以创建一个持久连接的网格，请求和响应可以在上面复用。\nHTTP 路由 当以 HTTP 模式操作并使用 REST 时，Envoy 支持路由子系统，能够根据路径、权限、内容类型和运行时间值来路由和重定向请求。在将 Envoy 作为构建 API 网关的前台 / 边缘代理时，这一功能非常有用，在构建服务网格（sidecar 部署模式）时，也可以利用这一功能。\ngRPC 准备就绪 Envoy 支持作为 gRPC 请求和响应的路由和负载均衡底层所需的所有 HTTP/2 功能。\n gRPC 是一个开源的远程过程调用（RPC）系统，它使用 HTTP/2 进行传输，并将协议缓冲区作为接口描述语言（IDL），它提供的功能包括认证、双向流和流量控制、阻塞 / 非阻塞绑定，以及取消和超时。\n 服务发现和动态配置 我们可以使用静态配置文件来配置 Envoy，这些文件描述了服务间通信方式。\n对于静态配置 Envoy 不现实的高级场景，Envoy 支持动态配置，在运行时自动重新加载配置。一组名为 xDS 的发现服务可以用来通过网络动态配置 Envoy，并为 Envoy 提供关于主机、集群 HTTP 路由、监听套接字和加密信息。\n健康检查 负载均衡器有一个特点，那就是只将流量路由到健康和可用的上游服务。Envoy 支持健康检查子系统，对上游服务集群进行主动健康检查。然后，Envoy 使用服务发现和健康检查信息的组合来确定健康的负载均衡目标。Envoy 还可以通过异常点检测子系统支持被动健康检查。\n高级负载均衡 Envoy 支持自动重试、断路、全局速率限制（使用外部速率限制服务）、影子请求（或流量镜像）、异常点检测和请求对冲。\n前端 / 边缘代理支持 Envoy 的特点使其非常适合作为边缘代理运行。这些功能包括 TLS 终端、HTTP/1.1、HTTP/2 和 HTTP/3 支持，以及 HTTP L7 路由。\nTLS 终止 应用程序和代理的解耦使网格部署模型中所有服务之间的 TLS 终止（双向 TLS）成为可能。\n一流的可观测性 为了便于观察，Envoy 会生成日志、指标和追踪。Envoy 目前支持 statsd（和兼容的提供者）作为所有子系统的统计。得益于可扩展性，我们也可以在需要时插入不同的统计提供商。\nHTTP/3（Alpha） Envoy 1.19.0 支持 HTTP/3 的上行和下行，并在 HTTP/1.1、HTTP/2 和 HTTP/3 之间进行双向转义。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"6bedec057ecb0868ce5739752a8d71b2","permalink":"https://lib.jimmysong.io/envoy-handbook/intro/what-is-envoy/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/envoy-handbook/intro/what-is-envoy/","section":"envoy-handbook","summary":"IT 行业正在向微服务架构和云原生解决方案发展。由于使用不同的技术开发了成百上千的微服务，这些系统可能变得复杂，难以调试。 作为一个应用开发者，你考虑的是业务逻辑——购买产品或生成发票。然而，任何像这样的业","tags":["Envoy"],"title":"什么是 Envoy？","type":"book"},{"authors":null,"categories":["安全"],"content":"云原生应用已经发展成为一个标准化的架构，由以下部分组成。\n 多个松散耦合的组件被称为微服务（通常或典型地以容器形式实现）。 一个应用服务基础设施，为用户、服务和设备提供安全通信、认证和授权等服务（例如，服务网格）。  由于安全、商业竞争力和其固有的结构（松散耦合的应用组件），这类应用需要一个不同的应用、部署和运行时监控范式 —— 统称为软件生命周期范式。DevSecOps（分别由开发、安全和运维的首字母缩写组成）是这些应用的开发、部署和运维的促进范式之一，其基本要素包括持续集成、持续交付和持续部署（CI/CD）管道。\nCI/CD 管道是将开发人员的源代码通过各个阶段的工作流程，如构建、功能测试、安全扫描漏洞、打包和部署，由带有反馈机制的自动化工具支持。在本文中，应用环境中涉及的整个源代码集被分为五种代码类型：\n 应用代码，它体现了执行一个或多个业务功能的应用逻辑。 应用服务代码，用于服务，如会话建立、网络连接等。 基础设施即代码，它是以声明性代码的形式存在的计算、网络和存储资源。 策略即代码，这是运行时策略（例如，零信任），以声明性代码的形式表达。 可观测性即代码，用于持续监测应用程序的健康状况，其中监测功能被表述为声明性代码。  因此，可以为所有五个代码类型创建单独的 CI/CD 管道。还描述了这些代码类型中的每一种所执行的功能，以强调它们在整个应用程序的执行中所发挥的作用。\n虽然云原生应用有一个共同的架构堆栈，但堆栈组件运行的平台可能有所不同。该平台是物理（裸机）或虚拟化（如 Kubernetes）上的一个抽象层。为了在本文中明确提及该平台或应用环境，它被称为 DevSecOps 原语参考平台，或简称为 参考平台。\n本文件的目的是为参考平台的 DevSecOps 原语的实施提供指导。本文还介绍了这种实施对高安全保障的好处，以及在管道内使用风险管理工具和仪表盘指标提供持续授权操作（C-ATO）的工件。\n 下一章   ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"2849a46d2a48fe0d150c196209b33882","permalink":"https://lib.jimmysong.io/service-mesh-devsecops/executive-summary/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/service-mesh-devsecops/executive-summary/","section":"service-mesh-devsecops","summary":"云原生应用已经发展成为一个标准化的架构，由以下部分组成。 多个松散耦合的组件被称为微服务（通常或典型地以容器形式实现）。 一个应用服务基础设施，为用户、服务和设备提供安全通信、认证和授权等服务（例如，服务","tags":["Service Mesh","DevSecOps"],"title":"执行摘要","type":"book"},{"authors":null,"categories":["云原生"],"content":"云原生一词已经被过度的采用，很多软件都号称是云原生，很多打着云原生旗号的会议也如雨后春笋般涌现。\n云原生本身甚至不能称为是一种架构，它首先是一种基础设施，运行在其上的应用称作云原生应用，只有符合云原生设计哲学的应用架构才叫云原生应用架构。\n云原生的设计理念 云原生系统的设计理念如下:\n 面向分布式设计（Distribution）：容器、微服务、API 驱动的开发； 面向配置设计（Configuration）：一个镜像，多个环境配置； 面向韧性设计（Resistancy）：故障容忍和自愈； 面向弹性设计（Elasticity）：弹性扩展和对环境变化（负载）做出响应； 面向交付设计（Delivery）：自动拉起，缩短交付时间； 面向性能设计（Performance）：响应式，并发和资源高效利用； 面向自动化设计（Automation）：自动化的 DevOps； 面向诊断性设计（Diagnosability）：集群级别的日志、metric 和追踪； 面向安全性设计（Security）：安全端点、API Gateway、端到端加密；  以上的设计理念很多都是继承自分布式应用的设计理念。虽然有如此多的理念但是我们仍然无法辨认什么样的设施才是云原生基础设施，不过可以先用排除法，我将解释什么不是云原生基础设施。\n什么不是云原生基础设施？ 云原生基础设施不等于在公有云上运行的基础设施。光是租用服务器并不会使您的基础设施云原生化。管理 IaaS 的流程与运维物理数据中心没什么两样，将现有架构迁移到云上也未必能获得回报。\n云原生不是指在容器中运行应用程序。Netflix 率先推出云原生基础设施时，几乎所有应用程序部署在虚拟机中，而不是在容器中。改变应用程序的打包方式并不意味着就会增加自治系统的可扩展性和优势。即使应用程序是通过 CI/CD 渠道自动构建和部署的，也不意味着您就可以从增强 API 驱动部署的基础设施中受益。\n这也并不意味着您只能运行容器编排器（例如 Kubernetes 和 Mesos）。容器编排器提供了云原生基础设施所需的许多平台功能，但并未按预期方式使用这些功能，这意味着您的应用程序会在一组服务器上运行，被动态调度。这是一个非常好的起步，但仍有许多工作要做。\n 调度器与编排器\n术语 “调度器” 和 “编排器” 通常可以互换使用。\n在大多数情况下，编排器负责集群中的所有资源利用（例如：存储，网络和 CPU）。该术语典型地用于描述执行许多任务的产品，如健康检查和云自动化。\n 调度器是编排平台的一个子集，仅负责选择运行在每台服务器上的进程和服务。\n云原生不是微服务或基础设施即代码。微服务意味着更快的开发周期和更小的独特功能，但是单体应用程序可以具有相同的功能，使其能够通过软件有效管理，并且还可以从云原生基础设施中受益。\n基础设施即代码以机器可解析语言或领域特定语言（DSL）定义、自动化您的基础设施。将代码应用于基础架构的传统工具包括配置管理工具（例如 Chef 和 Puppet）。这些工具在自动执行任务和提供一致性方面有很大帮助，但是它们在提供必要的抽象来描述超出单个服务器的基础设施方面存在缺陷。\n配置管理工具一次自动化一台服务器，并依靠人员将服务器提供的功能绑定在一起。这将人类定位为基础设施规模的潜在瓶颈。这些工具也不会使构建完整系统所需的云基础设施（例如存储和网络）的额外部分自动化。\n尽管配置管理工具为操作系统的资源（例如软件包管理器）提供了一些抽象，但它们并没有抽象出足够的底层操作系统来轻松管理它。如果一位工程师想要管理系统中的每个软件包和文件，这将是一个非常艰苦的过程，并且对于每个配置变体都是独一无二的。同样，定义不存在或不正确的资源的配置管理仅消耗系统资源并且不能提供任何价值。\n虽然配置管理工具可以帮助自动化部分基础设施，但它们无法更好地管理应用程序。我们将在后面的章节中通过查看部署，管理，测试和操作基础架构的流程，探讨云原生基础设施的不同之处，但首先，我们将了解哪些应用程序是成功的以及应该何时与原生基础设施一起使用。\n云原生应用程序 就像云改变了业务和基础设施之间的关系一样，云原生应用程序也改变了应用程序和基础设施之间的关系。我们需要了解与传统应用程序相比，云本身有什么不同，因此我们需要了解它们与基础设施的新关系。\n为了写好本书，也为了有一个共享词汇表，我们需要定义 “云原生应用程序” 是什么意思。云原生与 12 因素应用程序不同，即使它们可能共享一些类似的特征。如果你想了解更多细节，请阅读 Kevin Hoffman 撰写的 “超越 12 因素应用程序”（O’Reilly，2012）。\n云原生应用程序被设计为在平台上运行，并设计用于弹性，敏捷性，可操作性和可观测性。弹性包含失败而不是试图阻止它们；它利用了在平台上运行的动态特性。敏捷性允许快速部署和快速迭代。可操作性从应用程序内部控制应用程序生命周期，而不是依赖外部进程和监视器。可观测性提供信息来回答有关应用程序状态的问题。\n 云原生定义\n云原生应用程序的定义仍在发展中。还有像 CNCF 这样的组织可以提供其他的定义。\n 云原生应用程序通过各种方法获取这些特征。它通常取决于应用程序的运行位置以及企业流程和文化。以下是实现云原生应用程序所需特性的常用方法：\n 微服务 健康报告 遥测数据 弹性 声明式的，而不是命令式的  微服务 作为单个实体进行管理和部署的应用程序通常称为单体应用。最初开发应用程序时，单体有很多好处。它们更易于理解，并允许您在不影响其他服务的情况下更改主要功能。\n随着应用程序复杂性的增长，单体应用的益处逐渐减少。它们变得更难理解，而且失去了敏捷性，因为工程师很难推断和修改代码。\n对付复杂性的最好方法之一是将明确定义的功能分成更小的服务，并让每个服务独立迭代。这增加了应用程序的灵活性，允许根据需要更轻松地更改部分应用程序。每个微服务可以由单独的团队进行管理，使用适当的语言编写，并根据需要进行独立扩缩容。\n只要每项服务都遵守强有力的合约，应用程序就可以快速改进和改变。当然，转向微服务架构还有许多其他的考虑因素。其中最不重要的是弹性通信，我们在附录 A 中有讨论。\n我们无法考虑转向微服务的所有考虑因素。拥有微服务并不意味着您拥有云原生基础设施。如果您想阅读更多，我们推荐 Sam Newman 的 Building Microservices（O’Reilly，2015）。虽然微服务是实现您的应用程序灵活性的一种方式，但正如我们之前所说的，它们不是云原生应用程序的必需条件。\n健康报告  停止逆向工程应用程序并开始从内部进行监控。 —— Kelsey Hightower，Monitorama PDX 2016：healthz\n 没有人比开发人员更了解应用程序需要什么才能以健康的状态运行。很长一段时间，基础设施管理员都试图从他们负责运行的应用程序中找出 “健康” 该怎么定义。如果不实际了解应用程序的健康状况，他们尝试在应用程序不健康时进行监控并发出警报，这往往是脆弱和不完整的。\n为了提高云原生应用程序的可操作性，应用程序应该暴露健康检查。开发人员可以将其实施为命令或过程信号，以便应用程序在执行自我检查之后响应，或者更常见的是：通过应用程序提供 Web 服务，返回 HTTP 状态码来检查健康状态。\n Google Borg 示例\nGoogle 的 Borg 报告中列出了一个健康报告的例子：\n几乎每个在 Borg 下运行的任务都包含一个内置的 HTTP 服务器，该服务器发布有关任务运行状况和数千个性能指标（如 RPC 延迟）的信息。Borg 会监控运行状况检查 URL 并重新启动不及时响应或返回 HTTP 错误代码的任务。其他数据由监控工具跟踪，用于仪表板和服务级别目标（SLO）违规警报。\n 将健康责任转移到应用程序中使应用程序更容易管理和自动化。应用程序应该知道它是否正常运行以及它依赖于什么（例如，访问数据库）来提供业务价值。这意味着开发人员需要与产品经理合作来定义应用服务的业务功能并相应地编写测试。\n提供健康检查的应用程序示例包括 Zookeeper 的 ruok 命令和 etcd 的 HTTP / 健康端点。\n应用程序不仅仅有健康或不健康的状态。它们将经历一个启动和关闭过程，在这个过程中它们应该通过健康检查，报告它们的状态。如果应用程序可以让平台准确了解它所处的状态，平台将更容易知道如何操作它。\n一个很好的例子就是当平台需要知道应用程序何时可以接收流量。在应用程序启动时，如果它不能正确处理流量，它就应该表现为未准备好。此额外状态将防止应用程序过早终止，因为如果运行状况检查失败，平台可能会认为应用程序不健康，并且会反复停止或重新启动它。\n应用程序健康只是能够自动化应用程序生命周期的一部分。除了知道应用程序是否健康之外，您还需要知道应用程序是否正在进行哪些工作。这些信息来自遥测数据。\n遥测数据 遥测数据是进行决策所需的信息。确实，遥测数据可能与健康报告重叠，但它们有不同的用途。健康报告通知我们应用程序生命周期状态，而遥测数据通知我们应用程序业务目标。\n您测量的指标有时称为服务级指标（SLI）或关键性能指标（KPI）。这些是特定于应用程序的数据，可以确保应用程序的性能处于服务级别目标（SLO）内。如果您需要更多关于这些术语的信息以及它们与您的应用程序、业务需求的关系，我们推荐你阅读来自 Site Reliability Engineering（O’Reilly）的第 4 章。\n遥测和度量标准用于解决以下问题：\n 应用程序每分钟收到多少请求？ 有没有错误？ 什么是应用程序延迟？ 订购需要多长时间？  通常会将数据刮取或推送到时间序列数据库（例如 Prometheus 或 InfluxDB）进行聚合。遥测数据的唯一要求是它将被收集数据的系统格式化。\n至少，可能最好实施度量标准的 RED 方法，该方法收集应用程序的速率，错误和执行时间。\n请求率\n收到了多少个请求\n错误\n应用程序有多少错误\n时间\n多久才能收到回复\n遥测数据应该用于提醒而非健康监测。在动态的、自我修复的环境中，我们更少关注单个应用程序实例的生命周期，更多关注关于整体应用程序 SLO 的内容。健康报告对于自动应用程序管理仍然很重要，但不应该用于页面工程师。\n如果 1 个实例或 50 个应用程序不健康，只要满足应用程序的业务需求，我们可能不会收到警报。度量标准可让您知道您是否符合您的 SLO，应用程序的使用方式以及对于您的应用程序来说什么是 “正常”。警报有助于您将系统恢复到已知的良好状态。\n 如果它移动，我们跟踪它。有时候我们会画出一些尚未移动的图形，以防万一它决定为它运行。\n——Ian Malpass，衡量所有，衡量一切\n 警报也不应该与日志记录混淆。记录用于调试，开发和观察模式。它暴露了应用程序的内部功能。度量有时可以从日志（例如错误率）计算，但需要额外的聚合服务（例如 ElasticSearch）和处理。\n弹性 一旦你有遥测和监测数据，你需要确保你的应用程序对故障有适应能力。弹性是基础设施的责任，但云原生应用程序也需要承担部分工作。\n基础设施被设计为抵制失败。硬件用于需要多个硬盘驱动器，电源以及全天候监控和部件更换以保持应用程序可用。使用云原生应用程序，应用程序有责任接受失败而不是避免失败。\n 在任何平台上，尤其是在云中，最重要的特性是其可靠性。\n——David Rensin，e ARCHITECT Show：来自 Google 的关于云计算的速成课程\n 设计具有弹性的应用程序可能是整本书本身。我们将在云原生应用程序中考虑弹性的两个主要方面：为失败设计和优雅降级。\n为失败设计 唯一永远不会失败的系统是那些让你活着的系统（例如心脏植入物和刹车系统）。如果您的服务永远不会停止运行，您需要花费太多时间设计它们来抵制故障，并且没有足够的时间增加业务价值。您的 SLO 确定服务需要多长时间。您花费在工程设计上超出 SLO 的正常运行时间的任何资源都将被浪费掉。\n您应该为每项服务测量两个值，即平均无故障时 …","date":1651532400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"842cb1b8fa04f74755f40de312e068ba","permalink":"https://lib.jimmysong.io/cloud-native-handbook/intro/cloud-native-philosophy/","publishdate":"2022-05-03T00:00:00+01:00","relpermalink":"/cloud-native-handbook/intro/cloud-native-philosophy/","section":"cloud-native-handbook","summary":"云原生一词已经被过度的采用，很多软件都号称是云原生，很多打着云原生旗号的会议也如雨后春笋般涌现。 云原生本身甚至不能称为是一种架构，它首先是一种基础设施，运行在其上的应用称作云原生应用，只有符合云原生设","tags":["云原生"],"title":"云原生的设计哲学","type":"book"},{"authors":null,"categories":["云原生"],"content":"云原生社区是由 宋净超（Jimmy Song） 于 2020 年 5 月发起的，企业中立的云原生终端用户社区。社区秉持 “共识、共治、共建、共享” 的原则。社区的宗旨是：连接、中立、开源。立足中国，面向世界，企业中立，关注开源，回馈开源。了解更多请访问云原生社区官网：https://cloudnative.to。\n成立背景  Software is eating the world. —— Marc Andreessen\n “软件正在吞噬这个世界” 已被大家多次引用，随着云原生（Cloud Native）的崛起，我们想说的是 “Cloud Native is eating the software”。随着越来越多的企业将服务迁移上云，企业原有的开发模式以及技术架构已无法适应云的应用场景，其正在被重塑，向着云原生的方向演进。\n那么什么是云原生？云原生是一系列架构、研发流程、团队文化的最佳实践组合，以此支撑更快的创新速度、极致的用户体验、稳定可靠的用户服务、高效的研发效率。开源社区与云原生的关系密不可分，正是开源社区尤其是终端用户社区的存在，极大地促进了以容器、服务网格、微服务等为代表的云原生技术的持续演进！\n随着云计算的不断发展，云原生技术在全球范围内变得越来越受关注，同时国内社区同学也展现了对云原生技术热爱。近些年中国已经孕育众多的云原生技术爱好者，也有自发组织的一些相关技术交流和 meetup，同时在云原生领域也涌现了众多优秀的开源项目，在这样的背景下，一个有理想，有组织，有温度的云原生社区应运而生。\n加入社区 加入云原生社区，你将获得：\n 更接近源头的知识资讯 更富有价值的人际网络 更专业个性的咨询解答 更亲近意见领袖的机会 更快速高效的个人成长 更多知识分享曝光机会 更多行业人才挖掘发现  关注云原生社区微信公众号，进入公众号后台，点击 “加入我们”。\n   云原生社区公众号  参考  云原生社区成立 - cloudnative.to  ","date":1651532400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"52cb0c5df772be0873a31803666ed388","permalink":"https://lib.jimmysong.io/cloud-native-handbook/community/cnc/","publishdate":"2022-05-03T00:00:00+01:00","relpermalink":"/cloud-native-handbook/community/cnc/","section":"cloud-native-handbook","summary":"云原生社区是由 宋净超（Jimmy Song） 于 2020 年 5 月发起的，企业中立的云原生终端用户社区。社区秉持 “共识、共治、共建、共享” 的原则。社区的宗旨是：连接、中立、开源。立足中国，面向世界，企业中立，关注开","tags":["云原生"],"title":"云原生社区（中国）","type":"book"},{"authors":null,"categories":["云原生"],"content":"Istio 是一个服务网格的开源实现。Istio 支持以下功能。\n流量管理\n利用配置，我们可以控制服务间的流量。设置断路器、超时或重试都可以通过简单的配置改变来完成。\n可观测性\nIstio 通过跟踪、监控和记录让我们更好地了解你的服务，它让我们能够快速发现和修复问题。\n安全性\nIstio 可以在代理层面上管理认证、授权和通信的加密。我们可以通过快速的配置变更在各个服务中执行政策。\nIstio 组件 Istio 服务网格有两个部分：数据平面和控制平面。\n在构建分布式系统时，将组件分离成控制平面和数据平面是一种常见的模式。数据平面的组件在请求路径上，而控制平面的组件则帮助数据平面完成其工作。\nIstio 中的数据平面由 Envoy 代理组成，控制服务之间的通信。网格的控制平面部分负责管理和配置代理。\n   Istio 架构  Envoy（数据平面） Envoy 是一个用 C++ 开发的高性能代理。Istio 服务网格将 Envoy 代理作为一个 sidecar 容器注入到你的应用容器旁边。然后该代理拦截该服务的所有入站和出站流量。注入的代理一起构成了服务网格的数据平面。\nEnvoy 代理也是唯一与流量进行交互的组件。除了前面提到的功能 —— 负载均衡、断路器、故障注入等。Envoy 还支持基于 WebAssembly（WASM）的可插拔扩展模型。这种可扩展性使我们能够执行自定义策略，并为网格中的流量生成遥测数据。\nIstiod（控制平面） Istiod 是控制平面组件，提供服务发现、配置和证书管理功能。Istiod 采用 YAML 编写的高级规则，并将其转换为 Envoy 的可操作配置。然后，它把这个配置传播给网格中的所有 sidecar。\nIstiod 内部的 Pilot 组件抽象出特定平台的服务发现机制（Kubernetes、Consul 或 VM），并将其转换为 sidecar 可以使用的标准格式。\n使用内置的身份和凭证管理，我们可以实现强大的服务间和终端用户认证。通过授权功能，我们可以控制谁可以访问你的服务。\n控制平面的部分以前被称为 Citadel，作为一个证书授权机构，生成证书，允许数据平面中的代理之间进行安全的 mTLS 通信。\n","date":1651507200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"c196304bc2929f635aebd100c0e93b23","permalink":"https://lib.jimmysong.io/cloud-native-handbook/service-mesh/what-is-istio/","publishdate":"2022-05-03T00:00:00+08:00","relpermalink":"/cloud-native-handbook/service-mesh/what-is-istio/","section":"cloud-native-handbook","summary":"Istio 是一个服务网格的开源实现。Istio 支持以下功能。 流量管理 利用配置，我们可以控制服务间的流量。设置断路器、超时或重试都可以通过简单的配置改变来完成。 可观测性 Istio 通过跟踪、监控和记录让我们更好地了解你的","tags":["云原生"],"title":"什么是 Istio?","type":"book"},{"authors":null,"categories":["可观测性"],"content":"可观测性行业正处在巨变中。\n传统上，我们观察系统时使用的是一套筒仓式的、独立的工具，其中大部分包含结构不良（或完全非结构化）的数据。这些独立的工具也是垂直整合的。工具、协议和数据格式都属于一个特定的后端或服务，不能互换。这意味着更换或采用新的工具需要耗费时间来更换整个工具链，而不仅仅是更换后端。\n这种孤立的技术格局通常被称为可观测性的 “三大支柱”：日志、度量和（几乎没有）追踪（见图 1-1）。\n日志\n记录构成事务的各个事件。\n度量\n记录构成一个事务的事件的集合。\n追踪\n测量操作的延迟和识别事务中的性能瓶颈，或者类似的东西。传统上，许多组织并不使用分布式追踪，许多开发人员也不熟悉它。\n   图 1-1：可观测性的 “三大支柱”。  我们用这种方法工作了很久，以致于我们不常质疑它。但正如我们将看到的，“三大支柱” 并不是一种正确的结构化的可观测性方法。事实上，这个术语只是描述了某些技术碰巧被实现的方式，它掩盖了关于如何实际使用我们的工具的几个基本事实。\n什么是事务和资源？ 在我们深入探讨不同可观测性范式的利弊之前，重要的是要定义我们所观察的是什么。我们最感兴趣的分布式系统是基于互联网的服务。这些系统可以被分解成两个基本组成部分：事务和资源。\n事务（transaction） 代表了分布式系统需要执行的所有动作，以便服务能够做一些有用的事情。加载一个网页，购买一个装满物品的购物车，订购一个共享汽车：这些都是事务的例子。重要的是要理解，事务不仅仅是数据库事务。对每个服务的每个请求都是事务的一部分。\n例如，一个事务可能从一个浏览器客户端向一个网络代理发出 HTTP 请求开始。代理首先向认证系统发出请求以验证用户，然后将请求转发给前端应用服务器。应用服务器向各种数据库和后端系统发出若干请求。例如，一个消息系统：Kafka 或 AMQP，被用来排队等待异步处理的额外工作。所有这些工作都必须正确完成，以便向急切等待的用户提供结果。如果任何部分失败或耗时过长，其结果就是糟糕的体验，所以我们需要全面理解事务。\n一路下来，所有这些事务都在消耗资源（resource）。网络服务只能处理这么多的并发请求，然后它们的性能就会下降并开始失效。这些服务可以扩大规模，但它们与可能调用锁的数据库互动，造成瓶颈。数据库读取记录的请求可能会阻碍更新该记录的请求，反之亦然。此外，所有这些资源都是要花钱的，在每个月的月底，你的基础设施供应商会给你发账单。你消耗的越多，你的账单就越长。\n如何解决某个问题或提高服务质量？要么是开发人员修改事务，要么是运维人员修改可用资源。就这样了。这就是它的全部内容。当然，魔鬼就在细节中。\n第四章将详细介绍可观测性工具表示事务和资源的理想方式。但是，让我们回到描述迄今为止我们一直在做的非理想的方式。\n 不是所有事情都是事务\n请注意，除了跨事务之外，还有其他的计算模型。例如，桌面和移动应用程序通常是基于反应器（reactor） 模型，用户在很长一段时间内连续互动。照片编辑和视频游戏就是很好的例子，这些应用有很长的用户会话，很难描述为离散的事务。在这些情况下，基于事件的可观测性工具，如真实用户监控（RUM），可以增强分布式追踪，以更好地描述这些长期运行的用户会话。\n也就是说，几乎所有基于互联网的服务都是建立在事务模式上的。由于这些是我们关注的服务，观察事务是我们在本书中描述的内容。附录 B 中对 RUM 进行了更详细的描述。\n 可观测性的三大支柱 考虑到事务和资源，让我们来看看三大支柱模式。将这种关注点的分离标记为 “三大支柱”，听起来是有意的，甚至是明智的。支柱听起来很严肃，就像古代雅典的帕特农神庙。但这种安排实际上是一个意外。计算机的可观测性由多个孤立的、垂直整合的工具链组成，其真正的原因只是一个平庸的故事。\n这里有一个例子。假设有一个人想了解他们的程序正在执行的事务。所以他建立了一个日志工具：一个用于记录包含时间戳的消息的接口，一个用于将这些消息发送到某个地方的协议，以及一个用于存储和检索这些消息的数据系统。这足够简单。\n另一个人想监测在任何特定时刻使用的所有资源；他想捕捉指标。那么，他不会为此使用日志系统，对吗？他想追踪一个数值是如何随时间变化的，并跨越一组有限的维度。很明显，一大堆非结构化的日志信息与这个问题没有什么关系。因此，一个新的、完全独立的系统被创造出来，解决了生成、传输和存储度量的具体问题。\n另一个人想要识别性能瓶颈。同样，日志系统的非结构化性质使它变得无关紧要。识别性能瓶颈，比如一连串可以并行运行的操作，需要我们知道事务中每个操作的持续时间以及这些操作是如何联系在一起的。因此，我们建立了一个完全独立的追踪系统。由于新的追踪系统并不打算取代现有的日志系统，所以追踪系统被大量抽样，以限制在生产中运行第三个可观测系统的成本。\n这种零敲碎打的可观测性方法是人类工程的一个完全自然和可理解的过程。然而，它有其局限性，不幸的是，这些局限性往往与我们在现实世界中使用（和管理）这些系统的方式相悖。\n实际上我们如何观察系统？ 让我们来看看运维人员在调查问题时所经历的实际的、不加修饰的过程。\n调查包括两个步骤：注意到有事情发生，然后确定是什么原因导致了它的发生。\n当我们执行这些任务时，我们使用可观测性工具。但是，重要的是，我们并不是孤立地使用每一个工具。我们把它们全部放在一起使用。而在整个过程中，这些工具的孤立性给操作者带来了巨大的认知负担。\n通常情况下，当有人注意到一个重要的指标变得歪七扭八时，调查就开始了。在这种情况下，“毛刺” 是一个重要的术语，因为运维人员在这一点上所掌握的唯一信息是仪表盘上一条小线的形状，以及他们自己对该线的形状是否看起来 “正确” 的内部评估（图 1-2）。\n   图 1-2：传统上，寻找不同数据集之间的关联性是一种可怕的经历。  在确定了线的形状开始看起来 “不对劲” 的那一点后，运维人员将眯起眼睛，试图找到仪表板上同时出现 “毛刺” 的其他线。然而，由于这些指标是完全相互独立的，运维人员必须在他们的大脑中进行比较，而不需要计算机的帮助。\n不用说，盯着图表，希望找到一个有用的关联，需要时间和脑力，更不用说会导致眼睛疲劳。\n就个人而言，我会用一把尺子或一张纸，只看什么东西排成一排。在 “现代” 仪表盘中，标尺现在是作为用户界面的一部分而绘制的线。但这只是一个粗略的解决方案。识别相关性的真正工作仍然必须发生在操作者的头脑中，同样没有计算机的帮助。\n在对问题有了初步的、粗略的猜测后，运维人员通常开始调查他们认为可能与问题有关的事务（日志）和资源（机器、进程、配置文件）（图 1-3）。\n   图 1-3：找到与异常情况相关的日志也是一种可怕的经历。  在这里，计算机也没有真正的帮助。日志存储在一个完全独立的系统中，不能与任何指标仪表板自动关联。配置文件和其他服务的具体信息通常不在任何系统中，运维人员必须通过 SSH 或其他方式访问运行中的机器来查看它们。\n因此，运维人员再次被留下寻找相关性的工作，这次是在指标和相关日志之间。识别这些日志可能很困难；通常必须查阅源代码才能了解可能存在的日志。\n当找到一个（可能是，希望是）相关的日志，下一步通常是确定导致这个日志产生的事件链。这意味着要找到同一事务中的其他日志。\n缺乏关联性给操作者带来了巨大的负担。非结构化和半结构化的日志系统没有自动索引和按事务过滤日志的机制。尽管这是迄今为止运维人员最常见的日志工作流程，但他们不得不执行一系列特别的查询和过滤，将可用的日志筛选成一个子集，希望能代表事务的近似情况。为了成功，他们必须依靠应用程序开发人员来添加各种请求 ID 和记录，以便日后找到并拼接起来。\n在一个小系统中，这种重建事务的过程是乏味的，但却是可能的。但是一旦系统发展到包括许多横向扩展的服务，重建事务所需的时间就开始严重限制了调查的范围。图 1-4 显示了一个涉及许多服务的复杂事务。你将如何收集所有的日志？\n   图 1-4：在传统的日志记录中，找到构成一个特定事务的确切日志需要花费大量的精力。如果系统变得足够大，这几乎成为不可能。  分布式追踪是一个好的答案。它实际上拥有自动重建一个事务所需的所有 ID 和索引工具。不幸的是，追踪系统经常被看作是用于进行延迟分析的利基工具。因此，发送给它们的日志数据相对较少。而且，由于它们专注于延迟分析，追踪系统经常被大量采样，使得它们与这类调查无关。\n不是三根柱子，而是一股绳子 不用说，这是一个无奈之举。上述工作流程确实代表了一种可怕的状态。但是，由于我们已经在这种技术体制下生活了这么久，我们往往没有认识到，与它可以做到的相比，它实际上是多么低效。\n今天，为了了解系统是如何变化的，运维人员必须首先收集大量的数据。然后，他们必须根据仪表盘显示和日志扫描等视觉反馈，用他们的头脑来识别这些数据的相关性。这是一种紧张的脑力劳动。如果一个计算机程序能够自动扫描和关联这些数据，那么这些脑力劳动就是不必要的。如果运维人员能够专注于调查他们的系统是如何变化的，而不需要首先确定什么在变化，那么他们将节省大量宝贵的时间。\n在编写一个能够准确执行这种变化分析的计算机程序之前，所有这些数据点都需要被连接起来。日志需要被连接在一起，以便识别事务。衡量标准需要与日志联系在一起，这样产生的统计数据就可以与它们所测量的事务联系起来。每个数据点都需要与底层系统资源 —— 软件、基础设施和配置细节相关联，以便所有事件都能与整个系统的拓扑结构相关联。\n最终的结果是一个单一的、可遍历的图，包含了描述分布式系统状态所需的所有数据，这种类型的数据结构将给分析工具一个完整的系统视图。与其说是不相连的数据的 “三根支柱”，不如说是相互连接的数据的一股绳子。\nOpenTelemetry 因此诞生。如图 1-5 所示，OpenTelemetry 是一个新的遥测系统，它以一种综合的方式生成追踪、日志和指标。所有这些连接的数据点以相同的协议一起传输，然后可以输入计算机程序，以确定整个数据集的相关性。\n   图 1-5：OpenTelemetry 将所有这些孤立的信息，作为一个单一的、高度结构化的数据流连接起来。  这种统一的数据是什么样子的？在下一章，我们将把这三个支柱放在一边，从头开始建立一个新的模型。\n","date":1643990400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"2abc151a6ddc9d594b442810ef231cfe","permalink":"https://lib.jimmysong.io/opentelemetry-obervability/history/","publishdate":"2022-02-05T00:00:00+08:00","relpermalink":"/opentelemetry-obervability/history/","section":"opentelemetry-obervability","summary":"第 1 章：可观测性的历史","tags":["OpenTelemetry"],"title":"第 1 章：可观测性的历史","type":"book"},{"authors":null,"categories":["文化"],"content":"注意：在考虑这些要点时，请谨记 “Code Review 标准”。\n设计 审查中最重要的是 CL 的整体设计。CL 中各种代码的交互是否有意义？此变更是属于您的代码库（codebase）还是属于库（library）？它是否与您系统的其他部分很好地集成？现在是添加此功能的好时机吗？\n功能 这个 CL 是否符合开发者的意图？开发者的意图对代码的用户是否是好的？ “用户”通常都是最终用户（当他们受到变更影响时）和开发者（将来必须“使用”此代码）。\n大多数情况下，我们希望开发者能够很好地测试 CL，以便在审查时代码能够正常工作。但是，作为审查者，仍然应该考虑边缘情况，寻找并发问题，尝试像用户一样思考，并确保您单纯透过阅读方式审查时，代码没有包含任何 bug。\n当要检查 CL 的行为会对用户有重大影响时，验证 CL 的变化将变得十分重要。例如 UI 变更。当您只是阅读代码时，很难理解某些变更会如何影响用户。如果在 CL 中打 patch 或自行尝试这样的变更太不方便，您可以让开发人员为您提供功能演示。\n另一个在代码审查期间特别需要考虑功能的时机，就是如果 CL 中存在某种并行编程，理论上可能导致死锁或竞争条件。通过运行代码很难检测到这些类型的问题，并且通常需要某人（开发者和审查者）仔细思考它们以确保不会引入问题。 （请注意，这也是在可能出现竞争条件或死锁的情况下，不使用并发模型的一个很好的理由——它会使代码审查或理解代码变得非常复杂。）\n复杂度 CL 是否已经超过它原本所必须的复杂度？针对任何层级的 CL 请务必确认这点——每行程序是否过于复杂？ 功能太复杂了吗？类太复杂了吗？ “太复杂”通常意味着阅读代码的人无法快速理解。也可能意味着“开发者在尝试调用或修改此代码时可能会引入错误。”\n其中一种复杂性就是过度工程（over-engineering），如开发人员使代码过度通用，超过它原本所需的，或者添加系统当前不需要的功能。审查者应特别警惕过度工程。未来的问题应该在它实际到达后解决，且届时才能更清晰的看到其真实样貌及在现实环境里的需求，鼓励开发人员解决他们现在需要解决的问题，而不是开发人员推测可能需要在未来解决的问题。\n测试 将要求单元、集成或端到端测试视为应该做的适当变更。通常，除非 CL 处理紧急情况，否则应在与生产代码相同的 CL 中添加测试。\n确保 CL 中的测试正确，合理且有用。测试并非用来测试自己本身，且我们很少为测试编写测试——人类必须确保测试有效。\n当代码被破坏时，测试是否真的会失败？ 如果代码发生变化时，它们会开始产生误报吗？ 每个测试都会做出简单而有用的断言吗？ 不同测试方法的测试是否适当分开？\n请记住，测试也是必须维护的代码。不要仅仅因为它们不是主二进制文件的一部分而接受测试中的复杂性。\n命名 开发人员是否为所有内容选择了好名字？ 一个好名字应该足够长，可以完全传达项目的内容或作用，但又不会太长，以至于难以阅读。\n注释 开发者是否用可理解的英语撰写了清晰的注释？所有注释都是必要的吗？通常，注释解释为什么某些代码存在时很有用，且不应该用来解释某些代码正在做什么。如果代码无法清楚到去解释自己时，那么代码应该变得更简单。有一些例外（正则表达式和复杂算法通常会从解释他们正在做什么事情的注释中获益很多），但大多数注释都是针对代码本身可能无法包含的信息，例如决策背后的推理。\n查看此 CL 之前的注释也很有帮助。 也许有一个 TODO 现在可以删除，一个注释建议不要进行这种改变，等等。\n请注意，注释与类、模块或函数的文档不同，它们应该代表一段代码的目的，如何使用它，以及使用时它的行为方式。\n风格 Google 提供了所有主要语言的风格指南，甚至包括大多数小众语言。确保 CL 遵循适当的风格指南。\n如果您想改进风格指南中没有的一些样式点，请在评论前加上“Nit：”，让开发人员知道这是您认为可改善代码的小瑕疵，但不是强制性的。不要仅根据个人风格偏好阻止提交 CL。\nCL 的作者不应在主要风格变更中，包括与其他种类的变更。它会使得很难看到 CL 中的变更了什么，使合并和回滚更复杂，并导致其他问题。例如，如果作者想要重新格式化整个文件，让他们只将重新格式化变为一个 CL，其后再发送另一个包含功能变更的 CL。\n文档 如果 CL 变更了用户构建、测试、交互或发布代码的方式，请检查相关文档是否有更新，包括 README、g3doc 页面和任何生成的参考文档。如果 CL 删除或弃用代码，请考虑是否也应删除文档。 如果缺少文档，请询问。\n每一行 查看分配给您审查的每行代码。有时如数据文件、生成的代码或大型数据结构等东西，您可以快速扫过。但不要快速扫过人类编写的类、函数或代码块，并假设其中的内容是 OK 的。显然，某些代码需要比其他代码更仔细的审查——这是您必须做出的判断——但您至少应该确定您理解所有代码正在做什么。\n如果您觉得这些代码太难以阅读了并减慢您审查的速度，您应该在您尝试继续审核前要让开发者知道这件事，并等待他们为程序做出解释、澄清。在 Google，我们聘请了优秀的软件工程师，您就是其中之一。如果您无法理解代码，那么很可能其他开发人员也不会。因此，当您要求开发人员澄清此代码时，您也会帮助未来的开发人员理解这些代码。\n如果您了解代码但觉得没有资格做某些部分的审查，请确保 CL 上有一个合格的审查人，特别是对于安全性、并发性、可访问性、国际化等复杂问题。\n上下文 在广泛的上下文下查看 CL 通常很有帮助。通常，代码审查工具只会显示变更的部分的周围的几行。有时您必须查看整个文件以确保变更确实有意义。例如，您可能只看到添加了四行新代码，但是当您查看整个文件时，您会看到这四行是添加在一个 50 行的方法里，现在确实需要将它们分解为更小的方法。\n在整个系统的上下文中考虑 CL 也很有用。 这个 CL 是否改善了系统的代码健康状况，还是使整个系统更复杂，测试更少等等？不要接受降低系统代码运行状况的 CL。大多数系统通过许多小的变化而变得复杂，因此防止新变更引入即便很小的复杂性也非常重要。\n好的事情 如果您在 CL 中看到一些不错的东西，请告诉开发者，特别是当他们以一种很好的方式解决了您的的一个评论时。代码审查通常只关注错误，但也应该为良好实践提供鼓励。在指导方面，比起告诉他们他们做错了什么，有时更有价值的是告诉开发人员他们做对了什么。\n总结 在进行代码审查时，您应该确保：\n 代码设计精良。 该功能对代码用户是有好处的。 任何 UI 变更都是合理的且看起来是好的。 其中任何并行编程都是安全的。 代码并不比它需要的复杂。 开发人员没有实现他们将来可能需要，但不知道他们现在是否需要的东西。 代码有适当的单元测试。 测试精心设计。 开发人员使用了清晰的名称。 评论清晰有用，且大多用来解释为什么而不是做什么。 代码有适当记录成文件（通常在 g3doc 中）。 代码符合我们的风格指南。  确保查看您被要求查看的每一行代码，查看上下文，确保您提高代码健康状况，并赞扬开发人员所做的好事。\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"68dfb49f9d93687fe9f00cf57b32d625","permalink":"https://lib.jimmysong.io/eng-practices/review/reviewer/looking-for/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/eng-practices/review/reviewer/looking-for/","section":"eng-practices","summary":"注意：在考虑这些要点时，请谨记 “Code Review 标准”。 设计 审查中最重要的是 CL 的整体设计。CL 中各种代码的交互是否有意义？此变更是属于您的代码库（codebase）还是属于库（library）？它是否与您系","tags":null,"title":"Code Review 要点","type":"book"},{"authors":null,"categories":["Linux"],"content":"由于 eBPF 允许在 Linux 内核中运行自定义代码，在解释 eBPF 之前我需要确保你对内核的作用有所了解。然后我们将讨论为什么在修改内核行为这件事情上，eBPF 改变了游戏规则。\nLinux 内核 Linux 内核是应用程序和它们所运行的硬件之间的软件层。应用程序运行在被称为用户空间的非特权层，它不能直接访问硬件。相反，应用程序使用系统调用（syscall）接口发出请求，要求内核代表它行事。这种硬件访问可能涉及到文件的读写，发送或接收网络流量，或者只是访问内存。内核还负责协调并发进程，使许多应用程序可以同时运行。\n应用程序开发者通常不直接使用系统调用接口，因为编程语言给了我们更高级别的抽象和标准库，开发者更容易掌握这些接口。因此，很多人都不知道在程序运行时内核做了什么。如果你想了解内核调用频率，你可以使用 strace 工具来显示程序所做的所有系统调用。这里有一个例子，用 cat 从文件中读取 hello 这个词并将其写到屏幕上涉及到 100 多个系统调用：\nliz@liz-ebpf-demo-1:~$ strace -c cat liz.txt hello % time seconds usecs/call calls errors syscall ------ ----------- ----------- --------- --------- ------------- 0.00 0.000000\t0 5\tread 0.00 0.000000\t0 1\twrite 0.00 0.000000\t0 21\tclose 0.00 0.000000\t0 20\tfstat 0.00 0.000000\t0 23\tmmap 0.00 0.000000\t0 4\tmprotect 0.00 0.000000\t0 2\tmunmap 0.00 0.000000\t0 3\tbrk 0.00 0.000000\t0 4\tpread64 0.00 0.000000\t0 1\t1 access 0.00 0.000000\t0 1\texecve 0.00 0.000000\t0 2\t1 arch_prctl 0.00 0.000000\t0 1\tfadvise64 0.00 0.000000\t0 19\topenat ------ ----------- ----------- --------- --------- ------------- 100.00 0.000000 107 2 total 由于应用程序在很大程度上依赖于内核，这意味着如果我们能够观测到应用程序与内核的交互，我们就可以了解到很多关于它的行为方式。例如，如果你能够截获打开文件的系统调用，你就可以准确地看到任何应用程序访问了哪些文件。但是，怎么才能做到这种拦截呢？让我们考虑一下，如果我们想修改内核，添加新的代码，在系统调用时创建某种输出，会涉及到什么问题。\n向内核添加新功能 Linux 内核很复杂，在写这篇文章的时候有大约 3000 万行代码 1。对任何代码库进行修改都需要对现有的代码有一定的熟悉，所以除非你已经是一个内核开发者，否则这很可能是一个挑战。\n但你将面临的挑战并不是纯粹的技术问题。Linux 是一个通用的操作系统，在不同的环境和情况下使用。这意味着，如果你想对内核进行修改，这并不是简单地写出能用的代码。它必须被社区（更确切地说，是被 Linux 的创造者和主要开发者 Linus Torvalds）接受，你的改变将是为了大家的更大利益。而这并不是必然的——提交的内核补丁只有三分之一被接受 2。\n假如，你已经想出了一个好方法来拦截打开文件的系统调用。经过几个月的讨论和一些艰苦的开发工作，让我们想象一下，这个变化被接受到内核中。很好！但是，要到什么时候它才会出现在每个人的机器上呢？\n每隔两三个月就会有一个新的 Linux 内核版本，但是即使一个变化已经进入了其中一个版本，它仍然需要一段时间才能在大多数人的生产环境中使用。这是因为我们大多数人并不直接使用 Linux 内核——我们使用像 Debian、Red Hat、Alpine、Ubuntu 等 Linux 发行版，它们将 Linux 内核的一个版本与其他各种组件打包在一起。你可能会发现，你最喜欢的发行版使用的是几年前的内核版本。\n例如，很多企业用户都采用红帽 ® Enterprise Linux®（RHEL）。在撰写本文时，目前的版本是 RHEL8.5，发行日期为 2021 年 11 月。这使用的是基于 4.18 版本的内核。这个内核是在 2018 年 8 月发布的。\n如 图 2-1 中的漫画所示，将新功能从想法阶段转化为生产环境中的 Linux 内核，需要数年时间 3。\n  图 2-1. 向内核添加功能（Isovalent 公司的 Vadim Shchekoldin 绘制的漫画）  内核模块 如果你不想等上好几年才把你的改动写进内核，还有一个选择。Linux 内核可以接受内核模块（module），这些模块可以根据需要加载和卸载。如果你想改变或扩展内核行为，编写一个模块是理所当然的。在我们打开文件的系统调用的例子中，你可以写一个内核模块来实现。\n这里最大的挑战是，这仍然是全面的内核编程。用户在使用内核模块时历来非常谨慎，原因很简单：如果内核代码崩溃了，就会导致机器和上面运行的所有东西瘫痪。用户如何确保内核模块可以安全运行呢？\n“安全运行”并不仅仅意味着不崩溃——用户想知道内核模块从安全角度来看是否安全。是否包括攻击者可以利用的漏洞？我们是否相信模块的作者不会在其中加入恶意代码？因为内核是特权代码，它可以访问机器上的一切，包括所有的数据，所以内核中的恶意代码将是一个令人担忧的严重问题。这也适用于内核模块。\n考虑到内核的安全性，这就是为什么 Linux 发行商需要这么长时间来发布新版本的一个重要原因。如果其他人已经在各种情况下运行了数月或数年的内核版本，那些漏洞可能已经被修复。发行版的维护者可以有一些信心，他们提供给用户 / 客户的内核是经过加固的，也就是说，可以安全运行。\neBPF 提供了一个非常不同的安全方法：eBPF 验证器（verifier），它确保一个 eBPF 程序只有在安全运行的情况下才被加载。\neBPF 验证和安全 由于 eBPF 允许我们在内核中运行任意代码，需要有一种机制来确保它的安全运行，不会使用户的机器崩溃，也不会损害他们的数据。这个机制就是 eBPF 验证器。\n验证器对 eBPF 程序进行分析，以确保无论输入什么，它都会在一定数量的指令内安全地终止。例如，如果一个程序解除对一个指针的定义，验证器要求该程序首先检查指针，以确保它不是空的（null）。解除对指针的引用意味着 “查找这个地址的值”，而空值或零值不是一个有效的查找地址。如果你在一个应用程序中解引用一个空指针，该应用程序就会崩溃；而在内核中解引用一个空指针则会使整个机器崩溃，所以避免这种情况至关重要。\n验证也确保了 eBPF 程序只能访问其应该访问的内存。例如，有一个 eBPF 程序在网络堆栈中触发，并通过内核的 套接字缓冲区（socket buffer），其中包括正在传输的数据。有一些特殊的辅助函数，如 bpf_skb_load_bytes()，这个 eBPF 程序可以调用，从套接字缓冲区读取字节数据。另一个由系统调用触发的 eBPF 程序，没有可用的套接字缓冲区，将不允许使用这个辅助函数。验证器还确保程序只读取套接字缓冲区内的数据字节——它不允许访问任意的内存。这里的目的是确保 eBPF 程序是安全的。\n当然，仍然有可能编写一个恶意的 eBPF 程序。如果你可以出于合法的原因观测数据，你也可以出于非法的原因观测它。要注意只从可验证的来源加载可信的 eBPF 程序，并且只将管理 eBPF 工具的权限授予你信任的拥有 root 权限的人。\neBPF 程序的动态加载 eBPF 程序可以动态地加载到内核中和从内核中删除。不管是什么原因导致该事件的发生，一旦它们被附加到一个事件上就会被该事件所触发。例如，如果你将一个程序附加到打开文件的系统调用，那么只要任何进程试图打开一个文件，它就会被触发。当程序被加载时，该进程是否已经在运行，这并不重要。\n这也是使用 eBPF 的可观测性或安全工具的巨大优势之一——即刻获得了对机器上发生的一切事件的可视性。\n此外，如 图 2-2 所示，人们可以通过 eBPF 非常快速地创建新的内核功能，而不要求其他 Linux 用户都接受同样的变更。\n  图 2-2. 用 eBPF 添加内核功能（漫画：Vadim Shchekoldin Isovalent）  现在你已经看到了 eBPF 是如何允许对内核进行动态的、自定义的修改的，让我们来看看如果你想写一个 eBPF 程序会涉及哪些内容。\n参考   Linux 5.12 有大约 2880 万行代码，Phoronix（2021 年 3 月）。 ↩︎\n Yujuan Jiang 等人，《我的补丁能用了吗？要多久？》（论文，2013 年）。根据这篇研究论文，33% 的补丁将在 3 - 6 个月后被接受。 ↩︎\n 值得庆幸的是，现有功能的安全补丁会更快地被提供。 ↩︎\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"c4402dd6f934f3be27ac871fae51752b","permalink":"https://lib.jimmysong.io/what-is-ebpf/changing-the-kernel-is-hard/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/what-is-ebpf/changing-the-kernel-is-hard/","section":"what-is-ebpf","summary":"由于 eBPF 允许在 Linux 内核中运行自定义代码，在解释 eBPF 之前我需要确保你对内核的作用有所了解。然后我们将讨论为什么在修改内核行为这件事情上，eBPF 改变了游戏规则。 Linux 内核 Linux 内核是应用程序和它们所运行的硬件之间的软","tags":["eBPF"],"title":"第二章：修改内核很困难","type":"book"},{"authors":null,"categories":["文化"],"content":"为什么提交小型 CL? 小且简单的 CL 是指：\n 审查更快。审查者更容易抽多次五分钟时间来审查小型 CL，而不是留出 30 分钟来审查一个大型 CL。 审查得更彻底。如果是大的变更，审查者和提交者往往会因为大量细节的讨论翻来覆去而感到沮丧——有时甚至到了重要点被遗漏或丢失的程度。 不太可能引入错误。 由于您进行的变更较少，您和您的审查者可以更轻松有效地推断 CL 的影响，并查看是否已引入错误。 如果被拒绝，减少浪费的工作。 如果您写了一个巨大的 CL，您的评论者说整个 CL 的方向都错误了，你就浪费了很多精力和时间。 更容易合并。 处理大型 CL 需要很长时间，在合并时会出现很多冲突，并且必须经常合并。 更容易设计好。 打磨一个小变更的设计和代码健康状况比完善一个大变更的所有细节要容易得多。 减少对审查的阻碍。 发送整体变更的自包含部分可让您在等待当前 CL 审核时继续编码。 更简单的回滚。 大型 CL 更有可能触及在初始 CL 提交和回滚 CL 之间更新的文件，从而使回滚变得复杂（中间的 CL 也可能需要回滚）。  请注意，审查者可以仅凭 CL 过大而自行决定完全拒绝您的变更。通常他们会感谢您的贡献，但要求您以某种方式将其 CL 改成一系列较小的变更。在您编写完变更后，或者需要花费大量时间来讨论为什么审查者应该接受您的大变更，这可能需要做很多工作。首先编写小型 CL 更容易。\n什么是小型 CL？ 一般来说，CL 的正确大小是自包含的变更。这意味着：\n CL 进行了一项最小的变更，只解决了一件事。通常只是功能的一部分，而不是一个完整的功能。一般来说，因为编写过小的 CL 而犯错也比过大的 CL 犯错要好。与您的审查者讨论以确定可接受的大小。 审查者需要了解的关于 CL 的所有内容（除了未来的开发）都在 CL 的描述、现有的代码库或已经审查过的 CL 中。 对其用户和开发者来说，在签入 CL 后系统能继续良好的工作。 CL 不会过小以致于其含义难以理解。如果您添加新 API，则应在同一 CL 中包含 API 的用法，以便审查者可以更好地了解 API 的使用方式。这也可以防止签入未使用的 API。  关于多大算“太大”没有严格的规则。对于 CL 来说，100 行通常是合理的大小，1000 行通常太大，但这取决于您的审查者的判断。变更中包含的文件数也会影响其“大小”。一个文件中的 200 行变更可能没问题，但是分布在 50 个文件中通常会太大。\n请记住，尽管从开始编写代码开始就您就已经密切参与了代码，但审查者通常不清楚背景信息。对您来说，看起来像是一个可接受的大小的 CL 对您的审查者来说可能是压倒性的。如有疑问，请编写比您认为需要编写的要小的 CL。审查者很少抱怨收到过小的 CL 提交。\n什么时候大 CL 是可以的？ 在某些情况下，大变更也是可以接受的：\n 您通常可以将整个文件的删除视为一行变更，因为审核人员不需要很长时间审核。 有时一个大的 CL 是由您完全信任的自动重构工具生成的，而审查者的工作只是检查并确定想要这样的变更。但这些 CL 可以更大，尽管上面的一些警告（例如合并和测试）仍然适用。  按文件拆分 拆分 CL 的另一种方法是对文件进行分组，这些文件需要不同的审查者，否则就是自包含的变更。\n例如：您发送一个 CL 以修改协议缓冲区，另一个 CL 发送变更使用该原型的代码。您必须在代码 CL 之前提交 proto CL，但它们都可以同时进行审查。如果这样做，您可能希望通知两组审查者审查您编写的其他 CL，以便他们对您的变更具有更充足的上下文。\n另一个例子：你发送一个 CL 用于代码更改，另一个用于使用该代码的配置或实验；如果需要，这也更容易回滚，因为配置/实验文件有时会比代码变更更快地推向生产。\n分离出重构 通常最好在功能变更或错误修复的单独 CL 中进行重构。例如，移动和重命名类应该与修复该类中的错误的 CL 不同。审查者更容易理解每个 CL 在单独时引入的更改。\n但是，修复本地变量名称等小清理可以包含在功能变更或错误修复 CL 中。如果重构大到包含在您当前的 CL 中，会使审查更加困难的话，需要开发者和审查者一起判断是否将其拆开。\n将相关的测试代码保存在同一个 CL 中 避免将测试代码拆分为单独的 CL。验证代码修改的测试应该进入相同的 CL，即使它增加了代码行数。\n但是，独立的测试修改可以首先进入单独的 CL，类似于重构指南。包括：\n 使用新测试验证预先存在的已提交代码。 重构测试代码（例如引入辅助函数）。 引入更大的测试框架代码（例如集成测试）。  不要破坏构建 如果您有几个相互依赖的 CL，您需要找到一种方法来确保在每次提交 CL 后整个系统能够继续运作。否则可能会在您的 CL 提交的几分钟内打破所有开发人员的构建（如果您之后的 CL 提交意外出错，时间可能会甚至更长）。\n如果不能让它足够小 有时你会遇到看起来您的 CL 必须如此庞大，但这通常很少是正确的。习惯于编写小型 CL 的提交者几乎总能找到将功能分解为一系列小变更的方法。\n在编写大型 CL 之前，请考虑在重构 CL 之前是否可以为更清晰的实现铺平道路。与你的同伴聊聊，看看是否有人想过如何在小型 CL 中实现这些功能。\n如果以上的努力都失败了（这应该是非常罕见的），那么请在事先征得审查者的同意后提交大型 CL，以便他们收到有关即将发生的事情的警告。在这种情况下，做好完成审查过程需要很长一段时间的准备，对不引入错误保持警惕，并且在编写测试时要更下功夫。\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"b422b77d1170880fd9dc9dc3bdbbe90c","permalink":"https://lib.jimmysong.io/eng-practices/review/developer/small-cls/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/eng-practices/review/developer/small-cls/","section":"eng-practices","summary":"为什么提交小型 CL? 小且简单的 CL 是指： 审查更快。审查者更容易抽多次五分钟时间来审查小型 CL，而不是留出 30 分钟来审查一个大型 CL。 审查得更彻底。如果是大的变更，审查者和提交者往往会因为大量细节的讨论翻来覆去","tags":null,"title":"小型 CL","type":"book"},{"authors":null,"categories":["网络"],"content":"所有 BPF Map 都是有使用容量上限的。超出限制的插入将失败，从而限制了数据路径的可扩展性。下表显示了映射的默认值。每个限制都可以在源代码中更改。如果需要，将根据要求添加配置选项。\n   Map 名称 范围 默认限制 规模影响     连接跟踪 节点或端点 1M TCP/256k UDP 最大 1M 并发 TCP 连接，最大 256k 预期 UDP 应答   NAT 节点 512k 最大 512k NAT 条目   邻居表 节点 512k 最大 512k 邻居条目   端点 节点 64k 每个节点最多 64k 个本地端点 + 主机 IP   IP 缓存 节点 512k 最大 256k 端点（IPv4+IPv6），最大 512k 端点（IPv4 或 IPv6）跨所有集群   负载均衡器 节点 64k 跨所有集群的所有服务的最大 64k 累积后端   策略 端点 16k 特定端点的最大允许身份 + 端口 + 协议对 16k   代理 Map 节点 512k 最大 512k 并发重定向 TCP 连接到代理   隧道 节点 64k 跨所有集群最多 32k 节点（IPv4+IPv6）或 64k 节点（IPv4 或 IPv6）   IPv4 分片 节点 8k 节点上同时传输的最大 8k 个分段数据报   会话亲和性 节点 64k 来自不同客户端的最大 64k 关联   IP 掩码 节点 16k 基于 BPF 的 ip-masq-agent 使用的最大 16k IPv4 cidrs   服务源范围 节点 64k 跨所有服务的最大 64k 累积 LB 源范围   Egess 策略 端点 16k 跨所有集群的所有目标 CIDR 的最大 16k 端点    对于某些 BPF 映射，可以使用命令行选项 cilium-agent 覆盖容量上限。可以使用 --bpf-lb-map-max、 --bpf-ct-global-tcp-max、--bpf-ct-global-any-max、 --bpf-nat-global-max、--bpf-neigh-global-max、--bpf-policy-map-max和 --bpf-fragments-map-max 来设置给定容量。\n提示\n  如果指定了--bpf-ct-global-tcp-max和 / 或--bpf-ct-global-any-max ，则 NAT 表大小 ( --bpf-nat-global-max) 不得超过组合 CT 表大小（TCP + UDP）的 2/3。--bpf-nat-global-max 如果未显式设置或使用动态 BPF Map 大小（见下文），这将自动设置。   使用 --bpf-map-dynamic-size-ratio 标志，几个大型 BPF Map 的容量上限在代理启动时根据给定的总系统内存比率确定。例如，给定的 0.0025 比率导致 0.25% 的总系统内存用于这些映射。\n此标志会影响以下消耗系统中大部分内存的 BPF Map： cilium_ct_{4,6}_global、cilium_ct_{4,6}_any、 cilium_nodeport_neigh{4,6}、cilium_snat_v{4,6}_external 和 cilium_lb{4,6}_reverse_sk\nkube-proxy根据机器拥有的内核数设置为 linux 连接跟踪表中的最大条目数。 无论机器有多少内核，kube-proxy 默认每个内核的最大条目数是 32768 和最小条目数是 131072。\nCilium 有自己的连接跟踪表作为 BPF Map，并且此类映射的条目数是根据节点中的总内存量计算的，无论机器有多少内存，条目最少数是 131072。\n下表介绍了当 Cilium 配置为 -bpf-map-dynamic-size-ratio: 0.0025 时，kube-proxy 和 Cilium为自己的连接跟踪表设置的数值：\n   虚拟 CPU 内存 (GiB) Kube-proxy CT 条目 Cilium CT 条目     1 3.75 131072 131072   2 7.5 131072 131072   4 15 131072 131072   8 30 262144 284560   16 60 524288 569120   32 120 1048576 1138240   64 240 2097152 2276480   96 360 3145728 4552960    ","date":1655438400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"74e1773561833571b628f0790d0a1aa1","permalink":"https://lib.jimmysong.io/cilium-handbook/ebpf/maps/","publishdate":"2022-06-17T12:00:00+08:00","relpermalink":"/cilium-handbook/ebpf/maps/","section":"cilium-handbook","summary":"所有 BPF Map 都是有使用容量上限的。超出限制的插入将失败，从而限制了数据路径的可扩展性。下表显示了映射的默认值。每个限制都可以在源代码中更改。如果需要，将根据要求添加配置选项。 Map 名称 范围 默认限制 规模影响 连接","tags":["Cilium"],"title":"eBPF Map","type":"book"},{"authors":null,"categories":["网络"],"content":"用于 Pod 的 IPv4 地址通常是从 RFC1918 私有地址块分配的，因此不可公开路由。Cilium 会自动将离开集群的所有流量的源 IP 地址伪装成节点的 IPv4 地址，因为节点的 IP 地址已经可以在网络上路由。\n   IP 地址伪装示意图  对于 IPv6 地址，只有在使用 iptables 实现模式时才会执行伪装。\n可以使用 IPv4 选项和离开主机的 IPv6 流量禁用此行为。enable-ipv4-masquerade: false``enable-ipv6-masquerade: false\n配置   设置可路由 CIDR\n默认行为是排除本地节点的 IP 分配 CIDR 内的任何目标。如果 pod IP 可在更广泛的网络中路由，则可以使用以下选项指定该网络：ipv4-native-routing-cidr: 10.0.0.0/8 ，在这种情况下，该 CIDR 内的所有目的地都 不会 被伪装。\n  设置伪装接口\n请参阅实现模式以配置伪装接口。\n  实现模式 基于 eBPF 基于 eBPF 的实现是最有效的实现。它需要 Linux 内核 4.19，并且可以使用 bpf.masquerade=true helm 选项启用。\n当前的实现依赖于 BPF NodePort 特性。将来会删除该依赖项（GitHub Issue 13732）。\n伪装只能在那些运行 eBPF 伪装程序的设备上进行。这意味着如果输出设备运行程序，从 pod 发送到外部的数据包将被伪装（到输出设备 IPv4 地址）。如果未指定，程序将自动附加到 BPF NodePort 设备检测机制选择的设备上。要手动更改此设置，请使用 devices helm 选项。使用 cilium status 确定程序在哪些设备上运行：\n$ kubectl exec -it -n kube-system cilium-xxxxx -- cilium status | grep Masquerading Masquerading: BPF (ip-masq-agent) [eth0, eth1] 10.0.0.0/16 从上面的输出可以看出，程序正在 eth0 和 eth1 设备上运行。\n基于 eBPF 的伪装可以伪装以下 IPv4 四层协议的数据包：\n TCP UDP ICMP（仅 Echo 请求和 Echo 回复）  默认情况下，来自 pod 的所有发往 ipv4-native-routing-cidr 范围外 IP 地址的数据包都会被伪装，但发往其他集群节点的数据包除外。排除 CIDR 显示在 （cilium status ) 的上述输出中（10.0.0.0/16）。\n提示\n  启用 eBPF 伪装后，从 Pod 到集群节点外部 IP 的流量也不会被伪装。eBPF 实现在这方面不同于基于 iptables 的伪装。此限制在 GitHub Issue 17177 中进行了跟踪。   为了实现更细粒度的控制，Cilium 在 eBPF 中实现了ip-masq-agent，可以通过ipMasqAgent.enabled=true helm 选项启用。\n基于 eBPF 的 ip-masq-agent 支持配置文件中设置的 nonMasqueradeCIDRs 和 masqLinkLocal 选项。从 pod 发送到属于任何 CIDR 的目的地的数据包 nonMasqueradeCIDRs 不会被伪装。如果配置文件为空，代理将配置以下非伪装 CIDR：\n 10.0.0.0/8 172.16.0.0/12 192.168.0.0/16 100.64.0.0/10 192.0.0.0/24 192.0.2.0/24 192.88.99.0/24 198.18.0.0/15 198.51.100.0/24 203.0.113.0/24 240.0.0.0/4  此外，如果 masqLinkLocal 未设置或设置为 false，则 169.254.0.0/16 附加到非伪装 CIDR 列表中。\n代理使用 Fsnotify 跟踪配置文件的更新，因此 resyncInterval 不需要原始选项。\n下面的示例显示了如何通过配置代理 ConfigMap 并对其进行验证：\n$ cat agent-config/config nonMasqueradeCIDRs: - 10.0.0.0/8 - 172.16.0.0/12 - 192.168.0.0/16 masqLinkLocal: false $ kubectl create configmap ip-masq-agent --from-file=agent-config --namespace=kube-system $ # Wait ~60s until the ConfigMap is mounted into a cilium pod $ kubectl -n kube-system exec -ti cilium-xxxxx -- cilium bpf ipmasq list IP PREFIX/ADDRESS 10.0.0.0/8 169.254.0.0/16 172.16.0.0/12 192.168.0.0/16 \n  IPv6 流量当前不支持基于 eBPF 的伪装。   基于 iptables 这是适用于所有内核版本的遗留实现。\n默认行为将伪装所有离开非 Cilium 网络设备的流量。这通常会导致正确的行为。为了限制应在其上执行伪装的网络接口，可以使用egress-masquerade-interfaces: eth0 选项。\n提示\n  也可以指定接口前缀 eth+，匹配前缀的所有接口 eth 都将用于伪装。   ","date":1655438400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"62ba312be168ed796de6bcb3bc6c502a","permalink":"https://lib.jimmysong.io/cilium-handbook/networking/masquerading/","publishdate":"2022-06-17T12:00:00+08:00","relpermalink":"/cilium-handbook/networking/masquerading/","section":"cilium-handbook","summary":"用于 Pod 的 IPv4 地址通常是从 RFC1918 私有地址块分配的，因此不可公开路由。Cilium 会自动将离开集群的所有流量的源 IP 地址伪装成节点的 IPv4 地址，因为节点的 IP 地址已经可以在网络上路由。 IP 地址伪装示意图 对于 IPv6 地址，只有在","tags":["Cilium"],"title":"IP 地址伪装","type":"book"},{"authors":null,"categories":["网络"],"content":"所有安全策略的描述都是假设基于会话协议的有状态策略执行。这意味着策略的意图是描述允许的连接建立方向。如果策略允许A =\u0026gt; B，那么从 B 到 A 的回复数据包也会被自动允许。但是，并不自动允许 B 向 A 发起连接。如果希望得到这种结果，那么必须明确允许这两个方向。\n安全策略可以在 ingress 或 egress 处执行。对于 ingress，这意味着每个集群节点验证所有进入的数据包，并确定数据包是否被允许传输到预定的终端。相应地，对于 egress，每个集群节点验证出站数据包，并确定是否允许将数据包传输到预定目的地。\n为了在多主机集群中执行基于身份的安全，发送端点的身份被嵌入到集群节点之间传输的每个网络数据包中。然后，接收集群节点可以提取该身份，并验证一个特定的身份是否被允许与任何本地端点进行通信。\n默认安全策略 如果没有加载任何策略，默认行为是允许所有通信，除非明确启用了策略执行。一旦加载了第一条策略规则，就会自动启用策略执行，然后任何通信必须是白名单，否则相关数据包将被丢弃。\n同样，如果一个端点不受制于四层策略，则允许与所有端口进行通信。将至少一个 四层策略与一个端点相关联，将阻止与端口的所有连接，除非明确允许。\n","date":1655438400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"696a9e9b2a0b3166b1a3fd08e09a596a","permalink":"https://lib.jimmysong.io/cilium-handbook/security/policyenforcement/","publishdate":"2022-06-17T12:00:00+08:00","relpermalink":"/cilium-handbook/security/policyenforcement/","section":"cilium-handbook","summary":"所有安全策略的描述都是假设基于会话协议的有状态策略执行。这意味着策略的意图是描述允许的连接建立方向。如果策略允许A =\u003e B，那么从 B 到 A 的回复数据包也会被自动允许。但是，并不自动允许 B 向 A 发起连接。如果希","tags":["Cilium"],"title":"策略执行","type":"book"},{"authors":null,"categories":["网络"],"content":"本节指定 Cilium 端点的生命周期。\nCilium 中的端点状态包括：\n restoring：端点在 Cilium 启动之前启动，Cilium 正在恢复其网络配置。 waiting-for-identity：Cilium 正在为端点分配一个唯一的身份。 waiting-to-regenerate：端点接收到一个身份并等待（重新）生成其网络配置。 regenerating：正在（重新）生成端点的网络配置。这包括为该端点编程 eBPF。 ready：端点的网络配置已成功（重新）生成。 disconnecting：正在删除端点。 disconnected：端点已被删除。     端点状态生命周期  可以使用 cilium endpoint list 和 cilium endpoint get CLI 命令查询端点的状态。\n当端点运行时，它会在 waiting-for-identity、waiting-to-regenerate、regenerating 和 ready 状态之间转换。进入 waiting-for-identity 状态的转换表明端点改变了它的身份。转换到 waiting-to-regenerate 或者 regenerating 状态表示要在端点上实施的策略由于身份、策略或配置的更改而发生了更改。\n端点在被删除时转换为 disconnecting 状态，无论其当前状态如何。\n初始化标识 在某些情况下，Cilium 无法在端点创建时立即确定端点的标签，因此无法在此时为端点分配身份。在知道端点的标签之前，Cilium 会暂时将一个特殊的单一标签 reserved:init 与端点相关联。当端点的标签变得已知时，Cilium 然后用端点的标签替换那个特殊的标签，并为端点分配一个适当的身份。\n这可能在以下情况下在端点创建期间发生：\n 通过 libnetwork 使用 docker 运行 Cilium 当 Kubernetes API 服务器不可用时使用 Kubernetes 在对应的 kvstore 不可用时处于 etcd 模式  要在初始化时允许进出端点的流量，您可以创建选择 reserved:init 标签的策略规则和/或允许进出特殊 init 实体的流量的规则。\n例如，编写一个规则，允许所有初始化端点从主机接收连接并执行 DNS 查询，可以如下完成：\napiVersion:\u0026#34;cilium.io/v2\u0026#34;kind:CiliumNetworkPolicymetadata:name:initspecs:- endpointSelector:matchLabels:\u0026#34;reserved:init\u0026#34;: \u0026#34;\u0026#34;ingress:- fromEntities:- hostegress:- toEntities:- alltoPorts:- ports:- port:\u0026#34;53\u0026#34;protocol:UDP同样，编写允许端点接收来自初始化端点的 DNS 查询的规则可以如下完成：\napiVersion:\u0026#34;cilium.io/v2\u0026#34;kind:CiliumNetworkPolicymetadata:name:\u0026#34;from-init\u0026#34;spec:endpointSelector:matchLabels:app:myServiceingress:- fromEntities:- init- toPorts:- ports:- port:\u0026#34;53\u0026#34;protocol:UDP如果任何入口（resp.egress）策略规则选择了 reserved:init 标签，那么所有到（resp.from）初始化端点（这些规则未明确允许）的入口（resp.egress）流量都将被丢弃。否则，如果策略执行模式是 never 或 default，则允许所有入口（或出口）流量（或来自）初始化端点。否则，所有入口（或出口）流量都会被丢弃。\n","date":1655438400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"89a9c0aedd3f83fb63031a0e9db570ce","permalink":"https://lib.jimmysong.io/cilium-handbook/policy/lifecycle/","publishdate":"2022-06-17T12:00:00+08:00","relpermalink":"/cilium-handbook/policy/lifecycle/","section":"cilium-handbook","summary":"本节指定 Cilium 端点的生命周期。 Cilium 中的端点状态包括： restoring：端点在 Cilium 启动之前启动，Cilium 正在恢复其网络配置。 waiting-for-identity：Cilium 正在为端点分配一个唯一的身","tags":["Cilium"],"title":"端点生命周期","type":"book"},{"authors":null,"categories":["网络"],"content":"集群网格将网络数据路径扩展到多个集群。它允许所有连接集群中的端点进行通信，同时提供完整的策略执行。负载均衡可通过 Kubernetes 注解获得。\n请参阅如何设置集群网格的说明。\n","date":1655438400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"4fe6d6e9bbc8aa30018d294cc146cdc1","permalink":"https://lib.jimmysong.io/cilium-handbook/clustermesh/","publishdate":"2022-06-17T12:00:00+08:00","relpermalink":"/cilium-handbook/clustermesh/","section":"cilium-handbook","summary":"集群网格将网络数据路径扩展到多个集群。它允许所有连接集群中的端点进行通信，同时提供完整的策略执行。负载均衡可通过 Kubernetes 注解获得。 请参阅如何设置集群网格的说明。","tags":["Cilium"],"title":"多集群（集群网格）","type":"book"},{"authors":null,"categories":["网络"],"content":"可观测性由 Hubble 提供，它可以以完全透明的方式深入了解服务的通信和行为以及网络基础设施。Hubble 能够在多集群（集群网格） 场景中提供节点级别、集群级别甚至跨集群的可视性。有关 Hubble 的介绍以及它与 Cilium 的关系，请阅读 Cilium 和 Hubble 简介部分。\n默认情况下，Hubble API 的范围仅限于 Cilium 代理运行的每个单独节点。换句话说，网络可视性仅提供给本地 Cilium 代理观察到的流量。在这种情况下，与 Hubble API 交互的唯一方法是使用 Hubble CLI（hubble）查询通过本地 Unix Domain Socket 提供的 Hubble API。Hubble CLI 二进制文件默认安装在 Cilium 代理 pod 上。\n部署 Hubble Relay 后，Hubble 提供完整的网络可视性。在这种情况下，Hubble Relay 服务提供了一个 Hubble API，它在 ClusterMesh 场景中涵盖整个集群甚至多个集群。可以通过将 Hubble CLI（hubble）指向 Hubble Relay 服务或通过 Hubble UI 访问 Hubble 数据。Hubble UI 是一个 Web 界面，可以自动发现三层/四层甚至七层的服务依赖图，允许用户友好的可视化和过滤数据流作为服务图。\n 下一章   ","date":1655438400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"234f4d335aee4d7464563bb29ddf93ae","permalink":"https://lib.jimmysong.io/cilium-handbook/concepts/observability/","publishdate":"2022-06-17T12:00:00+08:00","relpermalink":"/cilium-handbook/concepts/observability/","section":"cilium-handbook","summary":"可观测性由 Hubble 提供，它可以以完全透明的方式深入了解服务的通信和行为以及网络基础设施。Hubble 能够在多集群（集群网格） 场景中提供节点级别、集群级别甚至跨集群的可视性。有关 Hubble 的介绍以及它与 Cilium 的关系，请阅","tags":["Cilium"],"title":"可观测性","type":"book"},{"authors":null,"categories":["网络"],"content":"Kubernetes 版本 以下列出的所有 Kubernetes 版本都经过 e2e 测试，并保证与此 Cilium 版本兼容。此处未列出的旧 Kubernetes 版本不支持 Cilium。较新的 Kubernetes 版本未列出，这取决于新版本的的向后兼容性。\n 1.16 1.17 1.18 1.19 1.20 1.21 1.22 1.23  系统要求 Cilium 需要 Linux 内核 \u0026gt;= 4.9。有关所有系统要求的完整详细信息，请参阅系统要求。\n在 Kubernetes 中启用 CNI CNI（容器网络接口）是 Kubernetes 用来委托网络配置的插件层。必须在 Kubernetes 集群中启用 CNI 才能安装 Cilium。这是通过将 --network-plugin=cni 参数在所有节点上传递给 kubelet 来完成的。有关更多信息，请参阅Kubernetes CNI 网络插件文档。\n启用自动节点 CIDR 分配（推荐） Kubernetes 具有自动分配每个节点 IP CIDR 的能力。如果启用，Cilium 会自动使用此功能。这是在 Kubernetes 集群中处理 IP 分配的最简单方法。要启用此功能，只需在启动时添加以下标志 kube-controller-manager：\n--allocate-node-cidrs 此选项不是必需的，但强烈推荐。\n","date":1655438400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"b53295acc7d3110b9eebaa9ea86e8c57","permalink":"https://lib.jimmysong.io/cilium-handbook/kubernetes/requirements/","publishdate":"2022-06-17T12:00:00+08:00","relpermalink":"/cilium-handbook/kubernetes/requirements/","section":"cilium-handbook","summary":"Kubernetes 版本 以下列出的所有 Kubernetes 版本都经过 e2e 测试，并保证与此 Cilium 版本兼容。此处未列出的旧 Kubernetes 版本不支持 Cilium。较新的 Kubernetes 版本未列出，这取决于新版本的的向后兼容性。 1.16 1.17 1.18 1.19 1.20 1.21 1.22 1.23 系统要求 Cilium 需要 Linux 内核 \u003e= 4.9。有","tags":["Cilium"],"title":"要求","type":"book"},{"authors":null,"categories":["Linux"],"content":"在这一章中，让我们来谈谈编写 eBPF 代码。我们需要考虑在内核中运行的 eBPF 程序本身，以及与之交互的用户空间代码。\n内核和用户空间代码 首先，你可以用什么编程语言来编写 eBPF 程序？\n内核接受字节码形式的 eBPF 程序 1。人工编写这种字节码是可能的，就像用汇编语言写应用程序代码一样——但对人类来说，使用一种可以被编译（即自动翻译）为字节码的高级语言通常更实用。\n由于一些原因，eBPF 程序不能用任意的高级语言编写。首先，语言编译器需要支持发出内核所期望的 eBPF 字节码格式。其次，许多编译语言都有运行时特性——例如 Go 的内存管理和垃圾回收，使它们不适合。在撰写本文时，编写 eBPF 程序的唯一选择是 C（用 clang/llvm 编译）和最新的 Rust。迄今为止，绝大多数的 eBPF 代码都是用 C 语言发布的，考虑到它是 Linux 内核的语言，这是有道理的。\n至少，用户空间的程序需要加载到内核中，并将其附加到正确的事件中。有一些实用程序，如 bpftool，可以帮助我们解决这个问题，但这些都是低级别的工具，假定你有丰富的 eBPF 知识，它们是为 eBPF 专家设计的，而不是普通用户。在大多数基于 eBPF 的工具中，有一个用户空间的应用程序，负责将 eBPF 程序加载到内核中，传入任何配置参数，并以用户友好的方式显示 eBPF 程序收集的信息。\n至少在理论上，eBPF 工具的用户空间部分可以用任何语言编写，但在实践中，有一些库只支持相当少的语言。其中包括 C、Go、Rust 和 Python。这种语言的选择更加复杂，因为并不是所有的语言都有支持 libbpf 的库，libbpf 已经成为使 eBPF 程序在不同版本的内核中可移植的流行选择。(我们将在 第四章 中讨论 libbpf）。\n附属于事件的自定义程序 eBPF 程序本身通常是用 C 或 Rust 编写的，并编入一个对象文件 2。这是一个标准的 ELF（可执行和可链接格式，Executable and Linkable Format）文件，可以用像 readelf 这样的工具来检查，它包含程序字节码和任何映射的定义（我们很快就会讨论）。如 图 3-1 所示，如果在前一章中提到的验证器允许的话，用户空间程序会读取这个文件并将其加载到内核中。\n  图 3-1. 用户空间应用程序使用 bpf() 系统调用从 ELF 文件中加载 eBPF 程序到内核中  eBPF 程序加载到内核中时必须被附加到事件上。每当事件发生，相关的 eBPF 程序就会运行。有一个非常广泛的事件，你可以将程序附加到其中；我不会涵盖所有的事件，但以下是一些更常用的选项。\n从函数中进入或退出 你可以附加一个 eBPF 程序，在内核函数进入或退出时被触发。当前的许多 eBPF 例子都使用了 kprobes（附加到一个内核函数入口点）和 kretprobes（函数退出）的机制。在最新的内核版本中，有一个更有效的替代方法，叫做 fentry/fexit 3。\n请注意，你不能保证在一个内核版本中定义的所有函数一定会在未来的版本中可用，除非它们是稳定 API 的一部分，如 syscall 接口。\n你也可以用 uprobes 和 uretprobes 将 eBPF 程序附加到用户空间函数上。\nTracepoints 你也可以将 eBPF 程序附加到内核内定义的 tracepoints 4。通过在 /sys/kernel/debug/tracing/events 下查找机器上的事件。\nPerf 事件 Perf 5 是一个收集性能数据的子系统。你可以将 eBPF 程序挂到所有收集 perf 数据的地方，这可以通过在你的机器上运行 perf list 来确定。\nLinux 安全模块接口 LSM 接口在内核允许某些操作之前检查安全策略。你可能见过 AppArmor 或 SELinux，利用了这个接口。通过 eBPF，你可以将自定义程序附加到相同的检查点上，从而实现灵活、动态的安全策略和一些运行时安全工具的新方法。\n网络接口——eXpress Data Path eXpress Data Path（XDP）允许将 eBPF 程序附加到网络接口上，这样一来，每当收到一个数据包就会触发 eBPF 程序。它可以检查甚至修改数据包，程序的退出代码可以告诉内核如何处理该数据包：传递、放弃或重定向。这可以构成一些非常有效的网络功能 6 的基础。\n套接字和其他网络钩子 当应用程序在网络套接字上打开或执行其他操作时，以及当消息被发送或接收时，你可以附加运行 eBPF 程序。在内核的网络堆栈中也有称为 流量控制（traffic control） 或 tc 的 钩子，eBPF 程序可以在初始数据包处理后运行。\n一些功能可以单独用 eBPF 程序实现，但在许多情况下，我们希望 eBPF 代码能从用户空间的应用程序接收信息，或将数据传递给用户空间的应用程序。允许数据在 eBPF 程序和用户空间之间，或在不同的 eBPF 程序之间传递的机制被称为 map。\neBPF Map map 的开发是 eBPF 缩略语中的 e 代表 extended 重要区别之一。\nmap 是与 eBPF 程序一起定义的数据结构体。有各种不同类型的 map ，但它们本质上都是键值存储。eBPF 程序可以读取和写入 map，用户空间代码也可以。map 的常见用途包括：\n eBPF 程序写入关于事件的指标和其他数据，供用户空间代码以后检索。 用户空间代码编写配置信息，供 eBPF 程序读取并作出相应的行为。 eBPF 程序将数据写入 map ，供另一个 eBPF 程序以后检索，允许跨多个内核事件的信息协调。  如果内核和用户空间的代码都要访问同一个映射，它们需要对存储在该映射中的数据结构体有一个共同的理解。这可以通过在用户空间和内核代码中加入定义这些数据结构体的头文件来实现，但是如果这些代码不是用相同的语言编写的，作者将需要仔细创建逐个字节兼容的结构体定义。\n我们已经讨论了 eBPF 工具的主要组成部分：在内核中运行的 eBPF 程序，加载和与这些程序交互的用户空间代码，以及允许程序共享数据的 map 。为了更具体化，让我们看一个例子。\nOpensnoop 示例 在 eBPF 程序的例子中，我选择了 opennoop，一个可以显示任何进程所打开的文件的工具。这个工具的原始版本是 Brendan Gregg 最初在 BCC 项目中编写的许多 BPF 工具之一，你可以在 GitHub 上找到。它后来被重写为 libbpf（你将在下一章见到它），在这个例子中，我使用 libbpf-tools 目录下的较新版本。\n当你运行 opensnoop 时，你将看到的输出在很大程度上取决于当时在虚拟机上发生了什么，但它应该看起来像这样。\nPID COMM\tFD\tERR\tPATH 93965 cat\t3\t0\t/etc/ld.so.cache 93965 cat\t3\t0\t/lib/x86_64-linux-gnu/libc.so.6 93965 cat\t3\t0\t/usr/lib/locale/locale-archive 93965 cat\t3\t0\t/usr/share/locale/locale.alias ... 每一行输出表示一个进程打开（或试图打开）一个文件。这些列显示了进程的 ID，运行的命令，文件描述符，错误代码的指示，以及被打开的文件的路径。\nOpensnoop 的工作方式是将 eBPF 程序附加到 open() 和 openat() 系统调用上，所有应用程序都必须通过这些调用来要求内核打开文件。让我们深入了解一下这是如何实现的。为了简洁起见，我们将不看每一行代码，但我希望这足以让你了解它是如何工作的（如果你对这么深的内容不感兴趣的话，请跳到下一章！）。\nOpensnoop eBPF 代码 eBPF 代码是用 C 语言编写的，在 opensnoop.bpf.c 文件中。在这个文件的开头，你可以看到两个 eBPF map 的定义 —— start 和 events：\nstruct { __uint(type, BPF_MAP_TYPE_HASH); __uint(max_entries, 10240); __type(key, u32); __type(value, struct args_t); } start SEC(\u0026#34;.maps\u0026#34;); struct { __uint(type, BPF_MAP_TYPE_PERF_EVENT_ARRAY); __uint(key_size, sizeof(u32)); __uint(value_size, sizeof(u32)); } events SEC(\u0026#34;.maps\u0026#34;); 当 ELF 对象文件被创建时，它包含了每个 map 和每个要加载到内核的程序的部分，SEC() 宏定义了这些部分。\n当我们研究这个程序的时候，你会看到，在系统调用被处理的时候，start map 被用来临时存储系统调用的参数 —— 包括被打开的文件的名称。events map 7 用于将事件信息从内核中的 eBPF 代码传递给用户空间的可执行程序。如 图 3-2 所示。\n  图 3-2. 调用 open() 触发 eBPF 程序，将数据存储在 opensnoop 的 eBPF map 中  在 opensnoop.bpf.c 文件的后面，你会发现两个极其相似的函数：\nSEC(\u0026#34;tracepoint/syscalls/sys_enter_open\u0026#34;) int tracepoint__syscalls__sys_enter_open(struct trace_event_raw_sys_enter* ctx) 和\nSEC(\u0026#34;tracepoint/syscalls/sys_enter_openat\u0026#34;) int tracepoint__syscalls__sys_enter_openat(struct trace_event_raw_sys_enter* ctx) 有两个不同的系统调用用于打开文件 8：openat() 和 open()。它们是相同的，除了 openat() 有一个额外的参数是目录文件描述符，而且要打开的文件的路径名是相对于该目录而言的。同样，除了处理参数上的差异，opennoop 中的两个函数也是相同的。\n正如你所看到的，它们都需要一个参数，即一个指向名为 trace_event_raw_sys_enter 结构体的指针。你可以在你运行的特定内核生成的 vmlinux 头文件中找到这个结构体的定义。编写 eBPF 程序之道包括找出每个程序接收的结构体作为其上下文，以及如何访问其中的信息。\n这两个函数使用一个 BPF 辅助函数来检索调用这个 syscall 的进程 ID：\nu64 id = bpf_get_current_pid_tgid(); 这段代码得到了文件名和传递给系统调用的标志，并把它们放在一个叫做 args 的结构体中：\nargs.fname = (const char *)ctx-\u0026gt;args[0]; args.flags = (int)ctx-\u0026gt;args[1]; 这个结构体被写入 start map 中，使用当前程序 ID 作为键。\n这就是 eBPF 程序在进入 syscall 时所做的一切。但在 opensnoop.bpf.c 中定义了另一对 eBPF 程序，当系统调用退出时被触发。\n这个程序和它的双胞胎 openat() 在函数 trace_exit() 中共享代码。你有没有注意到，所有被 eBPF 程序调用的函数的前缀都是 static __always_inline？这迫使编译器将这些函数的指令放在内联中，因为在旧的内核中，BPF 程序不允许跳转到一个单独的函数。新的内核和 LLVM 版本可以支持非内联的函数调用，但这是一种安全的方式，可以确保 BPF 验证器满意（现在还有一个 BPF 尾部调用的概念，即执 …","date":1654142400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"37aff517f23b6e68dd289bce789da763","permalink":"https://lib.jimmysong.io/what-is-ebpf/ebpf-programs/","publishdate":"2022-06-02T12:00:00+08:00","relpermalink":"/what-is-ebpf/ebpf-programs/","section":"what-is-ebpf","summary":"在这一章中，让我们来谈谈编写 eBPF 代码。我们需要考虑在内核中运行的 eBPF 程序本身，以及与之交互的用户空间代码。 内核和用户空间代码 首先，你可以用什么编程语言来编写 eBPF 程序？ 内核接受字节码形式的 eBPF 程序 1。人工编写这","tags":["eBPF"],"title":"第三章：eBPF 程序","type":"book"},{"authors":null,"categories":null,"content":"现在我们将一些问题转移到了云中的 DevOps 平台。\n分解单体应用 传统的 n 层单体式应用部署到云中后很难维护，因为它们经常对云基础设施提供的部署环境做出不可靠的假设，这些假设云很难提供。 例如以下要求：\n 可访问已挂载的共享文件系统 P2P 应用服务器集群 共享库 配置文件位于常用的配置文件目录  大多数这些假设都出于这样的事实：单体应用通常都部署在长期运行的基础设施中，并与其紧密结合。不幸的是，单体应用并不太适合弹性和短暂（非长期支持）生命周期的基础设施。\n但是即使我们可以构建一个不需要这些假设的单体应用，我们依然有一些问题需要解决：\n 单体式应用的变更周期耦合，使独立业务能力无法按需部署，阻碍创新速度。 嵌入到单体应用中的服务不能独立于其他服务进行扩展，因此负载更难于优化。 新加入组织的开发人员必须适应新的团队，经常学习新的业务领域，并且一次就熟悉一个非常大的代码库。 这样会增加 3-6 个月的适应时间，才能实现真正的生产力。 尝试通过堆积开发人员来扩大开发组织，增加了昂贵的沟通和协调成本。 技术栈需要长期承诺。引进新技术太过冒险，可能会对整体产生不利影响。  细心的读者会注意到，该列表正好与 “微服务” 的列表相反。将组织分解为业务能力团队还要求我们将应用程序分解成微服务。只有这样，我们才能从云计算基础架构中获得最大的收益。\n分解数据 仅仅将单体应用分解为微服务还是远远不够的。数据模型必须要解耦。如果业务能力团队被认为是自主的，却被迫通过单一的数据存储进行协作，那么单体应用对创新的阻碍将依然存在。\n事实上，产品架构必须从数据开始的说法是有争议的。由 Eric Evans（Addison-Wesley）在领域驱动设计（DDD）中提出的原理认为，我们的成功在很大程度上取决于领域模型的质量（以及支持它的普遍存在的语言）。要使领域模型有效，还必须在内部一致 —— 我们不应该在同一模型内的一致定义中找到重复定义的术语或概念。\n创建不具有这种不一致的联合领域模型是非常困难和昂贵的（可以说是不可能的）。Evans 将业务的整体领域模型的内部一致性子集称为有界上下文。\n最近与航空公司客户合作时，我们讨论了他们业务的核心概念，自然是 “航空公司预订” 的话题。该集团可以在其预定业务中划分十七种不同的逻辑定义，几乎不能将它们调和为一个。相反，每个定义的所有细微差别都被仔细地描绘成一个个单一的概念，这将成为组织的巨大瓶颈。\n有界上下文允许你在整个组织中保持单一概念的不一致定义，只要它们在有界上下文中一致地定义。\n因此，我们首先需要确定可以在内部保持一致的领域模型的细分。我们在这些细分上画出固定的边界，划分出有界上下文。然后，我们可以将业务能力团队与这些环境相匹配，这些团队将构建提供这些功能的微服务。\n微服务提供了一个有用的定义，用于定义 12 因素应用程序应该是什么。12 因素主要是技术规范，而微服务主要是业务规范。通过定义有界上下文，为它们分配一组业务能力，委托业务能力团队对这些业务能力负责，并建立 12 因素应用程序。在这些应用程序可以独立部署的情况下，为业务能力团队的运维提供了一组有用的技术工具。\n我们将有界上下文与每个服务模式的数据库结合，每个微服务封装、管理和保护自己的领域模型和持久存储。在每个服务模式的数据库中，只允许一个应用程序服务访问逻辑数据存储，逻辑数据存储可能是以多租户集群中的单个 schema 或专用物理数据库中存在。对这些概念的任何外部访问都是通过一个明确定义的业务协议来实现的，该协议的实现方式为 API（通常是 REST，但可能是任何协议）。\n这种分解允许应用拥有多语言支持的持久性，或者基于数据形态和读写访问模式选择不同的数据存储。然而，数据必须经常通过事件驱动技术重新组合，以便请求交叉上下文。诸如命令查询责任隔离（CQRS）和事件溯源（Event Sourcing）之类的技术通常在跨上下文同步类似概念时很有帮助，这超出了本文的范围。\n容器化 容器镜像（例如通过 LXC、Docker 或 Rocket 项目准备的镜像）正在迅速成为云原生应用架构的部署单元。然后通过诸如 Kubernetes、Marathon 或 Lattice 等各种调度解决方案实例化这样的容器镜像。亚马逊和 Google 等公有云供应商也提供一流的解决方案，用于容器化调度和部署。容器利用现代的 Linux 内核原语，如控制组（cgroups）和命名空间来提供类似的资源分配和隔离功能，这些功能与虚拟机提供的功能相比，具有更少的开销和更强的可移植性。应用程序开发人员将需要将应用程序包装成容器镜像，以充分利用现代云基础架构的功能。\n从管弦乐编排到舞蹈编舞 不仅仅服务交付、数据建模和治理必须分散化，服务集成也是如此。企业服务集成传统上是通过企业服务总线（ESB）实现的。ESB 成为管理服务之间交互的所有路由、转换、策略、安全性和其他决策的所有者。我们将其称之为编排，类似于导演，它决定了乐团演出期间演奏音乐的流程。ESB 和编排可以产生非常简单和令人愉快的架构图，但它们的简单性仅仅是表面性的。在 ESB 中隐藏的是复杂的网络。管理这种复杂性成为全职工作，这成为应用开发团队的持续瓶颈。正如我们在联合数据模型所看到的，像 ESB 这样的联合集成解决方案成为阻碍幅度的巨大难题。\n诸如微服务这样的云原生架构更倾向于舞蹈，它们类似于芭蕾舞中的舞者。它们将心智放置在端点上，类似于 Unix 架构中的虚拟管道和智能过滤器，而不是放在集成机制中。当舞台上的情况与原来的计划有所不同时，没有导演告诉舞者该怎么做。相反，他们会自适应。同样，服务通过客户端负载均衡和断路器等模式，适应环境中不断变化的各种情况。\n虽然架构图看起来像一个庞杂的网络，但它们的复杂性并不比传统的 SOA 大。编排简单地承认并暴露了系统原有的复杂性。再次，这种转变是为了支持从云原生架构中寻求速度所需的自治。团队能够适应不断变化的环境，而无需承担与其他团队协调的开销，并避免了在集中管理的 ESB 中协调变更所造成的开销。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"dddfc179912544cb42ac1ceba4e76786","permalink":"https://lib.jimmysong.io/migrating-to-cloud-native-application-architectures/changes-needed/technical-change/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/migrating-to-cloud-native-application-architectures/changes-needed/technical-change/","section":"migrating-to-cloud-native-application-architectures","summary":"现在我们将一些问题转移到了云中的 DevOps 平台。 分解单体应用 传统的 n 层单体式应用部署到云中后很难维护，因为它们经常对云基础设施提供的部署环境做出不可靠的假设，这些假设云很难提供。 例如以下要求： 可访问已挂载的共","tags":null,"title":"2.3 技术变革","type":"book"},{"authors":null,"categories":["安全"],"content":"所涉及的关键原语和实施任务是。\n 管道和 CI/CD 管道的概念 CI/CD 管道的构建块 设计和执行 CI/CD 管道 自动化的战略 CI/CD 管道中对安全自动化工具的要求  3.3.1 管道的概念和 CI/CD 管道 DevSecOps 作为一种敏捷应用开发、部署和运维的方法论或框架，与其他方法论一样，是由 各个阶段 组成的。信息在各阶段中的顺序和流动被称为工作流，其中一些阶段可以平行执行，而其他阶段则必须遵循一个顺序。每个阶段可能需要调用一个独特的工作来执行该阶段的活动。\nDevSecOps 在流程工作流中引入的一个独特概念是 管道 的概念。有了管道，就不需要为启动 / 执行流程的每个阶段单独编写作业。相反，只有一个作业从初始阶段开始，自动触发与其他阶段有关的活动 / 任务（包括顺序的和并行的），并创建一个无错误的智能工作流程。\nDevSecOps 中的管道被称为 CI/CD 管道，这是基于它所完成的总体任务和它所包含的两个单独阶段。CD 可以表示持续交付或持续部署阶段。根据这后一个阶段，CI/CD 可以涉及以下任务：\n 构建、测试、安全和交付：经过测试的修改后的代码被交付到暂存区。 构建、测试、安全、交付和部署：暂存区的代码会自动部署。  在前者中，自动化在交付阶段就结束了，接下来在托管平台基础设施中部署修改后的应用程序的任务是手动执行的。在后者中，部署也是自动化的。管道中任何阶段的自动化都可以通过将管道阶段表达为代码的工具来实现。\nCI/CD 管道的工作流程如下图 1 所示。\n   图 1：CI/CD 管道工作流程  图中所示的单元和集成测试使用了第 3.2 节中描述的 SAST、DAST 和 SCA 工具。应该注意的是，当测试失败时，组织可以选择继续构建过程。根据组织如何平衡风险容忍度和业务需求，当一个特定的测试失败时，它可以选择 fail-open（记录并继续）或 fail-closed（停止 / 中断）。在失败关闭的情况下，开发人员得到测试结果报告，必须修复问题，并重新启动 CI 过程。\n持续集成涉及到开发人员经常将代码变化合并到一个中央存储库中，在那里运行自动化的构建和测试。构建是将源代码转换为可执行代码的过程，以便在其上运行的平台。在 CI/CD 管道软件中，开发者的修改是通过创建构建和运行针对构建的自动测试来验证的。这个过程避免了在等待发布日将修改合并到发布分支时可能发生的 集成挑战。\n持续交付是持续集成之后的一个阶段，在这个阶段，代码的变化在构建阶段之后被部署到测试和 / 或暂存环境。持续交付到生产环境包括指定一个发布频率 —— 每天、每周、每两周或其他时期 —— 基于软件的性质或组织运营的市场。这意味着在自动化测试的基础上，有一个预定的发布过程，尽管应用程序可以通过点击一个按钮在任何时候被部署。持续交付中的部署过程的特点是手动，但诸如代码迁移到生产服务器、建立网络参数和指定运行时配置数据等任务可以由自动化脚本来完成。\n持续部署类似于持续交付，只是发布是 自动进行 的，代码的修改在完成后立即提供给客户。自动发布过程在很多情况下可能包括 A/B 测试，以促进新功能的缓慢上线，以便在出现错误 / 误差时减轻失败的影响。\n持续交付和持续部署之间的区别如图 2 中所示。\n   图 2：持续交付和持续部署之间的区别  3.3.2 CI/CD 管道的构建块 定义 CI/CD 管道资源、构建管道和执行这些管道的主要软件是 CI/CD 管道软件。这类软件的架构可能有轻微的变化，取决于特定的产品。以下是对 CI/CD 工具（管道软件）运行情况的概述：\n 一些 CI/CD 工具在应用程序和相关资源托管的平台上自然运行（即容器编排和资源管理平台），而其他工具需要通过其 API 集成到应用程序托管平台。使用应用托管平台原生的 CI/CD 工具的一些优势是：  它使部署、维护和管理 CI/CD 工具本身更加容易。 CI/CD 工具定义的每个管道都成为另一个平台原生资源，并以同样的方式管理。事实上，执行管道所需的所有实体，如任务和管道（然后分别作为其他实体的蓝图，如任务运行和管道运行），可以作为建立在平台原生资源之上的 自定义资源定义（CRD）来创建。具有这种架构的软件可以被其他 CI/CD 管道软件产品所使用，以促进管道的快速定义。   一些 CI/CD 工具与代码库集成，以扫描 / 检查应用程序代码。这些类型的工具与每个应用程序和每个环境的代码库有关联。当应用程序模块、基础设施或配置发生变化时，它们被存储在这些代码库中。通过 webhooks 或其他方式连接到代码库的 CI/CD 管道软件在提交时被激活（推送工作流模型）或通过来自这些存储库的拉取请求。 一些 CI/CD 工具单独为本地平台执行 CD 功能（例如，Kubernetes 平台的 Jenkins X）或为多个技术栈执行 CD 功能（例如，多云部署的 Spinnaker）。这类工具的困难在于，它们可能缺乏完成 CI 功能的本地工具（例如，测试代码的工具，构建应用镜像，或将其推送到注册中心）。  3.3.3 准备和执行 CI/CD 管道 创建 CI/CD 管道的目的是为了实现源代码的频繁更新、重建，以及将更新的模块自动部署到生产环境中。\n涉及的 关键任务 是：\n 确保 DevSecOps 平台中的所有单个组件（管道软件、SDLC 软件、代码库、可观测性工具等）都可用。 通过认证、验证或定制测试，确保这些组件的安全。 将 CI 和 CD 工具与 SDLC 工具整合起来 —— 访问令牌、调用脚本、管道定义。 根据部署环境（即内部或云端的应用程序托管平台），在 IaC 工具（与 GitOps）中设置配置细节 将运行时工具与部署环境相结合。 设计仪表盘并定义要监控的事件、要生成的警报和要监控的应用程序状态变量（如内存利用率等），通过与日志聚合器、指标生成器和跟踪生成器等工具的连接。  执行任务包括：\n 设置源代码库。建立一个存储库（如 GitHub 或 GitLab），用于存储具有适当版本控制的应用程序源代码。 构建过程。使用自动代码构建工具，配置并执行生成可执行文件的构建过程（对于那些需要更新的代码部分）。 保证过程的安全。通过使用 SAST 和 DAST 工具进行单元测试，确保构建时没有静态和动态的漏洞。这和上面的任务是由 CI 工具激活的。 描述部署环境。这可能涉及描述（使用 IaC）物理 / 虚拟资源，以便在云或企业数据中心部署应用程序。 创建交付管道。创建一个将自动部署应用程序的管道。这个任务和前面的任务是由 CD 工具启用的。 测试代码并执行管道。在适当的测试之后，只要有新的代码出现在资源库中，就执行 CI 工具。当构建过程成功后，执行 CD 工具，将应用程序部署到暂存 / 生产环境中。 激活运行时工具和仪表板，启动运行时监控。  重申一下，CI/CD 流程的三个主要阶段是构建 / 测试、发布 / 打包和部署。以下功能将其转化为一个管道：\n  当一个服务的源代码被更新时，推送到源代码库的代码变化会触发代码构建工具。\n  代码开发环境或代码构建工具（如 IDE），通常与安全测试工具（如静态漏洞分析工具）集成，以促进安全编译代码工件的生成，从而将安全纳入 CI 管道。\n  在代码构建工具中生成的编译代码工件会触发交付 / 打包工具，该工具可能与它自己的一套工具（例如，动态漏洞分析、动态渗透测试工具、用于识别所附库中的漏洞的软件组成分析工具）集成在一起，并且还创建与部署环境有关的配置参数。\n  然后，发布 / 打包工具的输出被自动送入 CD 工具，该工具将软件包部署到 所需的环境 中（例如，中转、生产）。\n  CI/CD 管道的工作流程不应产生无人参与的印象。以下团队 / 角色对 CI/CD 管道做出 贡献：\n 开发团队：这个团队的成员为他们的应用程序申报第三方现成软件（OSS）的依赖性，审查 DevSecOps 系统围绕脆弱依赖性提出的建议，按照建议进行更新，并编写足够的测试案例以确保所有功能验证（消除运行时的错误）。 首席信息安全官（CISO）：在与安全团队协商后，CISO 定义了 DevSecOps 系统的整体范围（深度和广度），从而可以适当地配置它，以满足应用程序的关键任务需求。 安全团队：这个团队的成员按照最佳实践创建管道，包括执行所有必要的安全功能的任务（例如，SBOM 生成、漏洞扫描、代码构建、代码签名、引入新的测试工具、进行审计等）。具体来说，在某些情况下，安全团队的成员可能会负责设计、构建和维护策略即代码和相关管道。 基础设施团队：这个团队的成员创建、维护和升级基础设施。 QA 团队：这个团队的成员开发集成测试案例。 部署团队 / 发布团队：这个团队的成员为各种环境（UAT/PreProd/Prod）创建管道和包，并为这些环境进行适当的配置和供应。  这些团队进行的许多活动中，有一些包括 CI/CD 管道中采用的工具的定制、更新和增强（例如，用最新的已知漏洞数据库更新静态漏洞分析工具）。在手工操作过程中，应谨慎行事，以免阻塞管道。在设定平均生产时间目标的同时，还应通过使用 “合并（GitLab）或拉取（GitHub）请求” 和这些请求的多个批准者来减少风险，如内部威胁。这个管道由发布后团队设计、维护和执行，该团队 —— 除了监控功能 —— 还执行 其他流程，如合规管理、备份流程和资产跟踪。\n3.3.4 自动化的战略 与其他涉及从编码到发布的线性流程的软件开发模式相比，DevOps 使用了一个具有交付管道的前向过程（即构建 / 安全、交付 / 打包和发布）和一个具有反馈回路的反向过程（即计划和监控），形成一个递归的工作流程。自动化在这些活动中的作用是改善工作流程。持续集成强调测试自动化，以确保每当新的提交被集成到主分支时，应用程序不会被破坏。自动化带来了以下好处：\n 产生有关软件静态和运行时流程的数据。 减少开发和部署时间。 架构的内置安全、隐私和合规性。  以下是推荐的 自动化策略，以便于更好地利用组织资源，并在高效、安全的应用环境方面获得最大利益。\n选择要自动化的活动。例如，以下是测试活动自动化的有效候选项目：\n 测试其功能需符合法规要求的模块（如 PCI-DSS、HIPAA、Sarbanes-Oxley）。 具有中度至高度重复性的任务。 测试执行时间序列操作的模块，如消息发布者和消息订阅者。 测试涉及跨越多个服务的事务的工作流程（例如，请求跟踪）。 测试那些资源密集型和可能成为性能瓶颈的服务。  在根据上述标准选择自动化的候选者后，必须应用通常的风险分析来选择一个子集，以提供最佳的回报，并使理想的安全指标（如深度防御）最大化。一些推荐的策略包括：\n 使用每年节省的小时数的成本效益比来确定哪些 流程 需要自动化的优先次序。 使用 关键绩效指标（KPI）（例如，识别故障或问题、纠正或恢复的平均时间）作为标记来完善 DevSecOps 流程。 根据应用，对基础设施服务应用不同的权重（例如，授权和其他策略的执行，监测系统状态以确保安全运行状态，在系统可用性、延迟、从中断中恢复的平均时间等方面的网络弹性），以确定对 DevSecOps 流程的资源分配。  3.3.5 CI/CD 管道中对安全自动化工具的要求 在 CI/CD 管道中使用的各种功能（如静态漏洞分析、动态漏洞分析、软件构成分析）的安全自动化工具需要有不同的接口和警报 / 报告要求，因为它们必须根据使用管道阶段（如构建、打包、发布）而无缝运行。这些要求是：\n 安全自动化工具应与集成开发环境（IDE）工具一起工作，并帮助开发人员优先处理和补救静态漏洞。需要这些功能来促进开发人员的采用和提高生产力。 安全自动化工具应该是灵活的，以支持特定的工作流程，并为安全服务提供扩展能力。 在构建阶段进行静态漏洞检查的工具确保了数据流的安全，而那些进行动态漏洞检查的工具则确保了运行时应用程序状态的安全。  必须提到的是，安全自动化工具是有成本的，因此，这些工具的使用程度是基于风险因素分析。 …","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"b57f84ceeb7188a9904f203ab76de36c","permalink":"https://lib.jimmysong.io/service-mesh-devsecops/devsecops/key-primitives-and-implementation-tasks/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/service-mesh-devsecops/devsecops/key-primitives-and-implementation-tasks/","section":"service-mesh-devsecops","summary":"所涉及的关键原语和实施任务是。 管道和 CI/CD 管道的概念 CI/CD 管道的构建块 设计和执行 CI/CD 管道 自动化的战略 CI/CD 管道中对安全自动化工具的要求 3.3.1 管道的概念和 CI/CD 管道 DevSecOps 作为一种敏捷应用开发、部署和运维的方法论或框架，与其他方法","tags":["Service Mesh","DevSecOps"],"title":"3.3 DevSecOps 关键原语和实施任务","type":"book"},{"authors":null,"categories":null,"content":"本章中我们讨论了两种帮助我们迁移到云原生应用架构的方法：\n分解原架构\n我们使用以下方式分解单体应用：\n 所有新功能都使用微服务形式构建。 通过隔离层将微服务与单体应用集成。 通过定义有界上下文来分解服务，逐步扼杀单体架构。  使用分布式系统\n分布式系统由以下部分组成：\n 版本化，分布式，通过配置服务器和管理总线刷新配置。 动态发现远端依赖。 去中心化的负载均衡策略 通过熔断器和隔板阻止级联故障 通过 API 网关集成到特定的客户端上  还有很多其他的模式，包括自动化测试、持续构建与发布管道等。欲了解更多信息，请阅读 Toby Clemson 的 《 Testing Strategies in a Microservice Architecture》，以及 Jez Humbl 和 David Farley（AddisonWesley）的《Continuous Delivery: Reliable Software Releases through Build, Test, and Deployment Automation》。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"c8427aa24349d60791c1326d6fc8b90f","permalink":"https://lib.jimmysong.io/migrating-to-cloud-native-application-architectures/migration-cookbook/summary/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/migrating-to-cloud-native-application-architectures/migration-cookbook/summary/","section":"migrating-to-cloud-native-application-architectures","summary":"本章中我们讨论了两种帮助我们迁移到云原生应用架构的方法： 分解原架构 我们使用以下方式分解单体应用： 所有新功能都使用微服务形式构建。 通过隔离层将微服务与单体应用集成。 通过定义有界上下文来分解服务，逐步扼杀","tags":null,"title":"3.3 本章小结","type":"book"},{"authors":null,"categories":["安全"],"content":"为应用程序分配基础设施的传统方法包括最初用配置参数和持续的任务配置计算和网络资源，如补丁管理（如操作系统和库），建立符合合规法规（如数据隐私），并进行漂移（当前配置不再提供预期的操作状态）纠正。\n基础设施即代码（IaC）是一种声明式的代码，它对计算机指令进行编码，这些指令封装了通过服务的管理 API 在公共云服务或私有数据中心部署虚拟基础设施所需的 参数。换句话说，基础设施是以声明式的方式定义的，并使用用于应用程序代码的相同的源代码控制工具（如 GitOps）进行版本控制。根据特定的 IaC 工具，这种语言可以是脚本语言（如 JavaScript、Python、TypeScript 等）或专有配置语言（如 HCL），可能与标准化语言（如 JSON）兼容也可能不兼容。基本指令包括告诉系统如何配置和管理 基础设施（无论是单个计算实例还是完整的服务器，如物理服务器或虚拟机）、容器、存储、网络连接、连接拓扑和负载均衡器。在某些情况下，基础设施可能是短暂的，基础设施的寿命（无论是不可变的还是可变的）不需要继续配置管理。配置可以与应用程序代码的单个提交相联系，使用的工具可以将应用程序代码和基础设施代码以一种合乎逻辑、富有表现力、为开发和运维团队所熟悉的方式连接起来，其中应用程序代码越来越多地定义了云应用的基础设施资源 要求。\n因此，IaC 涉及编码所有的软件部署任务（分配服务器的类型，如裸机、虚拟机或容器，服务器的资源内容）和这些服务器及其网络的配置。包含这种代码类型的软件也被称为资源管理器或部署管理器。换句话说，IaC 软件可以自动管理整个 IT 基础设施的生命周期（资源的配置和取消配置），并实现一个可编程的基础设施。将这种软件作为 CI/CD 管道的一部分进行整合，不仅可以实现敏捷的部署和维护，还可以实现安全和满足性能需求的强大应用平台。\n4.3.1 对 IaC 的保护 当基础设施是 IaC 中的代码时，它可能包括有可能成为漏洞的 bug 和疏忽，因此，就像在应用程序代码中一样被利用。因此，保护 IaC 就是保护基础设施的定义和最终的部署环境。任何一段 IaC 在进入 GitOps 并被合并之前，都必须进行潜在漏洞的扫描。\n此外，只有当有一个有条不紊的漂移管理过程时，才能获得安全应用平台的保证。只有当 IaC 中定义的架构是部署环境中实际存在的架构时，才能获得这种保证，因为这种等同性可能会被通过控制台或 CLI 进行的无意或有意的更改所改变，从而绕过 IaC。确保这种对等性必须在部署后立即进行，并在运行期间定期进行，因为对架构的任何改变都可能导致引入安全设计缺陷，并可能需要对 IaC 进行修改。\n4.3.2 配置和基础设施之间的区别 基础设施经常与 配置 相混淆，后者将计算机系统、软件、依赖关系和设置维持在一个理想的、一致的状态。例如，将一台新购买的服务器放到机架上，并将其连接到交换机上，使其与现有网络相连（或启动一个新的虚拟机并为其分配网络接口），属于基础设施的定义。相反，在服务器启动后，安装 HTTPS 服务器并对其进行配置属于配置管理。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"bf72042ef9f8624205c67198f5763749","permalink":"https://lib.jimmysong.io/service-mesh-devsecops/implement/ci-cd-pipeline-for-infrastructure-as-code/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/service-mesh-devsecops/implement/ci-cd-pipeline-for-infrastructure-as-code/","section":"service-mesh-devsecops","summary":"为应用程序分配基础设施的传统方法包括最初用配置参数和持续的任务配置计算和网络资源，如补丁管理（如操作系统和库），建立符合合规法规（如数据隐私），并进行漂移（当前配置不再提供预期的操作状态）纠正。 基础设","tags":["Service Mesh","DevSecOps"],"title":"4.3 基础设施即代码的 CI/CD 管道","type":"book"},{"authors":null,"categories":["Envoy"],"content":"在这一节中，我们将解释 Envoy 的基本构建模块。\nEnvoy 配置的根被称为引导配置。它包含了一些字段，我们可以在这里提供静态或动态的资源和高级别的 Envoy 配置（例如，Envoy 实例名称、运行时配置、启用管理界面等等）。\n为了开始学习，我们将主要关注静态资源，在课程的后面，我们将介绍如何配置动态资源。\nEnvoy 输出许多统计数据，这取决于启用的组件和它们的配置。我们会在整个课程中提到不同的统计信息，在课程后面的专门模块中，我们会更多地讨论统计信息。\n下图显示了通过这些概念的请求流。\n   Envoy 构建块  这一切都从监听器开始。Envoy 暴露的监听器是命名的网络位置，可以是一个 IP 地址和一个端口，也可以是一个 Unix 域套接字路径。Envoy 通过监听器接收连接和请求。考虑一下下面的 Envoy 配置。\nstatic_resources:listeners:- name:listener_0address:socket_address:address:0.0.0.0port_value:10000filter_chains:[{}]通过上面的 Envoy 配置，我们在 0.0.0.0 地址的 10000 端口上声明了一个名为 listener_0 的监听器。这意味着 Envoy 正在监听 0.0.0.0:10000 的传入请求。\n每个监听器都有不同的部分需要配置。然而，唯一需要的设置是地址。上述配置是有效的，你可以用它来运行 Envoy—— 尽管它没有用，因为所有的连接都会被关闭。\n我们让 filter_chains 字段为空，因为在接收数据包后不需要额外的操作。\n为了进入下一个构件（路由），我们需要创建一个或多个网络过滤器链（filter_chains），至少要有一个过滤器。\n网络过滤器通常对数据包的有效载荷进行操作，查看有效载荷并对其进行解析。例如，Postgres 网络过滤器解析数据包的主体，检查数据库操作的种类或其携带的结果。\nEnvoy 定义了三类过滤器：监听器过滤器、网络过滤器和 HTTP 过滤器。监听器过滤器在收到数据包后立即启动，通常对数据包的头信息进行操作。监听器过滤器包括代理监听器过滤器（提取 PROXY 协议头），或 TLS 检查器监听器过滤器（检查流量是否为 TLS，如果是，则从 TLS 握手中提取数据）。\n每个通过监听器进来的请求可以流经多个过滤器。我们还可以写一个配置，根据传入的请求或连接属性选择不同的过滤器链。\n   过滤器链  一个特殊的、内置的网络过滤器被称为 HTTP 连接管理器过滤器（HTTP Connection Manager Filter）或 HCM。HCM 过滤器能够将原始字节转换为 HTTP 级别的消息。它可以处理访问日志，生成请求 ID，操作头信息，管理路由表，并收集统计数据。我们将在以后的课程中对 HCM 进行更详细的介绍。\n就像我们可以为每个监听器定义多个网络过滤器（其中一个是 HCM）一样，Envoy 也支持在 HCM 过滤器中定义多个 HTTP 级过滤器。我们可以在名为 http_filters 的字段下定义这些 HTTP 过滤器。\n   HCM 过滤器  HTTP 过滤器链中的最后一个过滤器必须是路由器过滤器（envoy.filters.HTTP.router）。路由器过滤器负责执行路由任务。这最终把我们带到了第二个构件 —— 路由。\n我们在 HCM 过滤器的 route_config 字段下定义路由配置。在路由配置中，我们可以通过查看元数据（URI、Header 等）来匹配传入的请求，并在此基础上，定义流量的发送位置。\n路由配置中的顶级元素是虚拟主机。每个虚拟主机都有一个名字，在发布统计数据时使用（不用于路由），还有一组被路由到它的域。\n让我们考虑下面的路由配置和域的集合。\nroute_config:name:my_route_configvirtual_hosts:- name:tetrate_hostsdomains:[\u0026#34;tetrate.io\u0026#34;]routes:...- name:test_hostsdomains:[\u0026#34;test.tetrate.io\u0026#34;,\u0026#34;qa.tetrate.io\u0026#34;]routes:...如果传入请求的目的地是 tetrate.io（即 Host/Authority 标头被设置为其中一个值），则 tetrate_hosts  虚拟主机中定义的路由将得到处理。\n同样，如果 Host/Authority 标头包含 test.tetrate.io 或 qa.tetrate.io，test_hosts 虚拟主机下的路由将被处理。使用这种设计，我们可以用一个监听器（0.0.0.0:10000）来处理多个顶级域。\n如果你在数组中指定多个域，搜索顺序如下：\n 精确的域名（例如：tetrate.io）。 后缀域名通配符（如 *.tetrate.io）。 前缀域名通配符（例如：tetrate.*）。 匹配任何域的特殊通配符（*）。  在 Envoy 匹配域名后，是时候处理所选虚拟主机中的 routes 字段了。这是我们指定如何匹配一个请求，以及接下来如何处理该请求（例如，重定向、转发、重写、发送直接响应等）的地方。\n我们来看看一个例子。\nstatic_resources:listeners:- name:listener_0address:socket_address:address:0.0.0.0port_value:10000filter_chains:- filters:- name:envoy.filters.network.http_connection_managertyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManagerstat_prefix:hello_world_servicehttp_filters:- name:envoy.filters.http.routerroute_config:name:my_first_routevirtual_hosts:- name:direct_response_servicedomains:[\u0026#34;*\u0026#34;]routes:- match:prefix:\u0026#34;/\u0026#34;direct_response:status:200body:inline_string:\u0026#34;yay\u0026#34;配置的顶部部分与我们之前看到的一样。我们已经添加了 HCM 过滤器、统计前缀（hello_world_service）、单个 HTTP 过滤器（路由器）和路由配置。\n在虚拟主机内，我们要匹配任何域名。在 routes 下，我们匹配前缀（/），然后我们可以发送一个响应。\n当涉及到匹配请求时，我们有多种选择。\n   路由匹配 描述 示例     prefix 前缀必须与:path 头的开头相符。 /hello 与 hello.com/hello、hello.com/helloworld 和 hello.com/hello/v1 匹配。   path 路径必须与:path 头完全匹配。 /hello 匹配 hello.com/hello，但不匹配 hello.com/helloworld 或 hello.com/hello/v1   safe_regex 所提供的正则表达式必须与:path 头匹配。 /\\{3} 匹配任何以 / 开头的三位数。例如，与 hello.com/123 匹配，但不能匹配 hello.com/hello 或 hello.com/54321。   connect_matcher 匹配器只匹配 CONNECT 请求。     一旦 Envoy 将请求与路由相匹配，我们就可以对其进行路由、重定向或返回一个直接响应。在这个例子中，我们通过 direct_response 配置字段使用直接响应。\n你可以把上述配置保存到 envoy-direct-response.yaml 中。\n我们将使用一个名为 func-e 的命令行工具。func-e 允许我们选择和使用不同的 Envoy 版本。\n我们可以通过运行以下命令下载 func-e CLI。\ncurl https://func-e.io/install.sh | sudo bash -s -- -b /usr/local/bin 现在用我们创建的配置运行 Envoy。\n\n  下面的命令基于 Envoy 1.20 版本，对于 Envoy 1.22 或更新版本会有兼容性问题，见此 Issue。   func-e use 1.20 func-e run -c envoy-direct-response.yaml 一旦 Envoy 启动，我们就可以向 localhost:10000 发送一个请求，以获得我们配置的直接响应。\n$ curl localhost:10000 yay 同样，如果我们添加一个不同的主机头（例如 -H \u0026#34;Host: hello.com\u0026#34;）将得到相同的响应，因为 hello.com 主机与虚拟主机中定义的域相匹配。\n在大多数情况下，从配置中直接发送响应是一个很好的功能，但我们会有一组端点或主机，我们将流量路由到这些端点或主机。在 Envoy 中做到这一点的方法是通过定义集群。\n集群（Cluster）是一组接受流量的上游类似主机。这可以是你的服务所监听的主机或 IP 地址的列表。\n例如，假设我们的 hello world 服务是在 127.0.0.0:8000 上监听。然后，我们可以用一个单一的端点创建一个集群，像这样。\nclusters:- name:hello_world_serviceload_assignment:cluster_name:hello_world_serviceendpoints:- lb_endpoints:- endpoint:address:socket_address:address:127.0.0.1port_value:8000集群的定义与监听器的定义在同一级别，使用 clusters 字段。我们在路由配置中引用集群时，以及在导出统计数据时，都会使用集群。该名称在所有集群中必须是唯一的。\n在 load_assignment 字段下，我们可以定义要进行负载均衡的端点列表，以及负载均衡策略设置。\nEnvoy 支持多种负载均衡算法（round-robin、Maglev、least-request、random），这些算法是由静态引导配置、DNS、动态 xDS（CDS 和 EDS 服务）以及主动 / 被动健康检查共同配置的。如果我们没有通过 lb_policy 字段明确地设置负载均衡算法，它默认为 round-robin。\nendpoints 字段定义了一组属于特定地域的端点。使用可选的 locality 字段，我们可以指定上游主机的运行位置，然后在负载均衡过程中使用（即，将请求代理到离调用者更近的端点）。\n添加新的端点指示负载均衡器在一个以上的接收者之间分配流量。通常情况下，负载均衡器对所有端点一视同仁，但集群定义允许在端点内建立一个层次结构。\n例如，端点可以有一个 权重（weight） 属性，这将指示负载均衡器与其他端点相比，向这些端点发送更多 / 更少的流量。\n另一种层次结构类型是基于地域性的（locality），通常用于定义故障转移架构。这种层次结构允许我们定义地理上比较接近的 “首选” 端点，以及在 “首选” 端点变得不健康的情况下应该使用的 “备份” 端点。\n由于我们只有一个端点，所以我们还没有设置 locality。在 lb_endpoints 字段下，可以定义 Envoy 可以路由流量的实际端点。\n我们可以在 Cluster 中配置以下可选功能：\n 主动健康检查（health_checks） …","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"b5b6eefc1bd51b60edc27e4b851d5743","permalink":"https://lib.jimmysong.io/envoy-handbook/intro/envoy-building-blocks/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/envoy-handbook/intro/envoy-building-blocks/","section":"envoy-handbook","summary":"在这一节中，我们将解释 Envoy 的基本构建模块。 Envoy 配置的根被称为引导配置。它包含了一些字段，我们可以在这里提供静态或动态的资源和高级别的 Envoy 配置（例如，Envoy 实例名称、运行时配置、启用管理界面等等）。 为了开","tags":["Envoy"],"title":"Envoy 的构建模块","type":"book"},{"authors":null,"categories":["云原生"],"content":"基础架构是指支持应用程序的所有软件和硬件，包括数据中心、操作系统、部署流水线、配置管理以及支持应用程序生命周期所需的任何系统或软件。\n已经有无数的时间和金钱花在了基础架构上。通过多年来不断的技术演化和实践提炼，有些公司已经能够运行大规模的基础架构和应用程序，并且拥有卓越的敏捷性。高效运行的基础架构可以使得迭代更快，缩短投向市场的时间，从而加速业务发展。\n使用云原生基础架构是有效运行云原生应用程序的要求。如果没有正确的设计和实践来管理基础架构，即使是最好的云原生应用程序也会浪费。本书中的实践并不一定需要有巨大的基础架构规模，但如果您想从云计算中获取回报，您应该听从开创了这些模式的人的经验。\n在我们探索如何构建云中运行的应用程序的基础架构之前，我们需要了解我们是如何走到这一步。首先，我们将讨论采用云原生实践的好处。接下来，我们将看一下基础架构的简历，然后讨论下一阶段的功能，称为 “云原生”，以及它与您的应用程序、运行的平台及业务之间的关系。\n在明白了这一点后，我们将向您展示解决方案及实现。\n云原生的优势 采用本书中的模式有很多好处。它们仿照谷歌、Netflix 和亚马逊这些成功的公司 —— 不是单靠模式保证它们的成功，而是它们提供了这些公司成功所需的可扩展性和敏捷性。\n通过选择在公有云中运行基础架构，您可以更快地创造价值并专注于业务目标。只需构建您的产品所需的内容，并从其他提供商那里获得服务，就可以缩短交付时间，提高灵活性。有些人可能因为 “供应商锁定” 而犹豫不决，但最糟糕的锁定是您自己建立的锁定。有关不同类型的锁定，以及您应该如何处理的更多信息，请参阅附录 B。\n消费服务还可让您使用所需服务构建定制平台（有时称为服务即平台 [SaaP]）。当您使用云托管的服务时无需精专于管理应用程序所需要的每项服务。这极大地加强了业务变更和业务增值的能力。\n当您无法使用服务时，您应该构建应用程序来管理基础架构。当您这样做时，规模瓶颈不再取决于每个运维工程师可以管理多少台服务器。相反，您可以像扩展应用程序一样来扩展您的基础架构。换句话说，如果您能够运行可扩展的应用程序，则可以使用应用程序扩展您的基础架构。\n同样的好处适用于构建灵活且易于调试的基础架构。您可以使用与管理业务应用程序相同的工具来洞察您的基础架构。\n云原生实践还可以缩小传统工程角色之间的差距（DevOps 的共同目标）。系统工程师将能够从应用程序中学习最佳实践，开发工程师可以拥有应用程序运行所在的基础架构的所有权。\n云原生基础架构的解决方案不一定适用于所有问题，您有责任了解它是否适合您的环境（参见第 2 章）。然而，在创造了这些实践的公司以及采用以该模式创建的工具的公司中，云原生基础架构的成功可以说显而易见。请参见附录 C 的一个例子。\n在深入了解解决方案之前，先让我是探究一下是什么问题导致这些模式的出现。\n服务器 在互联网早期，Web 基础架构始于物理服务器。服务器庞大，吵闹且昂贵，需要大量的电力和人员投入以保持它们的运行。需要细心照料，尽可能保持长时间运行。 与云基础架构相比，购买这些设备让应用程序运行在上面会更困难。\n一旦您买了服务器，它就是您的了，无论好坏，都要维护。使用物理服务器适合已确定成本的业务。持有物理服务器并运行的时间越长，您花费的钱越多。做适当的产能规划并确保您获得最佳的投资回报是很重要的。\n物理服务器非常棒，因为它们功能强大，可以根据需要进行配置。故障率相对较低，使用冗余电源供应，风扇和 RAID 控制器来避免出现故障。也可以持续运行很长时间。企业可以通过延长保修和更换零部件，从购买的硬件中挤出额外的价值。\n但是，物理服务器会导致浪费。服务器不仅没有被充分利用，而且还带来了很多开销。在同一台服务器上运行多个应用程序是很困难的。当在同一台服务器上最大限度得部署多个应用程序时，软件冲突，网络路由和用户访问都变得更加复杂。\n硬件虚拟化承诺可以解决其中的一些问题。\n虚拟化 虚拟化使用软件来模拟物理服务器的硬件。虚拟服务器可以按需创建，完全可以通过软件编程，只要您可以模拟硬件，就永远不会出现损耗。\n使用 hypervisor 可以增加这些优势，因为您可以在物理服务器上运行多个虚拟机（VM）。它还使得应用程序可移植，因为您可以将虚拟机从一台物理服务器移动到另一台物理服务器。\n然而，运行自己的虚拟化平台的一个问题是虚拟机仍然需要硬件来运行。公司仍然需要拥有运行物理服务器所需的所有人员和流程，但是现在容量规划变得更加困难，因为他们还必须考虑到虚拟机的开销。至少，公有云出现之前就是如此。\n基础架构即服务 基础架构即服务（IaaS）是云提供商的众多产品之一。它提供了原始的网络、存储和计算能力，客户可以根据需要使用它们。它还包括一些支持服务，如身份和访问管理（IAM）、供应和库存系统。\nIaaS 允许公司摆脱他们的所有硬件，并从别人那里租用虚拟机或物理服务器。这释放了大量人力资源，摆脱了购买、维护以及在某些情况下容量规划所需的流程。\nIaaS 从根本上改变了基础架构与业务的关系。不是随着时间的推移受益的资本支出，而是运营业务的运营支出。企业可以像支付电力和人们的时间一样支付基础架构。通过基于消费的计费，您越早摆脱基础架构，运营成本就越低。\n托管的基础架构还为客户提供了可消费的 HTTP 应用编程接口（API），以便按需创建和管理基础架构。工程师不需要购买订单并等待物品出货，就可以进行 API 调用，并创建服务器。服务器可以轻松删除和丢弃。\n在云中运行基础架构不会使您的基础架构成为云原生。IaaS 仍然需要基础架构管理。在购买和管理物理资源之外，您可以（也有许多公司）认为 IaaS 与过去购买服务器在自己的数据中心架设的传统基础架构一模一样。\n即使没有 “货架和堆叠”，仍然有大量的操作系统、监控软件和支持工具。自动化工具帮助减少了运行应用程序所需的时间，但通常根深蒂固的流程会削弱 IaaS 的优势。\n平台即服务 就像 IaaS 对 VM 消费者隐藏了物理服务器一样，平台即服务（PaaS）也对应用程序隐藏了操作系统。开发人员编写应用程序代码并定义应用程序的依赖关系，平台负责创建运行，管理和暴露它所必要的基础架构。与需要基础架构管理的 IaaS 不同，PaaS 中的基础架构由平台提供商管理。\n事实证明，PaaS 限制要求开发人员以不同的方式编写应用程序，以便平台可以有效管理。应用程序必须包含允许由平台管理而不访问底层操作系统的功能。工程师不能再依赖 SSH 登入到服务器来读取磁盘上的日志文件。现在应用程序的生命周期和管理由 PaaS 控制，工程师和应用程序需要适应这个流程。\n这些限制带来了很大的好处。应用程序开发周期变短了，因为工程师不需要花时间管理基础架构。在平台上运行的应用程序是我们现在称为 “云原生应用程序” 的开始。利用代码中的平台限制，已经一定程度上改变了当今编写应用程序的方式。\n 12 因素应用程序\nHeroku 是提供公有 PaaS 的早期先驱之一。通过自己平台的多年扩展，该公司能够确定帮助应用程序在其环境中更好运行的模式。Heroku 定义了应用程序时应实现的 12 个主要因素。\n这 12 个因素是通过将代码逻辑与数据分离来使开发人员更高效，尽可能自动化，独立的构建、传输和运行阶段过程；并声明所有的应用程序的依赖关系。\n 如果您使用 PaaS 提供商所提供的基础架构，恭喜您已拥有云原生基础架构的诸多优势。这包括 Google App Engine、AWS Lambda 和 Azure Cloud Services 等平台。任何成功的云原生基础架构都将向应用工程师展示自助服务平台，以部署和管理代码。\n但是，许多 PaaS 平台不足以满足业务需求。它们通常会限制平台运行的语言、库和功能以实现从应用程序中抽离基础架构的承诺。公有 PaaS 提供商还将限制哪些服务可以与应用程序集成以及这些应用程序可以在哪里运行。\n公有平台牺牲了应用程序的灵活性，使基础架构成为别人的问题。图 1-1 是如果您运行自己的数据中心，在 IaaS 中创建基础架构，在 PaaS 上运行应用程序或通过软件即服务（SaaS）运行应用程序时需要管理的组件的直观表示。\n您需要运行的基础架构组件越少越好；但是在公有 PaaS 提供商中运行所有应用程序可能不是一种选择。\n   图 1-1. 基础架构层  云原生基础架构 “云原生” 是一个过度使用的术语。尽管它已被市场所劫持，但仍具有工程和管理上的意义。对我们来说，它意味着在这个公有云提供商的世界中正发生的技术变革。\n云原生基础架构是隐藏在有用的抽象背后的基础架构，由 API 控制，由软件管理并具有运行应用程序的目的。利用这些特征运行基础架构，能以可扩展高效的方式管理该基础架构。\n当它们成功地向消费者隐藏复杂性时，抽象是有用的。它们可以实现技术的更复杂的使用，但是它们也限制了技术的使用方式。它们适用于底层技术，例如 TCP 如何提取 IP 或更高级别的技术，如虚拟机如何抽象物理服务器。抽象应该总是允许消费者 “向上移动堆栈” 而不是重新实现底层。\n云原生基础架构需要抽象基础 IaaS 产品以提供自己的抽象。新层负责控制它下面的 IaaS，并将自己的 API 暴露给消费者控制。\n由软件管理的基础架构是云中的一个关键区别点。软件控制的基础架构使基础架构能够扩展，并且在弹性、供应和可维护性方面也发挥着重要作用。软件需要了解基础架构的抽象概念，并知道如何获取抽象资源并相应地在可消费的 IaaS 组件中实现它。\n这些模式不仅影响基础架构的运行方式，而且在云原生基础架构上运行的应用程序类型、在其上工作的人员类型与传统基础架构中是不同的。\n如果云原生基础架构看起来很像 PaaS 产品，那么我们如何才能知道构建自己的产品时需要注意什么？我将快速描述一些领域，它们可能看起来像是云原生解决方案，但不提供云原生基础架构的所有方面。\n什么不是云原生基础架构？ 云原生基础架构不等于在公有云上运行基础架构。仅租用服务器并不会使您的基础架构云原生化。管理 IaaS 的流程与运行物理数据中心通常没有什么不同，许多将现有基础架构迁移到云的公司都未能获得回报。\n云原生不等于在容器中运行应用程序。Netflix 最先推出云原生基础架构时，几乎所有应用程序都部署在虚拟机中，而不是在容器中。使用容器的方式打包应用程序并不能意味着拥有了自治系统的可扩展性和优势。即使应用程序是通过持续集成和持续交付渠道自动构建和部署的，也不等于就可以从 API 驱动部署的基础架构中受益。\n也不是说只要您运行了容器编排器（例如 Kubernetes 和 Mesos）就是云原生架构。容器编排器提供了云原生基础架构所需的平台功能，但如果未按预期方式使用这些功能的话，那也只是将应用程序会动态调度到一组服务器上而已。这是一个非常好的起步，但仍有很多工作要做。\n 调度器与编排器\n“调度器” 和 “编排器” 这两个术语通常可以互换使用。\n在大多数情况下，编排器负责集群中的所有资源使用（例如：存储，网络和 CPU）。该术语通常用于描述执行许多任务的产品，如健康检查和云自动化。\n 调度器是编排平台的一个子集，仅负责为进程和服务选择所运行的服务器。\n云原生不等于微服务或基础架构即代码。微服务意味着更快的开发周期和更小的独立功能，但是单体应用程序可以具有相同的功能，使其能够通过软件有效管理，并且还可以从云原生基础架构中受益。\n基础架构即代码以机器可解析的语言或领域特定语言（DSL）定义、使基础架构自动化。使用代码管理基础架构的传统工具包括配置管理工具，例如 Chef 和 Puppet。这些工具在自动执行任务和提供一致性方面很有用，但是对于为超出单个服务器的基础架构提供必要的抽象描述方面存在缺陷。\n配置管理工具一次只自动化一台服务器，并靠人将服务器提供的功能绑定在一起。人成了管理大规模基础架构的潜在瓶颈。这些工具也不会使构建完整系统所需的云基础架构（例如存储和网络）的额外部分自动化。\n尽管配置管理工具为操作系统的资源（例如软件包管理器） …","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"f28b6734d0a0fe9af73d53e56e99a3ac","permalink":"https://lib.jimmysong.io/cloud-native-infra/what-is-cloud-native-infrastructure/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/cloud-native-infra/what-is-cloud-native-infrastructure/","section":"cloud-native-infra","summary":"基础架构是指支持应用程序的所有软件和硬件，包括数据中心、操作系统、部署流水线、配置管理以及支持应用程序生命周期所需的任何系统或软件。 已经有无数的时间和金钱花在了基础架构上。通过多年来不断的技术演化和实","tags":["云原生"],"title":"第 1 章：什么是云原生基础架构？","type":"book"},{"authors":null,"categories":["安全"],"content":"Kubernetes® 是一个开源系统，可以自动部署、扩展和管理在容器中运行的应用程序，并且通常托管在云环境中。与传统的单体软件平台相比，使用这种类型的虚拟化基础设施可以提供一些灵活性和安全性的好处。然而，安全地管理从微服务到底层基础设施的所有方面，会引入其他的复杂性。本报告中详述的加固指导旨在帮助企业处理相关风险并享受使用这种技术的好处。\nKubernetes 中三个常见的破坏源是供应链风险、恶意威胁者和内部威胁。\n供应链风险往往是具有挑战性的，可以在容器构建周期或基础设施收购中出现。恶意威胁者可以利用 Kubernetes 架构的组件中的漏洞和错误配置，如控制平面、工作节点或容器化应用程序。内部威胁可以是管理员、用户或云服务提供商。对组织的 Kubernetes 基础设施有特殊访问权的内部人员可能会滥用这些特权。\n本指南描述了与设置和保护 Kubernetes 集群有关的安全挑战。包括避免常见错误配置的加固策略，并指导国家安全系统的系统管理员和开发人员如何部署 Kubernetes，并提供了建议的加固措施和缓解措施的配置示例。本指南详细介绍了以下缓解措施：\n 扫描容器和 Pod 的漏洞或错误配置。 以尽可能少的权限运行容器和 Pod。 使用网络隔离来控制漏洞可能造成的损害程度。 使用防火墙来限制不需要的网络连接，并使用加密技术来保护机密。 使用强大的认证和授权来限制用户和管理员的访问，以及限制攻击面。  使用日志审计，以便管理员可以监控活动，并对潜在的恶意活动发出警告。\n定期审查所有 Kubernetes 设置，并使用漏洞扫描，以帮助确保风险得到适当考虑并应用安全补丁。\n有关其他安全加固指导，请参见互联网安全中心 Kubernetes 基准、Docker 和 Kubernetes 安全技术实施指南、网络安全和基础设施安全局（CISA）分析报告以及 Kubernetes 文档。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"c5a39f5eebb33101c46b6bb37c408814","permalink":"https://lib.jimmysong.io/kubernetes-hardening-guidance/executive-summary/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/kubernetes-hardening-guidance/executive-summary/","section":"kubernetes-hardening-guidance","summary":"Kubernetes® 是一个开源系统，可以自动部署、扩展和管理在容器中运行的应用程序，并且通常托管在云环境中。与传统的单体软件平台相比，使用这种类型的虚拟化基础设施可以提供一些灵活性和安全性的好处。然","tags":["Kubernetes","安全"],"title":"执行摘要","type":"book"},{"authors":null,"categories":["云原生"],"content":"本文参考的是 OAM 规范中对云原生应用的定义，并做出了引申。\n云原生应用是一个相互关联但又不独立的组件（service、task、worker）的集合，这些组件与配置结合在一起并在适当的运行时实例化后，共同完成统一的功能目的。\n云原生应用模型 下图是 OAM 定义的云原生应用模型示意图，为了便于理解，图中相同颜色的部分为同一类别的对象定义。\n   云原生应用模型  OAM 的规范中定义了以下对象，它们既是 OAM 规范中的基本术语也是云原生应用的基本组成。\n Workload（工作负载）：应用程序的工作负载类型，由平台提供。 Component组件）：定义了一个 Workload 的实例，并以基础设施中立的术语声明其运维特性。 Trait（特征）：用于将运维特性分配给组件实例。 ApplicationScope（应用作用域）：用于将组件分组成具有共同特性的松散耦合的应用。 ApplicationConfiguration（应用配置）：描述 Component 的部署、Trait 和 ApplicationScope。  关注点分离 下图是不同角色对于该模型的关注点示意图。\n   云原生应用模型中的目标角色  我们可以看到对于一个云原生应用来说，不同的对象是由不同的角色来负责的：\n 基础设施运维：提供不同的 Workload 类型供开发者使用； 应用运维：定义适用于不同 Workload 的运维属性 Trait 和管理 Component 的 ApplicationScope 即作用域； 应用开发者：负责应用组件 Component 的定义； 应用开发者和运维：共同将 Component 与运维属性 Trait 绑定在一起，维护应用程序的生命周期；  基于 OAM 中的对象定义的云原生应用可以充分利用平台能力自由组合，开发者和运维人员的职责可以得到有效分离，组件的复用性得到大幅提高。\n定义标准 CNCF 中的有几个定义标准的「开源项目」，其中有的项目都已经毕业。\n SMI（Service Mesh Interface）：服务网格接口 Cloud Events：Serverless 中的事件标准 TUF：更新框架标准 SPIFFE：身份安全标准  这其中唯独没有应用定义标准，CNCF SIG App delivery 即是要做这个的。当然既然要指定标准，自然要对不同平台和场景的逻辑做出更高级别的抽象（这也意味着你在掌握了底层逻辑的情况下还要学习更多的概念），这样才能屏蔽底层差异。\nOAM 简介 OAM 全称是 Open Application Model，从名称上来看它所定义的就是一种模型，同时也实现了基于 OAM 的我认为这种模型旨在定义了云原生应用的标准。\n 开放（Open）：支持异构的平台、容器运行时、调度系统、云供应商、硬件配置等，总之与底层无关 应用（Application）：云原生应用 模型（Model）：定义标准，以使其与底层平台无关  既然要制定标准，自然要对不同平台和场景的逻辑做出更高级别的抽象（这也意味着你在掌握了底层逻辑的情况下还要学习更多的概念），这样才能屏蔽底层差异。本文将默认底层平台为 Kubernetes。\n 是从管理大量 CRD 中汲取的经验。 业务和研发的沟通成本，比如 YAML 配置中很多字段是开发人员不关心的。  设计原则 OAM 规范的设计遵循了以下原则：\n 关注点分离：根据功能和行为来定义模型，以此划分不同角色的职责， 平台中立：OAM 的实现不绑定到特定平台； 优雅：尽量减少设计复杂性； 复用性：可移植性好，同一个应用程序可以在不同的平台上不加改动地执行； 不作为编程模型：OAM 提供的是应用程序模型，描述了应用程序的组成和组件的拓扑结构，而不关注应用程序的具体实现。  下图是 OAM 规范示意图。\n   OAM 规范示意图  OAM 工作原理 OAM 的工作原理如下图所示（图片引用自孙健波在《OAM: 云原生时代的应用模型与 下一代 DevOps 技术》中的分享）。\n   OAM 的原理  OAM Spec 定义了云原生应用的规范（使用一些 CRD 定义）， KubeVela 可以看做是 OAM 规范的解析器，将应用定义翻译为 Kubernetes 中的资源对象。可以将上图分为三个层次：\n 汇编层：即人工或者使用工具来根据 OAM 规范定义汇编出一个云原生应用的定义，其中包含了该应用的工作负载和运维能力配置。 转义层：汇编好的文件将打包为 YAML 文件，由 KubeVela 或其他 OAM 的实现将其转义为 Kubernetes 或其他云服务（例如 Istio）上可运行的资源对象。 执行层：执行经过转义好的云平台上的资源对象并执行资源配置。  参考  The Open Application Model specification - github.com  ","date":1651532400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"426673f3b33529cb05073dd8ae9afabb","permalink":"https://lib.jimmysong.io/cloud-native-handbook/intro/define-cloud-native-app/","publishdate":"2022-05-03T00:00:00+01:00","relpermalink":"/cloud-native-handbook/intro/define-cloud-native-app/","section":"cloud-native-handbook","summary":"本文参考的是 OAM 规范中对云原生应用的定义，并做出了引申。 云原生应用是一个相互关联但又不独立的组件（service、task、worker）的集合，这些组件与配置结合在一起并在适当的运行时实例化后，共同完","tags":["云原生"],"title":"什么是云原生应用？","type":"book"},{"authors":null,"categories":["云原生"],"content":"Kubernetes 一词来自希腊语，意思是 “飞行员” 或 “舵手”。这个名字很贴切，Kubernetes 可以帮助你在波涛汹涌的容器海洋中航行。\nKubernetes 是做什么的？什么是 Docker？什么是容器编排？Kubernetes 是如何工作和扩展的？你可能还有很多其他的问题，本文将一一为你解答。\n这篇文章适合初学者，尤其是那些工作忙碌，没有办法抽出太多时间来了解 Kubernetes 和云原生的开发者们，希望本文可以帮助你进入 Kubernetes 的世界。\n简而言之，Kubernetes 提供了一个平台或工具来帮助你快速协调或扩展容器化应用，特别是在 Docker 容器。让我们深入了解一下这些概念。\n容器和容器化 那么什么是容器呢？\n要讨论容器化首先要谈到虚拟机 (VM)，顾名思义，虚拟机就是可以远程连接的虚拟服务器，比如 AWS 的 EC2 或阿里云的 ECS。\n接下来，假如你要在虚拟机上运行一个网络应用 —— 包括一个 MySQL 数据库、一个 Vue 前端和一些 Java 库，在 Ubuntu 操作系统 (OS) 上运行。你不用熟悉其中的每一个技术 —— 你只要记住，一个应用程序由各种组件、服务和库组成，它们运行在操作系统上。\n现在，将应用程序打包成一个虚拟机镜像，这个镜像中包括了 Ubuntu 操作系统。这使得虚拟机变得非常笨重 —— 通常有几个 G 的大小。\n虚拟机镜像包含了整个操作系统及所有的库，对应用程序来说，这个镜像过于臃肿，其中大部分组件并没有被应用程序直接调用。如果你需要重新创建、备份或扩展这个应用程序，就需要复制整个环境（虚拟机镜像），在新环境中启动应用通常需要几十秒甚至几分钟时间。如果你想单独升级应用中的某个组件，比如说 Vue 应用，就需要重建整个虚拟机镜像。另外，如果你的两个应用依赖同一个底层镜像，升级底层镜像会同时影响这两个应用，而有时候，你只需要升级其中一个应用的依赖而已。这就是所谓的 “依赖陷阱”。\n解决这个问题的办法就是容器。容器是继虚拟机之后更高层次的抽象，在这层抽象中，整个应用程序的每个组件被单独打包成一个个独立的单元，这个单元就是所谓的容器。通过这种方式，可以将代码和应用服务从底层架构中分离出来，实现了完全的可移植性（在任何操作系统或环境上运行应用的能力）。所以在上面的例子中，Ubuntu 操作系统就是一个单元（容器）。MySQL 数据库是另一个容器，Vue 环境和随之而来的库也是一个容器。\n但是，MySQL 数据库是如何自己 “运行” 的？数据库本身肯定也要在操作系统上运行吧？没错！\n更高层次的容器，比如 MySQL 容器，实际上会包含必要的库来与底层的操作系统容器通信和集成。所以你可以把容器看成是整个应用堆栈中的一层，每层都依赖于下层的单元。而这就类似于船舶或港口中集装箱的堆叠方式，每个容器的稳定性都依赖于下面的容器的支持。所以应用容器的核心是一个受控的执行环境。它们允许你从头开始定义整个环境，从操作系统开始，到你要使用的各个版本的库，再到你要添加的代码版本。\n与容器相关的一个重要概念是微服务。将应用程序的各个组件拆分并打包成独立的服务，这样每个组件都可以很容易地被替换、升级、调试。上面的例子中，我们会为 Vue 前端创建一个微服务，为 MySQL 数据库创建另一个微服务，为 Java 中间件部分创建另一个微服务，以此类推。很明显，微服务与容器化是相辅相成的。\n从 Docker 开始 现在你已经对容器有一定了解了吧？Docker 是最常用的容器化工具，也是最流行的容器运行时。\nDocker 开源于 2013 年。用于打包和创建容器，管理基于容器的应用。所有 Linux 发行版、Windows 和 macOS 都支持 Docker。\n还有其他的容器化工具，如 CoreOS rkt、Mesos Containerizer 和 LXC。但是目前，绝大多数的容器化应用都是在 Docker 上运行的。\n再到 Kubernetes 首先，简单介绍一下历史。Kubernetes 是 Google 基于其内部容器调度平台 Borg 的经验开发的。2014 年开源，并作为 CNCF（云原生计算基金会）的核心发起项目。\n那么 Kubernetes 又跟容器是什么关系呢？让我们再回到上面的例子。假设我们的应用爆火，每天的注册用户越来越多。\n现在，我们需要增加后端资源，使浏览我们网站的用户在浏览页面时加载时间不会过长或者超时。最简单的方式就是增加容器的数量，然后使用负载均衡器将传入的负载（以用户请求的形式）分配给容器。\n这样做虽然行之有效，但也只能在用户规模有限的情况下使用。当用户请求达到几十万或几百万时，这种方法也是不可扩展的。你需要管理几十个也许是几百个负载均衡器，这本身就是另一个令人头疼的问题。如果我们想对网站或应用进行任何升级，也会遇到问题，因为负载均衡不会考虑到应用升级的问题。我们需要单独配置每个负载均衡器，然后升级该均衡器所服务的容器。想象一下，当你有 20 个负载均衡器和每周 5 或 6 个小的更新时，你将不得不进行大量的手工劳动。\n我们需要的是一种可以一次性将变更传递给所有受控容器的方法，同时也需要一种可以轻松地调度可用容器的方法，这个过程还必须要是自动化的，这正是 Kubernetes 所做的事情。\n接下来，我们将探讨 Kubernetes 究竟是如何工作的，它的各种组件和服务，以及更多关于如何使用 Kubernetes 来编排、管理和监控容器化环境。为了简单起见，假设我们使用的是 Docker 容器，尽管如前所述，Kubernetes 除了支持 Docker 之外，还支持其他几种容器平台。\nKubernetes 架构和组件 首先，最重要的是你需要认识到 Kubernetes 利用了 “期望状态” 原则。就是说，你定义了组件的期望状态，而 Kubernetes 要将它们始终调整到这个状态。\n例如，你想让你的 Web 服务器始终运行在 4 个容器中，以达到负载均衡的目的，你的数据库复制到 3 个不同的容器中，以达到冗余的目的。这就是你想要的状态。如果这 7 个容器中的任何一个出现故障，Kubernetes 引擎会检测到这一点，并自动创建出一个新的容器，以确保维持所需的状态。\n现在我们来定义一些 Kubernetes 的重要组件。\n当你第一次设置 Kubernetes 时，你会创建一个集群。所有其他组件都是集群的一部分。你也可以创建多个虚拟集群，称为命名空间 (namespace)，它们是同一个物理集群的一部分。这与你可以在同一物理服务器上创建多个虚拟机的方式非常相似。如果你不需要，也没有明确定义的命名空间，那么你的集群将在始终存在的默认命名空间中创建。\nKubernetes 运行在节点 (node) 上，节点是集群中的单个机器。如果你有自己的硬件，节点可能对应于物理机器，但更可能对应于在云中运行的虚拟机。节点是部署你的应用或服务的地方，是 Kubernetes 工作的地方。有 2 种类型的节点 ——master 节点和 worker 节点，所以说 Kubernetes 是主从结构的。\n主节点是一个控制其他所有节点的特殊节点。一方面，它和集群中的任何其他节点一样，这意味着它只是另一台机器或虚拟机。另一方面，它运行着控制集群其他部分的软件。它向集群中的所有其他节点发送消息，将工作分配给它们，工作节点向主节点上的 API Server 汇报。\nMaster 节点本身也包含一个名为 API Server 的组件。这个 API 是节点与控制平面通信的唯一端点。API Server 至关重要，因为这是 worker 节点和 master 节点就 pod、deployment 和所有其他 Kubernetes API 对象的状态进行通信的点。\nWorker 节点是 Kubernetes 中真正干活的节点。当你在应用中部署容器或 pod（稍后定义）时，其实是在将它们部署到 worker 节点上运行。Worker 节点托管和运行一个或多个容器的资源。\nKubernetes 中的逻辑而非物理的工作单位称为 pod。一个 pod 类似于 Docker 中的容器。记得我们在前面讲到，容器可以让你创建独立、隔离的工作单元，可以独立运行。但是要创建复杂的应用程序，比如 Web 服务器，你经常需要结合多个容器，然后在一个 pod 中一起运行和管理。这就是 pod 的设计目的 —— 一个 pod 允许你把多个容器，并指定它们如何组合在一起来创建应用程序。而这也进一步明确了 Docker 和 Kubernetes 之间的关系 —— 一个 Kubernetes pod 通常包含一个或多个 Docker 容器，所有的容器都作为一个单元来管理。\nKubernetes 中的 service 是一组逻辑上的 pod。把一个 service 看成是一个 pod 的逻辑分组，它提供了一个单一的 IP 地址和 DNS 名称，你可以通过它访问服务内的所有 pod。有了服务，就可以非常容易地设置和管理负载均衡，当你需要扩展 Kubernetes pod 时，这对你有很大的帮助，我们很快就会看到。\nReplicationController 或 ReplicaSet 是 Kubernetes 的另一个关键功能。它是负责实际管理 pod 生命周期的组件 —— 当收到指令时或 pod 离线或意外停止时启动 pod，也会在收到指示时杀死 pod，也许是因为用户负载减少。所以换句话说，ReplicationController 有助于实现我们所期望的指定运行的 pod 数量的状态。\n什么是 Kubectl？ kubectl 是一个命令行工具，用于与 Kubernetes 集群和其中的 pod 通信。使用它你可以查看集群的状态，列出集群中的所有 pod，进入 pod 中执行命令等。你还可以使用 YAML 文件定义资源对象，然后使用 kubectl 将其应用到集群中。\nKubernetes 中的自动扩展 请记住，我们使用 Kubernetes 而不是直接使用 Docker 的原因之一，是因为 Kubernetes 能够自动扩展应用实例的数量以满足工作负载的需求。\n自动缩放是通过集群设置来实现的，当服务需求增加时，增加节点数量，当需求减少时，则减少节点数量。但也要记住，节点是 “物理” 结构 —— 我们把 “物理” 放在引号里，因为要记住，很多时候，它们实际上是虚拟机。\n无论如何，节点是物理机器的事实意味着我们的云平台必须允许 Kubernetes 引擎创建新机器。各种云提供商对 Kubernetes 支持基本都满足这一点。\n我们再继续说一些概念，这次是和网络有关的。\n什么是 kubernetes Ingress 和 Egress？ 外部用户或应用程序与 Kubernetes pod 交互，就像 pod 是一个真正的服务器一样。我们需要设置安全规则允许哪些流量可以进入和离开 “服务器”，就像我们为托管应用程序的服务器定义安全规则一样。\n进入 Kubernetes pod 的流量称为 Ingress，而从 pod 到集群外的出站流量称为 egress。我们创建入口策略和出口策略的目的是限制不需要的流量进入和流出服务。而这些策略也是定义 pod 使用的端口来接受传入和传输传出数据 / 流量的地方。\n什么是 Ingress Controller？ 但是在定义入口和出口策略之前，你必须首先启动被称为 Ingress Controller（入口控制器）的组件；这个在集群中默认不启动。有不同类型的入口控制器，Kubernetes 项目默认只支持 Google Cloud 和开箱即用的 Nginx 入口控制器。通常云供应商都会提供自己的入口控制器。\n什么是 Replica 和 ReplicaSet？ 为了保证应用程序的弹性，需要在不同节点上创建多个 pod 的副本。这些被称为 Replica。假设你所需的状态策略是 “ …","date":1651532400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"928dc62a501fa4b747c18cc5f3e0a886","permalink":"https://lib.jimmysong.io/cloud-native-handbook/intro/quick-start/","publishdate":"2022-05-03T00:00:00+01:00","relpermalink":"/cloud-native-handbook/intro/quick-start/","section":"cloud-native-handbook","summary":"Kubernetes 一词来自希腊语，意思是 “飞行员” 或 “舵手”。这个名字很贴切，Kubernetes 可以帮助你在波涛汹涌的容器海洋中航行。 Kubernetes 是做什么的？什么是 Docker？什么是容器编排？Kubernetes 是如何工作","tags":["云原生"],"title":"云原生快速入门","type":"book"},{"authors":null,"categories":["云原生"],"content":"你可能参加过各种云原生、服务网格相关的 meetup，在社区里看到很多人在分享和讨论 Istio，但是对于自己是否真的需要 Istio 感到踌躇，甚至因为它的复杂性而对服务网格的前景感到怀疑。那么，在你继阅读 Istio SIG 后续文章之前，请先仔细阅读本文，审视一下自己公司的现状，看看你是否有必要使用服务网格，处于 Istio 应用的哪个阶段。\n本文不是对应用服务网格的指导，而是根据社区里经常遇到的问题而整理。在使用 Istio 之前，请先考虑下以下因素：\n 你的团队里有多少人？ 你的团队是否有使用 Kubernetes、Istio 的经验？ 你有多少微服务？ 这些微服务使用什么语言？ 你的运维、SRE 团队是否可以支持服务网格管理？ 你有采用开源项目的经验吗？ 你的服务都运行在哪些平台上？ 你的应用已经容器化并使用 Kubernetes 管理了吗？ 你的服务有多少是部署在虚拟机、有多少是部署到 Kubernetes 集群上，比例如何？ 你的团队有制定转移到云原生架构的计划吗？ 你想使用 Istio 的什么功能？ Istio 的稳定性是否能够满足你的需求？ 你是否可以忍受 Istio 带来的性能损耗？  Istio 作为目前最流行的 Service Mesh 技术之一，拥有活跃的社区和众多的落地案例。但如果你真的想在你的生产环境大规模落地 Isito，这看似壮观美好的冰山下，却是暗流涌动，潜藏着无数凶险。\n使用 Istio 无法做到完全对应用透明 服务通信和治理相关的功能迁移到 Sidecar 进程中后， 应用中的 SDK 通常需要作出一些对应的改变。\n比如 SDK 需要关闭一些功能，例如重试。一个典型的场景是，SDK 重试 m 次，Sidecar 重试 n 次，这会导致 m * n 的重试风暴，从而引发风险。\n此外，诸如 trace header 的透传，也需要 SDK 进行升级改造。如果你的 SDK 中还有其它特殊逻辑和功能，这些可能都需要小心处理才能和 Isito Sidecar 完美配合。\nIstio 对非 Kubernetes 环境的支持有限 在业务迁移至 Istio 的同时，可能并没有同步迁移至 Kubernetes，而还运行在原有 PAAS 系统之上。 这会带来一系列挑战：\n 原有 PAAS 可能没有容器网络，Istio 的服务发现和流量劫持都可能要根据旧有基础设施进行适配才能正常工作 如果旧有的 PAAS 单个实例不能很好的管理多个容器（类比 Kubernetes 的 Pod 和 Container 概念），大量 Istio Sidecar 的部署和运维将是一个很大的挑战 缺少 Kubernetes webhook 机制，Sidecar 的注入也可能变得不那么透明，而需要耦合在业务的部署逻辑中  只有 HTTP 协议是一等公民 Istio 原生对 HTTP 协议提供了完善的全功能支持，但在真实的业务场景中，私有化协议却非常普遍，而 Istio 却并未提供原生支持。\n这导致使用私有协议的一些服务可能只能被迫使用 TCP 协议来进行基本的请求路由，这会导致很多功能的缺失，这其中包括 Istio 非常强大的基于内容的消息路由，如基于 header、 path 等进行权重路由。\n扩展 Istio 的成本并不低 虽然 Istio 的总体架构是基于高度可扩展而设计，但由于整个 Istio 系统较为复杂，如果你对 Istio 进行过真实的扩展，就会发现成本不低。\n以扩展 Istio 支持某一种私有协议为例，首先你需要在 Istio 的 api 代码库中进行协议扩展，其次你需要修改 Istio 代码库来实现新的协议处理和下发，然后你还需要修改 xds 代码库的协议，最后你还要在 Envoy 中实现相应的 Filter 来完成协议的解析和路由等功能。\n在这个过程中，你还可能面临上述数个复杂代码库的编译等工程挑战（如果你的研发环境不能很好的使用 Docker 或者无法访问部分国外网络的情况下）。\n即使做完了所有的这些工作，你也可能面临这些工作无法合并回社区的情况，社区对私有协议的扩展支持度不高，这会导致你的代码和社区割裂，为后续的升级更新带来隐患。\nIstio 在集群规模较大时的性能问题 Istio 默认的工作模式下，每个 Sidecar 都会收到全集群所有服务的信息。如果你部署过 Istio 官方的 Bookinfo 示例应用，并使用 Envoy 的 config dump 接口进行观察，你会发现，仅仅几个服务，Envoy 所收到的配置信息就有将近 20w 行。\n可以想象，在稍大一些的集群规模，Envoy 的内存开销、Istio 的 CPU 开销、XDS 的下发时效性等问题，一定会变得尤为突出。\nIstio 这么做一是考虑这样可以开箱即用，用户不用进行过多的配置，另外在一些场景，可能也无法梳理出准确的服务之间的调用关系，因此直接给每个 Sidecar 下发了全量的服务配置，即使这个 Sidecar 只会访问其中很小一部分服务。\n当然这个问题也有解法，你可以通过 Sidecar CRD 来显示定义服务调用关系，使 Envoy 只得到他需要的服务信息，从而大幅降低 Envoy 的资源开销，但前提是在你的业务线中能梳理出这些调用关系。\nXDS 分发没有分级发布机制 当你对一个服务的策略配置进行变更的时候，XDS 不具备分级发布的能力，所有访问这个服务的 Envoy 都会立即收到变更后的最新配置。这在一些对变更敏感的严苛生产环境，可能是有很高风险甚至不被允许的。\n如果你的生产环境严格要求任何变更都必须有分级发布流程，那你可能需要考虑自己实现一套这样的机制。\nIstio 组件故障时是否有退路？ 以 Istio 为代表的 Sidecar 架构的特殊性在于，Sidecar 直接承接了业务流量，而不像一些其他的基础设施那样，只是整个系统的旁路组件（比如 Kubernetes）。\n因此在 Isito 落地初期，你必须考虑，如果 Sidecar 进程挂掉，服务怎么办？是否有退路？是否能 fallback 到直连模式？\n在 Istio 落地过程中，是否能无损 fallback，通常决定了核心业务能否接入 Service Mesh。\nIsito 技术架构的成熟度还没有达到预期 虽然 Istio 1.0 版本已经发布了很久，但是如果你关注社区每个版本的迭代，就会发现，Istio 目前架构依然处于不太稳定的状态，尤其是 1.5 版本前后的几个大版本，先后经历了去除 Mixer 组件、合并为单体架构、仅支持高版本 Kubernetes 等等重大变动，这对于已经在生产环境中使用了 Istio 的用户非常不友好，因为升级会面临各种不兼容性问题。\n好在社区也已经意识到这一问题，2021 年社区也成立了专门的小组，重点改善 Istio 的兼容性和用户体验。\nIstio 缺乏成熟的产品生态 Istio 作为一套技术方案，却并不是一套产品方案。\n如果你在生产环境中使用，你可能还需要解决可视化界面、权限和账号系统对接、结合公司已有技术组件和产品生态等问题，仅仅通过命令行来使用，可能并不能满足你的组织对权限、审计、易用性的要求。\n而 Isito 自带的 Kiali 功能还十分简陋，远远没有达到能在生产环境使用的程度，因此你可能需要研发基于 Isito 的上层产品。\nIstio 目前解决的问题域还很有限 Istio 目前主要解决的是分布式系统之间服务调用的问题，但还有一些分布式系统的复杂语义和功能并未纳入到 Istio 的 Sidecar 运行时之中，比如消息发布和订阅、状态管理、资源绑定等等。\n云原生应用将会朝着多 Sidecar 运行时或将更多分布式能力纳入单 Sidecar 运行时的方向继续发展，以使服务本身变得更为轻量，让应用和基础架构彻底解耦。\n如果你的生产环境中，业务系统对接了非常多和复杂的分布式系系统中间件，Istio 目前可能并不能完全解决你的应用的云原生化诉求。\n写在最后 看到这里，你是否感到有些沮丧，而对 Isito 失去信心？\n别担心，上面列举的这些问题，实际上并不影响 Isito 依然是目前最为流行和成功的 Service Mesh 技术选型之一。Istio 频繁的变动，一定程度上也说明它拥有一个活跃的社区，我们应当对一个新的事物报以信心，Isito 的社区也在不断听取来自终端用户的声音，朝着大家期待的方向演进。\n同时，如果你的生产环境中的服务规模并不是很大，服务已经托管于 Kubernetes 之上，也只使用那些 Istio 原生提供的能力，那么 Istio 依然是一个值得尝试的开箱即用方案。\n但如果你的生产环境比较复杂，技术债务较重，专有功能和策略需求较多，亦或者服务规模庞大，那么在开始使用 Istio 之前，你需要仔细权衡上述这些要素，以评估在你的系统之中引入 Istio 可能带来的复杂度和潜在成本。\n参考  在生产环境使用 Istio 前的若干考虑要素 - cloudnative.to  ","date":1651507200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"f2278b4726a326bb988c2256c79108bc","permalink":"https://lib.jimmysong.io/cloud-native-handbook/service-mesh/do-you-need-a-service-mesh/","publishdate":"2022-05-03T00:00:00+08:00","relpermalink":"/cloud-native-handbook/service-mesh/do-you-need-a-service-mesh/","section":"cloud-native-handbook","summary":"你可能参加过各种云原生、服务网格相关的 meetup，在社区里看到很多人在分享和讨论 Istio，但是对于自己是否真的需要 Istio 感到踌躇，甚至因为它的复杂性而对服务网格的前景感到怀疑。那么，在你继阅读 Istio SIG 后续","tags":["云原生"],"title":"你是否需要 Istio？","type":"book"},{"authors":null,"categories":["可观测性"],"content":"眯着眼睛看图表并不是寻找相关性的最佳方式。目前在运维人员头脑中进行的大量工作实际上是可以自动化的。这使运维人员可以在识别问题、提出假设和验证根本原因之间迅速行动。\n为了建立更好的工具，我们需要更好的数据。遥测必须具备以下两个要求以支持高质量的自动分析：\n 所有的数据点都必须用适当的索引连接在一个图上。 所有代表常见操作的数据点必须有明确的键和值。  在这一章中，我们将从一个基本的构件开始浏览现代遥测数据模型：属性。\n属性：定义键和值 最基本的数据结构是属性（attribute），定义为一个键和一个值。OpenTelemetry 的每个数据结构都包含一个属性列表。分布式系统的每个组件（HTTP 请求、SQL 客户端、无服务器函数、Kubernetes Pod）在 OpenTelemetry 规范中都被定义为一组特定的属性。这些定义被称为 OpenTelemetry 语义约定。表 2-1 显示了 HTTP 约定的部分列表。\n   属性 类型 描述 示例     http.method string HTTP 请求类型 GET; POST; HEAD   http.target string 在 HTTP 请求行中传递的完整的请求目标或等价物 /path/12314/   http.host string HTTP host header 的值。当标头为空或不存在时，这个属性应该是相同的。 www.example.org   http.scheme string 识别所使用协议的 URI 方案 http; https   http.status_code int HTTP 请求状态码 200    表 2-1：HTTP 规范的部分列表\n有了这样一个标准模式，分析工具就可以对它们所监测的系统进行详细的表述，同时进行细微的分析，而在使用定义不明确或不一致的数据时，是不可能做到的。\n事件：一切的基础 OpenTelemetry 中最基本的对象是事件（event）。事件只是一个时间戳和一组属性。使用一组属性而不是简单的消息 / 报文，可以使分析工具正确地索引事件，并使它们可以被搜索到。\n有些属性对事件来说是独一无二的。时间戳、消息和异常细节都是特定事件的属性的例子。\n然而，大多数属性对单个事件来说并不独特。相反，它们是一组事件所共有的。例如，http.target 属性与作为 HTTP 请求的一部分而记录的每个事件有关。如果在每个事件上反复记录这些属性，效率会很低。相反，我们把这些属性拉出到围绕事件的封装中，在那里它们可以被写入一次。我们把这些封装称为上下文（context）。\n有两种类型的上下文：静态和动态（如图 2-1 所示）。静态上下文定义了一个事件发生的物理位置。在 OpenTelemetry 中，这些静态属性被称为资源。一旦程序启动，这些资源属性的值通常不会改变。\n   图 2-1：虽然事件有一些特定的事件属性，但大多数属性属于事件发生的上下文之一。  动态上下文定义了事件所参与的活动操作。这个操作层面的上下文被称为跨度（span）。每次操作执行时，这些属性的值都会改变。\n不是所有的事件都有两种类型的上下文。只有资源的自由浮动事件，如程序启动时发出的事件，被称为日志（log）。作为分布式事务的一部分而发生的事件被称为跨度事件（span event）。\n资源：观察服务和机器 资源（静态上下文）描述了一个程序正在消费的物理和虚拟信息结构。服务、容器、部署和区域都是资源。图 2-2 显示了一个典型的购物车结账事务中所涉及的资源。\n   图 2-2：一个事务，被视为一组资源。  系统运行中的大多数问题都源于资源争夺，许多并发的事务试图在同一时间利用相同的资源。通过将事件放在它们所使用的资源的上下文中，就有可能自动检测出许多类型的资源争夺。\n像事件一样，资源可以被定义为一组属性。表 2-2 显示了一个服务资源的例子。\n表 2-2：服务资源的例子\n   属性 类型 描述 示例     service.name string 服务的逻辑名称 shopping cart   service.instance.id string 服务实例的 ID 627cc493- f310-47de-96bd-71410b7dec09   service.version string 服务 API 或者实现的版本号 2.0.0    除了识别机器所需的基本信息，配置设置也可以作为资源被记录下来。要访问一台正在运行的机器来了解它是如何配置的，这个负担太让人害怕了。相反，在配置文件中发现的任何重要信息也应该表示为一种资源。\n跨度：观察事务 跨度（动态上下文）描述计算机操作。跨度有一个操作名称，一个开始时间，一个持续时间，以及一组属性。\n标准操作是使用语义约定来描述的，比如上面描述的 HTTP 约定。但也有一些特定的应用属性，如 ProjectID 和 AccountID，可以由应用开发者添加。\n跨度也是我们描述因果关系的方式。为了正确记录整个事务，我们需要知道哪些操作是由其他哪些操作触发的。为了做到这一点，我们需要给跨度增加三个属性：TraceID、SpanID 和 ParentID，如表 2-3 所示。\n表 2-3：跨度的三个额外属性\n   属性 类型 描述 示例     traceid 16 字节数组 识别整个事务 4bf92f3577b34da6a3ce929d0e0e4736   spanid 8 字节数组 识别当前操作 00f067aa0ba902b7   parentid 8 字节数组 识别父操作 53ce929d0e0e4736    这三个属性是 OpenTelemetry 的基础。通过添加这些属性，我们所有的事件现在可以被组织成一个图，代表它们的因果关系。这个图现在可以以各种方式进行索引，我们稍后会讨论这个问题。\n追踪：看似日志，胜过日志 我们现在已经从简单的事件变成了组织成与资源相关的操作图的事件。这种类型的图被称为追踪（trace）。图 2-3 显示了一种常见的可视化追踪方式，重点是识别操作的延迟。\n   图 2-3：当日志被组织成一个图形时，它们就变成了追踪。  从本质上讲，追踪只是用更好的索引来记录日志。当你把适当的上下文添加到适当的结构化的日志中时，可以得到追踪的定义。\n想想你花了多少时间和精力通过搜索和过滤来收集这些日志；那是收集数据的时间，而不是分析数据的时间。而且，你要翻阅的日志越多，执行并发事务数量不断增加的机器堆积，就越难收集到真正相关的那一小部分日志。\n然而，如果你有一个 TraceID，收集这些日志只是一个简单的查询。通过 TraceID 索引，你的存储工具可以自动为你做这项工作；找到一个日志，你就有了该事务中的所有日志，不需要额外的工作。\n既然如此，为什么你还会要那些没有 “追踪\u0026#34;ID 的 “日志”？我们已经习惯了传统的日志管理迫使我们做大量的工作来连接这些点。但这些工作实际上是不必要的；它是我们数据中缺乏结构的副产品。\n分布式追踪不仅仅是一个测量延迟的工具；它是一个定义上下文和因果关系的数据结构。它是把所有东西联系在一起的胶水。正如我们将看到的，这种胶水包括最后一个支柱 —— 指标。\n指标：观察事件的总体情况 现在我们已经确定了什么是事件，让我们来谈谈事件的聚合。在一个活跃的系统中，同样的事件会不断发生，我们以聚合的方式查看它们的属性来寻找模式。属性的值可能出现得太频繁，或者不够频繁，在这种情况下，我们要计算这些值出现的频率。或者该值可能超过某个阈值，在这种情况下，我们想衡量该值是如何随时间变化的。或者我们可能想以直方图的形式来观察数值的分布。\n这些聚合事件被称为度量。就像普通的事件一样，度量有一组属性和一组语义上的便利条件来描述普通概念。表 2-4 显示了一些系统内存的例子属性。\n表 2-4：系统内存的属性\n   属性 值类型 属性值     system.memory.usage int64 used, free, cached, other   system.memory.utilization double used, free, cached, other    与事件相关的指标：统一的系统 传统上，我们认为指标是与日志完全分开的。但实际上它们是紧密相连的。例如，假设一个 API 有一个衡量每分钟错误数量的指标。那是一个统计数字。然而，每一个错误都是由一个特定的事务行为产生的，使用特定的资源。这些细节在我们每次递增该计数器时都会出现，我们想知道这些细节。\n当运维人员被提醒发现错误突然激增时，他们会想到的第一个问题自然是：“是什么导致了这个激增？” 看一下例子的追踪可以回答这个问题。在失败的事务中较早发生的事件（或未能发生的事件）可能是错误的来源。\n在 OpenTelemetry 中，当指标事件在跨度的范围内发生时，这些追踪的样本会自动与指标相关联，作为追踪的范例（trace examplar）。这意味着不需要猜测或寻找日志。OpenTelemetry 明确地将追踪和度量联系在一起。一个建立在 OpenTelemetry 上的分析工具可以让你从仪表盘上直接看到追踪，只需一次点击。如果有一个模式 —— 例如，一个特定的属性值与导致一个特定错误的追踪密切相关 —— 这个模式可以被自动识别。\n自动分析和编织 事件、资源、跨度、指标和追踪：这些都被 OpenTelemetry 连接在一个图中，并且它们都被发送到同一个数据库，作为一个整体进行分析。这就是下一代的可观测性工具。\n现代可观测性将建立在使用结构的数据上，这些结构允许分析工具在所有类型的事件和总量之间进行关联，这些关联将对我们如何实践可观测性产生深远影响。\n向全面观察我们的系统过渡将有许多好处。但我相信，这些新工具提供的主要省时功能将是各种形式的自动关联检测。在寻找根本原因时，注意到相关关系可以产生大量的洞察力。如图 2-4 所示，相关性往往是产生根本原因假设的关键因素，然后可以进一步调查。\n   图 2-4：可能提供关键洞察力、指向根本原因的关联实例。  平均表现是什么样子的？异常值是什么样子的？哪些趋势在一起变化，它们的共同点是什么？相关性可能发生在许多地方：跨度中的属性之间、追踪中的跨度之间、追踪和资源之间、指标内部，以及所有这些地方都有。当所有这些数据被连接到一个图中时，这些相关性就可以被发现了。\n这就是为什么统一的数据编织是如此关键。任何自动匹配的分析的价值完全取决于被分析的数据的结构和质量。机器遍历数据的图表；它们不会进行逻辑的飞跃。准确的统计分析需要一个有意设计的遥测系统来支持它。\n重点：自动分析为您节省时间 为什么我们关心相关性分析的自动化？因为时间和复杂性对我们不利。随着系统规模的扩大，它们最终变得太复杂了，任何操作者都无法完全掌握系统的情况，而且在建立一个假设时，永远没有足够的时间来调查每一个可能的联系。\n问题是，选择调查什么需要直觉，而直觉往往需要对组成分布式系统的每个组件有深刻的了解。随着企业系统的增长和工程人员的相应增加，任何一个工程师对每个系统深入了解的部分自然会缩减到整个系统的一小部分。直觉并不能很好地扩展。\n直觉也极易被误导；问题经常出现在意想不到的地方。根据定义，可以预见的问题几乎不会经常发生。剩下的就是所有未曾预料到的问题了，这些问题已经超出了我们的直觉。\n这就是自动关联检测的作用。有了正确的数据，机器可以更有效地检测出相关的关联。这使得运维人员能够快速行动，反复测试各种假设，直到他们知道足够的信息来制定解决方案。\n","date":1643990400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"3ebe29250964919d7448963fb74df17e","permalink":"https://lib.jimmysong.io/opentelemetry-obervability/the-value-of-structured-data/","publishdate":"2022-02-05T00:00:00+08:00","relpermalink":"/opentelemetry-obervability/the-value-of-structured-data/","section":"opentelemetry-obervability","summary":"第 2 章：结构化数据的价值","tags":["OpenTelemetry"],"title":"第 2 章：结构化数据的价值","type":"book"},{"authors":null,"categories":["云原生"],"content":"IT 行业正在向微服务架构和云原生解决方案发展。由于使用不同的技术开发了成百上千的微服务，这些系统可能变得复杂，难以调试。\n作为一个应用开发者，你考虑的是业务逻辑——购买产品或生成发票。然而，任何像这样的业务逻辑都会导致不同服务之间的多个服务调用。每个服务可能都有它的超时、重试逻辑和其他可能需要调整或微调的网络特定代码。\n如果在任何时候最初的请求失败了，就很难通过多个服务来追踪，准确地指出失败发生的地方，了解请求为什么失败。是网络不可靠吗？是否需要调整重试或超时？或者是业务逻辑问题或错误？\n服务可能使用不一致的跟踪和记录机制，使这种调试的复杂性增加。这些问题使你很难确定问题发生在哪里，以及如何解决。如果你是一个应用程序开发人员，而调试网络问题不属于你的核心技能，那就更是如此。\n将网络问题从应用程序堆栈中抽离出来，由另一个组件来处理网络部分，让调试网络问题变得更容易。这就是 Envoy 所做的事情。\n在每个服务实例旁边都有一个 Envoy 实例在运行。这种类型的部署也被称为 Sidecar 部署。Envoy 的另一种模式是边缘代理，用于构建 API 网关。\nEnvoy 和应用程序形成一个原子实体，但仍然是独立的进程。应用程序处理业务逻辑，而 Envoy 则处理网络问题。\n在发生故障的情况下，分离关注点可以更容易确定故障是来自应用程序还是网络。\n为了帮助网络调试，Envoy 提供了以下高级功能。\n进程外架构 Envoy 是一个独立的进程，旨在与每个应用程序一起运行 —— 也就是我们前面提到的 Sidecar 部署模式。集中配置的 Envoy 的集合形成了一个透明的服务网格。\n路由和其他网络功能的责任被推给了 Envoy。应用程序向一个虚拟地址（localhost）而不是真实地址（如公共 IP 地址或主机名）发送请求，不知道网络拓扑结构。应用程序不再承担路由的责任，因为该任务被委托给一个外部进程。\n与其让应用程序管理其网络配置，不如在 Envoy 层面上独立于应用程序管理网络配置。在一个组织中，这可以使应用程序开发人员解放出来，专注于应用程序的业务逻辑。\nEnvoy 适用于任何编程语言。你可以用 Go、Java、C++ 或其他任何语言编写你的应用程序，而 Envoy 可以在它们之间架起桥梁。Envoy 的行为是相同的，无论应用程序的编程语言或它们运行的操作系统是什么。\nEnvoy 还可以在整个基础设施中透明地进行部署和升级。这与为每个单独的应用程序部署库升级相比，后者可能是非常痛苦和耗时的。\n进程外架构是有益的，因为它使我们在不同的编程语言 / 应用堆栈中保持一致，我们可以免费获得独立的应用生命周期和所有的 Envoy 网络功能，而不必在每个应用中单独解决这些问题。\nL3/L4 过滤器结构 Envoy 是一个 L3/L4 网络代理，根据 IP 地址和 TCP 或 UDP 端口进行决策。它具有一个可插拔的过滤器链，可以编写你的过滤器来执行不同的 TCP/UDP 任务。\n过滤器链（Filter Chain） 的想法借鉴了 Linux shell，即一个操作的输出被输送到另一个操作中。例如：\nls -l | grep \u0026#34;Envoy*.cc\u0026#34; | wc -l Envoy 可以通过堆叠所需的过滤器来构建逻辑和行为，形成一个过滤器链。许多过滤器已经存在，并支持诸如原始 TCP 代理、UDP 代理、HTTP 代理、TLS 客户端认证等任务。Envoy 也是可扩展的，我们可以编写我们的过滤器。\nL7 过滤器结构 Envoy 支持一个额外的 HTTP L7 过滤器层。我们可以在 HTTP 连接管理子系统中插入 HTTP 过滤器，执行不同的任务，如缓冲、速率限制、路由 / 转发等。\n一流的 HTTP/2 支持 Envoy 同时支持 HTTP/1.1 和 HTTP/2，并且可以作为一个透明的 HTTP/1.1 到 HTTP/2 的双向代理进行操作。这意味着任何 HTTP/1.1 和 HTTP/2 客户端和目标服务器的组合都可以被桥接起来。即使你的传统应用没有通过 HTTP/2 进行通信，如果你把它们部署在 Envoy 代理旁边，它们最终也会通过 HTTP/2 进行通信。\n推荐在所有的服务间配置的 Envoy 使用 HTTP/2，以创建一个持久连接的网格，请求和响应可以在上面复用。\nHTTP 路由 当以 HTTP 模式操作并使用 REST 时，Envoy 支持路由子系统，能够根据路径、权限、内容类型和运行时间值来路由和重定向请求。在将 Envoy 作为构建 API 网关的前台 / 边缘代理时，这一功能非常有用，在构建服务网格（sidecar 部署模式）时，也可以利用这一功能。\ngRPC 准备就绪 Envoy 支持作为 gRPC 请求和响应的路由和负载均衡底层所需的所有 HTTP/2 功能。\n gRPC 是一个开源的远程过程调用（RPC）系统，它使用 HTTP/2 进行传输，并将协议缓冲区作为接口描述语言（IDL），它提供的功能包括认证、双向流和流量控制、阻塞 / 非阻塞绑定，以及取消和超时。\n 服务发现和动态配置 我们可以使用静态配置文件来配置 Envoy，这些文件描述了服务间通信方式。\n对于静态配置 Envoy 不现实的高级场景，Envoy 支持动态配置，在运行时自动重新加载配置。一组名为 xDS 的发现服务可以用来通过网络动态配置 Envoy，并为 Envoy 提供关于主机、集群 HTTP 路由、监听套接字和加密信息。\n健康检查 负载均衡器有一个特点，那就是只将流量路由到健康和可用的上游服务。Envoy 支持健康检查子系统，对上游服务集群进行主动健康检查。然后，Envoy 使用服务发现和健康检查信息的组合来确定健康的负载均衡目标。Envoy 还可以通过异常点检测子系统支持被动健康检查。\n高级负载均衡 Envoy 支持自动重试、断路、全局速率限制（使用外部速率限制服务）、影子请求（或流量镜像）、异常点检测和请求对冲。\n前端 / 边缘代理支持 Envoy 的特点使其非常适合作为边缘代理运行。这些功能包括 TLS 终端、HTTP/1.1、HTTP/2 和 HTTP/3 支持，以及 HTTP L7 路由。\nTLS 终止 应用程序和代理的解耦使网格部署模型中所有服务之间的 TLS 终止（双向 TLS）成为可能。\n一流的可观测性 为了便于观察，Envoy 会生成日志、指标和追踪。Envoy 目前支持 statsd（和兼容的提供者）作为所有子系统的统计。得益于可扩展性，我们也可以在需要时插入不同的统计提供商。\nHTTP/3（Alpha） Envoy 1.19.0 支持 HTTP/3 的上行和下行，并在 HTTP/1.1、HTTP/2 和 HTTP/3 之间进行双向转义。\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"5a9510f318e2f53c719e503c81c6dc95","permalink":"https://lib.jimmysong.io/cloud-native-handbook/service-mesh/what-is-envoy/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/cloud-native-handbook/service-mesh/what-is-envoy/","section":"cloud-native-handbook","summary":"IT 行业正在向微服务架构和云原生解决方案发展。由于使用不同的技术开发了成百上千的微服务，这些系统可能变得复杂，难以调试。 作为一个应用开发者，你考虑的是业务逻辑——购买产品或生成发票。然而，任何像这样的业","tags":["云原生"],"title":"什么是 Envoy？","type":"book"},{"authors":null,"categories":["文化"],"content":"总结 现在您已经知道了 Code Review 要点，那么管理分布在多个文件中的评论的最有效方法是什么？\n 变更是否有意义？它有很好的描述吗？ 首先看一下变更中最重要的部分。整体设计得好吗？ 以适当的顺序查看 CL 的其余部分。  第一步：全面了解变更 查看 CL 描述和 CL 大致上用来做什么事情。这种变更是否有意义？如果在最初不应该发生这样的变更，请立即回复，说明为什么不应该进行变更。当您拒绝这样的变更时，向开发人员建议应该做什么也是一个好主意。\n例如，您可能会说“看起来你已经完成一些不错的工作，谢谢！但实际上，我们正朝着删除您在这里修改的 FooWidget 系统的方向演进，所以我们不想对它进行任何新的修改。不过，您来重构下新的 BarWidget 类怎么样？“\n请注意，审查者不仅拒绝了当前的 CL 并提供了替代建议，而且他们保持礼貌地这样做。这种礼貌很重要，因为我们希望表明，即使不同意，我们也会相互尊重。\n如果您获得了多个您不想变更的 CL，您应该考虑重整开发团队的开发过程或外部贡献者的发布过程，以便在编写CL之前有更多的沟通。最好在他们完成大量工作之前说“不”，避免已经投入心血的工作现在必须被抛弃或彻底重写。\n第二步：检查 CL 的主要部分 查找作为此 CL “主要”部分的文件。通常，包含大量的逻辑变更的文件就是 CL 的主要部分。先看看这些主要部分。这有助于为 CL 的所有较小部分提供上下文，并且通常可以加速代码审查。如果 CL 太大而无法确定哪些部分是主要部分，请向开发人员询问您应该首先查看的内容，或者要求他们将 CL 拆分为多个 CL。\n如果在该部分发现存在一些主要的设计问题时，即使没有时间立即查看 CL 的其余部分，也应立即留下评论告知此问题。因为事实上，因为该设计问题足够严重的话，继续审查其余部分很可能只是浪费宝贵的时间，因为其他正在审查的程序可能都将无关或消失。\n立即发送这些主要设计评论非常重要，有两个主要原因：\n 通常开发者在发出 CL 后，在等待审查时立即开始基于该 CL 的新工作。如果您正在审查的 CL 中存在重大设计问题，那么他们以后的 CL 也必须要返工。您应该赶在他们在有问题的设计上做了太多无用功之前通知他们。 主要的设计变更比起小的变更来说需要更长的时间才能完成。开发人员基本都有截止日期；为了完成这些截止日期并且在代码库中仍然保有高质量代码，开发人员需要尽快开始 CL 的任何重大工作。  第三步：以适当的顺序查看 CL 的其余部分 一旦您确认整个 CL 没有重大的设计问题，试着找出一个逻辑顺序来查看文件，同时确保您不会错过查看任何文件。 通常在查看主要文件之后，最简单的方法是按照代码审查工具向您提供的顺序浏览每个文件。有时在阅读主代码之前先阅读测试也很有帮助，因为这样您就可以了解该变更应当做些什么。\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"32281be908130af8de14e98ca384a3f9","permalink":"https://lib.jimmysong.io/eng-practices/review/reviewer/navigate/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/eng-practices/review/reviewer/navigate/","section":"eng-practices","summary":"总结 现在您已经知道了 Code Review 要点，那么管理分布在多个文件中的评论的最有效方法是什么？ 变更是否有意义？它有很好的描述吗？ 首先看一下变更中最重要的部分。整体设计得好吗？ 以适当的顺序查看 CL 的其余部分。 第一步：全","tags":null,"title":"查看 CL 的步骤","type":"book"},{"authors":null,"categories":["文化"],"content":"当您发送 CL 进行审查时，您的审查者可能会对您的 CL 发表一些评论。以下是处理审查者评论的一些有用信息。\n不是针对您 审查的目标是保持代码库和产品的质量。当审查者对您的代码提出批评时，请将其视为在帮助您、代码库和 Google，而不是对您或您的能力的个人攻击。\n有时，审查者会感到沮丧并在评论中表达他们的挫折感。对于审查者来说，这不是一个好习惯，但作为开发人员，您应该为此做好准备。问问自己，“审查者试图与我沟通的建设性意见是什么？”然后像他们实际说的那样操作。\n永远不要愤怒地回应代码审查评论。这严重违反了专业礼仪且将永远存在于代码审查工具中。如果您太生气或恼火而无法好好的回应，那么请离开电脑一段时间，或者做一些别的事情，直到您感到平静，可以礼貌地回答。\n一般来说，如果审查者没有以建设性和礼貌的方式提供反馈，请亲自向他们解释。如果您无法亲自或通过视频通话与他们交谈，请向他们发送私人电子邮件。以友善的方式向他们解释您不喜欢的东西以及您希望他们以怎样不同的方式来做些什么。如果他们也以非建设性的方式回复此私人讨论，或者没有预期的效果，那么请酌情上报给您的经理。\n修复代码 如果审查者说他们不了解您的代码中的某些内容，那么您的第一反应应该是澄清代码本身。 如果无法澄清代码，请添加代码注释，以解释代码存在的原因。 只有在想增加的注释看起来毫无意义时，您才能在代码审查工具中进行回复与解释。\n如果审查者不理解您的某些代码，那么代码的未来读者可能也不会理解。在代码审查工具中回复对未来的代码读者没有帮助，但澄清代码或添加代码注释确可以实实在在得帮助他们。\n自我反思 编写 CL 可能需要做很多工作。在终于发送一个 CL 用于审查后，我们通常会感到满足的，认为它已经完成，并且非常确定不需要进一步的工作。这通常是令人满意的。因此，当审查者回复对可以改进的事情的评论时，很容易本能地认为评论是错误的，审查者正在不必要地阻止您，或者他们应该让您提交 CL。但是，无论您目前多么确定，请花一点时间退一步，考虑审查者是否提供有助于对代码库和对 Google 的有价值的反馈。您首先应该想到的应该是，“审查者是否正确？”\n如果您无法回答这个问题，那么审查者可能需要澄清他们的意见。\n如果您已经考虑过并且仍然认为自己是正确的，请随时回答一下为什么您的方法对代码库、用户和/或 Google 更好。通常，审查者实际上是在提供建议，他们希望您自己思考什么是最好的。您可能实际上对审阅者不知道的用户、代码库或 CL 有所了解。所以提供并告诉他们更多的上下文。通常，您可以根据技术事实在自己和审查者之间达成一些共识。\n解决冲突 解决冲突的第一步应该是尝试与审查者达成共识。 如果您无法达成共识，请参阅“代码审查标准”，该标准提供了在这种情况下遵循的原则。\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"54414c32ea995ca6c230c4577001cada","permalink":"https://lib.jimmysong.io/eng-practices/review/developer/handling-comments/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/eng-practices/review/developer/handling-comments/","section":"eng-practices","summary":"当您发送 CL 进行审查时，您的审查者可能会对您的 CL 发表一些评论。以下是处理审查者评论的一些有用信息。 不是针对您 审查的目标是保持代码库和产品的质量。当审查者对您的代码提出批评时，请将其视为在帮助您、代码库和","tags":null,"title":"如何处理审查者的评论","type":"book"},{"authors":null,"categories":["网络"],"content":"根据所使用的 Linux 内核版本，eBPF 数据路径可以完全在 eBPF 中实现不同的功能集。如果某些所需功能不可用，则使用旧版 iptables 实现提供该功能。有关详细信息，请参阅 IPsec 要求。\nkube-proxy 互操作性 下图显示了 kube-proxy 安装的 iptables 规则和 Cilium 安装的 iptables 规则的集成。\n   kube-proxy 与 Cilium 的 iptables 规则集成   下一章   ","date":1655438400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"5f1590ef4c4cd3ab780650d2fb40d45d","permalink":"https://lib.jimmysong.io/cilium-handbook/ebpf/iptables/","publishdate":"2022-06-17T12:00:00+08:00","relpermalink":"/cilium-handbook/ebpf/iptables/","section":"cilium-handbook","summary":"根据所使用的 Linux 内核版本，eBPF 数据路径可以完全在 eBPF 中实现不同的功能集。如果某些所需功能不可用，则使用旧版 iptables 实现提供该功能。有关详细信息，请参阅 IPsec 要求。 kube-proxy 互操作性 下图显示了 kube-proxy 安装的 iptables 规则和 Cilium 安装的 iptables 规","tags":["Cilium"],"title":"Iptables 用法","type":"book"},{"authors":null,"categories":["网络"],"content":"默认情况下，Cilium 将 eBPF 数据路径配置为执行 IP 分片跟踪，以允许不支持分片的协议（例如 UDP）通过网络透明地传输大型消息。IP 分片跟踪在 eBPF 中使用 LRU（最近最少使用）映射实现，需要 Linux 4.10 或更高版本。可以使用以下选项配置此功能：\n --enable-ipv4-fragment-tracking：启用或禁用 IPv4 分片跟踪。默认启用。 --bpf-fragments-map-max：控制使用 IP 分片的最大活动并发连接数。对于默认值，请参阅eBPF Maps。  注意\n当使用 kube-proxy 运行 Cilium 时，碎片化的 NodePort 流量可能会由于内核错误而中断，其中路由 MTU 不受转发数据包的影响。纤毛碎片跟踪需要第一个逻辑碎片首先到达。由于内核错误，可能会在外部封装层上发生额外的碎片，从而导致数据包重新排序并导致无法跟踪碎片。\n内核错误已被 修复 并向后移植到所有维护的内核版本。如果您发现连接问题，请确保您的节点上的内核包最近已升级，然后再报告问题。\n提示\n  这是一个测试版功能。如果你遇到任何问题，请提供反馈并提交 GitHub Issue。    下一章   ","date":1655438400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"314374e8e3d300e23fcc10c51b62edf2","permalink":"https://lib.jimmysong.io/cilium-handbook/networking/fragmentation/","publishdate":"2022-06-17T12:00:00+08:00","relpermalink":"/cilium-handbook/networking/fragmentation/","section":"cilium-handbook","summary":"默认情况下，Cilium 将 eBPF 数据路径配置为执行 IP 分片跟踪，以允许不支持分片的协议（例如 UDP）通过网络透明地传输大型消息。IP 分片跟踪在 eBPF 中使用 LRU（最近最少使用）映射实现，需要 Linux 4.10 或更高版本。可以","tags":["Cilium"],"title":"IPv4 分片处理","type":"book"},{"authors":null,"categories":["网络"],"content":"Cilium 能够透明地将四层代理注入任何网络连接中。这被用作执行更高级别网络策略的基础（请参阅基于 DNS 和 七层示例）。\n可以注入以下代理：\n Envoy  注入 Envoy 代理 注意\n  此功能目前正处于测试阶段。   如果你有兴趣编写 Envoy 代理的 Go 扩展，请参考开发者指南。\n   Envoy 代理注入示意图  如上所述，该框架允许开发人员编写少量 Go 代码（绿色框），专注于解析新的 API 协议，并且该 Go 代码能够充分利用 Cilium 功能，包括高性能重定向 Envoy、丰富的七层感知策略语言和访问日志记录，以及通过 kTLS 对加密流量的可视性（即将推出）。总而言之，作为开发者的你只需要关心解析协议的逻辑，Cilium + Envoy + eBPF 就完成了繁重的工作。\n本指南基于假设的 r2d2 协议（参见 proxylib/r2d2/r2d2parser.go）的简单示例，该协议可用于很久以前在遥远的星系中与简单的协议机器人对话。但它也指向了 cilium/proxylib 目录中已经存在的其他真实协议，例如 Memcached 和 Cassandra。\n 下一章   ","date":1655438400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"fe43b78b02b278383648f5f704e384fd","permalink":"https://lib.jimmysong.io/cilium-handbook/security/proxy/","publishdate":"2022-06-17T12:00:00+08:00","relpermalink":"/cilium-handbook/security/proxy/","section":"cilium-handbook","summary":"Cilium 能够透明地将四层代理注入任何网络连接中。这被用作执行更高级别网络策略的基础（请参阅基于 DNS 和 七层示例）。 可以注入以下代理： Envoy 注入 Envoy 代理 注意 此功能目前正处于测试阶段。 如果你有兴趣编写 Envoy 代理的 Go 扩展，请参","tags":["Cilium"],"title":"代理注入","type":"book"},{"authors":null,"categories":["网络"],"content":"策略规则到端点映射 确定哪些策略规则当前对端点有效，并且可以将来自 cilium endpoint list 和 cilium endpoint get 的数据与 cilium policy get 配对。cilium endpoint get 将列出适用于端点的每个规则的标签。可以传递标签列表给 cilium policy get 以显示确切的源策略。请注意，不能单独获取没有标签的规则（无标签会返回节点上的完整策略）。具有相同标签的规则将一起返回。\n在下面的示例中，其中一个 deathstar pod 的端点 id 是 568。我们可以打印应用于它的所有策略：\n$ # Get a shell on the Cilium pod $ kubectl exec -ti cilium-88k78 -n kube-system -- /bin/bash $ # print out the ingress labels $ # clean up the data $ # fetch each policy via each set of labels $ # (Note that while the structure is \u0026#34;...l4.ingress...\u0026#34;, it reflects all L3, L4 and L7 policy. $ cilium endpoint get 568 -o jsonpath=\u0026#39;{range ..status.policy.realized.l4.ingress[*].derived-from-rules}{@}{\u0026#34;\\n\u0026#34;}{end}\u0026#39;|tr -d \u0026#39;][\u0026#39; | xargs -I{} bash -c \u0026#39;echo \u0026#34;Labels: {}\u0026#34;; cilium policy get {}\u0026#39; Labels: k8s:io.cilium.k8s.policy.name=rule1 k8s:io.cilium.k8s.policy.namespace=default [ { \u0026#34;endpointSelector\u0026#34;: { \u0026#34;matchLabels\u0026#34;: { \u0026#34;any:class\u0026#34;: \u0026#34;deathstar\u0026#34;, \u0026#34;any:org\u0026#34;: \u0026#34;empire\u0026#34;, \u0026#34;k8s:io.kubernetes.pod.namespace\u0026#34;: \u0026#34;default\u0026#34; } }, \u0026#34;ingress\u0026#34;: [ { \u0026#34;fromEndpoints\u0026#34;: [ { \u0026#34;matchLabels\u0026#34;: { \u0026#34;any:org\u0026#34;: \u0026#34;empire\u0026#34;, \u0026#34;k8s:io.kubernetes.pod.namespace\u0026#34;: \u0026#34;default\u0026#34; } } ], \u0026#34;toPorts\u0026#34;: [ { \u0026#34;ports\u0026#34;: [ { \u0026#34;port\u0026#34;: \u0026#34;80\u0026#34;, \u0026#34;protocol\u0026#34;: \u0026#34;TCP\u0026#34; } ], \u0026#34;rules\u0026#34;: { \u0026#34;http\u0026#34;: [ { \u0026#34;path\u0026#34;: \u0026#34;/v1/request-landing\u0026#34;, \u0026#34;method\u0026#34;: \u0026#34;POST\u0026#34; } ] } } ] } ], \u0026#34;labels\u0026#34;: [ { \u0026#34;key\u0026#34;: \u0026#34;io.cilium.k8s.policy.name\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;rule1\u0026#34;, \u0026#34;source\u0026#34;: \u0026#34;k8s\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;io.cilium.k8s.policy.namespace\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;source\u0026#34;: \u0026#34;k8s\u0026#34; } ] } ] Revision: 217 $ # repeat for egress $ cilium endpoint get 568 -o jsonpath=\u0026#39;{range ..status.policy.realized.l4.egress[*].derived-from-rules}{@}{\u0026#34;\\n\u0026#34;}{end}\u0026#39; | tr -d \u0026#39;][\u0026#39; | xargs -I{} bash -c \u0026#39;echo \u0026#34;Labels: {}\u0026#34;; cilium policy get {}\u0026#39; toFQDNs 规则故障排除 随着 DNS 数据的变化，在应用策略很长时间后，效果 toFQDNs 可能会发生变化。这会使调试意外阻塞的连接或瞬时故障变得困难。Cilium 提供 CLI 工具来内省在多个守护进程层中应用 FQDN 策略的状态：\n  cilium policy get 应显示导入的 FQDN 策略：\n{ \u0026#34;endpointSelector\u0026#34;: { \u0026#34;matchLabels\u0026#34;: { \u0026#34;any:class\u0026#34;: \u0026#34;mediabot\u0026#34;, \u0026#34;any:org\u0026#34;: \u0026#34;empire\u0026#34;, \u0026#34;k8s:io.kubernetes.pod.namespace\u0026#34;: \u0026#34;default\u0026#34; } }, \u0026#34;egress\u0026#34;: [ { \u0026#34;toFQDNs\u0026#34;: [ { \u0026#34;matchName\u0026#34;: \u0026#34;api.twitter.com\u0026#34; } ] }, { \u0026#34;toEndpoints\u0026#34;: [ { \u0026#34;matchLabels\u0026#34;: { \u0026#34;k8s:io.kubernetes.pod.namespace\u0026#34;: \u0026#34;kube-system\u0026#34;, \u0026#34;k8s:k8s-app\u0026#34;: \u0026#34;kube-dns\u0026#34; } } ], \u0026#34;toPorts\u0026#34;: [ { \u0026#34;ports\u0026#34;: [ { \u0026#34;port\u0026#34;: \u0026#34;53\u0026#34;, \u0026#34;protocol\u0026#34;: \u0026#34;ANY\u0026#34; } ], \u0026#34;rules\u0026#34;: { \u0026#34;dns\u0026#34;: [ { \u0026#34;matchPattern\u0026#34;: \u0026#34;*\u0026#34; } ] } } ] } ], \u0026#34;labels\u0026#34;: [ { \u0026#34;key\u0026#34;: \u0026#34;io.cilium.k8s.policy.derived-from\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;CiliumNetworkPolicy\u0026#34;, \u0026#34;source\u0026#34;: \u0026#34;k8s\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;io.cilium.k8s.policy.name\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;fqdn\u0026#34;, \u0026#34;source\u0026#34;: \u0026#34;k8s\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;io.cilium.k8s.policy.namespace\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;source\u0026#34;: \u0026#34;k8s\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;io.cilium.k8s.policy.uid\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;fc9d6022-2ffa-4f72-b59e-b9067c3cfecf\u0026#34;, \u0026#34;source\u0026#34;: \u0026#34;k8s\u0026#34; } ] }   发出 DNS 请求后，应通过以下方式获得 FQDN 到 IP 的映射 ：cilium fqdn cache list\n# cilium fqdn cache list Endpoint FQDN TTL ExpirationTime IPs 2761 help.twitter.com. 604800 2019-07-16T17:57:38.179Z 104.244.42.67,104.244.42.195,104.244.42.3,104.244.42.131 2761 api.twitter.com. 604800 2019-07-16T18:11:38.627Z 104.244.42.194,104.244.42.130,104.244.42.66,104.244.42.2   如果允许流量，则这些 IP 应通过以下方式具有相应的本地身份 ：cilium identity list | grep \u0026lt;IP\u0026gt;\n# cilium identity list | grep -A 1 104.244.42.194 16777220 cidr:104.244.42.194/32 reserved:world   ","date":1655438400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"1ae09d6bb7a751aaec2b661a9ef78c3a","permalink":"https://lib.jimmysong.io/cilium-handbook/policy/troubleshooting/","publishdate":"2022-06-17T12:00:00+08:00","relpermalink":"/cilium-handbook/policy/troubleshooting/","section":"cilium-handbook","summary":"策略规则到端点映射 确定哪些策略规则当前对端点有效，并且可以将来自 cilium endpoint list 和 cilium endpoint get 的数据与 cilium policy get 配对。cilium endpoint get 将列出适用于端点的每个规则的标签。可以传递标签列表给 cilium policy get 以显示确切的源策略。请注意，","tags":["Cilium"],"title":"故障排除","type":"book"},{"authors":null,"categories":["Linux"],"content":"现在你已经看到了 eBPF 编程的例子，了解到它是如何工作的。虽然基础示例使得 eBPF 看起来相对简单，但也有一些复杂的地方使得 eBPF 编程充满挑战。\n长期以来，有个问题使得编写和发布 eBPF 程序相对困难，那就是内核兼容性。\n跨内核的可移植性 eBPF 程序可以访问内核数据结构，而这些结构可能在不同的内核版本中发生变化。这些结构本身被定义在头文件中，构成了 Linux 源代码的一部分。在过去编译 eBPF 程序时，必须基于你想运行这些程序的内核兼容的头文件集。\nBCC 对可移植性的处理方法 为了解决跨内核的可移植性问题，BCC 1（BPF 编译器集合，BPF Compiler Collection）项目采取了在运行时编译 eBPF 代码的方法，在目标机器上就地进行。这意味着编译工具链需要安装到每个你想让代码运行 2 的目标机器上，而且你必须在工具启动之前等待编译完成，而且文件系统上必须有内核头文件（实际上并不总是这样）。这就引出了 BPF CO-RE。\nCO-RE CO-RE（Compile Once, Run Everyone，编译一次，到处运行）方法由以下元素组成。\nBTF（BPF Type Format）\n这是一种用于表达数据结构和函数签名布局的格式。现代 Linux 内核支持 BTF，因此你可以从运行中的系统中生成一个名为 vmlinux.h 的头文件，其中包含一个 BPF 程序可能需要的关于内核的所有数据结构信息。\nlibbpf，BPF 库\nlibbpf 一方面提供了加载 eBPF 程序和映射到内核的功能，另一方面也在可移植性方面也起着重要的作用：它依靠 BTF 信息来调整 eBPF 代码，以弥补其编译时的数据结构与目标机器上的数据结构之间的差异。\n编译器支持\nclang 编译器得到了增强，因此当它编译 eBPF 程序时，它包括所谓的 BTF 重定位（relocation），这使得 libbpf 在加载 BPF 程序和映射到内核时知道要调整什么。\n可选的 BPF 骨架\n使用 bpftool gen skeleton 可以从编译的 BPF 对象文件中自动生成一个骨架，其中包含用户空间代码可以方便调用的函数，以管理 BPF 程序的生命周期 —— 将它们加载到内核，附加到事件等等。这些函数是更高层次的抽象，对开发者来说比直接使用 libbpf 更方便。\n关于 CO-RE 的更详细的解释，请阅读 Andrii Nakryiko 的出色 描述。\n自 5.4 版本 3 以来，vmlinux 文件形式的 BTF 信息已经包含在 Linux 内核中，但 libbpf 可以利用的原始 BTF 数据也可以为旧内核生成。在 BTF Hub 上有关于如何生成 BTF 文件的信息，以及用于各种 Linux 发行版的文件档案。\nBPF CO-RE 方法使得 eBPF 程序更易于在任意 Linux 发行版上运行 —— 或者至少在新 Linux 发行版上支持任意 eBPF 能力。但这并不能使 eBPF 更优雅：它本质上仍然是内核编程。\nLinux 内核知识 很快就会发现，为了编写更高级的工具，你需要一些关于 Linux 内核的领域知识。你需要了解你可以访问的数据结构，取决于你的 eBPF 代码被调用的环境。不是所有应用程序的开发者都有解析网络数据包、访问套接字缓冲区或处理系统调用参数的经验。\n内核将如何对你 eBPF 代码的行为做出反应？正如你在 第二章 中了解到的，内核由数百万行代码组成。它的文档可能是稀少的，所以你可能会发现自己不得不阅读内核的源代码来弄清楚某些东西是如何工作的。\n你还需要弄清楚你的 eBPF 代码应该附加到哪些事件。由于可以将 kprobe 附加到整个内核的任何函数入口点，这可能不是一个简单的决定。在某些情况下，这可能很明确 —— 例如，如果你想访问一个传入的网络数据包，那么适当的网络接口上的 XDP 钩子是一个明显的选择。如果你想提供对特定内核事件的可观测性，在内核代码中找到合适的点可能并不难。\n但在其他情况下，选择可能不那么明显。例如，简单地使用 kprobes 来钩住构成内核系统调用接口的函数的工具，可能会被名为 time-of-check 到 time-of-use（TOCTTOU）的安全漏洞所影响。攻击者有一个小的机会窗口，他们可以在 eBPF 代码读取参数后，但在参数被复制到内核内存之前，改变系统调用的参数。在 DEF CON 29 4 上，Rex Guo 和 Junyuan Zeng 做了一个关于这个问题的 出色演讲。一些被最广泛使用的 eBPF 工具是以相当天真的方式编写的，极易受到这种攻击。这不是一个简单的漏洞，而且有办法减轻这些攻击，但如果你正在保护高度敏感的数据，对抗复杂的、有动机的对手，请深入了解你使用的工具是否可能受到影响。\n你已经看到了 BPF CO-RE 是如何使 eBPF 程序在不同的内核版本上工作的，但它只考虑到了数据结构布局的变化，而没有考虑到内核行为的更大变化。例如，如果你想把一个 eBPF 程序附加到内核中的一个特定的函数或 tracepoint 上，你可能需要一个 B 计划，如果该函数或 tracepoint 在不同的内核版本中不存在，该怎么做。\n编排多个 eBPF 程序 当前有很多基于 eBPF 的工具提供了一套可观测能力，通过将 eBPF 程序与一组内核事件挂钩来实现。其中大部分是由 Brendan Gregg 和其他人在 BCC 和 bpftrace 工具中所做的工作而开创的。很多工具（通常是商业的）可能会提供更漂亮的图形和用户界面，但他们还是在这些 eBPF 程序的基础上实现的。\n当你想写代码来编排不同类型的事件之间的交互时，事情就变得相当复杂了。举个例子，Cilium 通过内核的网络堆栈 5 在不同的点上观察到网络数据包，基于来自 Kubernetes CNI（容器网络接口）关于 Kubernetes pod 的信息，对流量进行操作。构建这个系统需要 Cilium 开发人员深入了解内核如何处理网络流量，以及用户空间的 pod 和 容器 概念如何映射到内核概念，如 cgroups 和命 namespace。在实践中，一些 Cilium 的维护者也是内核的开发者，他们致力于增强 eBPF 和网络支持；因此，他们拥有这些知识。\n底线是，尽管 eBPF 提供了一个极其有效和强大的平台来连接到内核，但对于没有大量内核经验的普通开发者来说，这并不容易。如果你对 eBPF 编程感兴趣，我非常鼓励你把它作为练习来学习；在这个领域积累经验可能是非常有价值的，因为它在未来几年内一定会成为受欢迎的专业技能。但实际上，大多数组织不太可能在内部建立许多定制的 eBPF 工具，而是利用专业 eBPF 社区的项目和产品。\n让我们继续思考为什么这些基于 eBPF 的项目和产品在云原生环境中如此强大。\n参考   你可以在 GitHub 页面 上找到 BCC。 ↩︎\n 一些项目采取了将 eBPF 源和所需工具链打包成一个容器镜像的方法。这避免了安装工具链的复杂性和任何随之而来的依赖管理，但这仍意味着编译步骤在目标机器上运行。 ↩︎\n 更多信息见 Andrii Nakryiko 的 IO Visor 帖子。 ↩︎\n Rex Guo and Junyuan Zeng, “Phantom Attack: 逃离系统调用监控，\u0026#34;（DEF CON，2021 年 8 月 5-8 日）。 ↩︎\n Cilium 文档描述了 eBPF 程序如何附加到不同的网络能力钩子，组合起来以实现复杂的网络能力。 ↩︎\n   ","date":1654142400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"919ace53c74a169f4e14ad662edc1da3","permalink":"https://lib.jimmysong.io/what-is-ebpf/ebpf-complexity/","publishdate":"2022-06-02T12:00:00+08:00","relpermalink":"/what-is-ebpf/ebpf-complexity/","section":"what-is-ebpf","summary":"现在你已经看到了 eBPF 编程的例子，了解到它是如何工作的。虽然基础示例使得 eBPF 看起来相对简单，但也有一些复杂的地方使得 eBPF 编程充满挑战。 长期以来，有个问题使得编写和发布 eBPF 程序相对困难，那就是内核兼容性。 跨内核的","tags":["eBPF"],"title":"第四章：eBPF 的复杂性","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"这一章将介绍 Kubernetes 的设计理念及基本概念。\nKubernetes 设计理念与分布式系统 分析和理解 Kubernetes 的设计理念可以使我们更深入地了解 Kubernetes 系统，更好地利用它管理分布式部署的云原生应用，另一方面也可以让我们借鉴其在分布式系统设计方面的经验。\n分层架构 Kubernetes 设计理念和功能其实就是一个类似 Linux 的分层架构，如下图所示\n   Kubernetes 分层架构示意图   核心层：Kubernetes 最核心的功能，对外提供 API 构建高层的应用，对内提供插件式应用执行环境 应用层：部署（无状态应用、有状态应用、批处理任务、集群应用等）和路由（服务发现、DNS 解析等） 管理层：系统度量（如基础设施、容器和网络的度量），自动化（如自动扩展、动态 Provision 等）以及策略管理（RBAC、Quota、PSP、NetworkPolicy 等） 接口层：kubectl 命令行工具、客户端 SDK 以及集群联邦 生态系统：在接口层之上的庞大容器集群管理调度的生态系统，可以划分为两个范畴  Kubernetes 外部：日志、监控、配置管理、CI、CD、Workflow、FaaS、OTS 应用、ChatOps 等 Kubernetes 内部：CRI、CNI、CVI、镜像仓库、Cloud Provider、集群自身的配置和管理等    API 设计原则 对于云计算系统，系统 API 实际上处于系统设计的统领地位，正如本文前面所说，Kubernetes 集群系统每支持一项新功能，引入一项新技术，一定会新引入对应的 API 对象，支持对该功能的管理操作，理解掌握的 API，就好比抓住了 Kubernetes 系统的牛鼻子。Kubernetes 系统 API 的设计有以下几条原则：\n 所有 API 应该是声明式的。正如前文所说，声明式的操作，相对于命令式操作，对于重复操作的效果是稳定的，这对于容易出现数据丢失或重复的分布式环境来说是很重要的。另外，声明式操作更容易被用户使用，可以使系统向用户隐藏实现的细节，隐藏实现的细节的同时，也就保留了系统未来持续优化的可能性。此外，声明式的 API，同时隐含了所有的 API 对象都是名词性质的，例如 Service、Volume 这些 API 都是名词，这些名词描述了用户所期望得到的一个目标分布式对象。 API 对象是彼此互补而且可组合的。这里面实际是鼓励 API 对象尽量实现面向对象设计时的要求，即 “高内聚，松耦合”，对业务相关的概念有一个合适的分解，提高分解出来的对象的可重用性。事实上，Kubernetes 这种分布式系统管理平台，也是一种业务系统，只不过它的业务就是调度和管理容器服务。 高层 API 以操作意图为基础设计。如何能够设计好 API，跟如何能用面向对象的方法设计好应用系统有相通的地方，高层设计一定是从业务出发，而不是过早的从技术实现出发。因此，针对 Kubernetes 的高层 API 设计，一定是以 Kubernetes 的业务为基础出发，也就是以系统调度管理容器的操作意图为基础设计。 低层 API 根据高层 API 的控制需要设计。设计实现低层 API 的目的，是为了被高层 API 使用，考虑减少冗余、提高重用性的目的，低层 API 的设计也要以需求为基础，要尽量抵抗受技术实现影响的诱惑。 尽量避免简单封装，不要有在外部 API 无法显式知道的内部隐藏的机制。简单的封装，实际没有提供新的功能，反而增加了对所封装 API 的依赖性。内部隐藏的机制也是非常不利于系统维护的设计方式，例如 StatefulSet 和 ReplicaSet，本来就是两种 Pod 集合，那么 Kubernetes 就用不同 API 对象来定义它们，而不会说只用同一个 ReplicaSet，内部通过特殊的算法再来区分这个 ReplicaSet 是有状态的还是无状态。 API 操作复杂度与对象数量成正比。这一条主要是从系统性能角度考虑，要保证整个系统随着系统规模的扩大，性能不会迅速变慢到无法使用，那么最低的限定就是 API 的操作复杂度不能超过 O(N)，N 是对象的数量，否则系统就不具备水平伸缩性了。 API 对象状态不能依赖于网络连接状态。由于众所周知，在分布式环境下，网络连接断开是经常发生的事情，因此要保证 API 对象状态能应对网络的不稳定，API 对象的状态就不能依赖于网络连接状态。 尽量避免让操作机制依赖于全局状态，因为在分布式系统中要保证全局状态的同步是非常困难的。  控制机制设计原则  控制逻辑应该只依赖于当前状态。这是为了保证分布式系统的稳定可靠，对于经常出现局部错误的分布式系统，如果控制逻辑只依赖当前状态，那么就非常容易将一个暂时出现故障的系统恢复到正常状态，因为你只要将该系统重置到某个稳定状态，就可以自信的知道系统的所有控制逻辑会开始按照正常方式运行。 假设任何错误的可能，并做容错处理。在一个分布式系统中出现局部和临时错误是大概率事件。错误可能来自于物理系统故障，外部系统故障也可能来自于系统自身的代码错误，依靠自己实现的代码不会出错来保证系统稳定其实也是难以实现的，因此要设计对任何可能错误的容错处理。 尽量避免复杂状态机，控制逻辑不要依赖无法监控的内部状态。因为分布式系统各个子系统都是不能严格通过程序内部保持同步的，所以如果两个子系统的控制逻辑如果互相有影响，那么子系统就一定要能互相访问到影响控制逻辑的状态，否则，就等同于系统里存在不确定的控制逻辑。 假设任何操作都可能被任何操作对象拒绝，甚至被错误解析。由于分布式系统的复杂性以及各子系统的相对独立性，不同子系统经常来自不同的开发团队，所以不能奢望任何操作被另一个子系统以正确的方式处理，要保证出现错误的时候，操作级别的错误不会影响到系统稳定性。 每个模块都可以在出错后自动恢复。由于分布式系统中无法保证系统各个模块是始终连接的，因此每个模块要有自我修复的能力，保证不会因为连接不到其他模块而自我崩溃。 每个模块都可以在必要时优雅地降级服务。所谓优雅地降级服务，是对系统鲁棒性的要求，即要求在设计实现模块时划分清楚基本功能和高级功能，保证基本功能不会依赖高级功能，这样同时就保证了不会因为高级功能出现故障而导致整个模块崩溃。根据这种理念实现的系统，也更容易快速地增加新的高级功能，因为不必担心引入高级功能影响原有的基本功能。  Kubernetes 的核心技术概念和 API 对象 API 对象是 Kubernetes 集群中的管理操作单元。Kubernetes 集群系统每支持一项新功能，引入一项新技术，一定会新引入对应的 API 对象，支持对该功能的管理操作。例如副本集 Replica Set 对应的 API 对象是 RS。\n每个 API 对象都有 3 大类属性：元数据 metadata、规范 spec 和状态 status。元数据是用来标识 API 对象的，每个对象都至少有 3 个元数据：namespace，name 和 uid；除此以外还有各种各样的标签 labels 用来标识和匹配不同的对象，例如用户可以用标签 env 来标识区分不同的服务部署环境，分别用 env=dev、env=testing、env=production 来标识开发、测试、生产的不同服务。规范描述了用户期望 Kubernetes 集群中的分布式系统达到的理想状态（Desired State），例如用户可以通过复制控制器 Replication Controller 设置期望的 Pod 副本数为 3；status 描述了系统实际当前达到的状态（Status），例如系统当前实际的 Pod 副本数为 2；那么复制控制器当前的程序逻辑就是自动启动新的 Pod，争取达到副本数为 3。\nKubernetes 中所有的配置都是通过 API 对象的 spec 去设置的，也就是用户通过配置系统的理想状态来改变系统，这是 Kubernetes 重要设计理念之一，即所有的操作都是声明式（Declarative）的而不是命令式（Imperative）的。声明式操作在分布式系统中的好处是稳定，不怕丢操作或运行多次，例如设置副本数为 3 的操作运行多次也还是一个结果，而给副本数加 1 的操作就不是声明式的，运行多次结果就错了。\nPod Kubernetes 有很多技术概念，同时对应很多 API 对象，最重要的也是最基础的是 Pod。Pod 是在 Kubernetes 集群中运行部署应用或服务的最小单元，它是可以支持多容器的。Pod 的设计理念是支持多个容器在一个 Pod 中共享网络地址和文件系统，可以通过进程间通信和文件共享这种简单高效的方式组合完成服务。Pod 对多容器的支持是 K8 最基础的设计理念。比如你运行一个操作系统发行版的软件仓库，一个 Nginx 容器用来发布软件，另一个容器专门用来从源仓库做同步，这两个容器的镜像不太可能是一个团队开发的，但是他们一块儿工作才能提供一个微服务；这种情况下，不同的团队各自开发构建自己的容器镜像，在部署的时候组合成一个微服务对外提供服务。\nPod 是 Kubernetes 集群中所有业务类型的基础，可以看作运行在 Kubernetes 集群中的小机器人，不同类型的业务就需要不同类型的小机器人去执行。目前 Kubernetes 中的业务主要可以分为长期伺服型（long-running）、批处理型（batch）、节点后台支撑型（node-daemon）和有状态应用型（stateful application）；分别对应的小机器人控制器为 Deployment、Job、DaemonSet 和 StatefulSet，本文后面会一一介绍。\n副本控制器（Replication Controller，RC） RC 是 Kubernetes 集群中最早的保证 Pod 高可用的 API 对象。通过监控运行中的 Pod 来保证集群中运行指定数目的 Pod 副本。指定的数目可以是多个也可以是 1 个；少于指定数目，RC 就会启动运行新的 Pod 副本；多于指定数目，RC 就会杀死多余的 Pod 副本。即使在指定数目为 1 的情况下，通过 RC 运行 Pod 也比直接运行 Pod 更明智，因为 RC 也可以发挥它高可用的能力，保证永远有 1 个 Pod 在运行。RC 是 Kubernetes 较早期的技术概念，只适用于长期伺服型的业务类型，比如控制小机器人提供高可用的 Web 服务。\n副本集（Replica Set，RS） RS 是新一代 RC，提供同样的高可用能力，区别主要在于 RS 后来居上，能支持更多种类的匹配模式。副本集对象一般不单独使用，而是作为 Deployment 的理想状态参数使用。\n部署（Deployment） 部署表示用户对 Kubernetes 集群的一次更新操作。部署是一个比 RS 应用模式更广的 API 对象，可以是创建一个新的服务，更新一个新的服务，也可以是滚动升级一个服务。滚动升级一个服务，实际是创建一个新的 RS，然后逐渐将新 RS 中副本数增加到理想状态，将旧 RS 中的副本数减小到 0 的复合操作；这样一个复合操作用一个 RS 是不太好描述的，所以用一个更通用的 Deployment 来描述。以 Kubernetes 的发展方向，未来对所有长期伺服型的的业务的管理，都会通过 Deployment 来管理。\n服务（Service） RC、RS 和 Deployment 只是保证了支撑服务的微服务 Pod 的数量，但是没有解决如何访问这些服务的问题。一个 Pod 只是一个运行服务的实例，随时可能在一个节点上停止，在另一个节点以一个新的 IP 启动一个新的 Pod，因此不能以确定的 IP 和端口号提供服务。要稳定地提供服务需要服务发现和负载均衡能力。服务发现完成的工作，是针对客户端访问的服务，找到对应的的后端服务实例。在 K8 集群中， …","date":1653062400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"9a4aeaa81913ffa1359ea9c3c7c65640","permalink":"https://lib.jimmysong.io/kubernetes-handbook/architecture/perspective/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/architecture/perspective/","section":"kubernetes-handbook","summary":"这一章将介绍 Kubernetes 的设计理念及基本概念。 Kubernetes 设计理念与分布式系统 分析和理解 Kubernetes 的设计理念可以使我们更深入地了解 Kubernetes 系统，更好地利用它管理分布式部署的云原生应用，另一方面也可以让我们借鉴其在分布式系统设计方面的经","tags":["Kubernetes"],"title":"Kubernetes 的设计理念","type":"book"},{"authors":null,"categories":["安全"],"content":"从理论上讲，DevSecOps 原语可以应用于许多应用架构，但最适合于基于微服务的架构，由于应用是由相对较小的、松散耦合的模块组成的，被称为微服务，因此允许采用敏捷开发模式。即使在基于微服务的架构中，DevSecOps 原语的实现也可以采取不同的形式，这取决于平台。在本文中，所选择的平台是一个容器编排和资源管理平台（如 Kubernetes）。该平台是物理（裸机）或虚拟化（如虚拟机、容器）基础设施上的一个抽象层。为了在本文中明确提及该平台或应用环境，它被称为 DevSecOps 原语参考平台，或简称为 参考平台。\n在描述参考平台的 DevSecOps 原语的实现之前，我们假设在部署服务网格组件方面采用了以下 尽职调查：\n 用于部署和管理基于服务网格的基础设施（如网络路由）、策略执行和监控组件的安全设计模式 测试证明这些服务网格组件在应用的各个方面（如入口、出口和内部服务）的各种情况下都能按预期工作。  为参考平台实施 DevSecOps 原语所提供的指导与 (a) DevSecOps 管道中使用的工具和 (b) 提供应用服务的服务网格软件无关，尽管来自 Istio 等服务网格产品的例子被用来将它们与现实世界的应用工件（如容器、策略执行模块等）联系起来。\n以下是对参考平台所呈现的整个应用环境中的代码类型（在执行摘要中提到）的稍微详细的描述。请注意，这些代码类型包括那些支持实施 DevSecOps 原语的代码。\n 应用代码：体现了执行一个或多个业务功能的应用逻辑，由描述业务事务和数据库访问的代码组成。 应用服务代码（如服务网格代码）：为应用提供各种服务，如服务发现、建立网络路由、网络弹性服务（如负载均衡、重试），以及安全服务（如根据策略强制执行认证、授权等，见第 4 章）。 基础设施即代码：以声明性代码的形式表达运行应用程序所需的计算、网络和存储资源。 策略即代码：包含声明性代码，用于生成实现安全目标的规则和配置参数，例如在运行期间通过安全控制（如认证、授权）实现零信任。 可观测性即代码：触发与日志（记录所有事务）和追踪（执行应用程序请求所涉及的通信途径）以及监控（在运行期间跟踪应用程序状态）有关的软件。  代码类型 3、4 和 5 可能与代码类型 2 有重叠。\n本文件涵盖了与上述所有五种代码类型相关的管道或工作流程的实施。因此，整个应用环境（不仅仅是应用代码）受益于应用代码的所有最佳实践（例如，敏捷迭代开发、版本控制、治理等）。基础设施即代码、策略即代码和可观测性即代码属于一个特殊的类别，称为声明性代码。当使用 “xx 即代码” 的技术时，编写的代码（例如，用于配置资源的代码）被管理，类似于应用源代码。这意味着它是有版本的，有文件的，并且有类似于应用源代码库的访问控制定义。通常，使用特定领域的声明性语言：声明需求，并由相关工具将其转换为构成运行时实例的工件。例如，在基础设施即代码（IaC）的情况下，声明性语言将基础设施建模为一系列的资源。相关的配置管理工具将这些资源集中起来，并生成所谓的 清单，定义与所定义的资源相关的平台（运行时实例）的最终状态。这些清单存储在与配置管理工具相关的服务器中，并由该工具用于为指定平台上的运行时实例创建编译的配置指令。清单通常以平台中立的表示方式（如 JSON）进行编码，并通过 REST API 反馈给平台资源配置代理。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"0929a4abc243eb01c17c981028a01a8e","permalink":"https://lib.jimmysong.io/service-mesh-devsecops/intro/scope/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/service-mesh-devsecops/intro/scope/","section":"service-mesh-devsecops","summary":"从理论上讲，DevSecOps 原语可以应用于许多应用架构，但最适合于基于微服务的架构，由于应用是由相对较小的、松散耦合的模块组成的，被称为微服务，因此允许采用敏捷开发模式。即使在基于微服务的架构中，D","tags":["Service Mesh","DevSecOps"],"title":"1.1 范围","type":"book"},{"authors":null,"categories":null,"content":"本章中，我们讨论了希望通过软件赋予我们业务的能力并迁移到云原生应用架构的动机：\n速度\n比我们的竞争对手更快速得创新、试验并传递价值。\n安全\n在保持稳定性、可用性和持久性的同时，具有快速行动的能力。\n扩展\n根据需求变化弹性扩展。\n移动性\n客户可以随时随地通过任何设备无缝的跟我们交互。\n我们还研究了云原生应用架构的独特特征，以及如何赋予我们这些能力：\n12因素应用\n一套优化应用设计速度，安全性和规模的模式。\n微服务\n一种架构模式，可帮助我们将部署单位与业务能力保持一致，使每个业务能够独立和自主地移动，这样一来也更快更安全。\n自服务敏捷基础设施\n云平台使开发团队能够在应用和服务抽象层面上运行，提供基础架构级速度，安全性和扩展性。\n基于API的协作\n将服务间交互定义为可自动验证协议的架构模式，通过简化的集成工作实现速度和安全性。\n抗脆弱性\n随着速度和规模扩大，系统的压力随之增加，系统的响应能力和安全性也随之提高。\n在下一章中，我们将探讨大多数企业为采用云原生应用架构而需要做出哪些改变。\n 下一章   ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"5c7e576ca79ab2100f1d6a4d651727bc","permalink":"https://lib.jimmysong.io/migrating-to-cloud-native-application-architectures/the-rise-of-cloud-native/summary/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/migrating-to-cloud-native-application-architectures/the-rise-of-cloud-native/summary/","section":"migrating-to-cloud-native-application-architectures","summary":"本章中，我们讨论了希望通过软件赋予我们业务的能力并迁移到云原生应用架构的动机： 速度 比我们的竞争对手更快速得创新、试验并传递价值。 安全 在保持稳定性、可用性和持久性的同时，具有快速行动的能力。 扩展 根据需求","tags":null,"title":"1.3：本章小结","type":"book"},{"authors":null,"categories":null,"content":"本章中，我们探讨了大多数企业采用云原生应用架构所需要做出的变革。从宏观总体上看是权力下放和自治：\nDevOps\n技能集中化转变为跨职能团队。\n持续交付\n发行时间表和流程的权力下放。\n自治\n决策权力下放。\n我们将这种权力下放编成两个主要的团队结构：\n业务能力团队\n自主决定设计、流程和发布时间表的跨职能团队。\n平台运营团队\n为跨职能团队提供他们所需要运行平台。\n而在技术上，我们也分散自治：\n单体应用到微服务\n将个人业务能力的控制分配给单个自主服务。\n有界上下文\n将业务领域模型的内部一致子集的控制分配到微服务。\n容器化\n将对应用包装的控制分配给业务能力团队。\n编排\n将服务集成控制分配给服务端点。\n所有这些变化造就了无数的自治单元，辅助我们以期望的创新速度安全前行。\n在最后一章中，我们将通过一组操作手册，深入研究迁移到云原生应用架构的技术细节。\n 下一章   ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"b48cfc7a3b74929a365d558c26556695","permalink":"https://lib.jimmysong.io/migrating-to-cloud-native-application-architectures/changes-needed/summary/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/migrating-to-cloud-native-application-architectures/changes-needed/summary/","section":"migrating-to-cloud-native-application-architectures","summary":"本章中，我们探讨了大多数企业采用云原生应用架构所需要做出的变革。从宏观总体上看是权力下放和自治： DevOps 技能集中化转变为跨职能团队。 持续交付 发行时间表和流程的权力下放。 自治 决策权力下放。 我们将这种权力下放编","tags":null,"title":"2.4 本章小结","type":"book"},{"authors":null,"categories":["安全"],"content":"策略即代码涉及编纂所有策略，并作为 CI/CD 管道的一部分运行，使其成为应用程序运行时的一个组成部分。策略类别的例子包括授权策略、网络策略和实现工件策略（例如，容器策略）。典型的 策略即代码软件的策略管理能力可能带有一套预定义的策略类别和策略，也支持通过提供策略模板定义新的策略类别和 相关策略。策略即代码所要求的尽职调查是，它应该提供保护，防止与应用环境（包括基础设施）相关的所有已知威胁，只有当该代码被定期扫描和更新，以应对与应用类别（如网络应用）和托管基础设施相关的威胁，才能确保这一点。下面的表 1 中给出了一些策略类别和相关策略的例子。\n表 1：策略类别和策略实例\n   策略类别 策略示例     网络策略和零信任策略 - 封锁指定端口 - 指定入口主机名称 - 一般来说，所有的网络访问控制策略   实施工件策略（例如，容器策略） - 对服务器进行加固，对基础镜像进行漏洞扫描 - 确保容器不以 root 身份运行 - 阻止容器的权限升级   存储策略 - 设置持久性卷大小 - 设置持久性卷回收策略   访问控制策略 - 确保策略涵盖所有数据对象 - 确保策略涵盖管理和应用访问的所有角色 - 确保数据保护策略涵盖静态数据、传输中数据和使用中数据 - 确保所有类型的策略不存在冲突   供应链策略 - 只允许经批准的容器注册表 - 只允许经认证的库   审计和问责策略 - 确保有与审计和问责职能相关的策略    在策略即代码软件中定义的策略可以转化为应用基础设施运行时配置参数中的以下内容：\n 强化策略的可执行性（例如，服务代理中的 WASM）。 用于调用外部策略决策模块的触发器（例如，调用外部授权服务器，根据对与当前访问请求相关的访问控制策略的评估，做出允许 / 拒绝的决定）。 它还可能影响 IaC，以确保在部署环境中提供适当的资源，以执行安全、隐私和合规要求。  ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"b6c7a42d093c20fd8424297fa73a2561","permalink":"https://lib.jimmysong.io/service-mesh-devsecops/implement/ci-cd-pipeline-for-policy-as-code/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/service-mesh-devsecops/implement/ci-cd-pipeline-for-policy-as-code/","section":"service-mesh-devsecops","summary":"策略即代码涉及编纂所有策略，并作为 CI/CD 管道的一部分运行，使其成为应用程序运行时的一个组成部分。策略类别的例子包括授权策略、网络策略和实现工件策略（例如，容器策略）。典型的 策略即代码软件的策略管理能力可能","tags":["Service Mesh","DevSecOps"],"title":"4.4 策略即代码的 CI/CD 管道","type":"book"},{"authors":null,"categories":["云原生"],"content":"云原生基础架构并不适合所有人。任何架构设计都经过了一系列的权衡。您只有熟悉自己的需求才能决定哪些权衡是有益的，哪些是有害的。\n不要在不了解架构的影响和限制的情况下采用工具或设计。我们相信云原生基础架构有很多好处，但需要意识到不应该盲目的采用。我们不愿意引导大家通过错误的方式来满足需求。\n怎么知道是否应该使用云原生基础架构设计？确定云原生基础架构是否适合您，下面是一些需要了解的问题：\n 您有云原生应用程序吗？(有关可从云原生基础架构中受益的应用程序功能，请参阅第 1 章) 您的工程团队是否愿意且能够编写出体现其作业功能的生产质量代码？ 您在本地或公有云是否拥有基于 API 驱动的基础架构（IaaS）？ 您的业务是否需要更快的开发迭代或非线性人员 / 系统缩放比例？  如果您对所有这些问题都回答 “yes”，那么您可能会从本书其余部分介绍的基础架构中受益。如果您对这些问题中的某个问题回答是 “no”，这并不意味着您无法从某些云原生实践中受益，但是您可能需要做更多工作，然后才能从此类基础架构中充分受益。\n在业务准备好之前武断地采用云原生基础架构效果会很糟糕，因为这会强制使用一个不正确的解决方案。在没有充分调查的情况下可能会失败，您可以能会把云原生架构看做是有缺陷或毫无用处的。鉴于之前尝试的云原生方案的失败，今后该方案也可能很难再次采用，无论它是否是正确的解决方案。\n在您准备将组织和技术转变为云原生时，我们将讨论一些需要关注的领域。要考虑的事情有很多，关键领域是您的应用程序、组织中的人员、基础架构系统和您的业务。\n应用程序 应用程序是准备工作中最简单的部分。设计模式已经很完善，自公共云出现以来，工具性能得到了显着提升。如果您无法构建云原生应用程序并通过自动部署管道来验证它们的话，则不应继续采用云原生基础架构。\n构建云原生应用程序并不一定需要微服务。这并不意味着您必须用最流行的语言开发所有软件。您必须编写可以由软件管理的软件。\n在开发过程中，人只会与云原生应用程序进行交互。其他一切都应该由基础架构或其他应用程序来管理。\n应用程序应该可以动态地扩展出多个实例。扩展通常意味着负载均衡器后运行着同一个应用程序有多个副本。假定应用程序将状态存储在存储服务（即数据库）中，并且不需要运行实例之间的复杂协调。\n动态应用程序管理意味着不需要人参与这项工作。应用程序度量触发了基础架构操作扩展应用程序。这是大多数云环境的基本特征。运行动态伸缩的资源组并不意味着您拥有云原生基础架构；但如果您的应用程序可以自动伸缩，它可能表明您的应用程序已准备就绪了。\n为了使应用程序受益，编写应用程序和配置基础架构的人员需要支持这种工作方法。如果没有人愿意放弃对软件的控制，您将永远无法实现它的好处。\n人 人是云原生基础架构中最难的部分。\n如果您想建立一个能够用软件取代人们职能和决策的架构，那么您需要确保人们了解您有最大的诉求。不仅需要人们接受变化，还需要他们自己主动寻求改变。\n开发应用程序很困难，运维基础架构很难。应用程序开发人员经常相信他们可以用工具和自动化取代基础架构运维，运维人员希望应用程序开发人员能够编写更可靠的代码，并提供自动调试和恢复。这些紧张关系是 DevOps 的基础，DevOps 有许多其他书籍，包括由 Jennifer Davis 和 Katherine Daniels 撰写的 Effective DevOps（O’Reilly，2016）。\n人们不会扩大规模，也不擅长重复无聊的工作。\n应用程序和系统工程师的目标应该是消除无聊和重复的任务，以便他们可以专注于更有趣的问题。他们需要具备开发可以包含业务逻辑和决策的软件的技能。需要有足够的工程师来编写所需的软件，更重要的是维护它。\n最关键的方面是他们需要一起工作。如果没有其他方面的支持，工程的一方无法迁移到运行和管理应用程序的新方式。团队组织和沟通结构非常重要。\n我们会尽快将团队准备好，但首先，我们必须确定基础架构迁移到云原生的时机。\n系统 云原生应用程序需要系统抽象。应用程序不应该关注单个硬编码主机名。如果您的应用程序无法在个别主机上运行，那就说明您的系统尚未准备好使用于云原生基础架构。\n使用单个服务器（虚拟机或物理机）运行操作系统，并将其转换为访问资源的方法，这就是我们所说的 “抽象”。单个系统不应该是应用程序部署的目标。资源（CPU、内存和磁盘）应该集中在所有可用的机器上，然后由平台根据应用程序的请求进行分配。\n在云原生基础架构中，您必须隐藏底层系统以提高可靠性。云计算基础架构（如应用程序）会预期基础组件故障，并且可以优雅地处理此类故障。这是必要的，因为基础架构工程师不再控制堆栈中的所有内容。\n Kubernetes 云原生基础架构\nKubernetes 是一个框架，它使云的方式管理应用程序变得更加容易。但是，您也可以用一种非云原生的方式使用 Kubernetes。\nKubernetes 公开了基于其核心功能的扩展，但这不是您的基础架构的最终目标。其他项目 (例如，OpenShift) 建立在它之上，将 Kubernetes 从开发人员和应用程序中抽象出来。\n应用程序应该运行在平台上。云原生基础架构并且鼓励这样运行基础架构的方式。\n如果您的应用程序是动态的，但基础架构是静态的，那么您很快就会陷入单靠 Kubernetes 无法解决的僵局。\n 当这不再是一个挑战时，基础架构已经准备好成为云原生的了。一旦基础架构变得简单，自动化、自助服务和动态，就有可能被忽略。当系统可以被忽略，并且技术变得单调时，是时候向上移动堆栈了。\n如果您的系统管理依赖于硬件定制或在 “混合云” 中运行，则您的系统可能还没有准备好。可能需要管理一个数据中心，并且私有化。您需要保持警惕，将建立数据中心的责任与管理基础架构的责任分开。\n谷歌、Facebook、亚马逊和微软都发现通过开放的计算项目从头开始创建硬件是有好处的。之所以创建自己的硬件是因为有性能和成本的限制。因为硬件设计和基础架构构建者之间存在明确的责任分离，这些公司能够在创建定制硬件的同时运行云原生基础架构。它们不会受到 “内部部署” 的阻碍。相反，他们可以共同优化其硬件和软件，以获得更高的效率和性能。\n管理自己的数据中心需要大量时间和金钱的投入。创建私有云也是如此。两者都需要建立和管理数据中心团队、创建和维护 API 的团队以及在 IaaS API 之上创建抽象的团队。\n所有这些都可以完成，决定管理整个堆栈是否有价值取决于您的业务。\n现在，我们看看业务领域需要做哪些准备才能迁移到云原生。\n业务  如果系统的架构和组织的架构不一致，则组织的架构会胜出。\n—— 鲁斯马兰，“康威定律”\n 企业变革速度非常缓慢。当通过扩展人员来管理扩展系统不再有效时，以及产品开发需要更多灵活性时，他们可能已经准备好采用云原生实践了。\n人无法无限扩展。对于增加管理更多服务器或开发更多代码的每个人来说，支持他们的人力基础架构（例如办公室空间）都有一定的压力。因为需要更多的沟通和协调，还会有更多额外的开销。\n正如我们在第 1 章中讨论的那样，通过使用公有云，您可以通过租用服务器来减少一些流程和人员开销。即使使用公有云，您仍然会需要管理基础架构详细信息的人员（例如服务器，服务和用户帐户）。\n当沟通结构反映业务需要创建的基础架构和应用程序时，业务已准备好采用云原生实践。这包括反映像微服务这样架构的沟通结构。他们可能是小型的独立团队，无需通过层层管理与其他团队交流或合作。\n DevOps 和 Cloud Native\nDevOps 可以补充团队合作的方式，并影响使用的工具类型。公司采用后有很多好处，包括快速原型化和提高部署速度。它也非常注重组织的文化。\n云原生需要高性能组织，但更注重于设计、架构和健康度，而不是团队工作流程和文化。如果您原以为必须解决应用程序开发人员、基础架构运维以及技术部门中任何人员之间的交互问题，才可以成功地实现云原生模式的话，那么您可能会对此感到意外。\n 迫使业务变化的另一个限制因素是应用程序对敏捷性的要求更高了。企业不仅需要快速部署，还需要彻底改变部署的内容。\n部署的原始数量无关紧要。重要的是尽可能快地提供客户价值。相信部署的软件将第一次，甚至是第 100 次，满足所有客户的需求，这是一个谬论。\n当业务意识到需要频繁迭代和更改时，它可能已经准备好采用云原生应用程序了。只要在人员效率和流程方面遇到限制，且可以随时更改它，就可以准备迁移到云原生基础架构了。\n所有那些表明何时采用云原生的因素都不能说明全部情况。任何设计都需要权衡折衷。因此，在某些情况下，云原生基础架构不是正确的选择。\n什么情况下不需要云原生基础架构 只有了解了系统有哪些限制，再清楚系统可以带来的好处才有用。也就是说，是否采用一个系统的决定性因素是它的限制而使用它可以带来的利益。\n记住需求随时间变化也很重要。现在的关键功能可能在未来并不是。同样，如果下面的情况目前并不理想，那么您可以不采用云原生。\n技术限制 就像应用程序一样，在基础架构中，最简单的是技术性限制。如果您知道什么时候应该采用有技术优势的云原生基础架构，那么您可以思考下何时不应该采用云原生基础架构。\n第一个限制是没有云原生应用程序。正如在第一章中讨论的那样，如果您的应用程序需要人工交互，无论是调度、重新启动还是搜索日志，云原生基础架构都没有多大好处。\n即使您有一个可以动态调度的应用程序，也不会使其成为云原生。如果您的应用程序在 Kubernetes 上运行，但仍需要人工设置监控、日志收集和负载均衡，则它不是云原生。只是将应用程序部署在 Kubernetes 运行并不意味着云原生。\n如果您有一个编排调度器，重要的是看看它是如何运行的。您是否需要下订单、创建工单或发送电子邮件以获取服务器？\n这些是您没有自助服务基础架构的指标，这是云计算的一项要求。\n在云中，您只需提供帐单信息并调用 API。即使您在内部运行服务器，您也应该有一个可以构建 IaaS 的团队，然后将云原生基础架构分层布局。\n如果您要在自己的数据中心中构建云环境，图 2-1 显示了您的基础架构组件适合的示例。所有原始组件（例如，计算、存储、网络）都应该可以从自助式 IaaS API 中获得。\n   图 2-1. 云原生基础架构的示例图层  在公有云中，您拥有 IaaS 和托管服务，但这并不意味着您的业务已准备好使用公有云。\n当您构建运行应用程序的平台时，了解您正在进行的操作非常重要。最初的开发只是构建和维护平台所需花费的一小部分，特别是对业务至关重要的平台。\n维护通常会消耗大约 40％到 80％（平均 60％）的软件成本。因此，这可能是最重要的生命周期阶段，发现业务需求和建立开发所需的技能可能对于一个小团队来说太过分了。一旦您掌握了开发所需平台的技能，您仍然需要投入时间来改进和维护系统。这需要比初始开发更长的时间。\n公有云提供商的产品为企业提供绝佳的运行环境。如果您不能或者不愿意让您的平台成为业务，那么您不应该自己创建一个平台。\n请记住，您不必自己构建一切。您可以使用可以组装到所需平台的服务或产品。\n可靠性仍然是基础架构的关键特性。如果您还没有准备好放弃对底层基础架构堆栈的控制，并且仍然通过接受故障来制造可靠的产品，那么云原生基础架构并不是正确的选择。\n非技术限制同样重要，可能超出您的控制范围。\n业务限制 如果现有流程不支持更改基础架构，则需要首先克服该障碍。幸运的是，您不必一个人做。\n本书希望有助于向需要说服力的人清楚地解释云原生的好处和流程。还有许多案例研究和公司分享他们采用这些做法的经验。本书附录 C 中将提供一个案例研究，您还可以找到相关示例并与同行和管理层分享。\n如果企业还没有实验的途径和支持尝试新事物的文化（以及伴随失败而来的后果），那么改变流程可能是不可能的。在这种情况下，您的需要等待达到必须改变的临界点，或者说服管理层认为改变是必要的。\n以一个外部的视角准确辨别一个企业是否准备好采用云原生是不可能的。不过，下面这些流程可以明确指示一个公司未准备接纳云原生：\n 需要人工干预的资源请 …","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"2052dff8a60a2e46aab025fdf6f1d2e4","permalink":"https://lib.jimmysong.io/cloud-native-infra/when-to-adopt-cloud-native/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/cloud-native-infra/when-to-adopt-cloud-native/","section":"cloud-native-infra","summary":"云原生基础架构并不适合所有人。任何架构设计都经过了一系列的权衡。您只有熟悉自己的需求才能决定哪些权衡是有益的，哪些是有害的。 不要在不了解架构的影响和限制的情况下采用工具或设计。我们相信云原生基础架构有","tags":["云原生"],"title":"第 2 章：采纳云原生基础架构的时机","type":"book"},{"authors":null,"categories":["安全"],"content":"勘误 1\nPDF 原文第 4 页，kubelet 端口，默认应为 10250，而不是 10251。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"a95a8d264405f5f828a845f088eb75db","permalink":"https://lib.jimmysong.io/kubernetes-hardening-guidance/corrigendum/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/kubernetes-hardening-guidance/corrigendum/","section":"kubernetes-hardening-guidance","summary":"勘误 1 PDF 原文第 4 页，kubelet 端口，默认应为 10250，而不是 10251。","tags":["Kubernetes","安全"],"title":"勘误","type":"book"},{"authors":null,"categories":["Envoy"],"content":"本节将为你介绍 HTTP 连接管理器中的请求匹配。\n路径匹配 我们只谈了一个使用前缀字段匹配前缀的匹配规则。下面的表格解释了其他支持的匹配规则。\n   规则名称 描述     prefix 前缀必须与path标头的开头相匹配。例如，前缀 /api 将匹配路径 /api 和 /api/v1，而不是 /。   path 路径必须与确切的path标头相匹配（没有查询字符串）。例如，路径 /api 将匹配路径 /api，但不匹配 /api/v1 或 /。   safe_regex 路径必须符合指定的正则表达式。例如，正则表达式 ^/products/\\d+$ 将匹配路径 /products/123 或 /products/321，但不是 /products/hello 或 /api/products/123。   connect_matcher 匹配器只匹配 CONNECT 请求（目前在 Alpha 中）。    默认情况下，前缀和路径匹配是大小写敏感的。要使其不区分大小写，我们可以将 case_sensitive 设置为 false。注意，这个设置不适用于 safe_regex 匹配。\nHeader 匹配 另一种匹配请求的方法是指定一组 Header。路由器根据路由配置中所有指定的 Header 检查请求 Header。如果所有指定的头信息都存在于请求中，并且设置了相同的值，则进行匹配。\n多个匹配规则可以应用于Header。\n范围匹配\nrange_match 检查请求 Header 的值是否在指定的以十进制为单位的整数范围内。该值可以包括一个可选的加号或减号，后面是数字。\n为了使用范围匹配，我们指定范围的开始和结束。起始值是包含的，而终止值是不包含的（[start, end)）。\n- match:prefix:\u0026#34;/\u0026#34;headers:- name:minor_versionrange_match:start:1end:11上述范围匹配将匹配 minor_version 头的值，如果它被设置为 1 到 10 之间的任何数字。\n存在匹配\npresent_match 检查传入的请求中是否存在一个特定的头。\n- match:prefix:\u0026#34;/\u0026#34;headers:- name:debugpresent_match:true如果我们设置了debug头，无论头的值是多少，上面的片段都会评估为true。如果我们把 present_match 的值设为 false，我们就可以检查是否有 Header。\n字符串匹配\nstring_match 允许我们通过前缀或后缀，使用正则表达式或检查该值是否包含一个特定的字符串，来准确匹配头的值。\n- match:prefix:\u0026#34;/\u0026#34;headers:# 头部`regex_match`匹配所提供的正则表达式- name:regex_matchstring_match:safe_regex_match:google_re2:{}regex:\u0026#34;^v\\\\d+$\u0026#34;# Header `exact_match`包含值`hello`。- name:exact_matchstring_match:exact:\u0026#34;hello\u0026#34;# 头部`prefix_match`以`api`开头。- name:prefix_matchstring_match:prefix:\u0026#34;api\u0026#34;# 头部`后缀_match`以`_1`结束- name:suffix_matchstring_match:suffix:\u0026#34;_1\u0026#34;# 头部`contains_match`包含值 \u0026#34;debug\u0026#34;- name:contains_matchstring_match:contains:\u0026#34;debug\u0026#34;反转匹配\n如果我们设置了 invert_match，匹配结果就会反转。\n- match:prefix:\u0026#34;/\u0026#34;headers:- name:versionrange_match:start:1end:6invert_match:true上面的片段将检查 version 头中的值是否在 1 和 5 之间；然而，由于我们添加了 invert_match 字段，它反转了结果，检查头中的值是否超出了这个范围。\ninvert_match 可以被其他匹配器使用。例如：\n- match:prefix:\u0026#34;/\u0026#34;headers:- name:envcontains_match:\u0026#34;test\u0026#34;invert_match:true上面的片段将检查 env 头的值是否包含字符串test。如果我们设置了 env 头，并且它不包括字符串test，那么整个匹配的评估结果为真。\n查询参数匹配 使用 query_parameters 字段，我们可以指定路由应该匹配的 URL 查询的参数。过滤器将检查来自path头的查询字符串，并将其与所提供的参数进行比较。\n如果有一个以上的查询参数被指定，它们必须与规则相匹配，才能评估为真。\n请考虑以下例子。\n- match:prefix:\u0026#34;/\u0026#34;query_parameters:- name:envpresent_match:true如果有一个名为 env 的查询参数被设置，上面的片段将评估为真。它没有说任何关于该值的事情。它只是检查它是否存在。例如，使用上述匹配器，下面的请求将被评估为真。\nGET /hello?env=test 我们还可以使用字符串匹配器来检查查询参数的值。下表列出了字符串匹配的不同规则。\n   规则名称 描述     exact 必须与查询参数的精确值相匹配。   prefix 前缀必须符合查询参数值的开头。   suffix 后缀必须符合查询参数值的结尾。   safe_regex 查询参数值必须符合指定的正则表达式。   contains 检查查询参数值是否包含一个特定的字符串。    除了上述规则外，我们还可以使用 ignore_case 字段来指示精确、前缀或后缀匹配是否应该区分大小写。如果设置为 “true”，匹配就不区分大小写。\n下面是另一个使用前缀规则进行不区分大小写的查询参数匹配的例子。\n- match:prefix:\u0026#34;/\u0026#34;query_parameters:- name:envstring_match:prefix:\u0026#34;env_\u0026#34;ignore_case:true如果有一个名为 env 的查询参数，其值以 env_开头，则上述内容将评估为真。例如，env_staging 和 ENV_prod 评估为真。\ngRPC 和 TLS 匹配器 我们可以在路由上配置另外两个匹配器：gRPC 路由匹配器（grpc）和 TLS 上下文匹配器（tls_context）。\ngRPC 匹配器将只在 gRPC 请求上匹配。路由器检查内容类型头的 application/grpc 和其他 application/grpc+ 值，以确定该请求是否是 gRPC 请求。\n例如：\n- match:prefix:\u0026#34;/\u0026#34;grpc:{} 注意 gRPC 匹配器没有任何选项。\n 如果请求是 gRPC 请求，上面的片段将匹配路由。\n同样，如果指定了 TLS 匹配器，它将根据提供的选项来匹配 TLS 上下文。在 tls_context 字段中，我们可以定义两个布尔值——presented 和 validated。presented字段检查证书是否被出示。validated字段检查证书是否被验证。\n例如：\n- match:prefix:\u0026#34;/\u0026#34;tls_context:presented:truevalidated:true如果一个证书既被出示又被验证，上述匹配评估为真。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"dd919aeaca1f50991a338952f0a1a207","permalink":"https://lib.jimmysong.io/envoy-handbook/hcm/request-matching/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/envoy-handbook/hcm/request-matching/","section":"envoy-handbook","summary":"本节将为你介绍 HTTP 连接管理器中的请求匹配。 路径匹配 我们只谈了一个使用前缀字段匹配前缀的匹配规则。下面的表格解释了其他支持的匹配规则。 规则名称 描述 prefix 前缀必须与path标头的开头相匹配。例如，前缀 /api 将匹配路","tags":["Envoy"],"title":"请求匹配","type":"book"},{"authors":null,"categories":["云原生"],"content":"我们在前面看到了通过客户端库来治理服务的架构图，那是我们在改造成服务网格架构前使用微服务架构通常的形式，下图是使用服务网格架构的最终形式。\n   服务网格架构示意图  当然在达到这一最终形态之前我们需要将架构一步步演进，下面给出的是参考的演进路线。\nIngress 或边缘代理 如果你使用的是 Kubernetes 做容器编排调度，那么在进化到服务网格架构之前，通常会使用 Ingress Controller，做集群内外流量的反向代理，如使用 Traefik 或 Nginx Ingress Controller。\n   Ingress 或边缘代理架构示意图  这样只要利用 Kubernetes 的原有能力，当你的应用微服务化并容器化需要开放外部访问且只需要 L7 代理的话这种改造十分简单，但问题是无法管理服务间流量。\n路由器网格 Ingress 或者边缘代理可以处理进出集群的流量，为了应对集群内的服务间流量管理，我们可以在集群内加一个 Router 层，即路由器层，让集群内所有服务间的流量都通过该路由器。\n   路由器网格架构示意图  这个架构无需对原有的单体应用和新的微服务应用做什么改造，可以很轻易的迁移进来，但是当服务多了管理起来就很麻烦。\nProxy per Node 这种架构是在每个节点上都部署一个代理，如果使用 Kubernetes 来部署的话就是使用 DaemonSet 对象，Linkerd 第一代就是使用这种方式部署的，一代的 Linkerd 使用 Scala 开发，基于 JVM 比较消耗资源，二代的 Linkerd 使用 Go 开发。\n   Proxy per node 架构示意图  这种架构有个好处是每个节点只需要部署一个代理即可，比起在每个应用中都注入一个 sidecar 的方式更节省资源，而且更适合基于物理机 / 虚拟机的大型单体应用，但是也有一些副作用，比如粒度还是不够细，如果一个节点出问题，该节点上的所有服务就都会无法访问，对于服务来说不是完全透明的。\nSidecar 代理 / Fabric 模型 这个一般不会成为典型部署类型，当企业的服务网格架构演进到这一步时通常只会持续很短时间，然后就会增加控制平面。跟前几个阶段最大的不同就是，应用程序和代理被放在了同一个部署单元里，可以对应用程序的流量做更细粒度的控制。\n   Sidecar代理/Fabric模型示意图  这已经是最接近服务网格架构的一种形态了，唯一缺的就是控制平面了。所有的 sidecar 都支持热加载，配置的变更可以很容易的在流量控制中反应出来，但是如何操作这么多 sidecar 就需要一个统一的控制平面了。\nSidecar 代理 / 控制平面 下面的示意图是目前大多数服务网格的架构图，也可以说是整个服务网格架构演进的最终形态。\n   Sidecar 代理/控制平面架构示意图  这种架构将代理作为整个服务网格中的一部分，使用 Kubernetes 部署的话，可以通过以 sidecar 的形式注入，减轻了部署的负担，可以对每个服务的做细粒度权限与流量控制。但有一点不好就是为每个服务都注入一个代理会占用很多资源，因此要想方设法降低每个代理的资源消耗。\n多集群部署和扩展 以上都是单个服务网格集群的架构，所有的服务都位于同一个集群中，服务网格管理进出集群和集群内部的流量，当我们需要管理多个集群或者是引入外部的服务时就需要网格扩展和多集群配置。\n","date":1651507200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"3ac6a8fb7fff328f6970bc4915ae71a5","permalink":"https://lib.jimmysong.io/cloud-native-handbook/service-mesh/service-mesh-patterns/","publishdate":"2022-05-03T00:00:00+08:00","relpermalink":"/cloud-native-handbook/service-mesh/service-mesh-patterns/","section":"cloud-native-handbook","summary":"我们在前面看到了通过客户端库来治理服务的架构图，那是我们在改造成服务网格架构前使用微服务架构通常的形式，下图是使用服务网格架构的最终形式。 服务网格架构示意图 当然在达到这一最终形态之前我们需要将架构一步","tags":["云原生"],"title":"服务网格的部署模式","type":"book"},{"authors":null,"categories":["可观测性"],"content":"自动化开始听起来很神奇，但我们要面对现实：计算机分析不能每天告诉你系统有什么问题或为你修复它。它只能为你节省时间。\n至此，我想停止夸赞，我必须承认自动化的一些局限性。我这样做是因为围绕结合人工智能和可观测性会有相当多的炒作。这种炒作导致了惊人的论断，大意是：“人工智能异常检测和根源分析可以完全诊断问题！” 和 “人工智能操作将完全管理你的系统！”\n谨防炒作 为了摆脱此类炒作，我想明确的是，这类梦幻般的营销主张并不是我所宣称的现代可观测性将提供的。事实上，我预测许多与人工智能问题解决有关的主张大多是夸大其词。\n为什么人工智能不能解决我们的问题？一般来说，机器无法识别软件中的 “问题”，因为定义什么是 “问题” 需要一种主观的分析方式。而我们所讨论的那种现实世界的人工智能不能以任何程度的准确性进行主观决策。机器将始终缺乏足够的背景。\n例如，我们说该版本降低了性能，这里面也隐含了一个期望，就是这个版本包含了每个用户都期望的新功能。对于这个版本，性能退步是一个特征，而不是一个错误。\n虽然它们可能看起来像类似的活动，但在确定相关关系和确定根本原因之间存在着巨大的鸿沟。当你有正确的数据结构时，相关性是一种客观的分析 —— 你只需要计算数字。哪些相关关系是相关的，并表明真正问题的来源，总是需要主观的分析，对数据的解释。所有这些相关关系意味着什么？正如杰弗里・李波斯基（Jeffrey Lebowski）所说：“嗯，你知道，这只是你的观点，伙计。”\n神奇的 AIOps 在调查一个系统时，有两种类型的分析起作用：\n 客观的分析，基于事实的、可衡量的、可观测的。 主观分析，基于解释、观点和判断的。  这种二分法 —— 客观与主观，与可计算性理论中一个重要的问题有关，即 停机问题（halting problem）。停机问题的定义是，在给定任意计算机程序及其输入的描述的情况下，是否可以编写一个计算机程序来确定任意程序是否会结束运行或永远继续运行。简而言之，在 1936 年，艾伦・图灵（Alan Turning）证明了解决停机问题的一般算法是不存在的，这个证明的延伸可以应用于计算机软件中许多形式的识别 “问题”。\n阿兰・图灵的意思是，我们没有办法拥有神奇的 AIOps（IT 运维的人工智能）。寻找那些承诺将繁琐的客观分析自动化的工具 —— 计算数字是机器的强项！但要小心那些声称能找到问题根源并自动修复的工具。它们很可能会让你失望。\n这就是为什么我们可以确定相关关系，但不能确定因果关系：想象一下，有一台机器可以确定任意计算机程序中的问题行为是什么，并确定该行为的根本原因。如果我们真的造出了这样一台机器，那就是开香槟的时候了，因为这意味着我们终于解决了停机问题！但是，目前还没有迹象表明，机器可以解决这个问题。然而，没有迹象表明机器学习已经超越了艾伦・图灵的统一计算模型；你可以相信，这不会发生。\n做出正确的决定和修复的工作还是要靠你自己。\n时间是最宝贵的资源 然而，我们不需要神奇的 AIOps 来看到我们工作流程的巨大改善。识别相关性，同时获取相关信息，以便你能有效地浏览这些信息，这是计算机绝对可以做到的事情！这将为你节省时间。大量的时间。这么多的时间，它将从根本上改变你调查系统的方式。\n减少浪费的时间是实践现代可观测性的核心。即使是在简单的系统中，通过分析数字来识别相关性也是很困难的，而在大规模的系统中，这几乎是不可能的。通过将认知负担转移到机器上，运维人员能够有效地管理那些已经超出人类头脑所能容纳的系统。\n但是，我们分析遥测方式的这种转变并不是可观测性世界中即将发生的唯一重大变化。我们需要的大部分遥测数据来自于我们没有编写的软件：我们所依赖的开源库、数据库和管理服务。这些系统在传统上一直在为产生遥测数据而奋斗。我们可以获得哪些数据，以及这些数据来自哪里，也将发生根本性的变化。\n","date":1643990400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"72aa94d210f61545bd73808ac66f7ad5","permalink":"https://lib.jimmysong.io/opentelemetry-obervability/the-limitations-of-automated-analysis/","publishdate":"2022-02-05T00:00:00+08:00","relpermalink":"/opentelemetry-obervability/the-limitations-of-automated-analysis/","section":"opentelemetry-obervability","summary":"第 3 章：自动分析的局限性","tags":["OpenTelemetry"],"title":"第 3 章：自动分析的局限性","type":"book"},{"authors":null,"categories":["文化"],"content":"为什么尽快进行 Code Review？ 在Google，我们优化了开发团队共同开发产品的速度。，而不是优化单个开发者编写代码的速度。个人开发的速度很重要，它并不如整个团队的速度那么重要。\n当代码审查很慢时，会发生以下几件事：\n 整个团队的速度降低了。是的，对审查没有快速响应的个人的确完成了其他工作。但是，对于团队其他人来说重要的新功能与缺陷修復将会被延迟数天、数周甚至数月，只因为每个 CL 正在等待审查和重新审查。 开发者开始抗议代码审查流程。如果审查者每隔几天只响应一次，但每次都要求对 CL 进行重大更改，那么开发者可能会变得沮丧。通常，开发者将表达对审查者过于“严格”的抱怨。如果审查者请求相同实质性更改（确实可以改善代码健康状况），但每次开发者进行更新时都会快速响应，则抱怨会逐渐消失。大多数关于代码审查流程的投诉实际上是通过加快流程来解决的。 代码健康状况可能会受到影响。如果审查速度很慢，则造成开发者提交不尽如人意的 CL 的压力会越来越大。审查太慢还会阻止代码清理、重构以及对现有 CL 的进一步改进。  Code Review 应该有多快？ 如果您没有处于重点任务的中，那么您应该在收到代码审查后尽快开始。\n一个工作日。是应该响应代码审查请求所需的最长时间（即第二天早上的第一件事）。\n遵循这些指导意味着典型的 CL 应该在一天内进行多轮审查（如果需要）。\n速度 vs. 中断 有一种情况下个人速度胜过团队速度。如果您正处于重点任务中，例如编写代码，请不要打断自己进行代码审查。研究表明，开发人员在被打断后需要很长时间才能恢复到顺畅的开发流程中。因此，编写代码时打断自己实际上比让另一位开发人员等待代码审查的代价更加昂贵。\n相反，在回复审查请求之前，请等待工作中断点。可能是当你的当前编码任务完成，午餐后，从会议返回，从厨房回来等等。\n快速响应 当我们谈论代码审查的速度时，我们关注的是响应时间，而不是 CL 需要多长时间才能完成整个审查并提交。理想情况下，整个过程也应该是快速的，快速的个人响应比整个过程快速发生更为重要。\n即使有时需要很久才能完成整个审查流程，但在整个过程中获得审查者的快速响应可以显着减轻开发人员对“慢速”代码审查感到的挫败感。\n如果您太忙而无法对 CL 进行全面审查，您仍然可以发送快速回复，让开发人员知道您什么时候可以开始，或推荐其他能够更快回复的审查人员，或者提供一些大体的初步评论。 （注意：这并不意味着您应该中断编码，即使发送这样的响应，也要在工作中的合理断点处发出响应。）\n重要的是，审查人员要花足够的时间进行审查，确信他们的“LGTM”意味着“此代码符合我们的标准。”但是，理想情况下，个人反应仍然应该很快。\n跨时区审查 在处理时区差异时，尝试在他们还在办公室时回复作者。 如果他们已经下班回家了，那么请确保在第二天回到办公室之前完成审查。\n带评论的 LGTM 为了加快代码审查，在某些情况下，即使他们也在 CL 上留下未解决的评论，审查者也应该给予 LGTM/Approval，这可以是以下任何一种情况：\n 审查者确信开发人员将适当地处理所有审查者的剩余评论。 其余的更改很小，不必由开发者完成。  如果不清楚的话，审查者应该指定他们想要哪些选项。\n当开发者和审查者处于不同的时区时，带评论的 LGTM 尤其值得考虑，否则开发者将等待一整天才能获得 “LGTM，Approval”。\n大型 CL 如果有人向您发送了代码审查太大，您不确定何时有时间查看，那么您应该要求开发者将 CL 拆分为几个较小的 CL 而不是一次审查的一个巨大的 CL。这通常可行，对审查者非常有帮助，即使需要开发人员的额外工作。\n如果 CL 无法分解为较小的 CL，并且您没有时间快速查看整个内容，那么至少要对 CL 的整体设计写一些评论并将其发送回开发人员以进行改进。作为审查者，您的目标之一应该在不牺牲代码健康状况的前提下，始终减少开发者能够快速采取某种进一步的操作的阻力。\n代码审查随时间推移而改进 如果您遵循这些准则，并且您对代码审查非常严格，那么您应该会发现整个代码审核流程会随着时间的推移而变得越来越快。开发者可以了解健康代码所需的内容，并向您发送从一开始就很棒的 CL，且需要的审查时间越来越短。审查者学会快速响应，而不是在审查过程中添加不必要的延迟。但是，从长远来看，不要为了提高想象中的代码审查速度，而在代码审查标准或质量方面妥协，实际上这样做对于长期来说不会有任何帮助。\n紧急情况 还有一些紧急情况，CL 必须非常快速地通过整个审查流程，并且质量准则将放宽。请查看什么是紧急情况？ 中描述的哪些情况属于紧急情况，哪些情况不属于紧急情况。\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"fc0f0fdc9e39a211c52bfc8272fd66cd","permalink":"https://lib.jimmysong.io/eng-practices/review/reviewer/speed/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/eng-practices/review/reviewer/speed/","section":"eng-practices","summary":"为什么尽快进行 Code Review？ 在Google，我们优化了开发团队共同开发产品的速度。，而不是优化单个开发者编写代码的速度。个人开发的速度很重要，它并不如整个团队的速度那么重要。 当代码审查很慢时，会发生","tags":null,"title":"Code Review 速度","type":"book"},{"authors":null,"categories":["网络"],"content":"本节介绍 Kubernetes 的网络策略方面。\n命名空间 命名空间 用于在 Kubernetes 中创建虚拟集群。包括 NetworkPolicy 和 CiliumNetworkPolicy 在内的所有 Kubernetes 对象都属于一个特定的命名空间。根据定义和创建策略的方式，会自动考虑 Kubernetes 命名空间：\n 作为 CiliumNetworkPolicy CRD 和 NetworkPolicy 创建和导入的网络策略适用于命名空间内，即该策略仅适用于该命名空间内的 pod。但是，可以授予对其他命名空间中的 pod 的访问权限，如下所述。 通过 API 参考 直接导入的网络策略适用于所有命名空间，除非如下所述指定命名空间选择器。  提示\n  虽然有意支持通过 fromEndpoints 和 toEndpoints 中的 k8s:io.kubernetes.pod.namespace 标签指定命名空间 。禁止在 endpointSelector 中指定命名空间，因为这将违反 Kubernetes 的命名空间隔离原则。endpointSelector 总是适用于与 CiliumNetworkPolicy 资源本身相关的命名空间的 pod。   示例：强制命名空间边界 此示例演示如何为命名空间强制实施基于 Kubernetes 命名空间的边界，ns1 和 ns2 通过在任一命名空间的所有 pod 上启用默认拒绝，然后允许来自同一命名空间内的所有 pod 的通信。\napiVersion:\u0026#34;cilium.io/v2\u0026#34;kind:CiliumNetworkPolicymetadata:name:\u0026#34;isolate-ns1\u0026#34;namespace:ns1spec:endpointSelector:matchLabels:{}ingress:- fromEndpoints:- matchLabels:{}---apiVersion:\u0026#34;cilium.io/v2\u0026#34;kind:CiliumNetworkPolicymetadata:name:\u0026#34;isolate-ns1\u0026#34;namespace:ns2spec:endpointSelector:matchLabels:{}ingress:- fromEndpoints:- matchLabels:{}示例：跨命名空间公开 pod 以下示例将所有 ns1 命名空间所有具有 name=leia 标签的 pod 暴露给 ns2 命名空间具有 name=luke 标签的 pod。\n请参阅示例 YAML 文件 以获取完整的功能示例，包括部署到不同命名空间的 pod。\napiVersion:\u0026#34;cilium.io/v2\u0026#34;kind:CiliumNetworkPolicymetadata:name:\u0026#34;k8s-expose-across-namespace\u0026#34;namespace:ns1spec:endpointSelector:matchLabels:name:leiaingress:- fromEndpoints:- matchLabels:k8s:io.kubernetes.pod.namespace:ns2name:luke示例：允许 egress 到 kube-system 命名空间中的 kube-dns 以下示例允许 public 创建策略的命名空间中的所有 pod 与 kube-system 空间中 53/UDP 端口上的 kube-dns 通信。\napiVersion:\u0026#34;cilium.io/v2\u0026#34;kind:CiliumNetworkPolicymetadata:name:\u0026#34;allow-to-kubedns\u0026#34;namespace:publicspec:endpointSelector:{}egress:- toEndpoints:- matchLabels:k8s:io.kubernetes.pod.namespace:kube-systemk8s-app:kube-dnstoPorts:- ports:- port:\u0026#39;53\u0026#39;protocol:UDP服务账户 Kubernetes 服务账户用于将身份与 Kubernetes 管理的 pod 或进程相关联，并授予身份对 Kubernetes 资源和机密的访问权限。Cilium 支持基于 Pod 的服务账户身份来规范网络安全策略。\nPod 的服务账户可以通过服务账户准入控制器定义， 也可以直接在 Pod、Deployment、ReplicationController 资源中指定，如下所示：\napiVersion:v1kind:Podmetadata:name:my-podspec:serviceAccountName:leia...例子 以下示例授予在 “luke” 服务账户下运行的任何 pod 向与 “leia” 服务账户关联的所有运行的 pod 发出 TCP 80 端口 80 上的 HTTP GET /public 请求。\n请参阅示例 YAML 文件 以获取完整的功能示例，包括部署和服务账户资源。\napiVersion:\u0026#34;cilium.io/v2\u0026#34;kind:CiliumNetworkPolicymetadata:name:\u0026#34;k8s-svc-account\u0026#34;spec:endpointSelector:matchLabels:io.cilium.k8s.policy.serviceaccount:leiaingress:- fromEndpoints:- matchLabels:io.cilium.k8s.policy.serviceaccount:luketoPorts:- ports:- port:\u0026#39;80\u0026#39;protocol:TCPrules:http:- method:GETpath:\u0026#34;/public$\u0026#34;多集群 当使用集群网格操作多个集群时，集群名称通过标签公开 io.cilium.k8s.policy.cluster，可用于将策略限制到特定集群。\napiVersion:\u0026#34;cilium.io/v2\u0026#34;kind:CiliumNetworkPolicymetadata:name:\u0026#34;allow-cross-cluster\u0026#34;description:\u0026#34;Allow x-wing in cluster1 to contact rebel-base in cluster2\u0026#34;spec:endpointSelector:matchLabels:name:x-wingio.cilium.k8s.policy.cluster:cluster1egress:- toEndpoints:- matchLabels:name:rebel-baseio.kubernetes.pod.namespace:defaultio.cilium.k8s.policy.cluster:cluster2注意策略规则中的 io.kubernetes.pod.namespace: default。它确保策略适用于 cluster2 默认命名空间中的 rebel-base，而不考虑 x-wing 部署在 cluster1 中的命名空间。如果政策规则的命名空间标签被省略，它默认为策略本身应用的相同命名空间，这可能不是部署跨集群策略时想要的。\n集群范围的策略 CiliumNetworkPolicy 只允许绑定限制到特定命名空间的策略。在某些情况下，可以使用 Cilium 的 CiliumClusterwideNetworkPolicy Kubernetes 自定义资源来实现集群范围内的策略效果。该策略的规范与CiliumNetworkPolicy 的规范相同，只是它没有命名空间。\n在集群中，这个策略将允许从任何命名空间中匹配标签 name=luke 的 pod 到任何命名空间中匹配标签 name=leia 的 pod 的入站流量。\napiVersion:\u0026#34;cilium.io/v2\u0026#34;kind:CiliumClusterwideNetworkPolicymetadata:name:\u0026#34;clusterwide-policy-example\u0026#34;spec:description:\u0026#34;Policy for selective ingress allow to a pod from only a pod with given label\u0026#34;endpointSelector:matchLabels:name:leiaingress:- fromEndpoints:- matchLabels:name:luke示例：允许所有流量进入 kube-dns 以下示例允许集群中的所有 Cilium 托管端点与 kube-system 命名空间中 53/UDP 端口上的 kube-dns 通信。\napiVersion:\u0026#34;cilium.io/v2\u0026#34;kind:CiliumClusterwideNetworkPolicymetadata:name:\u0026#34;wildcard-from-endpoints\u0026#34;spec:description:\u0026#34;Policy for ingress allow to kube-dns from all Cilium managed endpoints in the cluster\u0026#34;endpointSelector:matchLabels:k8s:io.kubernetes.pod.namespace:kube-systemk8s-app:kube-dnsingress:- fromEndpoints:- {}toPorts:- ports:- port:\u0026#34;53\u0026#34;protocol:UDP示例：添加健康端点 以下示例将健康实体添加到所有 Cilium 托管端点，以检查集群连接健康状况。\napiVersion:\u0026#34;cilium.io/v2\u0026#34;kind:CiliumClusterwideNetworkPolicymetadata:name:\u0026#34;cilium-health-checks\u0026#34;spec:endpointSelector:matchLabels:\u0026#39;reserved:health\u0026#39;:\u0026#39;\u0026#39;ingress:- fromEntities:- remote-nodeegress:- toEntities:- remote-node 下一章   ","date":1655438400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"066e02717e8ca65f4c7565a860a12488","permalink":"https://lib.jimmysong.io/cilium-handbook/policy/kubernetes/","publishdate":"2022-06-17T12:00:00+08:00","relpermalink":"/cilium-handbook/policy/kubernetes/","section":"cilium-handbook","summary":"本节介绍 Kubernetes 的网络策略方面。 命名空间 命名空间 用于在 Kubernetes 中创建虚拟集群。包括 NetworkPolicy 和 CiliumNetworkPolicy 在内的所有 Kubernetes 对象都属于一个特定的命名空间。根据定义和创建策略的方式，会自动考虑 Kubernetes 命名空间： 作为 CiliumNetworkPolicy CRD 和 NetworkPolicy 创建和导入的网络策略适","tags":["Cilium"],"title":"Kubernetes 网络策略","type":"book"},{"authors":null,"categories":["网络"],"content":"如果你在 Kubernetes 上运行 Cilium，你可以从 Kubernetes 为你分发的策略中受益。在这种模式下，Kubernetes 负责在所有节点上分发策略，Cilium 会自动应用这些策略。三种格式可用于使用 Kubernetes 本地配置网络策略：\n 在撰写本文时，标准的 NetworkPolicy 资源支持指定三层/四层入口策略，并带有标记为 beta 的有限出口支持。 扩展的 CiliumNetworkPolicy 格式可用作 CustomResourceDefinition 支持入口和出口的三层到七层层策略规范。 CiliumClusterwideNetworkPolicy 格式，它是集群范围的CustomResourceDefinition，用于指定由 Cilium 强制执行的集群范围的策略。规范与 CiliumNetworkPolicy 相同，没有指定命名空间。  Cilium 支持同时运行多个策略类型。但是，在同时使用多种策略类型时应谨慎，因为理解跨多种策略类型的完整允许流量集可能会令人困惑。如果不注意，这可能会导致意外的策略允许行为。\n网络策略 有关详细信息，请参阅官方 NetworkPolicy 文档。\nKubernetes 网络策略的已知缺失功能：\n   特征 GItHub Issue     ipBlock使用 pod IP 设置 GitHub Issue 9209   SCTP GitHub Issue 5719    Cilium 网络策略 CiliumNetworkPolicy 与 标准 NetworkPolicy 非常相似。目的是提供 NetworkPolicy 尚不支持的功能 。理想情况下，所有功能都将合并到标准资源格式中，并且不再需要此 CRD。\nGo 中资源的原始规范如下所示：\ntype CiliumNetworkPolicy struct { // +deepequal-gen=false  metav1.TypeMeta `json:\u0026#34;,inline\u0026#34;` // +deepequal-gen=false  metav1.ObjectMeta `json:\u0026#34;metadata\u0026#34;` // Spec is the desired Cilium specific rule specification.  Spec *api.Rule `json:\u0026#34;spec,omitempty\u0026#34;` // Specs is a list of desired Cilium specific rule specification.  Specs api.Rules `json:\u0026#34;specs,omitempty\u0026#34;` // Status is the status of the Cilium policy rule  //  // +deepequal-gen=false  // +kubebuilder:validation:Optional  Status CiliumNetworkPolicyStatus `json:\u0026#34;status\u0026#34;` }   metadata\n描述策略。这包括：策略的名称，在命名空间中是唯一的注入策略的命名空间一组标签来识别 Kubernetes 中的资源。\n  spec\n包含规则基础的字段。\n  Specs\n包含Rule Basics列表的字段。如果必须自动删除或添加多个规则，则此字段很有用。\n  Status\n提供有关策略是否已成功应用的可视性。\n  例子 有关示例策略的详细列表，请参阅 三层示例、四层示例和 七层 示例。\nCiliumClusterwideNetworkPolicy 类似于 CiliumNetworkPolicy，除了\n CiliumClusterwideNetworkPolicy 定义的策略是非命名空间和集群范围的 它允许使用节点选择器。在内部，该策略与 CiliumNetworkPolicy 相同，因此该策略规范的效果也相同。  go 中资源的原始规范如下所示：\ntype CiliumClusterwideNetworkPolicy struct { // Spec is the desired Cilium specific rule specification.  Spec *api.Rule // Specs is a list of desired Cilium specific rule specification.  Specs api.Rules // Status is the status of the Cilium policy rule.  //  // The reason this field exists in this structure is due a bug in the k8s  // code-generator that doesn\u0026#39;t create a `UpdateStatus` method because the  // field does not exist in the structure.  //  // +kubebuilder:validation:Optional  Status CiliumNetworkPolicyStatus } ","date":1655438400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"67ecbf316dd3cd60423e956d3c06ad4b","permalink":"https://lib.jimmysong.io/cilium-handbook/kubernetes/policy/","publishdate":"2022-06-17T12:00:00+08:00","relpermalink":"/cilium-handbook/kubernetes/policy/","section":"cilium-handbook","summary":"如果你在 Kubernetes 上运行 Cilium，你可以从 Kubernetes 为你分发的策略中受益。在这种模式下，Kubernetes 负责在所有节点上分发策略，Cilium 会自动应用这些策略。三种格式可用于使用 Kubernetes 本地配置网络策略： 在撰写本","tags":["Cilium"],"title":"网络策略","type":"book"},{"authors":null,"categories":["Linux"],"content":"近年来，云原生应用已呈指数级增长。在本章中，我将讨论为什么 eBPF 如此适合于云原生环境。为了更具象化，我将提到 Kubernetes，但同样适用于任何容器平台。\n每台主机一个内核 要理解为什么 eBPF 在云原生世界中如此强大，你需要搞清楚一个概念：每台机器（或虚拟机）只有一个内核，所有运行在该机器上的容器都共享同一个内核 1 如 图 5-1 所示，内核了解主机上运行的所有应用代码。\n  图 5-1. 同一主机上的所有容器共享一个内核  通过对内核的检测，就像我们在使用 eBPF 时做的那样，我们可以同时检测在该机器上运行的所有应用程序代码。当我们将 eBPF 程序加载到内核并将其附加到事件上时，它就会被触发，而不考虑哪个进程与该事件有关。\neBPF 与 sidecar 模式的比较 在 eBPF 之前，Kubernetes 的可观测性和安全工具大多都采用了 sidecar 模式。这种模式允许你在与应用程序相同的 pod 中，单独部署一个工具容器。这种模式的发明是一个进步，因为这意味着不再需要直接在应用程序中编写工具代码。仅仅通过部署 sidecar，工具就获得了同一 pod 中的其他容器的可视性。注入 sidecar 的过程通常是自动化的，所以这提供了一种机制，以确保你的所有应用程序都被仪器化。\n每个 sidecar 容器都会消耗资源，而这要乘以注入了 sidecar 的 pod 的数量。这可能是非常重要的 —— 例如，如果每个 sidecar 需要它自己的路由信息副本，或策略规则，这就是浪费（关于这一点，Thomas Graf 写了一篇 关于服务网格 sidecar 与 eBPF 的比较）。\nSidecar 的另一个问题是，你不能保证机器上的每一个应用程序都被正确检测。设想下有一个攻击者设法破坏了你的一台主机，并启动了一个单独的 pod 来运行，比如，加密货币挖矿程序。他们不可能对你有礼貌，用你的 sidecar 可观测或安全工具来检测他们的挖矿 pod。你需要一个单独的系统来了解这种活动。\n但同样的加密货币矿工与运行在该主机上的合法 pod 共享内核。如果你使用基于 eBPF 的工具，如 图 5-2 所示，矿工会自动受到它的影响。\n  图 5-2. 旁观者只能观测到他们自己 pod 的活动，但 eBPF 程序可以观测到所有活动  eBPF 和进程隔离 我主张将功能整合到一个单一的、基于 eBPF 的代理中，而不是每个 pod 的 sidecar 中。如果该代理可以访问机器上运行的所有 pod，这不是一种安全风险吗？我们不是失去了应用程序之间的隔离，而这种隔离可以防止它们相互干扰吗？\n作为一个容器安全领域的过来人，我可以体会到你对此的担忧，但重要的是要挖掘底层机制，以真正理解为什么它不是一开始可能出现的缺陷。\n请注意，这些 pod 共享同一个内核，而内核原生不能感知 pod 或容器。相反，内核对进程进行操作，并使用 cgroup 和 namespace 来隔离进程。这些结构由内核监管，以隔离用户空间中的进程，防止它们互相干扰。只要数据在内核中处理（例如，从磁盘中读取或发送到网络中），你就依赖于内核的正确行为。只有内核代码控制文件权限。没有其他层面的东西可以阻止内核忽略文件权限的东西，内核可以从任何文件中读取数据 —— 只是内核本身不会这样做。\n存在于 Linux 系统中的安全控制措施假定内核本身是可以信任的。它们的存在是为了防止在用户空间运行的代码产生不良行为。\n我们在 第二章 中看到，eBPF 检查器确保 eBPF 程序只能访问它有权限的内存。检查器检查程序时不可能超出其职权范围，包括确保内存为当前进程所拥有或为当前网络包的一部分。这意味着 eBPF 代码比它周围的内核代码受到更严格的控制，内核代码不需要通过任何类型的检查器。\n如果攻击者逃脱了容器化的应用程序而到了节点上，而且还能够提升权限，那么该攻击者就可以危害到同一节点上的其他应用程序。由于这些逃逸是未知的，作为一个容器安全专家，我不建议在没有额外安全工具的情况下，在共享机器上与不受信任的应用程序或用户一起运行敏感的应用程序。对于高度敏感的数据，你甚至可能不希望在虚拟机中与不受信任的用户在同一裸机上运行。但是，如果你准备在同一台虚拟机上并行运行应用程序（这在许多不是特别敏感的应用程序中是完全合理的），那么 eBPF 就不会在共享内核已经存在的风险之上增加额外的风险。\n当然，恶意的 eBPF 程序可能造成各种破坏，当然也很容易写出劣迹的 eBPF 代码 —— 例如，复制每个网络数据包并将其发送给窃听者。默认情况下，非 root 用户没有加载 eBPF 程序 2 的权限， 只有当你真正信任他们时，你才应该授予用户或软件系统这种权限，就像 root 权限一样。因此，必须小心你所运行的代码的出处（有一个倡议正在进行中，以支持 eBPF 程序的签名检查来帮助解决这个问题）。你也可以使用 eBPF 程序来监视其他的 eBPF 程序！现在你已经对为什么 eBPF 是云原生工具的强大基础有了一个概念，下一章给你举一些来自云原生生态系统中的 eBPF 工具的具体例子。\n参考   这基本正确，除非你使用的是虚拟化技术，像 KataContainer、Firecracker 或 unikernels 这样的方法，每个 “容器” 在自己的虚拟机中运行。 ↩︎\n Linux CAP_BPF 授予加载 BPF 程序的权限。 ↩︎\n   ","date":1654142400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"54d79be6939ec16f29ca408b02e60b72","permalink":"https://lib.jimmysong.io/what-is-ebpf/ebpf-in-cloud-native-environments/","publishdate":"2022-06-02T12:00:00+08:00","relpermalink":"/what-is-ebpf/ebpf-in-cloud-native-environments/","section":"what-is-ebpf","summary":"近年来，云原生应用已呈指数级增长。在本章中，我将讨论为什么 eBPF 如此适合于云原生环境。为了更具象化，我将提到 Kubernetes，但同样适用于任何容器平台。 每台主机一个内核 要理解为什么 eBPF 在云原生世界中如此","tags":["eBPF"],"title":"第五章：云原生环境中的 eBPF","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"Etcd 是 Kubernetes 集群中的一个十分重要的组件，用于保存集群所有的网络配置和对象的状态信息。在后面具体的安装环境中，我们安装的 etcd 的版本是 v3.1.5，整个 Kubernetes 系统中一共有两个服务需要用到 etcd 用来协同和存储配置，分别是：\n 网络插件 flannel、对于其它网络插件也需要用到 etcd 存储网络的配置信息 Kubernetes 本身，包括各种对象的状态和元信息配置  注意：flannel 操作 etcd 使用的是 v2 的 API，而 Kubernetes 操作 etcd 使用的 v3 的 API，所以在下面我们执行 etcdctl 的时候需要设置 ETCDCTL_API 环境变量，该变量默认值为 2。\n原理 Etcd 使用的是 raft 一致性算法来实现的，是一款分布式的一致性 KV 存储，主要用于共享配置和服务发现。关于 raft 一致性算法请参考 该动画演示。\n关于 Etcd 的原理解析请参考 Etcd 架构与实现解析。\n使用 Etcd 存储 Flannel 网络信息 我们在安装 Flannel 的时候配置了 FLANNEL_ETCD_PREFIX=\u0026#34;/kube-centos/network\u0026#34; 参数，这是 Flannel 查询 etcd 的目录地址。\n查看 Etcd 中存储的 flannel 网络信息：\n$ etcdctl --ca-file=/etc/kubernetes/ssl/ca.pem --cert-file=/etc/kubernetes/ssl/kubernetes.pem --key-file=/etc/kubernetes/ssl/kubernetes-key.pem ls /kube-centos/network -r 2018-01-19 18:38:22.768145 I | warning: ignoring ServerName for user-provided CA for backwards compatibility is deprecated /kube-centos/network/config /kube-centos/network/subnets /kube-centos/network/subnets/172.30.31.0-24 /kube-centos/network/subnets/172.30.20.0-24 /kube-centos/network/subnets/172.30.23.0-24 ```查看 flannel 的配置：```bash $ etcdctl --ca-file=/etc/kubernetes/ssl/ca.pem --cert-file=/etc/kubernetes/ssl/kubernetes.pem --key-file=/etc/kubernetes/ssl/kubernetes-key.pem get /kube-centos/network/config 2018-01-19 18:38:22.768145 I | warning: ignoring ServerName for user-provided CA for backwards compatibility is deprecated {\u0026#34;Network\u0026#34;: \u0026#34;172.30.0.0/16\u0026#34;, \u0026#34;SubnetLen\u0026#34;: 24, \u0026#34;Backend\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;host-gw\u0026#34;} } 使用 Etcd 存储 Kubernetes 对象信息 Kubernetes 使用 etcd v3 的 API 操作 etcd 中的数据。所有的资源对象都保存在 /registry 路径下，如下：\nThirdPartyResourceData apiextensions.k8s.io apiregistration.k8s.io certificatesigningrequests clusterrolebindings clusterroles configmaps controllerrevisions controllers daemonsets deployments events horizontalpodautoscalers ingress limitranges minions monitoring.coreos.com namespaces persistentvolumeclaims persistentvolumes poddisruptionbudgets pods ranges replicasets resourcequotas rolebindings roles secrets serviceaccounts services statefulsets storageclasses thirdpartyresources 如果你还创建了 CRD（自定义资源定义），则在此会出现 CRD 的 API。\n查看集群中所有的 Pod 信息 例如我们直接从 etcd 中查看 kubernetes 集群中所有的 pod 的信息，可以使用下面的命令：\nETCDCTL_API=3 etcdctl get /registry/pods --prefix -w json|python -m json.tool 此时将看到 json 格式输出的结果，其中的key使用了base64 编码，关于 etcdctl 命令的详细用法请参考 使用 etcdctl 访问 kubernetes 数据。\nEtcd V2 与 V3 版本 API 的区别 Etcd V2 和 V3 之间的数据结构完全不同，互不兼容，也就是说使用 V2 版本的 API 创建的数据只能使用 V2 的 API 访问，V3 的版本的 API 创建的数据只能使用 V3 的 API 访问。这就造成我们访问 etcd 中保存的 flannel 的数据需要使用 etcdctl 的 V2 版本的客户端，而访问 kubernetes 的数据需要设置 ETCDCTL_API=3 环境变量来指定 V3 版本的 API。\nEtcd 数据备份 我们安装的时候指定的 Etcd 数据的存储路径是 /var/lib/etcd，一定要对该目录做好备份。\n参考  etcd 官方文档 - etcd.io etcd v3 命令和 API - blog.csdn.net Etcd 架构与实现解析 - jolestar.com  ","date":1653062400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"b8ac2e0b993abda49231600ef0dec5fd","permalink":"https://lib.jimmysong.io/kubernetes-handbook/architecture/etcd/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/architecture/etcd/","section":"kubernetes-handbook","summary":"Etcd 是 Kubernetes 集群中的一个十分重要的组件，用于保存集群所有的网络配置和对象的状态信息。在后面具体的安装环境中，我们安装的 etcd 的版本是 v3.1.5，整个 Kubernetes 系统中一共有两个服务需要用到 etcd 用来协同和存储配置，分别是：","tags":["Kubernetes"],"title":"Etcd 解析","type":"book"},{"authors":null,"categories":["安全"],"content":"在联邦政府的各个机构中，有几个 DevSecOps 倡议，其重点和焦点各不相同，这取决于软件和任务需求所带来的流程。尽管并不详尽，但以下是对 这些倡议 的简要概述：\n DevSecOps 管道参与构建、签入和签出一个名为 Iron Bank 的容器镜像仓库，这是一个经过国防部审查的强化容器镜像库。 空军的 Platform One，也就是实现了连续操作授权（C-ATO）概念的 DevSecOps 平台，这又简化了国防部的授权程序，以适应现代连续软件部署的速度和频率。 国家地理空间情报局（NGA）在 \u0026#34;NGA 软件之路\u0026#34; 中概述了其 DevSecOps 战略，其中为其每个软件产品规定了三个关键指标：可用性、准备时间和部署频率，以及用于实现 DevSecOps 管道的七个不同产品系列的规格，包括消息传递和工作流工具。 医疗保险和医疗补助服务中心（CMS）正在采用一种 DevSecOps 方法，其中一个重点是为软件材料清单（SBOM）奠定基 —— 这是一种正式记录，包含用于构建软件的各种组件的细节和供应链关系。制作 SBOM 的目的是为了实现持续诊断和缓解（CDM）计划下的目标。 在海军水面作战中心（NSWC），DevSecOps 原语的实施方法被用来教导和培训软件工作人员，让他们了解各种软件指标以及自动化作为实现这些指标的助推器的作用。 陆军的 DevSecOps 倡议被称为 “陆军软件工厂”，重点是建立技能组合而不是建立软件。它利用 DevSecOps 能力（管道和平台即服务功能）作为技术加速器，在产品管理、用户体验、用户界面（UI/UX）设计、平台和软件工程方面提高效率和熟练度。  ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"7a8635d047a7e7c59bf1254b34815698","permalink":"https://lib.jimmysong.io/service-mesh-devsecops/intro/related-devsecops-initiatives/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/service-mesh-devsecops/intro/related-devsecops-initiatives/","section":"service-mesh-devsecops","summary":"在联邦政府的各个机构中，有几个 DevSecOps 倡议，其重点和焦点各不相同，这取决于软件和任务需求所带来的流程。尽管并不详尽，但以下是对 这些倡议 的简要概述： DevSecOps 管道参与构建、签入和签出一个名为 Iron Bank 的容器镜像仓库，这是一","tags":["Service Mesh","DevSecOps"],"title":"1.2 相关的 DevSecOps 倡议","type":"book"},{"authors":null,"categories":["安全"],"content":"可观测性即代码在应用程序的每个服务组件中部署一个监控代理，以收集三种类型的数据（在第 4.1 节中描述），将它们发送到专门的工具，将它们关联起来，进行分析，并在仪表板上显示分析后的综合数据，以呈现整个应用程序级别的情况。这种综合数据的一个例子是日志模式，它提供了一个日志数据的视图，该视图是在使用一些标准（例如，一个服务或一个事件）对日志数据进行过滤后呈现的。数据根据共同的模式（例如，基于时间戳或 IP 地址范围）被分组，以方便解释。不寻常的发生被识别出来，然后这些发现可以被用来指导和加速 进一步的调查。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"fb7bb7df2369f0681aed659e2ec83b0f","permalink":"https://lib.jimmysong.io/service-mesh-devsecops/implement/ci-cd-pipeline-for-observability-as-code/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/service-mesh-devsecops/implement/ci-cd-pipeline-for-observability-as-code/","section":"service-mesh-devsecops","summary":"可观测性即代码在应用程序的每个服务组件中部署一个监控代理，以收集三种类型的数据（在第 4.1 节中描述），将它们发送到专门的工具，将它们关联起来，进行分析，并在仪表板上显示分析后的综合数据，以呈现整个应用程序","tags":["Service Mesh","DevSecOps"],"title":"4.5 可观测性即代码的 CI/CD 管道","type":"book"},{"authors":null,"categories":["云原生"],"content":"我们在前一章中讨论了在采用云原生基础架构的前提。在部署之前，需要有由 API 驱动的基础架构（IaaS）供给。\n在本章中，我们将探讨云原生基础架构拓扑的概念，并在云中实现它们。我们将学习可以帮助运维人员控制其基础架构的常用工具和模式。\n部署基础架构的第一步应该是能够将其表述出来。传统上，可以在白板上处理，或者如果幸运的话，可以在公司 wiki 上存储的文档中处理。今天，一切都变得更加程序化，基础架构表述通常以便于应用程序解释的方式记录。无论如何表述，全面的表述基础架构的需求是不变的。\n正如人们所期望的那样，精巧的云基础架构可以从简单的设计到非常复杂的设计。无论复杂性如何，必须对基础架构的表现给予高度的重视，以确保设计的可重复性。能够清晰地传递想法更为重要。因此，明确、准确和易于理解的基础架构级资源表述势在必行。\n我们也将从制作精良的表述中获得很多好处：\n 随着时间的推移，基础架构设计可以共享和版本化。 基础架构设计可以被 fork 和修改以适应特殊情况。 表述隐含的是文档。  随着本章向前推进，我们将看到基础架构表述是如何成为基础架构部署的第一步。我们将以不同的方式探索表述基础架构的能力和潜在缺陷。\n表述基础架构 首先，我们需要理解表述基础架构的两个角色：作者和观众。\n作者将定义基础架构，通常是人类运维人员或管理员。观众将负责解释基础架构表述。有时候，这是一个运维人员执行手动步骤，但希望它是一个可以自动分析和创建基础架构的部署工具。作者在准确表达基础架构方面表现得越好，我们就可以在听众解释表达的能力中获得更多的信心。\n创作基础架构表述时主要关心的是要让观众了解它。如果目标受众是人，则表述可能以技术图或抽象代码的形式出现。如果目标受众是一个程序，那么表示可能需要更详细的信息和具体的实施步骤。\n尽管有观众，作者应该让观众更容易使用。随着复杂性的增加以及人与程序共同使用基础架构，这将变得非常困难。\n表示法需要易于理解，以便能够对其进行准确分析。易于阅读但分析不准确的表述否定了整个工作。观众应该总是努力去解释他们的表述，而不是做出假设。\n为了使表达成功，解释需要可预测。如果作者忽略了一个重要的细节，那么最好的观众就会很快失败。具有可预测性将在应用变更时减少错误的发生，并有助于在作者和受众之间建立信任。\n基础架构即图 我们用到了白板，开始绘制一张基础架构图。通常情况下，这个过程始于在角落上代表互联网的云形状，以及一些指向方框的箭头。每个框代表系统中的一个组件，箭头表示它们之间的交互。图 3-1 是基础架构图的一个例子。\n   图 3-1. 简单的基础架构图  这是一个非常有效的头脑风暴和将想法传达给其他人的方法。它允许对复杂的基础架构设计进行快速而强大的表示。\n图片适用于人类，大量人群和 CEO。这些图也适用，因为它们使用常用语来表示关系。例如，此框可能会将数据发送到那个框，但不会将数据发送到其他框。\n不幸的是，图表对于计算机来说几乎是不可能理解的。在计算机视觉迎头赶上之前，基础架构图仍然是一个代表，可以用眼球来解释，而不是代码。\n 从图中进行部署\n在例 3-1 中，我们看一个来自 bash_history 文件的熟悉的代码片段。它代表一个基础架构运营商，作为描述基础服务器与网络、存储和转租服务运行的图表的受众。\n运维人员已经手动部署了一台新的虚拟机，并通过 SSH 连接到了该机器并开始配置它。在这种情况下，人类充当图解释者，然后在基础架构环境中采取行动。\n大多数基础架构工程师在他们的职业生涯中都这样做了，而且这些步骤对于某些系统管理员来说应该是非常熟悉的。\n 例 3-1. bash_history\nsudo emacs /etc/networking/interfaces sudo ifdown eth0 sudo ifup eth0 sudo fdisk -l sudo emacs /etc/fstab sudo mount -a sudo systemctl enable kubelet 基础架构即脚本 如果您是一个系统管理员，您工作的一部分是在复杂系统中进行更改；确保这些更改是正确的也是您的责任。需要将这些变化传播到广阔的系统中是非常现实的。不幸的是，人为错误也是如此。管理员为这项工作编写便利脚本并不奇怪。\n脚本可以帮助减少重复任务中人为错误的数量，但自动化是一把双刃剑。这并不意味着准确性或成功。\n 对于 SRE，自动化可以让你力量倍增，单它不是万能药。当然，加倍的力量并不会自然地改变应用力的准确性：不经意地进行自动化可能会产生很多的问题。\n——Niall Murphy、John Looney 和 Kacirek，自动化在谷歌的演变\n 编写脚本是自动执行步骤以产生所需结果的好方法。该脚本可以执行各种任务，例如安装 HTTP 服务器，配置并运行它。但是，脚本中的步骤在调用时很少考虑到它们的结果或系统的状态。\n在这种情况下，脚本是编码数据，表示创建所需基础架构应该发生的情况。另一位运维人员或管理员可以评估您的脚本，并希望了解脚本正在做什么。换句话说，他们会解释你的基础架构表示。了解所需的基础架构需要了解步骤如何影响系统。\n脚本的运行时会按照它们定义的顺序执行这些步骤，但运行时不知道它正在生成什么。脚本是代码，脚本的执行结果希望是所需的基础架构。\n这适用于普遍的场景，但这种方法存在一些缺陷。最明显的缺陷是运行相同的脚本可能获得两个不同的结果。\n如果脚本第一次运行的环境与第二次运行的环境大不相同？从科学的角度来说，这将类似于程序中的缺陷，并会使实验数据无效。\n使用脚本来表示基础架构的另一个缺陷是缺少声明状态。脚本的运行时不理解结束状态，因为它只提供了执行步骤。人类需要从步骤中解释理想的结果，以了解如何进行改变。\n我们看到过很多人类难以理解的代码。随着配置脚本复杂性的增长，我们解释脚本的能力就会减弱。此外，您的基础架构页需要随时间而变化，脚本将不可避免地需要更改。\n如果不将步骤抽象为声明性状态，为了给每个可能的初始状态创建过程，脚本将不断增长。这包括抽象出操作系统（例如 apt 和 DNF）之间的步骤和差异，以及验证可以安全地跳过哪些步骤。\n基础架构即代码带来了一些工具，这些工具提供了一些抽象，以帮助减轻使用脚本管理基础架构的负担。\n 从脚本部署\n创建基础架构的下一个发展是开始采用先前手动管理基础架构的流程，并通过将工作封装在脚本中来简化它。想象一下，我们有一个名为 createVm.sh 的 bash 脚本，它将在我们的本地工作站中创建一台虚拟机。\n该脚本需要两个参数。第一个是分配给虚拟机上的网络接口的静态 IP 地址。第二个是以千兆字节为单位的大小，用于创建卷并将其挂载到虚拟机。\n示例 3-2 将基础架构的基本表示形式显示为脚本。该脚本将提供新的基础架构，并在新创建的基础架构上运行任意配置脚本。该脚本可能演变为高度可定制的，并且可能是（危险地）自动化的，只需点击一下按钮即可运行。\n 例 3-2. 基础架构即脚本\n#!/bin/bash  # Create a VM with a NIC on 10.0.0.17 and a 100gb volume createVm.sh 10.0.0.17 100 # Transfer the bootstrapping script scp ~/vm_provision.sh user@10.0.0.17:vm_provision.sh -v # Run the bootstrap script ssh user@10.0.0.17 sh ~/vm_provision.sh 基础架构即代码 配置管理曾经是代表基础架构的主要角色。我们可以将配置管理视为抽象脚本，自动考虑初始状态以执行正确的过程。最重要的是，配置管理允许作者声明节点的期望状态，而不是实现它所需的每一步。\n配置管理是基础架构即代码的第一步，但相关工具很少超出单个服务器的范围。配置管理工具在定义特定资源和他们的状态方面做得非常出色，但由于基础架构需要资源之间的协调，所以出现了复杂性。\n例如，服务的 DNS 条目在提供服务之前不可用。在主机可用之前不应该提供该服务。如果不能在独立节点之间协调多个资源，则配置管理提供的抽象化是不足的。有些工具增加了协调资源之间配置的能力，但协调通常是程序性的，责任落到了人们的协调资源和理解所需状态上。\n您的基础架构不包含没有通信的独立实体。代表基础架构的工具需要考虑到这一点。因此，需要另一种表示来管理低级别抽象（例如操作系统）以及供应和协调。\n2014 年 7 月，有个开源工具在代码发布的时候采用了更高级别的基础架构抽象概念。这个名为 Terraform 的工具非常成功。它在配置管理完善并且公有云的采用呈上升趋势的时间节点发布。用户看到了新环境中工具的局限性，Terraform 很好的满足了他们的需求。\n 在 2011 年时，我们最初将基础架构视代码。我们注意到我们正在编写工具来解决许多项目的基础架构问题，并希望将流程标准化。\n——Hashicorp 首席执行官兼 Terraform 创始人 Mitchell Hashimoto\n Terraform 使用专门的领域特定语言（DSL）表示基础架构，它在人类可理解的图像和机器可分析的代码之间做了良好的折衷。Terraform 最成功的部分是抽象的基础架构视图，资源协调以及应用时利用现有工具的能力。Terraform 与云 API 进行通信以配置基础架构，并可在必要时使用配置管理来配置节点。\n这是该行业的根本性转变，因为我们看到一次性配置脚本正在消失。越来越多的运营商开始在新的 DSL 中开发基础架构表示。过去在基础架构上手动操作的工程师现在正在开发代码。\n新的 DSL 解决了将基础架构表示为脚本的问题，并成为表示基础架构的标准。工程师发现他们正在开发更好的基础架构代码，并允许 Terraform 对其进行解释。与配置管理代码一样，工程师们开始将他们的基础架构表述存储在版本控制系统中，并将基础架构与软件等同看待。\n通过表述基础架构的标准化方式，我们摆脱了学习各种专有云 API 的痛苦。尽管并非所有云资源都可以用单一表示抽象出来，但大多数用户可以接受其代码中的云锁定。拥有人类可读并且机器可解析的基础架构表示，而不仅仅是独立的资源声明，这一点永远得改变了行业。\n 从代码部署\n在面临将基础架构部署为脚本的挑战之后，我们已经创建了一个程序来解析输入并针对我们的基础架构采取行动。\n例 3-3 显示了从 Terraform 开源库中获取的 Terraform 配置。注意代码中有变量，需要在运行时解析。\n基础架构的声明性表示很重要，因为它没有定义创建基础架构的各个步骤。这使我们能够分离需要调配的部分和调配的部分。这就是使这种基础架构代表成为新范例的原因；这也是向软件基础架构演进的第一步。\n以这种方式来表示基础架构对于工程师来说是一种常见的强大做法。用户可以使用 Terraform 来应用基础架构。\n 例 3-3. example.tf\n# Create our DNSimple record resource \u0026#34;dnsimple_record\u0026#34; \u0026#34;web\u0026#34; {domain = \u0026#34;${var.dnsimple_domain}\u0026#34; name = \u0026#34;terraform\u0026#34; value = \u0026#34;${hostname}\u0026#34; type = \u0026#34;CNAME\u0026#34; ttl = 3600 } 基础架构即软件 基础架构即代码是朝着正确方向发展的强大举措。但是代码是基础架构的静态表示，并且有其局限性。您可以自动执行部署代码更改的过程，但除非部署工具持续运行，否则仍会出现配置漂移。传统上，部署工具只能在一个方向上工作：它只能创建新对象，并且不能轻易删除或修改现有对象。\n为了掌握基础架构，我们的部署工具需要根据基础架构的初始表示进行工作，并对数据进行变更以创建更灵活的系统。当我们开始将基础架构表示视为一个可持续执行所需状态的可版本化数据体时，下一步就是将基础架构视为软件。\nTerraform 从配置 …","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"7ae097c9964f2b63ffccb9a61920118b","permalink":"https://lib.jimmysong.io/cloud-native-infra/evolution-of-cloud-native-developments/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/cloud-native-infra/evolution-of-cloud-native-developments/","section":"cloud-native-infra","summary":"我们在前一章中讨论了在采用云原生基础架构的前提。在部署之前，需要有由 API 驱动的基础架构（IaaS）供给。 在本章中，我们将探讨云原生基础架构拓扑的概念，并在云中实现它们。我们将学习可以帮助运维人员控制其基","tags":["云原生"],"title":"第 3 章：云原生部署的演变","type":"book"},{"authors":null,"categories":["安全"],"content":"Kubernetes，经常被缩写为 “K8s”，是一个开源的容器或编排系统，用于自动部署、扩展和管理容器化应用程序。它管理着构成集群的所有元素，从应用中的每个微服务到整个集群。与单体软件平台相比，将容器化应用作为微服务使用可以提供更多的灵活性和安全优势，但也可能引入其他复杂因素。\n   图 1：Kubernetes 集群组件的高层视图  本指南重点关注安全挑战，并尽可能提出适用于国家安全系统和关键基础设施管理员的加固策略。尽管本指南是针对国家安全系统和关键基础设施组织的，但也鼓励联邦和州、地方、部落和领土（SLTT）政府网络的管理员实施所提供的建议。Kubernetes 集群的安全问题可能很复杂，而且经常在利用其错误配置的潜在威胁中被滥用。以下指南提供了具体的安全配置，可以帮助建立更安全的 Kubernetes 集群。\n建议 每个部分的主要建议摘要如下：\n Kubernetes Pod 安全  使用构建的容器，以非 root 用户身份运行应用程序 在可能的情况下，用不可变的文件系统运行容器 扫描容器镜像，以发现可能存在的漏洞或错误配置 使用 Pod 安全政策来执行最低水平的安全，包括:  防止有特权的容器 拒绝经常被利用来突破的容器功能，如 hostPID、hostIPC、hostNetwork、allowedHostPath 等 拒绝以 root 用户身份执行或允许提升为根用户的容器 使用安全服务，如 SELinux®、AppArmor® 和 seccomp，加固应用程序，防止被利用。     网络隔离和加固  使用防火墙和基于角色的访问控制（RBAC）锁定对控制平面节点的访问 进一步限制对 Kubernetes etcd 服务器的访问 配置控制平面组件，使用传输层安全（TLS）证书进行认证、加密通信 设置网络策略来隔离资源。不同命名空间的 Pod 和服务仍然可以相互通信，除非执行额外的隔离，如网络策略 将所有凭证和敏感信息放在 Kubernetes Secret 中，而不是配置文件中。使用强大的加密方法对 Secret 进行加密   认证和授权  禁用匿名登录（默认启用） 使用强大的用户认证 创建 RBAC 策略以限制管理员、用户和服务账户活动   日志审计  启用审计记录（默认为禁用） 在节点、Pod 或容器级故障的情况下，持续保存日志以确保可用性 配置一个 metric logger   升级和应用安全实践  立即应用安全补丁和更新 定期进行漏洞扫描和渗透测试 当组件不再需要时，将其从环境中移除    架构概述 Kubernetes 使用集群架构。一个 Kubernetes 集群是由一些控制平面和一个或多个物理或虚拟机组成的，称为工作节点。工作者节点承载 Pod，其中包含一个或多个容器。容器是包含软件包及其所有依赖关系的可执行镜像。见图 2：Kubernetes 架构。\n   控制平面对集群进行决策。这包括调度容器的运行，检测 / 应对故障，并在部署文件中指定的副本数量没有得到满足时启动新的 Pod。以下逻辑组件都是控制平面的一部分：\n Controller manager（默认端口：10252） - 监视 Kubernetes 集群，以检测和维护 Kubernetes 环境的几个方面，包括将 Pod 加入到服务中，保持一组 Pod 的正确数量，并对节点的丢失做出反应。 Cloud controller manager（默认端口：10258） - 一个用于基于云的部署的可选组件。云控制器与云服务提供商接口，以管理集群的负载均衡器和虚拟网络。 Kubernetes API Server（默认端口：6443 或 8080） - 管理员操作 Kubernetes 的接口。因此，API 服务器通常暴露在控制平面之外。API 服务器被设计成可扩展的，可能存在于多个控制平面节点上。 Etcd（默认端口范围：2379-2380） - 持久化的备份存储，关于集群状态的所有信息都保存在这里。Etcd 不应该被直接操作，而应该通过 API 服务器来管理。 Scheduler（默认端口：10251） - 跟踪工作节点的状态并决定在哪里运行 Pod。Kube-scheduler 只可以由控制平面内的节点访问。  Kubernetes 工作节点是专门为集群运行容器化应用的物理或虚拟机。除了运行容器引擎外，工作节点还承载以下两个服务，允许从控制平面进行协调：\n Kubelet（默认端口：10250） - 在每个工作节点上运行，以协调和验证 Pod 的执行。 Kube-proxy - 一个网络代理，使用主机的数据包过滤能力，确保 Kubernetes 集群中数据包的正确路由。  集群通常使用云服务提供商（CSP）的 Kubernetes 服务或在企业内部托管。在设计 Kubernetes 环境时，组织应了解他们在安全维护集群方面的责任。CSP 管理大部分的 Kubernetes 服务，但组织可能需要处理某些方面，如认证和授权。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"ed1dff92944751022cd7da7ab5ac5119","permalink":"https://lib.jimmysong.io/kubernetes-hardening-guidance/introduction/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/kubernetes-hardening-guidance/introduction/","section":"kubernetes-hardening-guidance","summary":"Kubernetes，经常被缩写为 “K8s”，是一个开源的容器或编排系统，用于自动部署、扩展和管理容器化应用程序。它管理着构成集群的所有元素，从应用中的每个微服务到整个集群。与","tags":["Kubernetes","安全"],"title":"简介","type":"book"},{"authors":null,"categories":["Envoy"],"content":"Envoy 支持在同一虚拟主机内将流量分割到不同的路由。我们可以在两个或多个上游集群之间分割流量。\n有两种不同的方法。第一种是使用运行时对象中指定的百分比，第二种是使用加权集群。\n使用运行时的百分比进行流量分割 使用运行时对象的百分比很适合于金丝雀发布或渐进式交付的场景。在这种情况下，我们想把流量从一个上游集群逐渐转移到另一个。\n实现这一目标的方法是提供一个 runtime_fraction 配置。让我们用一个例子来解释使用运行时百分比的流量分割是如何进行的。\nroute_config:virtual_hosts:- name:hello_vhostdomains:[\u0026#34;hello.io\u0026#34;]routes:- match:prefix:\u0026#34;/\u0026#34;runtime_fraction:default_value:numerator:90denominator:HUNDREDroute:cluster:hello_v1- match:prefix:\u0026#34;/\u0026#34;route:cluster:hello_v2上述配置声明了两个版本的 hello 服务：hello_v1 和 hello_v2。\n在第一个匹配中，我们通过指定分子（90）和分母（HUNDRED）来配置 runtime_fraction 字段。Envoy 使用分子和分母来计算最终的分数值。在这种情况下，最终值是 90%（90/100 = 0.9 = 90%）。\nEnvoy 在 [0，分母] 范围内生成一个随机数（例如，在我们的案例中是 [0，100]）。如果随机数小于分子值，路由器就会匹配该路由，并将流量发送到我们案例中的集群 hello_v1。\n如果随机数大于分子值，Envoy 继续评估其余的匹配条件。由于我们有第二条路由的精确前缀匹配，所以它是匹配的，Envoy 会将流量发送到集群 hello_v2。一旦我们把分子值设为 0，所有随机数会大于分子值。因此，所有流量都会流向第二条路由。\n我们也可以在运行时键中设置分子值。例如：\nroute_config:virtual_hosts:- name:hello_vhostdomains:[\u0026#34;hello.io\u0026#34;]routes:- match:prefix:\u0026#34;/\u0026#34;runtime_fraction:default_value:numerator:0denominator:HUNDREDruntime_key:routing.hello_ioroute:cluster:hello_v1- match:prefix:\u0026#34;/\u0026#34;route:cluster:hello_v2...layered_runtime:layers:- name:static_layerstatic_layer:routing.hello_io:90在这个例子中，我们指定了一个名为 routing.hello_io 的运行时键。我们可以在配置中的分层运行时字段下设置该键的值——这也可以从文件或通过运行时发现服务（RTDS）动态读取和更新。为了简单起见，我们在配置文件中直接设置。\n当 Envoy 这次进行匹配时，它将看到提供了runtime_key，并将使用该值而不是分子值。有了运行时键，我们就不必在配置中硬编码这个值了，我们可以让 Envoy 从一个单独的文件或 RTDS 中读取它。\n当你有两个集群时，使用运行时百分比的方法效果很好。但是，当你想把流量分到两个以上的集群，或者你正在运行 A/B 测试或多变量测试方案时，它就会变得复杂。\n使用加权集群进行流量分割 当你在两个或多个版本的服务之间分割流量时，加权集群的方法是理想的。在这种方法中，我们为多个上游集群分配了不同的权重。而带运行时百分比的方法使用了许多路由，我们只需要为加权集群提供一条路由。\n我们将在下一个模块中进一步讨论上游集群。为了解释用加权集群进行的流量分割，我们可以把上游集群看成是流量可以被发送到的终端的集合。\n我们在路由内指定多个加权集群（weighted_clusters），而不是设置一个集群（cluster）。\n继续前面的例子，我们可以这样重写配置，以代替使用加权集群。\nroute_config:virtual_hosts:- name:hello_vhostdomains:[\u0026#34;hello.io\u0026#34;]routes:- match:prefix:\u0026#34;/\u0026#34;route:weighted_clusters:clusters:- name:hello_v1weight:90- name:hello_v2weight:10在加权的集群下，我们也可以设置 runtime_key_prefix，它将从运行时密钥配置中读取权重。注意，如果运行时密钥配置不在那里，Envoy 会使用每个集群旁边的权重。\nroute_config:virtual_hosts:- name:hello_vhostdomains:[\u0026#34;hello.io\u0026#34;]routes:- match:prefix:\u0026#34;/\u0026#34;route:weighted_clusters:runtime_key_prefix:routing.hello_ioclusters:- name:hello_v1weight:90- name:hello_v2weight:10...layered_runtime:layers:- name:static_layerstatic_layer:routing.hello_io.hello_v1:90routing.hello_io.hello_v2:10权重代表 Envoy 发送给上游集群的流量的百分比。所有权重的总和必须是 100。然而，使用 total_weight 字段，我们可以控制所有权重之和必须等于的值。例如，下面的片段将 total_weight 设置为 15。\nroute_config:virtual_hosts:- name:hello_vhostdomains:[\u0026#34;hello.io\u0026#34;]routes:- match:prefix:\u0026#34;/\u0026#34;route:weighted_clusters:runtime_key_prefix:routing.hello_iototal_weight:15clusters:- name:hello_v1weight:5- name:hello_v2weight:5- name:hello_v3weight:5为了动态地控制权重，我们可以设置 runtime_key_prefix。路由器使用运行时密钥前缀值来构建与每个集群相关的运行时密钥。如果我们提供了运行时密钥前缀，路由器将检查 runtime_key_prefix + \u0026#34;.\u0026#34; + cluster_name 的值，其中 cluster_name 表示集群数组中的条目（例如 hello_v1、hello_v2）。如果 Envoy 没有找到运行时密钥，它将使用配置中指定的值作为默认值。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"a2618a1eda79d1c40def36d07c63ecdf","permalink":"https://lib.jimmysong.io/envoy-handbook/hcm/traffic-splitting/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/envoy-handbook/hcm/traffic-splitting/","section":"envoy-handbook","summary":"Envoy 支持在同一虚拟主机内将流量分割到不同的路由。我们可以在两个或多个上游集群之间分割流量。 有两种不同的方法。第一种是使用运行时对象中指定的百分比，第二种是使用加权集群。 使用运行时的百分比进行流量分割 使用","tags":["Envoy"],"title":"流量分割","type":"book"},{"authors":null,"categories":["可观测性"],"content":"到目前为止，我们已经从数据的角度讨论了现代可观测性。但是，现代可观测性还有另一个方面，从长远来看，它可能被证明同样重要：如何对待产生数据的仪表（instrumention）。\n大多数软件系统都是用现成的部件构建的：网络框架、数据库、HTTP 客户端、代理服务器、编程语言。在大多数组织中，很少有这种软件基础设施是在内部编写的。相反，这些组件是在许多组织中共享的。最常见的是，这些共享组件是以开放源码（OSS）库的形式出现的，并具有许可权。\n由于这些开放源码软件库几乎囊括了一般系统中的所有关键功能，因此获得这些库的高质量说明对大多数可观测性系统来说至关重要。\n传统上，仪表是 “单独出售” 的。这意味着，软件库不包括产生追踪、日志或度量的仪表。相反，特定解决方案的仪表是在事后添加的，作为部署可观测系统的一部分。\n 什么是特定解决方案仪表？\n在本章中，术语 “特定解决方案仪表” 是指任何旨在与特定的可观测系统一起工作的仪表，使用的是作为该特定系统的数据存储系统的产物而开发的客户端。在这些系统中，客户端和存储系统常常深深地融合在一起。因此，如果一个应用要从一个观测系统切换到另一个观测系统，通常需要进行全面的重新布设。\n 针对解决方案的仪表是 “三大支柱” 中固有的垂直整合的遗留问题。每个后端都摄取特定类型的专有数据；因此，这些后端的创建者也必须提供产生这些数据的仪表。\n这种工具化的方法给参与软件开发的每个人都带来了麻烦：供应商、用户和开放源码库的作者。\n可观测性被淹没在特定解决方案的仪表中 从可观测性系统的角度来看，仪表化代表了巨大的开销。\n在过去，互联网应用是相当同质化的，可以围绕一个特定的网络框架来建立可观测性系统。Java Spring、Ruby on Rails 或 .NET。但随着时间的推移，软件的多样性已经爆炸性增长。现在为每一个流行的网络框架和数据库客户端维护仪表是一项巨大的负担。\n这导致的重复劳动难以估量。传统上，供应商将他们在仪表上的投资作为销售点和把关的一种形式。但是，日益增长的软件开发速度已经开始使这种做法无法维持了。对于一个合理规模的仪表设备团队来说，覆盖面实在是太大了，无法跟上。\n这种负担对于新的、新颖的观测系统，特别是开放源码软件的观测项目来说尤其严峻。如果一个新的系统在编写了大量的仪表之前无法在生产中部署，而一个开放源码软件项目在广泛部署之前也无法吸引开发者的兴趣，那么科学进步就会陷入僵局。这对于基于追踪的系统来说尤其如此，它需要端到端的仪表来提供最大的价值。\n应用程序被锁定在特定解决方案的仪表中 从应用开发者的角度来看，特定解决方案的仪表代表了一种有害的锁定形式。\n可观测性是一个交叉性的问题。要彻底追踪、记录或度量一个大型的应用程序，意味着成千上万的仪表 API 调用将遍布整个代码库。改变可观测性系统需要把所有这些工具去掉，用新系统提供的不同工具来代替。\n替换仪表是一项重大的前期投资，即使只是为了尝试一个新的系统。更糟的是，大多数系统都太大了以致于在所有服务中同时更换所有仪表是不可行的。大多数系统需要逐步推出新的仪表设备，但这样的上线可能很难设计。\n被特定解决方案的仪表所 “困住” 是非常令人沮丧的。在可观测性供应商开始努力提供自己的仪表的同时，用户也开始拒绝采用这种仪表。由于了解到重新安装仪表的工作量，许多用户强烈希望他们正在考虑的任何新的观测系统能与他们目前使用的仪表一起工作。\n为了支持这一要求，许多可观测性系统试图与其他几个系统提供的仪表一起工作。但这种拼凑的方式降低了每个系统所摄取的数据的质量。从许多来源摄取数据意味着对输入的数据不再有明确的定义，当预期的数据不均衡且定义模糊时，分析工具就很难完成它们的工作。\n针对开源软件的特定解决方案的仪表基本上是不可能的 从一个开放源码库作者的角度来看，特定解决方案的仪表化是一个悲剧。\n来自开放源码软件库的遥测数据对于操作建立在它们之上的应用程序来说至关重要。最了解哪些数据对操作至关重要的人，以及操作者应该如何利用这些数据来补救问题的人，就是实际编写软件的开源库的开发者。\n但是，库的作者却陷入了困境。正如我们将看到的，没有任何一个特定解决方案的仪表化 API，无论写得多么好，都无法作为开放源码库可以接受的选择。\n如何挑选一个日志库？ 假设你正在编写世界上最伟大的开源网络框架。在生产过程中，很多事情都会出错，你自然希望把错误、调试和性能信息传达给你的用户。你使用哪个日志库？\n有很多体面的日志库。事实上，有很多，无论你选择哪个库，你都会有很多用户希望你选择一个不同的库。如果你的 Web 框架选择了一个日志库，而数据库客户端库选择了另一个，怎么办？如果这两个都不是用户想要使用的呢？如果他们选择了同一个库的不兼容的版本呢？\n没有一个完美的方法可以将多个特定解决方案的日志库组合成一个连贯的系统。虽然日志足够简单，不同解决方案的大杂烩可能是可行的，但对于特定解决方案的指标和追踪来说，情况并非如此。\n因此，开放源码软件库通常没有内置的日志、度量或追踪功能。取而代之的是，库提供了 “可观测性钩子”，这需要用户编写和维护一堆适配器，将使用的库连接到他们的可观测性系统上。\n作者们有大量的知识，他们想通信关于他们的系统应该如何运行的知识，但他们没有明确的方法去做。如果你问任何写过大量开源软件的人，他们会告诉你。这种情况是痛苦的！而且是不幸的！一些库的作者确实试图选择一个日志库，但却发现他们无意中为一些用户造成了版本冲突，同时迫使其他用户编写日志适配器来捕捉使用其实际日志库的数据。\n但是对于大多数库来说，可观测性只是一个事后的想法。虽然库的作者经常编写大量的测试套件，但他们很少花时间去考虑运行时的可观测性。考虑到库有大量的测试工具，但可观测性工具为零，这种结果并不令人惊讶。\n正如我们在接下来的几章中所看到的，现代可观测性的设计是为了使在可观测性管道中发挥作用的每个人的代理权最大化。但受益最大的是库的作者；对于特定解决方案的仪表，他们目前根本没有选择。\n分解问题 我们可以通过设计一个可观测性系统来解决上面列出的所有问题，以明确地解决每个人的需求。在本章的其余部分，我们将把现代观测系统的设计分解为基本要求。这些要求将为第五章中描述的 OpenTelemetry 的结构提供动力。\n要求：独立的仪表、遥测和分析 归根结底，计算机系统实际上就是人类系统。像可观测性这样的跨领域问题，几乎与每一个软件组件都有互动。同时，传输和处理遥测数据可能是一个大批量的活动，以至于一个大规模的观测系统会产生自己的操作问题。这意味着，许多不同的人，以不同的身份，需要与观测系统的不同方面交互。为了很好地服务于他们，这个系统必须确保每个参与其中的人都有他们所需要的代理权，以便快速和独立地执行任务。提供代理权是设计一个有效的观测系统的基本要求。\n让我们首先确定与运行中的软件系统有关的每个角色的责任：库的作者、应用程序的所有者、操作者和响应者。\n库的作者了解他们软件的情况\n对于封装了关键功能的软件库，如网络和请求管理，库的作者也必须管理追踪系统的各个方面：注入、提取和上下文传播。\n应用程序拥有者组织软件并管理依赖关系\n应用程序所有者选择构成其应用程序的组件，并确保它们编译成一个连贯的、有功能的系统。应用程序所有者还编写应用程序级别的工具，它必须与库作者提供的指令（和上下文传播）进行正确的交互。\n运维人员管理遥测的生产和传输\n运维管理从应用到响应者的可观测性数据的传输。他们必须能够选择数据的格式以及数据的发送地点。当数据在产生时，他们必须操作传输系统：管理缓冲、处理和传送数据所需的所有资源。\n响应者消费遥测数据并产生有用的见解\n要做到这一点，应对者必须了解数据结构及其内在意义（结构和意义将在第三章中详细描述）。当新的和改进的分析工具出现时，反应者还需要将其添加到他们的工具箱中。\n这些角色代表不同的决策点：\n 库的作者只能通过发布其代码的新版本来进行修改。 应用程序所有者只能通过部署其可执行文件的新版本来进行更改。 运维人员只能通过管理可执行文件的拓扑结构和配置来进行改变。 响应者只能根据他们收到的数据做出改变。  传统的三大支柱方法扰乱了所有这些角色。垂直整合的一个副作用是，几乎所有的数据变化都需要进行代码修改。几乎任何对可观测性系统的非微不足道的改变都需要应用程序所有者进行代码修改。要求其他人进行你所关心的改变，这样会有很大的阻力，并可能导致压力、冲突和不作为。\n显然，一个设计良好的可观测性系统应该侧重于允许每个人尽可能多的代理和直接控制，它应该避免将开发者变成意外的看门人。\n要求：零依赖性 应用程序是由依赖关系（网络框架、数据库客户端），加上依赖关系的依赖关系（OpenTelemetry 或其他仪表库），加上它们的依赖关系的依赖关系的依赖关系（无论这些仪表库依赖什么）组成。这些都被称为反式的依赖关系。\n如果任何两个依赖关系之间有冲突，应用程序就无法运行。例如，两个库可能分别需要一个不同的（不兼容的）底层网络库的版本，如 gRPC。这可能会导致一些不好的情况。例如，一个新版本的库可能包括一个需要的安全补丁，但也包括一个升级的依赖关系，这就产生了依赖关系冲突。\n诸如此类的过渡性依赖冲突给应用程序所有者带来了很大的麻烦，因为这些冲突无法独立解决。相反，应用程序所有者必须联系库的作者，要求他们提供一个解决方案，这最终需要时间（假设库的作者回应了这个请求）。\n为了使现代可观测性发挥作用，库必须能够嵌入仪表，而不必担心当他们的库被用于组成应用程序时而导致问题。因此，可观测性系统必须提供不包含可能无意中引发横向依赖冲突的依赖性的仪表。\n要求：严格的后向兼容和长期支持 当一个仪表化的 API 破坏了向后的兼容性，坏事就会发生。一个精心设计的应用程序最终可能会有成千上万的仪表调用站点。由于 API 的改变而不得不更新数以千计的调用站点是一个相当大的工作量。\n这就产生了一种特别糟糕的依赖性冲突，即一个库中的仪表化不再与另一个库中的仪表化兼容。\n因此，仪表化 API 必须在很长的时间范围内具有严格的向后兼容能力。理想的情况是，仪表化 API 一旦变得稳定，就永远不会破坏向后兼容。新的、实验性的 API 功能的开发方式必须保证它们的存在不会在包含稳定仪表的库之间产生冲突。\n分离关注点是良好设计的基础 在下一章中，我们将深入研究 OpenTelemetry 的架构，看看它是如何满足上面提出的要求的。但在这之前，我想说的是一个重要的问题。\n如果你分析这些需求，你可能会注意到一些奇特的现象：它们中几乎没有任何专门针对可观测性的内容。相反，重点是尽量减少依赖性，保持向后的兼容性，并确保不同的用户可以在没有无谓干扰的情况下发挥作用。\n每一个要求都指出了关注点分离是一个关键的设计特征。但是这些特性并不是 OpenTelemetry 所独有的。任何寻求广泛采用的软件库都会很好地包括它们。在这个意义上，OpenTelemetry 的设计也可以作为设计一般的开源软件的指南。下次当你开始一个新的开放源码软件项目时，请预先考虑这些要求，相应地设计你的库及你的用户会感谢你的。\n","date":1643990400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"1ef167264065609609abcbc50f579f10","permalink":"https://lib.jimmysong.io/opentelemetry-obervability/supporting-open-source-and-native-instrumentation/","publishdate":"2022-02-05T00:00:00+08:00","relpermalink":"/opentelemetry-obervability/supporting-open-source-and-native-instrumentation/","section":"opentelemetry-obervability","summary":"第 4 章：支持开源和原生监测","tags":["OpenTelemetry"],"title":"第 4 章：支持开源和原生监测","type":"book"},{"authors":null,"categories":["云原生"],"content":"在这一节中，我们将解释 Envoy 的基本构建模块。\nEnvoy 配置的根被称为引导配置。它包含了一些字段，我们可以在这里提供静态或动态的资源和高级别的 Envoy 配置（例如，Envoy 实例名称、运行时配置、启用管理界面等等）。\n为了开始学习，我们将主要关注静态资源，在课程的后面，我们将介绍如何配置动态资源。\nEnvoy 输出许多统计数据，这取决于启用的组件和它们的配置。我们会在整个课程中提到不同的统计信息，在课程后面的专门模块中，我们会更多地讨论统计信息。\n下图显示了通过这些概念的请求流。\n   Envoy 构建块  这一切都从监听器开始。Envoy 暴露的监听器是命名的网络位置，可以是一个 IP 地址和一个端口，也可以是一个 Unix 域套接字路径。Envoy 通过监听器接收连接和请求。考虑一下下面的 Envoy 配置。\nstatic_resources:listeners:- name:listener_0address:socket_address:address:0.0.0.0port_value:10000filter_chains:[{}]通过上面的 Envoy 配置，我们在 0.0.0.0 地址的 10000 端口上声明了一个名为 listener_0 的监听器。这意味着 Envoy 正在监听 0.0.0.0:10000 的传入请求。\n每个监听器都有不同的部分需要配置。然而，唯一需要的设置是地址。上述配置是有效的，你可以用它来运行 Envoy—— 尽管它没有用，因为所有的连接都会被关闭。\n我们让 filter_chains 字段为空，因为在接收数据包后不需要额外的操作。\n为了进入下一个构件（路由），我们需要创建一个或多个网络过滤器链（filter_chains），至少要有一个过滤器。\n网络过滤器通常对数据包的有效载荷进行操作，查看有效载荷并对其进行解析。例如，Postgres 网络过滤器解析数据包的主体，检查数据库操作的种类或其携带的结果。\nEnvoy 定义了三类过滤器：监听器过滤器、网络过滤器和 HTTP 过滤器。监听器过滤器在收到数据包后立即启动，通常对数据包的头信息进行操作。监听器过滤器包括代理监听器过滤器（提取 PROXY 协议头），或 TLS 检查器监听器过滤器（检查流量是否为 TLS，如果是，则从 TLS 握手中提取数据）。\n每个通过监听器进来的请求可以流经多个过滤器。我们还可以写一个配置，根据传入的请求或连接属性选择不同的过滤器链。\n   过滤器链  一个特殊的、内置的网络过滤器被称为 HTTP 连接管理器过滤器（HTTP Connection Manager Filter）或 HCM。HCM 过滤器能够将原始字节转换为 HTTP 级别的消息。它可以处理访问日志，生成请求 ID，操作头信息，管理路由表，并收集统计数据。我们将在以后的课程中对 HCM 进行更详细的介绍。\n就像我们可以为每个监听器定义多个网络过滤器（其中一个是 HCM）一样，Envoy 也支持在 HCM 过滤器中定义多个 HTTP 级过滤器。我们可以在名为 http_filters 的字段下定义这些 HTTP 过滤器。\n   HCM 过滤器  HTTP 过滤器链中的最后一个过滤器必须是路由器过滤器（envoy.filters.HTTP.router）。路由器过滤器负责执行路由任务。这最终把我们带到了第二个构件 —— 路由。\n我们在 HCM 过滤器的 route_config 字段下定义路由配置。在路由配置中，我们可以通过查看元数据（URI、Header 等）来匹配传入的请求，并在此基础上，定义流量的发送位置。\n路由配置中的顶级元素是虚拟主机。每个虚拟主机都有一个名字，在发布统计数据时使用（不用于路由），还有一组被路由到它的域。\n让我们考虑下面的路由配置和域的集合。\nroute_config:name:my_route_configvirtual_hosts:- name:tetrate_hostsdomains:[\u0026#34;tetrate.io\u0026#34;]routes:...- name:test_hostsdomains:[\u0026#34;test.tetrate.io\u0026#34;,\u0026#34;qa.tetrate.io\u0026#34;]routes:...如果传入请求的目的地是 tetrate.io（即 Host/Authority 标头被设置为其中一个值），则 tetrate_hosts  虚拟主机中定义的路由将得到处理。\n同样，如果 Host/Authority 标头包含 test.tetrate.io 或 qa.tetrate.io，test_hosts 虚拟主机下的路由将被处理。使用这种设计，我们可以用一个监听器（0.0.0.0:10000）来处理多个顶级域。\n如果你在数组中指定多个域，搜索顺序如下：\n 精确的域名（例如：tetrate.io）。 后缀域名通配符（如 *.tetrate.io）。 前缀域名通配符（例如：tetrate.*）。 匹配任何域的特殊通配符（*）。  在 Envoy 匹配域名后，是时候处理所选虚拟主机中的 routes 字段了。这是我们指定如何匹配一个请求，以及接下来如何处理该请求（例如，重定向、转发、重写、发送直接响应等）的地方。\n我们来看看一个例子。\nstatic_resources:listeners:- name:listener_0address:socket_address:address:0.0.0.0port_value:10000filter_chains:- filters:- name:envoy.filters.network.http_connection_managertyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManagerstat_prefix:hello_world_servicehttp_filters:- name:envoy.filters.http.routerroute_config:name:my_first_routevirtual_hosts:- name:direct_response_servicedomains:[\u0026#34;*\u0026#34;]routes:- match:prefix:\u0026#34;/\u0026#34;direct_response:status:200body:inline_string:\u0026#34;yay\u0026#34;配置的顶部部分与我们之前看到的一样。我们已经添加了 HCM 过滤器、统计前缀（hello_world_service）、单个 HTTP 过滤器（路由器）和路由配置。\n在虚拟主机内，我们要匹配任何域名。在 routes 下，我们匹配前缀（/），然后我们可以发送一个响应。\n当涉及到匹配请求时，我们有多种选择。\n   路由匹配 描述 示例     prefix 前缀必须与:path 头的开头相符。 /hello 与 hello.com/hello、hello.com/helloworld 和 hello.com/hello/v1 匹配。   path 路径必须与:path 头完全匹配。 /hello 匹配 hello.com/hello，但不匹配 hello.com/helloworld 或 hello.com/hello/v1   safe_regex 所提供的正则表达式必须与:path 头匹配。 /\\{3} 匹配任何以 / 开头的三位数。例如，与 hello.com/123 匹配，但不能匹配 hello.com/hello 或 hello.com/54321。   connect_matcher 匹配器只匹配 CONNECT 请求。     一旦 Envoy 将请求与路由相匹配，我们就可以对其进行路由、重定向或返回一个直接响应。在这个例子中，我们通过 direct_response 配置字段使用直接响应。\n你可以把上述配置保存到 envoy-direct-response.yaml 中。\n我们将使用一个名为 func-e 的命令行工具。func-e 允许我们选择和使用不同的 Envoy 版本。\n我们可以通过运行以下命令下载 func-e CLI。\ncurl https://func-e.io/install.sh | sudo bash -s -- -b /usr/local/bin 现在我们用我们创建的配置运行 Envoy。\nfunc-e run -c envoy-direct-response.yaml 一旦 Envoy 启动，我们就可以向 localhost:10000 发送一个请求，以获得我们配置的直接响应。\n$ curl localhost:10000 yay 同样，如果我们添加一个不同的主机头（例如 -H \u0026#34;Host: hello.com\u0026#34;）将得到相同的响应，因为 hello.com 主机与虚拟主机中定义的域相匹配。\n在大多数情况下，从配置中直接发送响应是一个很好的功能，但我们会有一组端点或主机，我们将流量路由到这些端点或主机。在 Envoy 中做到这一点的方法是通过定义集群。\n集群（Cluster）是一组接受流量的上游类似主机。这可以是你的服务所监听的主机或 IP 地址的列表。\n例如，假设我们的 hello world 服务是在 127.0.0.0:8000 上监听。然后，我们可以用一个单一的端点创建一个集群，像这样。\nclusters:- name:hello_world_serviceload_assignment:cluster_name:hello_world_serviceendpoints:- lb_endpoints:- endpoint:address:socket_address:address:127.0.0.1port_value:8000集群的定义与监听器的定义在同一级别，使用 clusters 字段。我们在路由配置中引用集群时，以及在导出统计数据时，都会使用集群。该名称在所有集群中必须是唯一的。\n在 load_assignment 字段下，我们可以定义要进行负载均衡的端点列表，以及负载均衡策略设置。\nEnvoy 支持多种负载均衡算法（round-robin、Maglev、least-request、random），这些算法是由静态引导配置、DNS、动态 xDS（CDS 和 EDS 服务）以及主动 / 被动健康检查共同配置的。如果我们没有通过 lb_policy 字段明确地设置负载均衡算法，它默认为 round-robin。\nendpoints 字段定义了一组属于特定地域的端点。使用可选的 locality 字段，我们可以指定上游主机的运行位置，然后在负载均衡过程中使用（即，将请求代理到离调用者更近的端点）。\n添加新的端点指示负载均衡器在一个以上的接收者之间分配流量。通常情况下，负载均衡器对所有端点一视同仁，但集群定义允许在端点内建立一个层次结构。\n例如，端点可以有一个 权重（weight） 属性，这将指示负载均衡器与其他端点相比，向这些端点发送更多 / 更少的流量。\n另一种层次结构类型是基于地域性的（locality），通常用于定义故障转移架构。这种层次结构允许我们定义地理上比较接近的 “首选” 端点，以及在 “首选” 端点变得不健康的情况下应该使用的 “备份” 端点。\n由于我们只有一个端点，所以我们还没有设置 locality。在 lb_endpoints 字段下，可以定义 Envoy 可以路由流量的实际端点。\n我们可以在 Cluster 中配置以下可选功能：\n 主动健康检查（health_checks） 断路器 (circuit_breakers) 异常点检测（outlier_detection） 在处理上游的 HTTP 请求时有额外的协议选项 一组可选的网络过滤器，应用于所有出站连接等  和监 …","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"d118fb6c34fa567c54ddc2a95f2fe1ff","permalink":"https://lib.jimmysong.io/cloud-native-handbook/service-mesh/envoy-building-blocks/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/cloud-native-handbook/service-mesh/envoy-building-blocks/","section":"cloud-native-handbook","summary":"在这一节中，我们将解释 Envoy 的基本构建模块。 Envoy 配置的根被称为引导配置。它包含了一些字段，我们可以在这里提供静态或动态的资源和高级别的 Envoy 配置（例如，Envoy 实例名称、运行时配置、启用管理界面等等）。 为了开","tags":["云原生"],"title":"Envoy 的构建模块","type":"book"},{"authors":null,"categories":["文化"],"content":"总结  保持友善。 解释你的推理。 在给出明确的指示与只指出问题并让开发人员自己决定间做好平衡。 鼓励开发人员简化代码或添加代码注释，而不仅仅是向你解释复杂性。  礼貌 一般而言，对于那些正在被您审查代码的人，除了保持有礼貌且尊重以外，重要的是还要确保您（的评论）是非常清楚且有帮助的。你并不总是必须遵循这种做法，但在说出可能令人不安或有争议的事情时你绝对应该使用它。 例如：\n糟糕的示例：“为什么这里你使用了线程，显然并发并没有带来什么好处？”\n好的示例：“这里的并发模型增加了系统的复杂性，但没有任何实际的性能优势，因为没有性能优势，最好是将这些代码作为单线程处理而不是使用多线程。”\n解释为什么 关于上面的“好”示例，您会注意到的一件事是，它可以帮助开发人员理解您发表评论的原因。 并不总是需要您在审查评论中包含此信息，但有时候提供更多解释，对于表明您的意图，您在遵循的最佳实践，或为您建议如何提高代码健康状况是十分恰当的。\n给予指导 一般来说，修复 CL 是开发人员的责任，而不是审查者。 您无需为开发人员详细设计解决方案或编写代码。\n但这并不意味着审查者应该没有帮助。一般来说，您应该在指出问题和提供直接指导之间取得适当的平衡。指出问题并让开发人员做出决定通常有助于开发人员学习，并使代码审查变得更容易。它还可能产生更好的解决方案，因为开发人员比审查者更接近代码。\n但是，有时直接说明，建议甚至代码会更有帮助。代码审查的主要目标是尽可能获得最佳 CL。第二个目标是提高开发人员的技能，以便他们随着时间的推移需要的审查越来越少。\n接受解释 如果您要求开发人员解释一段您不理解的代码，那通常会导致他们更清楚地重写代码。偶尔，在代码中添加注释也是一种恰当的响应，只要它不仅仅是解释过于复杂的代码。\n**仅在代码审查工具中编写的解释对未来的代码阅读者没有帮助。**这仅在少数情况下是可接受的，例如当您查看一个您不熟悉的领域时，开发人员会用来向您解释普通读者已经知道的内容。\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"278558262e64bbd397064b7e418d57c9","permalink":"https://lib.jimmysong.io/eng-practices/review/reviewer/comments/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/eng-practices/review/reviewer/comments/","section":"eng-practices","summary":"总结 保持友善。 解释你的推理。 在给出明确的指示与只指出问题并让开发人员自己决定间做好平衡。 鼓励开发人员简化代码或添加代码注释，而不仅仅是向你解释复杂性。 礼貌 一般而言，对于那些正在被您审查代码的人，除了保","tags":null,"title":"如何撰写 Code Review 评论","type":"book"},{"authors":null,"categories":["网络"],"content":"在 Kubernetes 中管理 pod 时，Cilium 将创建一个 CiliumEndpoint 的自定义资源定义（CRD）。每个由 Cilium 管理的 pod 都会创建一个 CiliumEndpoint，名称相同且在同一命名空间。CiliumEndpoint 对象包含的信息与 cilium endpoint get 在.status 字段下的 json 输出相同，但可以为集群中的所有 pod 获取。添加 -o json 将导出每个端点的更多信息。这包括端点的标签、安全身份和对其有效的策略。\n例如：\n$ kubectl get ciliumendpoints --all-namespaces NAMESPACE NAME AGE default app1-55d7944bdd-l7c8j 1h default app1-55d7944bdd-sn9xj 1h default app2 1h default app3 1h kube-system cilium-health-minikube 1h kube-system microscope 1h 提示\n  每个 cilium-agent pod 都会创建一个 CiliumEndpoint 来代表自己的 agent 间健康检查端点。这些不是 Kubernetes 中的 pod，而是在 kube-system 命名空间中。它们被命名为 cilium-health-\u0026lt;节点名\u0026gt;。   ","date":1655438400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"3c87defb2a5599256ee40dd58168d26b","permalink":"https://lib.jimmysong.io/cilium-handbook/kubernetes/ciliumendpoint/","publishdate":"2022-06-17T12:00:00+08:00","relpermalink":"/cilium-handbook/kubernetes/ciliumendpoint/","section":"cilium-handbook","summary":"在 Kubernetes 中管理 pod 时，Cilium 将创建一个 CiliumEndpoint 的自定义资源定义（CRD）。每个由 Cilium 管理的 pod 都会创建一个 CiliumEndpoint，名称相同且在同一命名空间。CiliumEndpoint 对象包含的信息与 cilium endpoint","tags":["Cilium"],"title":"端点 CRD","type":"book"},{"authors":null,"categories":["Linux"],"content":"现在你已经了解了什么是 eBPF 以及它是如何工作的，我们再探索一些可能会在生产部署中使用的基于 eBPF 技术的工具。我们将举一些基于 eBPF 的开源项目的例子，这些项目提供了三方面的能力：网络、可观测性和安全。\n网络 eBPF 程序可以连接到网络接口和内核的网络堆栈的各个点。在每个点上，eBPF 程序可以丢弃数据包，将其发送到不同的目的地，甚至修改其内容。这就实现了一些非常强大的功能。让我们来看看通常用 eBPF 实现的几个网络功能。\n负载均衡 Facebook 正在大规模的使用 eBPF 的网络功能，因此你不必对 eBPF 用于网络的可扩展性有任何怀疑。他们是 BPF 的早期采用者，并在 2018 年推出了 Katran，一个开源的四层负载均衡器。\n另一个高度扩展的负载均衡器的例子是来自 Cloudflare 的 Unimog 边缘负载均衡器。通过在内核中运行，eBPF 程序可以操作网络数据包，并将其转发到适当的目的地，而不需要数据包通过网络堆栈和用户空间。\nCilium 项目作为一个 eBPF Kubernetes 网络插件更为人所知（我一会儿会讨论），但作为独立的负载均衡器，它也被用于大型电信公司和企业内部部署。同样，因为它能够在早期阶段处理数据包，而不需要进入到用户空间，它具有很高的性能。\nKubernetes 网络 CNCF 项目 Cilium 最初基于 eBPF 的 CNI 实现。它最初是由一群从事 eBPF 工作的内核维护者发起的，他们认识到 eBPF 在云原生网络中的应用潜力。它现在被用作谷歌 Kubernetes 引擎、亚马逊 EKS Anywhere 和阿里云的默认数据平面。\n在云原生环境下，pod 在不断的启停，每个 pod 都会被分配一个 IP 地址。在启用 eBPF 网络之前，当 pod 启停的时候，每个节点都必须为它们更新 iptables 规则，以便在 pod 之间进行路由；而当这些 iptable 规则规模变大后，将十分不便于管理。如 图 6-1 所示，Cilium 极大地简化了路由，仅需在 eBPF 中创建的一个简单的查找表，就可以获得 可观的性能改进。\n另一个在传统的 iptables 版本之外还增加了 eBPF 实现的 Kubernetes CNI 是 Calico。\n  图 6-1. 用 eBPF 绕过主机网络堆栈  服务网格 eBPF 作为服务网格数据平面的基础也是非常有意义的。许多服务网格在七层，即应用层运行，并使用代理组件（如 Envoy）来辅助应用程序。在 Kubernetes 中，这些代理通常以 sidecar 模式部署，每个 pod 中有一个代理容器，这样代理就可以访问 pod 的网络命名空间。正如你在 第五章 中看到的，eBPF 有一个比 sidecar 模型更有效的方法。由于内核可以访问主机中所有 pod 的命名空间，我们可以使用 eBPF 连接 pod 中的应用和主机上的代理，如 图 6-2 所示。\n  图 6-2. eBPF 实现了服务网格的高效无 sidecar 模型，每个节点一个代理，而不是每个应用 pod 一个代理  我还有一篇关于使用 eBPF 实现更高效的服务网格数据平面的文章，Solo.io 上也有发布过类似文章。在写这篇文章的时候，Cilium 服务网格已进入测试阶段，并显示出比传统的 sidecar 代理方法具有更大的 性能提升。\n可观测性 正如你在本报告前面所看到的，eBPF 程序可以获得对机器上发生的一切的可观测性。通过收集事件数据并将其传递给用户空间，eBPF 实现了一系列强大的可观测性工具，可以向你展示你的应用程序是如何执行和表现的，而不需要对这些应用程序做任何改变。\n在本报告的前面，你已经看到了 BCC 项目，几年来，Brendan Gregg 在 Netflix 做了开创性的工作，展示了这些 eBPF 工具如何被用来 观测我们感兴趣的几乎任何指标，而且是大规模和高性能的。\nKinvolk 的 Inspektor Gadget 将其中一些起源于 BCC 的工具带入了 Kubernetes 的世界，这样你就可以在命令行上轻松观测特定的工作负载。\n新一代的项目和工具正在这项工作的基础上，提供基于 GUI 的观测能力。CNCF 项目 Pixie 可以让你运行预先写好的或自定义的脚本，通过一个强大的、视觉上吸引人的用户界面查看指标和日志。因为它是基于 eBPF 的，这意味着你可以自动检测所有应用程序，获得性能数据，而无需进行任何代码修改或配置。图 6-3 显示的只是 Pixie 中众多可视化的一个例子。\n  图 6-3. 一个小型 Kubernetes 集群上运行的所有东西的 Pixie 火焰图  另一个名为 Parca 的可观测性项目专注于连续剖析，使用 eBPF 对 CPU 使用率等指标进行有效采样，可以用来检测性能瓶颈。\nCilium 的 Hubble 组件是一个具有命令行界面和用户界面的可观测性工具（如 图 6-4 所示），它专注于 Kubernetes 集群中的网络流。\n  图 6-4. Cilium 的 Hubble 用户界面显示了 Kubernetes 集群中的网络流量  在云原生环境中，IP 地址不断被动态重新分配，基于 IP 地址的传统网络观测工具的作用非常有限。作为一个 CNI，Cilium 可以访问工作负载身份信息，这意味着 Hubble 可以显示由 Kubernetes pod、服务和命名空间标识的服务映射和流量数据。这对于诊断网络问题十分有用。\n能够观测到活动，这是安全工具的基础，这些工具将正在发生的事情与策略或规则相比较，以了解该活动是预期的还是可疑的。让我们来看看一些使用 eBPF 来提供云原生安全能力的工具。\n安全 有一些强大的云原生工具，通过使用 eBPF 检测甚至防止恶意活动来增强安全性。我将其分为两类：一类是确保网络活动的安全，另一类是确保应用程序在运行时的预期行为。\n网络安全 由于 eBPF 可以检查和操纵网络数据包，它在网络安全方面有许多用途。基本原理是，如果一个网络数据包被认为是恶意的或有问题的，因为它不符合一些安全验证标准，就可以被简单地丢弃。eBPF 可以很高效的来验证这一点，因为它可以钩住内核中网络堆栈的相关部分，甚至在网卡上 1。这意味着策略外的或恶意的数据包可以在产生网络堆栈处理和传递到用户空间的处理成本之前被丢弃。\n这里有一个 eBPF 早期在生产中大规模使用的一个例子 —— Cloudflare 的 DDoS（分布式拒绝服务）保护。DDoS 攻击者用许多网络信息淹没目标机，希望目标机忙于处理这些信息，导致无法提供有效工作。Cloudflare 的工程师使用 eBPF 程序，在数据包到达后立即对其进行检查，并迅速确定一个数据包是否是这种攻击的一部分，如果是，则将其丢弃。数据包不必通过内核的网络堆栈，因此需要的处理资源要少得多，而且目标可以应对更大规模的恶意流量。\neBPF 程序也被用于动态缓解 ”死亡数据包“ 的内核漏洞 2。攻击者以这样的方式制作一个网络工作数据包——利用了内核中的一个错误，使其无法正确处理该数据包。与其等待内核补丁的推出，不如通过加载一个 eBPF 程序来缓解攻击，该程序可以寻找这些特别制作的数据包并将其丢弃。这一点的真正好处是，eBPF 程序可以动态加载，而不必改变机器上的任何东西。\n在 Kubernetes 中，网络策略 是一等资源，但它是由网络插件来执行的。一些 CNI，包括 Cilium 和 Calico，为更强大的规则提供了扩展的网络策略功能，例如允许或禁止流量到一个由完全限定域名而不是仅仅由 IP 地址指定的目的地。在 app.networkpolicy.io 有一个探索网络策略及其效果的好工具，如 图 6-5 所示。\n  图 6-5. 网络策略编辑器显示了一个策略效果的可视化表示  标准的 Kubernetes 网络策略规则适用于进出应用 pod 的流量，但由于 eBPF 对所有网络流量都有可视性，它也可用于主机防火墙功能，限制进出主机（虚拟机）的流量 3。\neBPF 也可以被用来提供透明的加密，无论是通过 WireGuard 还是 IPsec 4。在这里，透明 意味着应用程序不需要任何修改 —— 事实上，应用程序可以完全不知道其网络流量是被加密的。\n运行时安全 eBPF 也被用来构建工具，检测恶意程序，防止恶意行为。这些恶意程序包括访问未经许可的文件，运行可执行程序，或试图获得额外的权限。\n事实上，你很可能已经以 seccomp 的形式使用了基于 BPF 的安全策略，这是一个Linux 功能，限制应用程序可以调用的系统调用集。\nCNCF 项目 Falco 扩展了这种限制应用程序可以进行系统调用的想法。Falco 的规则定义是用 YAML 创建的，这比 seccomp 配置文件更容易阅读和理解。默认的 Falco 驱动是一个内核模块，但也有一个 eBPF 探针驱动，它与 ”原始系统调用“ 事件相联系。它不会阻止这些系统调用的完成，但它可以生成日志或其他通知，提醒操作人员注意潜在的恶意程序。\n正如我们在 第三章 中看到的，eBPF 程序可以附加到 LSM 接口上，以防止恶意行为或修复已知的漏洞。例如，Denis Efremov 写了一个 eBPF 程序 来防止 exec() 系统调用在没有传递任何参数的情况下运行，以修复 PwnKit 5 的高危漏洞。eBPF也可用于缓解投机执行的 ”Spectre“ 攻击 6。\nTracee 是另一个使用 eBPF 的运行时安全开源项目。除了基于系统调用的检查之外，它还使用 LSM 接口。这有助于避免受到 TOCTTOU 竞争 条件的影响，因为只检查系统调用时可能会出现这种情况。Tracee 支持用 Open Policy Agent 的 Rego 语言定义的规则，也允许用 Go 定义的插件规则。\nCilium 的 Tetragon 组件提供了另一种强大的方法，使用 eBPF 来监控 容器安全可观测性的四个黄金信号：进程执行、网络套接字、文件访问和七层网络身份。这使操作人员能够准确地看到所有恶意或可疑事件，直击特定 pod 中的可执行文件名称和用户身份。例如，如果你受到加密货币挖矿的攻击，你可以看到到底是什么可执行程序打开了与矿池的网络连接，什么时候，从哪个 pod。这些取证是非常有价值的，可以了解漏洞是如何发生的，并使其容易建立安全策略，以防止类似的攻击再次发生。\n如果你想更深入地了解 eBPF 的安全可观测性这一主题，请查看 Natália Ivánkó 和 Jed Salazar 的报告 7。请关注云原生 eBPF 领域，因为不久之后我们就会看到利用 BPF LSM 和其他 eBPF 定制的工具来提供安全执行和可以观测能力。\n我们在网络、可观测性和安全方面对几个云原生工具进行了考察。与前几代相比，eBPF 的使用为它们两个关键优势：\n 从内核中的有利位置来看，eBPF 程序对所有进程都有可视性。 通过避免内核和用户空间执行之间的转换，eBPF 程序为收集事件数据或处理网络数据包提供了一种极其有效的方式。  这并不意味着我们应该使用 eBPF 来处理所有的事情！在 eBPF 中编写特定业务的应用程序是没有意义的，就像我们不可能将应用程序写成内核模块一样。但是也有一些例外情况，比如对于高频交易这样对性能有极高的情况下。正如我们在本章中所看到的那样，eBPF 主要是用于为其他应用程序提供工具。\n参考   有些网卡或驱动支持 XDP 或 eXpress Data Path 钩子，允许 eBPF 程序完全从内核中卸载出来。 ↩︎\n Daniel Borkmann 在他的演讲中讨论了这个问题，《BPF 更适合作为数据平面》（eBPF 峰会（线上），2020 年）。 ↩︎\n 见 Cilium 的主机防火墙文档。 ↩︎\n Tailscale 有这两种加密协议的比较。 ↩︎\n 见 Bharat Jogi 的博 …","date":1654142400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"14776f82b2d541f1e3a962ccdfe26c11","permalink":"https://lib.jimmysong.io/what-is-ebpf/ebpf-tools/","publishdate":"2022-06-02T12:00:00+08:00","relpermalink":"/what-is-ebpf/ebpf-tools/","section":"what-is-ebpf","summary":"现在你已经了解了什么是 eBPF 以及它是如何工作的，我们再探索一些可能会在生产部署中使用的基于 eBPF 技术的工具。我们将举一些基于 eBPF 的开源项目的例子，这些项目提供了三方面的能力：网络、可观测性和安全。 网络 eBPF 程序可以","tags":["eBPF"],"title":"第六章：eBPF 工具","type":"book"},{"authors":null,"categories":["安全"],"content":"由于 DevSecOps 的基本要素跨越了开发（安全的构建和测试、打包）、交付 / 部署和持续监控（以确保运行期间的安全状态），本文建议的目标受众包括软件开发、运维和安全团队。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"79520c75ee3deb518e2c6fd701dd7fbf","permalink":"https://lib.jimmysong.io/service-mesh-devsecops/intro/target-audience/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/service-mesh-devsecops/intro/target-audience/","section":"service-mesh-devsecops","summary":"由于 DevSecOps 的基本要素跨越了开发（安全的构建和测试、打包）、交付 / 部署和持续监控（以确保运行期间的安全状态），本文建议的目标受众包括软件开发、运维和安全团队。","tags":["Service Mesh","DevSecOps"],"title":"1.3 目标受众","type":"book"},{"authors":null,"categories":["安全"],"content":"无论代码类型如何，CI/CD 管道都有一些共同的实施问题需要解决。确保流程安全涉及到为操作构建任务分配角色。自动化工具（例如，Git Secrets）可用于此目的。为保证 CI/CD 管道的安全，以下安全任务应被视为最低限度：\n 强化托管代码和工件库的服务器。 确保用于访问存储库的凭证，如授权令牌和生成拉动请求的凭证。 控制谁可以在容器镜像注册处签入和签出，因为它们是 CI 管道产生的工件的存储处，是 CI 和 CD 管道之间的桥梁。 记录所有的代码和构建更新活动。 如果在 CI 管道中构建或测试失败 —— 向开发人员发送构建报告并停止进一步的管道任务。配置代码库自动阻止来自 CD 管道的所有拉取请求。 如果审计失败，将构建报告发送给安全团队，并停止进一步的管道任务。 确保开发人员只能访问应用程序代码，而不能访问五种管道代码类型中的任何一种。 在构建和发布过程中，在每个需要的 CI/CD 阶段签署发布工件（最好是多方签署）。 在生产发布期间，验证所有需要的签名（用多个阶段的密钥生成），以确保没有人绕过管道。  ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"94137557c910112c7b91f61be4daf585","permalink":"https://lib.jimmysong.io/service-mesh-devsecops/implement/securing-the-ci-cd-pipeline/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/service-mesh-devsecops/implement/securing-the-ci-cd-pipeline/","section":"service-mesh-devsecops","summary":"无论代码类型如何，CI/CD 管道都有一些共同的实施问题需要解决。确保流程安全涉及到为操作构建任务分配角色。自动化工具（例如，Git Secrets）可用于此目的。为保证 CI/CD 管道的安全，以下安全任务应被视为","tags":["Service Mesh","DevSecOps"],"title":"4.6 确保 CI/CD 管道的安全","type":"book"},{"authors":null,"categories":["Envoy"],"content":"HCM 支持在加权集群、路由、虚拟主机和 / 或全局配置层面操纵请求和响应头。\n注意，我们不能直接从配置中修改所有的 Header，使用 Wasm 扩展的情况除外。然后，我们可以修改 :authority  header，例如下面的情况。\n不可变的头是伪头（前缀为:，如:scheme）和host头。此外，诸如 :path 和 :authority 这样的头信息可以通过 prefix_rewrite、regex_rewrite 和 host_rewrite 配置来间接修改。\nEnvoy 按照以下顺序对请求 / 响应应用这些头信息：\n 加权的集群级头信息 路由级 Header 虚拟主机级 Header 全局级 Header  这个顺序意味着 Envoy 可能会用更高层次（路由、虚拟主机或全局）配置的头来覆盖加权集群层次上设置的 Header。\n在每一级，我们可以设置以下字段来添加 / 删除请求 / 响应头。\n response_headers_to_add：要添加到响应中的 Header 信息数组。 response_headers_to_remove：要从响应中移除的 Header 信息数组。 request_headers_to_add：要添加到请求中的 Header 信息数组。 request_headers_to_remove：要从请求中删除的 Header 信息数组。  除了硬编码标头值之外，我们还可以使用变量来为标头添加动态值。变量名称以百分数符号（%）为分隔符。支持的变量名称包括 %DOWNSTREAM_REMOTE_ADDRESS%、%UPSTREAM_REMOTE_ADDRESS%、%START_TIME%、%RESPONSE_FLAGS% 和更多。你可以在这里找到完整的变量列表。\n让我们看一个例子，它显示了如何在不同级别的请求 / 响应中添加 / 删除头信息。\nroute_config:response_headers_to_add:- header:key:\u0026#34;header_1\u0026#34;value:\u0026#34;some_value\u0026#34;# 如果为真（默认），它会将该值附加到现有值上。# 否则它将替换现有的值append:falseresponse_headers_to_remove:\u0026#34;header_we_dont_need\u0026#34;virtual_hosts:- name:hello_vhostrequest_headers_to_add:- header:key:\u0026#34;v_host_header\u0026#34;value:\u0026#34;from_v_host\u0026#34;domains:[\u0026#34;hello.io\u0026#34;]routes:- match:prefix:\u0026#34;/\u0026#34;route:cluster:helloresponse_headers_to_add:- header:key:\u0026#34;route_header\u0026#34;value:\u0026#34;%DOWNSTREAM_REMOTE_ADDRESS%\u0026#34;- match:prefix:\u0026#34;/api\u0026#34;route:cluster:hello_apiresponse_headers_to_add:- header:key:\u0026#34;api_route_header\u0026#34;value:\u0026#34;api-value\u0026#34;- header:key:\u0026#34;header_1\u0026#34;value:\u0026#34;this_will_be_overwritten\u0026#34;标准 Header Envoy 在收到请求（解码）和向上游集群发送请求（编码）时，会操作一组头信息。\n当使用裸露的 Envoy 配置将流量路由到单个集群时，在编码过程中会设置以下头信息。\n\u0026#39;:authority\u0026#39;, \u0026#39;localhost:10000\u0026#39; \u0026#39;:path\u0026#39;, \u0026#39;/\u0026#39; \u0026#39;:method\u0026#39;, \u0026#39;GET\u0026#39; \u0026#39;:scheme\u0026#39;, \u0026#39;http\u0026#39; \u0026#39;user-agent\u0026#39;, \u0026#39;curl/7.64.0\u0026#39; \u0026#39;accept\u0026#39;, \u0026#39;*/*\u0026#39; \u0026#39;x-forwarded-proto\u0026#39;, \u0026#39;http\u0026#39; \u0026#39;x-request-id\u0026#39;, \u0026#39;14f0ac76-128d-4954-ad76-823c3544197e\u0026#39; \u0026#39;x-envoy-expected-rq-timeout-ms\u0026#39;, \u0026#39;15000\u0026#39; 在编码（响应）时，会发送一组不同的头信息。\n\u0026#39;:status\u0026#39;, \u0026#39;200\u0026#39; \u0026#39;x-powered-by\u0026#39;, \u0026#39;Express\u0026#39; \u0026#39;content-type\u0026#39;, \u0026#39;text/html; charset=utf-8\u0026#39; \u0026#39;content-length\u0026#39;, \u0026#39;563\u0026#39; \u0026#39;etag\u0026#39;, \u0026#39;W/\u0026#34;233-b+4UpNDbOtHFiEpLMsDEDK7iTeI\u0026#34;\u0026#39; \u0026#39;date\u0026#39;, \u0026#39;Fri, 16 Jul 2021 21:59:52 GMT\u0026#39; \u0026#39;x-envoy-upstream-service-time\u0026#39;, \u0026#39;2\u0026#39; \u0026#39;server\u0026#39;, \u0026#39;envoy\u0026#39; 下表解释了 Envoy 在解码或编码过程中设置的不同头信息。\n   Header 描述     :scheme 设置并提供给过滤器，并转发到上游。(对于 HTTP/1，:scheme 头是由绝对 URL 或 x-forwaded-proto 头值设置的) 。   user-agent 通常由客户端设置，但在启用 add_user_agent 时可以修改（仅当 Header 尚未设置时）。该值由 --service-cluster命令行选项决定。   x-forwarded-proto 标准头，用于识别客户端用于连接到代理的协议。该值为 http 或 https。   x-request-id Envoy 用来唯一地识别一个请求，也用于访问记录和跟踪。   x-envoy-expected-rq-timeout-ms 指定路由器期望请求完成的时间，单位是毫秒。这是从 x-envoy-upstream-rq-timeout-ms 头值中读取的（假设设置了 respect_expected_rq_timeout）或从路由超时设置中读取（默认为 15 秒）。   x-envoy-upstream-service-time 端点处理请求所花费的时间，以毫秒为单位，以及 Envoy 和上游主机之间的网络延迟。   server 设置为 server_name 字段中指定的值（默认为 envoy）。    根据不同的场景，Envoy 会设置或消费一系列其他头信息。当我们在课程的其余部分讨论这些场景和功能时，我们会引出不同的头信息。\nHeader 清理 Header 清理是一个出于安全原因添加、删除或修改请求 Header 的过程。有一些头信息，Envoy 有可能会进行清理。\n   Header 描述     x-envoy-decorator-operation 覆盖由追踪机制产生的任何本地定义的跨度名称。   x-envoy-downstream-service-cluster 包含调用者的服务集群（对于外部请求则删除）。由 -service-cluster 命令行选项决定，要求 user_agent 设置为 true。   x-envoy-downstream-service-node 和前面的头一样，数值由 --service--node选项决定。   x-envoy-expected-rq-timeout-ms 指定路由器期望请求完成的时间，单位是毫秒。这是从 x-envoy-upstream-rq-timeout-ms 头值中读取的（假设设置了 respect_expected_rq_timeout）或从路由超时设置中读取（默认为 15 秒）。   x-envoy-external-address 受信任的客户端地址（关于如何确定，详见下面的 XFF）。   x-envoy-force-trace 强制收集的追踪。   x-envoy-internal 如果请求是内部的，则设置为 “true”（关于如何确定的细节，见下面的 XFF）。   x-envoy-ip-tags 如果外部地址在 IP 标签中被定义，由 HTTP IP 标签过滤器设置。   x-envoy-max-retries 如果配置了重试策略，重试的最大次数。   x-envoy-retry-grpc-on 对特定 gRPC 状态代码的失败请求进行重试。   x-envoy-retry-on 指定重试策略。   x-envoy-upstream-alt-stat-name Emist 上游响应代码 / 时间统计到一个双统计树。   x-envoy-upstream-rq-per-try-timeout-ms 设置路由请求的每次尝试超时。   x-envoy-upstream-rq-timeout-alt-response 如果存在，在请求超时的情况下设置一个 204 响应代码（而不是 504）。   x-envoy-upstream-rq-timeout-ms 覆盖路由配置超时。   x-forwarded-client-certif 表示一个请求流经的所有客户端 / 代理中的部分证书信息。   x-forwarded-for 表示 IP 地址请求通过了。更多细节见下面的 XFF。   x-forwarded-proto 设置来源协议（http 或 https）。   x-request-id Envoy 用来唯一地识别一个请求。也用于访问日志和追踪。    是否对某个特定的头进行清理，取决于请求来自哪里。Envoy 通过查看 x-forwarded-for 头（XFF）和 internal_address_config 设置来确定请求是外部还是内部。\nXFF XFF 或 x-forwaded-for 头表示请求在从客户端到服务器的途中所经过的 IP 地址。下游和上游服务之间的代理在代理请求之前将最近的客户的 IP 地址附加到 XFF 列表中。\nEnvoy 不会自动将 IP 地址附加到 XFF 中。只有当 use_remote_address（默认为 false）被设置为 true，并且 skip_xff_append 被设置为 false 时，Envoy 才会追加该地址。\n当 use_remote_address 被设置为 true 时，HCM 在确定来源是内部还是外部以及修改头信息时，会使用客户端连接的真实远程地址。这个值控制 Envoy 如何确定可信的客户端地址。\n可信的客户端地址\n可信的客户端地址是已知的第一个准确的源 IP 地址。向 Envoy 代理发出请求的下游节点的源 IP 地址被认为是正确的。\n请注意，完整的 XFF 有时不能被信任，因为恶意的代理可以伪造它。然而，如果一个受信任的代理将最后一个地址放在 XFF 中，那么它就可以被信任。例如，如果我们看一下请求路径 IP1 -\u0026gt; IP2 -\u0026gt; IP3 -\u0026gt; Envoy，IP3 是 Envoy 会认为信任的节点。\nEnvoy 支持通过 original_ip_detection_extensions 字段设置的扩展，以帮助确定原始 IP 地址。目前，有两个扩展：custom_header 和 xff。\n通过自定义头的扩展，我们可以提供一个包含原始下游远程地址的头名称。此外，我们还可以告诉 HCM 将检测到的地址视为可信地址。\n通过 xff 扩展，我们可以指定从 x-forwarded-for 头的右侧开始的额外代理跳数来信任。如果我们将这个值设置为 1 还使用上面的例子，受信任的地址将是 IP2 和 IP3。\nEnvoy 使用可信的客户端地址来确定请求是内部还是外部。如果我们把 use_remote_address 设置为 true，那么如果请求不包含 XFF，并且直接下游节点与 Envoy 的连接有一个内部源地址，那么就认为是内部请求。Envoy 使用 RFC1918 或 RFC4193 来确定内部源地址。\n如果我们把 use_remote_address 设置为 false（默认值），只有当 XFF 包含上述两个 RFC 定义的单一 …","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"0fe93e77f4e191d98c24ec9af4bb72de","permalink":"https://lib.jimmysong.io/envoy-handbook/hcm/header-manipulation/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/envoy-handbook/hcm/header-manipulation/","section":"envoy-handbook","summary":"HCM 支持在加权集群、路由、虚拟主机和 / 或全局配置层面操纵请求和响应头。 注意，我们不能直接从配置中修改所有的 Header，使用 Wasm 扩展的情况除外。然后，我们可以修改 :authority header，例如下面的情况。 不可变的头","tags":["Envoy"],"title":"Header 操作","type":"book"},{"authors":null,"categories":["云原生"],"content":"在前一章中，我们了解了基础架构的各种表示以及围绕其部署工具的各种方法。在本章中，我们将看看如何设计部署和管理基础架构的应用程序。在上一章中我们重点关注基础架构即软件的开放世界，有时称为基础架构即应用。\n在云原生环境中，传统的基础架构运维人员需要转变为基础架构软件工程师。与过去的其他运维角色不同，这仍然是一种新兴的做法。我们迫切需要开始探索这种模式和制定标准。\n基础架构即软件与基础架构即代码之间的根本区别在于，软件会持续运行，并会根据调节器模式创建或改变基础架构，我们将在本章后面对其进行解释。此外，基础架构即软件的新范例是，软件现在与数据存储具有更传统的关系，并公开用于定义所需状态的API。例如，该软件可能会根据数据存储中的需要改变基础架构的表示形式，并且可以很好地管理数据存储本身！希望进行协调的状态更改通过API发送到软件，而不是通过运行静态代码库中的程序。\n迈向基础架构即软件的第一步是让基础架构的运维人员意识到自己是软件工程师。我们热烈欢迎您来到这个领域！先前的工具（例如配置管理）也有类似的改变基础架构运维人员的工作职能的目标，但是运维人员通常只会在狭窄的应用范围内编写有限的DSL（即单一节点抽象）。\n作为一名基础架构工程师，您的任务不仅是掌握设计、管理和运维基础架构的基本原则，还需要具有将您的专业知识封装成坚如磐石的应用程序的能力。这些应用程序代表了我们将要管理和改变的基础架构。\n构建管理基础架构软件工程不是一件容易的事情。我们有管理传统应用的所有问题和担忧，而且我们正处于一个尴尬的境地。基础架构软件工程看上去似乎很荒谬，构建软件来部署基础架构，这样就可以在新创建的基础架构之上运行相同的软件，这很尴尬。\n首先，我们需要了解这个新领域中工程软件的细微差别。我们将研究在云原生社区中得到验证的模式，以了解在应用程序中编写干净和逻辑代码的重要性。但首先，基础架构从哪里来？\n自举问题 1987年3月22日，周日，Richard M. Stallman发送了一封电子邮件到GCC邮件列表，报告成功使用C编译器完成了自行编译：\n 该编译器在68020上编译正确，最近又在vax上进行了编译。最近在68020上正确编译了Emacs，并且还编译了tex-in-C和Kyoto Common Lisp。但是，可能仍然有许多错误，希望你能帮我找到。\n我将离开一个月，所以现在报告的错误将得不到处理。——Richard M. Stallman\n 这是软件历史上的一个重要转折点，因为工程软件首次完成了自举（Bootstrap）。Stallman开创了一个可以自行编译的编译器。即使在哲学上接受这个表述可能也是困难的。\n今天我们正在解决与基础架构相同的问题。工程师必须想办法解决几乎不可能的系统自举问题，并在运行时生效。\n有一种方法是手动创建云计算和基础架构应用程序中的第一个基础架构。尽管这种方法确实有效，但它通常伴随着警告，即运维人员应该在部署更合适的基础架构后销毁初始引导基础架构。这种方法乏味、难以重复且容易出现人为错误。\n解决这个问题的更优雅和云原生方法是做出（通常是正确的）假设，即试图引导基础架构软件的任何人都有本地机器，我们可以利用这个本地机器。现有机器（您的计算机）可作为第一个部署工具，自动在云中创建基础架构。基础架构就位后，您的本地部署工具可以将其自身部署到新创建的基础架构并持续运行。良好的部署工具可以让你在完成后轻松清理。\n在初始基础架构引导问题解决后，我们可以使用基础架构应用程序来引导新的基础架构。现在本地计算机已经被排除在外，现在我们完全运行在云端。\nAPI 在前面的章节中，我们讨论了表示基础架构的各种方法。在本章中，我们将探讨为基础架构提供API的概念。\n当用软件实现API时，很可能会通过数据结构来完成。因此，根据您使用的编程语言，将API视为类、字典、数组、对象或结构体是安全的。\nAPI将是数据值的任意定义，可能是字符串、整数或布尔值。API将通过JSON或YAML格式进行编码和解码甚至可能存储在数据库中。\n对于大多数软件工程师来说，为程序提供可版本化的API是很常见的做法。这允许程序随着时间移动、改变和增长。工程师可以声称支持较旧的API版本并提供向后兼容性保证。在基础架构即软件中，由于这些原因，使用API是首选的。\n寻找一个API作为基础架构的接口是用户使用基础架构即软件的许多线索之一。传统上，基础架构即代码是用户将要管理的基础架构的直接表示，而API是对要管理的具体底层资源之上的抽象。\n最终，API只是代表基础架构的数据结构。\n状态 在基础架构即软件工具的环境中，我们要管理的对象是基础架构。因此，对象状态只是我们的程序对软件的审计表示。\n对象的状态最终将回到基础架构表示的内存中。这些内存中的表示应映射到用于声明基础架构的原始API。审计的API或对象状态通常需要保存。\n存储介质（有时称为状态存储）可用于存储新审计的API。介质可以是任何传统存储系统，例如本地文件系统、云对象存储或数据库。如果数据存储在类似文件系统的存储中，那么该工具将很可能以逻辑方式对数据进行编码，以便可以在运行时轻松对数据进行编码和解码。常见的编码包括JSON、YAML和TOML。\n当设计程序时您可能会想要将用于存储其他数据的特权信息存储起来。这究竟是不是最佳实践具体取决于您的安全性要求以及您计划存储数据的位置。\n记住存储机密可能是一个漏洞，这一点很重要。在设计软件来控制堆栈最基本的部分时，安全性至关重要。所以通常值得额外的努力来确保机密信息是安全的。\n除了存储有关程序和云提供商凭证的元信息之外，工程师还需要存储有关基础架构的信息。重要的是要记住，基础架构将以某种方式呈现，理想情况下，该程序易于解码。记住对系统进行更改不会立即发生，而随着时间的推移也很重要。\n存储这些数据并能够轻松访问是设计基础架构管理应用程序的重要部分。仅基础架构定义很可能就已经是系统中最具智慧价值的部分。我们来看一个基本的例子，看看这些数据和程序如何一起工作。\n重新审视例4-1至4-4，因为它们被用作本章进一步演示的具体例子。\n文件系统状态存储示例\n想象一下，数据存储在一个名为state的目录中。在该目录中，有三个文件：\n meta_information.yaml secrets.yaml infrastructure.yaml  这个简单的数据存储可以准确地封装需要保留的信息，以便有效管理基础架构。\nsecrets.yaml和infrastructure.yaml文件存储基础架构的表示形式，meta_information.yaml文件（示例4-1）存储其他重要信息，例如基础架构上次调配时间，调配时间和日志信息。\n例4-1. state/meta_information.yaml\nlastExecution:exitCode:0timestamp:2017-08-01 15:32:11 +00:00user:krislogFile:/var/log/infra.log第二个文件secrets.yaml保存私人信息，用于在程序执行过程中以任意方式验证（例4-2）。\n重申一下，以这种方式存储机密可能是不安全的。我们仅以secrets.yaml为例。\n例4-2. state/secrets.yaml\napiAccessToken:a8233fc28d09a9c27b2e2fapiSecret:8a2976744f239eaa9287f83b23309023dprivateKeyPath:~/.ssh/id_rsa第三个文件infrastructure.yaml将包含API的编码表示形式，包括使用的API版本（示例4-3）。我们可以在这里找到基础架构表示，例如网络和DNS信息，防火墙规则和虚拟机定义。\n例4-3. state/infrastructure.yaml\nlocation:\u0026#34;San Francisco 2\u0026#34;name:infra1dns:fqdn:infra.example.comnetwork:cidr:10.0.0.0/12serverPools:- bootstrapScript:/opt/infra/bootstrap.shdiskSize:largeworkload:mediummemory:mediumsubnetHostsCount:256firewalls:- rules:- ingressFromPort:22ingressProtocol:tcpingressSource:0.0.0.0/0ingressToPort:22image:ubuntu-16-04-x64起初infrastructure.yaml文件可能看起来只不过是基础架构代码的一个例子。但是，如果仔细观察，您会发现许多定义的指令都是具体基础架构之上的抽象。例如，subnetHostsCount指令是一个整数值并定义了子网中主机的预定数量。该程序将设法为运维人员划分网络中定义的更大的无类别域间路由（CIDR）值。运维人员不会声明子网，只需要声明有多少主机。软件会帮运维人员完成剩下的操作。\n程序运行时可能会更新API并将新的表示写入数据存储区（本案例中仅是一个文件）。继续我们的subnetHostsCount示例，假设程序确实为我们挑选了一个子网CIDR。新的数据结构可能如例4-4所示。\nlocation:\u0026#34;San Francisco 2\u0026#34;name:infra1dns:fqdn:infra.example.comnetwork:cidr:10.0.0.0/12serverPools:- bootstrapScript:/opt/infra/bootstrap.shdiskSize:largeworkload:mediummemory:mediumsubnetHostsCount:256assignedSubnetCIDR:10.0.100.0/24firewalls:- rules:- ingressFromPort:22ingressProtocol:tcpingressSource:0.0.0.0/0ingressToPort:22image:ubuntu-16-04-x64请注意程序如何编写assignedSubnetCIDR指令，而不是由运维人员操作。另外，请记住应用程序如何更新API是用户以软件方式与基础架构进行交互的标志。\n现在，请记住，这只是一个例子，并不一定主张使用抽象计算子网CIDR。不同的用例可能需要在应用程序中进行不同的抽象和实现。关于构建基础架构应用程序的一个好处是，用户可以以任何他们认为可以解决自己问题的方式设计软件。\n数据存储（infrastructure.yaml文件）现在可以被认为是软件工程领域的传统数据存储。也就是说，该程序可以对文件进行完全的写入控制。\n我们会发现，这会带来风险，但对工程师来说也是一个巨大的胜利。基础架构表示不必存储在文件系统的文件中。相反，它可以存储在任何数据存储中，如传统数据库或键/值存储系统。\n为了理解软件如何处理这种新的基础架构表示的复杂性，我们必须理解系统中的两种状态——API形式的预期状态，可在infrastructure.yaml文件中找到，另一种可以在现实（或审计）中观察到的实际状态。\n在这个例子中，软件还没有做任何事情或者采取任何行动，而我们正处于管理时间线的开始。因此，实际状态将是什么都没有，而预期状态将是封装在infrastructure.yaml文件中的任何状态。\n调节器模式 调节器模式（reconciler pattern）是一种软件模式，可用于管理云原生基础架构。该模式强化了基础架构的两种表现形式——第一种是基础架构的实际状态，第二种是基础架构的预期状态。\n调节器模式将迫使工程师以两个独立的途径忘记这些表示，以及实现一个解决方案，以协调实际状态达到预期状态。\n调节器模式可以被认为是一套四种方法和四种哲学规则：\n 所有的输入和输出都使用数据结构。 确保数据结构是不可变的。 保持资源映射简单。 使实际状态符合预期状 …","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"84ca80e9e5a558194c9111d523e6d70e","permalink":"https://lib.jimmysong.io/cloud-native-infra/designing-infrastructure-applicaitons/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/cloud-native-infra/designing-infrastructure-applicaitons/","section":"cloud-native-infra","summary":"在前一章中，我们了解了基础架构的各种表示以及围绕其部署工具的各种方法。在本章中，我们将看看如何设计部署和管理基础架构的应用程序。在上一章中我们重点关注基础架构即软件的开放世界，有时称为基础架构即应用。","tags":["云原生"],"title":"第 4 章：设计基础架构应用程序","type":"book"},{"authors":null,"categories":["安全"],"content":"Kubernetes 可以成为数据和 / 或计算能力盗窃的重要目标。虽然数据盗窃是传统上的主要动机，但寻求计算能力（通常用于加密货币挖掘）的网络行为者也被吸引到 Kubernetes 来利用其底层基础设施。除了资源盗窃，网络行为者还可能针对 Kubernetes 造成拒绝服务。下面的威胁代表了 Kubernetes 集群最可能的破坏源。\n 供应链风险 - 对供应链的攻击载体是多种多样的，并且在减轻风险方面具有挑战性。供应链风险是指对手可能颠覆构成系统的任何元素的风险，包括帮助提供最终产品的产品组件、服务或人员。这可能包括用于创建和管理 Kubernetes 集群的第三方软件和供应商。供应链的潜在威胁会在多个层面上影响 Kubernetes，包括：  容器 / 应用层面 - 在 Kubernetes 中运行的应用及其第三方依赖的安全性，它们依赖于开发者的可信度和开发基础设施的防御能力。来自第三方的恶意容器或应用程序可以为网络行为者在集群中提供一个立足点。 基础设施 - 托管 Kubernetes 的底层系统有其自身的软件和硬件依赖性。系统作为工作节点或控制平面一部分的，任何潜在威胁都可能为网络行为者在集群中提供一个立足点。   恶意威胁行为者 - 恶意行为者经常利用漏洞从远程位置获得访问权。Kubernetes 架构暴露了几个 API，网络行为者有可能利用这些 API 进行远程利用。  控制平面 - Kubernetes 控制平面有各种组件，通过通信来跟踪和管理集群。网络行为者经常利用缺乏适当访问控制的暴露的控制平面组件。 工作节点 - 除了运行容器引擎外，工作者节点还承载着 kubelet 和 kube-proxy 服务，这些都有可能被网络行为者利用。此外，工作节点存在于被锁定的控制平面之外，可能更容易被网络行为者利用。 容器化的应用程序 - 在集群内运行的应用程序是常见的目标。应用程序经常可以在集群之外访问，使它们可以被远程网络行为者接触到。然后，网络行为者可以从已经被破坏的应用出发，或者利用暴露的应用程序的内部可访问资源在集群中提升权限。   内部威胁 - 威胁者可以利用漏洞或使用个人在组织内工作时获得的特权。来自组织内部的个人被赋予特殊的知识和特权，可以用来威胁 Kubernetes 集群。  管理员 - Kubernetes 管理员对运行中的容器有控制权，包括在容器化环境中执行任意命令的能力。Kubernetes 强制的 RBAC 授权可以通过限制对敏感能力的访问来帮助降低风险。然而，由于 Kubernetes 缺乏双人制的完整性控制，即必须有至少一个管理账户才能够获得集群的控制权。管理员通常有对系统或管理程序的物理访问权，这也可能被用来破坏 Kubernetes 环境。 用户 - 容器化应用程序的用户可能有知识和凭证来访问 Kubernetes 集群中的容器化服务。这种程度的访问可以提供足够的手段来利用应用程序本身或其他集群组件。 云服务或基础设施供应商 - 对管理 Kubernetes 节点的物理系统或管理程序的访问可被用来破坏 Kubernetes 环境。云服务提供商通常有多层技术和管理控制，以保护系统免受特权管理员的影响。    ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"6b93b0af0915e17422e6179cefbbc504","permalink":"https://lib.jimmysong.io/kubernetes-hardening-guidance/threat-model/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/kubernetes-hardening-guidance/threat-model/","section":"kubernetes-hardening-guidance","summary":"Kubernetes 可以成为数据和 / 或计算能力盗窃的重要目标。虽然数据盗窃是传统上的主要动机，但寻求计算能力（通常用于加密货币挖掘）的网络行为者也被吸引到 Kubernetes 来利用其底层基础设施。除了资源盗窃，网络行为者还可能针对 Kubernetes 造成","tags":["Kubernetes","安全"],"title":"威胁建模","type":"book"},{"authors":null,"categories":["可观测性"],"content":"第 2 章描述了实现自动分析所需的数据模型，第 4 章描述了支持原生 开源仪表的额外要求，并赋予各角色（应用程序所有者、运维和响应者）自主权。这就是我们对现代可观测性的概念模型。\n在本报告的其余部分，我们描述了这个新模型的一个事实实现，即 OpenTelemetry。这一章描述了构成 OpenTelemetry 遥测管道的所有组件。后面的章节将描述稳定性保证、建议的设置以及 OpenTelemetry 现实中的部署策略。有关该项目的更多细节可以在附录中找到。\n信号 OpenTelemetry 规范被组织成不同类型的遥测，我们称之为信号（signal）。主要的信号是追踪。日志和度量是其他例子。信号是 OpenTelemetry 中最基本的设计单位。\n每一个额外的信号首先是独立开发的，然后与追踪和其他相关信号整合。这种分离允许开发新的、实验性的信号，而不影响已经变得稳定的信号的兼容性保证。\nOpenTelemetry 是一个跨领域的关注点（cross-cutting concern），它在事务通过每个库和服务时追踪其执行。为了达到这个目的，所有的信号都建立在低级别的上下文传播系统之上，该系统为信号提供了一个地方来存储它们需要与当前正在执行的代码相关联的任何事务级数据。因为上下文传播系统与追踪系统是完全分开的，其他跨领域的问题也可以利用它。图 5-1 说明了这个分层结构。\n   图 5-1：所有 OpenTelemetry 信号都建立在一个共享的上下文传播系统之上。其他的，非可观测性的交叉关注也可以使用上下文传播机制来通过分布式系统传输他们的数据。  上下文（Context） 上下文对象是一个与执行上下文相关联的键值存储，例如线程或循环程序。如何实现这一点取决于语言，但 OpenTelemetry 在每种语言中都提供一个上下文对象。\n信号在上下文对象中存储它们的数据。因为 OpenTelemetry 的 API 调用总是可以访问整个上下文对象，所以信号有可能成为集成的，并在上下文共享数据，而不需要改变 API。例如，如果追踪和度量信号都被启用，记录一个度量可以自动创建一个追踪范例。日志也是如此：如果有的话，日志会自动绑定到当前的追踪。\n传播器（Propagator） 为了使分布式追踪发挥作用，追踪上下文必须被参与事务的每个服务所共享。传播器通过序列化和反序列化上下文对象来实现这一点，允许信号在网络工作请求中追踪其事务。\n追踪（Tracing） OpenTelemetry 追踪系统是基于 OpenTracing 和 OpenCensus。这两个系统，以及流行的 Zipkin 和 Jaeger 项目，都是基于谷歌开发的 Dapper 追踪系统。OpenTelemetry 试图与所有这些基于 Dapper 的系统兼容。\nOpenTelemetry 追踪包括一个叫做 链接（link） 的概念，它允许单独的追踪被组合成一个更大的图。这被用来连接事务和后台处理，以及观察大型异步系统，如 Kafka 和 AMQP。\n指标（Metric） 度量指标（metric）是一个很大的话题，包含各种各样的方法和实现。OpenTelemetry 度量信号被设计成与 Prometheus 和 StatsD 完全兼容。\n指标包括追踪样本，自动将指标与产生它们的追踪样本联系起来。手工将指标和追踪联系起来往往是一项繁琐且容易出错的任务，自动执行这项任务将为运维人员节省大量的时间。\n日志（Log） OpenTelemetry 结合了高度结构化的日志 API 和高速日志处理系统。现有的日志 API 可以连接到 OpenTelemetry，避免了对应用程序的重新测量。\n每当它出现的时候，日志就会自动附加到当前的追踪中。这使得事务日志很容易找到，并允许自动分析，以找到同一追踪中的日志之间的准确关联。\nBaggage OpenTelemetry Baggage 是一个简单但通用的键值系统。一旦数据被添加为 Baggage（包袱）它就可以被所有下游服务访问。这允许有用的信息，如账户和项目 ID，在事务的后期变得可用，而不需要从数据库中重新获取它们。例如，一个使用项目 ID 作为索引的前端服务可以将其作为 Baggage 添加，允许后端服务也通过项目 ID 对其跨度和指标进行索引。\n你可以将 Baggage 看做是一种分布式文本的形式。直接放入上下文对象的项目只能在当前服务中访问。与追踪上下文一样，作为 Baggage 添加的项目被作为 header 注入网络请求，允许下游服务提取它们。\n与上下文对象一样，Baggage 本身不是一个可观测性工具。它更像是一个通用的数据存储和传输系统。除了可观测性之外，其他跨领域的工具，例如，功能标记、A/B 测试和认证，可以使用 Baggage 来存储他们需要追踪当前事务的任何状态。\n然而，Baggage 是有代价的。因为每增加一个项目都必须被编码为一个头，每增加一个项目都会增加事务中每一个后续网络请求的大小。这就是为什么我们称它为 Baggage。我建议，Baggage 要少用，作为交叉关注的一部分。Baggage 不应该被用作明确定义的服务 API 的 “方便” 替代品，以明确地向下游应用程序发送参数。\nOpenTelemetry 客户端架构 应用程序通过安装一系列的软件库来检测 OpenTelemetry：API、SDK（软件开发工具包）、SDK 插件和库检测。这套库被称为 OpenTelemetry 客户端。图 5-2 显示了这些组件之间的关系。\n   图 5-2：OpenTelemetry 客户端架构。为了帮助管理依赖性，OpenTelemetry 将实现与仪表使用的 API 分开。  在许多语言中，OpenTelemetry 提供安装程序，这有助于自动安装和设置 OpenTelemetry 客户端。然而，可用的自动化程度取决于语言。在 Java 中，OpenTelemetry 提供了一个 Java 代理，它通过动态地注入所有必要的组件来实现安装的完全自动化。在 Go 中，OpenTelemetry 包必须通过编写代码来安装和初始化，就像任何其他 Go 包一样。Python、Ruby 和 NodeJS 介于两者之间，提供不同程度的自动化。\n在学习 OpenTelemetry 时，了解在你使用的语言中如何设置是很重要的。特别是，一定要学习如何安装仪表，因为不同的语言有很大的不同。\n请查看客户端文档，了解更多的入门细节。\n客户端架构：仪表 API OpenTelemetry API 是指用于编写仪表的一组组件。该 API 被设计成可以直接嵌入到开放源码软件库以及应用程序中。这是 OpenTelemetry 的唯一部分，共享库和应用逻辑应该直接依赖它。\n提供者（Provider） API 与任何实现完全分开。当一个应用程序启动时，可以通过为每个信号注册一个提供者来加载一个实现。提供者成为所有 API 调用的接收者。\n当没有加载提供者时，API 默认为无操作提供者。这使得 OpenTelemetry 仪表化可以安全地包含在共享库中。如果应用程序不使用 OpenTelemetry，API 调用就会变成 no-ops，不会产生任何开销。\n对于生产使用，我们建议使用官方的 OpenTelemetry 提供商，我们称之为 OpenTelemetry SDK。\n 为什么有多个实现方案？\nAPI 和实现的分离有很多好处。但是，如果用户被迫总是安装官方的 OpenTelemetry SDK，这又有什么意义？是否有必要安装另一个实现？SDK 已经具有很强的扩展性。\n我们相信是有的。虽然我们希望 OpenTelemetry 仪表是通用的，但建立一个对所有用例都理想的单一实现是不可能的。尽管我们相信 OpenTelemetry SDK 很好，但也应该有一个选择，那就是使用另一种实现。实现的灵活性是提供通用仪表 API 的一个关键特征。\n首先，这种分离保证了 OpenTelemetry 不会产生无法克服的依赖冲突。我们总是可以选择加载一个包括不同依赖链的实现。\n另一个原因是性能。OpenTelemetry SDK 是一个可扩展的、通用的框架。虽然 SDK 的设计是为了尽可能地提高性能，但扩展性和性能总是要权衡一下的。例如，通过外来函数接口创建与 OpenTelemetry C++ SDK 的绑定，有可能成为 Ruby、Python 和 Node.js 等动态脚本语言的一个非常有效的选择。\n 还有一些流媒体架构显示了有希望的性能提升。在许多这样的优化解决方案中，编写插件和生命周期钩子的能力将受到严重限制；支持这些类型的功能所需的数据结构在这些优化解决方案中是不存在的。归根结底，没有 “完美的实现”；只有权衡。\nAPI/SDK 的分离是一个关键的设计选择，该项目大量使用了这一点。例如，除了 SDK 之外，每一种语言都有一个 no-op 的实现，它是默认安装的。还有一个 Fake/Mock 实现，我们用它来测试。而且，还有可能实现更多创造性的实现。例如，为分布式系统建立开发者工具，如一个实时调试器，它可以跨越网络边界工作。\n客户端架构：SDK OpenTelemetry 项目为 OpenTelemetry API 提供了一个官方实现，我们称之为 OpenTelemetry SDK。该 SDK 通过提供一个插件框架来实现 OpenTelemetry API。下面将介绍追踪 SDK；类似的架构也适用于度量和日志。\n基本数据结构是一个无锁的 SpanData 对象。当用户开始一个跨度时，SpanData 对象被创建，当用户添加属性和事件时，它被自动建立起来。一旦一个跨度结束，SpanData 对象将不再被更新，可以安全地传递给后台线程。\nSDK 的插件架构被组织成一个流水线。对于追踪来说，该管道由一连串的 SpanProcessors 组成。每个处理器对 SpanData 对象进行两次同步访问：一次是在跨度开始时，另一次是在跨度结束后。采样器、日志附加器和数据清洗器是 SpanProcessors 的例子。链中的最后一个处理器通常是一个 BatchSpanProcessor，它管理着一个已完成的跨度的缓冲区。输出器可以连接到 BatchSpanProcessor，通过网络将成批的跨度传递到遥测管道中的下一个服务，通常将它们发送到收集器或直接发送到追踪后端。一旦跨度被导出，管道就完成了，SpanData 对象也被释放。\n采样器（Sampler） OpenTelemetry 提供了几种常见的采样算法，包括前期采样和基于优先级的采样。采样可以帮助控制成本，但它是有代价的：你将会错过数据。在启用任何种类的采样算法之前，重要的是要检查你计划使用的分析工具支持哪些类型的采用。意外的采样可能会破坏某些形式的分析。一些工具需要他们自己的采样插件。例如，AWS X-Ray 使用它自己的采样算法，它可以作为 AWS 特定的采样插件使用。\n导出器（Exporter） OpenTelemetry 为 OTLP（OpenTelemetry Protocol）、Jaeger、Zipkin、Prometheus 和 StatsD 提供导出器。由第三方维护的其他导出器可以在每种语言的 OpenTelemetry-Contrib 资源库中找到。使用 OpenTelemetry 注册表来了解目前有哪些插件可用。\n客户端架构：库仪表化 为了正常工作，OpenTelemetry 需要端到端的工具。这不是可有可无的：如果关键的库不包括仪表，上下文传播将被破坏。\n一般来说，必须检测的库包括 HTTP 客户端、HTTP 服务器、应用框架、消息传递 / 队列系统和数据库客户端。这些库经常在上下文传播中起作用。\n HTTP 客户端必须创建一个客户端 span 来记录请求。客户端还必须使用一个传播器，将当前的上下文作为一组 HTTP 头信息注入到请求中。 HTTP 服务器（应用框架） …","date":1643990400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"0329e26e4b51c83ff6de3f3f8d4fc9ae","permalink":"https://lib.jimmysong.io/opentelemetry-obervability/architectural-overview/","publishdate":"2022-02-05T00:00:00+08:00","relpermalink":"/opentelemetry-obervability/architectural-overview/","section":"opentelemetry-obervability","summary":"第 5 章：OpenTelemetry 架构概述","tags":["OpenTelemetry"],"title":"第 5 章：OpenTelemetry 架构概述","type":"book"},{"authors":null,"categories":["云原生"],"content":"HCM 是一个网络级的过滤器，将原始字节转译成 HTTP 级别的消息和事件（例如，收到的 Header，收到的 Body 数据等）。\nHCM 过滤器还处理标准的 HTTP 功能。它支持访问记录、请求 ID 生成和跟踪、Header 操作、路由表管理和统计等功能。\n从协议的角度来看，HCM 原生支持 HTTP/1.1、WebSockets、HTTP/2 和 HTTP/3（仍在 Alpha 阶段）。\nEnvoy 代理被设计成一个 HTTP/2 复用代理，这体现在描述 Envoy 组件的术语中。\nHTTP/2 术语\n在 HTTP/2 中，流是已建立的连接中的字节的双向流动。每个流可以携带一个或多个消息（message）。消息是一个完整的帧（frame）序列，映射到一个 HTTP 请求或响应消息。最后，帧是 HTTP/2 中最小的通信单位。每个帧都包含一个帧头（frame header），它至少可以识别该帧所属的流。帧可以携带有关 HTTP Header、消息有效载荷等信息。\n无论流来自哪个连接（HTTP/1.1、HTTP/2 或 HTTP/3），Envoy 都使用一个叫做 编解码 API（codec API） 的功能，将不同的线程协议翻译成流、请求、响应等协议无关模型。协议无关的模型意味着大多数 Envoy 代码不需要理解每个协议的具体内容。\nHTTP 过滤器 在 HCM 中，Envoy 支持一系列的 HTTP 过滤器。与监听器级别的过滤器不同，这些过滤器对 HTTP 级别的消息进行操作，而不知道底层协议（HTTP/1.1、HTTP/2 等）或复用能力。\n有三种类型的 HTTP 过滤器。\n 解码器（Decoder）：当 HCM 对请求流的部分进行解码时调用。 编码器（Encoder）：当 HCM 对响应流的部分进行编码时调用。 解码器 / 编码器（Decoder/Encoder）：在两个路径上调用，解码和编码  下图解释了 Envoy 如何在请求和响应路径上调用不同的过滤器类型。\n   请求响应路径及 HTTP 过滤器  像网络过滤器一样，单个的 HTTP 过滤器可以停止或继续执行后续的过滤器，并在单个请求流的范围内相互分享状态。\n数据共享 在高层次上，我们可以把过滤器之间的数据共享分成静态和动态。\n静态包含 Envoy 加载配置时的任何不可变的数据集，它被分成三个部分。\n1. 元数据\nEnvoy 的配置，如监听器、路由或集群，都包含一个metadata数据字段，存储键 / 值对。元数据允许我们存储特定过滤器的配置。这些值不能改变，并在所有请求 / 连接中共享。例如，元数据值在集群中使用子集选择器时被使用。\n2. 类型化的元数据\n类型化元数据不需要为每个流或请求将元数据转换为类型化的类对象，而是允许过滤器为特定的键注册一个一次性的转换逻辑。来自 xDS 的元数据在配置加载时被转换为类对象，过滤器可以在运行时请求类型化的版本，而不需要每次都转换。\n3. HTTP 每路过滤器配置\n与适用于所有虚拟主机的全局配置相比，我们还可以指定每个虚拟主机或路由的配置。每个路由的配置被嵌入到路由表中，可以在 typed_per_filter_config 字段下指定。\n另一种分享数据的方式是使用动态状态。动态状态会在每个连接或 HTTP 流中产生，并且它可以被产生它的过滤器改变。名为 StreamInfo 的对象提供了一种从 map 上存储和检索类型对象的方法。\n过滤器顺序 指定 HTTP 过滤器的顺序很重要。考虑一下下面的 HTTP 过滤器链。\nhttp_filters:- filter_1- filter_2- filter_3一般来说，链中的最后一个过滤器通常是路由器过滤器。假设所有的过滤器都是解码器 / 编码器过滤器，HCM 在请求路径上调用它们的顺序是filter_1、filter_2、filter_3。\n在响应路径上，Envoy 只调用编码器过滤器，但顺序相反。由于这三个过滤器都是解码器 / 编码器过滤器，所以在响应路径上的顺序是 filter_3、filter_2、filter_1。\n内置 HTTP 过滤器 Envoy 已经内置了几个 HTTP 过滤器，如 CORS、CSRF、健康检查、JWT 认证等。你可以在这里找到 HTTP 过滤器的完整列表。\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"67f3f0116dcdf06f07a4196e35b240c9","permalink":"https://lib.jimmysong.io/cloud-native-handbook/service-mesh/http-conneciton-manager/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/cloud-native-handbook/service-mesh/http-conneciton-manager/","section":"cloud-native-handbook","summary":"HCM 是一个网络级的过滤器，将原始字节转译成 HTTP 级别的消息和事件（例如，收到的 Header，收到的 Body 数据等）。 HCM 过滤器还处理标准的 HTTP 功能。它支持访问记录、请求 ID 生成和跟踪、Header 操作、路由表管理和统计","tags":["云原生"],"title":"HTTP 连接管理器介绍","type":"book"},{"authors":null,"categories":["文化"],"content":"有时开发人员会拖延（Pushback）代码审查。他们要么不同意您的建议，要么抱怨您太严格。\n谁是对的？ 当开发人员不同意您的建议时，请先花点时间考虑一下是否正确。通常，他们比你更接近代码，所以他们可能真的对它的某些方面有更好的洞察力。他们的论点有意义吗？从代码健康的角度来看它是否有意义？如果是这样，让他们知道他们是对的，把问题解决。\n但是，开发人员并不总是对的。在这种情况下，审查人应进一步解释为什么认为他们的建议是正确的。好的解释在描述对开发人员回复的理解的同时，还会解释为什么请求更改。\n特别是，当审查人员认为他们的建议会改善代码健康状况时，他们应该继续提倡更改，如果他们认为最终的代码质量改进能够证明所需的额外工作是合理的。提高代码健康状况往往只需很小的几步。\n有时需要几轮解释一个建议才能才能让对方真正理解你的用意。只要确保始终保持礼貌，让开发人员知道你有听到他们在说什么，只是你不同意该论点而已。\n沮丧的开发者 审查者有时认为，如果审查者人坚持改进，开发人员会感到不安。有时候开发人员会感到很沮丧，但这样的感觉通常只会持续很短的时间，后来他们会非常感谢您在提高代码质量方面给他们的帮助。通常情况下，如果您在评论中表现得很有礼貌，开发人员实际上根本不会感到沮丧，这些担忧都仅存在于审核者心中而已。开发者感到沮丧通常更多地与评论的写作方式有关，而不是审查者对代码质量的坚持。\n稍后清理 开发人员拖延的一个常见原因是开发人员（可以理解）希望完成任务。他们不想通过另一轮审查来完成该 CL。所以他们说会在以后的 CL 中清理一些东西，所以您现在应该 LGTM 这个 CL。一些开发人员非常擅长这一点，并会立即编写一个修复问题的后续 CL。但是，经验表明，在开发人员编写原始 CL 后，经过越长的时间这种清理发生的可能性就越小。实际上，通常除非开发人员在当前 CL 之后立即进行清理，否则它就永远不会发生。这不是因为开发人员不负责任，而是因为他们有很多工作要做，清理工作在其他工作中被丢失或遗忘。因此，在代码进入代码库并“完成”之前，通常最好坚持让开发人员现在清理他们的 CL。让人们“稍后清理东西”是代码库质量退化的常见原因。\n如果 CL 引入了新的复杂性，除非是紧急情况，否则必须在提交之前将其清除。如果 CL 暴露了相关的问题并且现在无法解决，那么开发人员应该将 bug 记录下来并分配给自己，避免后续被遗忘。又或者他们可以选择在程序中留下 TODO 的注释并连结到刚记录下的 bug。\n关于严格性的抱怨 如果您以前有相当宽松的代码审查，并转而进行严格的审查，一些开发人员会抱怨得非常大声。通常提高代码审查的速度会让这些抱怨逐渐消失。\n有时，这些投诉可能需要数月才会消失，但最终开发人员往往会看到严格的代码审查的价值，因为他们会看到代码审查帮助生成的优秀代码。而且一旦发生某些事情时，最响亮的抗议者甚至可能会成为你最坚定的支持者，因为他们会看到审核变严格后所带来的价值。\n解决冲突 如果上述所有操作仍无法解决您与开发人员之间的冲突，请参阅 “Code Review 标准”以获取有助于解决冲突的指导和原则。\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"9d9412b0a136bd6c025787338e4b4687","permalink":"https://lib.jimmysong.io/eng-practices/review/reviewer/pushback/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/eng-practices/review/reviewer/pushback/","section":"eng-practices","summary":"有时开发人员会拖延（Pushback）代码审查。他们要么不同意您的建议，要么抱怨您太严格。 谁是对的？ 当开发人员不同意您的建议时，请先花点时间考虑一下是否正确。通常，他们比你更接近代码，所以他们可能真的","tags":null,"title":"处理 Code Review 中的拖延","type":"book"},{"authors":null,"categories":["网络"],"content":"在 Kubernetes 中管理 pod 时，Cilium 将为 Cilium 管理的每个 pod 创建一个 CiliumEndpoint（CEP）的自定义资源定义（CRD）。如果启用了 enable-cilium-endpoint-slice，那么 Cilium 还会创建一个 CiliumEndpointSlice （CES）类型的 CRD，将一组具有相同安全身份的 CEP 对象分组到一个 CES 对象中，并广播 CES 对象来向其他代理传递身份，而不是通过广播 CEP 来实现。在大多数情况下，这减少了控制平面上的负载，可以使用相同的主资源维持更大规模的集群。\n例如：\n$ kubectl get ciliumendpointslices --all-namespaces NAME AGE ces-548bnpgsf-56q9f 171m ces-dy4d8x6j2-qgc2z 171m ces-f6qfylrxh-84vxm 171m ces-k29rv92f5-qb4sw 171m ces-m9gs68csm-w2qg8 171m ","date":1655438400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"584fa33f44b5e303504dd9075c486172","permalink":"https://lib.jimmysong.io/cilium-handbook/kubernetes/ciliumendpointslice/","publishdate":"2022-06-17T12:00:00+08:00","relpermalink":"/cilium-handbook/kubernetes/ciliumendpointslice/","section":"cilium-handbook","summary":"在 Kubernetes 中管理 pod 时，Cilium 将为 Cilium 管理的每个 pod 创建一个 CiliumEndpoint（CEP）的自定义资源定义（CRD）。如果启用了 enable-cilium-endpoint-slice，那么 Cilium 还会创","tags":["Cilium"],"title":"端点切片 CRD","type":"book"},{"authors":null,"categories":["Linux"],"content":"我希望这个简短的报告能让你了解 eBPF 和它的强大之处。我真正希望的是，你已经准备好尝试一些基于 eBPF 的工具！如果你想在技术方面深入研究，可以从 ebpf.io 开始，在那里你会找到更多关于技术和 ebPF 基金会的信息。对于编码实例，可以在 GitHub 上的 ebpf-beginners 仓库里找到。\n为了了解其他人是如何利用 eBPF 工具的，请参加 eBPF Summit 和 Cloud Native eBPF Day 等活动，在这些活动中，用户分享他们的成功和学习经验。还有一个活跃的 Slack 频道 ebpf.io/slack。我希望能在那里见到你！\n","date":1654142400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"f3e0969ad7ff04619fa7d7701801b827","permalink":"https://lib.jimmysong.io/what-is-ebpf/conclusion/","publishdate":"2022-06-02T12:00:00+08:00","relpermalink":"/what-is-ebpf/conclusion/","section":"what-is-ebpf","summary":"我希望这个简短的报告能让你了解 eBPF 和它的强大之处。我真正希望的是，你已经准备好尝试一些基于 eBPF 的工具！如果你想在技术方面深入研究，可以从 ebpf.io 开始，在那里你会找到更多关于技术和 ebPF 基金会的信息。对于编码实例，可","tags":["eBPF"],"title":"第七章：结论","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"容器运行时接口（Container Runtime Interface），简称 CRI。CRI 中定义了 容器 和 镜像 的服务的接口，因为容器运行时与镜像的生命周期是彼此隔离的，因此需要定义两个服务。该接口使用 Protocol Buffer，基于 gRPC，在 Kubernetes v1.10 + 版本中是在 pkg/kubelet/apis/cri/runtime/v1alpha2 的 api.proto 中定义的。\nCRI 架构 Container Runtime 实现了 CRI gRPC Server，包括 RuntimeService 和 ImageService。该 gRPC Server 需要监听本地的 Unix socket，而 kubelet 则作为 gRPC Client 运行。\n   CRI 架构 - 图片来自 kubernetes blog  启用 CRI 除非集成了 rktnetes，否则 CRI 都是被默认启用了，从 Kubernetes 1.7 版本开始，旧的预集成的 docker CRI 已经被移除。\n要想启用 CRI 只需要在 kubelet 的启动参数重传入此参数：--container-runtime-endpoint 远程运行时服务的端点。当前 Linux 上支持 unix socket，windows 上支持 tcp。例如：unix:///var/run/dockershim.sock、 tcp://localhost:373，默认是 unix:///var/run/dockershim.sock，即默认使用本地的 docker 作为容器运行时。\nCRI 接口 Kubernetes 1.9 中的 CRI 接口在 api.proto 中的定义如下：\n// Runtime service defines the public APIs for remote container runtimes service RuntimeService { // Version returns the runtime name, runtime version, and runtime API version.  rpc Version (VersionRequest) returns (VersionResponse) {} // RunPodSandbox creates and starts a pod-level sandbox. Runtimes must ensure  //the sandbox is in the ready state on success.  rpc RunPodSandbox (RunPodSandboxRequest) returns (RunPodSandboxResponse) {} // StopPodSandbox stops any running process that is part of the sandbox and  //reclaims network resources (e.g., IP addresses) allocated to the sandbox.  // If there are any running containers in the sandbox, they must be forcibly  //terminated.  // This call is idempotent, and must not return an error if all relevant  //resources have already been reclaimed. kubelet will call StopPodSandbox  //at least once before calling RemovePodSandbox. It will also attempt to  //reclaim resources eagerly, as soon as a sandbox is not needed. Hence,  //multiple StopPodSandbox calls are expected.  rpc StopPodSandbox (StopPodSandboxRequest) returns (StopPodSandboxResponse) {} // RemovePodSandbox removes the sandbox. If there are any running containers  //in the sandbox, they must be forcibly terminated and removed.  // This call is idempotent, and must not return an error if the sandbox has  //already been removed.  rpc RemovePodSandbox (RemovePodSandboxRequest) returns (RemovePodSandboxResponse) {} // PodSandboxStatus returns the status of the PodSandbox. If the PodSandbox is not  //present, returns an error.  rpc PodSandboxStatus (PodSandboxStatusRequest) returns (PodSandboxStatusResponse) {} // ListPodSandbox returns a list of PodSandboxes.  rpc ListPodSandbox (ListPodSandboxRequest) returns (ListPodSandboxResponse) {} // CreateContainer creates a new container in specified PodSandbox  rpc CreateContainer (CreateContainerRequest) returns (CreateContainerResponse) {} // StartContainer starts the container.  rpc StartContainer (StartContainerRequest) returns (StartContainerResponse) {} // StopContainer stops a running container with a grace period (i.e., timeout).  // This call is idempotent, and must not return an error if the container has  //already been stopped.  // TODO: what must the runtime do after the grace period is reached?  rpc StopContainer (StopContainerRequest) returns (StopContainerResponse) {} // RemoveContainer removes the container. If the container is running, the  //container must be forcibly removed.  // This call is idempotent, and must not return an error if the container has  //already been removed.  rpc RemoveContainer (RemoveContainerRequest) returns (RemoveContainerResponse) {} // ListContainers lists all containers by filters.  rpc ListContainers (ListContainersRequest) returns (ListContainersResponse) {} // ContainerStatus returns status of the container. If the container is not  //present, returns an error.  rpc ContainerStatus (ContainerStatusRequest) returns (ContainerStatusResponse) {} // UpdateContainerResources updates ContainerConfig of the container.  rpc UpdateContainerResources (UpdateContainerResourcesRequest) returns (UpdateContainerResourcesResponse) {} // ExecSync runs a command in a container synchronously.  rpc ExecSync (ExecSyncRequest) returns (ExecSyncResponse) {} // Exec prepares a streaming endpoint to execute a command in the container.  rpc Exec (ExecRequest) returns (ExecResponse) {} // Attach prepares a streaming endpoint to attach to a running container.  rpc Attach (AttachRequest) returns (AttachResponse) {} // PortForward prepares a streaming endpoint to forward ports from a PodSandbox.  rpc PortForward (PortForwardRequest) returns (PortForwardResponse) {} // ContainerStats returns stats of the container. If the container does not  //exist, the call returns an error.  rpc ContainerStats (ContainerStatsRequest) returns (ContainerStatsResponse) {} // ListContainerStats returns stats of all running containers.  rpc ListContainerStats (ListContainerStatsRequest) returns (ListContainerStatsResponse) {} // UpdateRuntimeConfig updates the runtime configuration based on the given request.  rpc UpdateRuntimeConfig (UpdateRuntimeConfigRequest) returns (UpdateRuntimeConfigResponse) {} // Status …","date":1653062400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"c33b071ea8bef2a9080dbf3d59b621a8","permalink":"https://lib.jimmysong.io/kubernetes-handbook/architecture/open-interfaces/cri/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/architecture/open-interfaces/cri/","section":"kubernetes-handbook","summary":"容器运行时接口（Container Runtime Interface），简称 CRI。CRI 中定义了 容器 和 镜像 的服务的接口，因为容器运行时与镜像的生命周期是彼此隔离的，因此需要定义两个服务。该接口使用 Protocol Buffer，","tags":["Kubernetes"],"title":"容器运行时接口（CRI）","type":"book"},{"authors":null,"categories":["安全"],"content":"由于参考平台是由容器编排和资源管理平台以及服务网格软件组成的，以下出版物为确保该平台的安全提供了指导，并为本文件的内容提供了背景信息。\n SP800-204，基于微服务的应用系统的安全策略，讨论了基于微服务的应用的特点和安全要求，以及满足这些要求的总体策略。 SP800-204A，使用服务网格构建基于微服务的安全应用，为基于微服务的应用的各种安全服务（如建立安全会话、安全监控等）提供了部署指导，这些服务使用基于独立于应用代码运行的服务代理的专用基础设施（即服务网格）。 SP800-204B，使用服务网格的基于微服务的应用的基于属性的访问控制，为在服务网格中构建满足安全要求的认证和授权框架提供了部署指导，例如：（1）通过在任何一对服务之间的通信中实现相互认证来实现零信任；（2）基于访问控制模型，如基于属性的访问控制（ABAC）模型的强大访问控制机制，可用于表达广泛的策略集，并在用户群、对象（资源）和部署环境方面可扩展。 SP800-190，应用容器安全指南，解释了与容器技术相关的安全问题，并为在规划、实施和维护容器时解决这些问题提出了实用建议。这些建议是针对容器技术架构中的每个层级提供的。  ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"6c7973f466ff63332ece71203b226d4e","permalink":"https://lib.jimmysong.io/service-mesh-devsecops/intro/relationship-to-other-nist-guidance-documents/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/service-mesh-devsecops/intro/relationship-to-other-nist-guidance-documents/","section":"service-mesh-devsecops","summary":"由于参考平台是由容器编排和资源管理平台以及服务网格软件组成的，以下出版物为确保该平台的安全提供了指导，并为本文件的内容提供了背景信息。 SP800-204，基于微服务的应用系统的安全策略，讨论了基于微服","tags":["Service Mesh","DevSecOps"],"title":"1.4 与其他 NIST 指导文件的关系","type":"book"},{"authors":null,"categories":["安全"],"content":"下一个常见问题涉及工作流模型。所有的 CI/CD 管道都可以有两种类型的工作流程模型，这取决于作为管道一部分部署的自动化工具。\n 基于推的模式 基于拉的模式  在支持基于推模式的 CI/CD 工具中，在管道的一个阶段或阶段所做的改变会触发后续阶段或阶段的改变。例如，通过一系列的编码脚本，CI 系统中的新构建会触发管道中 CD 部分的变化，从而改变部署基础设施（如 Kubernetes 集群）。使用 CI 系统作为部署变化的基础，其安全方面的缺点是有可能将凭证暴露在部署环境之外，尽管已尽最大努力确保 CI 脚本的安全，因为 CI 脚本是在部署基础设施的信任域之外运行的。由于 CD 工具拥有生产系统的 key，基于推送的模式就变得不安全了。\n在基于拉的工作流程模型中，与部署环境有关的运维（例如 Kubernetes 运维、Flux、ArgoCD）一旦观察到有新镜像被推送到注册表，就会从环境内部拉动新镜像。新镜像被从注册表中拉出，部署清单被自动更新，新镜像被部署在环境（如集群）中。因此，实际的部署基础设施状态与 Git 部署库中声明性描述的状态实现了衔接。此外，部署环境凭证（例如集群凭证）不会暴露在生产环境之外。因此，强烈建议采用基于拉的模式，即通常使用 GitOps 仓库来存储源代码和构建。\n4.7.1 GitsOps 的 CI/CD 工作流程模型 —— 基于拉的模型 GitOps 工作流模型是对 CI/CD 管道的改进（针对管道的交付部分），它使用了基于拉的工作流模型，而不是许多 CI/CD 工具支持的基于推的模型。在这个模型中，流水线的 CI 部分没有变化，因为 CI 引擎（如 Jenkins、GitLab CI）仍然用于为修改后的代码创建构建。\n回归测试，以及与相关存储库中的主要源代码集成 / 合并，尽管它不用于在管道中触发持续交付（直接推送更新）。\n相反，一个单独的 GitOps Operator 根据主干代码的更新来管理部署。\nOperator（例如，Flux、ArgoCD）是一个由协调平台管理的行为体，可以继承集群的配置、安全和可用性。使用这种行为体可以提高安全性，因为在集群内部的代理会监听它被允许访问的所有代码和镜像仓库的更新，并将镜像和配置更新拉入集群。代理使用的拉方式具有以下安全特性：\n 只执行协调平台中定义的授权策略所允许的操作；信任与集群共享，不单独管理。 与所有协调平台对象进行原生绑定，并了解操作是否已经完成或需要重试。  ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"de6ec5e6d15667ae05d6a9620dd33ecd","permalink":"https://lib.jimmysong.io/service-mesh-devsecops/implement/workflow-models-in-ci-cd-pipelines/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/service-mesh-devsecops/implement/workflow-models-in-ci-cd-pipelines/","section":"service-mesh-devsecops","summary":"下一个常见问题涉及工作流模型。所有的 CI/CD 管道都可以有两种类型的工作流程模型，这取决于作为管道一部分部署的自动化工具。 基于推的模式 基于拉的模式 在支持基于推模式的 CI/CD 工具中，在管道的一个阶段或阶段所做的改变会","tags":["Service Mesh","DevSecOps"],"title":"4.7 CI/CD 管道中的工作流模型","type":"book"},{"authors":null,"categories":["安全"],"content":"Pod 是 Kubernetes 中最小的可部署单元，由一个或多个容器组成。Pod 通常是网络行为者在利用容器时的初始执行环境。出于这个原因，Pod 应该被加固，以使利用更加困难，并限制成功入侵的影响。\n   图3：有 sidecar 代理作为日志容器的 Pod 组件  “非 root” 容器和 “无 root” 容器引擎 默认情况下，许多容器服务以有特权的 root 用户身份运行，应用程序在容器内以 root 用户身份执行，尽管不需要有特权的执行。\n通过使用非 root 容器或无 root 容器引擎来防止 root 执行，可以限制容器受损的影响。这两种方法都会对运行时环境产生重大影响，因此应该对应用程序进行全面测试，以确保兼容性。\n非 root 容器：容器引擎允许容器以非 root 用户和非 root 组成员身份运行应用程序。通常情况下，这种非默认设置是在构建容器镜像的时候配置的。附录 A：非 root 应用的 Dockerfile 示例 显示了一个 Dockerfile 示例，它以非 root 用户身份运行一个应用。\n非 root 用户。另外，Kubernetes 可以在 SecurityContext:runAsUser 指定一个非零用户的情况下，将容器加载到 Pod。虽然 runAsUser 指令在部署时有效地强制非 root 执行，但 NSA 和 CISA 鼓励开发者构建的容器应用程序，以非 root 用户身份执行。在构建时集成非 root 用户执行，可以更好地保证应用程序在没有 root 权限的情况下正常运行。\n无 root 的容器引擎：一些容器引擎可以在无特权的上下文中运行，而不是使用以 root 身份运行的守护程序。在这种情况下，从容器化应用程序的角度来看，执行似乎是使用 root 用户，但执行被重新映射到主机上的引擎用户上下文。虽然无 root 容器引擎增加了一个有效的安全层，但许多引擎目前是作为实验性发布的，不应该在生产环境中使用。管理员应该了解这一新兴技术，并在供应商发布与 Kubernetes 兼容的稳定版本时寻求采用无 root 容器引擎。\n不可变的容器文件系统 默认情况下，容器在自己的上下文中被允许不受限制地执行。在容器中获得执行权限的网络行为者可以在容器中创建文件、下载脚本和修改应用程序。Kubernetes 可以锁定一个容器的文件系统，从而防止许多暴露后的活动。\n然而，这些限制也会影响合法的容器应用程序，并可能导致崩溃或异常行为。为了防止损害合法的应用程序，Kubernetes 管理员可以为应用程序需要写访问的特定目录挂载二级读 / 写文件系统。附录 B：只读文件系统的部署模板示例 显示了一个具有可写目录的不可变容器的例子。\n构建安全的容器镜像 容器镜像通常是通过从头开始构建容器或在从存储库中提取的现有镜像基础上创建的。除了使用可信的存储库来构建容器外，镜像扫描是确保部署的容器安全的关键。在整个容器构建工作流程中，应该对镜像进行扫描，以识别过时的库、已知的漏洞或错误配置，如不安全的端口或权限。\n   图4：容器的构建工作流程，用 webhook 和准入控制器进行优化  实现镜像扫描的一种方法是使用准入控制器。准入控制器是 Kubernetes 的原生功能，可以在对象的持久化之前，但在请求被验证和授权之后，拦截和处理对 Kubernetes API 的请求。可以实现一个自定义或专有的 webhook，以便在集群中部署任何镜像之前执行扫描。如果镜像符合 webhook 配置中定义的组织的安全策略，这个准入控制器可以阻止部署。\nPod 安全策略  Pod 的创建应遵守最小授权原则。\n Pod 安全策略（PSP）1是一个集群范围内的策略，它规定了 Pod 在集群内执行的安全要求 / 默认值。虽然安全机制通常是在 Pod/Deployment 配置中指定的，但 PSP 建立了一个所有 Pod 必须遵守的最低安全门槛。一些 PSP 字段提供默认值，当 Pod 的配置省略某个字段时使用。其他 PSP 字段被用来拒绝创建不符合要求的 Pod。PSP 是通过 Kubernetes 准入控制器执行的，所以 PSP 只能在 Pod 创建期间执行要求。PSP 并不影响已经在集群中运行的 Pod。\nPSP 很有用，可以在集群中强制执行安全措施。PSP 对于由具有分层角色的管理员管理的集群特别有效。在这些情况下，顶级管理员可以施加默认值，对低层级的管理员强制执行要求。NSA 和 CISA 鼓励企业根据自己的需要调整 附录 C：Pod 安全策略示例 中的 Kubernetes 加固 PSP 模板。下表描述了一些广泛适用的 PSP 组件。\n表 1: Pod 安全策略组件\n   字段名称 使用方法 建议     privileged 控制 Pod 是否可以运行有特权的容器。 设置为 false。   hostPID、hostIPC 控制容器是否可以共享主机进程命名空间。 设置为 false。   hostNetwork 控制容器是否可以使用主机网络。 设置为 false。   allowedHostPaths 将容器限制在主机文件系统的特定路径上。 使用一个 “假的” 路径名称（比如 /foo 标记为只读）。省略这个字段的结果是不对容器进行准入限制。   readOnlyRootFilesystem 需要使用一个只读的根文件系统。 可能时设置为 true。   runAsUser, runAsGroup, supplementalGroups, fsGroup 控制容器应用程序是否能以 root 权限或 root 组成员身份运行。 - 设置 runAsUser 为 MustRunAsNonRoot。- 将 runAsGroup 设置为非零（参见附录 C 中的例子：Pod 安全策略示例）。     将 supplementalGroups 设置为非零（见附录 C 的例子）。将 fsGroup 设置为非零（参见附录 C 中的例子：Pod 安全策略示例）。   allowPrivilegeEscalation 限制升级到 root 权限。 设置为 false。为了有效地执行 runAsUser: MustRunAsNonRoot 设置，需要采取这一措施。   seLinux 设置容器的 SELinux 上下文。 如果环境支持 SELinux，可以考虑添加 SELinux 标签以进一步加固容器。   AppArmor 注解 设置容器所使用的 AppArmor 配置文件。 在可能的情况下，通过采用 AppArmor 来限制开发，以加固容器化的应用程序。   seccomp 注解 设置用于沙盒容器的 seccomp 配置文件。 在可能的情况下，使用 seccomp 审计配置文件来识别运行中的应用程序所需的系统调用；然后启用 seccomp 配置文件来阻止所有其他系统调用。    注意：由于以下原因，PSP 不会自动适用于整个集群：\n 首先，在应用 PSP 之前，必须为 Kubernetes 准入控制器启用 PodSecurityPolicy 插件，这是 kube-apiserver 的一部分。 第二，策略必须通过 RBAC 授权。管理员应从其集群组织内的每个角色中验证已实施的 PSP 的正确功能。  在有多个 PSP 的环境中，管理员应该谨慎行事，因为 Pod 的创建会遵守最小限制性授权策略。以下命令描述了给定命名空间的所有 Pod 安全策略，这可以帮助识别有问题的重叠策略。\nkubectl get psp -n \u0026lt;namespace\u0026gt; 保护 Pod 服务账户令牌 默认情况下，Kubernetes 在创建 Pod 时自动提供一个服务账户（Service Account），并在运行时在 Pod 中挂载该账户的秘密令牌（token）。许多容器化的应用程序不需要直接访问服务账户，因为 Kubernetes 的协调工作是在后台透明进行的。如果一个应用程序被破坏了。Pod 中的账户令牌可以被网络行为者收集并用于进一步破坏集群。当应用程序不需要直接访问服务账户时，Kubernetes 管理员应确保 Pod 规范禁用正在加载的秘密令牌。这可以通过 Pod 的 YAML 规范中的 automountServiceAccountToken: false 指令来完成。\n加固容器引擎 一些平台和容器引擎提供了额外的选项来加固容器化环境。一个强有力的例子是使用管理程序来提供容器隔离。管理程序依靠硬件来执行虚拟化边界，而不是操作系统。管理程序隔离比传统的容器隔离更安全。在 Windows® 操作系统上运行的容器引擎可以被配置为使用内置的 Windows 管理程序 Hyper-V®，以增强安全性。\n此外，一些注重安全的容器引擎将每个容器部署在一个轻量级的管理程序中，以实现深度防御。由管理程序支持的容器可以减少容器的突破。\n  译者注：Pod Security Policy 已在 1.21 版本中宣布弃用，作为替代，1.22 引入了内置的 Pod Security Admission 控制器以及新的 Pod Security Standards 标准。来源 ↩︎\n   ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"b8d62954b039b8bcb7e7631228ca435d","permalink":"https://lib.jimmysong.io/kubernetes-hardening-guidance/kubernetes-pod-security/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/kubernetes-hardening-guidance/kubernetes-pod-security/","section":"kubernetes-hardening-guidance","summary":"Pod 是 Kubernetes 中最小的可部署单元，由一个或多个容器组成。Pod 通常是网络行为者在利用容器时的初始执行环境。出于这个原因，Pod 应该被加固，以使利用更加困难，并限制成功入侵的影响。 图3：有 sidecar 代理作为日志容器的 Pod","tags":["Kubernetes","安全"],"title":"Kubernetes Pod 安全","type":"book"},{"authors":null,"categories":["云原生"],"content":"在构建应用程序以管理基础架构时，我们要将需要公开的 API 与要创建的应用程序等量看待。这些 API 将代表您的基础架构的抽象，而应用程序将使用 API 消费这些这些基础架构。\n务必牢牢掌握两者的重要性，和如何利用它们来创建可扩展的弹性基础架构。\n在本章中，我们将举一个虚构的云原生应用程序和 API 示例，这些应用程序和 API 会经历正常的应用程序周期。如果您想了解更多有关管理云原生应用程序的信息，请参阅第 7 章。\n设计 API  这里的 API 是指处理数据结构中的基础架构表示，而不关心如何暴露或消费这些 API。通常使用 HTTP RESTful 端点来传递数据结构，API 如何实现对本章并不重要。\n 随着基础架构的不断发展，运行在基础架构之上的应用程序也要随之演变。为这些应用程序的功能将随着时间而改变，因此基础架构是也是隐性地演变。随着基础架构的不断发展，管理它的应用程序也必须发展。\n基础架构的功能、需求和发展将永无止境。如果幸运的话，云供应商的 API 将会保持稳定，不会频繁更改。作为基础架构工程师，我们需要做好准备，以适应这些需求。我们需要准备好发展我们的基础架构和运行其上的应用程序。\n我们必须创建可缩放的应用程序，并准备对其进行扩展。为了做到这一点，我们需要了解在不破坏应用程序现有流程的情况下对应用程序进行大量更改的细微差别。\n管理基础架构的工程应用的好处在于它解放了运维人员的生产力。\n应用程序中使用的抽象现在由工程师来完成。我们可以详尽或抽象的描述 API，这都可以。通过具体和抽象定义的强大组合可以帮助运维人员准确地描述他们需要管理基础架构。\n添加功能 根据功能的性质，向基础架构应用程序添加功能可能非常简单也可能非常复杂。添加功能的目标是能够添加新功能而不会危害现有功能。我们绝不希望引入会以给系统其他组件带来负面影响的功能。此外，我们一直希望确保系统输入在合理的时间内保持有效。\n例 5-1 是本书前面介绍的基础架构 API 演化的具体示例。我们称之为 API v1。\n例 5-1. v1.json\n{ \u0026#34;virtualMachines\u0026#34;: [{ \u0026#34;name\u0026#34;: \u0026#34;my-vm\u0026#34;, \u0026#34;size\u0026#34;: \u0026#34;large\u0026#34;, \u0026#34;localIp\u0026#34;: \u0026#34;10.0.0.111\u0026#34;, \u0026#34;subnet\u0026#34;: \u0026#34;my-subnet\u0026#34; }], \u0026#34;subnets\u0026#34;: [{ \u0026#34;name\u0026#34;: \u0026#34;my-subnet\u0026#34;, \u0026#34;cidr\u0026#34;: \u0026#34;10.0.100.0/24\u0026#34; }] } 想象一下，我们希望实现一项功能，允许基础架构运维人员为虚拟机定义 DNS 记录。新的 API 看起来略有不同。在例 5-2 中，我们将定义一个名为 version 的顶级指令，告诉应用程序这是 API 的 v2 版本。我们还将添加一个新的块，用于在虚拟机块的上下文中定义 DNS 记录。这是 v1 中不支持的新指令。\n例 5-2. v2.json\n{ \u0026#34;version\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;virtualMachines\u0026#34;: [{ \u0026#34;name\u0026#34;: \u0026#34;my-vm\u0026#34;, \u0026#34;size\u0026#34;: \u0026#34;large\u0026#34;, \u0026#34;localIp\u0026#34;: \u0026#34;10.0.0.111\u0026#34;, \u0026#34;subnet\u0026#34;: \u0026#34;my-subnet\u0026#34;, \u0026#34;dnsRecords\u0026#34;: [{ \u0026#34;type\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;ttl\u0026#34;: 60, \u0026#34;value\u0026#34;: \u0026#34;my-vm.example.com\u0026#34; }] }], \u0026#34;subnets\u0026#34;: [{ \u0026#34;name\u0026#34;: \u0026#34;my-subnet\u0026#34;, \u0026#34;cidr\u0026#34;: \u0026#34;10.0.100.0/24\u0026#34; }] } 这两个对象都是有效的，应用程序应该继续支持它们。应用程序应检测到 v2 对象是否打算使用内置于应用程序中的 DNS 新功能。该应用程序应该足够聪明，以适当地导航到新功能。将资源应用于云时，新的 v2 对象的资源集将与第一个 v1 对象相同，但添加了单个 DNS 资源。\n这引入了一个有趣的问题：应用程序应该如何处理旧的 API 对象？应用程序应仍可以在云中创建资源，且支持无 DNS 的虚拟机。\n随着时间的推移，运维人员可以修改现有虚拟机对象以使用新的 DNS 功能。应用程序自动检测到增量并为新功能创建 DNS 记录。\n弃用功能 让我们快速转到下一个 API v3。在这种情况下，我们的 API 不断发展，我们在表示 IP 地址方面已陷入僵局。\n在 API v1 中，我们能够通过本地 IP 指令方便地为网络接口声明一个本地 IP 地址。我们现在的任务是为虚拟机提供多种网络接口。需要注意的是，这将与最初的 API v1 冲突。\n让我们来看一下示例 5-3 中新的 v3 版本的 API。\n例 5-3. v3.json\n{ \u0026#34;version\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;virtualMachines\u0026#34;: [{ \u0026#34;name\u0026#34;: \u0026#34;my-vm\u0026#34;, \u0026#34;size\u0026#34;: \u0026#34;large\u0026#34;, \u0026#34;networkInterfaces\u0026#34;: [{ \u0026#34;type\u0026#34;: \u0026#34;local\u0026#34;, \u0026#34;ip\u0026#34;: \u0026#34;10.0.0.11\u0026#34; }], \u0026#34;subnet\u0026#34;: \u0026#34;my-subnet\u0026#34;, \u0026#34;dnsRecords\u0026#34;: [{ \u0026#34;type\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;ttl\u0026#34;: 60, \u0026#34;value\u0026#34;: \u0026#34;my-vm.example.com\u0026#34; }] }], \u0026#34;subnets\u0026#34;: [{ \u0026#34;name\u0026#34;: \u0026#34;my-subnet\u0026#34;, \u0026#34;cidr\u0026#34;: \u0026#34;10.0.100.0/24\u0026#34; }] } 使用定义多个网络接口所需的新数据结构，我们已弃用本地 IP 指令。但是我们并没有删除定义 IP 地址的概念，我们只是简单地重组了它。这意味着我们可以分两个阶段废弃该指令。首先警告，然后是拒绝。\n在警告阶段，我们的应用程序可能会输出不再支持本地 IP 指令的警告。应用程序可以接受在对象中定义的指令，并将旧 API v2 转换为新 API v3。\n转换将采用为本地 IP 定义的值，并在新网络接口指令中创建与初始值相匹配的单个块。应用程序可以继续处理 API 对象，就好像用户发送了 v3 对象而不是 v2 对象一样。预计用户会注意到该指令已被弃用，并及时更新其表示。\n在拒绝阶段，应用程序将彻底拒绝 v2 API。用户将被迫更新他们的 API 到更新的版本，或者甘愿在基础架构中冒此风险。\n 弃用是非常危险的\n这是一个极其危险的过程，成功将用户引导到新版本可能会非常困难。拒绝输入必须给出很好的理由。\n如果输入信息的会破坏应用程序保障，则应拒绝该信息。否则，最佳实践通常是警告并继续。\n破坏用户的输入很容易让运维人员感到不安和沮丧。\n 基础架构工程师在对 API 进行版本控制时，必须对在何时弃用哪些功能做出最佳判断。此外，工程师需要花时间给出巧妙的解决方案，这些方案可以是警告或转换。在某些情况下，做到悄无声息的 API 转换对不断发展的云原生基础架构来说是一个巨大的胜利。\n基础架构变异 基础架构需要随着时间的推移而变化。这是云原生环境的本质。不仅应用程序频繁部署，而且运行基础架构的云供应商也在不断变化。\n基础架构的变化可以有多种形式，比如扩大或缩小基础架构，复制整个环境或消费新资源。\n当运维人员承担变更基础架构的任务时，我们可以看到 API 的真实价值。假设我们想要扩展环境中的虚拟机数量。不需要更改 API 版本，但对基础架构的表示做一些小的调整将很快反映出变化。就这么简单。\n然而，重要的是要记住，在这种情况下，运维可能是一个人，也可能是另一个软件。\n请记住，我们故意将 API 构造成易于被计算机解码。我们可以在 API 的两端使用该软件！\n 使用 Operator 消费和生产 API\nOperater—— 构建云原生产品和平台的 CoreOS 公司创造了这个术语，即 Kubernetes 控制器，实现了软件取代人类参与管理特定应用的需求。通过协调预期状态和设定预期状态来实现。\nCoreOS 在他们的博客文章中这样描述 Operator：\nOperator 是特定应用程序的控制器，它代表 Kubernetes 用户扩展 Kubernetes API 以创建、配置和管理复杂有状态应用程序的实例。它建立在基本的 Kubernetes 资源和控制器概念的基础上，但包含一个域或特定于应用程序的知识体系以实现常见任务的自动化。\n该模式规定 Operator 可以通过给定声明性指令集来更改环境。Operator 是工程师应该创建的用于管理其基础架构的云原生应用程序类型的完美示例。\n 设想一个简单的情景 —— 自动调节器（autoscaler）。假设我们有一个非常简单的软件，可以检查环境中虚拟机上的平均负载。我们可以定义一个规则，只要平均负载平均值高于 0.7，我们就需要创建更多的虚拟机来均匀地分配我们的负载。\nOperator 的规则会随着负载平均值的增加而不再适用，最终 Operator 需要用另一台虚拟机更新基础架构 API。这样可以扩大我们的基础架构，但同样我们也可以很容易的定义另一个规则当平均负载降至 0.2 以下时缩小虚拟机规模。请注意，Operator 这个术语在这里应该是一个应用程序，而不是一个人。\n这是自动缩放的一个非常原始的例子，但该模式清楚地表明软件现在可以开始扮演人类运维人员的角色。\n有许多工具可以帮助扩展如 Kubernetes、Nomad 和 Mesos 等基础架构上的应用程序负载。这假定应用程序层运行一个编排调度器上，它将为我们管理应用程序负载。\n想象一下，如果多个基础架构管理应用程序使用相同的 API，那么会进一步将基础架构 API 的价值最大化。这是一个非常强大的基础架构演进模式。\n我们来看看相同的 API—— 记住它只有几千字节的数据，并且在两个独立的基础架构管理应用程序运行。图 5-1 显示了一个示例，两个基础架构应用程序从相同的 API 获取数据但将基础架构部署到各自独立的云环境。\n   图 5-1. 一个 API 被部署在两个云中  该模型为基础架构工程师提供了在多个云提供商之间提供通用抽象的强大功能。现在我们可以看到应用程序如何确保 API 在多个地方代表相同的基础架构。如果基础架构 API 负责提供自己的抽象和资源调配，则基础架构不必与单个云提供商的抽象相关联。用户可以在他们选择的云中创建独特的基础架构排列。\n维护云提供商兼容性 虽然保持 API 与云提供商的兼容性将会有很多工作要做，但对于部署工作流程和供应流程时，很少需要改变。请记住，人类比技术更难改变。如果您可以保证人类的环境一致，这将抵消所需的技术开销。\n您还应该权衡多云兼容性的好处。如果它不是您的基础架构的需求，您可以节省大量的工作。考虑云厂商锁定时请参阅附录 B。\n我们也可以在同一个云中运行不同的基础架构管理应用程序。这些应用程序可能会对 API 进行不同的解释，这会导致对于运维人员的意图的定义略有不同。根据运维人员定义的基础架构的意图，在管理应用程序之间进行切换可能只是我们所需要的。图 5-2 显示了两个应用程序正在读取相同的 API 源，但在实现数据时会根据环境和需要而不同。\n   图 5-2. 一个 API 以不同的方式部署在同一个云中  结论 与基础架构 API 相比，基础架构应用的排列组合是无止境的。这为基础架构工程师提供了一个非常灵活和可扩展的解决方案，希望能够以不同的环境和方式掌握基础架构。\n我们为满足基础架构要求而可能构建的各种应用程序现在已成为基础架构本身的代表。这是第 3 章中定义的基础设施即软件的缩影。\n请务必记住，我们构建的应用程序本身就是云原生应用程序。这是一个有趣的故事，因为我们正在构建云原生应用程序来管理云原生基础架构。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"301fee696476bf4e66d41d6e403369a0","permalink":"https://lib.jimmysong.io/cloud-native-infra/developing-infrastructure-applications/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/cloud-native-infra/developing-infrastructure-applications/","section":"cloud-native-infra","summary":"在构建应用程序以管理基础架构时，我们要将需要公开的 API 与要创建的应用程序等量看待。这些 API 将代表您的基础架构的抽象，而应用程序将使用 API 消费这些这些基础架构。 务必牢牢掌握两者的重要性，和如何利用它们来创建可","tags":["云原生"],"title":"第 5 章：开发基础架构应用程序","type":"book"},{"authors":null,"categories":["Envoy"],"content":"HCM 支持修改和定制由 Envoy 返回的响应。请注意，这对上游返回的响应不起作用。\n本地回复是由 Envoy 生成的响应。本地回复的工作原理是定义一组映射器（mapper），允许过滤和改变响应。例如，如果没有定义任何路由或上游集群，Envoy 会发送一个本地 HTTP 404。\n每个映射器必须定义一个过滤器，将请求属性与指定值进行比较（例如，比较状态代码是否等于 403）。我们可以选择从多个过滤器来匹配状态代码、持续时间、Header、响应标志等。\n除了过滤器字段，映射器还有新的状态代码（status_code）、正文（body 和 body_format_override）和 Header（headers_to_add）字段。例如，我们可以有一个匹配请求状态代码 403 的过滤器，然后将状态代码改为 500，更新正文，或添加 Header。\n下面是一个将 HTTP 503 响应改写为 HTTP 401 的例子。注意，这指的是 Envoy 返回的状态代码。例如，如果上游不存在，Envoy 将返回一个 503。\n...- name:envoy.filters.network.http_connection_managertyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManagerlocal_reply_config:mappers:- filter:status_code_filter:comparison:op:EQvalue:default_value:503runtime_key:some_keyheaders_to_add:- header:key:\u0026#34;service\u0026#34;value:\u0026#34;unavailable\u0026#34;append:falsestatus_code:401body:inline_string:\u0026#34;Not allowed\u0026#34; 注意 runtime_key 字段是必须的。如果 Envoy 找不到运行时密钥，它就会返回到 default_value。\n ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"4403512ad89d569960782b15810d1619","permalink":"https://lib.jimmysong.io/envoy-handbook/hcm/reply-modification/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/envoy-handbook/hcm/reply-modification/","section":"envoy-handbook","summary":"HCM 支持修改和定制由 Envoy 返回的响应。请注意，这对上游返回的响应不起作用。 本地回复是由 Envoy 生成的响应。本地回复的工作原理是定义一组映射器（mapper），允许过滤和改变响应。例如，如果没有定义任何路由或上游集","tags":["Envoy"],"title":"修改响应","type":"book"},{"authors":null,"categories":["可观测性"],"content":"OpenTelemetry 被设计成允许长期稳定性和不确定性并存的局面。在 OpenTelemetry 中，稳定性保证是在每个信号的基础上提供的。与其看版本号，不如检查你想使用的信号的稳定性等级。\n信号生命周期 图 6-1 显示了新信号是如何被添加到 OpenTelemetry 的。实验性信号仍在开发中。它们可能在任何时候改变并破坏兼容性。实验性信号的开发是以规范提案的形式开始的，它是与一组原型一起开发的。一旦实验性信号准备好在生产中使用，信号的特性就会被冻结，新信号的测试版就会以多种语言创建。测试版可能不是完整的功能，它们可能会有一些突破性的变化，但它们被认为是为早期采用者的产品反馈做好准备。一旦一个信号被认为可以被宣布为稳定版本，就会发布一个候选版本。如果候选版本能够在一段时间内保持稳定，没有问题，那么该信号的规范和测试版都被宣布为稳定。\n一旦一个信号变得稳定，它就属于 OpenTelemetry 的长期支持保障范围。OpenTelemetry 非常重视向后兼容和无缝升级。详情见以下章节。\n如果 OpenTelemetry 信号的某个组件需要退役，该组件将被标记为废弃的。被废弃的组件不再获得新的功能，但它们仍然被 OpenTelemetry 的长期支持保证所覆盖。如果可能的话，该组件将永远不会被删除，并将继续发挥作用。如果一个组件必须被删除，将提前宣布删除日期。\n实验性的功能总是与稳定性的功能保持在不同的包中，稳定的功能永远不能引用实验性的功能。这确保了新的开发不会影响现有特性的稳定性。只要库只依赖于稳定的特性，它们就不会经历破坏性的 API 变化。\n   图 6-1：OpenTelemetry 中的每个主要功能都被赋予了一个稳定性等级，并遵循相同的生命周期。  API 的稳定性 OpenTelemetry API 预计将被数以千计的库所依赖，有数以百万计的调用站点。因此，API 的稳定部分决不能破坏向后的兼容性。应用程序的所有者和库的开发者不应该为了升级到一个新版本的 API 而重新测量他们的应用程序。\n如果一个 OpenTelemetry API 被废弃（这不太可能），被废弃的 API 仍将保持稳定并发挥功能。\n 原生工具是稳定的工具\n携带原生 OpenTelemetry 仪表的 OSS 库应该只使用稳定的 API，因为实验性功能的改变可能会造成依赖性冲突。\n也就是说，我们鼓励进行测试。如果一个库愿意为实验性的 OpenTelemetry 功能提供支持，这是一个给项目提供反馈和参与新功能设计的好方法。然而，我们建议将与实验性 OpenTelemetry 功能的集成作为可选的插件提供，终端用户必须单独安装才能启用。\n一旦功能变得稳定，就没有必要把它们作为一个单独的插件。事实上，最好是将 OpenTelemetry 原生集成，因为用户可能会忘记安装插件。这样一来，如果应用程序所有者安装了 OpenTelemetry SDK，他们就会自动开始接收来自每个库的数据。如果没有安装 SDK，API 的调用就没有意义了。原生仪表是我们希望 OpenTelemetry 能够简化应用程序所有者的观察能力的一种方式 —— 它已经存在于每一个库中，只要它需要，就可以随时使用。\n SDK 和收集器的稳定性 SDK 的稳定性集中在两个方面：插件接口和资源使用。SDK 可能偶尔会废止一个插件接口。为了确保应用程序的所有者能够干净利落地进行升级，必须在废弃的接口被删除之前添加一个替代接口，而使用废弃接口的流行插件必须被迁移到新的接口上。通过以这种方式安全地迁移插件生态系统，可以避免应用程序所有者陷入这样的境地：他们想要升级，但却被一个无法使用的插件所阻挡。在废弃和移除一个插件接口之间必须有至少 6 个月的时间，而且废弃的接口只有在维护它们会造成性能问题时才会被移除。否则，我们将无限期地保留这些被废弃的接口。\n说到性能，OpenTelemetry SDK 的稳定部分必须避免性能倒退，以确保 SDK 的较新版本在升级时不会引起资源争夺。很明显，启用新版本中增加的功能可能需要额外的资源。但是，简单地升级 SDK 不应该导致性能退步。\n与 SDK 一样，收集器试图避免性能退步，并为收集器插件生态系统提供一个渐进的升级路径。\n升级 OpenTelemetry 客户端 在运行 OpenTelemetry 时，我们希望用户能保持最新的 SDK 版本。有两个事件可能会迫使用户升级 SDK：一个库将其仪表升级到新版本的 API，或者 OpenTelemetry 发布一个重要的安全补丁。\n上面列出的稳定性保证确保了这种升级路径始终是可行的。只要一个应用程序只依赖于稳定的信号，升级应该只涉及依赖性的提升。教程不需要重写，插件也不会突然变得不受支持。\nOpenTelemetry 致力于向后兼容的一个很好的例子是它对其前身 OpenTracing 的支持。OpenTelemetry 追踪信号与 OpenTracing API 完全兼容，OpenTelemetry 和 OpenTracing API 调用可以混合到同一个应用程序中。OpenTracing 用户可以升级到 OpenTelemetry 而不需要重写现有的仪表。\n","date":1643990400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"a8280488efa53998e991e73ce4e5bf02","permalink":"https://lib.jimmysong.io/opentelemetry-obervability/stability-and-long-term-support/","publishdate":"2022-02-05T00:00:00+08:00","relpermalink":"/opentelemetry-obervability/stability-and-long-term-support/","section":"opentelemetry-obervability","summary":"第 6 章：稳定和长期支持","tags":["OpenTelemetry"],"title":"第 6 章：稳定和长期支持","type":"book"},{"authors":null,"categories":["文化"],"content":"有时候紧急 CL 必须尽快通过 code review 过程。\n什么是紧急情况？ 紧急 CL 是这样的小更新：允许主要发布继续而不是回滚，修复显著影响用户生产的错误，处理紧迫的法律问题，关闭主要安全漏洞等。\n在紧急情况下，我们确实关心 Code Review 的整体速度，而不仅仅是响应的速度。仅在这种情况下，审查人员应该更关心审查的速度和代码的正确性（是否解决了紧急情况？）。此外（显然）这类状况的审查应该优先于所有其他 code reivew。\n但是，在紧急情况解决后，您应该再次查看紧急 CL 并进行更彻底的审查。\n什么不是紧急情况？ 需要说明的是，以下情况并非紧急情况：\n 想要在本周而不是下周推出（除非有一些实际硬性截止日期，例如合作伙伴协议）。 开发人员已经在很长一段时间内完成了一项功能想要获得 CL。 审查者都在另一个时区，目前是夜间或他们已离开现场。 现在是星期五，在开发者在过周末之前获得这个 CL 会很棒。 今天因为软（非硬）截止日期，经理表示必须完成此审核并签入 CL。 回滚导致测试失败或构建破坏的 CL。  等等。\n什么是 Hard Deadline？ 硬性截止日期（Hard Deadline）是指如果你错过它会发生灾难性的事情。例如：\n 对于合同义务，必须在特定日期之前提交 CL。 如果在某个日期之前没有发布，您的产品将在市场上完全失败。 一些硬件制造商每年只发送一次新硬件。如果您错过了向他们提交代码的截止日期，那么这可能是灾难性的，具体取决于您尝试发布的代码类型。  延迟发布一周并不是灾难性的。错过重要会议可能是灾难性的，但往往不是。\n大多数截止日期都是软截止日期，而非最后期限。软截止日期表示希望在特定时间内完成某项功能。它们很重要，但你不应该以牺牲代码健康为前提来达到。\n如果您的发布周期很长（几周），那么在下一个周期之前就可能会牺牲代码审查质量来获取功能。然而，如果重复这种模式，往往会给项目建立压倒性技术债务。如果开发人员在周期结束时经常提交 CL，只需要进行表面评审就必须“进入”，那么团队应该修改其流程，以便在周期的早期发生大的功能变更，并有足够的时间进行良好的审查。\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"e58bdd5c7c6cadaf5039d55bc431c112","permalink":"https://lib.jimmysong.io/eng-practices/review/emergencies/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/eng-practices/review/emergencies/","section":"eng-practices","summary":"有时候紧急 CL 必须尽快通过 code review 过程。 什么是紧急情况？ 紧急 CL 是这样的小更新：允许主要发布继续而不是回滚，修复显著影响用户生产的错误，处理紧迫的法律问题，关闭主要安全漏洞等。 在紧急情况下，我们确实关心 Code Review 的","tags":null,"title":"紧急情况","type":"book"},{"authors":null,"categories":["网络"],"content":"Cilium 与多个 Kubernetes API 组兼容。有些是废弃的或测试版的，可能只在 Kubernetes 的特定版本中可用。\n所有列出的 Kubernetes 版本都经过 e2e 测试，保证与 Cilium 兼容。本表中未列出的旧版 Kubernetes 不支持 Cilium。较新的 Kubernetes 版本，虽然没有列出，但将取决于 Kubernetes 提供的后向兼容性。\n   Kubernetes 版本 Kubernetes NetworkPolicy API CiliumNetworkPolicy     1.16, 1.17, 1.18, 1.19, 1.20, 1.21, 1.22, 1.23 networking.k8s.io/v1 cilium.io/v2 有一个 CRD    Cilium 在 Kubernetes 中使用了一个网络策略的 CRD。这个 CRD 的模式验证可能会有变化，它可以验证 Cilium Clusterwide Network Policy（CCNP）或 Cilium Network Policy（CNP）的正确性。\nCRD 本身有一个注解，即 io.cilium.k8s.crd.schema.version，有模式定义版本。默认情况下，Cilium 会自动更新 CRD 及其验证，使用较新的版本。\n下表列出了所有的 Cilium 版本和它们预期的 schema 验证版本。\n   Cilium 版本 CNP 和 CCNP Schema 版本     v1.9.0-rc0 1.22.1   v1.9.0-rc1 1.22.2   v1.9.0-rc2 1.22.2   v1.9.0-rc3 1.22.3   v1.9.0 1.22.3   v1.9.1 1.22.3   v1.9.2 1.22.3   v1.9.3 1.22.3   v1.9.4 1.22.3   v1.9.5 1.22.3   v1.9.6 1.22.4   v1.9.7 1.22.5   v1.9.8 1.22.5   v1.9.9 1.22.6   v1.9.10 1.22.6   v1.9.11 1.22.6   v1.9.12 1.22.6   v1.9.13 1.22.6   v1.9.14 1.22.6   v1.9.15 1.22.6   v1.9.16 1.22.6   v1.9 1.22.6   v1.10.0-rc0 1.23.1   v1.10.0-rc1 1.23.2   v1.10.0-rc2 1.23.2   v1.10.0 1.23.2   v1.10.1 1.23.2   v1.10.2 1.23.3   v1.10.3 1.23.3   v1.10.4 1.23.3   v1.10.5 1.23.3   v1.10.6 1.23.4   v1.10.7 1.23.4   v1.10.8 1.23.4   v1.10.9 1.23.4   v1.10.10 1.23.4   v1.10.11 1.23.4   v1.10 1.23.4   v1.11.0-rc0 1.24.1   v1.11.0-rc1 1.24.1   v1.11.0-rc2 1.24.2   v1.11.0-rc3 1.24.2   v1.11.0 1.24.2   v1.11.1 1.24.3   v1.11.2 1.24.3   v1.11.3 1.24.3   v1.11.4 1.24.3   v1.11.5 1.24.3   v1.11 1.24.3   latest / master 1.25.4    ","date":1655438400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"694c718fa6299e3ebc72a1cd8517e903","permalink":"https://lib.jimmysong.io/cilium-handbook/kubernetes/compatibility/","publishdate":"2022-06-17T12:00:00+08:00","relpermalink":"/cilium-handbook/kubernetes/compatibility/","section":"cilium-handbook","summary":"Cilium 与多个 Kubernetes API 组兼容。有些是废弃的或测试版的，可能只在 Kubernetes 的特定版本中可用。 所有列出的 Kubernetes 版本都经过 e2e 测试，保证与 Cilium 兼容。本表中未列出的旧版 Kubernetes 不支持 Cilium。较新的 Kubernetes 版本，虽然没有列出，但将取决于 Kubernetes 提供","tags":["Cilium"],"title":"Kubernetes 兼容性","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"容器网络接口（Container Network Interface），简称 CNI，是 CNCF 旗下的一个项目，由一组用于配置 Linux 容器的网络接口的规范和库组成，同时还包含了一些插件。CNI 仅关心容器创建时的网络分配，和当容器被删除时释放网络资源。有关详情请查看 GitHub。\nKubernetes 源码的 vendor/github.com/containernetworking/cni/libcni 目录中已经包含了 CNI 的代码，也就是说 Kubernetes 中已经内置了 CNI。\n接口定义 CNI 的接口中包括以下几个方法：\ntype CNI interface { AddNetworkList (net *NetworkConfigList, rt *RuntimeConf) (types.Result, error) DelNetworkList (net *NetworkConfigList, rt *RuntimeConf) error AddNetwork (net *NetworkConfig, rt *RuntimeConf) (types.Result, error) DelNetwork (net *NetworkConfig, rt *RuntimeConf) error } 该接口只有四个方法，添加网络、删除网络、添加网络列表、删除网络列表。\n设计考量 CNI 设计的时候考虑了以下问题：\n 容器运行时必须在调用任何插件之前为容器创建一个新的网络命名空间。 然后，运行时必须确定这个容器应属于哪个网络，并为每个网络确定哪些插件必须被执行。 网络配置采用 JSON 格式，可以很容易地存储在文件中。网络配置包括必填字段，如 name 和 type 以及插件（类型）。网络配置允许字段在调用之间改变值。为此，有一个可选的字段 args，必须包含不同的信息。 容器运行时必须按顺序为每个网络执行相应的插件，将容器添加到每个网络中。 在完成容器生命周期后，运行时必须以相反的顺序执行插件（相对于执行添加容器的顺序）以将容器与网络断开连接。 容器运行时不能为同一容器调用并行操作，但可以为不同的容器调用并行操作。 容器运行时必须为容器订阅 ADD 和 DEL 操作，这样 ADD 后面总是跟着相应的 DEL。 DEL 可能跟着额外的 DEL，但是，插件应该允许处理多个 DEL（即插件 DEL 应该是幂等的）。 容器必须由 ContainerID 唯一标识。存储状态的插件应该使用（网络名称，容器 ID）的主键来完成。 运行时不能调用同一个网络名称或容器 ID 执行两次 ADD（没有相应的 DEL）。换句话说，给定的容器 ID 必须只能添加到特定的网络一次。  CNI 插件 CNI 插件必须实现一个可执行文件，这个文件可以被容器管理系统（例如 rkt 或 Kubernetes）调用。\nCNI 插件负责将网络接口插入容器网络命名空间（例如，veth 对的一端），并在主机上进行任何必要的改变（例如将 veth 的另一端连接到网桥）。然后将 IP 分配给接口，并通过调用适当的 IPAM 插件来设置与 “IP 地址管理” 部分一致的路由。\n参数 CNI 插件必须支持以下操作：\n将容器添加到网络 参数：\n 版本调用者正在使用的 CNI 规范（容器管理系统或调用插件）的版本。 容器 ID由运行时分配的容器的唯一明文标识符。一定不能是空的。 网络命名空间路径要添加的网络名称空间的路径，即 /proc/[pid]/ns/net 或绑定挂载 / 链接。 网络配置描述容器可以加入的网络的 JSON 文档。架构如下所述。 额外的参数这提供了一个替代机制，允许在每个容器上简单配置 CNI 插件。 容器内接口的名称这是应该分配给容器（网络命名空间）内创建的接口的名称；因此它必须符合 Linux 接口名称上的标准限制。  结果：\n 接口列表根据插件的不同，这可以包括沙箱（例如容器或管理程序）接口名称和 / 或主机接口名称，每个接口的硬件地址以及接口所在的沙箱（如果有的话）的详细信息。 分配给每个接口的 IP 配置分配给沙箱和 / 或主机接口的 IPv4 和 / 或 IPv6 地址，网关和路由。 DNS 信息包含 nameserver、domain、search domain 和 option 的 DNS 信息的字典。  从网络中删除容器 参数：\n  版本调用者正在使用的 CNI 规范（容器管理系统或调用插件）的版本。\n  容器 ID，如上所述。\n  网络命名空间路径，如上定义。\n  网络配置，如上所述。\n  额外的参数，如上所述。\n  上面定义的容器内的接口的名称。\n  所有参数应与传递给相应的添加操作的参数相同。\n  删除操作应释放配置的网络中提供的 containerid 拥有的所有资源。\n  报告版本：\n 参数：无。 结果：插件支持的 CNI 规范版本信息。  {“cniVersion”：“0.3.1”，// 此输出使用的 CNI 规范的版本 “supportedVersions”：[“0.1.0”，“0.2.0”，“0.3.0”，“0.3.1”] // 此插件支持的 CNI 规范版本列表 } CNI 插件的详细说明请参考：CNI SPEC。\nIP 分配 作为容器网络管理的一部分，CNI 插件需要为接口分配（并维护）IP 地址，并安装与该接口相关的所有必要路由。这给了 CNI 插件很大的灵活性，但也给它带来了很大的负担。众多的 CNI 插件需要编写相同的代码来支持用户需要的多种 IP 管理方案（例如 dhcp、host-local）。\n为了减轻负担，使 IP 管理策略与 CNI 插件类型解耦，我们定义了 IP 地址管理插件（IPAM 插件）。CNI 插件的职责是在执行时恰当地调用 IPAM 插件。 IPAM 插件必须确定接口 IP/subnet，网关和路由，并将此信息返回到 “主” 插件来应用配置。 IPAM 插件可以通过协议（例如 dhcp）、存储在本地文件系统上的数据、网络配置文件的 “ipam” 部分或上述的组合来获得信息。\nIPAM 插件 像 CNI 插件一样，调用 IPAM 插件的可执行文件。可执行文件位于预定义的路径列表中，通过 CNI_PATH 指示给 CNI 插件。 IPAM 插件必须接收所有传入 CNI 插件的相同环境变量。就像 CNI 插件一样，IPAM 插件通过 stdin 接收网络配置。\n可用插件 Main：接口创建  bridge：创建网桥，并添加主机和容器到该网桥 ipvlan：在容器中添加一个 ipvlan 接口 loopback：创建一个回环接口 macvlan：创建一个新的 MAC 地址，将所有的流量转发到容器 ptp：创建 veth 对 vlan：分配一个 vlan 设备  IPAM：IP 地址分配  dhcp：在主机上运行守护程序，代表容器发出 DHCP 请求 host-local：维护分配 IP 的本地数据库  Meta：其它插件  flannel：根据 flannel 的配置文件创建接口 tuning：调整现有接口的 sysctl 参数 portmap：一个基于 iptables 的 portmapping 插件。将端口从主机的地址空间映射到容器。  参考  containernetworking/cni - github.com containernetworking/plugins - github.com Container Networking Interface Specification - github.com CNI Extension conventions - github.com  ","date":1653062400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"54b8763373b626e80077f479d8a5a797","permalink":"https://lib.jimmysong.io/kubernetes-handbook/architecture/open-interfaces/cni/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/architecture/open-interfaces/cni/","section":"kubernetes-handbook","summary":"容器网络接口（Container Network Interface），简称 CNI，是 CNCF 旗下的一个项目，由一组用于配置 Linux 容器的网络接口的规范和库组成，同时还包含了一些插件。CNI 仅关心容器创建时的网络分配，和当容器被","tags":["Kubernetes"],"title":"容器网络接口（CNI）","type":"book"},{"authors":null,"categories":["安全"],"content":"本文件的结构如下：\n第二章简要介绍了参考平台，为其提供了实施 DevSecOps 原语的指导。\n第三章介绍了 DevSecOps 的基本要素（即管道），设计和执行管道的方法，以及自动化在执行中的作用。\n第四章涵盖了管道的所有方面，包括（a）所有管道需要解决的共同问题，（b）对第 1.1 节中列出的参考平台中五种代码类型的管道的描述，以及（c）DevSecOps 在整个生命周期中对整个应用环境（有五种代码类型的参考平台，因此承载着 DevSecOps 的实施）的安全保证的好处，包括 持续授权操作（C-ATO）。\n第五章提供了摘要和结论。\n 下一章   ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"93f1cfb5612c4054dc7bd887d07b6123","permalink":"https://lib.jimmysong.io/service-mesh-devsecops/intro/organization-of-this-document/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/service-mesh-devsecops/intro/organization-of-this-document/","section":"service-mesh-devsecops","summary":"本文件的结构如下： 第二章简要介绍了参考平台，为其提供了实施 DevSecOps 原语的指导。 第三章介绍了 DevSecOps 的基本要素（即管道），设计和执行管道的方法，以及自动化在执行中的作用。 第四章涵盖了管道的所有方面，包括（a）所有管","tags":["Service Mesh","DevSecOps"],"title":"1.5 本文件的组织","type":"book"},{"authors":null,"categories":["安全"],"content":"最后一个常见的问题是安全测试。无论代码类型是什么（例如，应用服务、Iac、Pac 或可观测性），基于微服务的基础设施的 DevSecOps 的 CI/CD 管道与服务网格应包括由自动化工具或作为服务提供的应用安全测试（AST）。这些工具会分析和测试应用程序的安全漏洞。根据 Gartner 的说法，有 四种 主要的 AST 技术：\n 静态 AST（SAST）工具：分析应用程序的源码、字节码或二进制代码的安全漏洞，通常在编程和 / 或测试软件生命周期（SLC）阶段。具体来说，这项技术涉及到在提交中查看应用程序并分析其依赖关系的 技术。如果任何依赖关系包含问题或已知的安全漏洞，提交将被标记为不安全的，不允许继续部署。这也可以包括在代码中找到应该被删除的硬编码密码 / 秘密。 动态 AST（DAST）工具：在测试或运行阶段，分析应用程序的动态运行状态。它们模拟针对应用程序（通常是支持网络的应用程序、服务和 API）的攻击，分析应用程序的反应，并确定它是否有漏洞。特别是，DAST 工具比 SAST 更进一步，在 CI 工作中启动生产环境的副本，以扫描所产生的容器和 可执行文件。动态方面有助于系统捕捉在启动时正在加载的依赖关系，例如那些不会被 SAST 捕捉的依赖关系。 交互式 AST（IAST）工具：将 DAST 的元素与被测试的应用程序的仪器相结合。它们通常作为测试运行环境中的一个代理来实现（例如，对 Java 虚拟机或.NET CLR 进行检测），观察操作或识别攻击漏洞。 软件组成分析（SCA）工具：用于识别应用程序中使用的开源和第三方组件、其已知的安全漏洞以及典型的对抗性许可限制。  4.8.1 AST 工具的功能和覆盖要求 一般来说，测试工具（包括特定类别的 AST 工具）应该满足的总体指标是\n 通过识别安全、隐私和合规性方面的差距，提高应用程序的发布质量。 与开发人员已经在使用的工具整合。 要尽可能少的测试工具，但提供必要的风险覆盖。 API 和微服务层面的低级单元测试应该有足够的可视性来确定覆盖率。 包括更高层次的 UI/UX 和系统测试。 具备深入的代码分析能力，以检测运行时的缺陷。 提高发布的速度。 要有成本效益。  特别是对 AST 工具的功能要求包括进行以下类型的扫描：\n 漏洞扫描。探测应用程序的安全弱点，这些弱点可能会使它们受到攻击。 容器镜像扫描。分析容器镜像的内容和构建过程，以检测安全问题、漏洞或缺陷做法（例如，硬编码密码 / 秘密）。 监管 / 合规性扫描。评估对特定合规要求的遵守情况。  每当源代码库中的代码被修改时，都要进行漏洞扫描，以确保当前的修订版不包含任何有漏洞的 依赖。\nAST 工具和 / 或服务的理想特征，以及行为分析的技术：\n 分析源码、字节码或二进制代码 观察应用程序的行为，以确定引入安全漏洞的编码、设计、打包、部署和运行时条件。  作为 CI/CD 管道任务的一部分，扫描应用程序代码的安全漏洞和错误配置应涉及以下工件：\n 容器镜像应被扫描以发现漏洞。 在容器从基础镜像（如上所述进行扫描）构建之后，应该对容器的文件系统进行漏洞和错误配置的扫描。 应该对 Git 存储库（包含应用程序源代码）进行扫描，以发现漏洞和错误配置。  容器镜像包括操作系统包（如 Alpine、UBI、RHEL、CentOS 等）和特定语言包（如 Bundler、Composer、npm、yarn 等）。\n对基础设施即代码进行安全漏洞扫描，通过防止这些漏洞进入生产，减少了操作工作量，尽管它不能取代对运行时安全的检查，因为漂移的风险始终存在。然而，必须对架构的所有部署后（运行时）变化（由于漂移）的原因进行分析，并通过向 IaC 推送适当的更新来解决，从而使其成为管道的一部分，并在后续部署中不再出现。这种方法有利于使用运行时检查来补救安全设计缺陷。\n基础设施即代码的文件可以在下面找到：\n 容器编排平台本身，以促进部署（例如，Kubernetes YAML 基础设施即代码文件）。 作为 CI/CD 管道软件的一部分而发现的专用基础设施即代码文件（例如，HashiCorp Terraform 基础设施即代码文件，AWS CloudFormation 基础设施即代码文件）。  应用服务代码、策略即代码和可观测性即代码文件可以在专门的应用服务组件（如服务网格）的数据平面和控制平面组件中找到，并且应该对安全漏洞（如授权策略的信息泄露）和错误配置进行扫描。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"940702adba252b05f78a2d7766749020","permalink":"https://lib.jimmysong.io/service-mesh-devsecops/implement/security-testing-common-requirement-for-ci-cd-pipelines-for-all-code-types/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/service-mesh-devsecops/implement/security-testing-common-requirement-for-ci-cd-pipelines-for-all-code-types/","section":"service-mesh-devsecops","summary":"最后一个常见的问题是安全测试。无论代码类型是什么（例如，应用服务、Iac、Pac 或可观测性），基于微服务的基础设施的 DevSecOps 的 CI/CD 管道与服务网格应包括由自动化工具或作为服务提供的应用安全测试（AST）。这些工","tags":["Service Mesh","DevSecOps"],"title":"4.8 安全测试——所有代码类型的 CI/CD 管道的共同要求","type":"book"},{"authors":null,"categories":["云原生"],"content":"基础架构是用来支撑应用程序的。可信任的软件对于工程成功至关重要。如果每次在终端输入 ls 命令，都会发生随机动作，那么你将永远也不会相信 ls，而是去找另一种方式来列出目录中的文件。\n我们的基础架构必须值得信任。本章旨在建立信任和验证基础架构的意识形态。我们将描述的实践旨在增加对应用程序和基础架构工程的信心。\n软件测试在当今的软件工程领域非常普遍。然而，如何测试基础架构还没有很明确的最佳实践。\n这意味着本书中的所有章节中，这一节应该是最令人兴奋的！在该领域像您这样的工程师有充分的空间发挥出色的影响力。\n软件测试是一种证明软件可以正常工作的有效做法，证明软件在各种特殊情况下软件仍然可以正常运行。因此，如果我们将相同的范例应用于基础架构测试，测试目标如下：\n 证明基础架构按预期运行。 证明基础架构不会失败。 证明这两种情况在各种边缘情况下都是正确的。  衡量基础架构是否有效需要我们先定义什么叫有效。现在，您应该对使用基础架构和工程代表应用程序的想法感到满意。\n定义基础架构 API 的人应该花时间构思一个可以创建有效基础架构的理智的 API。例如，如果创建了一个定义虚拟机的 API 但里面没有使之可以运行的网络信息，这种做法就很愚蠢。您应该在 API 中创建有用的抽象，然后使用第 3 章和第 4 章中提出的想法来确保 API 创建出正确的基础架构组件。\n我们开始开发一个心智模型，通过定义 API 对基础架构的健全性进行检查。这意味着我们可以翻转逻辑并想象出相反的情况，这将是除了原始心智模型之外的所有东西。\n很值得为基础架构定义基本的完整性测试。测试基础架构的第一步是证明您的基础架构是按预期存在的，并且没有任何东西的违背原意而存在。\n在本章中，我们将探索基础架构测试，这将为新的测试工具奠定基础。\n我们该测试什么？ 在开始编写代码之前，我们首先必须确定要测试的内容。\n使用测试驱动开发是测试优先的常见做法。测试是为了证明测试点而编写的，从一开始就隐含失败。软件的开发周期都需要通过测试；也就是说，软件是为了满足测试中定义的每个要求而开发的。这是一个强大的实践，可以帮助软件保持专注，帮助工程师建立对软件的信心。\n这是一个可以用多种方式回答的哲学问题。建立值得信赖的基础架构是必不可少的。例如，如果存在依赖基础架构的业务问题，则应该对其进行测试。更重要的是，许多业务不仅依赖基础架构，还会在出现问题时自行修复。\n确定基础架构需要填补的问题空间代表了将需要编写的第一轮测试。\n远景规划是基础架构测试的另一个重要方面，但应该谨慎。在足够的前瞻性和过度工程化之间有一条看不见的界限。如果有疑问，坚持最少量的测试逻辑。\n在我们完全了解需要的测试之后，就可以考虑实施测试套件。\n编写可测试代码 调节器模式不仅旨在创建整洁的基础架构应用程序，而且还鼓励编写可测试的基础架构代码。\n这意味着，在应用程序的每一个重要步骤中，我们总是会重新创建一个相同类型的新对象，也就是说基础系统的每个主要组件都会使用相同的输入和输出。使用相同的输入和输出可以更轻松地以编程方式测试软件的小型组件。测试将确保您的组件按预期工作。\n然而，在编写基础架构测试代码时，还有许多经验值得借鉴。我们将在看看在一些虚拟情景测试基础架构的具体示例。通过浏览场景，您将学到测试基础架构代码的经验教训。\n我们还会给出工程师一些规则，应在编写可测试基础架构代码时遵守这些规则。\n验证 采用非常基础的基础架构定义，如示例 6-1。\n例 6-1. infrastructure.json\n{ \u0026#34;virtualMachines\u0026#34;: [{ \u0026#34;name\u0026#34;: \u0026#34;my-vm\u0026#34;, \u0026#34;size\u0026#34;: \u0026#34;large\u0026#34;, \u0026#34;localIp\u0026#34;: \u0026#34;192.168.1.111\u0026#34;, \u0026#34;subnet\u0026#34;: \u0026#34;my-subnet\u0026#34; }], \u0026#34;subnets\u0026#34;: [{ \u0026#34;name\u0026#34;: \u0026#34;my-subnet\u0026#34;, \u0026#34;cidr\u0026#34;: \u0026#34;10.0.100.0/24\u0026#34; }] } 这些数据的目的显而易见：确保一个名为my-vm的虚拟机的大小为large，IP 地址为192.168.1.111。这些数据也暗示确保一个名为my-subnet的子网将容纳虚拟机my-vm。\n希望您发现了这个数据的问题。虚拟机的 IP 地址超出了子网的可用 CIDR 范围。\n应用程序在运行此数据时应该会失败，因为虚拟机设置了无效的网络。如果将我们的应用程序构建为盲目地允许部署任何数据，那么我们将创建一个可以联网的基础架构。尽管我们应该编写测试以确保新的虚拟机能够在网络上路由，但是我们可以做一些其他事情来帮助加强我们的应用程序并使测试更加容易。\n在应用程序处理输入之前，我们可以首先尝试验证输入。这在软件工程中是很常见的做法。\n想象一下，如果不是盲目部署这个基础架构，我们首先会尝试验证输入。在运行时，应用程序可以很容易的检测到虚拟机的 IP 地址在虚拟机所连接的子网中无效。这将阻止输入到达基础架构环境。由于知道应用程序将故意拒绝无效的基础架构表示，我们可以编写 happy 和 sad 测试来确保实现此行为。\nHappy 测试可以对条件进行正面处理。换句话说，它是一种向应用程序发送有效 API 对象并确保应用程序接受有效输入的测试。Sad 测试，可以对相反的情况或负面情况进行分析。例 6-1 是一个 sad 测试的例子，它将一个无效的 API 对象发送给应用程序，并确保应用程序拒绝无效输入。\n这种新模式使测试基础架构非常快速，而且通常不用费什么力气。一个工程师就可以开发大量的 happy 和 sad 测试，即使是最奇怪的应用程序输入也是如此。此外，测试集合可以随着时间的推移而增长；在欺骗 API 对象流入环境场景中时，工程师可以快速添加测试以防止其再次发生。\n输入验证是测试最基本的事情之一。通过在我们的应用程序中编写简单的验证来检查理智的值，我们可以开始过滤应用程序的输入。这也给了我们一个很容易定义错误并快速返回错误的途径。\n验证提供信心，而不会让您等待基础架构发生变异。这为面向 API 开发的工程师创建了更快的反馈循环。\n输入代码库 编写易于测试的代码非常重要。经常出错可能会导致成本上升，因此需要围绕专有输入设计应用程序。专有输入是仅与程序中的一个点相关的输入，获得所需输入的唯一方法是线性执行程序。以这种方式线性编写代码对于人类大脑来说是有意义的，但这也是有效测试的最难的模式之一，特别是当涉及到测试基础架构时。\n专有输入陷入困境的例子如下：\n 函数 DoSomething () 的调用返回 Something {}。 Something {} 传递给函数 NextStep (Something) 并返回 SomethingElse {}。 SomethingElse {} 被传递给函数 FinalStep (something else)，返回 true 或 false。  这里的问题是，为了测试 FinalStep () 函数，我们首先需要遍历步骤 1 和 2。在测试的情况下，这会引入复杂性和更多的失败点；甚至可能不会在测试执行的环境中工作。\n更优雅的解决方案是以这样一种方式构造代码，即可以在程序的其余部分使用相同的数据结构上调用最后的 step ()：\n 代码初始化 GreatSomething {} 实现了方法 great.DoSomething ()。 GreatSomething {} 实现 NextStep () 方法。 GreatSomething {} 实现了 something.FinalStep () 方法。  从测试的角度来看，我们可以为我们希望测试的任何步骤填充 GreatSomething {}，并相应地调用这些方法。这个例子中的方法现在负责处理它们扩展的对象中定义的内存。这与最后一种方法不同，在这种方法中，特殊的内存中的结构被传递到每个函数中。\n这是一种更为优雅的设计，因为测试工程师可以轻松地在整个过程中的任何一步合成内存，而不必担心学习相同数据的一种表示形式。 这是更加模块化的，如果可以迅速发现任何故障，我们可以介入。\n当您开始编写构成应用程序的软件时，请记住，您将需要在传统的运行时间轴上的许多地方跳入代码库。构建代码以使其易于在任何时候输入代码库并在内部进行测试，这是至关重要的。 在这种情况下，您可以成为自己最好的朋友，也可以成为最大的敌人。\n自我意识  告诉我你如何衡量我，我会告诉你我的行为。\n——Eliyahu M. Goldratt\n 在编写代码和测试时注意自己的置信度。自我意识是软件工程中最重要的部分之一，也是最容易被忽视的部分之一。\n测试的最终目标是增加对应用程序的信心。就基础架构领域来说，我们的目标就是增强对基础架构的信心。\n测试基础架构的方法没有对错之分。可以在应用程序中通过代码覆盖率和单元测试来建立信心，但是对于基础架构来说这样做可能会存在误导。\n代码覆盖率是以程序化方式衡量代码可以满足预期的行为。这个度量标准可以用作原始数据点，但我们要知道即使是覆盖率达到 100％的代码库仍然可能会出现极端中断，这一点至关重要。\n如果你以代码覆盖率来衡量测试结果，那么工程师就会编写更容易被测试覆盖的代码，而不是编写更适应该任务的代码。Dan Ariely 在他刊登于哈弗商业评论的文章 “衡量标准决定一切” ：\n人们的行为会根据衡量指标作出相应的调整。衡量指标会促使某个人在该指标上做出优化。你想要得到什么，就要去衡量什么。\n应该衡量的唯一指标是信心，即我们的基础架构可以按预期工作，并且可以证明这一点。\n衡量信心几乎是不可能的。但是有些方法可以从工程师的心理和情绪中抽取有意义的数据集。\n问自己以下几个问题，记录下答案：\n 我担心这行不通吗？ 我可以肯定，这将做我认为会做的事吗？ 如果有人更改此文件，会发生什么情况？  从问题中提取数据的最强好的方式是比较以前的经验水平。例如，工程师可以做出如下陈述，团队的其他成员很快就会明白他想要传达的内容：\n比起上个季度，这次代码发布更令人担忧。\n现在，根据团队以前的经验，我们可以开始为我们的信心水平制定一套标准，从 0 开始表示完全没有信心，随着时间的流逝然后增加到非常自信。当我们了解了我们担心应用程序的哪些问题之后，再为了增加信心而制定测试内容就很简单了。\n测试类型 了解测试的类型以及测试方式将有助于工程师增加其对基础架构应用程序的信心。这些测试不需要编写，而且没有正确或错误之分。唯一的问题是我们相信应用程序会做我们想要它做的事情。\n基础架构断言 在软件工程中，有一个重要的概念是断言，这是一种强制的方式 —— 完全确定条件是否成立。目前已经有许多成功的框架使用断言来测试软件。断言是一个微小的函数，它将测试条件是否为真。这些功能可以在各种测试场景中使用，以证明概念正在发挥作用和增加我们的信心。\n在本章的其余部分中，我们将提到基础架构断言。您需要对这些断言的内容以及他们希望完成的内容有基本的了解。您还需要对 Go 语言有基本的了解，才能充分认识这些断言是做什么的。\n在基础架构领域需要声明我们的基础架构有效。对于您的项目来说值得练习下使用构建这些断言功能的库。开源社区也可以从这个工具包测试基础架构中受益。\n例 6-2 显示了 Go 语言中的断言模式。假设我们想测试虚拟机是否可以解析公共主机名，然后路由到它们。\n例 6-2. assertNetwork.go\ntype VirtualMachine struct {localIp string} func (v *VirtualMachine) AssertResolvesHostname (hostname string, expectedIp string, message string) error { // Logic to query DNS for a hostname,  //and compare to the expectedIp  return nil } func (v *VirtualMachine) AssertRouteable …","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"69cad2ad6e3a08e21f6f3f0999d42c54","permalink":"https://lib.jimmysong.io/cloud-native-infra/testing-cloud-native-infrastructure/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/cloud-native-infra/testing-cloud-native-infrastructure/","section":"cloud-native-infra","summary":"基础架构是用来支撑应用程序的。可信任的软件对于工程成功至关重要。如果每次在终端输入 ls 命令，都会发生随机动作，那么你将永远也不会相信 ls，而是去找另一种方式来列出目录中的文件。 我们的基础架构必须值得信任","tags":["云原生"],"title":"第 6 章：测试云原生基础架构","type":"book"},{"authors":null,"categories":["Envoy"],"content":"唯一的请求 ID 对于通过多个服务追踪请求、可视化请求流和精确定位延迟来源至关重要。\n我们可以通过 request_id_extension 字段配置请求 ID 的生成方式。如果我们不提供任何配置，Envoy 会使用默认的扩展，称为 UuidRequestIdConfig。\n默认扩展会生成一个唯一的标识符（UUID4）并填充到 x-request-id HTTP 头中。Envoy 使用 UUID 的第 14 个位点来确定跟踪的情况。\n如果第 14 个比特位（nibble）被设置为 9，则应该进行追踪采样。如果设置为 a，应该是由于服务器端的覆盖（a）而强制追踪，如果设置为 b，应该是由客户端的请求 ID 加入而强制追踪。\n之所以选择第 14 个位点，是因为它在设计上被固定为 4。因此，4 表示一个默认的 UUID 没有跟踪状态，例如 7b674932-635d-4ceb-b907-12674f8c7267（说明：第 14 比特位实际为第 13 个数字）。\n我们在 UuidRequestIdconfig 中的两个配置选项是 pack_trace_reason 和 use_request_id_for_trace_sampling。\n.....route_config:name:local_routerequest_id_extension:typed_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.request_id.uuid.v3.UuidRequestIdConfigpack_trace_reason:falseuse_request_id_for_trace_sampling:falsehttp_filters:- name:envoy.filters.http.router...pack_trace_reaseon 是一个布尔值，控制实现是否改变 UUID 以包含上述的跟踪采样决定。默认值是 true。use_request_id_for_trace_sampling 设置是否使用 x-request-id 进行采样。默认值也是 true。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"20a8e239982dddaf0078cce4cbf28362","permalink":"https://lib.jimmysong.io/envoy-handbook/hcm/request-id-generation/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/envoy-handbook/hcm/request-id-generation/","section":"envoy-handbook","summary":"唯一的请求 ID 对于通过多个服务追踪请求、可视化请求流和精确定位延迟来源至关重要。 我们可以通过 request_id_extension 字段配置请求 ID 的生成方式。如果我们不提供任何配置，Envoy 会使用默认的扩展，称为 UuidRequestId","tags":["Envoy"],"title":"生成请求 ID","type":"book"},{"authors":null,"categories":["安全"],"content":"集群网络是 Kubernetes 的一个核心概念。容器、Pod、服务和外部服务之间的通信必须被考虑在内。默认情况下，很少有网络策略来隔离资源，防止集群被破坏时的横向移动或升级。资源隔离和加密是限制网络行为者在集群内转移和升级的有效方法。\n关键点\n 使用网络策略和防火墙来隔离资源。 确保控制平面的安全。 对流量和敏感数据（例如 Secret）进行静态加密。  命名空间 Kubernetes 命名空间是在同一集群内的多个个人、团队或应用程序之间划分集群资源的一种方式。默认情况下，命名空间不会被自动隔离。然而，命名空间确实为一个范围分配了一个标签，这可以用来通过 RBAC 和网络策略指定授权规则。除了网络隔离之外，策略可以限制存储和计算资源，以便在命名空间层面上对 Pod 进行更好的控制。\n默认有三个命名空间，它们不能被删除：\n kube-system（用于 Kubernetes 组件） kube-public（用于公共资源） default（针对用户资源）  用户 Pod 不应该放在 kube-system 或 kube-public 中，因为这些都是为集群服务保留的。可以用 YAML 文件，如 附录 D：命名空间示例 ，可以用来创建新的命名空间。不同命名空间中的 Pod 和服务仍然可以相互通信，除非有额外的隔离措施，如网络策略。\n网络策略 网络策略控制 Pod、命名空间和外部 IP 地址之间的流量。默认情况下，没有网络策略应用于 Pod 或命名空间，导致 Pod 网络内的入口和出口流量不受限制。通过适用于 Pod 或 Pod 命名空间的网络策略，Pod 将被隔离。一旦一个 Pod 在网络策略中被选中，它就会拒绝任何适用的策略对象所不允许的任何连接。\n要创建网络策略，需要一个支持 NetworkPolicy API 的网络插件。使用 podSelector 和 / 或 namespaceSelector 选项来选择 Pod。附录 E 中展示了一个网络策略的例子。网络策略的格式可能有所不同，这取决于集群使用的容器网络接口（CNI）插件。管理员应该使用选择所有 Pod 的默认策略来拒绝所有入口和出口流量，并确保任何未选择的 Pod 被隔离。然后，额外的策略可以放松这些允许连接的限制。\n外部 IP 地址可以使用 ipBlock 在入口和出口策略中使用，但不同的 CNI 插件、云提供商或服务实现可能会影响 NetworkPolicy 处理的顺序和集群内地址的重写。\n资源政策 除了网络策略，LimitRange 和 ResourceQuota 是两个可以限制命名空间或节点的资源使用的策略。LimitRange 策略限制了特定命名空间内每个 Pod 或容器的单个资源，例如，通过强制执行最大计算和存储资源。每个命名空间只能创建一个 LimitRange 约束，如 附录 F 的 LimitRange 示例中所示。Kubernetes 1.10 和更新版本默认支持 LimitRange。\n与 LimitRange 策略不同的是，ResourceQuotas 是对整个命名空间的资源使用总量的限制，例如对 CPU 和内存使用总量的限制。如果用户试图创建一个违反 LimitRange 或 ResourceQuota 策略的 Pod，则 Pod 创建失败。附录 G 中显示了一个 ResourceQuota 策略的示例。\n控制平面加固 控制平面是 Kubernetes 的核心，使用户能够查看容器，安排新的 Pod，读取 Secret，在集群中执行命令。由于这些敏感的功能，控制平面应受到高度保护。除了 TLS 加密、RBAC 和强大的认证方法等安全配置外，网络隔离可以帮助防止未经授权的用户访问控制平面。Kubernetes API 服务器运行在 6443 和 8080 端口上，这些端口应该受到防火墙的保护，只接受预期的流量。8080 端口，默认情况下，可以在没有 TLS 加密的情况下从本地机器访问，请求绕过认证和授权模块。不安全的端口可以使用 API 服务器标志 --insecure-port=0 来禁用。Kubernetes API 服务器不应该暴露在互联网或不信任的网络中。网络策略可以应用于 kube-system 命名空间，以限制互联网对 kube-system 的访问。如果对所有命名空间实施默认的拒绝策略，kube-system 命名空间仍然必须能够与其他控制平面和工作节点进行通信。\n下表列出了控制平面的端口和服务。\n表 2：控制平面端口\n   端口 方向 端口范围 目的     TCP Inbound 6443 or 8080 if not disabled Kubernetes API server   TCP Inbound 2379-2380 etcd server client API   TCP Inbound 10250 kubelet API   TCP Inbound 10251 kube-scheduler   TCP Inbound 10252 kube-controller-manager   TCP Inbound 10258 cloud-controller-manager（可选）    Etcd  etcd 后端数据库是一个关键的控制平面组件，也是集群中最重要的安全部分。\n etcd 后端数据库存储状态信息和集群 Secret。它是一个关键的控制平面组件，获得对 etcd 的写入权限可以使网络行为者获得对整个集群的 root 权限。Etcd 只能通过 API 服务器访问，集群的认证方法和 RBAC 策略可以限制用户。etcd 数据存储可以在一个单独的控制平面节点上运行，允许防火墙限制对 API 服务器的访问。管理员应该设置 TLS 证书以强制执行 etcd 服务器和 API 服务器之间的 HTTPS 通信。etcd 服务器应被配置为只信任分配给 API 服务器的证书。\nKubeconfig 文件 kubeconfig 文件包含关于集群、用户、命名空间和认证机制的敏感信息。Kubectl 使用存储在工作节点的 $HOME/.kube 目录下的配置文件，并控制平面本地机器。网络行为者可以利用对该配置目录的访问，获得并修改配置或凭证，从而进一步破坏集群。配置文件应该被保护起来，以防止非故意的改变，未经认证的非 root 用户应该被阻止访问这些文件。\n工作节点划分 工作节点可以是一个虚拟机或物理机，这取决于集群的实现。由于节点运行微服务并承载集群的网络应用，它们往往是被攻击的目标。如果一个节点被破坏，管理员应主动限制攻击面，将工作节点与其他不需要与工作节点或 Kubernetes 服务通信的网段分开。防火墙可用于将内部网段与面向外部的工作节点或整个 Kubernetes 服务分开，这取决于网络的情况。机密数据库或不需要互联网访问的内部服务，这可能需要与工作节点的可能攻击面分离。\n下表列出了工作节点的端口和服务。\n表 3：工作节点端口\n   端口 方向 端口范围 目的     TCP Inbound 10250 kubelet API   TCP Inbound 30000-32767 NodePort Services    加密 管理员应配置 Kubernetes 集群中的所有流量 —— 包括组件、节点和控制计划之间的流量（使用 TLS 1.2 或 1.3 加密）。\n加密可以在安装过程中设置，也可以在安装后使用 TLS 引导（详见 Kubernetes 文档）来创建并向节点分发证书。对于所有的方法，必须在节点之间分发证书，以便安全地进行通信。\nSecret  默认情况下，Secret 被存储为未加密的 base64 编码的字符串，并且可以被任何有 API 权限的人检索。\n Kubernetes Secret 维护敏感信息，如密码、OAuth 令牌和 SSH 密钥。与在 YAML 文件、容器镜像或环境变量中存储密码或令牌相比，将敏感信息存储在 Secret 中提供了更大的访问控制。默认情况下，Kubernetes 将 Secret 存储为未加密的 base64 编码字符串，任何有 API 权限的人都可以检索到。可以通过对 secret 资源应用 RBAC 策略来限制访问。\n可以通过在 API 服务器上配置静态数据加密或使用外部密钥管理服务（KMS）来对秘密进行加密，该服务可以通过云提供商提供。要启用使用 API 服务器的 Secret 数据静态加密，管理员应修改 kube-apiserver 清单文件，以执行使用 --encryption-provider-config 参数执行。附录 H 中显示了一个 encryption-provider-config 的例子：加密实例。使用 KMS 提供者可以防止原始加密密钥被存储在本地磁盘上。要用 KMS 提供者加密 Secret，encryption-provider-config 文件中应指定 KMS 提供者，如附录 I 的 KMS 配置示例所示。\n在应用了 encryption-provider-config 文件后，管理员应该运行以下命令来读取和加密所有的 Secret。\nkubectl get secrets --all-namespaces -o json | kubectl replace -f - 保护敏感的云基础设施 Kubernetes 通常被部署在云环境中的虚拟机上。因此，管理员应该仔细考虑 Kubernetes 工作节点所运行的虚拟机的攻击面。在许多情况下，在这些虚拟机上运行的 Pod 可以在不可路由的地址上访问敏感的云元数据服务。这些元数据服务为网络行为者提供了关于云基础设施的信息，甚至可能是云资源的短期凭证。网络行为者滥用这些元数据服务进行特权升级。Kubernetes 管理员应通过使用网络策略或通过云配置策略防止 Pod 访问云元数据服务。由于这些服务根据云供应商的不同而不同，管理员应遵循供应商的指导来加固这些访问载体。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"b0b9135a069c9755963eeaaee6b0cbe0","permalink":"https://lib.jimmysong.io/kubernetes-hardening-guidance/network-separation-and-hardening/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/kubernetes-hardening-guidance/network-separation-and-hardening/","section":"kubernetes-hardening-guidance","summary":"集群网络是 Kubernetes 的一个核心概念。容器、Pod、服务和外部服务之间的通信必须被考虑在内。默认情况下，很少有网络策略来隔离资源，防止集群被破坏时的横向移动或升级。资源隔离和加密是限制网络行为者在集群内转移和升","tags":["Kubernetes","安全"],"title":"网络隔离和加固","type":"book"},{"authors":null,"categories":["可观测性"],"content":"现在我们了解了组成 OpenTelemetry 的各个构件，我们应该如何将它们组合成一个强大的生产管道？\n答案取决于你的出发点是什么。OpenTelemetry 是模块化的，设计成可以在各种不同的规模下工作。你只需要使用相关的部分。这就是说，我们已经创建了一个建议的路线图供你遵循。\n安装 OpenTelemetry 客户端 可以单独使用 OpenTelemetry 客户端而不部署收集器。这种基本设置通常是绿地部署的充分起点，无论是测试还是初始生产。OpenTelemetry SDK 可以被配置为直接向大多数可观测性服务传输遥测数据。\n挑选一个导出器 默认情况下，OpenTelemetry 使用 OTLP 导出数据。该 SDK 提供了几种常见格式的导出器。Zipkin、Prometheus、StatsD 等。如果你使用的可观测性后端没有原生支持 OTLP，那么这些其他格式中的一种很可能会被支持。安装正确的导出器并将数据直接发送到你的后端系统。\n安装库仪表 除了 SDK，OpenTelemetry 仪表必须安装在所有 HTTP 客户端、Web 框架、数据库和应用程序的消息队列中。如果这些库中有一个缺少仪表，上下文传播就会中断，导致不完整的追踪和混乱的数据。\n在某些语言中，如 Java，仪表可以自动安装，这就更容易了。请确保了解 OpenTelemetry 如何在你使用的编程语言中管理仪表，并仔细检查仪表是否正确安装在你的应用程序中。\n选择传播器 仔细检查你的系统需要哪些传播器也很重要。默认情况下，OpenTelemetry 使用 W3C 的追踪上下文和 Baggage 传播器。然而，如果你的应用程序需要与使用不同的追踪传播器的服务进行通信，如 Zipkin 的 B3 或 AWS 的 X-Amzn，那么改变 OTEL_PROPAGATORS 配置以包括这个额外的传播器。\n如果 OpenTelemetry 最终要取代这些其他的追踪系统，我建议同时运行 trace-context 和额外的追踪传播器。这将使你在部署中逐步取代旧系统时，能够无缝地过渡到 W3C 标准。\n部署本地收集器 虽然有些系统有可能只使用客户端，但通过在你的应用程序所运行的机器上添加一个本地收集器，可以改善你的操作体验。\n运行一个本地收集器有许多好处，如图 7-1 所示。收集器可以生成机器指标（CPU、RAM 等），这是遥测的一个重要部分。收集器还可以完成任何需要的数据处理任务，如从追踪和日志数据中清除 PII。\n   图 7-1：一个本地收集器（C）从本地应用程序（A）接收遥测数据，同时收集主机指标（H）。收集器将合并的遥测数据输出到管道的下一个阶段。  运行收集器后就可以将大多数遥测配置从你的应用程序中移出。遥测配置通常是特定的部署，而不是特定的应用。SDK 可以简单地设置为使用默认配置，总是将 OTLP 数据导出到预定义的本地端口。通过管理本地收集器，运维可以在不需要与应用程序开发人员协调或重新启动应用程序的情况下进行配置更改。在通过复杂的 CI/CD（持续集成 / 持续交付）管道移动应用程序时，这尤其有帮助，因为在不同的暂存和负载测试环境中，遥测需要不同的处理方式。\n快速发送遥测数据到本地收集器，可以作为一个缓冲器来处理负载，并确保在应用程序崩溃时，缓冲的遥测数据不会丢失。\n部署收集器处理器池 如果你的本地收集器开始执行大量的缓冲和数据处理，它就会从你的应用程序中窃取资源。这可以通过部署一个只运行收集器的机器池来解决，这些机器位于负载均衡器后面，如图 7-2 所示。现在可以根据数据吞吐量来管理收集器池的大小。\n   图 7-2：应用程序（A）可以通过使用路由器或负载均衡器向收集器（C）池发送遥测信息。  本地收集器现在可以关闭其处理器以释放资源。它们继续收集机器级遥测数据，作为来自本地应用程序的 OTLP 的转发机制。\n添加额外的处理池 有时，单个收集器池是不够的。一些任务可能需要以不同的速度扩展。将收集器池分割成一个更专门的池的管道，可能允许更有效和可管理的扩展策略，因为每个专门的收集器池的工作负载变得更可预测。\n一旦你达到了这个规模，就没有什么部署的问题了。大规模系统的专门需求往往是独特的，这些需求将驱动你的可观测性管道的拓扑结构。利用收集器提供的灵活性，根据你的需求来定制每一件事情。我建议对每个收集器配置的资源消耗进行基准测试，并使用这些信息来创建弹性的、自动扩展的收集器池。\n用收集器管理现有的遥测数据 上面描述的路线图适用于上线 OpenTelemetry。但你应该如何处理现有的遥测？大多数运行中的系统已经有了某种形式的指标、日志和（可能有）追踪。而大型的、长期运行的系统往往最终会有多个遥测解决方案的补丁。不同的组件可能是在不同的时代建立的，有些组件可能是从外部继承的，比如收购。从可观测性的角度来看，这可能会导致混乱的局面。\n即使在这样复杂的遗留情况下，仍然有可能过渡到 OpenTelemetry，而不需要停机或一次重写所有的服务。秘诀是首先部署一个收集器，作为一个透明的代理。\n在收集器中，为接收你的系统目前产生的每一种类型的遥测设置接收器，并与以完全相同的格式发送遥测的导出器相连。一个 StatsD 接收器连接到一个 StatsD 导出器，一个 Zipkin 接收器连接到一个 Zipkin 导出器，以此类推。这种透明的代理可以逐步推出，而不会造成干扰。一旦所有的远程测量都由这些收集器来调解，就可以引入额外的处理。甚至在你把你的仪表切换到 OpenTelemetry 之前，你可能会发现这些收集器是管理和组织你当前拼凑的遥测系统的一个有用的方法。图 7-3 显示了一个收集器处理来自各种来源的数据。\n   图 7-3：收集器可以帮助管理复杂的、拼凑的可观测性系统，这些系统以各种格式向各种存储系统发送数据。  为了开始将服务切换到 OpenTelemetry，可以在收集器上添加一个 OTLP 接收器，与现有的导出器相连。随着服务转向使用 OpenTelemetry 客户端，它们将 OTLP 发送到收集器，收集器将把 OTLP 翻译成这些系统以前产生的相同数据。这使得 OpenTelemetry 可以由不同的应用团队逐步上线，而不会出现中断。\n转移供应商 一旦所有的遥测流量都通过收集器发送，切换到一个新的可观测性后端就变得很容易了：只需在收集器中添加一个导出器，将数据发送到你想尝试的新系统，并将遥测数据同时发送到旧系统和新系统。通过向两个系统发送数据，你创造了一个重叠的覆盖范围。如果你喜欢新系统，你可以在一段时间后让旧系统退役，以避免在可视性方面产生差距。图 7-4 说明了这个过程。\n   图 7-4：使用收集器在可观测性后端之间迁移而不中断服务。  也可以使用 OpenTelemetry 在多个供应商之间进行测验。你可以同时向多个系统发送遥测信息，并直接比较系统，看哪一个最适合你的需要。\n","date":1643990400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"16f19f36772970ce7b130f2c1ed4484e","permalink":"https://lib.jimmysong.io/opentelemetry-obervability/suggested-setups-and-telemetry-pipelines/","publishdate":"2022-02-05T00:00:00+08:00","relpermalink":"/opentelemetry-obervability/suggested-setups-and-telemetry-pipelines/","section":"opentelemetry-obervability","summary":"第 7 章：建议的设置和遥测管道","tags":["OpenTelemetry"],"title":"第 7 章：建议的设置和遥测管道","type":"book"},{"authors":null,"categories":["网络"],"content":"验证安装 检查 DaemonSet 的状态并验证所有需要的实例都处于 ready 状态：\n$ kubectl --namespace kube-system get ds NAME DESIRED CURRENT READY NODE-SELECTOR AGE cilium 1 1 0 \u0026lt;none\u0026gt; 3s 在此示例中，我们看到 1 个期望的状态，0 个 ready 状态。这表明有问题。下一步是通过在 k8s-app=cilium 标签上匹配列出所有 cilium pod，并根据每个 pod 的重启次数对列表进行排序，以便轻松识别失败的 pod：\n$ kubectl --namespace kube-system get pods --selector k8s-app=cilium \\  --sort-by=\u0026#39;.status.containerStatuses[0].restartCount\u0026#39; NAME READY STATUS RESTARTS AGE cilium-813gf 0/1 CrashLoopBackOff 2 44s cilium-813gf pod 失败并且已经重新启动了 2 次。让我们打印该 pod 的日志文件来调查原因：\n$ kubectl --namespace kube-system logs cilium-813gf INFO _ _ _ INFO ___|_| |_|_ _ _____ INFO | _| | | | | | | INFO |___|_|_|_|___|_|_|_| INFO Cilium 0.8.90 f022e2f Thu, 27 Apr 2017 23:17:56 -0700 go version go1.7.5 linux/amd64 CRIT kernel version: NOT OK: minimal supported kernel version is \u0026gt;= 4.8 在此示例中，失败的原因是在工作节点上运行的 Linux 内核不符合系统要求。\n如果根据这些简单的步骤无法发现问题的原因，请来我们的 Slack channel。\n集群外的 APIserver 如果你出于某种原因在集群外部运行 Kubernetes Apiserver（例如将主节点保留在防火墙后面），请确保您也在主节点上运行 Cilium。否则，由 Apiserver 创建的 Kubernetes pod 代理将无法路由到 pod IP，并且你在尝试将流量代理到 pod 时可能会遇到错误。\n你可以将 Cilium 作为静态 pod 运行，或者为 Cilium DaemonSet 设置容忍，以确保将 Cilium pod 安排在你的主节点上。执行此操作的确切方法取决于你的设置。\n","date":1655438400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"a3f5cbae95d174e8a2abd74ee52996d9","permalink":"https://lib.jimmysong.io/cilium-handbook/kubernetes/troubleshooting/","publishdate":"2022-06-17T12:00:00+08:00","relpermalink":"/cilium-handbook/kubernetes/troubleshooting/","section":"cilium-handbook","summary":"验证安装 检查 DaemonSet 的状态并验证所有需要的实例都处于 ready 状态： $ kubectl --namespace kube-system get ds NAME DESIRED CURRENT READY NODE-SELECTOR AGE cilium 1 1 0 \u003cnone\u003e 3s 在此示例中，我们看到 1 个期望的状态，0 个 ready 状态。这表明有问题。下一步是通过在 k8s-app=cilium 标签上匹配列出所有 cilium pod，并根","tags":["Cilium"],"title":"故障排除","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"容器存储接口（Container Storage Interface），简称 CSI，CSI 试图建立一个行业标准接口的规范，借助 CSI 容器编排系统（CO）可以将任意存储系统暴露给自己的容器工作负载。有关详细信息，请查看设计方案。\ncsi 卷类型是一种 out-tree（即跟其它存储插件在同一个代码路径下，随 Kubernetes 的代码同时编译的） 的 CSI 卷插件，用于 Pod 与在同一节点上运行的外部 CSI 卷驱动程序交互。部署 CSI 兼容卷驱动后，用户可以使用 csi 作为卷类型来挂载驱动提供的存储。\nCSI 持久化卷支持是在 Kubernetes v1.9 中引入的，作为一个 alpha 特性，必须由集群管理员明确启用。换句话说，集群管理员需要在 apiserver、controller-manager 和 kubelet 组件的 “--feature-gates =” 标志中加上 “CSIPersistentVolume = true”。\nCSI 持久化卷具有以下字段可供用户指定：\n driver：一个字符串值，指定要使用的卷驱动程序的名称。必须少于 63 个字符，并以一个字符开头。驱动程序名称可以包含 “。”、“ - ”、“_” 或数字。 volumeHandle：一个字符串值，唯一标识从 CSI 卷插件的 CreateVolume 调用返回的卷名。随后在卷驱动程序的所有后续调用中使用卷句柄来引用该卷。 readOnly：一个可选的布尔值，指示卷是否被发布为只读。默认是 false。  使用说明 下面将介绍如何使用 CSI。\n动态配置 可以通过为 CSI 创建插件 StorageClass 来支持动态配置的 CSI Storage 插件启用自动创建/删除 。\n例如，以下 StorageClass 允许通过名为 com.example.team/csi-driver 的 CSI Volume Plugin 动态创建 “fast-storage” Volume。\nkind:StorageClassapiVersion:storage.k8s.io/v1metadata:name:fast-storageprovisioner:com.example.team/csi-driverparameters:type:pd-ssd要触发动态配置，请创建一个 PersistentVolumeClaim 对象。例如，下面的 PersistentVolumeClaim 可以使用上面的 StorageClass 触发动态配置。\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:my-request-for-storagespec:accessModes:- ReadWriteOnceresources:requests:storage:5GistorageClassName:fast-storage当动态创建 Volume 时，通过 CreateVolume 调用，将参数 type：pd-ssd 传递给 CSI 插件 com.example.team/csi-driver 。作为响应，外部 Volume 插件会创建一个新 Volume，然后自动创建一个 PersistentVolume 对象来对应前面的 PVC 。然后，Kubernetes 会将新的 PersistentVolume 对象绑定到 PersistentVolumeClaim，使其可以使用。\n如果 fast-storage StorageClass 被标记为默认值，则不需要在 PersistentVolumeClaim 中包含 StorageClassName，它将被默认使用。\n预配置 Volume 您可以通过手动创建一个 PersistentVolume 对象来展示现有 Volumes，从而在 Kubernetes 中暴露预先存在的 Volume。例如，暴露属于 com.example.team/csi-driver 这个 CSI 插件的 existingVolumeName Volume：\napiVersion:v1kind:PersistentVolumemetadata:name:my-manually-created-pvspec:capacity:storage:5GiaccessModes:- ReadWriteOncepersistentVolumeReclaimPolicy:Retaincsi:driver:com.example.team/csi-drivervolumeHandle:existingVolumeNamereadOnly:false附着和挂载 您可以在任何的 pod 或者 pod 的 template 中引用绑定到 CSI volume 上的 PersistentVolumeClaim。\nkind:PodapiVersion:v1metadata:name:my-podspec:containers:- name:my-frontendimage:dockerfile/nginxvolumeMounts:- mountPath:\u0026#34;/var/www/html\u0026#34;name:my-csi-volumevolumes:- name:my-csi-volumepersistentVolumeClaim:claimName:my-request-for-storage当一个引用了 CSI Volume 的 pod 被调度时， Kubernetes 将针对外部 CSI 插件进行相应的操作，以确保特定的 Volume 被 attached、mounted， 并且能被 pod 中的容器使用。\n关于 CSI 实现的详细信息请参考设计文档。\n创建 CSI 驱动 Kubernetes 尽可能少地指定 CSI Volume 驱动程序的打包和部署规范。这里记录了在 Kubernetes 上部署 CSI Volume 驱动程序的最低要求。\n最低要求文件还包含概述部分，提供了在 Kubernetes 上部署任意容器化 CSI 驱动程序的建议机制。存储提供商可以运用这个机制来简化 Kubernetes 上容器式 CSI 兼容 Volume 驱动程序的部署。\n作为推荐部署的一部分，Kubernetes 团队提供以下 sidecar（辅助）容器：\n  External-attacher\n可监听 Kubernetes VolumeAttachment 对象并触发 ControllerPublish 和 ControllerUnPublish 操作的 sidecar 容器，通过 CSI endpoint 触发 ；\n  External-provisioner\n监听 Kubernetes PersistentVolumeClaim 对象的 sidecar 容器，并触发对 CSI 端点的 CreateVolume 和DeleteVolume 操作；\n  Driver-registrar(DEPRECATED)\n使用 Kubelet（将来）注册 CSI 驱动程序的 sidecar 容器，并将 NodeId （通过 GetNodeID 调用检索到 CSI endpoint）添加到 Kubernetes Node API 对象的 annotation 里面。\n  Cluster Driver Registrar\n创建 CSIDriver 这个集群范围的 CRD 对象。\n  Node Driver Registrar\n替代 Driver-registrar。\n  存储供应商完全可以使用这些组件来为其插件构建 Kubernetes Deployment，同时让它们的 CSI 驱动程序完全意识不到 Kubernetes 的存在。\n另外 CSI 驱动完全是由第三方存储供应商自己维护的，在 kubernetes 1.9 版本中 CSI 还处于 alpha 版本。\n参考  Container Storage Interface (CSI) - github.com  ","date":1653062400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"b58aa1c976c340e4811faddea473758b","permalink":"https://lib.jimmysong.io/kubernetes-handbook/architecture/open-interfaces/csi/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/architecture/open-interfaces/csi/","section":"kubernetes-handbook","summary":"容器存储接口（Container Storage Interface），简称 CSI，CSI 试图建立一个行业标准接口的规范，借助 CSI 容器编排系统（CO）可以将任意存储系统暴露给自己的容器工作负载。有关详细信息，请查看设计","tags":["Kubernetes"],"title":"容器存储接口（CSI）","type":"book"},{"authors":null,"categories":["安全"],"content":"DevSecOps 的好处包括：\n 各个 IT 团队之间，特别是开发人员、运维和安全团队以及其他利益相关者之间更好的沟通和协作。导致 更好的生产力。 简化软件开发、交付和部署过程 —— 由于自动化，停机时间减少，发布时间加快，基础设施和运维成本降低，效率提高。 通过实施零信任来减少攻击面，这也限制了横向移动，从而防止攻击升级。具有现代行为预防能力的持续监控进一步促进了这一点。 安全优势。通过对每个请求的验证监控、警报和反馈机制来提高安全性，因为可观测性是代码。这些将在以下段落中详细描述。具体的能力包括：  a. 运行时：杀死恶意容器。 b. 反馈：由于一个错误的程序更新了代码并重新触发了管道，所以反馈到了正确的存储库。 c. 监测新的和终止的服务，并调整相关服务（如服务代理）。 d. 启用安全断言。不可绕过 —— 通过在同一空间执行的代理、安全会话、强大的认证和授权以及安全的状态转换。   启用持续授权操作（C-ATO），在本节末尾详细描述。 对每个请求的验证和上述的反馈机制将在下面进一步描述： 每个请求的验证。来自用户或客户端应用程序（服务）的每个请求都要经过验证和授权（使用 OPA 或任何外部授权引擎或 接纳控制器 等机制，它们是平台的组成部分）。授权引擎提供特定于应用域的策略执行，而接纳控制器则提供与特定平台的端点对象（如 Pod、部署、命名空间）相关的平台特定策略。具体来说，接纳控制器会进行突变和验证。突变的接纳控制器解析每个请求，并在将其向下转发之前对请求进行修改（突变）。一个例子是为没有被用户在请求中设置的规格设置默认值，以确保在集群上运行的工作负载是统一的，并遵循集群管理员定义的特定标准。另一个例子是为 Pod 添加特定的资源限制（如果资源限制没有为该 Pod 设置），然后向下转发（如果请求中没有这个字段，通过添加这个字段来突变请求）。通过这样做，集群中的所有 Pod 将始终有一个根据规范设置的资源限制，除非明确说明。验证接纳控制器会拒绝那些不遵循特定规范的请求。例如，没有一个 Pod 请求可以将安全上下文设置为以根用户身份运行。 反馈机制：  一些在运行时发现的问题的补救措施可能需要在源代码中处理或修复。应该有一个流程，针对正确的代码库自动打开一个问题，以修复问题并重新触发 DevSecOps 管道。 向应用程序托管平台提供反馈回路（例如，杀死包含恶意容器的 Pod 的通知）。 通过监控应用程序的配置，提供主动的动态安全（例如，监控引入到应用程序的新荚 / 容器，并生成和注入代理以照顾其安全通信需求）。 启用关于应用程序的几个安全断言：不可绕过（即在所有使用场景下始终执行的策略）、整个应用程序代码的受信任和不受信任部分、没有凭证和特权泄漏、受信任的通信路径和安全状态转换。 启用关于性能参数的断言（例如，网络弹性参数，如在故障、冗余和可恢复性功能下继续运行）。 总的来说，更快地吸收反馈意见，使软件得到更快的改进。    ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"426a193b5fe19182dbacd20b0a8778bd","permalink":"https://lib.jimmysong.io/service-mesh-devsecops/implement/benefits-of-devsecops-primitives-to-application-security-in-the-service-mesh/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/service-mesh-devsecops/implement/benefits-of-devsecops-primitives-to-application-security-in-the-service-mesh/","section":"service-mesh-devsecops","summary":"DevSecOps 的好处包括： 各个 IT 团队之间，特别是开发人员、运维和安全团队以及其他利益相关者之间更好的沟通和协作。导致 更好的生产力。 简化软件开发、交付和部署过程 —— 由于自动化，停机时间减少，发布时间加快，基础设施和","tags":["Service Mesh","DevSecOps"],"title":"4.9 DevSecOps 原语对服务网格中应用安全的好处","type":"book"},{"authors":null,"categories":["Envoy"],"content":"Envoy 支持许多可配置的超时，这取决于你使用代理的场景。\n我们将在 HCM 部分看一下不同的可配置超时。请注意，其他过滤器和组件也有各自的超时时间，我们在此不做介绍。\n在配置的较高层次上设置的一些超时 —— 例如在 HCM 层次，可以覆盖较低层次上的配置，例如 HTTP 路由层次。\n最著名的超时可能是请求超时。请求超时（request_timeout）指定了 Envoy 等待接收整个请求的时间（例如 120s）。当请求被启动时，该计时器被激活。当最后一个请求字节被发送到上游时，或者当响应被启动时，定时器将被停用。默认情况下，如果没有提供或设置为 0，则超时被禁用。\n类似的超时称为 idle_timeout，表示如果没有活动流，下游或上游连接何时被终止。默认的空闲超时被设置为 1 小时。空闲超时可以在 HCM 配置的 common_http_protocol_options 中设置，如下所示。\n...filters:- name:envoy.filters.network.http_connection_managertyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManagerstat_prefix:ingress_httpcommon_http_protocol_options:# 设置空闲超时为 10 分钟idle_timeout:600s...为了配置上游连接的空闲超时，我们可以使用相同的字段 common_http_protocol_options，但在集群部分。\n还有一个与 Header 有关的超时，叫做 request_headers_timeout。这个超时规定了 Envoy 等待接收请求头信息的时间（例如 5s）。该计时器在收到头信息的第一个字节时被激活。当收到头信息的最后一个字节时，该时间就会被停用。默认情况下，如果没有提供或设置为 0，则超时被禁用。\n其他一些超时也可以设置，比如 stream_idle_timeout、drain_timeout 和 delayed_close_timeout。\n接下来就是路由超时。如前所述，路由层面的超时可以覆盖 HCM 的超时和一些额外的超时。\n路由 timeout 是指 Envoy 等待上游做出完整响应的时间。一旦收到整个下游请求，该计时器就开始计时。超时的默认值是 15 秒；但是，它与永不结束的响应（即流媒体）不兼容。在这种情况下，需要禁用超时，而应该使用 stream_idle_timeout。\n我们可以使用 idle_timeout 字段来覆盖 HCM 层面上的 stream_idle_timeout。\n我们还可以提到 per_try_timeout 设置。这个超时是与重试有关的，它为每次尝试指定一个超时。通常情况下，个别尝试应该使用比 timeout 域设置的值更短的超时。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"8af362e015ada559d62f1bc27496d636","permalink":"https://lib.jimmysong.io/envoy-handbook/hcm/timeout/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/envoy-handbook/hcm/timeout/","section":"envoy-handbook","summary":"Envoy 支持许多可配置的超时，这取决于你使用代理的场景。 我们将在 HCM 部分看一下不同的可配置超时。请注意，其他过滤器和组件也有各自的超时时间，我们在此不做介绍。 在配置的较高层次上设置的一些超时 —— 例如在 HCM 层次，","tags":["Envoy"],"title":"超时","type":"book"},{"authors":null,"categories":["云原生"],"content":"云原生应用程序依赖基础架构才能运行，反过来说，云原生基础架构也需要云原生应用程序来维持。\n对于传统基础架构，维护和升级基本上都是由人工完成。可能是在单机上手动运行服务或使用自动化工具定义基础结构和应用程序的快照。\n但是，如果基础架构可以由应用程序管理，同时基础架构又可以管理应用程序，那么基础架构工具就会成为另一种应用程序。工程师对于基础架构的责任可以用调解器模式表示，内置于该基础架构上运行的应用程序中。\n我们花了三章来说明如何构建可以管理基础架构的应用程序。本章将介绍如何在基础架构上运行云原生应用或其它任何应用。\n如前所述，保持基础架构和应用程序的简单非常重要。解决应用程序复杂性最常用的方法就是把应用程序分解成小的，易于理解的组件。通常通过创建单一职责的服务来实现，或者将代码分解为一系列事件触发的函数。\n随着小型、可部署单元的扩容成多份即使是最自动化的基础架构也可能被压垮。管理大量应用程序的唯一方法是让它们承担第 1 章中所述的功能性操作。应用程序需要在可以按规模管理之前变成原生云。\n学习完本章不会帮助您建下一个伟大的应用程序，但您将了解一些基础知识，这块可以让您的应用程序在云原生基础架构是良好运行。\n应用程序设计 已经有很多教您如何构建应用程序的书了，本书不打算再讨论。但是，了解应用程序架构如何影响基础架构设计仍然很重要。\n正如我们在第 1 章中讨论的那样，我们假设应用程序设计成云原生的，这样可以从云原生基础架构中获得最大收益。云原生的本质是应用程序由软件而不是人类来管理。\n应用程序的设计和打包方式是分开考虑的。应用程序可以是云原生的，打包为 RPM 或 DEB 文件，可以部署到虚拟机而不是容器。它们可以是单体应用或微服务，可以用 Java 或 Go 编写。\n这些实现细节不影响应用程序被设计成在云上运行。\n假设我们有一个用 Go 编写的应用程序，使用容器打包，运行在 Kubernetes 上，视为微服务运行。\n我们假想的这个应用是 “云原生” 的吗？\n如果应用程序将所有活动日志记录到文件，还硬编码数据库 IP 地址呢？也许它不接受运行时配置并将状态存储在本地磁盘上。如果它不以可预见的方式存在或挂起并等待人工调试呢？\n这个应用所选择的语言和打包方式，可能会让您觉的它是云原生的，但实际上它根本不是。像 Kubernetes 这样的框架可以通过各种功能来管理这个应用，但即使它可以运行，但还是需要人类来维护。\n第 1 章详细介绍了使应用程序在云原生基础架构上运行得更好的一些特点。如果我们具有第 1 章中规定的特点，应用程序还有另一个考虑因素：我们如何有效地管理它们？\n实施云原生模式 诸如弹性伸缩、服务发现、配置、日志、健康检查和相关监控指标等功能都可以以不同方式在应用程序中实现。实现这些功能的常见做法是通过导入实现相关功能的标准语言库。Netflix OSS 和 Twitter 的 Finagle 是在 Java 语言库中实现这些功能的很好的例子。\n应用程序可以导入和使用库，自动获得库中提供许多相关的功能，无需额外的代码。当一个组织内支持的语言很少时，这种模式很有意义。这种模式很容易实现最佳实践。\n当组织开始实施微服务时，它们往往倾向于使用多语言服务。这样可以自由地为不同的服务选择正确的语言，但是很难为每种语言维护库。\n获得该功能的另一种方法是通过所谓的 “Sidecar” 模式。此模式应用程序本身与实现管理功能的应用程序绑定在一起。通常作为单独的容器来实现，但也可以通过在虚拟机上运行另一个守护进程来实现。\nSidecar 的例子包括以下内容：\nEnvoy 代理\n为服务增加弹性伸缩和监控指标\n注册\n通过外部服务发现注册服务\n动态配置\n订阅配置更改并通知服务进程重新加载\n健康检查\n提供用于检查应用程序运行状况的 HTTP 端点 (Endpoints)\nSidecar 容器还可以用来适配 Polyglot 容器，通过暴露特定于语言的端点与使用库的应用程序进行交互。来自 Netflix 的 Prana 正是为那些不使用标准 Java 库的应用程序而定制的。\n当有团队集中管理特定的 Sidecar 进程时，Sidecar 模式很有意义。如果工程师想要在它们的服务中暴露监控指标，它们可以将其构建到应用程序中，或者一个单独的团队也可以提供处理日志记录输出并公开计算出的监控指标的 Sidecar 应用。\n在这两种情况下，都可以在不修改应用程序的情况下为应用添加功能。在可以使用软件管理应用程序后，我们来看看如何管理应用程序的生命周期。\n应用程序生命周期 云原生应用程序除了生命周期应该由软件管理以外，其生命周期本身与传统应用程序并没有什么不同。\n本章不打算解释管理应用程序时涉及的所有模式和选项。我们将简要讨论几个阶段，在云原生基础架构之上运行云原生应用，这几个阶段受益程度最高：部署、运行和下线。\n这些主题并不都包含所有选项，但还有很多其他书籍和文章可供参考，这取决于应用程序的架构，语言和所选库。\n部署 部署是应用程序最依赖基础架构的一个领域。虽然没有什么东西会阻止应用程序自行部署，但基础架构管理还可以管理更多的方面。\n本文不会涉及集成和交付，但是在这个领域的一些做法很明确。应用程序部署不仅仅是获取和运行代码。\n云原生应用程序旨在由软件管理应用生命周期的各个阶段。这包括周期性的健康检查和部署初始化。应尽可能地消除技术、流程和策略中人为造成的瓶颈。\n应用程序的部署首先应该是自动、自助的，如果应用正在活跃开发中，则应该是被频繁触发的。也应该可以被测试、验证可稳定运行。\n新版本和新功能的发布时很少会有一次性替换应用程序的所有实例的情况。新功能在配置标志成 “gated”，可以在不重启应用的情况下选择性地动态启用新功能。版本升级部分发布，通过测试进行验证，并在所有测试通过时以受控方式发布。\n当启用新功能或部署新版本时，应该存在控制流向或隔离应用流量的机制（请参阅附录 A）。通过缓慢的部署和更快的应用性能反馈循环进行新功能的试用，可以限制中断带来的影响。\n基础架构应负责部署软件的所有细节。工程师可以定义应用程序版本，基础架构要求和依赖关系，并且基础架构将朝着该状态发展，直至满足所有要求或需求更改。\n运行 运行应用程序应该是应用程序生命周期中最平稳最稳定的阶段。运行软件最重要的两个的方面在第 1 章中讨论：了解应用程序在做什么以及可操作性即可以根据需要更改应用程序。\n我们已经在第 1 章中详细介绍了关于应用报告健康和遥测数据的可观测性，但是当事情不按预期工作时，你会做什么？如果应用程序的遥测数据显示它不符合 SLO，那么如何解决和调试应用程序？\n对于云原生应用程序，你不应该通过 SSH 连接到服务器的形式查看日志。如果你需要 SSH，更应该考虑使用日志或其它的服务替代。\n你仍然需要访问应用程序（API）和日志数据（云日志记录）以及获取在堆栈中的服务，但这值得通过演练来查看是否需要传统工具。当事件中断时，你需要一个调试应用程序和基础架构组件的方法。\n在做系统调试时，你应该首先查看你的基础架构测试，如第 5 章所述。测试应公开所有未正确配置或未提供预期性能的基础架构组件。\n不能说因为你不管理底层基础架构就意味着基础架构不可能出问题。通过测试来验证期望值将确保你的基础架构能够以你期望的方式运行。\n在排除基础架构后，你应该查看应用程序以获取更多信息。应用程序调试的最佳位置是应用性能管理（APM）以及可能通过 OpenTracing 等标准进行的分布式应用程序跟踪。\nOpenTracing 示例、实现和 APM 不在本书的范围之内。总而言之，OpenTracing 允许你在整个应用程序中跟踪调用，以更轻松地识别网络和应用程序通信问题。OpenTracing 的示例可视化可以在图 7-1 中看到。APM 为你的应用程序添加了用于向收集服务报告指标和故障的工具。\n   图 7-1. OpenTracing 可视化  当测试和跟踪仍然没有暴露出问题时，有时你只需要在应用程序上启用更详细的日志记录。但是，如何在不破坏问题的情况下启用调试？\n运行时配置对于应用程序很重要，但在云原生环境中，无须重启应用程序，配置就应该是动态的。配置选项仍然通过应用程序中的库实现，但标志值应该能够通过集中协调器，应用程序 API 调用，HTTP 协议 Header 或多种方式进行动态更改。\nNetflix 的 Archaius 和 Facebook 的 GateKeeper 是动态配置的两个例子。前 Facebook 工程师经理 Justin Mitchell 在 Quora 的帖子中分享到：\n GateKeeper 是从代码部署中解耦出来的功能。我们可以在几天或几周内发布新功能，因为我们观察了用户指标、性能并确保服务可以随时扩展。\n 允许对应用程序配置进行动态控制，可以实现更多对曝光过度的新功能的控制并更好地测试已部署代码的覆盖范围。可以很容易的发布新代码并不意味着这是适合所有情况的正确解决方案。\n基础架构可以帮助解决此问题，并通过协调何时启用新功能和基于高级网络策略的路由控制来启用更灵活的应用程序。这种模式还允许更细粒度的控制和更好的协调发布或回滚场景。\n在动态的自助服务环境中，部署的应用程序数量将快速增长。你需要确保有一个简单的方法来动态调试应用类似自助服务模型中部署的应用。\n工程师喜欢发布新应用程序一样，反过来很难让它们下线旧应用程序。即使如此，旧应用下线仍然是应用程序生命周期中的关键阶段。\n下线 部署新的应用程序和服务在快速迭代的环境中很常见。下线应用程序应该像创建应用一样自动化。\n如果新的服务和资源被自动化部署与监控，则它们应该按照相同标准下线。尽快部署新服务而不删除未使用的服务是应对技术债务的最简单方法。\n识别应该下线的服务和资源，这是一个特定的业务。你可以使用应用程序遥测的经验数据来了解某个应用程序是否正在被使用，但是下线应用程序的决定应由该业务决定。\n基础架构组件（例如，VM 实例和负载均衡器端点）应在不需要时被自动清理。自动化组件清理的一个例子是 Netflix 的 Janitor Monkey。该公司在一篇博文中解释道：\n Janitor Monkey 通过应用一组规则来决定资源是否应该成为候选的清理内容。如果任何规则确定该资源是被清理的候选内容，则 Janitor Monkey 标记该资源并安排清理时间。\n 所有这些应用阶段的目标是让基础架构和软件来管理原本由人类管理的方面。我们采用协调模式与组件元数据相结合的方式来不断运行，并根据当前上下文对需要采取的高层次操作做出决策。以此来取代由人类编写的临时自动化脚本。\n应用程序的生命周期不是唯一一个需要依赖于基础架构的阶段。还有一些每个阶段都要依赖于基础架构服务的程序。我们将在下一节讨论一些提供给这类应用的支持服务和基础架构 API。\n对运行于基础架构上的应用的要求 云原生应用程序对基础架构的期望不仅只是执行二进制文件，它们还需要抽象、隔离与保证应用程序运行和管理。对于应用程序来说，需要提供 hook 和 API 以允许基础架构管理它们。为了实现这种模式，两者就需要有一种共生关系。\n我们在第 1 章中定义了云原生应用程序，并刚刚讨论了一些生命周期的要求。现在让我们看看云原生应用程序对从运行它们的基础架构建设的更多期望：\n 运行与隔离 资源分配和调度 环境隔离 服务发现 状态管理 监控和记录 监控指标聚合 调试和跟踪  所有这些期望都应该是服务的默认选项，或者是由自助 API 提供。我们将更详细地解释每个要求，以确保这些期望被明确的定义。\n应用程序运行和隔离 除了有时候需要的解释器，传统应用程序只需要一个内核就可以运行。 云原生应用仍然需要它们，但云原生应用运行时同样也需要与操作系统和其他应用程序隔离。隔离使多个应用能够在同一台服务器上运行并控制它们的依赖和资源。\n应用隔离有时被称为多租户。该术语可用于在同一服务器上运行的多个应用程序以及在共享集群中运行应用程序的多个用户。用户可以运行经过验证的可信代码，也可以运行你不能 …","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"ad20846ea2138695a69ea7682da56024","permalink":"https://lib.jimmysong.io/cloud-native-infra/managing-cloud-native-applications/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/cloud-native-infra/managing-cloud-native-applications/","section":"cloud-native-infra","summary":"云原生应用程序依赖基础架构才能运行，反过来说，云原生基础架构也需要云原生应用程序来维持。 对于传统基础架构，维护和升级基本上都是由人工完成。可能是在单机上手动运行服务或使用自动化工具定义基础结构和应用程","tags":["云原生"],"title":"第 7 章：管理云原生应用程序","type":"book"},{"authors":null,"categories":["安全"],"content":"认证和授权是限制访问集群资源的主要机制。如果集群配置错误，网络行为者可以扫描知名的 Kubernetes 端口，访问集群的数据库或进行 API 调用，而不需要经过认证。用户认证不是 Kubernetes 的一个内置功能。然而，有几种方法可以让管理员在集群中添加认证。\n认证  管理员必须向集群添加一个认证方法，以实现认证和授权机制。\n Kubernetes 集群有两种类型的用户：服务账户和普通用户账户。服务账户代表 Pod 处理 API 请求。认证通常由 Kubernetes 通过 ServiceAccount Admission Controller 使用承载令牌自动管理。不记名令牌被安装到 Pod 中约定俗成的位置，如果令牌不安全，可能会在集群外使用。正因为如此，对 Pod Secret 的访问应该限制在那些需要使用 Kubernetes RBAC 查看的人身上。对于普通用户和管理员账户，没有自动的用户认证方法。管理员必须在集群中添加一个认证方法，以实现认证和授权机制。\nKubernetes 假设由一个独立于集群的服务来管理用户认证。Kubernetes 文档中列出了几种实现用户认证的方法，包括客户端证书、承载令牌、认证插件和其他认证协议。至少应该实现一种用户认证方法。当实施多种认证方法时，第一个成功认证请求的模块会缩短评估的时间。管理员不应使用静态密码文件等弱方法。薄弱的认证方法可能允许网络行为者冒充合法用户进行认证。\n匿名请求是被其他配置的认证方法拒绝的请求，并且不与任何个人用户或 Pod 相联系。在一个设置了令牌认证并启用了匿名请求的服务器中，没有令牌的请求将作为匿名请求执行。在 Kubernetes 1.6 和更新的版本中，匿名请求是默认启用的。当启用 RBAC 时，匿名请求需要  system:anonymous 用户或 system:unauthenticated 组的明确授权。匿名请求应该通过向 API 服务器传递 --anonymous-auth=false 选项来禁用。启用匿名请求可能会允许网络行为者在没有认证的情况下访问集群资源。\n基于角色的访问控制 RBAC 是根据组织内个人的角色来控制集群资源访问的一种方法。在 Kubernetes 1.6 和更新的版本中，RBAC 是默认启用的。要使用 kubectl 检查集群中是否启用了 RBAC，执行 kubectl api-version。如果启用，应该列出rbac.authorization.k8s.io/v1 的 API 版本。云 Kubernetes 服务可能有不同的方式来检查集群是否启用了 RBAC。如果没有启用 RBAC，在下面的命令中用 --authorization-mode 标志启动 API 服务器。\nkube-apiserver --authorization-mode=RBAC 留下授权模式标志，如 AlwaysAllow，允许所有的授权请求，有效地禁用所有的授权，限制了执行最小权限的访问能力。\n可以设置两种类型的权限：Roles 和 ClusterRoles。Roles 为特定命名空间设置权限，而 ClusterRoles 则为所有集群资源设置权限，而不考虑命名空间。Roles 和 ClusterRoles 只能用于添加权限。没有拒绝规则。如果一个集群被配置为使用 RBAC，并且匿名访问被禁用，Kubernetes API 服务器将拒绝没有明确允许的权限。附录 J 中显示了一个 RBAC 角色的例子：pod-reader RBAC 角色。\n一个 Role 或 ClusterRole 定义了一个权限，但并没有将该权限与一个用户绑定。RoleBindings 和 ClusterRoleBindings 用于将一个 Roles 或 ClusterRoles 与一个用户、组或服务账户联系起来。角色绑定将角色或集群角色的权限授予定义的命名空间中的用户、组或服务账户。ClusterRoles 是独立于命名空间而创建的，然后可以使用 RoleBinding 来限制命名空间的范围授予个人。ClusterRoleBindings 授予用户、群组或服务账户跨所有集群资源的 ClusterRoles。RBAC RoleBinding 和 ClusterRoleBinding 的例子在附录 K：RBAC RoleBinding 和 ClusterRoleBinding 示例中。\n要创建或更新 Roles 和 ClusterRoles，用户必须在同一范围内拥有新角色所包含的权限，或者拥有对 rbac.authorization.k8s.io API 组中的 Roles 或 ClusterRoles 资源执行升级动词的明确权限。创建绑定后，Roles 或 ClusterRoles 是不可改变的。要改变一个角色，必须删除该绑定。\n分配给用户、组和服务账户的权限应该遵循最小权限原则，只给资源以必要的权限。用户或用户组可以被限制在所需资源所在的特定命名空间。默认情况下，为每个命名空间创建一个服务账户，以便 Pod 访问 Kubernetes API。可以使用 RBAC 策略来指定每个命名空间的服务账户的允许操作。对 Kubernetes API 的访问是通过创建 RBAC 角色或 ClusterRoles 来限制的，该角色具有适当的 API 请求动词和所需的资源，该行动可以应用于此。有一些工具可以通过打印用户、组和服务账户及其相关分配的 Roles 和 ClusterRoles 来帮助审计 RBAC 策略。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"6ac45830cf25146d3deec701992dbcc0","permalink":"https://lib.jimmysong.io/kubernetes-hardening-guidance/authentication-and-authorization/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/kubernetes-hardening-guidance/authentication-and-authorization/","section":"kubernetes-hardening-guidance","summary":"认证和授权是限制访问集群资源的主要机制。如果集群配置错误，网络行为者可以扫描知名的 Kubernetes 端口，访问集群的数据库或进行 API 调用，而不需要经过认证。用户认证不是 Kubernetes 的一个内置功能。然而，有几种方法可以让管理员在集","tags":["Kubernetes","安全"],"title":"认证和授权","type":"book"},{"authors":null,"categories":["可观测性"],"content":"上线一个新的遥测系统可能是一项复杂的工作。它需要整个工程组织的支持，不能一蹴而就。不要低估这可能会产生的问题！\n在大型组织中，通常有许多服务团队负责系统的不同部分。通常情况下，每个团队都需要付出一定的努力来使他们所管理的服务得到充分的工具化。而这些团队都有自己积压的工作，他们当然希望能够优先处理这些工作。\n不幸的是，可观测性计划在开始提供价值和证明其价值之前就会耗尽人的耐心。但通过仔细的计划和协调，这种情况是可以避免的。\n主要目标 在推广 OpenTelemetry 时，重要的是要记住，任何基于分布式追踪的可观测性系统都需要对参与事务的每个服务进行检测，以提供最大价值。如果只有部分服务被检测到，那么追踪就会被分割成小的、不相连的部分。\n这种散乱的仪表的结果是不可取的。这种情况——不一致的仪表和断裂的追踪是你想要避免的主要事情。如果追踪是断开的，运维人员仍然需要在他们的头脑中把所有的东西拼凑起来，以获得他们系统的情况。更糟糕的是，自动分析工具可以使用的数据非常有限。与人类运维人员不同，他们可以运用直觉，跳出框框来思考问题，而分析工具却只能使用他们得到的数据。由于数据有限，他们提供有用的见解的机会也将是有限的。\n为了避免这个陷阱，集中精力对一个工作流程进行检测，在进入下一个工作流程之前对其进行完整的追踪。大多数工作流程并不涉及大系统的每一个部分，所以这种方法将最大限度地减少分析开始和价值实现之前所需的工作量。\n选择一个高价值的目标 谈到实现价值，在开始进行仪表测量工作时，重要的是要有一个有吸引力的目标！这一点很重要。\n最有可能的是，有一个特别的问题促使人们去部署 OpenTelemetry。如果是这样的话，就把重点放在解决该问题所需的最小的推广上。否则，想一想那些众所周知的问题，解决它们就值得公布了。\n目前有哪些痛苦的、长期的问题在困扰着运维？减少系统延迟在哪里可以直接转化为商业价值？当人们问 “为什么这么慢？” 时，他们说的是系统的哪一部分？在选择第一个工作流程时，请以这些信息为指导。\n识别一个有吸引力的目标有两个好处。首先，它可以更有利于说服众人，因为有一个具体的理由来做这项工作。这使得它更容易说服有关团队优先考虑增加指导，并以协调的方式进行。\n第二，速战速决给你的新的可观测性系统一个闪亮的机会。第一次通过分布式追踪的视角来分析一个软件系统时，几乎总是会产生有用的见解。证明可观测性的价值可以引起很多人的兴趣，并有助于降低采用时任何挥之不去的障碍。\n如果直接进入生产是困难的，“生产支持” 系统也是一个好的开始。可以对 CI/CD 系统进行检测，以帮助了解构建和部署的性能。在这里，整个组织都会感受到性能的大幅提升，并可以为将 OpenTelemetry 转移到生产中提供良好的理由。\n集中遥测管理 展开和管理遥测系统从集中化中获益良多。在一些组织中，会有一个平台或信息结构团队可以接触到每一项服务。像这样的团队是集中管理遥测的一个好地方，这可以提供巨大的帮助。遥测管道最好被认为是它自己的系统；允许一个团队操作整个遥测管道，往往比要求许多团队各自拥有系统的一部分要好。\n在软件层面，将 OpenTelemetry 设置与已经广泛部署的代码管理工具——例如共享的启动脚本和应用框架整合起来，减少了每个团队需要管理的代码量。这有助于确保服务与最新的版本和配置保持同步，并使采用更加容易。\n另一个关键工具是一个集中的知识库。OpenTelemetry 有文档，但它是通用的。创建特定于在你的组织内部署、管理和使用 OpenTelemetry 的文档。大多数工程师都是第一次接触 OpenTelemetry 和分布式追踪，这对他们的帮助怎么强调都不为过。\n先广度后深度 还有一个关于合理分配精力的说明。当对一个工作流程进行端对端检测时，通常只需安装 OpenTelemetry 附带的仪表，加上你的组织所使用的任何内部或自创框架的仪表即可。没有必要深入检测应用程序代码，至少在开始时没有必要。OpenTelemetry 附带的标准跨度和指标足以让你识别大多数问题；必要时可以有选择地增加更深层次的检测。在添加这种细节之前，请确保你已经建立并运行了端到端的追踪。\n这就是说，有一个快速的方法可以为你的追踪增加很多细节，那就是把任何现有的日志转换成追踪事件。这可以通过创建一个简单的日志附加器来实现，它可以抓取当前的跨度，并将日志作为一个事件附加到它上面。现在，当你查找追踪时，你所有的应用日志都可以得到，这比在传统的日志工具中寻找它们要容易得多。OpenTelemetry 确实为一些常见的日志系统提供了日志附加程序，但它们也很容易编写。\n与管理层合作 如果你是一个工程师在读这篇文章，我有一个补充说明。推广一个新的遥测系统可能需要组织很多人。幸运的是，有些人已经在做这种组织工作了 —— 经理们！这就是我们的工作。\n但是，如果你想启动其中的一个项目，说服工程或项目经理来帮助你是很好的第一步。他们将对如何完成项目有宝贵的见解，并能在你可能不参加的会议上推销该项目。有时候，组织人比组织代码更难，所以不要害怕寻求帮助！\n加入社区 最后，在个人和组织层面上，考虑加入 OpenTelemetry 社区！维护者和项目负责人都很友好，非常平易近人。社区是一个很好的获取援助和专业知识的资源的地方；我们总是很乐意帮助新的用户得到指导。还有一种汗水文化：如果有你想看到的 OpenTelemetry 功能，加入一个工作组并提供帮助是使它们得到优先考虑的一个好办法。\n至少，一定要给我们反馈。我们从用户那里听到的越多，我们就越能专注于最重要的问题。我们的目标是建立一个推动下一代可观测性的标准。没有你，我们做不到，我们的大门永远是敞开的。\n谢谢你的阅读 我希望你喜欢这份关于 OpenTelemetry 的可观测性的未来的报告。如果你有任何问题、评论、反馈或基于你所读的内容的灵感，请随时在 Twitter 上与我联系，我是 @tedsuo。\n要想获得 OpenTelemetry 的帮助，请加入云原生计算基金会（CNCF）Slack 上的 #OpenTelemetry 频道。我希望能在那里见到你！\n","date":1643990400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"2b8b19ee91b20be28a40d23fb48e1aca","permalink":"https://lib.jimmysong.io/opentelemetry-obervability/roll-out/","publishdate":"2022-02-05T00:00:00+08:00","relpermalink":"/opentelemetry-obervability/roll-out/","section":"opentelemetry-obervability","summary":"第 8 章：如何在组织中推广 OpenTelemetry","tags":["OpenTelemetry"],"title":"第 8 章：如何在组织中推广 OpenTelemetry","type":"book"},{"authors":null,"categories":["安全"],"content":"在参考平台中，整个应用系统的运行状态或执行状态是由于基础设施代码（例如，用于服务间通信的网络路由、资源配置代码）、策略代码（例如，指定认证和授权策略的代码）和会话管理代码（例如，建立 mTLS 会话的代码、生成 JWT 令牌的代码）的执行的组合，这些代码由可观测性作为代码的执行所揭示。服务网格的可观测性代码在运行期间将基础设施、策略和会话管理代码的执行输出转发给各种监控工具，这些工具产生适用的指标和日志聚合工具以及追踪工具，这些工具又将其输出转发给集中式仪表板。作为这些工具输出的组成部分的分析，使系统管理员能够获得整个应用系统运行时状态的综合全局视图。正是通过持续监控和零信任设计功能实现的 DevSecOps 平台的运行时性能，为云原生应用提供了所有必要的安全保障。\n在 DevSecOps 管道中，实现持续 ATO 的活动是：\n 检查合规的代码。可以检查以下代码是否符合《风险管理框架》的规定  (a) IaC：生成网络路线，资源配置 (b) 策略即代码：对 AuthN 和 AuthZ 策略进行编码 (c) 会话管理代码：mTLS 会话，JWT 令牌 (d) 可观测性代码\n具体的风险评估功能包括生成可操作的任务的能力，指定代码级别的指导，以及用于验证合规性的测试计划。此外，风险评估工具可以为仪表盘中显示的所有工件提供完整的可追溯性，以及持续 ATO 所需的报告能力。\n 显示运行时状态的仪表板。通过触发新管道的过程，提供修复安全和性能瓶颈问题（影响可用性）所需的警报和反馈。此外，仪表盘生成工具有一些功能，使系统管理员能够分析宏观层面的特征，以及根据不断发展的系统和消费者对应用程序运行环境的需求，动态地改变要显示的工件的构成。   下一章   ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"d44a0e32218d9b2257d02fcaae5dc1f4","permalink":"https://lib.jimmysong.io/service-mesh-devsecops/implement/leveraging-devsecops-for-continuous-authorization-to-operate-c-ato/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/service-mesh-devsecops/implement/leveraging-devsecops-for-continuous-authorization-to-operate-c-ato/","section":"service-mesh-devsecops","summary":"在参考平台中，整个应用系统的运行状态或执行状态是由于基础设施代码（例如，用于服务间通信的网络路由、资源配置代码）、策略代码（例如，指定认证和授权策略的代码）和会话管理代码（例如，建立 mTLS 会话的代码、生成","tags":["Service Mesh","DevSecOps"],"title":"4.10 利用 DevSecOps 进行持续授权操作（C-ATO）","type":"book"},{"authors":null,"categories":["Istio"],"content":"Aeraki 是腾讯云在 2021 年 3 月开源的一个服务网格领域的项目。Aeraki 提供了一个端到端的云原生服务网格协议扩展解决方案，以一种非侵入的方式为 Istio 提供了强大的第三方协议扩展能力，支持在 Istio 中对 Dubbo、Thrift、Redis，以及对私有协议进行流量管理。Aeraki 的架构如下图所示：\n   Aeraki架构图  来源：https://istio.io/latest/blog/2021/aeraki/\n从 Aeraki 架构图中可以看到，Aeraki 协议扩展解决方案包含了两个组件：\n Aeraki：Aeraki 作为一个 Istio 增强组件运行在控制面，通过自定义 CRD 向运维提供了用户友好的流量规则配置。Aeraki 将这些流量规则配置翻译为 Envoy 配置，通过 Istio 下发到数据面的 sidecar 代理上。Aeraki 还作为一个 RDS 服务器为数据面的 MetaProtocol Proxy 提供动态路由。Aeraki 提供的 RDS 和 Envoy 的 RDS 有所不同，Envoy RDS 主要为 HTTP 协议提供动态路由，而 Aeraki RDS 旨在为所有基于 MetaProtocol 框架开发的七层协议提供动态路由能力。 MetaProtocol Proxy：基于 Envoy 实现的一个通用七层协议代理。依托 Envoy 成熟的基础库，MetaProtocol Proxy 是在 Envoy 代码基础上的扩展。它为七层协议统一实现了服务发现、负载均衡、RDS 动态路由、流量镜像、故障注入、本地 / 全局限流等基础能力，大大降低了在 Envoy 上开发第三方协议的难度，只需要实现编解码的接口，就可以基于 MetaProtocol 快速开发一个第三方协议插件。  如果没有使用 MetaProtocol Proxy，要让 Envoy 识别一个七层协议，则需要编写一个完整的 TCP filter，这个 filter 需要实现路由、限流、遥测等能力，需要投入大量的人力。对于大部分的七层协议来说，需要的流量管理能力是类似的，因此没有必要在每个七层协议的 filter 实现中重复这部分工作。Aeraki 项目采用了一个 MetaProtocol Proxy 来统一实现这些能力，如下图所示：\n   MetaProtocol Proxy 架构图  基于 MetaProtocol Proxy，只需要实现编解码接口部分的代码就可以编写一个新的七层协议 Envoy Filter。除此之外，无需添加一行代码，Aeraki 就可以在控制面提供该七层协议的配置下发和 RDS 动态路由配置。\n   采用 MetaProtocol 编写 Envoy Filter 的对比  Aeraki + MetaProtocol 套件降低了在 Istio 中管理第三方协议的难度，将 Istio 扩展成为一个支持所有协议的全栈服务网格。目前 Aeraki 项目已经基于 MetaProtocol 实现了 Dubbo 和 Thrift 协议。相对 Envoy 自带的 Dubbo 和 Thrift Filter，基于 MetaProtocol 的 Dubbo 和 Thrift 实现功能更为强大，提供了 RDS 动态路由，可以在不中断存量链接的情况下对流量进行高级的路由管理，并且提供了非常灵活的 Metadata 路由机制，理论上可以采用协议数据包中携带的任意字段进行路由。QQ 音乐和央视频 APP 等业务也正在基于 Aeraki 和 MetaProtocol 进行开发，以将一些私有协议纳入到服务网格中进行管理。\n除此之外，Aeraki 中还提供了 xDS 配置下发优化的 lazyXDS 插件、Consul、etcd、Zookeeper 等各种第三方服务注册表对接适配，Istio 运维实战电子书等工具，旨在解决 Istio 在落地中遇到的各种实际问题，加速服务网格的成熟和产品化。\n参考  Aeraki GitHub - github.com  ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"e8117700864f4613fa52e1c8eb6fc9ed","permalink":"https://lib.jimmysong.io/istio-handbook/ecosystem/aeraki/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/istio-handbook/ecosystem/aeraki/","section":"istio-handbook","summary":"Aeraki 是腾讯云在 2021 年 3 月开源的一个服务网格领域的项目。Aeraki 提供了一个端到端的云原生服务网格协议扩展解决方案，以一种非侵入的方式为 Istio 提供了强大的第三方协议扩展能力，支持在 Istio 中对 Dubbo、Thrif","tags":["Istio","Service Mesh"],"title":"Aeraki","type":"book"},{"authors":null,"categories":["Istio"],"content":"AuthorizationPolicy（授权策略）实现了对网格中工作负载的访问控制。\n授权策略支持访问控制的 CUSTOM、DENY 和 ALLOW 操作。当 CUSTOM、DENY 和 ALLOW 动作同时用于一个工作负载时，首先评估 CUSTOM 动作，然后是 DENY 动作，最后是 ALLOW 动作。评估是按以下顺序进行：\n 如果有任何 CUSTOM 策略与请求相匹配，如果评估结果为拒绝，则拒绝该请求。 如果有任何 DENY 策略与请求相匹配，则拒绝该请求。 如果没有适合该工作负载的 ALLOW 策略，允许该请求。 如果有任何 ALLOW 策略与该请求相匹配，允许该请求。 拒绝该请求。  Istio 授权策略还支持 AUDIT 动作，以决定是否记录请求。AUDIT 策略不影响请求是否被允许或拒绝到工作负载。请求将完全基于 CUSTOM、DENY 和 ALLOW 动作被允许或拒绝。\n如果工作负载上有一个与请求相匹配的 AUDIT 策略，则请求将被内部标记为应该被审计。必须配置并启用一个单独的插件，以实际履行审计决策并完成审计行为。如果没有启用这样的支持插件，该请求将不会被审计。目前，唯一支持的插件是 Stackdriver 插件。\n示例 下面是一个 Istio 授权策略的例子。\n它将 action 设置为 ALLOW 来创建一个允许策略。默认动作是 ALLOW，但在策略中明确规定是很有用的。\n它允许请求来自：\n 服务账户 cluster.local/ns/default/sa/sleep 或 命名空间 test  来访问以下工作负载：\n 在前缀为 /info 的路径上使用 GET 方法，或者 在路径 /data 上使用 POST 方法  且当请求具有由 https://accounts.google.com 发布的有效 JWT 令牌时。\n任何其他请求将被拒绝。\napiVersion:security.istio.io/v1beta1kind:AuthorizationPolicymetadata:name:httpbinnamespace:foospec:action:ALLOWrules:- from:- source:principals:[\u0026#34;cluster.local/ns/default/sa/sleep\u0026#34;]- source:namespaces:[\u0026#34;test\u0026#34;]to:- operation:methods:[\u0026#34;GET\u0026#34;]paths:[\u0026#34;/info*\u0026#34;]- operation:methods:[\u0026#34;POST\u0026#34;]paths:[\u0026#34;/data\u0026#34;]when:- key:request.auth.claims[iss]values:[\u0026#34;https://accounts.google.com\u0026#34;]下面是另一个例子，它将 action 设置为 DENY 以创建一个拒绝策略。它拒绝来自 dev 命名空间对 foo 命名空间中所有工作负载的 POST 请求。\napiVersion:security.istio.io/v1beta1kind:AuthorizationPolicymetadata:name:httpbinnamespace:foospec:action:DENYrules:- from:- source:namespaces:[\u0026#34;dev\u0026#34;]to:- operation:methods:[\u0026#34;POST\u0026#34;]下面的授权策略将 action 设置为 AUDIT。它将审核任何对前缀为 /user/profile 的路径的 GET 请求。\napiVersion:security.istio.io/v1beta1kind:AuthorizationPolicymetadata:namespace:ns1name:anynamespec:selector:matchLabels:app:myapiaction:AUDITrules:- to:- operation:methods:[\u0026#34;GET\u0026#34;]paths:[\u0026#34;/user/profile/*\u0026#34;]授权策略的范围（目标）由 metadata/namespace 和一个可选的 selector 决定。\n metadata/namespace 告诉策略适用于哪个命名空间。如果设置为根命名空间，该策略适用于网格中的所有命名空间。 工作负载 selector 可以用来进一步限制策略的适用范围。  例如，以下授权策略适用于 bar 命名空间中包含标签 app: httpbin 的工作负载。\napiVersion:security.istio.io/v1beta1kind:AuthorizationPolicymetadata:name:policynamespace:barspec:selector:matchLabels:app:httpbin以下授权策略适用于命名空间 foo 中的所有工作负载。\napiVersion:security.istio.io/v1beta1kind:AuthorizationPolicymetadata:name:policynamespace:foospec:{}以下授权策略适用于网格中所有命名空间中包含标签 version: v1 的工作负载（假设根命名空间被配置为 istio-config）。\napiVersion:security.istio.io/v1beta1kind:AuthorizationPolicymetadata:name:policynamespace:istio-configspec:selector:matchLabels:version:v1 关于 AuthorizationPolicy 配置的详细用法请参考 Istio 官方文档。 关于认证策略条件的详细配置请参考 Istio 官方文档。  参考  Authorization Policy - istio.io Authorization Policy Conditions - istio.io  ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"7291da1aaffd2e47faa1e9526c52611d","permalink":"https://lib.jimmysong.io/istio-handbook/config-security/authorization-policy/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/istio-handbook/config-security/authorization-policy/","section":"istio-handbook","summary":"AuthorizationPolicy（授权策略）实现了对网格中工作负载的访问控制。 授权策略支持访问控制的 CUSTOM、DENY 和 ALLOW 操作。当 CUSTOM、DENY 和 ALLOW 动作同时用于一个工作负载时，首先评","tags":["Istio","Service Mesh"],"title":"AuthorizationPolicy","type":"book"},{"authors":null,"categories":["Istio"],"content":"为了排除 Istio 的问题，对 Envoy 的工作原理有一个基本的了解是很有帮助的。Envoy 配置是一个 JSON 文件，分为多个部分。我们需要了解 Envoy 的基本概念是监听器、路由、集群和端点。\n这些概念映射到 Istio 和 Kubernetes 资源，如下图所示。\n   Envoy 概念与 Istio 和 Kubernetes 的映射  监听器是命名的网络位置，通常是一个 IP 和端口。Envoy 对这些位置进行监听，这是它接收连接和请求的地方。\n每个 sidecar 都有多个监听器生成。每个 sidecar 都有一个监听器，它被绑定到 0.0.0.0:15006。这是 IP Tables 将所有入站流量发送到 Pod 的地址。第二个监听器被绑定到 0.0.0.0:15001，这是所有从 Pod 中出站的流量地址。\n当一个请求被重定向（使用 IP Tables 配置）到 15001 端口时，监听器会把它交给与请求的原始目的地最匹配的虚拟监听器。如果它找不到目的地，它就根据配置的 OutboundTrafficPolicy 来发送流量。默认情况下，请求被发送到 PassthroughCluster，该集群连接到应用程序选择的目的地，Envoy 没有进行任何负载均衡。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"576d7ea8d8d5209b418f983142350ea7","permalink":"https://lib.jimmysong.io/istio-handbook/troubleshooting/envoy-basic/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/istio-handbook/troubleshooting/envoy-basic/","section":"istio-handbook","summary":"为了排除 Istio 的问题，对 Envoy 的工作原理有一个基本的了解是很有帮助的。Envoy 配置是一个 JSON 文件，分为多个部分。我们需要了解 Envoy 的基本概念是监听器、路由、集群和端点。 这些概念映射到 Istio 和 Kubernetes 资源，如下图所示。 Envoy 概念","tags":["Istio","Service Mesh"],"title":"Envoy 基础问题","type":"book"},{"authors":null,"categories":["Istio"],"content":"在学习一门新的技术之前，有必要先了解它的基本语境和术语，这样在阅读后续的章节时，可以保证我们的思想一致。在这一节我将为你介绍 Envoy 中常用的术语。\nEnvoy 中常用术语有：\n 主机（Host）：能够进行网络通信的实体（在手机或服务器等上的应用程序）。在 Envoy 中主机是指逻辑网络应用程序。只要每台主机都可以独立寻址，一块物理硬件上就运行多个主机。 下游（Downstream）：下游主机连接到 Envoy，主动向 Envoy 发送请求并期待获得响应。 上游（Upstream）：上游主机收到来自 Envoy 的连接请求同时相应该请求。 集群（Cluster）: 集群是 Envoy 连接到的一组逻辑上相似的上游主机。Envoy 通过服务发现发现集群中的成员。Envoy 可以通过主动运行状况检查来确定集群成员的健康状况。Envoy 如何将请求路由到集群成员由负载均衡策略确定。 网格（Mesh）：网格是一组互相协调以提供一致网络拓扑的主机。Envoy 网格是指一组 Envoy 代理，它们构成了由多种不同服务和应用程序平台组成的分布式系统的消息传递基础。 运行时配置：与 Envoy 一起部署的带外实时配置系统。可以在无需重启 Envoy 或 更改 Envoy 主配置的情况下，通过更改设置来影响操作。 监听器（Listener）: 监听器（listener）是可以由下游客户端连接的命名网络位置（例如，端口、unix 域套接字等）。Envoy 公开一个或多个下游主机连接的侦听器。一般是每台主机运行一个 Envoy，使用单进程运行，但是每个进程中可以启动任意数量的 Listener（监听器），目前只监听 TCP，每个监听器都独立配置一定数量的（L3/L4）网络过滤器。Listenter 也可以通过 Listener Discovery Service（LDS）动态获取。 监听器过滤器（Listener filter）：Listener 使用 listener filter（监听器过滤器）来操作链接的元数据。它的作用是在不更改 Envoy 的核心功能的情况下添加更多的集成功能。Listener filter 的 API 相对简单，因为这些过滤器最终是在新接受的套接字上运行。在链中可以互相衔接以支持更复杂的场景，例如调用速率限制。Envoy 已经包含了多个监听器过滤器。 HTTP 路由表（HTTP Route Table）：HTTP 的路由规则，例如请求的域名，Path 符合什么规则，转发给哪个 Cluster。 健康检查（Health checking）：健康检查会与 SDS 服务发现配合使用。但是，即使使用其他服务发现方式，也有相应需要进行主动健康检查的情况。  ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"86e3d15e79e619ca91d9f09c46523393","permalink":"https://lib.jimmysong.io/istio-handbook/data-plane/envoy-terminology/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/istio-handbook/data-plane/envoy-terminology/","section":"istio-handbook","summary":"在学习一门新的技术之前，有必要先了解它的基本语境和术语，这样在阅读后续的章节时，可以保证我们的思想一致。在这一节我将为你介绍 Envoy 中常用的术语。 Envoy 中常用术语有： 主机（Host）：能够进行网络通信的实体（在","tags":["Istio","Service Mesh"],"title":"Envoy 中的基本术语","type":"book"},{"authors":null,"categories":["Istio"],"content":"Gateway 描述了一个在网格边缘运行的负载均衡器，接收传入或传出的 HTTP/TCP 连接。该规范描述了一组应该暴露的端口、要使用的协议类型、负载均衡器的 SNI 配置等。\n示例 例如，下面的 Gateway 配置设置了一个代理，作为负载均衡器，暴露了 80 和 9080 端口（http）、443（https）、9443（https）和 2379 端口（TCP）的入口（ingress）。网关将被应用于运行在标签为 app: my-gateway-controller 的 pod 上的代理。虽然 Istio 将配置代理来监听这些端口，但用户有责任确保这些端口的外部流量被允许进入网格。\napiVersion:networking.istio.io/v1alpha3kind:Gatewaymetadata:name:my-gatewaynamespace:some-config-namespacespec:selector:app:my-gateway-controllerservers:- port:number:80name:httpprotocol:HTTPhosts:- uk.bookinfo.com- eu.bookinfo.comtls:httpsRedirect:true# 对 HTTP 请求发送 301 重定向- port:number:443name:https-443protocol:HTTPShosts:- uk.bookinfo.com- eu.bookinfo.comtls:mode:SIMPLE# 在此端口上启用 HTTPSserverCertificate:/etc/certs/servercert.pemprivateKey:/etc/certs/privatekey.pem- port:number:9443name:https-9443protocol:HTTPShosts:- \u0026#34;bookinfo-namespace/*.bookinfo.com\u0026#34;tls:mode:SIMPLE# 在此端口上启用 HTTPScredentialName:bookinfo-secret# 从 Kubernetes secret 中获取证书- port:number:9080name:http-wildcardprotocol:HTTPhosts:- \u0026#34;*\u0026#34;- port:number:2379# 通过此端口暴露内部服务name:mongoprotocol:MONGOhosts:- \u0026#34;*\u0026#34;上面的网关规范描述了负载均衡器的 L4 到 L6 属性。然后，VirtualService 可以被绑定到 Gateway 上，以控制到达特定主机或网关端口的流量的转发。\n例如，下面的 VirtualService 将 https://uk.bookinfo.com/reviews、https://eu.bookinfo.com/reviews、 http://uk.bookinfo.com:9080/reviews、http://eu.bookinfo.com:9080/reviews 的流量分成两个版本（prod 和 qa）的内部 reviews 服务，这两个版本分别运行在 prod 和 qa 命名空间中，端口为 9080。此外，包含 cookie user: dev-123 的请求将被发送到 qa 版本的特殊端口 7777。以上规则也适用于网格内部对 reviews.prod.svc.cluster.local 服务的请求。prod 版本获得 80% 的流量，qa 版本获得 20% 的流量。\napiVersion:networking.istio.io/v1alpha3kind:VirtualServicemetadata:name:bookinfo-rulenamespace:bookinfo-namespacespec:hosts:- reviews.prod.svc.cluster.local- uk.bookinfo.com- eu.bookinfo.comgateways:- some-config-namespace/my-gateway- mesh# 应用到网格中所有的 sidecarhttp:- match:- headers:cookie:exact:\u0026#34;user=dev-123\u0026#34;route:- destination:port:number:7777host:reviews.qa.svc.cluster.local- match:- uri:prefix:/reviews/route:- destination:port:number:9080# 如果它是 reviews 的唯一端口，则可以省略。host:reviews.prod.svc.cluster.localweight:80- destination:host:reviews.qa.svc.cluster.localweight:20下面的 VirtualService 将到达（外部）27017 端口的流量转发到 5555 端口的内部 Mongo 服务器。这个规则在网格内部不适用，因为网关列表中省略了保留名称 mesh。\napiVersion:networking.istio.io/v1alpha3kind:VirtualServicemetadata:name:bookinfo-mongonamespace:bookinfo-namespacespec:hosts:- mongosvr.prod.svc.cluster.local# 内部 Mongo 服务的名字gateways:- some-config-namespace/my-gateway# 如果 Gateway 与 VirtualService 处于同一命名空间，可以省略命名空间。tcp:- match:- port:27017route:- destination:host:mongo.prod.svc.cluster.localport:number:5555可以在 hosts 字段中命名空间/主机名语法来限制可以绑定到网关服务器的虚拟服务集。例如，下面的 Gateway 允许 ns1 命名空间中的任何 VirtualService 与之绑定，而只限制 ns2 命名空间中的 foo.bar.com 主机的 VirtualService 与之绑定。\napiVersion:networking.istio.io/v1alpha3kind:Gatewaymetadata:name:my-gatewaynamespace:some-config-namespacespec:selector:app:my-gateway-controllerservers:- port:number:80name:httpprotocol:HTTPhosts:- \u0026#34;ns1/*\u0026#34;- \u0026#34;ns2/foo.bar.com\u0026#34;配置项 下图是 Gateway 资源的配置拓扑图。\n  Gateway 资源配置拓扑图  Gateway 资源的顶级配置项目如下：\n selector：一个或多个标签，表明应在其上应用该网关配置的一组特定的 pod/VM。默认情况下，工作负载是根据标签选择器在所有命名空间中搜索的。这意味着命名空间 “foo” 中的网关资源可以根据标签选择命名空间 “bar” 中的 pod。这种行为可以通过 istiod 中的 PILOT_SCOPE_GATEWAY_TO_NAMESPACE 环境变量控制。如果这个变量被设置为 “true”，标签搜索的范围将被限制在资源所在的配置命名空间。换句话说，Gateway 资源必须驻留在与 Gateway 工作负载实例相同的命名空间中。如果选择器为nil，Gateway 将被应用于所有工作负载。 servers：描述了特定负载均衡器端口上的代理的属性。  关于 Gateway 配置的详细用法请参考 Istio 官方文档。\n参考  Gateway - istio.io  ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"b5bd0b758ec9b2a7e5422956fff568f0","permalink":"https://lib.jimmysong.io/istio-handbook/config-networking/gateway/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/istio-handbook/config-networking/gateway/","section":"istio-handbook","summary":"Gateway 描述了一个在网格边缘运行的负载均衡器，接收传入或传出的 HTTP/TCP 连接。该规范描述了一组应该暴露的端口、要使用的协议类型、负载均衡器的 SNI 配置等。 示例 例如，下面的 Gateway 配置设置了一个代理，作为负载均衡器，暴露了 80 和","tags":["Istio","Service Mesh"],"title":"Gateway","type":"book"},{"authors":null,"categories":["Istio"],"content":"Istio 1.11 引入了 Telemetry API，并在 1.13 中进行了完善。使用该 API，你可以一站式的灵活地配置指标、访问日志和追踪。\n使用 API 下面将向你介绍如何使用 Telemetry API。\n范围、继承和重写 Telemetry API 资源从 Istio 配置中继承的顺序：\n 根配置命名空间（例如：istio-system） 本地命名空间（命名空间范围内的资源，没有工作负载 selector） 工作负载（具有工作负载 selector 的命名空间范围的资源）  根配置命名空间（通常是 istio-system）中的 Telemetry API 资源提供网格范围内的默认行为。根配置命名空间中的任何特定工作负载选择器将被忽略 / 拒绝。在根配置命名空间中定义多个网格范围的 Telemetry API 资源是无效的。\n要想在某个特定的命名空间内覆盖网格范围的配置，可以通过在该命名空间中应用新的 Telemetry 资源（无需工作负载选择器）来实现。命名空间配置中指定的任何字段将完全覆盖父配置（根配置命名空间）中的字段。\n要想覆盖特定工作负载的遥测配置，可以在其命名空间中应用带有工作负载选择器的新的 Telemetry 资源来实现。\n工作负载选择 命名空间内的单个工作负载是通过 selector 选择的，该选择器允许基于标签选择工作负载。\n让两个不同的 Telemetry 资源使用 selector 选择同一个工作负载是无效的。同样，在一个命名空间中有两个不同的 Telemetry 资源而没有指定 selector 也是无效的。\n提供者选择 Telemetry API 使用提供者（Provider）的概念来表示要使用的协议或集成类型。提供者可以在 MeshConfig 中进行配置。\n下面是一个 MeshConfig 中配置提供者的例子。\ndata:mesh:|-extensionProviders: # The following content defines two example tracing providers. - name: \u0026#34;localtrace\u0026#34; zipkin: service: \u0026#34;zipkin.istio-system.svc.cluster.local\u0026#34; port: 9411 maxTagLength: 56 - name: \u0026#34;cloudtrace\u0026#34; stackdriver: maxTagLength: 256为方便起见，Istio 在开箱时就配置了一些默认设置的提供者。\n   提供者名称 功能     prometheus 指标   stackdriver 指标、追踪、日志记录   envoy 日志记录    此外，还可以设置一个默认的提供者，当 Telemetry 资源没有指定提供者时使用。\n示例 Telemetry API 资源继承自网格的根配置命名空间，通常是 istio-system。要配置整个网格的行为，在根配置命名空间中添加一个新的（或编辑现有的）Telemetry 资源。\n下面是一个使用上一节中提供者配置的例子。\napiVersion:telemetry.istio.io/v1alpha1kind:Telemetrymetadata:name:mesh-defaultnamespace:istio-systemspec:tracing:- providers:- name:localtracecustomTags:foo:literal:value:barrandomSamplingPercentage:100这个配置覆盖了 MeshConfig 的默认提供者，将 Mesh 默认设置为 localtrace 提供者。它还设置了 Mesh 范围内的采样百分比为 100，并配置了一个标签，将其添加到所有追踪跨度中，名称为 foo，值为 bar。\n配置命名空间范围内的跟踪行为 要为单个命名空间定制行为，请为所需的命名空间添加 Telemetry 资源。在命名空间资源中指定的任何字段将完全覆盖从配置层次中继承的字段配置。\n例如：\napiVersion:telemetry.istio.io/v1alpha1kind:Telemetrymetadata:name:namespace-overridenamespace:myappspec:tracing:- customTags:userId:header:name:userIddefaultValue:unknown当部署到一个具有先前 Mesh 范围范例配置的 Mesh 中时，这将导致 myapp 命名空间中的追踪行为，将追踪跨度发送到 localtrace 提供者，并以 100% 的速率随机选择请求进行追踪，但为每个跨度设置自定义标签，名称为 userId，其值取自 userId 请求头。重要的是，来自父配置的 foo: bar 标签将不会在 myapp 命名空间中使用。自定义标签的行为完全覆盖了 mesh-default.istio-system 资源中配置的行为。\n注意：Telemetry 资源中的任何配置都会完全覆盖配置层次中其父资源的配置。这包括提供者的选择。\n配置针对工作负载的行为 要为个别工作负载定制行为，请将遥测资源添加到所需的命名空间并使用 selector。工作负载特定资源中指定的任何字段将完全覆盖配置层次中的继承字段配置。\n例如：\napiVersion:telemetry.istio.io/v1alpha1kind:Telemetrymetadata:name:workload-overridenamespace:myappspec:selector:matchLabels:service.istio.io/canonical-name:frontendtracing:- disableSpanReporting:true在这个例子中，对于 myapp 命名空间中的 frontend 工作负载，追踪功能将被禁用。Istio 仍将转发追踪头信息，但不会向配置的追踪提供者报告任何跨度。\n注意：让两个带有工作负载选择器的 Telemetry 资源选择相同的工作负载是无效的。此时，行为是未定义的。\n参考  Telemetry API - istio.io Telemetry configuration - istio.io  ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"3f50c549d740ac530741cba62ceac7de","permalink":"https://lib.jimmysong.io/istio-handbook/observability/telemetry-api/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/istio-handbook/observability/telemetry-api/","section":"istio-handbook","summary":"Istio 1.11 引入了 Telemetry API，并在 1.13 中进行了完善。使用该 API，你可以一站式的灵活地配置指标、访问日志和追踪。 使用 API 下面将向你介绍如何使用 Telemetry API。 范围、继承和重写 Telemetry API 资源从 Istio 配置中继承的顺序： 根配置命名空间（","tags":["Istio","Service Mesh"],"title":"Telemetry API","type":"book"},{"authors":null,"categories":["Istio"],"content":"\n  Istio 官方已不再推荐使用 Operator 来安装 Istio，建议你使用 istioctl 命令或者 Helm 来安装。   要安装 Istio，我们需要一个运行中的 Kubernetes 集群实例。所有的云供应商都有一个管理的 Kubernetes 集群提供，我们可以用它来安装 Istio 服务网格。\n我们也可以在你的电脑上使用以下平台之一在本地运行一个 Kubernetes集群。\n要安装 Istio，我们需要一个运行中的 Kubernetes 集群实例。所有的云供应商都有一个管理的 Kubernetes 集群提供，我们可以用它来安装 Istio 服务网格。\n我们也可以在你的电脑上使用以下平台之一在本地运行一个 Kubernetes 集群。\n Minikube Docker Desktop kind MicroK8s  当使用本地 Kubernetes 集群时，确保你的计算机满足 Istio 安装的最低要求（如 16384MB 内存和 4 个 CPU）。另外，确保 Kubernetes 集群的版本是 v1.19.0 或更高。\nMinikube 在这次培训中，我们将使用 Minikube 的 Hypervisor。Hypervisor 的选择将取决于你的操作系统。要安装 Minikube 和 Hypervisor，你可以按照安装说明进行。\n安装了 Minikube 后，我们可以创建并启动 Kubernetes 集群。下面的命令使用 VirtualBox 管理程序启动一个 Minikube 集群。\nminikube start --memory=16384 --cpus=4 --driver=virtualbox\n请确保用你所使用的 Hypervisor 的名字替换 -driver=virtualbox。关于可用的选项，见下表。\n   标志名称 更多信息     hyperkit HyperKit   hyperv Hyper-V   kvm2 KVM   docker Docker   podman Podman   parallels Parallels   virtualbox VirtualBox   vmware VMware Fusion    为了检查集群是否正在运行，我们可以使用 Kubernetes CLI，运行 kubectl get nodes 命令。\n$ kubectl get nodes NAME STATUS ROLES AGE VERSION minikube Ready master 151m v1.19.0  注意：如果你使用 Brew 软件包管理器安装了 Minikube，你也安装了 Kubernetes CLI。\n Kubernetes CLI 如果你需要安装 Kubernetes CLI，请遵循这些说明。\n我们可以运行 kubectl version 来检查 CLI 是否已经安装。你应该看到与此类似的输出。\n$ kubectl version Client Version: version.Info{Major:\u0026#34;1\u0026#34;, Minor:\u0026#34;19\u0026#34;, GitVersion:\u0026#34;v1.19.2\u0026#34;, GitCommit:\u0026#34;f5743093fd1c663cb0cbc89748f730662345d44d\u0026#34;, GitTreeState:\u0026#34;clean\u0026#34;, BuildDate:\u0026#34;2020-09-16T21:51:49Z\u0026#34;, GoVersion:\u0026#34;go1.15.2\u0026#34;, Compiler:\u0026#34;gc\u0026#34;, Platform:\u0026#34;darwin/amd64\u0026#34;} Server Version: version.Info{Major:\u0026#34;1\u0026#34;, Minor:\u0026#34;19\u0026#34;, GitVersion:\u0026#34;v1.19.0\u0026#34;, GitCommit:\u0026#34;e19964183377d0ec2052d1f1fa930c4d7575bd50\u0026#34;, GitTreeState:\u0026#34;clean\u0026#34;, BuildDate:\u0026#34;2020-08-26T14:23:04Z\u0026#34;, GoVersion:\u0026#34;go1.15\u0026#34;, Compiler:\u0026#34;gc\u0026#34;, Platform:\u0026#34;linux/amd64\u0026#34;} 下载 Istio 在本课程中，我们将使用 Istio 1.10.3。安装 Istio 的第一步是下载 Istio CLI（istioctl）、安装清单、示例和工具。\n安装最新版本的最简单方法是使用 downloadIstio 脚本。打开一个终端窗口，打开你要下载 Istio 的文件夹，然后运行下载脚本。\n$ curl -L https://istio.io/downloadIstio | ISTIO_VERSION=1.10.3 sh - Istio 发行版被下载并解压到名为 istio-1.9.0 的文件夹中。为了访问 istioctl，我们应该把它添加到 path 中。\n$ cd istio-1.10.3 $ export PATH=$PWD/bin:$PATH 要检查 istioctl 是否在 path 里，运行 istioctl version。你应该看到这样的输出。\n$ istioctl version no running Istio pods in \u0026#34;istio-system\u0026#34; 1.10.3 安装 Istio Istio 支持多个配置文件（profile）。配置文件之间的区别在于所安装的组件。\n$ istioctl profile list Istio configuration profiles: default demo empty minimal preview remote 推荐用于生产部署的配置文件是 default 配置文件。我们将安装 demo 配置文件，因为它包含所有的核心组件，启用了跟踪和日志记录，并且是为了学习不同的 Istio 功能。\n我们也可以从 minimal 的组件开始，以后单独安装其他功能，如 ingress 和 egress 网关。\n因为我们将使用 Istio 操作员进行安装，所以我们必须先部署 Operator。\n要部署 Istio Operator，请运行：\n$ istioctl operator init Using operator Deployment image: docker.io/istio/operator:1.9.0 ✔ Istio operator installed ✔ Installation complete init 命令创建了 istio-operator 命名空间，并部署了 CRD、Operator Deployment 以及 operator 工作所需的其他资源。安装完成后，Operator 就可以使用了。\n要安装 Istio，我们必须创建 IstioOperator 资源，并指定我们要使用的配置配置文件。\n创建一个名为 demo-profile.yaml 的文件，内容如下：\napiVersion:v1kind:Namespacemetadata:name:istio-system---apiVersion:install.istio.io/v1alpha1kind:IstioOperatormetadata:namespace:istio-systemname:demo-istio-installspec:profile:demo我们还在文件中添加了命名空间资源，以创建 istio-system 命名空间。\n我们需要做的最后一件事是创建资源：\n$ kubectl apply -f demo-profile.yaml namespace/istio-system created istiooperator.install.istio.io/demo-istio-install created 一旦 Operator 检测到 IstioOperator 资源，它将开始安装 Istio。整个过程可能需要5分钟左右。\n为了检查安装的状态，我们可以看看 istio-system 命名空间中的Pod的状态。\n$ kubectl get po -n istio-system NAME READY STATUS RESTARTS AGE istio-egressgateway-6db9994577-sn95p 1/1 Running 0 79s istio-ingressgateway-58649bfdf4-cs4fk 1/1 Running 0 79s istiod-dd4b7db5-nxrjv 1/1 Running 0 111s 当所有的 Pod 都在运行时，Operator 已经完成了 Istio 的安装。\n启用 sidecar 注入 正如我们在上一节中所了解的，服务网格需要与每个应用程序一起运行的 sidecar 代理。\n要将 sidecar 代理注入到现有的 Kubernetes 部署中，我们可以使用 istioctl 命令中的 kube-inject 动作。\n然而，我们也可以在任何 Kubernetes 命名空间上启用自动 sidecar 注入。如果我们用 istio-injection=enabled 标记命名空间，Istio 会自动为我们在该命名空间中创建的任何 Kubernetes Pod 注入 sidecar。\n让我们通过添加标签来启用默认命名空间的自动 sidecar 注入。\n$ kubectl label namespace default istio-injection=enabled namespace/default labeled 要检查命名空间是否被标记，请运行下面的命令。default 命名空间应该是唯一一个启用了该值的命名空间。\n$ kubectl get namespace -L istio-injection NAME STATUS AGE ISTIO-INJECTION default Active 32m enabled istio-operator Active 27m disabled istio-system Active 15m kube-node-lease Active 32m kube-public Active 32m kube-system Active 32m 现在我们可以尝试在 default 命名空间创建一个 Deployment，并观察注入的代理。我们将创建一个名为 my-nginx 的 Deployment，使用 nginx 镜像的单一容器。\n$ kubectl create deploy my-nginx --image=nginx deployment.apps/my-nginx created 如果我们看一下 Pod，你会发现 Pod 里有两个容器。\n$ kubectl get po NAME READY STATUS RESTARTS AGE my-nginx-6b74b79f57-hmvj8 2/2 Running 0 62s 同样地，描述 Pod 显示 Kubernetes 同时创建了一个 nginx 容器和一个 istio-proxy 容器：\n$ kubectl describe po my-nginx-6b74b79f57-hmvj8 Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 118s default-scheduler Successfully assigned default/my-nginx-6b74b79f57-hmvj8 to minikube Normal Pulling 117s kubelet Pulling image \u0026#34;docker.io/istio/proxyv2:1.9.0\u0026#34; Normal Pulled 116s kubelet Successfully …","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"205763e0a8fa575b5c858e3bfb4f492a","permalink":"https://lib.jimmysong.io/istio-handbook/setup/istio-installation/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/istio-handbook/setup/istio-installation/","section":"istio-handbook","summary":"Istio 官方已不再推荐使用 Operator 来安装 Istio，建议你使用 istioctl 命令或者 Helm 来安装。 要安装 Istio，我们需要一个运行中的 Kubernetes 集群实例。所有的云供应商都有一个管理的 Kubernetes 集群提供，我们可以用它来安装 Istio 服务网格。 我们也可以","tags":["Istio","Service Mesh"],"title":"安装 Istio","type":"book"},{"authors":null,"categories":["Istio"],"content":"我们将使用谷歌云平台来托管 Kubernetes 集群。打开并登录到你的 GCP 控制台账户，并按照以下步骤创建一个 Kubernetes 集群。\n 从导航中，选择 Kubernetes Engine。 点击创建集群。 将集群命名为 boutique-demo。 选择区域选项，选择最接近你的位置的区域。 点击 default-pool，在节点数中输入 5。 点击 Nodes（节点）。 点击机器配置机器类型下拉，选择e2-medium (2 vCPU, 4 GB memory)。 点击 “Create\u0026#34;来创建集群。  集群的创建将需要几分钟的时间。一旦安装完成，集群将显示在列表中，如下图所示。\n   部署在 GCP 中的 Kubernetes 集群   你也可以将 Online Boutique 应用程序部署到托管在其他云平台上的 Kubernetes 集群，如 Azure 或 AWS。\n 访问集群 我们有两种方式来访问集群。我们可以从浏览器中使用 Cloud Shell。要做到这一点，点击集群旁边的 Connect，然后点击 Run in Cloud Shell 按钮。点击该按钮可以打开 Cloud Shell，并配置 Kubernetes CLI 来访问该集群。\n第二个选择是在你的电脑上安装 gcloud CLI，然后从你的电脑上运行相同的命令。\n安装 Istio 我们将使用 GetMesh CLI 在集群中安装 Istio 1.10.3。\n1. 下载 GetMesh CLI\ncurl -sL https://istio.tetratelabs.io/getmesh/install.sh | bash 2. 安装 Istio\ngetmesh istioctl install --set profile=demo 安装完成后，给默认命名空间设置上 istio-injection=enabled 标签：\nkubectl label namespace default istio-injection=enabled ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"624a10645646f9a947d7b22b59127154","permalink":"https://lib.jimmysong.io/istio-handbook/practice/create-cluster/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/istio-handbook/practice/create-cluster/","section":"istio-handbook","summary":"我们将使用谷歌云平台来托管 Kubernetes 集群。打开并登录到你的 GCP 控制台账户，并按照以下步骤创建一个 Kubernetes 集群。 从导航中，选择 Kubernetes Engine。 点击创建集群。 将集群命名为 boutique-demo。 选择区域选项，选择最接","tags":["Istio","Service Mesh"],"title":"创建集群","type":"book"},{"authors":null,"categories":["云原生"],"content":"我们讨论过只能使用云原生应用程序来创建基础架构。同时基础架构也负责运行这些应用程序。\n运行由应用程序配置和控制的基础架构可以轻松得扩展。我们通过学习如何通过扩展应用的方式来扩展基础架构。我们还通过学习如何保护应用程序来保护基础架构。\n在动态环境中，无法通过增加人手来管理这样的复杂性，同样也不能靠增加人手来处理策略和安全问题。\n这意味着，就像我们必须创建通过协调器模式强制执行基础架构状态的应用程序一样，我们需要创建实施安全策略的应用程序。在创建应用程序以执行的策略之前，我们需要以机器可解析的格式编写策略。\n策略即代码 由于策略没有明确定义的技术实现，所以策略难以纳入代码。它更多地关注业务如何实现而不是谁来实现。\n如何实现和谁来实现都会经常变化，但是实现方式变化更频繁且不容易被抽象化。它也是组织特定的，可能需要了解创建基础架构人员的沟通结构的具体细节。\n策略需要应用于应用程序生命周期的多个阶段。正如我们在第 7 章中所讨论的，应用程序通常有三个阶段：部署、运行和退役。\n部署阶段将在应用程序和基础架构变更发布之前先应用策略。这将包括部署规则和一致性测试。运行阶段将包括持续的遵守和执行访问控制和隔离。退役阶段很重要，以确保没有服务落后于未安排或未维护的状态。\n在这些阶段中，您需要将策略分解为明确的，可操作的实现。模糊的策略无法执行。您需要将实现放在代码中，然后创建应用程序或使用现有的应用程序来执行策略规则。\n您应该将策略视为代码。策略更改应视为应用程序更改并在版本控制中进行跟踪。\n控制应用程序部署的相同策略也应该适用于您的新策略部署。您可以使用与部署应用程序相同的工具跟踪和部署的基础架构组件越多，就越容易了解正在运行的内容以及变更如何影响系统。\n将策略作为代码带来的巨大好处是您可以轻松地添加或删除策略并对其进行跟踪，因此记录了谁执行了策略，何时执行了策略以及提交和提交请求的评论。由于该策略以代码形式存在，因此您现在可以为自己的策略编写测试！如果你想验证一个策略是否可以正常工作，你可以使用第 5 章中的测试实践。\n让我们更仔细地看看如何将策略应用到应用程序生命周期。\n部署 Gateway 部署 gateway 确保应用程序的部署符合业务规则。这意味着您将需要构建部署流水线，并且不允许从用户机器进行直接生产部署。\n在实施集中化策略之前，您需要集中控制，但是应该从小规模开始，并在实施之前证明解决方案可行。部署流水线的好处远不止于策略执行，而且应该是任何拥有少数开发人员的组织中的标准。\n以下是一些策略示例：\n 部署只有在所有测试都通过后才能进行。 新应用程序要求高级开发人员检查更改并对提取请求发表评论。 生产工件推送只能从部署流水线发生。  Gateway 不应该强制运行状态或应用程序的 API 请求。应用程序应该知道如何配置基础架构组件，并通过合规性和审计将策略应用于这些组件，而不是在应用程序部署期间应用。\n部署 gateway 策略的一个例子是，星期五下午 3 点之后，如果您的组织没有获得经理批准，不允许部署代码。\n这个很容易放入代码中。图 8-1 是代表策略的非常简化的图。\n   图 8-1. 部署策略  您可以看到该策略简单地检查了允许部署部署的一周中的时间和日期。如果是星期五和下午 3 点以后，那么策略会检查管理员确定。\n该策略可以通过经理发送的经过验证的电子邮件，经过验证的 API 调用或各种其他方式来获得 OK 通知。决定首选通信方法的内容以及等待批准的时间长度取决于策略。\n这个简单的例子可以用很多不同的选项进行扩展，但确保该策略不符合人类解析和执行是很重要的。人的解释是不同的，而不明确的策略通常不会得到执行。\n通过确保策略阻止新的部署，可以为解决生产环境的状态节省很多工作。有一系列可以通过软件验证的事件可以帮助您理解系统。版本控制和持续部署流水线可以验证代码；使用策略和流程作为代码可以验证软件的部署方式和时间。\n除了确保通过公司策略部署正确的事情之外，我们还应该轻松地使用模板部署受支持的事物，并通过一致性测试强制执行它们。\n合规性测试 在任何基础架构中，您都需要提供建议的方式来创建特定类型的应用程序。这些建议成为用户根据需要消费和拼凑在一起的基石。\n这些建议是可堆砌的，但又不能太小。需要在其预期的功能和自助服务方面可以理解。我们已经建议将云原生应用程序打包为容器并通过协调器使用；由您决定什么最适合您的用户以及您想要提供哪些组件\n 您可以通过多种方式为用户提供模板化基础架构。\n提供一个模板化的代码，比如 Jsonnet，或者完全模板化的应用程序，例如 Helm 的 chart。\n您还可以通过您的部署流水线提供模板。可以是 Terraform 模块或特定于部署的工具，例如 Spinnaker 模板。\n创建部署模板允许用户成为模板的消费者，随着最佳实践的发展，用户将自动受益。\n 基础架构模板的最关键的一点是使做正确的事情变得容易，并且很难做错事情。如果您满足客户的需求，那么获得适配器将会容易得多。\n但是，我们需要模板化基础架构的根本原因是可以执行合规性测试。合规性测试的存在是为了确保应用程序和基础架构组件符合组织的标准。\n对不符合标准的基础架构进行测试的一些示例如下：\n 不在自动缩放组中的服务或端点 不在负载均衡器后面的应用程序 前端层直接与数据库通信  这些标准是关于基础架构的信息，您可以通过调用云提供商的 API 来找到这些信息。合规性测试应持续运行，并强制执行公司采用的架构标准。\n如果发现基础架构组件或应用程序架构违反了公司提供的标准，则应尽早终止它们。越早可以对模板进行编码，就可以越早检查出不符合标准的应用程序。在应用程序的生命早期解决不受支持的体系结构非常重要，因此可以最大限度地减少对体系结构决策的依赖。\n合规性处理如何构建应用程序和维护可操作性。合规性测试确保应用程序和基础架构安全。\n一致性测试 合规性测试不会测试架构设计，而是集中于组件的实施，以确保它们遵守定义的策略。放在代码中的最简单的策略是那些有助于安全的策略。围绕组织需求（例如 HIPAA）的策略也应定义为代码并在合规性测试期间进行测试。\n合规策略的一些例子：\n 对象存储的用户访问受限，并且不能被公共因特网读取或写入 API 端点全部使用 HTTPS 并具有有效证书 虚拟机实例（如果有的话）没有过分宽松的防火墙规则  虽然这些策略不会使应用程序免受所有漏洞或错误的影响，但是如果应用程序确实被利用，合规性策略应将影响范围降至最低。\nNetflix 在其博客文章 “The Netflix Simian Army” 中通过其 Security Monkey 解释了其执行合规性测试的目的：\n Security Monkey 是 Conformity Monkey 的延伸。它会查找安全违规或漏洞（如未正确配置的 AWS 安全组），并终止违规实例。它还确保我们所有的 SSL 和 DRM 证书都是有效的，并且不会延期。\n 将您的策略放入代码并通过观察云提供商 API 继续运行它，可以让您保持更高的安全性，立即捕获不安全的设置，并随时跟踪版本控制系统的策略。根据这些策略不断测试您的基础架构的模型也非常适合调节器模式。\n如果您认为策略是需要应用和实施的配置类型，那么实施它可能会更简单。请务必记住，随着您的基础架构和业务需求的变化，您的合规策略也应该如此。\n部署测试在将应用程序部署到基础架构之前进行监视，合规性和合规性测试都处理正在运行的应用程序。确保您拥有策略的最后一个应用程序生命周期阶段是了解何时以及如何废止应用程序和生命周期组件。\n活动测试 合规性和一致性测试应该删除那些未通过定义策略的应用和基础架构。还应该有一个应用程序清理旧的和未使用的基础架构组件。高级使用模式应基于应用程序遥测数据，但仍有其他基础架构组件很容易被遗忘并需要退役。\n在云环境中，您可以根据需要消费资源，但很容易会忘记需求。如果没有自动清理旧的或未使用的资源，最后您会对账单感到惊讶，或者需要耗费大量人力进行手动审计和清理。\n您应该测试并自动清理的资源的一些示例包括：\n 旧磁盘快照 测试环境 以前的应用程序版本  负责清理的应用程序需要根据默认策略做正确的事情，并为工程师指定的异常提供灵活性。\n正如第 7 章所提到的，Netflix 已经实现了它所称的 “Janitor Monkey”，它的实现完美地描述了这种需要的模式：\n Janitor Monkey 在 “标记、通知和删除” 过程中工作。当 Janitor Monkey 将资源标记为候选清理对象时，它会安排删除资源的时间。删除时间在标记资源的规则中指定。\n每个资源都与一个所有者电子邮件相关联，该电子邮件可以在资源上指定为标签，或者您可以快速扩展 Janitor Monkey 以从您的内部系统获取信息。最简单的方法是使用默认的电子邮件地址，例如您的团队的电子邮件列表中的所有资源。您可以配置若干天，以指定何时让 Janitor Monkey 在计划终止之前向资源所有者发送通知。默认情况下，数字为 3，表示业主将在终止日期前 3 个工作日收到通知。\n在这 3 天期间，资源所有者可以决定资源是否可以删除。如果资源需要保留更长时间，则所有者可以使用简单的 REST 接口将资源标记为未被 Janitor Monkey 清除。所有者总是可以使用另一个 REST 接口来删除该标志，然后 Janitor Monkey 将能够再次管理该资源。\n当 Janitor Monkey 看到标记为清理候选者的资源并且预定的终止时间已经过去时，它将删除资源。如果资源所有者想要提前释放资源以节省成本，则还可以手动删除该资源。当资源状态改变而使资源不是清理候选者时，例如一个分离的 EBS 卷被附加到一个实例，Janitor Monkey 将取消该资源的标记并且不会终止。\n 拥有自动清理基础架构的应用程序可以降低您的复杂性和成本。该测试将协调模式应用到应用程序的最后生命周期阶段。\n还有其他一些基础架构的实践很重要，需要考虑。其中一些实践适用于传统基础架构，但在云原生环境中需要进行不同的处理。\n不断测试基础架构的各个方面有助于您了解自己遵守的策略。当基础架构频繁变更时，很难审计哪些变更可能导致停机或使用历史数据来预测未来趋势。\n如果您希望从账单声明中获取该信息或通过基础架构的当前快照来推断，您会很快发现它们所提供的信息是没用的。为了跟踪变化并预测未来，我们需要有审计工具，可以快速提供我们需要的信息。\n审计基础架构 在这个意义上审计云原生基础架构与审核调解器模式中的组件不同，它与第 6 章中讨论的测试框架也不同。相反，当我们谈论审计时，我们指的是对变更的高级概述和基础架构内的组件关系。\n跟踪基础架构中存在的内容以及它与其他组件的关系，为我们了解当前状态提供了重要的背景。当某些事情中断时，第一个问题几乎总是 “什么改变了？” 审计为我们回答了这个问题，并且可以用来告诉我们如果我们应用变更会受到什么影响。\n在传统的基础架构中，配置管理数据库（CMDB）通常是基础架构当前状态的真相来源。但是，CMDB 不会跟踪基础架构或资产关系的历史版本。\n云提供商可以通过库存 API 为您提供 CMDB 替代服务，但它们可能不会激励您显示历史趋势或让您查询您需要进行故障排除的特定详细信息，例如主机名。\n一个好的云审计工具可以让你显示当前基础架构与昨天或上周相比的差异（diff）。它应该能够将云提供商数据与其他来源（例如容器编排器）相结合，以便您可以查询云提供商可能没有的数据的基础架构，理想情况下，它可以自动构建组件关系的拓扑表示。\n如果您的应用程序完全在单个平台上运行（例如 Kubernetes），则收集资源依赖关系的拓扑信息要容易得多。自动可视化关系的另一种方法是统一关系发生的层次。\n对于在云中运行的服务，可以在服务之间的网络通信中识别关系。有很多方法可以识别服务之间的网络流量，但审计的重要考虑是对信息进行历史跟踪。您需要能够轻松识别关系何时发生变化，就像您识别组件之间关系一样容易。\n 自动识别服 …","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"77ccd82932a9eba38fd0a956779151c3","permalink":"https://lib.jimmysong.io/cloud-native-infra/securing-applications/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/cloud-native-infra/securing-applications/","section":"cloud-native-infra","summary":"我们讨论过只能使用云原生应用程序来创建基础架构。同时基础架构也负责运行这些应用程序。 运行由应用程序配置和控制的基础架构可以轻松得扩展。我们通过学习如何通过扩展应用的方式来扩展基础架构。我们还通过学习如","tags":["云原生"],"title":"第 8 章：保护应用程序","type":"book"},{"authors":null,"categories":["Envoy"],"content":"Envoy 的强大功能之一是支持动态配置。到现在为止，我们一直在使用静态配置。我们使用 static_resources 字段将监听器、集群、路由和其他资源指定为静态资源。\n当使用动态配置时，我们不需要重新启动 Envoy 进程就可以生效。相反，Envoy 通过从磁盘或网络上的文件读取配置，动态地重新加载配置。动态配置使用所谓的发现服务 API，指向配置的特定部分。这些 API 也被统称为 xDS。当使用 xDS 时，Envoy 调用外部基于 gRPC/REST 的配置供应商，这些供应商实现了发现服务 API 来检索配置。\n外部基于 gRPC/REST 的配置提供者也被称为控制平面。当使用磁盘上的文件时，我们不需要控制平面。Envoy 提供了控制平面的 Golang 实现，但是 Java 和其他控制平面的实现也可以使用。\nEnvoy 内部有多个发现服务 API。所有这些在下表中都有描述。\n   发现服务名称 描述     监听器发现服务（LDS） 使用 LDS，Envoy 可以在运行时发现监听器，包括所有的过滤器栈、HTTP 过滤器和对 RDS 的引用。   扩展配置发现服务（ECDS） 使用 ECDS，Envoy 可以独立于监听器获取扩展配置（例如，HTTP 过滤器配置）。   路由发现服务（RDS） 使用 RDS，Envoy 可以在运行时发现 HTTP 连接管理器过滤器的整个路由配置。与 EDS 和 CDS 相结合，我们可以实现复杂的路由拓扑结构。   虚拟主机发现服务（VHDS） 使用 VHDS 允许 Envoy 从路由配置中单独请求虚拟主机。当路由配置中有大量的虚拟主机时，就可以使用这个功能。   宽泛路由发现服务（SRDS） 使用 SRDS，我们可以把路由表分解成多个部分。当我们有大的路由表时，就可以使用这个 API。   集群发现服务（CDS） 使用 CDS，Envoy 可以发现上游集群。Envoy 将通过排空和重新连接所有现有的连接池来优雅地添加、更新或删除集群。Envoy 在初始化时不必知道所有的集群，因为我们可以在以后使用 CDS 配置它们。   端点发现服务（EDS） 使用 EDS，Envoy 可以发现上游集群的成员。   秘密发现服务（SDS） 使用 SDS，Envoy 可以为其监听器发现秘密（证书和私钥，TLS 会话密钥），并为对等的证书验证逻辑进行配置。   运行时发现服务（RTDS） 使用 RTDS，Envoy 可以动态地发现运行时层。    聚合发现服务（ADS）\n表中的发现服务是独立的，有不同的 gRPC/REST 服务名称。使用聚合发现服务（ADS），我们可以使用一个单一的 gRPC 服务，在一个 gRPC 流中支持所有的资源类型（监听器、路由、集群…）。ADS 还能确保不同资源的更新顺序正确。请注意，ADS 只支持 gRPC。如果没有 ADS，我们就需要协调其他 gRPC 流来实现正确的更新顺序。\n增量 gRPC xDS\n每次我们发送资源更新时，我们必须包括所有的资源。例如，每次 RDS 更新必须包含每条路由。如果我们不包括一个路由，Envoy 会认为该路由已被删除。这样做更新会导致很高的带宽和计算成本，特别是当有大量的资源在网络上被发送时。Envoy 支持 xDS 的 delta 变体，我们可以只包括我们想添加 / 删除 / 更新的资源，以改善这种情况。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"331d68452dca2dd5853482b77845ac12","permalink":"https://lib.jimmysong.io/envoy-handbook/dynamic-config/dynamic-configuration/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/envoy-handbook/dynamic-config/dynamic-configuration/","section":"envoy-handbook","summary":"Envoy 的强大功能之一是支持动态配置。到现在为止，我们一直在使用静态配置。我们使用 static_resources 字段将监听器、集群、路由和其他资源指定为静态资源。 当使用动态配置时，我们不需要重新启动 Envoy 进程就可以生效。相反，Envoy 通","tags":["Envoy"],"title":"动态配置简介","type":"book"},{"authors":null,"categories":["Istio"],"content":"多集群部署（两个或更多的集群）为我们提供了更大程度的隔离和可用性，但我们付出的代价是增加了复杂性。如果场景要求高可用性（HA），我们将不得不在多个区域和地区部署集群。\n我们需要做出的下一个决定是，决定我们是否要在一个网络内运行集群，或者是否要使用多个网络。\n下图显示了一个多集群方案（集群 A、B 和 C），跨两个网络部署。\n   多集群服务网格  网络部署模式 当涉及到多个网络时，集群内部运行的工作负载必须使用 Istio 网关才能到达其他集群的工作负载。使用多个网络可以实现更好的容错和网络地址的扩展。\n   多网络服务网格  控制平面部署模型 Istio 服务网格使用控制平面来配置网格内工作负载之间的所有通信。工作负载所连接的控制平面取决于其配置。\n在最简单的情况下，我们有一个服务网格，在一个集群中只有一个控制平面。这就是我们在本课程中一直使用的配置。\n共享控制平面模型涉及多个集群，控制平面只在一个集群中运行。该集群被称为主集群，而部署中的其他集群被称为远程集群。这些集群没有自己的控制平面，相反，它们从主集群共享控制平面。\n   共享的控制平面  另一种部署模式是，我们把所有的集群都视为由外部控制平面控制的远程集群。这使我们在控制平面和数据平面之间有了完全的分离。一个典型的外部控制平面的例子是当一个云供应商在管理它。\n为了实现高可用性，我们应该在多个集群、区域或地区部署多个控制平面实例，如下图所示。\n   多个控制平面  这种模式提供了更好的可用性和配置隔离。如果其中一个控制平面变得不可用，那么停电就只限于这一个控制平面。为了改善这一点，你可以实施故障转移，并配置工作负载实例，在发生故障时连接到另一个控制平面。\n为了达到最高的可用性，我们可以在每个集群内部署一个控制平面。\n网格部署模型 到目前为止，我们所看的所有图表和场景都是使用单一的网格。在单网格模型中，所有的服务都在一个网格中，不管它们跨越多少集群和网络。\n将多个网格联合起来的部署模型被称为多网格部署。在这种模式下，服务可以跨网格边界进行通信。该模型为我们提供了一个更清晰的组织边界，更强的隔离性，并允许我们重复使用服务名称和命名空间。\n当联合两个网格时，每个网格可以暴露一组服务和身份，所有参与的网格都可以识别这些身份。为了实现跨网格的服务通信，我们必须在两个网格之间实现信任。信任可以通过向网格导入信任包和为这些身份配置本地策略来建立。\n租户模式 租户是一组共享工作负载的共同访问权和权限的用户。租户之间的隔离是通过网络配置和策略完成的。Istio 支持命名空间和集群租户。请注意，我们在这里谈论的租户是软多租户，而不是硬租户。当多个租户共享同一个 Istio 控制平面时，没有保证对诸如噪音邻居问题的保护。\n在一个网格中，Istio 使用命名空间作为租户的单位。如果使用 Kubernetes，我们可以为每个命名空间的工作负载部署授予权限。默认情况下，来自不同命名空间的服务可以通过完全限定名相互通信。\n在安全模块中，我们已经学会了如何使用授权策略来提高隔离度，并限制只对适当的调用者进行访问。\n在多集群部署模型中，每个集群中共享相同名称的命名空间被认为是同一个命名空间。集群 A 中 default 命名空间的 Customers 服务与集群 B 中 default 命名空间中的 Customers 服务指的是同一个服务。当流量被发送到 Customers 服务时，负载均衡在两个服务的合并端点上进行，如下图所示。\n   多集群共享命名空间  为了在 Istio 中配置集群租约，我们需要将每个集群配置为一个独立的服务网格。网格可以由不同的团队控制和操作，我们可以将网格连接到一起，形成一个多网格部署。如果我们使用与之前相同的例子，在集群 A 的 default 命名空间中运行的服务 Customers 与集群 B 的 default 命名空间中的服务 Customers 所指的不是同一个服务。\n租户的另一个重要功能是隔离不同租户的配置。目前，Istio 并没有解决这个问题，不过，它通过在命名空间级别上的范围配置来尝试解决这个问题。\n最佳多集群部署 最佳的多集群部署拓扑结构是每个集群都有自己的控制平面。对于正常的服务网格部署规模，建议你使用多网格部署，并有一个单独的系统在外部协调网格。一般建议总是在集群间使用入口网关，即使它们在同一个网络中。直接的 pod 到 pod 的连接需要在多个集群之间填充终端数据，这可能会使事情变得缓慢和复杂。一个更简单的解决方案是让流量通过跨集群的入口来流动。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"878bd4d93cf2b01daa1cc316e6e4d967","permalink":"https://lib.jimmysong.io/istio-handbook/advanced/multicluster-deployment/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/istio-handbook/advanced/multicluster-deployment/","section":"istio-handbook","summary":"多集群部署（两个或更多的集群）为我们提供了更大程度的隔离和可用性，但我们付出的代价是增加了复杂性。如果场景要求高可用性（HA），我们将不得不在多个区域和地区部署集群。 我们需要做出的下一个决定是，决定我","tags":["Istio","Service Mesh"],"title":"多集群部署","type":"book"},{"authors":null,"categories":["Envoy"],"content":"本节将为你讲解 Envoy 中的访问日志配置。\n什么是访问日志？ 每当你打开浏览器访问谷歌或其他网站时，另一边的服务器就会收集你的访问信息。具体来说，它在收集和储存你从服务器上请求的网页数据。在大多数情况下，这些数据包括来源（即主机信息）、请求网页的日期和时间、请求属性（方法、路径、Header、正文等）、服务器返回的状态、请求的大小等等。所有这些数据通常被存储在称为访问日志（access log） 的文本文件中。\n通常，来自网络服务器或代理的访问日志条目遵循标准化的通用日志格式。不同的代理和服务器可以使用自己的默认访问日志格式。Envoy 有其默认的日志格式。我们可以自定义默认格式，并配置它，使其以与其他服务器（如 Apache 或 NGINX）有相同的格式写出日志。有了相同的访问日志格式，我们就可以把不同的服务器放在一起使用，用一个工具把数据记录和分析结合起来。\n本模块将解释访问日志在 Envoy 中是如何工作的，以及如何配置和定制。\n捕获和读取访问日志 我们可以配置捕获任何向 Envoy 代理发出的访问请求，并将其写入所谓的访问日志。让我们看看几个访问日志条目的例子。\n[2021-11-01T20:37:45.204Z] \u0026#34;GET / HTTP/1.1\u0026#34; 200 - 0 3 0 - \u0026#34;-\u0026#34; \u0026#34;curl/7.64.0\u0026#34; \u0026#34;9c08a41b-805f-42c0-bb17-40ec50a3377a\u0026#34; \u0026#34;localhost:10000\u0026#34; \u0026#34;-\u0026#34; [2021-11-01T21:08:18.274Z] \u0026#34;POST /hello HTTP/1.1\u0026#34; 200 - 0 3 0 - \u0026#34;-\u0026#34; \u0026#34;curl/7.64.0\u0026#34; \u0026#34;6a593d31-c9ac-453a-80e9-ab805d07ae20\u0026#34; \u0026#34;localhost:10000\u0026#34; \u0026#34;-\u0026#34; [2021-11-01T21:09:42.717Z] \u0026#34;GET /test HTTP/1.1\u0026#34; 404 NR 0 0 0 - \u0026#34;-\u0026#34; \u0026#34;curl/7.64.0\u0026#34; \u0026#34;1acc3559-50eb-463c-ae21-686fe34abbe8\u0026#34; \u0026#34;localhost:10000\u0026#34; \u0026#34;-\u0026#34; 输出包含三个不同的日志条目，并遵循相同的默认日志格式。默认的日志格式看起来像这样。\n[%START_TIME%] \u0026#34;%REQ(:METHOD)% %REQ(X-ENVOY-ORIGINAL-PATH?:PATH)% %PROTOCOL%\u0026#34; %RESPONSE_CODE% %RESPONSE_FLAGS% %BYTES_RECEIVED% %BYTES_SENT% %DURATION% %RESP(X-ENVOY-UPSTREAM-SERVICE-TIME)% \u0026#34;%REQ(X-FORWARDED-FOR)%\u0026#34; \u0026#34;%REQ(USER-AGENT)%\u0026#34; \u0026#34;%REQ(X-REQUEST-ID)%\u0026#34; \u0026#34;%REQ(:AUTHORITY)%\u0026#34; \u0026#34;%UPSTREAM_HOST%\u0026#34; 诸如 %RESPONSE_FLAGS%、%REQ(:METHOD)% 等值被称为命令操作符（command operator）。\n命令操作符 命令操作符提取相关数据并插入到 TCP 和 HTTP 的日志条目中。如果这些值没有设置或不可用（例如，TCP 中的 RESPONSE_CODE），日志将包含字符 -（或 JSON 日志的 \u0026#34;-\u0026#34;）。\n每个命令操作符都以字符 % 开始和结束，例如，%START_TIME%。如果命令操作符接受任何参数，那么我们可以在括号内提供这些参数。例如，如果我们想使用 START_TIME 命令操作符只记录日、月、年，那么我们可以通过在括号中指定这些值来进行配置：%START_TIME(%d-%m-%Y)%。\n让我们来看看不同的命令操作符。我们试图根据它们的共同属性将它们归入单独的表格。\n   命令操作符 描述 示例     START_TIME 请求开始时间，包括毫秒。 %START_TIME(%Y/%m/%dT%H:%M:%S%z ^%)%   PROTOCOL 协议（HTTP/1.1、HTTP/2 或 HTTP/3） ％PROTOCOL％   RESPONSE_CODE HTTP 响应代码。如果下游客户端断开连接，响应代码将被设置为 0。 %RESPONSE_CODE%   RESPONSE_CODE_DETAIL 关于 HTTP 响应的额外信息（例如，谁设置的以及为什么设置）。 RESPONSE_CODE_DETAIL%。   CONNECTION_TERMINATION_DETAILS 提供关于 Envoy 因 L4 原因终止连接的额外信息。 %CONNECTION_TERMINATION_DETAILS%   ROUTE_NAME 路由的名称。 %ROUTE_NAME%   CONNECTION_ID 下游连接的一个标识符。它可以用来交叉引用多个日志汇聚中的 TCP 访问日志，或交叉引用同一连接的基于计时器的报告。该标识符在一个执行过程中很可能是唯一的，但在多个实例或重新启动之间可能会重复。 %CONNECTION_ID%   GRPC_STATUS gRPC 状态代码，包括文本信息和一个数字。 %GRPC_STATUS%   HOSTNAME 系统主机名。 %HOSTNAME%   LOCAL_REPLY_BODY 被 Envoy 拒绝的请求的正文。 %LOCAL_REPLY_BODY%   FILTER_CHAIN_NAME 下游连接的网络过滤器链名称。 %FILTER_CHAIN_NAME%    大小 该组包含所有代表大小的命令操作符——从请求和响应头字节到接收和发送的字节。\n   命令操作符 描述 示例     REQUEST_HEADER_BYTES 请求头的未压缩字节。 %REQUEST_HEADER_BYTES%   RESPONSE_HEADERS_BYTES 响应 Header 的未压缩字节数。 %RESPONSE_HEADERS_BYTES％   RESPONSE_TRAILERS_BYTES 响应 trailer 的未压缩字节。 %RESPONSE_TRAILERS_BYTES%   BYTES_SENT 为 HTTP 发送的正文字节和为 TCP 发送的连接上的下游字节。 %BYTES_SENT%   BYTES_RECEIVED 收到的正文字节数。 %BYTES_RECEIVED%   UPSTREAM_WIRE_BYTES_SENT 由 HTTP 流向上游发送的总字节数。 %UPSTREAM_WIRE_BYTES_SENT％   UPSTREAM_WIRE_BYTES_RECEIVED 从上游 HTTP 流收到的字节总数。 %upstream_wire_bytes_received％   UPSTREAM_HEADER_BYTES_SENT 由 HTTP 流向上游发送的头字节的数量。 %UPSTREAM_HEADER_BYTES_SENT％   UPSTREAM_HEADER_BYTES_RECEIVED HTTP 流从上游收到的头字节的数量。 %UPSTREAM_HEADER_BYTES_RECEIVED%   DOWNSTREAM_WIRE_BYTES_SENT HTTP 流向下游发送的总字节数。 %downstream_wire_bytes_sent%   DOWNSTREAM_WIRE_BYTES_RECEIVED HTTP 流从下游收到的字节总数。 %DOWNSTREAM_WIRE_BYTES_RECEIVED%   DOWNSTREAM_HEADER_BYTES_SENT 由 HTTP 流向下游发送的头字节的数量。 %DOWNSTREAM_HEADER_BYTES_SENT%   DOWNSTREAM_HEADER_BYTES_RECEIVED HTTP 流从下游收到的头字节的数量。 %downstream_header_bytes_received%    时长    命令操作符 描述 示例     DURATION 从开始时间到最后一个字节输出，请求的总持续时间（以毫秒为单位）。 %DURATION%   REQUEST_DURATION 从开始时间到收到下游请求的最后一个字节，请求的总持续时间（以毫秒计）。 %REQUEST_DURATION%   RESPONSE_DURATION 从开始时间到从上游主机读取的第一个字节，请求的总持续时间（以毫秒计）。 RESPONSE_DURATION   RESPONSE_TX_DURATION 从上游主机读取的第一个字节到下游发送的最后一个字节，请求的总时间（以毫秒为单位）。 %RESPONSE_TX_DURATION%    响应标志 RESPONSE_FLAGS 命令操作符包含关于响应或连接的额外细节。下面的列表显示了 HTTP 和 TCP 连接的响应标志的值和它们的含义。\nHTTP 和 TCP\n UH：除了 503 响应代码外，在一个上游集群中没有健康的上游主机。 UF：除了 503 响应代码外，还有上游连接失败。 UO：上游溢出（断路），此外还有 503 响应代码。 NR：除了 404 响应代码外，没有为给定的请求配置路由，或者没有匹配的下游连接的过滤器链。 URX：请求被拒绝是因为达到了上游重试限制（HTTP）或最大连接尝试（TCP）。 NC：未找到上游集群。 DT：当一个请求或连接超过 max_connection_duration 或 max_downstream_connection_duration。  仅限 HTTP\n DC: 下游连接终止。 LH：除了 503 响应代码，本地服务的健康检查请求失败。 UT：除 504 响应代码外的上行请求超时。 LR：除了 503 响应代码外，连接本地重置。 UR：除 503 响应代码外的上游远程复位。 UC：除 503 响应代码外的上游连接终止。 DI：请求处理被延迟了一段通过故障注入指定的时间。 FI：该请求被中止，并有一个通过故障注入指定的响应代码。 RL：除了 429 响应代码外，该请求还被 HTTP 速率限制过滤器在本地进行了速率限制。 UAEX：该请求被外部授权服务拒绝。 RLSE：请求被拒绝，因为速率限制服务中存在错误。 IH：该请求被拒绝，因为除了 400 响应代码外，它还为一个严格检查的头设置了一个无效的值。 SI：除 408 响应代码外，流空闲超时。 DPE：下游请求有一个 HTTP 协议错误。 UPE：上游响应有一个 HTTP 协议错误。 UMSDR：上游请求达到最大流时长。 OM：过载管理器终止了该请求。  上游信息    命令操作符 描述 示例     UPSTREAM_HOST 上游主机 URL 或 TCP 连接的 tcp://ip:端口。 %UPSTREAM_HOST%   UPSTREAM_CLUSTER 上游主机所属的上游集群。如果运行时特性 envoy.reloadable_features.use_observable_cluster_name 被启用，那么如果提供了 alt_stat_name 就会被使用。 %UPSTREAM_CLUSTER%   UPSTREAM_LOCAL_ADDRESS 上游连接的本地地址。如果是一个 IP 地址，那么它包括地址和端口。 %UPSTREAM_LOCAL_ADDRESS％   UPSTREAM_TRANSPORT_FAILURE_REASON 如果由于传输套接字导致连接失败，则提供来自传输套接字的失败原因。 %UPSTREAM_TRANSPORT_FAILURE_REASON%    下游信息    命令描述符 描述 示例     DOWNSTREAM_REMOTE_ADDRESS 下游连接的远程地址。如果是一个 IP 地址，那么它包括地址和端口。 …","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"e676d92a1f373f8d3aeb324e626372c4","permalink":"https://lib.jimmysong.io/envoy-handbook/logging/access-logging/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/envoy-handbook/logging/access-logging/","section":"envoy-handbook","summary":"本节将为你讲解 Envoy 中的访问日志配置。 什么是访问日志？ 每当你打开浏览器访问谷歌或其他网站时，另一边的服务器就会收集你的访问信息。具体来说，它在收集和储存你从服务器上请求的网页数据。在大多数情况下，这些数据","tags":["Envoy"],"title":"访问日志","type":"book"},{"authors":null,"categories":["Envoy"],"content":"集群可以在配置文件中静态配置，也可以通过集群发现服务（CDS）API 动态配置。每个集群都是一个端点的集合，Envoy 需要解析这些端点来发送流量。\n解析端点的过程被称为服务发现。\n什么是端点？ 集群是一个识别特定主机的端点的集合。每个端点都有以下属性。\n地址 (address)\n该地址代表上游主机地址。地址的形式取决于集群的类型。对于 STATIC 或 EDS 集群类型，地址应该是一个 IP，而对于 LOGICAL 或 STRICT DNS 集群类型，地址应该是一个通过 DNS 解析的主机名。\n主机名 (hostname)\n一个与端点相关的主机名。注意，主机名不用于路由或解析地址。它与端点相关联，可用于任何需要主机名的功能，如自动主机重写。\n健康检查配置（health_check_config）\n可选的健康检查配置用于健康检查器联系健康检查主机。该配置包含主机名和可以联系到主机以执行健康检查的端口。注意，这个配置只适用于启用了主动健康检查的上游集群。\n服务发现类型 有五种支持的服务发现类型，我们将更详细地介绍。\n静态 (STATIC) 静态服务发现类型是最简单的。在配置中，我们为集群中的每个主机指定一个已解析的网络名称。例如：\nclusters:- name:my_cluster_nametype:STATICload_assignment:cluster_name:my_service_nameendpoints:- lb_endpoints:- endpoint:address:socket_address:address:127.0.0.1port_value:8080注意，如果我们不提供类型，默认为 STATIC。\n严格的 DNS（STRICT_DNS） 通过严格的 DNS，Envoy 不断地、异步地解析集群中定义的 DNS 端点。如果 DNS 查询返回多个 IP 地址，Envoy 假定它们是集群的一部分，并在它们之间进行负载均衡。同样，如果 DNS 查询返回 0 个主机，Envoy 就认为集群没有任何主机。\n关于健康检查的说明——如果多个 DNS 名称解析到同一个 IP 地址，则不共享健康检查。这可能会给上游主机造成不必要的负担，因为 Envoy 会对同一个 IP 地址进行多次健康检查（跨越不同的 DNS 名称）。\n当 respect_dns_ttl  字段被启用时，我们可以使用 dns_refresh_rate  控制 DNS 名称的连续解析。如果不指定，DNS 刷新率默认为 5000ms。另一个设置（dns_failure_refresh_rate）控制故障时的刷新频率。如果没有提供，Envoy 使用 dns_refresh_rate。\n下面是一个 STRICT_DNS 服务发现类型的例子。\nclusters:- name:my_cluster_nametype:STRICT_DNSload_assignment:cluster_name:my_service_nameendpoints:- lb_endpoints:- endpoint:address:socket_address:address:my-serviceport_value:8080逻辑 DNS（LOGICAL_DNS） 逻辑 DNS 服务发现与严格 DNS 类似，它使用异步解析机制。然而，它只使用需要启动新连接时返回的第一个 IP 地址。\n因此，一个逻辑连接池可能包含与各种不同上游主机的物理连接。这些连接永远不会耗尽，即使在 DNS 解析返回零主机的情况下。\n 什么是连接池？\n集群中的每个端点将有一个或多个连接池。例如，根据所支持的上游协议，每个协议可能有一个连接池分配。Envoy 中的每个工作线程也为每个集群维护其连接池。例如，如果 Envoy 有两个线程和一个同时支持 HTTP/1 和 HTTP/2 的集群，将至少有四个连接池。连接池的方式是基于底层线程协议的。对于 HTTP/1.1，连接池根据需要获取端点的连接（最多到断路限制）。当请求变得可用时，它们就被绑定到连接上。 当使用 HTTP/2 时，连接池在一个连接上复用多个请求，最多到 max_concurrent_streams 和 max_requests_per_connections 指定的限制。HTTP/2 连接池建立尽可能多的连接，以满足请求。\n 逻辑 DNS 的一个典型用例是用于大规模网络服务。通常使用轮询 DNS，它们在每次查询时返回多个 IP 地址的不同结果。如果我们使用严格的 DNS 解析，Envoy 会认为集群端点在每次内部解析时都会改变，并会耗尽连接池。使用逻辑 DNS，连接将保持存活，直到它们被循环。\n与严格的 DNS 一样，逻辑 DNS 也使用 respect_dns_ttl 和 dns_refresh_rate  字段来配置 DNS 刷新率。\nclusters:- name:my_cluster_nametype:LOGICAL_DNSload_assignment:cluster_name:my_service_nameendpoints:- lb_endpoints:- endpoint:address:socket_address:address:my-serviceport_value:8080端点发现服务（EDS） Envoy 可以使用端点发现服务来获取集群的端点。通常情况下，这是首选的服务发现机制。Envoy 获得每个上游主机的显式知识（即不需要通过 DNS 解析的负载均衡器进行路由）。每个端点都可以携带额外的属性，可以告知 Envoy 负载均衡的权重和金丝雀状态区，等等。\nclusters:- name:my_cluster_nametype:EDSeds_cluster_config:eds_config:...我们会在动态配置和 xDS 一章中更详细地解释动态配置。\n原始目的地（ORIGINAL_DST） 当与 Envoy 的连接通过 iptables REDIRECT 或 TPROXY 目标或与代理协议的连接时，我们使用原来的目标集群类型。\n在这种情况下，请求被转发到重定向元数据（例如，使用 x-envoy-original-dst-host 头）地址的上游主机，而无需任何配置或上游主机发现。\n当上游主机的连接闲置时间超过 cleanup_interval 字段中指定的时间（默认为 5000 毫秒）时，这些连接会被汇集起来并被刷新。\nclusters:- name:original_dst_clustertype:ORIGINAL_DSTlb_policy:ORIGINAL_DST_LBORIGINAL_DST 集群类型可以使用的唯一负载均衡策略是 ORIGINAL_DST_LB 策略。\n除了上述服务发现机制外，Envoy 还支持自定义集群发现机制。我们可以使用 cluster_type 字段配置自定义的发现机制。\nEnvoy 支持两种类型的健康检查，主动和被动。我们可以同时使用这两种类型的健康检查。在主动健康检查中，Envoy 定期向端点发送请求以检查其状态。使用被动健康检查，Envoy 监测端点如何响应连接。它使 Envoy 甚至在主动健康检查将其标记为不健康之前就能检测到一个不健康的端点。Envoy 的被动健康检查是通过异常点检测实现的。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"dd7f3dd2e7a823c8680347ed23cd1139","permalink":"https://lib.jimmysong.io/envoy-handbook/cluster/service-discovery/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/envoy-handbook/cluster/service-discovery/","section":"envoy-handbook","summary":"集群可以在配置文件中静态配置，也可以通过集群发现服务（CDS）API 动态配置。每个集群都是一个端点的集合，Envoy 需要解析这些端点来发送流量。 解析端点的过程被称为服务发现。 什么是端点？ 集群是一个识别","tags":["Envoy"],"title":"服务发现","type":"book"},{"authors":null,"categories":["Istio"],"content":"这一节中，我将为你介绍服务网格的架构，首先我们先来了解下服务网格常用的四种部署模式，然后是其最基本的组成部分——控制平面和数据平面。\n服务网格的部署模式 这四种模式分别是Sidecar 代理、节点共享代理、Service Account/节点共享代理、带有微代理的共享远程代理。\n   服务网格的部署模式  这几种架构模式的主要区别是代理所在的位置不同，模式一在每个应用程序旁运行一个代理，模式二在每个节点上运行一个代理，模式三在每个节点中，再根据 ServiceAccount 的数量，每个 ServiceAccount 运行一个代理，模式四在每个应用旁运行一个微代理，这个代理仅处理 mTLS ，而七层代理位于远程，这几种方式在安全性、隔离及运维开销上的对比如这个表格所示。\n   模式 内存开销 安全性 故障域 运维     Sidecar 代理 因为为每个 pod 都注入一个代理，所以开销最大。 由于 sidecar 必须与工作负载一起部署，工作负载有可能绕过 sidecar。 Pod 级别隔离，如果有代理出现故障，只影响到 Pod 中的工作负载。 可以单独升级某个工作负载的 sidecar 而不影响其他工作负载。   节点共享代理 每个节点上只有一个代理，为该节点上的所有工作负载所共享，开销小。 对加密内容和私钥的管理存在安全隐患。 节点级别隔离，如果共享代理升级时出现版本冲突、配置冲突或扩展不兼容等问题，则可能会影响该节点上的所有工作负载。 不需要考虑注入 Sidecar 的问题。   Service Account/节点共享代理 服务账户/身份下的所有工作负载都使用共享代理，开销小。 工作负载和代理之间的连接的认证及安全性无法保障。 节点和服务账号之间级别隔离，故障同“节点共享代理”。 同“节点共享代理”。   带有微代理的共享远程代理 因为为每个 pod 都注入一个微代理，开销比较大。 微代理专门处理 mTLS，不负责 L7 路由，可以保障安全性。 当需要应用7层策略时，工作负载实例的流量会被重定向到L7代理上，若不需要，则可以直接绕过。该L7代理可以采用共享节点代理、每个服务账户代理，或者远程代理的方式运行。 同“Sidecar 代理”。    Istio 是目前最流行的服务网格的开源实现，它的架构目前来说有两种，一种是 Sidecar 模式，这也是 Istio 最传统的部署架构，另一种是 Proxyless 模式。这两种模式，我们将在下一节中介绍。\n下面我们将介绍服务网格的两个主要组成部分——控制平面和数据平面。\n控制平面 控制平面的特点：\n 不直接解析数据包 与控制平面中的代理通信，下发策略和配置 负责网络行为的可视化 通常提供 API 或者命令行工具可用于配置版本化管理，便于持续集成和部署  数据平面 数据平面的特点：\n 通常是按照无状态目标设计的，但实际上为了提高流量转发性能，需要缓存一些数据，因此无状态也是有争议的 直接处理入站和出站数据包，转发、路由、健康检查、负载均衡、认证、鉴权、产生监控数据等 对应用来说透明，即可以做到无感知部署  ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"998587326c9704fc8bfea7ba7744d304","permalink":"https://lib.jimmysong.io/istio-handbook/concepts/service-mesh-architectures/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/istio-handbook/concepts/service-mesh-architectures/","section":"istio-handbook","summary":"这一节中，我将为你介绍服务网格的架构，首先我们先来了解下服务网格常用的四种部署模式，然后是其最基本的组成部分——控制平面和数据平面。 服务网格的部署模式 这四种模式分别是Sidecar 代理、节点共享代理、","tags":["Istio","Service Mesh"],"title":"服务网格架构","type":"book"},{"authors":null,"categories":["Envoy"],"content":"正如在介绍章节中提到的，监听器子系统处理下游或传入的请求处理。监听器子系统负责传入的请求和对客户端的响应路径。除了定义 Envoy 对传入请求进行 “监听” 的地址和端口外，我们还可以选择对每个监听器进行监听过滤器的配置。\n不要把监听器过滤器和我们前面讨论的网络过滤器链和 L3/L4 过滤器混淆起来。Envoy 在处理网络级过滤器之前先处理监听器过滤器，如下图所示。\n   监听器过滤器  请注意，在没有任何监听器过滤器的情况下操作 Envoy 也是常见的。\nEnvoy 会在网络级过滤器之前处理监听器过滤器。我们可以在监听器过滤器中操作连接元数据，通常是为了影响后来的过滤器或集群如何处理连接。\n监听器过滤器对新接受的套接字进行操作，并可以停止或随后继续执行进一步的过滤器。监听器过滤器的顺序很重要，因为 Envoy 在监听器接受套接字后，在创建连接前，会按顺序处理这些过滤器。\n我们可以使用监听器过滤器的结果来进行过滤器匹配，并选择一个合适的网络过滤器链。例如，我们可以使用 HTTP 检查器监听器过滤器来确定 HTTP 协议（HTTP/1.1 或 HTTP/2）。基于这个结果，我们就可以选择并运行不同的网络过滤器链。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"a5e49bb1d507d6e6f493d1c5644f061f","permalink":"https://lib.jimmysong.io/envoy-handbook/listener/listener-filters/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/envoy-handbook/listener/listener-filters/","section":"envoy-handbook","summary":"正如在介绍章节中提到的，监听器子系统处理下游或传入的请求处理。监听器子系统负责传入的请求和对客户端的响应路径。除了定义 Envoy 对传入请求进行 “监听” 的地址和端口外，我们还可以选择对每","tags":["Envoy"],"title":"监听器过滤器","type":"book"},{"authors":null,"categories":["Envoy"],"content":"扩展 Envoy 的一种方式是实现不同的过滤器，处理或增强请求。这些过滤器可以生成统计数据，翻译协议，修改请求，等等。\nHTTP 过滤器就是一个例子，比如外部的 authz 过滤器和其他内置于 Envoy 二进制的过滤器。\n此外，我们还可以编写我们的过滤器，让 Envoy 动态地加载和运行。我们可以通过正确的顺序声明来决定我们要在过滤器链中的哪个位置运行过滤器。\n我们有几个选择来扩展 Envoy。默认情况下，Envoy 过滤器是用 C++ 编写的。但是，我们可以用 Lua 脚本编写，或者使用 WebAssembly（WASM）来开发其他编程语言的 Envoy 过滤器。\n请注意，与 C++ 过滤器相比，Lua 和 Wasm 过滤器的 API 是有限的。\n1. 原生 C++ API\n第一个选择是编写原生 C++ 过滤器，然后将其与 Envoy 打包。这就需要我们重新编译 Envoy，并维护我们的版本。如果我们试图解决复杂或高性能的用例，采取这种方式是有意义的。\n2. Lua 过滤器\n第二个选择是使用 Lua 脚本。在 Envoy 中有一个 HTTP 过滤器，允许我们定义一个 Lua 脚本，无论是内联还是外部文件，并在请求和响应流程中执行。\n3. Wasm 过滤器\n最后一个选项是基于 Wasm 的过滤器。我们用这个选项把过滤器写成一个单独的 Wasm 模块，Envoy 在运行时动态加载它。\n在接下来的模块中，我们将学习更多关于 Lua 和 Wasm 过滤器的知识。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"9c69fdb2c5cf2cb72e40af04713a332d","permalink":"https://lib.jimmysong.io/envoy-handbook/extending-envoy/overview/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/envoy-handbook/extending-envoy/overview/","section":"envoy-handbook","summary":"扩展 Envoy 的一种方式是实现不同的过滤器，处理或增强请求。这些过滤器可以生成统计数据，翻译协议，修改请求，等等。 HTTP 过滤器就是一个例子，比如外部的 authz 过滤器和其他内置于 Envoy 二进制的过滤器。 此外，我们还可以编写我们","tags":["Envoy"],"title":"可扩展性概述","type":"book"},{"authors":null,"categories":["Istio"],"content":"下面将带您了解 Istio 流量管理相关的基础概念与配置示例。\n VirtualService：在 Istio 服务网格中定义路由规则，控制流量路由到服务上的各种行为。 DestinationRule：是 VirtualService 路由生效后，配置应用与请求的策略集。 ServiceEntry：通常用于在 Istio 服务网格之外启用的服务请求。 Gateway：为 HTTP/TCP 流量配置负载均衡器，最常见的是在网格边缘的操作，以启用应用程序的入口流量。 EnvoyFilter：描述了针对代理服务的过滤器，用来定制由 Istio Pilot 生成的代理配置。一定要谨慎使用此功能。错误的配置内容一旦完成传播，可能会令整个服务网格陷入瘫痪状态。这一配置是用于对 Istio 网络系统内部实现进行变更的。  注：本文中的示例引用自 Istio 官方 Bookinfo 示例，见：Istio 代码库，且对于配置的讲解都以在 Kubernetes 中部署的服务为准。\nVirtualService VirtualService 故名思义，就是虚拟服务，在 Istio 1.0 以前叫做 RouteRule。VirtualService 中定义了一系列针对指定服务的流量路由规则。每个路由规则都是针对特定协议的匹配规则。如果流量符合这些特征，就会根据规则发送到服务注册表中的目标服务（或者目标服务的子集或版本）。\n注意：VirtualService 中的规则是按照在 YAML 文件中的顺序执行的，这就是为什么在存在多条规则时，需要慎重考虑优先级的原因。\n配置说明\n下面是 VirtualService 的配置说明。\n   字段 类型 描述     hosts string[] 必要字段：流量的目标主机。可以是带有通配符前缀的 DNS 名称，也可以是 IP 地址。根据所在平台情况，还可能使用短名称来代替 FQDN。这种场景下，短名称到 FQDN 的具体转换过程是要靠下层平台完成的。一个主机名只能在一个 VirtualService 中定义。同一个 VirtualService 中可以用于控制多个 HTTP 和 TCP 端口的流量属性。Kubernetes 用户注意：当使用服务的短名称时（例如使用 reviews，而不是 reviews.default.svc.cluster.local），Istio 会根据规则所在的命名空间来处理这一名称，而非服务所在的命名空间。假设 “default” 命名空间的一条规则中包含了一个 reviews 的 host 引用，就会被视为 reviews.default.svc.cluster.local，而不会考虑 reviews 服务所在的命名空间。为了避免可能的错误配置，建议使用 FQDN 来进行服务引用。 hosts 字段对 HTTP 和 TCP 服务都是有效的。网格中的服务也就是在服务注册表中注册的服务，必须使用他们的注册名进行引用；只有 Gateway 定义的服务才可以使用 IP 地址。   gateways string[] Gateway 名称列表，Sidecar 会据此使用路由。VirtualService 对象可以用于网格中的 Sidecar，也可以用于一个或多个 Gateway。这里公开的选择条件可以在协议相关的路由过滤条件中进行覆盖。保留字 mesh 用来指代网格中的所有 Sidecar。当这一字段被省略时，就会使用缺省值（mesh），也就是针对网格中的所有 Sidecar 生效。如果提供了 gateways 字段，这一规则就只会应用到声明的 Gateway 之中。要让规则同时对 Gateway 和网格内服务生效，需要显式的将 mesh 加入 gateways 列表。   http HTTPRoute[] HTTP 流量规则的有序列表。这个列表对名称前缀为 http-、http2-、grpc- 的服务端口，或者协议为 HTTP、HTTP2、GRPC 以及终结的 TLS，另外还有使用 HTTP、HTTP2 以及 GRPC 协议的 ServiceEntry 都是有效的。进入流量会使用匹配到的第一条规则。   tls TLSRoute[] 一个有序列表，对应的是透传 TLS 和 HTTPS 流量。路由过程通常利用 ClientHello 消息中的 SNI 来完成。TLS 路由通常应用在 https-、tls- 前缀的平台服务端口，或者经 Gateway 透传的 HTTPS、TLS 协议端口，以及使用 HTTPS 或者 TLS 协议的 ServiceEntry 端口上。注意：没有关联 VirtualService 的 https- 或者 tls- 端口流量会被视为透传 TCP 流量。   tcp TCPRoute[] 一个针对透传 TCP 流量的有序路由列表。TCP 路由对所有 HTTP 和 TLS 之外的端口生效。进入流量会使用匹配到的第一条规则。    示例\n下面的例子中配置了一个名为 reviews 的 VirtualService，该配置的作用是将所有发送给 reviews 服务的流量发送到 v1 版本的子集。\napiVersion:networking.istio.io/v1alpha3kind:VirtualServicemetadata:name:reviewsspec:hosts:- reviewshttp:- route:- destination:host:reviewssubset:v1 该配置中流量的目标主机是 reviews，如果该服务和规则部署在 Kubernetes 的 default namespace 下的话，对应于 Kubernetes 中的服务的 DNS 名称就是 reviews.default.svc.cluster.local。 我们在 hosts 配置了服务的名字只是表示该配置是针对 reviews.default.svc.cluster.local 的服务的路由规则，但是具体将对该服务的访问的流量路由到哪些服务的哪些实例上，就是要通过 destination 的配置了。 我们看到上面的 VirtualService 的 HTTP 路由中还定义了一个 destination。destination 用于定义在网络中可寻址的服务，请求或连接在经过路由规则的处理之后，就会被发送给 destination。destination.host 应该明确指向服务注册表中的一个服务。Istio 的服务注册表除包含平台服务注册表中的所有服务（例如 Kubernetes 服务、Consul 服务）之外，还包含了 ServiceEntry 资源所定义的服务。VirtualService 中只定义流量发送给哪个服务的路由规则，但是并不知道要发送的服务的地址是什么，这就需要 DestinationRule 来定义了。 subset 配置流量目的地的子集，下文会讲到。VirtualService 中其实可以除了 hosts 字段外其他什么都不配置，路由规则可以在 DestinationRule 中单独配置来覆盖此处的默认规则。  Subset subset 不属于 Istio 创建的 CRD，但是它是一条重要的配置信息，有必要单独说明下。subset 是服务端点的集合，可以用于 A/B 测试或者分版本路由等场景。另外在 subset 中可以覆盖服务级别的即 VirtualService 中的定义的流量策略。\n以下是subset 的配置信息。对于 Kubernetes 中的服务，一个 subset 相当于使用 label 的匹配条件选出来的 service。\n   字段 类型 描述     name string 必要字段。服务名和 subset 名称可以用于路由规则中的流量拆分。   labels map\u0026lt;string, string\u0026gt; 必要字段。使用标签对服务注册表中的服务端点进行筛选。   trafficPolicy TrafficPolicy 应用到这一 subset 的流量策略。缺省情况下 subset 会继承 DestinationRule 级别的策略，这一字段的定义则会覆盖缺省的继承策略。    DestinationRule DestinationRule 所定义的策略，决定了经过路由处理之后的流量的访问策略。这些策略中可以定义负载均衡配置、连接池大小以及外部检测（用于在负载均衡池中对不健康主机进行识别和驱逐）配置。\n配置说明\n下面是 DestinationRule 的配置说明。\n   字段 类型 描述     name string 必要字段。服务名和 subset 名称可以用于路由规则中的流量拆分。   labels map\u0026lt;string, string\u0026gt; 必要字段。使用标签对服务注册表中的服务端点进行筛选。   trafficPolicy TrafficPolicy 应用到这一子集的流量策略。缺省情况下子集会继承 DestinationRule 级别的策略，这一字段的定义则会覆盖缺省的继承策略。    示例\n下面是一条对 productpage 服务的流量目的地策略的配置。\napiVersion:networking.istio.io/v1alpha3kind:DestinationRulemetadata:name:reviewsspec:host:reviewssubsets:- name:v1labels:version:v1该路由策略将所有对 reviews 服务的流量路由到 v1 的 subset。\nServiceEntry Istio 服务网格内部会维护一个与平台无关的使用通用模型表示的服务注册表，当你的服务网格需要访问外部服务的时候，就需要使用 ServiceEntry 来添加服务注册。\nEnvoyFilter EnvoyFilter 描述了针对代理服务的过滤器，用来定制由 Istio Pilot 生成的代理配置。一定要谨慎使用此功能。错误的配置内容一旦完成传播，可能会令整个服务网格陷入瘫痪状态。这一配置是用于对 Istio 网络系统内部实现进行变更的，属于高级配置，用于扩展 Envoy 中的过滤器的。\nGateway Gateway 为 HTTP/TCP 流量配置了一个负载均衡，多数情况下在网格边缘进行操作，用于启用一个服务的入口（ingress）流量，相当于前端代理。与 Kubernetes 的 Ingress 不同，Istio Gateway 只配置四层到六层的功能（例如开放端口或者 TLS 配置），而 Kubernetes 的 Ingress 是七层的。将 VirtualService 绑定到 Gateway 上，用户就可以使用标准的 Istio 规则来控制进入的 HTTP 和 TCP 流量。\nGateway 设置了一个集群外部流量访问集群中的某些服务的入口，而这些流量究竟如何路由到那些服务上则需要通过配置 VirtualServcie 来绑定。下面仍然以 productpage 这个服务来说明。\napiVersion:networking.istio.io/v1alpha3kind:Gatewaymetadata:name:bookinfo-gatewayspec:selector:istio:ingressgateway# 使用默认的控制器servers:- port:number:80name:httpprotocol:HTTPhosts:- \u0026#34;*\u0026#34;---apiVersion:networking.istio.io/v1alpha3kind:VirtualServicemetadata:name:bookinfospec:hosts:- \u0026#34;*\u0026#34;gateways:- bookinfo-gatewayhttp:- match:- uri:exact:/productpage- uri:exact:/login- …","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"974b5da33836557ad3ae8b68eff70895","permalink":"https://lib.jimmysong.io/istio-handbook/traffic-management/basic/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/istio-handbook/traffic-management/basic/","section":"istio-handbook","summary":"下面将带您了解 Istio 流量管理相关的基础概念与配置示例。 VirtualService：在 Istio 服务网格中定义路由规则，控制流量路由到服务上的各种行为。 DestinationRule：是 VirtualService 路由生效后，配置应用与请","tags":["Istio","Service Mesh"],"title":"流量管理基础概念","type":"book"},{"authors":null,"categories":["Envoy"],"content":"在整个课程中，我们已经多次提到了管理接口。Envoy 暴露了一个管理接口，允许我们修改 Envoy，并获得一个视图和查询指标和配置。\n管理接口由一个具有多个端点的 REST API 和一个简单的用户界面组成，如下图所示。\n   Envoy 管理接口  管理接口必须使用 admin 字段明确启用。例如：\nadmin:address:socket_address:address:127.0.0.1port_value:9901在启用管理接口时要小心。任何有权限进入管理接口的人都可以进行破坏性的操作，比如关闭服务器（/quitquit 端点）。我们还可能让他们访问私人信息（指标、集群名称、证书信息等）。目前（Envoy 1.20 版本），管理端点是不安全的，也没有办法配置认证或 TLS。有一个工作项目正在进行中，它将限制只有受信任的 IP 和客户端证书才能访问，以确保传输安全。\n在这项工作完成之前，应该只允许通过安全网络访问管理接口，而且只允许从连接到该安全网络的主机访问。我们可以选择只允许通过 localhost 访问管理接口，如上面的配置中所示。另外，如果你决定允许从远程主机访问，那么请确保你也设置了防火墙规则。\n在接下来的课程中，我们将更详细地了解管理接口的不同功能。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"2793f9bc675d039aaa32b9212af3507d","permalink":"https://lib.jimmysong.io/envoy-handbook/admin-interface/enabling-admin-interface/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/envoy-handbook/admin-interface/enabling-admin-interface/","section":"envoy-handbook","summary":"在整个课程中，我们已经多次提到了管理接口。Envoy 暴露了一个管理接口，允许我们修改 Envoy，并获得一个视图和查询指标和配置。 管理接口由一个具有多个端点的 REST API 和一个简单的用户界面组成，如下图所示。 Envoy","tags":["Envoy"],"title":"启用管理接口","type":"book"},{"authors":null,"categories":["Istio"],"content":"为了解释什么是认证或 authn，我们将从访问控制试图回答的问题开始：一个主体能否对一个对象执行操作？\n如果我们把上述问题翻译成 Istio 和 Kubernetes 的世界，它将是 “服务 X 能否对服务 Y 进行操作？”\n这个问题的三个关键部分是：主体、动作和对象。\n主体和对象都是 Kubernetes 中的服务。动作，假设我们谈论的是 HTTP，是一个 GET 请求，一个 POST，一个 PUT 等等。\n认证是关于主体（或者在我们的例子中是服务的身份）的。认证是验证某种凭证的行为，并确保该凭证是有效和可信的。一旦进行了认证，我们就有了一个经过认证的主体。下次你旅行时，你向海关官员出示你的护照或身份证，他们会对其进行认证，确保你的凭证（护照或身份证）是有效和可信的。\n在 Kubernetes 中，每个工作负载都被分配了一个独特的身份，它用来与其他每个工作负载进行通信 - 该身份以服务账户的形式提供给工作负载。服务账户是运行时中存在的身份 Pods。\nIstio 使用来自服务账户的 X.509 证书，它根据名为 SPIFFE（每个人的安全生产身份框架）的规范创建一个新的身份。\n证书中的身份被编码在证书的 Subject alternate name 字段中，它看起来像这样。\nspiffe://cluster.local/ns/\u0026lt;pod namespace\u0026gt;/sa/\u0026lt;pod service account\u0026gt; 当两个服务开始通信时，它们需要交换带有身份信息的凭证，以相互验证自己。客户端根据安全命名信息检查服务器的身份，看它是否是服务的授权运行者。\n服务器根据授权策略确定客户可以访问哪些信息。此外，服务器可以审计谁在什么时间访问了什么，并决定是否批准或拒绝客户对服务器的调用。\n安全命名信息包含从服务身份到服务名称的映射。服务器身份是在证书中编码的，而服务名称是由发现服务或 DNS 使用的名称。从一个身份 A 到一个服务名称 B 的单一映射意味着 “A 被允许和授权运行服务 B”。安全命名信息由 Pilot 生成，然后分发给所有 sidecar 代理。\n证书创建和轮换 对于网格中的每个工作负载，Istio 提供一个 X.509 证书。一个名为 pilot-agent 的代理在每个 Envoy 代理旁边运行，并与控制平面（istiod）一起工作，自动进行密钥和证书的轮转。\n   证书和秘钥管理  在运行时创建身份时，有三个部分在起作用：\n Citadel（控制平面的一部分） Istio 代理 Envoy 的秘密发现服务（SDS）  Istio Agent 与 Envoy sidecar 一起工作，通过安全地传递配置和秘密，帮助它们连接到服务网格。即使 Istio 代理在每个 pod 中运行，我们也认为它是控制平面的一部分。\n秘密发现服务（SDS）简化了证书管理。如果没有 SDS，证书必须作为秘密（Secret）创建，然后装入代理容器的文件系统中。当证书过期时，需要更新秘密，并重新部署代理，因为 Envoy 不会从磁盘动态重新加载证书。当使用 SDS 时，SDS 服务器将证书推送给 Envoy 实例。每当证书过期时，SDS 会推送更新的证书，Envoy 可以立即使用它们。不需要重新部署代理服务器，也不需要中断流量。在 Istio 中，Istio Agent 作为 SDS 服务器，实现了秘密发现服务接口。\n每次我们创建一个新的服务账户时，Citadel 都会为它创建一个 SPIFFE 身份。每当我们安排一个工作负载时，Pilot 会用包括工作负载的服务账户在内的初始化信息来配置其 sidecar。\n当工作负载旁边的 Envoy 代理启动时，它会联系 Istio 代理并告诉它工作负载的服务账户。代理验证该实例，生成 CSR（证书签名请求），将 CSR 以及工作负载的服务账户证明（在 Kubernetes 中，是 pod 的服务账户 JWT）发送给 Citadel。Citadel 将执行认证和授权，并以签名的 X.509 证书作为回应。Istio 代理从 Citadel 获取响应，将密钥和证书缓存在内存中，并通过 SDS 通过 Unix 域套接字将其提供给 Envoy。将密钥存储在内存中比存储在磁盘上更安全；在使用 SDS 时，Istio 绝不会将任何密钥写入磁盘作为其操作的一部分。Istio 代理还定期刷新凭证，在当前凭证过期前从 Citadel 检索任何新的 SVID（SPIFFE 可验证身份文件）。\n   身份签发流程   SVID 是一个工作负载可以用来向资源或调用者证明其身份的文件。它必须由一个权威机构签署，并包含一个 SPIFFE ID，它代表了提出该文件的服务的身份，例如，spiffe://clusterlocal/ns/my-namespace/sa/my-sa。\n 这种解决方案是可扩展的，因为流程中的每个组件只负责一部分工作。例如，Envoy 负责过期证书，Istio 代理负责生成私钥和 CSR，Citadel 负责授权和签署证书。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"73effe013bfa17ea49adbee2b2fe6de8","permalink":"https://lib.jimmysong.io/istio-handbook/security/authn/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/istio-handbook/security/authn/","section":"istio-handbook","summary":"为了解释什么是认证或 authn，我们将从访问控制试图回答的问题开始：一个主体能否对一个对象执行操作？ 如果我们把上述问题翻译成 Istio 和 Kubernetes 的世界，它将是 “服务 X 能否对服务 Y 进行操作？”","tags":["Istio","Service Mesh"],"title":"认证","type":"book"},{"authors":null,"categories":["安全"],"content":"日志记录了集群中的活动。审计日志是必要的，这不仅是为了确保服务按预期运行和配置，也是为了确保系统的安全。系统性的审计要求对安全设置进行一致和彻底的检查，以帮助识别潜在威胁。Kubernetes 能够捕获集群操作的审计日志，并监控基本的 CPU 和内存使用信息；然而，它并没有提供深入的监控或警报服务。\n关键点\n 在创建时建立 Pod 基线，以便能够识别异常活动。 在主机层面、应用层面和云端（如果适用）进行日志记录。 整合现有的网络安全工具，进行综合扫描、监控、警报和分析。 设置本地日志存储，以防止在通信失败的情况下丢失。  日志 在 Kubernetes 中运行应用程序的系统管理员应该为其环境建立一个有效的日志、监控和警报系统。仅仅记录 Kubernetes 事件还不足以了解系统上发生的行动的全貌。还应在主机级、应用级和云上（如果适用）进行日志记录。而且，这些日志可以与任何外部认证和系统日志相关联，以提供整个环境所采取的行动的完整视图，供安全审计员和事件响应者使用。\n在 Kubernetes 环境中，管理员应监控 / 记录以下内容：\n API 请求历史 性能指标 部署情况 资源消耗 操作系统调用 协议、权限变化 网络流量  当一个 Pod 被创建或更新时，管理员应该捕获网络通信、响应时间、请求、资源消耗和任何其他相关指标的详细日志以建立一个基线。正如上一节所详述的，匿名账户应被禁用，但日志策略仍应记录匿名账户采取的行动，以确定异常活动。\n应定期审计 RBAC 策略配置，并在组织的系统管理员发生变化时进行审计。这样做可以确保访问控制的调整符合基于角色的访问控制部分中概述的 RBAC 策略加固指导。\n审计应包括将当前日志与正常活动的基线测量进行比较，以确定任何日志指标和事件的重大变化。系统管理员应调查重大变动——例如，应用程序使用的变化或恶意程序的安装，如密码器，以确定根本原因。应该对内部和外部流量日志进行审计，以确保对连接的所有预期的安全限制已被正确配置，并按预期运行。管理员还可以在系统发展过程中使用这些审计，以确定何时不再需要外部访问并可以限制。\n日志可以导向外部日志服务，以确保集群外的安全专业人员的可用使用它们，尽可能接近实时地识别异常情况，并在发生损害时保护日志不被删除。如果使用这种方法，日志应该在传输过程中用 TLS 1.2 或 1.3 进行加密，以确保网络行为者无法在传输过程中访问日志并获得关于环境的宝贵信息。在利用外部日志服务器时，要采取的另一项预防措施是在 Kubernetes 内配置日志转发器，只对外部存储进行追加访问。这有助于保护外部存储的日志不被删除或被集群内日志覆盖。\nKubernetes 原生审计日志配置  Kubernetes 的审计功能默认是禁用的，所以如果没有写审计策略，就不会有任何记录。\n kube-apiserver 驻留在 Kubernetes 控制平面上，作为前端，处理集群的内部和外部请求。每个请求，无论是由用户、应用程序还是控制平面产生的，在其执行的每个阶段都会产生一个审计事件。当审计事件注册时，kube-apiserver 检查审计策略文件和适用规则。如果存在这样的规则，服务器会在第一个匹配的规则所定义的级别上记录该事件。Kubernetes 的内置审计功能默认是不启用的，所以如果没有写审计策略，就不会有任何记录。\n集群管理员必须写一个审计策略 YAML 文件，以建立规则，并指定所需的审计级别，以记录每种类型的审计事件。然后，这个审计策略文件被传递给 kube-apiserver，并加上适当的标志。一个规则要被认为是有效的，必须指定四个审计级别中的一个：none、Meatadataa、Request 或 RequestResponse。附录 L：审计策略展示了一个审计策略文件的内容，该文件记录了 RequestResponse 级别的所有事件。附录 M 向 kube-apiserver 提交审计策略文件的标志示例显示了 kube-apiserver 配置文件的位置，并提供了审计策略文件可以被传递给 kube-apiserver 的标志示例。附录 M 还提供了如何挂载卷和在必要时配置主机路径的指导。\nkube-apiserver 包括可配置的日志和 webhook 后端，用于审计日志。日志后端将指定的审计事件写入日志文件，webhook 后端可以被配置为将文件发送到外部 HTTP API。附录 M 中的例子中设置的 --audit-log-path 和 --audit-log-maxage 标志是可以用来配置日志后端的两个例子，它将审计事件写到一个文件中。log-path 标志是启用日志的最小配置，也是日志后端唯一需要的配置。这些日志文件的默认格式是 JSON，尽管必要时也可以改变。日志后端的其他配置选项可以在 Kubernetes 文档中找到。\n为了将审计日志推送给组织的 SIEM 平台，可以通过提交给 kube-apiserver 的 YAML 文件手动配置 webhook 后端。webhook 配置文件以及如何将该文件传递给 kube-apiserver 可以在附录 N：webhook 配置的示例中查看。关于如何在 kube-apiserver 中为 webhook 后端设置的配置选项的详尽列表，可以在 Kubernetes 文档中找到。\n工作节点和容器的日志记录 在 Kubernetes 架构中，有很多方法可以配置日志功能。在日志管理的内置方法中，每个节点上的 kubelet 负责管理日志。它根据其对单个文件长度、存储时间和存储容量的策略，在本地存储和轮转日志文件。这些日志是由 kubelet 控制的，可以从命令行访问。下面的命令打印了一个 Pod 中的容器的日志。\nkubectl logs [-f] [-p] POD [-c CONTAINER] 如果要对日志进行流式处理，可以使用 -f 标志；如果存在并需要来自容器先前实例的日志，可以使用 -p 标志；如果 Pod 中有多个容器，可以使用 -c 标志来指定一个容器。如果发生错误导致容器、Pod 或节点死亡，Kubernetes 中的本地日志解决方案并没有提供一种方法来保存存储在失败对象中的日志。NSA 和 CISA 建议配置一个远程日志解决方案，以便在一个节点失败时保存日志。\n远程记录的选项包括：\n   远程日志选项 使用的理由 配置实施     在每个节点上运行一个日志代理，将日志推送到后端 赋予节点暴露日志或将日志推送到后端的能力，在发生故障的情况下将其保存在节点之外。 配置一个 Pod 中的独立容器作为日志代理运行，让它访问节点的应用日志文件，并配置它将日志转发到组织的 SIEM。   在每个 Pod 中使用一个 sidecar 容器，将日志推送到一个输出流中 用于将日志推送到独立的输出流。当应用程序容器写入不同格式的多个日志文件时，这可能是一个有用的选项。 为每种日志类型配置 sidecar 容器，并用于将这些日志文件重定向到它们各自的输出流，在那里它们可以被 kubelet 处理。然后，节点级的日志代理可以将这些日志转发给 SIEM 或其他后端。   在每个 Pod 中使用一个日志代理 sidecar，将日志推送到后端 当需要比节点级日志代理所能提供的更多灵活性时。 为每个 Pod 配置，将日志直接推送到后端。这是连接第三方日志代理和后端的常用方法。   从应用程序中直接向后端推送日志 捕获应用程序的日志。Kubernetes 没有内置的机制直接来暴露或推送日志到后端。 各组织将需要在其应用程序中建立这一功能，或附加一个有信誉的第三方工具来实现这一功能。    Sidecar 容器与其他容器一起在 Pod 中运行，可以被配置为将日志流向日志文件或日志后端。Sidecar 容器也可以被配置为作为另一个标准功能容器的流量代理，它被打包和部署。\n为了确保这些日志代理在工作节点之间的连续性，通常将它们作为 DaemonSet 运行。为这种方法配置 DaemonSet，可以确保每个节点上都有一份日志代理的副本，而且对日志代理所做的任何改变在集群中都是一致的。\nSeccomp: 审计模式 除了上述的节点和容器日志外，记录系统调用也是非常有益的。在 Kubernetes 中审计容器系统调用的一种方法是使用安全计算模式（seccomp）工具。这个工具默认是禁用的，但可以用来限制容器的系统调用能力，从而降低内核的攻击面。Seccomp 还可以通过使用审计配置文件记录正在进行的调用。\n自定义 seccomp 配置文件用于定义哪些系统调用是允许的，以及未指定调用的默认动作。为了在 Pod 中启用自定义 seccomp 配置文件，Kubernetes 管理员可以将他们的 seccomp 配置文件 JSON 文件写入到 /var/lib/kubelet/seccomp/ 目录，并将 seccompProfile 添加到 Pod 的 securityContext。自定义的 seccompProfile 还应该包括两个字段。Type: Localhost 和 localhostProfile: myseccomppolicy.json。记录所有的系统调用可以帮助管理员了解标准操作需要哪些系统调用，使他们能够进一步限制 seccomp 配置文件而不失去系统功能。\nSYSLOG Kubernetes 默认将 kubelet 日志和容器运行时日志写入 journald，如果该服务可用的话。如果组织希望对默认情况下不使用的系统使用 syslog 工具，或者从整个集群收集日志并将其转发到 syslog 服务器或其他日志存储和聚合平台，他们可以手动配置该功能。Syslog 协议定义了一个日志信息格式化标准。Syslog 消息包括一个头——由时间戳、主机名、应用程序名称和进程 ID（PID）组成，以及一个以明文书写的消息。Syslog 服务，如 syslog-ng® 和 rsyslog，能够以统一的格式收集和汇总整个系统的日志。许多 Linux 操作系统默认使用 rsyslog 或 journald——一个事件日志守护程序，它优化了日志存储并通过 journalctl 输出 syslog 格式的日志。在运行某些 Linux 发行版的节点上，syslog 工具默认在操作系统层面记录事件。运行这些 Linux 发行版的容器，默认也会使用 syslog 收集日志。由 syslog 工具收集的日志存储在每个适用的节点或容器的本地文件系统中，除非配置了一个日志聚合平台来收集它们。\nSIEM 平台 安全信息和事件管理（SIEM）软件从整个组织的网络中收集日志。SIEM 软件将防火墙日志、应用程序日志等汇集在一起；将它们解析出来，提供一个集中的平台，分析人员可以从这个平台上监控系统安全。SIEM 工具在功能上有差异。一般来说，这些平台提供日志收集、威胁检测和警报功能。有些包括机器学习功能，可以更好地预测系统行为并帮助减少错误警报。在其环境中使用这些平台的组织可以将它们与 Kubernetes 集成，以更好地监测和保护集群。用于管理 Kubernetes 环境中的日志的开源平台是作为 SIEM 平台的替代品存在的。\n容器化环境在节点、Pod、容器和服务之间有许多相互依赖的关系。在这些环境中，Pod 和容器不断地在不同的节点上被关闭和重启。这给传统的 SIEM 带来了额外的挑战，它们通常使用 IP 地址来关联日志。即使是下一代的 SIEM 平台也不一定适合复杂的 Kubernetes 环境。然而，随着 Kubernetes 成为最广泛使用的容器编排平台，许多开发 SIEM 工具的组织已经开发了专门用于 Kubernetes 环境的产品变化，为这些容器化环境提供全面的监控解决方案。管理员应该了解他们平台的能力，并确保他们的日志充分捕捉到环境，以支持未来的事件响应。\n警报 Kubernetes 本身并不支持警报功能；然而， …","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"fcbd55f8feac9da9ecaf4c5e32283c76","permalink":"https://lib.jimmysong.io/kubernetes-hardening-guidance/logging/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/kubernetes-hardening-guidance/logging/","section":"kubernetes-hardening-guidance","summary":"日志记录了集群中的活动。审计日志是必要的，这不仅是为了确保服务按预期运行和配置，也是为了确保系统的安全。系统性的审计要求对安全设置进行一致和彻底的检查，以帮助识别潜在威胁。Kubernetes 能够捕获","tags":["Kubernetes","安全"],"title":"日志审计","type":"book"},{"authors":null,"categories":["Envoy"],"content":"我们可以在虚拟主机和路由层面定义重试策略。在虚拟主机级别设置的重试策略将适用于该虚拟主机的所有路由。如果在路由级别上定义了重试策略，它将优先于虚拟主机策略，并被单独处理——即路由级别的重试策略不会继承虚拟主机级别的重试策略的值。即使 Envoy 将重试策略独立处理，配置也是一样的。\n除了在配置中设置重试策略外，我们还可以通过请求头（即 x-envoy-retry-on 头）进行配置。\n在 Envoy 配置中，我们可以配置以下内容。\n 最大重试次数：Envoy 将重试请求，重试次数最多为配置的最大值。指数退避算法是用于确定重试间隔的默认算法。另一种确定重试间隔的方法是通过 Header（例如 x-envoy-upstream-rq-per-try-timeout-ms）。所有重试也包含在整个请求超时中，即 request_timeout 配置设置。默认情况下，Envoy 将重试次数设置为一次。 重试条件：我们可以根据不同的条件重试请求。例如，我们只能重试 5xx 响应代码，网关失败，4xx 响应代码，等等。 重试预算：重试预算规定了与活动请求数有关的并发请求的限制。这可以帮助防止过大的重试流量。 主机选择重试插件：重试期间的主机选择通常遵循与原始请求相同的过程。使用重试插件，我们可以改变这种行为，指定一个主机或优先级谓词，拒绝一个特定的主机，并导致重新尝试选择主机。  让我们看看几个关于如何定义重试策略的配置例子。我们使用 httpbin 并匹配返回 500 响应代码的 /status/500 路径。\nroute_config:name:5xx_routevirtual_hosts:- name:httpbindomains:[\u0026#34;*\u0026#34;]routes:- match:path:/status/500route:cluster:httpbinretry_policy:retry_on:\u0026#34;5xx\u0026#34;num_retries:5在 retry_policy 字段中，我们将重试条件（retry_on）设置为 500 ，这意味着我们只想在上游返回 HTTP 500 的情况下重试（将会如此）。Envoy 将重试该请求五次。这可以通过 num_retries 字段进行配置。\n如果我们运行 Envoy 并发送一个请求，该请求将失败（HTTP 500），并将创建以下日志条目：\n[2021-07-26T18:43:29.515Z] \u0026#34;GET /status/500 HTTP/1.1\u0026#34; 500 URX 0 0 269 269 \u0026#34;-\u0026#34; \u0026#34;curl/7.64.0\u0026#34; \u0026#34;1ae9ffe2-21f2-43f7-ab80-79be4a95d6d4\u0026#34; \u0026#34;localhost:10000\u0026#34; \u0026#34;127.0.0.1:5000\u0026#34; 注意到 500URX 部分告诉我们，上游响应为 500，URX 响应标志意味着 Envoy 拒绝了该请求，因为达到了上游重试限制。\n重试条件可以设置为一个或多个值，用逗号分隔，如下表所示。\n   重试条件（retry_on） 描述     5xx 在 5xx 响应代码或上游不响应时重试（包括 connect-failure 和 refused-stream）。   gatewayerror 对 502、 503 或响应 504 代码进行重试。   reset 如果上游根本没有回应，则重试。   connect-failure 如果由于与上游服务器的连接失败（例如，连接超时）而导致请求失败，则重试。   envoy-ratelimited 如果存在 x-envoy-ratelimited 头，则重试。   retriable-4xx 如果上游响应的是可收回的 4xx 响应代码（目前只有 HTTP 409），则重试。   refused-stream 如果上游以 REFUSED_STREAM 错误代码重置流，则重试。   retriable-status-codes 如果上游响应的任何响应代码与 x-envoy-retriable-status-codes 头中定义的代码相匹配（例如，以逗号分隔的整数列表，例如 \u0026#34;502,409\u0026#34;），则重试。   retriable-header 如果上游响应包括任何在 x-envoy-retriable-header-names 头中匹配的头信息，则重试。    除了控制 Envoy 重试请求的响应外，我们还可以配置重试时的主机选择逻辑。我们可以指定 Envoy 在选择重试的主机时使用的 retry_host_predicate。\n我们可以跟踪之前尝试过的主机（envoy.retry_host_predicates.previous_host），如果它们已经被尝试过，就拒绝它们。或者，我们可以使用 envoy.retry_host_predicates.canary_hosts  拒绝任何标记为 canary 的主机（例如，任何标记为 canary: true 的主机）。\n例如，这里是如何配置 previous_hosts 插件，以拒绝任何以前尝试过的主机，并重试最多 5 次的主机选择。\nroute_config:name:5xx_routevirtual_hosts:- name:httpbindomains:[\u0026#34;*\u0026#34;]routes:- match:path:/status/500route:cluster:httpbinretry_policy:retry_host_predicate:- name:envoy.retry_host_predicates.previous_hostshost_selection_retry_max_attempts:5在集群中定义了多个端点，我们会看到每次重试都会发送到不同的主机上。\n请求对冲 请求对冲背后的想法是同时向不同的主机发送多个请求，并使用首先响应的上游的结果。请注意，我们通常为幂等的请求配置这个功能，在这种情况下，多次进行相同的调用具有相同的效果。\n我们可以通过指定一个对冲策略来配置请求的对冲。目前，Envoy 只在响应请求超时的情况下进行对冲。因此，当一个初始请求超时时，会发出一个重试请求，而不取消原来超时的请求。Envoy 将根据重试策略向下游返回第一个良好的响应。\n可以通过设置 hedge_on_per_try_timeout 字段为 true 来配置对冲。就像重试策略一样，它可以在虚拟主机或路由级别上启用。\nroute_config:name:5xx_routevirtual_hosts:- name:httpbindomains:[\u0026#34;*\u0026#34;]hedge_policy:hedge_on_per_try_timeout:trueroutes:- match:... ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"1292efde041b8a65e56bc743d1de5b32","permalink":"https://lib.jimmysong.io/envoy-handbook/hcm/retries/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/envoy-handbook/hcm/retries/","section":"envoy-handbook","summary":"我们可以在虚拟主机和路由层面定义重试策略。在虚拟主机级别设置的重试策略将适用于该虚拟主机的所有路由。如果在路由级别上定义了重试策略，它将优先于虚拟主机策略，并被单独处理——即路由级别的重试策略不会继承","tags":["Envoy"],"title":"重试","type":"book"},{"authors":null,"categories":["可观测性"],"content":"OpenTelemetry 是一个大型项目。OpenTelemetry 项目的工作被划分为特殊兴趣小组（SIG）。虽然所有的项目决策最终都是通过 GitHub issue 和 pull request 做出的，但 SIG 成员经常通过 CNCF 的官方 Slack 保持联系，而且大多数 SIG 每周都会在 Zoom 上会面一次。\n任何人都可以加入一个 SIG。要想了解更多关于当前 SIG、项目成员和项目章程的细节，请查看 GitHub 上的 OpenTelemetry 社区档案库。\n规范 OpenTelemetry 是一个规范驱动的项目。OpenTelemetry 技术委员会负责维护该规范，并通过管理规范的 backlog 来指导项目的发展。\n小的改动可以以 GitHub issue 的方式提出，随后的 pull request 直接提交给规范。但是，对规范的重大修改是通过名为 OpenTelemetry Enhancement Proposals（OTEPs）的征求意见程序进行的。\n任何人都可以提交 OTEP。OTEP 由技术委员会指定的具体审批人进行审查，这些审批人根据其专业领域进行分组。OTEP 至少需要四个方面的批准才能被接受。在进行任何批准之前，通常需要详细的设计，以及至少两种语言的原型。我们希望 OTEP 的作者能够认真对待其他社区成员的要求和关注。我们的目标是确保 OpenTelemetry 适合尽可能多的受众的需求。\n一旦被接受，将根据 OTEP 起草规范变更。由于大多数问题已经在 OTEP 过程中得到了解决，因此规范变更只需要两次批准。\n项目治理 管理 OpenTelemetry 项目如何运作的规则和组织结构由 OpenTelemetry 治理委员会定义和维护，其成员经选举产生，任期两年。\n治理成员应以个人身份参与，而不是公司代表。但是，为同一雇主工作的委员会成员的数量有一个上限。如果因为委员会成员换了工作而超过了这个上限，委员会成员必须辞职，直到雇主代表的人数降到这个上限以下。\n发行版 OpenTelemetry 有一个基于插件的架构，因为有些观察能力系统需要一套插件和配置才能正常运行。\n发行版（distros）被定义为广泛使用的 OpenTelemetry 插件的集合，加上一组脚本或辅助功能，可能使 OpenTelemetry 与特定的后端连接更简单，或在特定环境中运行 OpenTelemetry。\n需要澄清的是，如果一个可观测性系统声称它与 OpenTelemetry 兼容，那么它应该总是可以使用 OpenTelemetry 而不需要使用某个特定的发行版。如果一个项目没有经过规范过程就扩展了 OpenTelemetry 的核心功能，或者包括任何导致它与上游 OpenTelemetry 仓库不兼容的变化，那么这个项目就是一个分叉，而不是一个发行版。\n注册表 为了便于发现目前有哪些语言、插件和说明，OpenTelemetry 提供了一个注册表。任何人都可以向 OpenTelemetry 注册表提交插件；在 OpenTelemetry 的 GitHub 组织内托管插件不是必须的。\n","date":1643990400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"21db822b2cec39856e2381ca897da87d","permalink":"https://lib.jimmysong.io/opentelemetry-obervability/organization/","publishdate":"2022-02-05T00:00:00+08:00","relpermalink":"/opentelemetry-obervability/organization/","section":"opentelemetry-obervability","summary":"附录 A：OpenTelemetry 项目组织","tags":["OpenTelemetry"],"title":"附录 A：OpenTelemetry 项目组织","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"该节将带领大家了解 Kubernetes 中的基本概念，尤其是作为 Kubernetes 中调度的最基本单位 Pod。\n本节中包括以下内容：\n 了解 Pod 的构成 Pod 的生命周期 Pod 中容器的启动顺序模板定义  Kubernetes 中的基本组件 kube-controller-manager 就是用来控制 Pod 的状态和生命周期的，在了解各种 controller 之前我们有必要先了解下 Pod 本身和其生命周期。\n","date":1653062400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"e4255c725d84ea3db1bcfc2069d5af2f","permalink":"https://lib.jimmysong.io/kubernetes-handbook/architecture/pod-state-and-lifecycle/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/architecture/pod-state-and-lifecycle/","section":"kubernetes-handbook","summary":"该节将带领大家了解 Kubernetes 中的基本概念，尤其是作为 Kubernetes 中调度的最基本单位 Pod。 本节中包括以下内容： 了解 Pod 的构成 Pod 的生命周期 Pod 中容器的启动顺序模板定义 Kubernetes 中的基本组件 kube-controller-manager 就是用来控制 Pod 的状态和生命周期的，在了解各","tags":["Kubernetes"],"title":"Pod 状态与生命周期管理","type":"book"},{"authors":null,"categories":["云原生"],"content":"如果您认为云原生基础架构是可购买的产品或是从云供应商那购买的服务器，我们很抱歉让您失望了。如果不采用这些做法并改变您建设和维护基础架构的方式，您就不会受益。\n它不仅仅影响服务器、网络和存储。它关乎的是工程师如何管理应用程序，就像接受故障一样。\n围绕云原生实践建立的文化与传统技术和工程组织有很大不同。我们并不是解决组织文化或结构问题的专家，但如果您希望改变组织结构，我们建议您从高绩效组织中实施 DevOps 实践的角度来看待价值观和经验教训。\n一些需要探索的地方是 Netflix 的文化套餐，它促进了自由和责任感，还有亚马逊的双比萨团队，这些团队以低开销推广自治团体。云原生应用程序需要与构建它们的团队具有相同的解耦特征。康威定律很好地描述了这一点：“设计系统的架构受制于产生这些设计的组织的沟通结构。”\n在我们结束本书时，我们希望关注哪些领域是您采用云原生实践时最重要的。我们还将讨论一些预测变化的基础架构模式，以便您知道将来要寻找什么。\n关注改变的地方 如果您拥有现有的基础架构或传统数据中心，则过渡到云原生不会在一夜之间发生。如果您有足够大的基础架构或两个以上的人员管理它，试图强制实施基础架构管理的新方法可能会失败。\n采用这些模式与配置新服务器或购买新软件无关。要开始采用云原生基础架构，最重要的是是首先关注这些领域：\n 人 架构 混乱 应用程序  直到您准备好这些区域以使用本书中描述的实践，才能开始更改基础架构。\n人  比竞争对手更快学习的能力可能是唯一的可持续竞争优势。\n——Arie de Geus\n 正如我们在第 2 章中所讨论的，人是实施任何变革中最难的部分。他们的抵制有很多原因，而那些要求进行变更以帮助其影响的人有责任。\n当需要改变的驱动激励时，变更更容易。主动提高潜力是一个很好的激励因素，但如果没有紧迫感，就很难改变行为。\n人们抗拒改变的原因大多来自恐惧。人们喜欢习惯，因为他们可以控制并避免意外。\n为了使任何技术取得成功的重大转变，您需要与人们合作以最大限度地减少他们的恐惧。给他们一种主人翁感，并解释变化的明确目标。确保突出新旧技术之间的相似之处，特别是在改变后他们将扮演的角色。\n人们了解这种变化并不是因为他们在旧系统或现有系统中的失败，这一点也很重要。他们需要明白，要求已经改变，环境不同，并且希望他们成为变革的一部分，因为你尊重他们所做的并且对他们能做的事有信心。\n他们需要学习新事物，为此，失败是必要的，预期的，并且是进步的标志。\n鼓励学习和实验，并奖励适应数据洞察的人员和系统。让工程师通过诸如 “百分之二十的时间” 之类的自由来探索新的可能性，可以做到这一点。\n如果敏捷系统没有改变，它就没有好处。一个不适应和改进的系统将无法满足正在改变和学习的企业的需求。\n一旦你能够激发人们寻求改变，你应该用信任和自由来赋予他们力量。不断引导他们将自己的目标与业务需求结合起来，并赋予他们应聘的管理职责。\n如果人们已经准备好采用本书中的做法，那么实现它就没有什么限制。关于创建组织变革的更深入的指导，我们推荐阅读 John P. Kotter（哈佛商业评论出版社）的 “领导变革”。\n改变环境文化需要组织的大量努力和支持，以及更改应用程序运行的基础架构。您选择的架构可能会对采用云原生模式的能力产生重大影响。\n架构  弹性、安全性、可伸缩性、可部署性、可测试性是架构问题。\n——Jez Humble\n 将应用程序迁移到云原生基础架构时，您需要考虑如何管理和设计应用程序。例如，作为云原生应用程序前身的 12 因子应用程序受益于在平台上运行。它们被设计为最小化手动管理，频繁更改和弹性。许多传统应用程序的架构都是为了抵制自动化，不经常升级和失败。在迁移它之前，您应该考虑应用程序的架构。\n单个应用程序架构是一个问题，但您还需要考虑应用程序如何与基础架构内的其他服务通信。应用程序应已云环境中支持的协议以及通过明确界定的接口进行通信，通过采用微服务保持应用程序范围很小可以帮助定义应用程序间接口和提高应用程序部署速度。但是，采用微服务会暴露出新的问题，如应用程序通信速度较慢以及分布式跟踪和策略控制网络的需求。 如果不能为您的基础架构提供好处，就不要采用微服务。\n虽然您几乎可以适应任何应用程序在容器中运行并使用容器编排器进行部署，但如果首选迁移所有关键业务数据库服务器，您将很快就会后悔。\n首先确定接近具有第 1 章概述的特性的应用程序并获得在云原生环境中运行它们的经验。一旦您围绕简单的应用程序集体获得经验和良好实践，那么您就可以决定接下来要做什么。\n您的服务器和服务也不例外。在将基础架构切换为不可变的之前，您应确保它解决了当前的问题，并意识到新问题。\n可靠系统中最重要的架构考虑是争取简单的解决方案。软件和系统自然会变得复杂，这会造成不稳定。在云中，您可以释放对许多区域的控制，因此在仍然可以控制的区域保持简单性很重要。\n无论您将内部部署基础架构迁移到云中还是创建新的解决方案，都要确保您在第 1 章中进行可用性数学计算，并为混乱情况做好准备。\n混沌管理  拥抱失败并期待混乱。\n——Netflix 的 Andrew Spyker\n 当您构建云原生应用程序和基础架构时，其目标是创建最小可行产品（MVP）和迭代。尝试指定您的客户需要什么或他们将如何使用您的产品可能会有用一段时间，但成功的应用程序将适应而不是预测。\n客户需求改变；应用程序需要随它们改变。您无法计划和构建完整的解决方案，因为在产品准备就绪时，需求已发生变化。\n保持敏捷并在现有技术基础上发展很重要。就像您为应用程序导入库一样，您应该为基础架构使用 IaaS 和 SaaS。你越努力建立自己，你就越能提供价值。\n无论何时释放对某物的控制，都会冒着意想不到的风险。如果因为导入的库已更新而导致应用程序中断，您将会知道这是什么感觉。\n您的应用程序依赖于库所提供的功能，该功能已更改。你是否应该删除库并编写自己的功能以便控制它？答案几乎总是不。相反，您更新应用程序以使用新库，或者将正在运行的较旧版本的库与应用程序捆绑在一起，以暂时避免损坏。\n运行在公有云上的基础架构也是如此。您不再控制硬连线网络，使用什么 RAID 控制器，或者虚拟机的哪个版本的虚拟机管理程序运行。您所拥有的只是可以在底层技术之上提供抽象的 API。\n您无法控制底层技术何时发生变化。如果云提供商弃用所有大型内存实例类型，则您别无选择，只能遵守。您要么适应新的尺寸，要么支付更换提供商的成本（时间和金钱）（请参阅附录 B 关于锁定）。\n最重要的是，您构建的基础架构不再可以从单个关键服务器获得。如果您采用云原生基础架构，无论您是否喜欢，您正在构建一个分布式系统。\n通过简单地避免失败来保持服务可用的旧做法不起作用。目标不再是您可以设计的最大数目的几个 9—— 而是你可以摆脱的最小数量的几个 9。\n站点可靠性工程（Site Reliability Engineering）以这种方式解释它：\n坚持 SLO 将百分之百满足是不现实的和不可取的：这样做可能会降低创新和部署的速度，需要昂贵的，过于保守的解决方案，或两者兼而有之。相反，最好允许一个错误预算 —— 一个可能错过 SLO 的速率，并且每天或每周对其进行跟踪。\n工程的目标不可用；它创造了商业价值。你应该制造弹性系统，但不要以过度工程解决方案为代价来避免混乱。\n测试更改以防止停机的旧方法也不起作用。为大型分布式系统创建测试环境并不重要。当服务经常更新并且部署是自助服务时尤其如此。\n当 Netflix 每天有 4,000 次部署或 Facebook 有 10,000 个同时运行的版本时，对环境进行快照是不可能的。测试环境需要动态分离生产部分。基础架构需要支持这种测试方法，并支持经常测试生产中的新代码所带来的失败。\n你可以测试一些混乱（参见第 5 章），但混沌根据定义是不可预测的。准备您的基础架构和应用程序，以便可预测地对混乱做出反应，而不要试图避免它。\n应用程序 基础架构的目的是运行应用程序。如果您的业务完全基于向其他公司提供基础架构，那么您的成功仍取决于其运行应用程序的能力。\n如果你建立它，他们不能保证来用。您创建的任何抽象需要提高运行应用程序的能力。\n 避免 “泄漏抽象”（Leaky Abstraction）\n抽象并不完全隐藏抽象的实现细节。泄漏抽象定律指出：“所有非平凡的抽象在一定程度上都是泄漏的。”\n这意味着你进一步抽象某事，隐藏事物的细节就越困难。\n例如，应用程序资源请求通常通过请求 CPU 核心的百分比，内存量和磁盘存储量来抽象化。这些资源的物理分配不直接由应用程序管理（API 不是由它们规定的），但对资源的请求很明显地表明运行应用程序的系统具有可用的这些类型的资源。\n相反，如果抽象是服务器或数据中心（例如，50 台服务器，0.2 个数据中心）的百分比，那么抽象并不具有相同的含义，因为不存在单一大小的服务器或数据中心单元。确保创建的抽象对于将使用它们的应用程序有意义。\n 云原生实践的好处是，随着您提高运行应用程序的能力，您还可以提高运行基础架构的能力。如果您遵循第 3-6 章的模式，您将很好地适应使用应用程序不断改进和调整您的基础架构以满足应用程序的需求。\n重点关注构建范围小，易于适应，易于操作和故障发生时具有弹性的应用程序。确保这些应用程序负责所有基础架构管理和更改。如果你这样做，你将创建云原生基础架构。\n预测未来 如果您已经采用了本书中的模式和实践，那么您现在处于未知领域。运行全球最大基础架构的公司已采用了这些做法。无论他们运行基础架构的新模式是否公开披露或仍在发现之中。\n好消息是您的基础架构现在被设计为敏捷和变化。您可以更轻松地适应您遇到的任何新挑战。\n为了展望未来的道路，我们可以不考虑现有的基础架构模式，而是考虑基础架构在哪里获得灵感 —— 软件。几乎所有基础架构采用的模式都来自软件开发模式。\n例如，分布式应用程序。很多年前，当软件暴露于互联网时，在单一代码库下的单个服务器上运行应用程序不会扩展。这包括管理应用程序的性能和流程限制。\n该应用程序需要复制到其他服务器上，然后进行负载均衡以满足需求。当达到限制时，它被分解成更小的组件，创建 API 以通过 HTTP 远程调用功能，而不是通过应用程序库。\n基础架构采取了类似的方法来扩大规模；在大多数领域只适应比软件更慢的速度。像 Kubernetes 这样的现代化基础架构平台将基础架构管理分解成更小的组件。这些组件可以根据其性能瓶颈（CPU、内存和 I/O）独立扩展，并且可以快速迭代。\n有些应用程序没有相同的扩展需求，也不需要适应新的架构。工程师的需要知道何时采用新技术。只有那些了解其堆栈的局限性和瓶颈的人才能决定什么是正确的方向。\n当发现新的限制时，请密切注意新的解决方案应用程序的发展。如果您了解限制是什么以及进行更改的原因，那么您将始终处于基础架构的领先地位。\n结论 本书的目标是帮助您更好地理解什么是云原生基础架构以及您为什么要采用它。虽然我们相信它比传统基础架构有很多优势，但我们不希望您在不理解它的情况下盲目使用任何技术。\n不要期望在不采用其伴随的文化和流程的情况下获得所有好处。只是在公有云中运行基础架构或运行容器编排器不会改变该基础架构如何适应的过程。\n在 AWS 中手动创建少量虚拟机，通过 SSH 连接到每台虚拟机，创建 Kubernetes 集群比改变人们的工作方式更容易。前者不是云原生的。\n请记住，配置基础架构的应用程序不是及时的静态快照；它们应该不断运行并将基础架构推向期望的状态。管理基础架构不是维护主机。您需要为资源创建抽象，用 API 来表示这些抽象，以及使用它们的应用程序。\n目标是满足您的应用程序的需求，并编写与您的业务流程和工作职能相当的软件应用程序。随着流程和功能的频繁变化，软件需要轻松适应。\n掌握它时，您将不断发现在创建新抽象，保持服务弹性以及推动可伸缩性极限方面的挑战。如果您能够找到新的限制和有用的抽象，请回馈给您所属的社区。\n开源并通过研究和社区回馈是这些模式是使得它们从先驱环境中出现。共享使更多的创新和 …","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"c27d901351fec64a80c3b924d415b618","permalink":"https://lib.jimmysong.io/cloud-native-infra/implementing-cloud-native-infrasctructure/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/cloud-native-infra/implementing-cloud-native-infrasctructure/","section":"cloud-native-infra","summary":"如果您认为云原生基础架构是可购买的产品或是从云供应商那购买的服务器，我们很抱歉让您失望了。如果不采用这些做法并改变您建设和维护基础架构的方式，您就不会受益。 它不仅仅影响服务器、网络和存储。它关乎的是工","tags":["云原生"],"title":"第 9 章：实施云原生基础架构","type":"book"},{"authors":null,"categories":["Envoy"],"content":"使用路由级别的请求镜像策略（request_mirroring_policies），我们可以配置 Envoy 将流量从一个集群镜像到另一个集群。\n流量镜像或请求镜像是指当传入的请求以一个集群为目标时，将其复制并发送给第二个集群。镜像的请求是 “发射并遗忘” 的，这意味着 Envoy 在发送主集群的响应之前不会等待影子集群的响应。\n请求镜像模式不会影响发送到主集群的流量，而且因为 Envoy 会收集影子集群的所有统计数据，所以这是一种有用的测试技术。\n除了 “发送并遗忘” 之外，还要确保你所镜像的请求是空闲的。否则，镜像请求会扰乱你的服务与之对话的后端。\n影子请求中的 authority/host 头信息将被添加 -shadow字符串。\n为了配置镜像策略，我们在要镜像流量的路由上使用 request_mirror_policies 字段。我们可以指定一个或多个镜像策略，以及我们想要镜像的流量的部分。\nroute_config:name:my_routevirtual_hosts:- name:httpbindomains:[\u0026#34;*\u0026#34;]routes:- match:prefix:/route:cluster:httpbinrequest_mirror_policies:cluster:mirror_httpbinruntime_fraction:default_value:numerator:100...上述配置将 100% 地接收发送到集群 httpbin 的传入请求，并将其镜像到mirror_httpbin。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"5b402661c10936d910a40644987df59a","permalink":"https://lib.jimmysong.io/envoy-handbook/hcm/request-mirroring/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/envoy-handbook/hcm/request-mirroring/","section":"envoy-handbook","summary":"使用路由级别的请求镜像策略（request_mirroring_policies），我们可以配置 Envoy 将流量从一个集群镜像到另一个集群。 流量镜像或请求镜像是指当传入的请求以一个集群为目标时，将其复制并发送","tags":["Envoy"],"title":"请求镜像","type":"book"},{"authors":null,"categories":["安全"],"content":"遵循本文件中概述的加固指南是确保在 Kubernetes 协调容器上运行的应用程序安全的一个步骤。然而，安全是一个持续的过程，跟上补丁、更新和升级是至关重要的。具体的软件组件因个人配置的不同而不同，但整个系统的每一块都应尽可能保持安全。这包括更新：Kubernetes、管理程序、虚拟化软件、插件、环境运行的操作系统、服务器上运行的应用程序，以及 Kubernetes 环境中托管的任何其他软件。\n互联网安全中心（CIS）发布了保护软件安全的基准。管理员应遵守 Kubernetes 和任何其他相关系统组件的 CIS 基准。管理员应定期检查，以确保其系统的安全性符合当前安全专家对最佳实践的共识。应定期对各种系统组件进行漏洞扫描和渗透测试，主动寻找不安全的配置和零日漏洞。任何发现都应在潜在的网络行为者发现和利用它们之前及时补救。\n随着更新的部署，管理员也应该跟上从环境中删除任何不再需要的旧组件。使用托管的 Kubernetes 服务可以帮助自动升级和修补 Kubernetes、操作系统和网络协议。然而，管理员仍然必须为他们的容器化应用程序打补丁和升级。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"5d33bbade4d5d8b49347e24dda4cfa79","permalink":"https://lib.jimmysong.io/kubernetes-hardening-guidance/upgrading-and-application-security-practices/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/kubernetes-hardening-guidance/upgrading-and-application-security-practices/","section":"kubernetes-hardening-guidance","summary":"遵循本文件中概述的加固指南是确保在 Kubernetes 协调容器上运行的应用程序安全的一个步骤。然而，安全是一个持续的过程，跟上补丁、更新和升级是至关重要的。具体的软件组件因个人配置的不同而不同，但整个系统的每一块都应尽","tags":["Kubernetes","安全"],"title":"升级和应用安全实践","type":"book"},{"authors":null,"categories":["可观测性"],"content":"路线图很快就会过时。有关最新的路线图，请参见 OpenTelemetry 状态页面。也就是说，以下是截至目前 OpenTelemetry 的状态。\n核心组件 目前，OpenTelemetry 追踪信号已被宣布为稳定的，并且在许多语言中都有稳定的实现。\n度量信号刚刚被宣布稳定，测试版的实施将在 2022 年第一季度广泛使用。\n日志信号预计将在 2022 年第一季度宣布稳定，测试版实现预计将在 2022 年第二季度广泛使用。2021 年，Stanza 项目被捐赠给 OpenTelemetry，为 OpenTelemetry 收集器增加了高效的日志处理能力。\n用于 HTTP/RPC、数据库和消息系统的语义公约预计将在 2022 年第一季度宣布稳定。\n未来 完成上述路线图就完成了对 OpenTelemetry 核心功能的稳定性要求。这是一个巨大的里程碑，因为它打开了进一步采用 OpenTelemetry 的大门，包括与数据库、管理服务和 OSS 库的原生集成。这些集成工作大部分已经以原型和测试版支持的形式在进行。随着 OpenTelemetry 的稳定性在 2022 年上半年完成，我预计在 2022 年下半年将出现支持 OpenTelemetry 的宣言，使 2022 年成为 “OpenTelemetry 之年”。\n但我们并没有就此止步。下一步是什么？\neBPF Extended Berkeley Packet Filter（eBPF）是一种在 Windows、Linux 和其他类似 Unix 的操作系统上提供底层网络访问的机制。利用 eBPF 将给 OpenTelemetry 提供一种极其有效的网络监控形式，不需要任何开发人员的工具。\n2021 年，Flowmill 项目被捐赠给了 OpenTelemetry。Flowmill 是一个基于 eBPF 的可观测性解决方案，专门设计用于观测分布式系统。Flowmill 的开发者与 Pixie 项目的开发者一起，正在努力为 OpenTelemetry Collector 增加 eBPF 支持。Pixie 是专门为 Kubernetes 设计的基于 eBPF 的观测工具，是 CNCF 旗下 OpenTelemetry 的一个姊妹项目。我们还在一起研究如何将底层（第 2 层）eBPF 数据与高层（第 7 层）分布式追踪数据进一步关联起来，这在业界尚属首次。\nRUM 真实用户监控（RUM）是一种可观测性工具，用于描述用户在长期运行的用户会话中如何与移动、网络和桌面客户端进行交互。RUM 与分布式追踪不同，因为图形用户界面（GUI）往往是基于反应器的系统，与数据库和网络服务器等基于事务的系统有根本的架构差异。长话短说：你不能把一个用户会话建模为一个追踪，然后就收工了。\nClient Instrumentation SIG 目前正在开发一个新的 RUM 设计，它将扩展并与 OpenTelemetry 现有的分布式追踪、指标和日志信号完全整合。\nOpenTelemetry 控制平面 目前，OpenTelemetry 收集器和 SDK 是作为独立的单元来管理的，必须重新启动才能改变它们的配置。目前，Agent Management SIG 正在开发一个控制平面，它可以报告这些组件的当前状态，并允许实时改变配置。它将允许运维人员或自动服务动态地控制整个遥测管道的处理。\n动态配置将允许对采样、日志水平和其他形式的资源管理进行细粒度、反应灵敏的控制，从而减少成本。最终，这个控制平面可以实现更先进的基于尾部的采样形式，即需要在整个部署中协调的采样技术。\n列式编码的 OTLP 目前的 OpenTelemetry 协议是对 OpenTelemetry 数据模型的一种有效而直接的编码。跨度、度量和日志被编码为跨度、度量和日志。这可以被认为是一个基于行的模型，每个跨度、度量和日志都被编码为元素列表中的一个离散元素。基于列的模型则将每个元素编码为一个单一的表格，其中所有元素共享相同的列，用于重叠的概念，如时间戳和属性。\n这种列式表示法有望优化数据批次的创建、大小和处理。这种方法的主要好处是：\n 更好的数据压缩率（一组相似的数据）。 更快的数据处理（更好的数据定位意味着更好地使用 CPU 缓存线）。 更快的序列化和反序列化（更少的对象需要处理）。 更快的批量创建（更少的内存分配）。 更好的 I/O 效率（传输的数据更少）。  这种方法的好处与批次的大小成比例地增加。使用现有的 “面向行” 的表示方法很适合小批量的情况。因此，列式编码将扩展当前的协议。目前提出的实施方案是基于 Apache Arrow，这是一种成熟的列式内存格式。\n这种优化的重点是允许高容量的数据源，如 CDN 和大型多租户系统，像常规服务一样参与可观测性。列式编码也将减少跨网络边界的遥测出口的成本。\n","date":1643990400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"603d251048616273c024bf6ba74c37ec","permalink":"https://lib.jimmysong.io/opentelemetry-obervability/roadmap/","publishdate":"2022-02-05T00:00:00+08:00","relpermalink":"/opentelemetry-obervability/roadmap/","section":"opentelemetry-obervability","summary":"附录 B：OpenTelemetry 项目路线图","tags":["OpenTelemetry"],"title":"附录 B：OpenTelemetry 项目路线图","type":"book"},{"authors":null,"categories":["安全"],"content":"下面的例子是一个 Dockerfile，它以非 root 用户和非 group 成员身份运行一个应用程序。\nFROMubuntu:latest# 升级和安装 make 工具RUN apt update \u0026amp;\u0026amp; apt install -y make# 从一个名为 code 的文件夹中复制源代码，并使用 make 工具构建应用程序。COPY ./codeRUN make /code# 创建一个新的用户（user1）和新的组（group1）；然后切换到该用户的上下文中。RUN useradd user1 \u0026amp;\u0026amp; groupadd group1USERuser1:group1# 设置容器的默认入口CMD /code/app","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"5f6c69d5c0dbe72fb20fea7ac4ffca37","permalink":"https://lib.jimmysong.io/kubernetes-hardening-guidance/appendix/a/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/kubernetes-hardening-guidance/appendix/a/","section":"kubernetes-hardening-guidance","summary":"下面的例子是一个 Dockerfile，它以非 root 用户和非 group 成员身份运行一个应用程序。 FROMubuntu:latest# 升级和安装 make 工具RUN apt update \u0026\u0026 apt install -y make# 从一个名为 code 的文件夹中复制源代码，并使用 make 工具构建应用程序。COPY ./codeRUN make /code# 创建一","tags":["Kubernetes","安全"],"title":"附录 A：非 root 应用的 Dockerfile 示例","type":"book"},{"authors":null,"categories":["云原生"],"content":"在云环境中运行时，应用程序需要具有弹性。网络通信方面特别容易出现故障。添加网络弹性的一种常见模式是创建一个导入到应用程序中的库，该库提供本附录中描述的网络弹性模式。但是，导入的库很难维护以多种语言编写的服务，且当新版本的网络库发布时，会增加应用程序测试和重新部署的负担。\n取代应用程序处理网络弹性逻辑的另一种方式是，可以将代理置于适当的位置，作为应用程序的保护和增强层。代理的优势在于避免应用程序需要额外的复杂代码，尽量减少开发人员的工作量。\n可以在连接层（物理或 SDN），应用程序或透明代理中处理网络弹性逻辑。虽然代理不是传统网络堆栈的一部分，但它们可用于透明地管理应用程序的网络弹性。\n透明代理可以在基础架构中的任何位置运行，但与应用程序的距离越近越有利。代理支持的协议还要尽可能全面，且可以代理的开放系统互连模型（OSI 模型）层。\n通过实施以下模式，代理在基础架构的弹性中扮演着积极的角色：\n 负载均衡 负载切分（Load shedding） 服务发现 重试和 deadline 断路  代理也可以用来为应用程序添加功能。包括：\n 安全和认证 路由（入口和出口） 洞察和监测  负载均衡 应用程序负载均衡的方式有很多，应始终将负载均衡器放在云原生应用程序之前的原因有：\nDigitalOcean 在 “5 个 DigitalOcean 负载均衡器使用案例” 中给出了一些很好的理由：\n 水平缩放 高可用性 应用程序部署 动态流量路由  协调透明代理（例如 Envoy 和 Linkerd）是负载均衡应用程序的一种方式。具有透明代理句柄负载均衡的一些好处是：\n 对所有端点的请求视图允许更好的负载均衡决策。 基于软件的负载均衡器可灵活选择正确的方式来平衡负载。  透明代理不必盲目地将流量传递给下一个路由器。正如 Bunyan 在其博客文章 “超越循环：负载均衡延迟” 中指出的，他们可以集中协调以对基础架构有更广泛的了解。这使得负载均衡能够从全局优化流量路由，而不是仅为本地优化快速分组切换。\n随着对端点中哪些服务正在发送和接收请求有了更深的了解，代理可以更合理地向发送流量。\n负载切分 “站点可靠性工程” 手册解释了负载切分（Load shedding）与负载均衡不同。尽管负载均衡试图找到正确的后端来发送流量，但是如果应用程序无法接受请求，负载剔除会有意地丢弃流量。\n通过删除负载来保护应用程序实例，可以确保应用程序不会重新启动或被迫进入不利条件。删除请求比等待超时并要求重新启动应用程序要快得多。\n当发生中断或流量过多时，减载可以帮助保护应用程序实例。一切正常时，应用程序应该通过服务发现发现其他相关服务。\n服务发现 服务发现通常由运行服务的编排系统处理。透明代理可以绑定到相同的数据并提供附加功能。\n代理可以通过将多个源绑定在一起（例如，DNS 和键值数据库）并将它们呈现在统一接口中来增强标准服务发现。这允许实现者在不重写所有应用程序代码的情况下改变其后端。如果在应用程序之外处理服务发现，则可以更改服务发现工具而不必重写任何应用程序代码。\n由于代理可以对基础架构中的请求提供更全面的视图，因此可以决定端点何时健康与否。这与其他功能配合使用，例如负载均衡和重试，以将流量路由到最佳端点。\n当允许服务彼此发现时，代理服务器还可以考虑额外的元数据。它们可以实现逻辑，如节点延迟或 “距离”，以确保为请求发现正确的服务。\n重试和 deadline 通常，应用程序会使用内置逻辑来知道如何处理对外部服务失败的请求。这也可以由代理无需额外的应用程序代码来处理。\n代理拦截应用程序的所有入口和出口流量并路由请求。如果传出请求失败，代理可以自动重试，而无需涉及应用程序。如果请求因任何其他原因返回，则代理可以根据其配置中的规则进行适当处理。\n这很好，只要应用程序对延迟有弹性。否则，代理应根据申请截止日期返回失败通知。\n截止日期允许应用程序指定允许请求的时间长度。由于代理可以 “追踪” 到目的地和返回的请求，因此它可以在使用代理的所有应用程序中强制执行最终期限策略。\n当超过 deadline 时，失败将返回给应用程序，并且可以决定适当的操作。选项可能会降级服务，但应用程序也可能选择将错误发回给用户。\n断路 该模式根据家用的断路器命名。当一切正常时，电路默认为 “关闭” 状态，允许流量流过断路器。当检测到故障时，电路 “打开” 并切断流量。\n 重试模式使应用程序能够重试操作，以期成功。断路器模式阻止应用程序执行可能失败的操作。\n——Alex Homer，云设计模式：云应用程序指令性架构指南\n 断开的电路可以是单个端点或整个服务。打开后，不会发送任何流量，所有发送流量的尝试都将立即返回失败。\n与家用电路不同，即使处于打开状态，代理也可以测试失败的端点。当检测到故障后再次可用时，可将损坏的端点置于 “半开” 状态。此状态将发送少量流量，直到端点被标记为失败或健康。\n这种模式可以使得应用程序快速失败，并且只能发送到健康端点，从而使应用程序更快。通过不断检查端点，网络可以自行修复并智能地路由流量。\n除了这些弹性功能外，代理还可以通过以下方式增强应用程序。\nTLS 和身份验证 代理可以终止传输层安全性（TLS）或代理支持的任何其他安全性。这使得安全逻辑能够集中管理，而不是在每个应用程序中重新实现。然后可以在整个基础架构中更新安全协议或证书，而无需重新部署应用程序。\n身份验证也是如此。但是，授权仍应由应用程序管理，因为它通常是更细粒度的应用程序特定功能。在应用程序监督它们之前，用户会话 cookie 可以由代理验证。这意味着只有通过认证的流量才会被应用程序看到。\n这不仅可以节省应用程序的时间，还可以防止某些类型的滥用导致的停机。\n路由（入口和出口） 当代理在所有应用程序之前运行时，它们控制流入和流出应用程序的流量。它们还可以管理流入和流出集群的流量。\n正如反向代理可以用于在 N 层体系结构中将流量路由到后端一样，服务代理也可能暴露于集群外部，用来路由到达的请求。这里的代理知道流量的另一个数据点来自何处和目的地。\n具有对所有服务间通信的深入了解的反向代理，可以比传统的反向代理更好地了解路由选择。\n洞察和监控 利用所有关于基础架构内流量流的知识，代理系统可以公开关于单个端点和整个集群范围内的流量视图的指标。这些数据点传统上已经在专有网络系统或难以自动化的协议中暴露出来（例如， SNMP）。\n由于代理可以立即知道何时端点无法访问，所以它们也最先了解端点何时不健康。虽然编排系统也可以检查应用程序运行状况，但应用程序可能不知道向编排工具报告不健康的状态。有了对同一服务所有端点的了解后，代理也可以成为监控服务运行状况的最佳位置。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"920af67b0ede8d5afb5316d39ee8136b","permalink":"https://lib.jimmysong.io/cloud-native-infra/appendix-a-patterns-for-network-resiilency/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/cloud-native-infra/appendix-a-patterns-for-network-resiilency/","section":"cloud-native-infra","summary":"在云环境中运行时，应用程序需要具有弹性。网络通信方面特别容易出现故障。添加网络弹性的一种常见模式是创建一个导入到应用程序中的库，该库提供本附录中描述的网络弹性模式。但是，导入的库很难维护以多种语言编写","tags":["云原生"],"title":"附录 A：网络弹性模式","type":"book"},{"authors":null,"categories":["Envoy"],"content":"速率限制是一种限制传入请求的策略。它规定了一个主机或客户端在一个特定的时间范围内发送多少次请求。一旦达到限制，例如每秒 100 个请求，我们就说发送请求的客户端受到速率限制。任何速率受限的请求都会被拒绝，并且永远不会到达上游服务。稍后，我们还将讨论可以使用的断路器，以及速率限制如何限制上游的负载并防止级联故障。\nEnvoy 支持全局（分布式）和局部（非分布式）的速率限制。\n全局和局部速率限制的区别在于，我们要用全局速率限制来控制对一组在多个 Envoy 实例之间共享的上游的访问。例如，我们想对一个叫做多个 Envoy 代理的数据库的访问进行速率限制。另一方面，局部速率限制适用于每一个 Envoy 实例。\n   全局和局部速率限制  局部和全局速率限制可以一起使用，Envoy 分两个阶段应用它们。首先，应用局部速率限制，然后是全局速率限制。\n我们将在接下来的章节中深入研究全局和局部速率限制，并解释这两种情况如何工作。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"a16fc821756404b66812c604848a3c32","permalink":"https://lib.jimmysong.io/envoy-handbook/hcm/rate-limiting-intro/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/envoy-handbook/hcm/rate-limiting-intro/","section":"envoy-handbook","summary":"速率限制是一种限制传入请求的策略。它规定了一个主机或客户端在一个特定的时间范围内发送多少次请求。一旦达到限制，例如每秒 100 个请求，我们就说发送请求的客户端受到速率限制。任何速率受限的请求都会被拒绝，并且","tags":["Envoy"],"title":"速率限制","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"本文将为您讲解 Pod 的基础概念。\n理解 Pod Pod 是 kubernetes 中你可以创建和部署的最小也是最简的单位。Pod 代表着集群中运行的进程。\nPod 中封装着应用的容器（有的情况下是好几个容器），存储、独立的网络 IP，管理容器如何运行的策略选项。Pod 代表着部署的一个单位：kubernetes 中应用的一个实例，可能由一个或者多个容器组合在一起共享资源。\n Docker 是 kubernetes 中最常用的容器运行时，但是 Pod 也支持其他容器运行时。\n 在 Kubernetes 集群中 Pod 有如下两种使用方式：\n 一个 Pod 中运行一个容器。“每个 Pod 中一个容器” 的模式是最常见的用法；在这种使用方式中，你可以把 Pod 想象成是单个容器的封装，kuberentes 管理的是 Pod 而不是直接管理容器。 在一个 Pod 中同时运行多个容器。一个 Pod 中也可以同时封装几个需要紧密耦合互相协作的容器，它们之间共享资源。这些在同一个 Pod 中的容器可以互相协作成为一个 service 单位 —— 一个容器共享文件，另一个 “sidecar” 容器来更新这些文件。Pod 将这些容器的存储资源作为一个实体来管理。  Kubernetes Blog 有关于 Pod 用例的详细信息，查看：\n The Distributed System Toolkit: Patterns for Composite Containers Container Design Patterns  每个 Pod 都是应用的一个实例。如果你想平行扩展应用的话（运行多个实例），你应该运行多个 Pod，每个 Pod 都是一个应用实例。在 Kubernetes 中，这通常被称为 replication。\nPod 中如何管理多个容器 Pod 中可以同时运行多个进程（作为容器运行）协同工作。同一个 Pod 中的容器会自动的分配到同一个 node 上。同一个 Pod 中的容器共享资源、网络环境和依赖，它们总是被同时调度。\n注意在一个 Pod 中同时运行多个容器是一种比较高级的用法。只有当你的容器需要紧密配合协作的时候才考虑用这种模式。例如，你有一个容器作为 web 服务器运行，需要用到共享的 volume，有另一个 “sidecar” 容器来从远端获取资源更新这些文件，如下图所示：\n   Pod 示意图  Pod 中可以共享两种资源：网络和存储。\n网络 每个 Pod 都会被分配一个唯一的 IP 地址。Pod 中的所有容器共享网络空间，包括 IP 地址和端口。Pod 内部的容器可以使用 localhost 互相通信。Pod 中的容器与外界通信时，必须分配共享网络资源（例如使用宿主机的端口映射）。\n存储 可以为一个 Pod 指定多个共享的 Volume。Pod 中的所有容器都可以访问共享的 volume。Volume 也可以用来持久化 Pod 中的存储资源，以防容器重启后文件丢失。\n使用 Pod 你很少会直接在 kubernetes 中创建单个 Pod。因为 Pod 的生命周期是短暂的，用后即焚的实体。当 Pod 被创建后（不论是由你直接创建还是被其他 Controller），都会被 Kubernetes 调度到集群的 Node 上。直到 Pod 的进程终止、被删掉、因为缺少资源而被驱逐、或者 Node 故障之前这个 Pod 都会一直保持在那个 Node 上。\n 注意：重启 Pod 中的容器跟重启 Pod 不是一回事。Pod 只提供容器的运行环境并保持容器的运行状态，重启容器不会造成 Pod 重启。\n Pod 不会自愈。如果 Pod 运行的 Node 故障，或者是调度器本身故障，这个 Pod 就会被删除。同样的，如果 Pod 所在 Node 缺少资源或者 Pod 处于维护状态，Pod 也会被驱逐。Kubernetes 使用更高级的称为 Controller 的抽象层，来管理 Pod 实例。虽然可以直接使用 Pod，但是在 Kubernetes 中通常是使用 Controller 来管理 Pod 的。\nPod 和 Controller Controller 可以创建和管理多个 Pod，提供副本管理、滚动升级和集群级别的自愈能力。例如，如果一个 Node 故障，Controller 就能自动将该节点上的 Pod 调度到其他健康的 Node 上。\n包含一个或者多个 Pod 的 Controller 示例：\n Deployment StatefulSet DaemonSet  通常，Controller 会用你提供的 Pod Template 来创建相应的 Pod。\nPod Templates Pod 模版是包含了其他 object 的 Pod 定义，例如 Replication Controllers，Jobs 和 DaemonSets。Controller 根据 Pod 模板来创建实际的 Pod。\n","date":1489075200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"9fc9d8d39a8f691cda03193941c0c493","permalink":"https://lib.jimmysong.io/kubernetes-handbook/objects/pod-overview/","publishdate":"2017-03-10T00:00:00+08:00","relpermalink":"/kubernetes-handbook/objects/pod-overview/","section":"kubernetes-handbook","summary":"本文将为您讲解 Pod 的基础概念。 理解 Pod Pod 是 kubernetes 中你可以创建和部署的最小也是最简的单位。Pod 代表着集群中运行的进程。 Pod 中封装着应用的容器（有的情况下是好几个容器），存储、独立的网络 IP，管理容器如何运行的策","tags":["Kubernetes"],"title":"Pod 概览","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"Pod 是 Kubernetes 中可以创建的最小部署单元，也是 Kubernetes REST API 中的顶级资源类型。\n在 Kuberentes V1 core API 版本中的 Pod 的数据结构如下图所示：\n   Pod Cheatsheet  什么是 Pod？ Pod 就像是豌豆荚一样，它由一个或者多个容器组成（例如 Docker 容器），它们共享容器存储、网络和容器运行配置项。Pod 中的容器总是被同时调度，有共同的运行环境。你可以把单个 Pod 想象成是运行独立应用的 “逻辑主机”—— 其中运行着一个或者多个紧密耦合的应用容器 —— 在有容器之前，这些应用都是运行在几个相同的物理机或者虚拟机上。\n尽管 kubernetes 支持多种容器运行时，但是 Docker 依然是最常用的运行时环境，我们可以使用 Docker 的术语和规则来定义 Pod。\nPod 中共享的环境包括 Linux 的 namespace、cgroup 和其他可能的隔绝环境，这一点跟 Docker 容器一致。在 Pod 的环境中，每个容器中可能还有更小的子隔离环境。\nPod 中的容器共享 IP 地址和端口号，它们之间可以通过 localhost 互相发现。它们之间可以通过进程间通信，例如 SystemV 信号或者 POSIX 共享内存。不同 Pod 之间的容器具有不同的 IP 地址，不能直接通过 IPC 通信。\nPod 中的容器也有访问共享 volume 的权限，这些 volume 会被定义成 pod 的一部分并挂载到应用容器的文件系统中。\n根据 Docker 的结构，Pod 中的容器共享 namespace 和 volume，不支持共享 PID 的 namespace。\n就像每个应用容器，pod 被认为是临时（非持久的）实体。在 Pod 的生命周期中讨论过，pod 被创建后，被分配一个唯一的 ID（UID），调度到节点上，并一致维持期望的状态直到被终结（根据重启策略）或者被删除。如果 node 死掉了，分配到了这个 node 上的 pod，在经过一个超时时间后会被重新调度到其他 node 节点上。一个给定的 pod（如 UID 定义的）不会被 “重新调度” 到新的节点上，而是被一个同样的 pod 取代，如果期望的话甚至可以是相同的名字，但是会有一个新的 UID。\nVolume 跟 pod 有相同的生命周期（当其 UID 存在的时候）。当 Pod 因为某种原因被删除或者被新创建的相同的 Pod 取代，它相关的东西（例如 volume）也会被销毁和再创建一个新的 volume。\n   Pod 示意图  说明：一个多容器 Pod，包含文件提取程序和 Web 服务器，该服务器使用持久卷在容器之间共享存储。\nPod 的动机 管理 Pod 是一个服务的多个进程的聚合单位，pod 提供这种模型能够简化应用部署管理，通过提供一个更高级别的抽象的方式。Pod 作为一个独立的部署单位，支持横向扩展和复制。共生（协同调度），命运共同体（例如被终结），协同复制，资源共享，依赖管理，Pod 都会自动的为容器处理这些问题。\n资源共享和通信 Pod 中的应用可以共享网络空间（IP 地址和端口），因此可以通过 localhost 互相发现。因此，pod 中的应用必须协调端口占用。每个 pod 都有一个唯一的 IP 地址，跟物理机和其他 pod 都处于一个扁平的网络空间中，它们之间可以直接连通。\nPod 中应用容器的 hostname 被设置成 Pod 的名字。\nPod 中的应用容器可以共享 volume。Volume 能够保证 pod 重启时使用的数据不丢失。\nPod 的使用 Pod 也可以用于垂直应用栈（例如 LAMP），这样使用的主要动机是为了支持共同调度和协调管理应用程序，例如：\n 内容管理系统、文件和数据加载器、本地换群管理器等。 日志和检查点备份、压缩、旋转、快照等。 数据变更观察者、日志和监控适配器、活动发布者等。 代理、桥接和适配器等。 控制器、管理器、配置器、更新器等。  通常单个 pod 中不会同时运行一个应用的多个实例。\n详细说明请看： The Distributed System ToolKit: Patterns for Composite Containers.\n其他替代选择 为什么不直接在一个容器中运行多个应用程序呢？\n 透明。让 Pod 中的容器对基础设施可见，以便基础设施能够为这些容器提供服务，例如进程管理和资源监控。这可以为用户带来极大的便利。 解耦软件依赖。每个容器都可以进行版本管理，独立的编译和发布。未来 kubernetes 甚至可能支持单个容器的在线升级。 使用方便。用户不必运行自己的进程管理器，还要担心错误信号传播等。 效率。因为由基础架构提供更多的职责，所以容器可以变得更加轻量级。  为什么不支持容器的亲和性的协同调度？\n这种方法可以提供容器的协同定位，能够根据容器的亲和性进行调度，但是无法实现使用 pod 带来的大部分好处，例如资源共享，IPC，保持状态一致性和简化管理等。\nPod 的持久性（或者说缺乏持久性） Pod 在设计支持就不是作为持久化实体的。在调度失败、节点故障、缺少资源或者节点维护的状态下都会死掉会被驱逐。\n通常，用户不需要手动直接创建 Pod，而是应该使用 controller（例如 Deployments），即使是在创建单个 Pod 的情况下。Controller 可以提供集群级别的自愈功能、复制和升级管理。\n使用集合 API 作为主要的面向用户的原语在集群调度系统中相对常见，包括 Borg、Marathon、Aurora 和 Tupperware。\nPod 原语有利于：\n 调度程序和控制器可插拔性 支持 pod 级操作，无需通过控制器 API “代理” 它们 将 pod 生命周期与控制器生命周期分离，例如用于自举（bootstrap） 控制器和服务的分离 —— 端点控制器只是监视 pod 将集群级功能与 Kubelet 级功能的清晰组合 ——Kubelet 实际上是 “pod 控制器” 高可用性应用程序，它们可以在终止之前及在删除之前更换 pod，例如在计划驱逐、镜像预拉取或实时 pod 迁移的情况下#3949  StatefulSet 控制器支持有状态的 Pod。在 1.4 版本中被称为 PetSet。在 kubernetes 之前的版本中创建有状态 pod 的最佳方式是创建一个 replica 为 1 的 replication controller。\nPod 的终止 因为 Pod 作为在集群的节点上运行的进程，所以在不再需要的时候能够优雅的终止掉是十分必要的（比起使用发送 KILL 信号这种暴力的方式）。用户需要能够发起一个删除 Pod 的请求，并且知道它们何时会被终止，是否被正确的删除。用户想终止程序时发送删除 pod 的请求，在 pod 可以被强制删除前会有一个宽限期，会发送一个 TERM 请求到每个容器的主进程。一旦超时，将向主进程发送 KILL 信号并从 API server 中删除。如果 kubelet 或者 container manager 在等待进程终止的过程中重启，在重启后仍然会重试完整的宽限期。\n示例流程如下：\n 用户发送删除 pod 的命令，默认宽限期是 30 秒； 在 Pod 超过该宽限期后 API server 就会更新 Pod 的状态为 “dead”； 在客户端命令行上显示的 Pod 状态为 “terminating”； 跟第三步同时，当 kubelet 发现 pod 被标记为 “terminating” 状态时，开始停止 pod 进程：  如果在 pod 中定义了 preStop hook，在停止 pod 前会被调用。如果在宽限期过后，preStop hook 依然在运行，第二步会再增加 2 秒的宽限期； 向 Pod 中的进程发送 TERM 信号；   跟第三步同时，该 Pod 将从该 service 的端点列表中删除，不再是 replication controller 的一部分。关闭的慢的 pod 将继续处理 load balancer 转发的流量； 过了宽限期后，将向 Pod 中依然运行的进程发送 SIGKILL 信号而杀掉进程。 Kubelet 会在 API server 中完成 Pod 的的删除，通过将优雅周期设置为 0（立即删除）。Pod 在 API 中消失，并且在客户端也不可见。  删除宽限期默认是 30 秒。 kubectl delete 命令支持 —grace-period=\u0026lt;seconds\u0026gt; 选项，允许用户设置自己的宽限期。如果设置为 0 将强制删除 pod。在 kubectl\u0026gt;=1.5 版本的命令中，你必须同时使用 --force 和 --grace-period=0 来强制删除 pod。 在 yaml 文件中可以通过 {{ .spec.spec.terminationGracePeriodSeconds }} 来修改此值。\n强制删除 Pod Pod 的强制删除是通过在集群和 etcd 中将其定义为删除状态。当执行强制删除命令时，API server 不会等待该 pod 所运行在节点上的 kubelet 确认，就会立即将该 pod 从 API server 中移除，这时就可以创建跟原 pod 同名的 pod 了。这时，在节点上的 pod 会被立即设置为 terminating 状态，不过在被强制删除之前依然有一小段优雅删除周期。\n强制删除对于某些 pod 具有潜在危险性，请谨慎使用。使用 StatefulSet pod 的情况下，请参考删除 StatefulSet 中的 pod 文章。\nPod 中容器的特权模式 从 Kubernetes1.1 版本开始，pod 中的容器就可以开启 privileged 模式，在容器定义文件的 SecurityContext 下使用 privileged flag。 这在使用 Linux 的网络操作和访问设备的能力时是很有用的。容器内进程可获得近乎等同于容器外进程的权限。在不需要修改和重新编译 kubelet 的情况下就可以使用 pod 来开发节点的网络和存储插件。\n如果 master 节点运行的是 kuberentes1.1 或更高版本，而 node 节点的版本低于 1.1 版本，则 API server 将也可以接受新的特权模式的 pod，但是无法启动，pod 将处于 pending 状态。\n执行 kubectl describe pod FooPodName，可以看到为什么 pod 处于 pending 状态。输出的 event 列表中将显示： Error validating pod \u0026#34;FooPodName\u0026#34;.\u0026#34;FooPodNamespace\u0026#34; from api, ignoring: spec.containers[0].securityContext.privileged: forbidden \u0026#39;\u0026lt;*\u0026gt;(0xc2089d3248)true\u0026#39;\n如果 master 节点的版本低于 1.1，无法创建特权模式的 pod。如果你仍然试图去创建的话，你得到如下错误：\nThe Pod \u0026#34;FooPodName\u0026#34; is invalid. spec.containers[0].securityContext.privileged: forbidden \u0026#39;\u0026lt;*\u0026gt;(0xc20b222db0)true\u0026#39; ","date":1653062400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"765e29d1c1d00d9ee0a9baa7e5da9ddb","permalink":"https://lib.jimmysong.io/kubernetes-handbook/objects/pod/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/objects/pod/","section":"kubernetes-handbook","summary":"Pod 是 Kubernetes 中可以创建的最小部署单元，也是 Kubernetes REST API 中的顶级资源类型。 在 Kuberentes V1 core API 版本中的 Pod 的数据结构如下图所示： Pod Cheatsheet 什么是 Pod？ Pod 就像是豌豆荚一样，它由一个或者多个容器组成（例如 Docker 容器），它们共享容器存储、网","tags":["Kubernetes"],"title":"Pod 解析","type":"book"},{"authors":null,"categories":["云原生"],"content":"关于使用云提供商和避免供应商锁定存在很多争议。这种辩论充满了意识心态之争。\n锁定通常是工程师和管理层关心的问题。应该将其与选择编程语言或框架一样作为应用程序的风险来权衡。编程语言和云提供商的选择是锁定的形式，工程师有责任了解风险并评估这些风险是否可以接受。\n当您选择供应商或技术时，请记住以下几点：\n 锁定是不可避免的。 锁定是一种风险，但并不总是很高。 不要外包思维。  锁定是不可避免的 在技术上有两种类型的锁定：\n技术锁定\n整个开发技术栈中底层技术\n供应商锁定\n大多数情况下，作为项目的一部分而使用的服务和软件（供应商锁定还可能包括硬件和操作系统，但我们只关注服务）\n技术锁定 开发人员将选择他们熟悉的技术或为正在开发的应用程序提供最大利益的技术。这些技术可以是供应商提供的技术（例如.NET 和 Oracle 数据库）到开源软件（例如 Python 和 PostgreSQL）。\n在此级别提供的锁定通常要求符合 API 或规范，这将影响应用程序的开发。也可以选择一些替代技术，但是这通常有很高的转换成本，因为技术对应用程序的设计有很大影响。\n供应商锁定 供应商，如云提供商，是另一种不同形式的锁定。在这种情况下，您正在消费供应商的资源。这可以是基础架构资源（例如，计算和存储），或者是托管软件（例如，Gmail）。\n消费资源的堆栈越高，应该从消耗的资源（例如 Heroku）中获得的价值就越高。高层次资源是从底层资源中抽离出来的，能产品的生产速度更快。\n锁定是一种风险 技术锁定通常是一次性决定或与供应商达成使用该技术的协议。如果您不再与供应商达成支持协议，则您的软件不会立即中断 —— 只是变得自我支持。\n开源软件可以在一定程度上减少来自技术的锁定，但并不能完全消除。使用开放标准可以进一步减少锁定，但了解开放标准与开放源代码之间的差异很重要。\n仅仅因为别人编写代码并不能使其成为标准。同样，专有系统可以形成非官方标准，允许从它们迁移出去（如 AWS S3）。\n供应商锁定的原因通常不仅仅是技术锁定，而是因为供应商锁定的风险高于技术风险。如果您不向供应商不支付费用，您的申请将停止运行；你不再能够访问你所支付的资源。\n如前所述，供应商服务提供更多价值，因为它们允许产品开发不需要所有较低级别的实现。不要避免托管服务来消除风险；你应该像对待其他任何事情一样权衡服务的风险和回报。\n如果服务提供标准接口，则风险非常低。接口越是自定义，或者产品越独特，切换的风险就越高。\n不要外包思维 本书的目标之一就是帮助您自己做出决定。在不了解建议的背景和是否适用于您的情况下，不要盲目听从其他人的意见或报告。\n如果您可以通过使用托管云服务更快地交付产品，则应该选择一个供应商开始使用。虽然衡量风险是好的，但将大把的时间花费在具有类似解决方案的多家供应商的争论上，又自己构建服务并不能节约您的时间。\n如果多个供应商提供类似的服务，请选择最容易采用的服务。开始使用该服务后，限制将很快显现。选择供应商时最重要的因素是选择一个与您具有相同创新步伐的供应商。\n如果供应商的创新速度比您快，那么您将无法利用其最新技术，还可能不得不花大量时间迁移旧技术。如果供应商的创新过于缓慢，那么您将不得不根据供应商提供的内容构建自己的抽象，而且您不会专注于您的业务目标。\n为了保持竞争力，您可能需要消费尚未拥有标准或替代品的资源（例如，新的和实验性的服务）。不要因为它们会使你陷入这项服务而害怕。重视保持竞争力或失去市场份额的风险，您的竞争对手可能会更快地创新。\n了解您无法避免的风险以及您的业务有多大风险。做可以最大化回报和将风险降至最低的决策。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"dce38ffe891bc5e223f37b8bc4bfded2","permalink":"https://lib.jimmysong.io/cloud-native-infra/appendix-b-lock-in/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/cloud-native-infra/appendix-b-lock-in/","section":"cloud-native-infra","summary":"关于使用云提供商和避免供应商锁定存在很多争议。这种辩论充满了意识心态之争。 锁定通常是工程师和管理层关心的问题。应该将其与选择编程语言或框架一样作为应用程序的风险来权衡。编程语言和云提供商的选择是锁定的","tags":["云原生"],"title":"附录 B：锁定","type":"book"},{"authors":null,"categories":["安全"],"content":"下面是一个使用只读根文件系统的 Kubernetes 部署模板的例子。\napiVersion:apps/v1kind:Deploymentmetadata:labels:app:webname:webspec:selector:matchLabels:app:webtemplate:metadata:labels:app:webname:webspec:containers:- command:[\u0026#34;sleep\u0026#34;]args:[\u0026#34;999\u0026#34;]image:ubuntu:latestname:websecurityContext:readOnlyRootFilesystem:true#使容器的文件系统成为只读volumeMounts:- mountPath:/writeable/location/here#创建一个可写卷name:volNamevolumes:- emptyDir:{}name:volName","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"d80d3219952633092d6ce3257dae19d0","permalink":"https://lib.jimmysong.io/kubernetes-hardening-guidance/appendix/b/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/kubernetes-hardening-guidance/appendix/b/","section":"kubernetes-hardening-guidance","summary":"下面是一个使用只读根文件系统的 Kubernetes 部署模板的例子。 apiVersion:apps/v1kind:Deploymentmetadata:labels:app:webname:webspec:selector:matchLabels:app:webtemplate:metadata:labels:app:webname:webspec:containers:- command:[\"sleep\"]args:[\"999\"]image:ubuntu:latestname:websec","tags":["Kubernetes","安全"],"title":"附录 B：只读文件系统的部署模板示例","type":"book"},{"authors":null,"categories":["Envoy"],"content":"局部速率限制过滤器对过滤器链处理的传入连接应用一个令牌桶速率限制。\n令牌桶算法的基础是令牌在桶中的类比。桶里的令牌以一个固定的速度被重新填满。每次收到一个请求或连接时，我们都会检查桶里是否还有令牌。如果有，就从桶中取出一个令牌，然后处理该请求。如果没有剩余的令牌，该请求就会被放弃（即速率限制）。\n   令牌桶算法  局部速率限制可以在监听器层面或虚拟主机或路由层面进行全局配置，就像全局速率限制一样。我们还可以在同一配置中结合全局和局部速率限制。\ntoken_bucket 指定了过滤器处理的请求所使用的配置。它包括桶可以容纳的最大令牌数量（max_tokens），每次填充的令牌数量（tokens_per_fill）以及填充间隔（fill_interval）。\n下面是一个最多可以容纳 5000 个令牌的桶的配置实例。每隔 30 秒，向桶中添加 100 个令牌。桶中的令牌容量永远不会超过 5000。\ntoken_bucket:max_tokens:5000tokens_per_fill:100fill_interval:30s为了控制令牌桶是在所有 worker 之间共享（即每个 Envoy 进程）还是按连接使用，我们可以设置 local_rate_limit_per_downstream_connection 字段。默认值是 false，这意味着速率限制被应用于每个 Envoy 进程。\n控制是否启用或强制执行某一部分请求的速率限制的两个设置被称为 filter_enabled 和 filter_enforced。这两个值在默认情况下都设置为 0%。\n速率限制可以被启用，但不一定对一部分请求强制执行。例如，我们可以对 50% 的请求启用速率限制。然后，在这 50% 的请求中，我们可以强制执行速率限制。\n   启用速率限制  以下配置对所有传入的请求启用并执行速率限制。\ntoken_bucket:max_tokens:5000tokens_per_fill:100fill_interval:30sfilter_enabled:default_value:numerator:100denominator:HUNDREDfilter_enforced:default_value:numerator:100denominator:HUNDRED我们还可以为限制速率的请求添加请求和响应头信息。我们可以在 request_headers_to_add_when_not_enforced 字段中提供一个头信息列表，Envoy 将为每个转发到上游的限速请求添加一个请求头信息。请注意，这只会在过滤器启用但未强制执行时发生。\n对于响应头信息，我们可以使用 response_headers_to_add  字段。我们可以提供一个 Header 的列表，这些 Header 将被添加到已被限制速率的请求的响应中。这只有在过滤器被启用或完全强制执行时才会发生。\n如果我们在前面的例子的基础上，这里有一个例子，说明如何在所有速率限制的请求中添加特定的响应头。\ntoken_bucket:max_tokens:5000tokens_per_fill:100fill_interval:30sfilter_enabled:default_value:numerator:100denominator:HUNDREDfilter_enforced:default_value:numerator:100denominator:HUNDREDresponse_headers_to_add:- append:falseheader:key:x-local-rate-limitvalue:\u0026#39;true\u0026#39;我们可以配置局部速率限制器，使所有虚拟主机和路由共享相同的令牌桶。为了在全局范围内启用局部速率限制过滤器（不要与全局速率限制过滤器混淆），我们可以在 http_filters 列表中为其提供配置。\n例如：\n...http_filters:- name:envoy.filters.http.local_ratelimittyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.http.local_ratelimit.v3.LocalRateLimitstat_prefix:http_local_rate_limitertoken_bucket:max_tokens:10000...- name:envoy.filters.http.router...如果我们想启用每个路由的局部速率限制，我们仍然需要将过滤器添加到 http_filters 列表中，而不需要任何配置。然后，在路由配置中，我们可以使用 typed_per_filter_config 并指定局部速率限制的过滤器配置。\n例如：\n...route_config:name:my_routevirtual_hosts:- name:my_servicedomains:[\u0026#34;*\u0026#34;]routes:- match:prefix:/route:cluster:some_clustertyped_per_filter_config:envoy.filters.http.local_ratelimit:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.http.local_ratelimit.v3.LocalRateLimittoken_bucket:max_tokens:10000tokens_per_fill:1000fill_interval:1sfilter_enabled:default_value:numerator:100denominator:HUNDREDfilter_enforced:default_value:numerator:100denominator:HUNDREDhttp_filters:- name:envoy.filters.http.local_ratelimittyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.http.local_ratelimit.v3.LocalRateLimitstat_prefix:http_local_rate_limiter- name:envoy.filters.http.router上述配置在 http_filter 列表中加载了局部速率限制过滤器。我们在路由配置中使用 typed_per_filter_config 来配置它，并以 envoy.filters.http.local_ratelimit 的名字来引用这个过滤器。\n使用描述符进行局部速率限制 就像我们在做全局速率限制时使用描述符一样，我们也可以把它们用于局部每条路由的速率限制。我们需要配置两个部分：路由上的操作和局部速率限制过滤器配置中的描述符列表。\n我们可以用为全局速率限制定义动作的方式为局部速率限制定义动作。\n...route_config:name:my_routevirtual_hosts:- name:my_servicedomains:[\u0026#34;*\u0026#34;]routes:- match:prefix:/route:cluster:some_clusterrate_limits:- actions:- header_value_match:descriptor_value:post_requestheaders:- name:\u0026#34;:method\u0026#34;exact_match:POST- header_value_match:descriptor_value:get_requestheaders:- name:\u0026#34;:method\u0026#34;exact_match:GET...第二部分是编写配置以匹配生成的描述符，并提供令牌桶信息。这在 `descriptors 字段下的速率限制过滤器配置中得到完成。\n例如：\ntyped_per_filter_config:envoy.filters.http.local_ratelimit:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.http.local_ratelimit.v3.LocalRateLimitstat_prefix:some_stat_prefixtoken_bucket:max_tokens:1000tokens_per_fill:1000fill_interval:60sfilter_enabled:...filter_enforced:...descriptors:- entries:- key:header_matchvalue:post_requesttoken_bucket:max_tokens:20tokens_per_fill:5fill_interval:30s- entries:- key:header_matchvalue:get_requesttoken_bucket:max_tokens:50tokens_per_fill:5fill_interval:20s...对于所有的 POST 请求（即 （\u0026#34;header_match\u0026#34;: \u0026#34;post_request\u0026#34;）），桶被设置为 20 个令牌，它每 30 秒重新填充 5 个令牌。对于所有的 GET 请求，该桶最多可以容纳 50 个令牌，每 20 秒重新填充 5 个令牌。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"f23ee906424b310ce5cf30c9f242cdec","permalink":"https://lib.jimmysong.io/envoy-handbook/hcm/local-rate-limiting/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/envoy-handbook/hcm/local-rate-limiting/","section":"envoy-handbook","summary":"局部速率限制过滤器对过滤器链处理的传入连接应用一个令牌桶速率限制。 令牌桶算法的基础是令牌在桶中的类比。桶里的令牌以一个固定的速度被重新填满。每次收到一个请求或连接时，我们都会检查桶里是否还有令牌。如果","tags":["Envoy"],"title":"局部速率限制","type":"book"},{"authors":null,"categories":["Envoy"],"content":"当许多主机向少数上游服务器发送请求，且平均延迟较低时，全局或分布式速率限制很有用。\n   许多主机上少许上游服务器发送请求  由于服务器不能快速处理这些请求，请求就会变得滞后。在这种情况下，众多的下游主机可以压倒少数的上游主机。全局速率限制器有助于防止级联故障。\n   速率限制  Envoy 与任何实现定义的 RPC/IDL 协议的外部速率限制服务集成。该服务的参考实现使用 Go、gRPC 和 Redis 作为其后端，可参考这里。\nEnvoy 调用外部速率限制服务（例如，在 Redis 中存储统计信息并跟踪请求），以得到该请求是否应该被速率限制的响应。\n使用外部速率限制服务，我们可以将限制应用于一组服务，或者，如果我们谈论的是服务网格，则应用于网格中的所有服务。我们可以控制进入网格的请求数量，作为一个整体。\n为了控制单个服务层面的请求率，我们可以使用局部速率限制器。局部速率限制器允许我们对每个服务有单独的速率限制。局部和全局速率限制通常是一起使用的。\n配置全局速率限制 在配置全局速率限制时，我们必须设置两个部分——客户端（Envoy）和服务器端（速率限制服务）。\n我们可以在 Envoy 侧将速率限制服务配置为网络级速率限制过滤器或 HTTP 级速率限制过滤器。\n当在网络层面上使用速率限制过滤器时，Envoy 对我们配置了过滤器的监听器的每个新连接调用速率限制服务。同样，使用 HTTP 级别的速率限制过滤器，Envoy 对安装了过滤器的监听器上的每个新请求调用速率限制服务，并且路由表指定应调用全局速率限制服务。所有到目标上游集群的请求以及从发起集群到目标集群的请求都可以被限制速率。\n在配置速率限制服务之前，我们需要解释动作、描述符（键 / 值对）和描述符列表的概念。\n   速率限制概念  在路由或虚拟主机层面的 Envoy 配置中，我们定义了一组动作。每个动作都包含一个速率限制动作的列表。让我们考虑下面的例子，在虚拟主机级别上定义速率限制。\nrate_limits:- actions:- header_value_match:descriptor_value:get_requestheaders:- name::methodprefix_match:GET- header_value_match:descriptor_value:pathheaders:- name::pathprefix_match:/api- actions:- header_value_match:descriptor_value:post_requestheaders:- name::methodprefix_match:POST- actions:- header_value_match:descriptor_value:get_requestheaders:- name::methodprefix_match:GET上面的片段定义了三个独立的动作，其中包含速率限制动作。Envoy 将尝试将请求与速率限制动作相匹配，并生成描述符发送到速率限制服务。如果 Envoy 不能将任何一个速率限制动作与请求相匹配，则不会创建描述符 ——即所有速率限制动作都必须相匹配。\n例如，如果我们收到一个到 /api 的 GET 请求，第一个动作与两个速率限制动作相匹配；因此会创建一个如下描述符。\n(\u0026#34;header_match\u0026#34;: \u0026#34;get_request\u0026#34;), (\u0026#34;header_match\u0026#34;: \u0026#34;path\u0026#34;) 第二个动作是不会匹配的。然而，最后一个也会匹配。因此，Envoy 将向速率限制服务发送以下描述符。\n(\u0026#34;header_match\u0026#34;: \u0026#34;get_request\u0026#34;), (\u0026#34;header_match\u0026#34;: \u0026#34;path\u0026#34;) (\u0026#34;header_match\u0026#34;: \u0026#34;get_request\u0026#34;) 让我们来看看另一个满足以下要求的客户端配置的例子。\n 对 /users 的 POST 请求被限制在每分钟 10 个请求。 对 /users 的请求被限制在每分钟 20 个请求。 带有 dev: true 头的 /api 请求被限制在每秒 10 个请求的速率。 向 /api 发出的带有 dev: false 头的请求被限制在每秒 5 个请求。 对 /api 的任何其他请求都没有速率限制。  请注意，这次我们是在路由层面上定义速率限制。\nroutes:- match:prefix:\u0026#34;/users\u0026#34;route:cluster:some_clusterrate_limits:- actions:- generic_key:descriptor_value:users- header_value_match:descriptor_value:post_requestheaders:- name:\u0026#34;:method\u0026#34;exact_match:POST- actions:- generic_key:descriptor_value:users- match:prefix:\u0026#34;/api\u0026#34;route:cluster:some_clusterrate_limits:- actions:- generic_key:descriptor_value:api- request_headers:header_name:devdescriptor_key:dev_request...http_filters:- name:envoy.filters.http.ratelimittyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.http.ratelimit.v3.RateLimitdomain:some_domainenable_x_ratelimit_headers:DRAFT_VERSION_03rate_limit_service:transport_api_version:V3grpc_service:envoy_grpc:cluster_name:rate-limit-cluster上述配置包含每条路由的 rate_limits 配置和 envoy. filters.http.ratelimit 过滤器配置。过滤器的配置指向速率限制服务的上游集群。我们还设置了域名（domain）和 enabled_x_ratelimit_headers  字段，指定我们要使用 x-ratelimit  头。我们可以按任意的域名来隔离一组速率限制配置。\n如果我们看一下路由中的速率限制配置，注意到我们是如何拆分动作以匹配我们想要设置的不同速率限制的。例如，我们有一个带有 api 通用密钥和请求头的动作。然而，在同一个配置中，我们也有一个只设置了通用密钥的动作。这使得我们可以根据这些动作配置不同的速率限制。\n让我们把动作翻译成描述符。\nGET /users --\u0026gt; (\u0026#34;generic_key\u0026#34;: \u0026#34;users\u0026#34;) POST /users --\u0026gt; (\u0026#34;generic_key\u0026#34;: \u0026#34;users\u0026#34;), (\u0026#34;header_match\u0026#34;: \u0026#34;post_request\u0026#34;) GET /api dev: some_header_value --\u0026gt; (\u0026#34;generic_key\u0026#34;: \u0026#34;api\u0026#34;), (\u0026#34;dev_request\u0026#34;: \u0026#34;some_header_value\u0026#34;) header_match 和 request_headers 之间的区别是，对于后者，我们可以根据特定的头信息值来创建速率限制（例如，dev: true 或 dev: something，因为头信息的值成为描述符的一部分）。\n在速率限制服务方面，我们需要开发一个配置，根据 Envoy 发送的描述符来指定速率限制。\n例如，如果我们向 /users 发送一个 GET 请求，Envoy 会向速率限制服务发送以下描述符。(\u0026#34;generic_key\u0026#34;: \u0026#34;users\u0026#34;)。然而，如果我们发送一个 POST 请求，描述符列表看起来像这样。\n(\u0026#34;generic_key\u0026#34;: \u0026#34;users\u0026#34;), (\u0026#34;header_match\u0026#34;: \u0026#34;post_request\u0026#34;) 速率限制服务配置是分层次的，允许匹配嵌套描述符。让我们看看上述描述符的速率限制服务配置会是什么样子。\ndomain:some_domaindescriptors:- key:generic_keyvalue:usersrate_limit:unit:MINUTErequests_per_unit:20descriptors:- key:header_matchvalue:post_requestrate_limit:unit:MINUTErequests_per_unit:10- key:generic_keyvalue:apidescriptors:- key:dev_requestvalue:truerate_limit:unit:SECONDrequests_per_unit:10- key:dev_requestvalue:falserate_limit:unit:SECONDrequests_per_unit:5我们之前在 Envoy 方面的配置中提到了 domain 值。现在我们可以看看如何使用域名。我们可以在整个代理机群中使用相同的描述符名称，但要用域名来分隔它们。\n让我们看看速率限制服务上的匹配对不同请求是如何工作的。\n   收到的请求 生成的描述符 速率限制 解释     GET /users (\u0026#34;generic_key\u0026#34;: \u0026#34;users\u0026#34;) 20 req/min 键 users 与配置中的第一层相匹配。由于配置中的第二层（header_match）没有包括在描述符中，所以使用了 users 键的速率限制。   POST /users (\u0026#34;generic_key\u0026#34;: \u0026#34;users\u0026#34;), (\u0026#34;header_match\u0026#34;: \u0026#34;post_request\u0026#34;) 10 req/min 发送的描述符和 header_match 一样匹配 users，所以使用 header_match 描述符下的速率限制。   GET /api (\u0026#34;generic_key\u0026#34;: \u0026#34;api\u0026#34;) 无速率限制 我们只有 api描述符的第一级匹配。然而，并没有配置速率限制。为了执行速率限制，我们需要第二级描述符，这些描述符只有在传入的请求中存在 Header dev 时才会被设置。   GET /apidev: true (\u0026#34;generic_key\u0026#34;: \u0026#34;api\u0026#34;), (\u0026#34;dev_request\u0026#34;: \u0026#34;true\u0026#34;) 10 req/second 列表中的第二个描述符与配置中的第二层相匹配（即我们匹配 api，然后也匹配 dev_request：true）。   GET /apidev: false (\u0026#34;generic_key\u0026#34;: \u0026#34;api\u0026#34;), (\u0026#34;dev_request\u0026#34;: \u0026#34;false\u0026#34;) 5 req/second 列表中的第二个描述符与配置中的第二层相匹配（即我们匹配 api，然后也匹配 dev_request：true）。   GET /apidev: hello (\u0026#34;generic_key\u0026#34;: \u0026#34;api\u0026#34;), (\u0026#34;dev_request\u0026#34;: \u0026#34;hello\u0026#34;) 无速率限制 列表中的第二个描述符与配置中的任何二级描述符都不匹配。    除了我们在上面的例子中使用的动作外，下表显示了我们可以用来创建描述符的其他动作。\n   动作名称 描述     source_cluster 源集群的速率限制   destination_cluster 目的地集群的速率限制   request_headers 对请求头的速率限制   remote_address 远程地址的速率限制   generic_key 对一个通用键的速率限制   header_value_match 对请求头的存在进行速率限制   metadata 元数据的速率限制    下图总结了这些操作与描述符和速率限制服务上的实际 …","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"bd2a5389138ef6bae5a2972a61509134","permalink":"https://lib.jimmysong.io/envoy-handbook/hcm/global-rate-limiting/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/envoy-handbook/hcm/global-rate-limiting/","section":"envoy-handbook","summary":"当许多主机向少数上游服务器发送请求，且平均延迟较低时，全局或分布式速率限制很有用。 许多主机上少许上游服务器发送请求 由于服务器不能快速处理这些请求，请求就会变得滞后。在这种情况下，众多的下游主机可以压倒","tags":["Envoy"],"title":"全局速率限制","type":"book"},{"authors":null,"categories":["Envoy"],"content":"在这个实验中，我们将学习如何配置使用不同的方式来匹配请求。我们将使用 direct_response，我们还不会涉及任何 Envoy 集群。\n路径匹配 以下是我们想放入配置中的规则：\n 所有请求都需要来自 hello.io 域名（即在向代理发出请求时，我们将使用 Host: hello.io 标头） 所有向路径 /api 发出的请求将返回字符串 hello - path 所有向根路径（即 /）发出的请求将返回字符串 hello - prefix 所有以 /hello 开头并在后面加上数字的请求（如 /hello/1，/hello/523）都应返回 hello - regex字符串  让我们看一下配置：\nstatic_resources:listeners:- name:listener_0address:socket_address:address:0.0.0.0port_value:10000filter_chains:- filters:- name:envoy.filters.network.http_connection_managertyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManagerstat_prefix:hello_servicehttp_filters:- name:envoy.filters.http.routerroute_config:name:routevirtual_hosts:- name:hello_vhostdomains:[\u0026#34;hello.io\u0026#34;]routes:- match:path:\u0026#34;/api\u0026#34;direct_response:status:200body:inline_string:\u0026#34;hello - path\u0026#34;- match:safe_regex:google_re2:{}regex:^/hello/\\d+$direct_response:status:200body:inline_string:\u0026#34;hello - regex\u0026#34;- match:prefix:\u0026#34;/\u0026#34;direct_response:status:200body:inline_string:\u0026#34;hello - prefix\u0026#34;由于我们只有一个域名，我们将使用一个单一的虚拟主机。域名数组将包含一个单一的域名：hello.io。这是 Envoy 要做的第一层匹配。\n然后，虚拟主机将有多个路径匹配。首先，我们将使用路径匹配，因为我们想准确匹配 /api 路径。其次，我们使用 ^/hello/\\d+$ 正则表达式来定义正则表达式匹配。最后，我们定义前缀匹配。注意，定义这些匹配的顺序很重要。如果我们把前缀匹配放在最前面，那么其余的匹配就不会被评估，因为前缀匹配永远是真的。\n将上述 YAML 保存为 2-lab-1-request-matching-1.yaml，然后运行 func-e run -c 2-lab-1-request-matching-1.yaml 来启动 Envoy 代理。\n从另一个单独的终端，我们可以进行一些测试调用。\n$ curl -H \u0026#34;Host: hello.io\u0026#34; localhost:10000 hello - prefix $ curl -H \u0026#34;Host: hello.io\u0026#34; localhost:10000/api hello - path $ curl -H \u0026#34;Host: hello.io\u0026#34; localhost:10000/hello/123 hello - regex 标头匹配 匹配传入请求的头信息可以与路径匹配相结合，以实现复杂的场景。在这个例子中，我们将使用前缀匹配和不同头信息匹配的组合。\n让我们设想一些规则：\n 所有带头信息 debug: 1 的 POST 请求发送到 /1，返回 422 状态码 所有发送至 /2 的头为 path 且与正则表达式 ^/hello/\\d+$ 相匹配的请求都会返回一个 200 状态码和消息 regex 所有将头名称 priority 设置为 1 到 5 之间的请求，发送到 /3 会返回一个 200 状态码和消息 priority 所有发送到 /4 的请求，如果存在 test 头，都会返回一个 500 状态码  以下是翻译成 Envoy 配置的上述规则：\nstatic_resources:listeners:- name:listener_0address:socket_address:address:0.0.0.0port_value:10000filter_chains:- filters:- name:envoy.filters.network.http_connection_managertyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManagerstat_prefix:hello_servicehttp_filters:- name:envoy.filters.http.routerroute_config:name:routevirtual_hosts:- name:vhostdomains:[\u0026#34;*\u0026#34;]routes:- match:path:\u0026#34;/1\u0026#34;headers:- name:\u0026#34;:method\u0026#34;string_match:exact:POST- name:\u0026#34;debug\u0026#34;string_match:exact:\u0026#34;1\u0026#34;direct_response:status:422- match:path:\u0026#34;/2\u0026#34;headers:- name:\u0026#34;path\u0026#34;safe_regex_match:google_re2:{}regex:^/hello/\\d+$direct_response:status:200body:inline_string:\u0026#34;regex\u0026#34;- match:path:\u0026#34;/3\u0026#34;headers:- name:\u0026#34;priority\u0026#34;range_match:start:1end:6direct_response:status:200body:inline_string:\u0026#34;priority\u0026#34;- match:path:\u0026#34;/4\u0026#34;headers:- name:\u0026#34;test\u0026#34;present_match:truedirect_response:status:500将上述 YAML 保存为 2-lab-1-request-matching-2.yaml 并运行\nfunc-e run -c 2-lab-1-request-matching-2.yaml 让我们试着发送几个请求，测试一下规则：\n$ curl -v -X POST -H \u0026#34;debug: 1\u0026#34; localhost:10000/1 ... \u0026gt; User-Agent: curl/7.64.0 \u0026gt; Accept: */* \u0026gt; debug: 1 \u0026gt; \u0026lt; HTTP/1.1 422 Unprocessable Entity $ curl -H \u0026#34;path: /hello/123\u0026#34; localhost:10000/2 regex $ curl -H \u0026#34;priority: 3\u0026#34; localhost:10000/3 priority $ curl -v -H \u0026#34;test: tst\u0026#34; localhost:10000/4 ... \u0026gt; User-Agent: curl/7.64.0 \u0026gt; Accept: */* \u0026gt; test: tst \u0026gt; \u0026lt; HTTP/1.1 500 Internal Server Error 查询参数匹配 与我们做路径和 Header 匹配的方式相同，我们也可以匹配特定的查询参数和它们的值。查询参数匹配支持与其他两项相同的匹配规则：匹配精确值、前缀和后缀，使用正则表达式，以及检查查询参数是否包含特定值。\n让我们考虑配置中的以下情景：\n 所有发送到路径 /1 并带有查询参数 test 请求都返回 422 状态码 所有发送到路径 /2 的查询参数为 env 的请求，其值以 env_开头（忽略大小写），返回 200 状态代码 所有发送到路径 /3 的查询参数 debug 设置为 true 的请求，都返回 500 状态码  上述规则转化为以下 Envoy 配置：\nstatic_resources:listeners:- name:listener_0address:socket_address:address:0.0.0.0port_value:10000filter_chains:- filters:- name:envoy.filters.network.http_connection_managertyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManagerstat_prefix:hello_servicehttp_filters:- name:envoy.filters.http.routerroute_config:name:routevirtual_hosts:- name:vhostdomains:[\u0026#34;*\u0026#34;]routes:- match:path:\u0026#34;/1\u0026#34;query_parameters:- name:testpresent_match:truedirect_response:status:422- match:path:\u0026#34;/2\u0026#34;query_parameters:- name:envstring_match:prefix:env_ignore_case:truedirect_response:status:200- match:path:\u0026#34;/3\u0026#34;query_parameters:- name:debugstring_match:exact:\u0026#34;true\u0026#34;direct_response:status:500将上述 YAML 保存为 2-lab-1-request-matching-3.yaml 并运行\nfunc-e run -c 2-lab-1-request-matching-3.yaml 让我们试着发送几个请求，测试一下规则。\n$ curl -v localhost:10000/1?test ... \u0026gt; GET /1?test HTTP/1.1 \u0026gt; Host: localhost:10000 \u0026gt; User-Agent: curl/7.64.0 \u0026gt; Accept: */* \u0026gt; \u0026lt; HTTP/1.1 422 Unprocessable Entity $ curl -v localhost:10000/2?env=eNv_prod ... \u0026gt; GET /2?env=eNv_prod HTTP/1.1 \u0026gt; Host: localhost:10000 \u0026gt; User-Agent: curl/7.64.0 \u0026gt; Accept: */* \u0026gt; \u0026lt; HTTP/1.1 200 OK $ curl -v localhost:10000/3?debug=true ... \u0026gt; GET /3?debug=true HTTP/1.1 \u0026gt; Host: localhost:10000 \u0026gt; User-Agent: curl/7.64.0 \u0026gt; Accept: */* \u0026gt; \u0026lt; HTTP/1.1 500 Internal Server Error ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"5d1e5a1f767a39d7d1a1c9dca1d1f682","permalink":"https://lib.jimmysong.io/envoy-handbook/hcm/lab1/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/envoy-handbook/hcm/lab1/","section":"envoy-handbook","summary":"在这个实验中，我们将学习如何配置使用不同的方式来匹配请求。我们将使用 direct_response，我们还不会涉及任何 Envoy 集群。 路径匹配 以下是我们想放入配置中的规则： 所有请求都需要来自 hello.io 域名（即在向代理","tags":["Envoy"],"title":"实验1：请求匹配","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"该特性在自 Kubernetes 1.6 版本推出 beta 版本。Init 容器可以在 PodSpec 中同应用程序的 containers 数组一起来指定。此前 beta 注解的值仍将保留，并覆盖 PodSpec 字段值。\n本文讲解 Init 容器的基本概念，这是一种专用的容器，在应用程序容器启动之前运行，用来包含一些应用镜像中不存在的实用工具或安装脚本。\n理解 Init 容器 Pod 能够具有多个容器，应用运行在容器里面，但是它也可能有一个或多个先于应用容器启动的 Init 容器。\nInit 容器与普通的容器非常像，除了如下两点：\n Init 容器总是运行到成功完成为止。 每个 Init 容器都必须在下一个 Init 容器启动之前成功完成。  如果 Pod 的 Init 容器失败，Kubernetes 会不断地重启该 Pod，直到 Init 容器成功为止。然而，如果 Pod 对应的 restartPolicy 为 Never，它不会重新启动。\n指定容器为 Init 容器，在 PodSpec 中添加 initContainers 字段，以 v1.Container 类型对象的 JSON 数组的形式，还有 app 的 containers 数组。 Init 容器的状态在 status.initContainerStatuses 字段中以容器状态数组的格式返回（类似 status.containerStatuses 字段）。\n与普通容器的不同之处 Init 容器支持应用容器的全部字段和特性，包括资源限制、数据卷和安全设置。 然而，Init 容器对资源请求和限制的处理稍有不同，在下面 资源 处有说明。 而且 Init 容器不支持 Readiness Probe，因为它们必须在 Pod 就绪之前运行完成。\n如果为一个 Pod 指定了多个 Init 容器，那些容器会按顺序一次运行一个。只有当前面的 Init 容器必须运行成功后，才可以运行下一个 Init 容器。当所有的 Init 容器运行完成后，Kubernetes 才初始化 Pod 和运行应用容器。\nInit 容器能做什么？ 因为 Init 容器具有与应用程序容器分离的单独镜像，所以它们的启动相关代码具有如下优势：\n 它们可以包含并运行实用工具，但是出于安全考虑，是不建议在应用程序容器镜像中包含这些实用工具的。 它们可以包含使用工具和定制化代码来安装，但是不能出现在应用程序镜像中。例如，创建镜像没必要 FROM 另一个镜像，只需要在安装过程中使用类似 sed、 awk、 python 或 dig 这样的工具。 应用程序镜像可以分离出创建和部署的角色，而没有必要联合它们构建一个单独的镜像。 Init 容器使用 Linux Namespace，所以相对应用程序容器来说具有不同的文件系统视图。因此，它们能够具有访问 Secret 的权限，而应用程序容器则不能。 它们必须在应用程序容器启动之前运行完成，而应用程序容器是并行运行的，所以 Init 容器能够提供了一种简单的阻塞或延迟应用容器的启动的方法，直到满足了一组先决条件。  示例 下面列举了 Init 容器的一些用途：\n  等待一个 Service 创建完成，通过类似如下 shell 命令：\nfor i in {1..100}; do sleep 1; if dig myservice; then exit 0; fi; exit 1   将 Pod 注册到远程服务器，通过在命令中调用 API，类似如下：\ncurl -X POST http://$MANAGEMENT_SERVICE_HOST:$MANAGEMENT_SERVICE_PORT/register -d \u0026#39;instance=$(\u0026lt;POD_NAME\u0026gt;)\u0026amp;ip=$(\u0026lt;POD_IP\u0026gt;)\u0026#39;   在启动应用容器之前等一段时间，使用类似 sleep 60 的命令。\n  克隆 Git 仓库到数据卷。\n  将配置值放到配置文件中，运行模板工具为主应用容器动态地生成配置文件。例如，在配置文件中存放 POD_IP 值，并使用 Jinja 生成主应用配置文件。\n  更多详细用法示例，可以在 StatefulSet 文档 和 生产环境 Pod 指南 中找到。\n使用 Init 容器 下面是 Kubernetes 1.5 版本 yaml 文件，展示了一个具有 2 个 Init 容器的简单 Pod。 第一个等待 myservice 启动，第二个等待 mydb 启动。 一旦这两个 Service 都启动完成，Pod 将开始启动。\napiVersion:v1kind:Podmetadata:name:myapp-podlabels:app:myappannotations:pod.beta.kubernetes.io/init-containers:\u0026#39;[ { \u0026#34;name\u0026#34;: \u0026#34;init-myservice\u0026#34;, \u0026#34;image\u0026#34;: \u0026#34;busybox\u0026#34;, \u0026#34;command\u0026#34;: [\u0026#34;sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;until nslookup myservice; do echo waiting for myservice; sleep 2; done;\u0026#34;] }, { \u0026#34;name\u0026#34;: \u0026#34;init-mydb\u0026#34;, \u0026#34;image\u0026#34;: \u0026#34;busybox\u0026#34;, \u0026#34;command\u0026#34;: [\u0026#34;sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;until nslookup mydb; do echo waiting for mydb; sleep 2; done;\u0026#34;] } ]\u0026#39;spec:containers:- name:myapp-containerimage:busyboxcommand:[\u0026#39;sh\u0026#39;,\u0026#39;-c\u0026#39;,\u0026#39;echo The app is running! \u0026amp;\u0026amp; sleep 3600\u0026#39;]这是 Kubernetes 1.6 版本的新语法，尽管老的 annotation 语法仍然可以使用。我们已经把 Init 容器的声明移到 spec 中：\napiVersion:v1kind:Podmetadata:name:myapp-podlabels:app:myappspec:containers:- name:myapp-containerimage:busyboxcommand:[\u0026#39;sh\u0026#39;,\u0026#39;-c\u0026#39;,\u0026#39;echo The app is running! \u0026amp;\u0026amp; sleep 3600\u0026#39;]initContainers:- name:init-myserviceimage:busyboxcommand:[\u0026#39;sh\u0026#39;,\u0026#39;-c\u0026#39;,\u0026#39;until nslookup myservice; do echo waiting for myservice; sleep 2; done;\u0026#39;]- name:init-mydbimage:busyboxcommand:[\u0026#39;sh\u0026#39;,\u0026#39;-c\u0026#39;,\u0026#39;until nslookup mydb; do echo waiting for mydb; sleep 2; done;\u0026#39;] 注意：版本兼容性问题\n1.5 版本的语法在 1.6 和 1.7 版本中仍然可以使用，但是我们推荐使用 1.6 版本的新语法。Kubernetes 1.8 以后的版本只支持新语法。在 Kubernetes 1.6 版本中，Init 容器在 API 中新建了一个字段。 虽然期望使用 beta 版本的 annotation，但在未来发行版将会被废弃掉。\n 下面的 YAML 文件展示了 mydb 和 myservice 两个 Service：\nkind:ServiceapiVersion:v1metadata:name:myservicespec:ports:- protocol:TCPport:80targetPort:9376---kind:ServiceapiVersion:v1metadata:name:mydbspec:ports:- protocol:TCPport:80targetPort:9377这个 Pod 可以使用下面的命令进行启动和调试：\n$ kubectl create -f myapp.yaml pod \u0026#34;myapp-pod\u0026#34; created $ kubectl get -f myapp.yaml NAME READY STATUS RESTARTS AGE myapp-pod 0/1 Init:0/2 0 6m $ kubectl describe -f myapp.yaml Name: myapp-pod Namespace: default [...] Labels: app=myapp Status: Pending [...] Init Containers: init-myservice: [...] State: Running [...] init-mydb: [...] State: Waiting Reason: PodInitializing Ready: False [...] Containers: myapp-container: [...] State: Waiting Reason: PodInitializing Ready: False [...] Events: FirstSeen LastSeen Count From SubObjectPath Type Reason Message --------- -------- ----- ---- ------------- -------- ------ ------- 16s 16s 1 {default-scheduler } Normal Scheduled Successfully assigned myapp-pod to 172.17.4.201 16s 16s 1 {kubelet 172.17.4.201} spec.initContainers{init-myservice} Normal Pulling pulling image \u0026#34;busybox\u0026#34; 13s 13s 1 {kubelet 172.17.4.201} spec.initContainers{init-myservice} Normal Pulled Successfully pulled image \u0026#34;busybox\u0026#34; 13s 13s 1 {kubelet 172.17.4.201} spec.initContainers{init-myservice} Normal Created Created container with docker id 5ced34a04634; Security:[seccomp=unconfined] 13s 13s 1 {kubelet 172.17.4.201} spec.initContainers{init-myservice} Normal Started Started container with docker id 5ced34a04634 $ kubectl logs myapp-pod -c init-myservice # Inspect the first init container $ kubectl logs myapp-pod -c init-mydb # Inspect the second init container 一旦我们启动了 mydb 和 myservice 这两个 Service，我们能够看到 Init 容器完成，并且 myapp-pod 被创建：\n$ kubectl create -f services.yaml service \u0026#34;myservice\u0026#34; created service \u0026#34;mydb\u0026#34; created $ kubectl get -f myapp.yaml NAME READY STATUS RESTARTS AGE myapp-pod 1/1 Running 0 9m 这个例子非常简 …","date":1653062400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"ea7113420707ce57d886c548ac7c8f63","permalink":"https://lib.jimmysong.io/kubernetes-handbook/objects/init-containers/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/objects/init-containers/","section":"kubernetes-handbook","summary":"该特性在自 Kubernetes 1.6 版本推出 beta 版本。Init 容器可以在 PodSpec 中同应用程序的 containers 数组一起来指定。此前 beta 注解的值仍将保留，并覆盖 PodSpec 字段值。 本文讲解 Init 容器的基本概念，这是一种专用的容器，在应用程序容器启动之前运行，用来","tags":["Kubernetes"],"title":"Init 容器","type":"book"},{"authors":null,"categories":["云原生"],"content":" 以下内容最初由 CNCF 发布在 Kubernetes.io 上，并且在此获得许可。\n 在 2014 年夏天，Box 对沉淀了十年的硬件和软件基础架构的痛苦，这无法与公司的需求保持一致。\n该平台为超过 5000 万用户（包括政府和大型企业如通用电气公司）管理和共享云中的内容，Box 最初是一个使用 PHP 写的具有数百万行的庞大代码，内置裸机数据中心。它已经开始将单体应用分解成微服务。 “随着我们扩展到全球各地，公有云战争正在升温，我们开始专注于如何在许多不同的环境和许多不同的云基础架构提供商之间运行我们的工作负载”，Box 联合创始人和服务架构师 Sam Ghods 说。 “迄今为止，这是一个巨大的挑战，因为所有这些不同的提供商，特别是裸机，都有非常不同的接口和与合作方式。”\n当 Ghods 参加 DockerCon 时，Box 的云原生之旅加速了。该公司已经认识到，它不能再仅仅使用裸机来运行应用程序，正在研究 Docker 容器化，使用 OpenStack 进行虚拟化以及支持公有云。\n在那次会议上，Google 宣布发布 Kubernetes 容器管理系统，Ghods 成功了。 “我们研究了许多不同的选择，但 Kubernetes 确实很出色，特别是因为 Borg 老兵的团队非常强大，可以以基础架构不可知的方式来运行云软件，” 他谈到 Google 内部的容器调度器 Borg。 “事实上，一开始它设计与裸机一样运行，就像我们可以在数据中心内迁移到它一样，然后也使用相同的工具和概念在公有云提供商上运行。”\n另外：Ghods 喜欢 Kubernetes 拥有一套通用的 API 对象，如 pod、服务、副本集和部署，这些对象创建了一个一致的接口来构建工具。 “甚至像 OpenShift 或 Deis 这样的构建在 Kubernetes 之上的 PaaS 层仍然将这些对象视为统一的原则，” 他说。 “我们很高兴能够在整个生态系统中共享这些抽象概念，这会产生比我们在其他潜在解决方案中更多的动力。”\n六个月后 Box 在一个生产数据中心的集群中部署了 Kubernetes。Kubernetes 在 0.11 版本之前仍然是测试版。他们从小版本开始：Ghods 的团队在 Kubernetes 上运行的第一个服务就是 Box API 监视器，确认了 Box 可以运行。 “这只是一个让整个管道运作正常的测试服务，” 他说。接下来是一些处理作业的守护进程，它们 “很好而且安全，因为如果他们遇到任何中断，也不会让来自客户的同步传入请求失败。”\n几个月后，该团队可以发送并要求提供信息的第一个实时服务启动。那时，Ghods 说：“我们对 Kubernetes 集群的稳定性感到满意。我们开始迁移一些服务，然后我们将扩大集群的规模和端口数量，最后每个数据中心的服务器数量大约为 100 台，这些服务器纯粹专用于 Kubernetes。在未来的 12 个月里，这个数字将会增长很多，达到数百甚至数千。“\n在观察开始使用 Kubernetes 进行微服务的团队时，“我们看到正在发布的微服务数量有所增加，”Ghodsnotes 说。 “显然，通过微服务构建软件的方式已被压抑很久，随着灵活性的提高帮助我们的开发人员提高了生产力，并为更好的架构选择做好了准备。”\nGhods 反映，作为早期采用者，Box 经历了不同的旅程。他说：“我们肯定是在等待某些事情稳定和功能发布，我们在这一步上被锁定，” 他说。 “在早期，我们对 Kubectl 应用等组件做了很多贡献，并等待 Kubernetes 发布，然后我们会升级，贡献更多，并来回多次。整个项目从我们第一次在 Kubernetes 上进行实际部署到 GA 需要大约 18 个月的时间。如果我们今天自己来做一遍同样的事情，可能会少于六个月。“\n无论如何，Box 无需为 Kubernetes 做过多修改。Ghods 说：“我们团队在 Box 中实施 Kubernetes 所做的绝大多数工作一直致力于在我们现有的（往往是遗留下来的）基础架构内的工作，例如将我们的基础操作系统从 RHEL6 升级到 RHEL7 或将其整合纳入到我们的监控基础架构 Nagios。但总体而言，Kubernetes 非常灵活，能够适应我们的许多限制因素，并且它在我们的裸机基础架构上运行非常成功。“\n对于 Box 来说，更大的挑战也许是文化上的挑战。Ghods 说：“Kubernetes 和一般的云本身代表了一个非常大的范式转换，并且它不是非常渐进的，”Ghods 说。 “我们可以这样说，Kubernetes 将会解决所有问题，因为它能够以正确的方式做事，一切都会变得更好。但是要记住，它不像其他许多解决方案那样可靠。你不能说这家或那家公司花了多少时间做这件事，因为还没有那么多。我们的团队必须真正为资源而战，因为我们的项目有一点点的恐惧。“\n从经验中学习，Ghods 为经历类似挑战的公司提供了以下两条建议：\n 提前和经常交付。对于 Box 来说，服务发现是一个巨大的问题，团队必须决定是建立一个临时解决方案还是等待 Kubernetes 本身满足 Box 的独特要求。经过多次辩论之后，“我们刚开始专注于提供可行的解决方案，然后处理可能在稍后迁移到更原始的解决方案，”Ghods 说。 “无论多么微不足道，团队的上述目标应始终是为基础架构上的实际生产用例服务。这有助于保持团队本身和组织对项目的看法。“ 保持开放的态度，了解公司必须从开发人员那里抽象出什么和没有抽象出什么。早期，团队在 Dockerfiles 之上构建了一个抽象，以帮助确保所有容器镜像具有正确的安全更新。事实证明这是多余的工作，因为容器镜像是不可变的，您可以在构建后扫描它们以确保它们不包含漏洞。因为通过容器化来管理基础架构是一个不连续的飞跃，所以最好先直接使用本地工具学习其独特的优势和注意事项。抽象只能在实际需要出现之后才能建立。  最后，影响力非常强大。“在 Kubernetes 之前，”Ghods 说，“我们的基础架构非常陈旧，需要 6 个多月才能部署一个新的微服务。现在，一个新的微服务部署时间不到五天。我们正在努力让它达到不到一天。诚然，这六个月的大部分时间都是由于我们的系统有多么糟糕，但裸机本质上是一个难以支持的平台，除非您有像 Kubernetes 这样的系统来帮助管理它。“\n按 Ghods 的估计，Box 距离完成 90％运行在 Kubernetes 上的目标还有几年的时间。 “到目前为止，我们已经完成了一项稳定的，关键任务的 Kubernetes 部署，它提供了很多价值，” 他说。 “现在我们 10％左右服务器都运行在 Kubernetes 上，我认为明年我们可能会超过一半。我们正在努力实现所有无状态服务使用案例，并计划在此之后将我们的重点转移到有状态服务。“\n事实上，这就是他在整个行业中的设想：Ghods 预测 Kubernetes 有机会成为新的云平台。 Kubernetes 提供了一个涵盖不同云平台的 API，包括裸机，以及 “当我们可以针对单一界面进行编程时，我不认为人们已经看到了可能的全部潜力”，他说。 “与 AWS 改变基础架构一样，您不必再考虑服务器或机柜或网络设备，Kubernetes 使您能够专注于您正在运行的软件，这非常令人兴奋。这是愿景。“\nGhods 指出了已经在开发或最近发布的作为云平台的项目：集群联邦，Dashboard UI 和 CoreOS 的 etcd operator。 “我真的相信这是我在云基础架构中看到的最激动人心的事情，” 他说，“因为它是一个前所未有的自动化和智能环境，其基础架构对每个基础架构平台都是可移植和不可知的。”\n由于早期决定使用裸机，Box 不得已开始了 Kubernetes 之旅。但是 Ghods 表示，即使公司现在不必对云提供商不可知，Kubernetes 也可能很快成为行业标准，因为越来越多的工具和扩展是围绕 API 构建的。\n“同样的方式，偏离 Linux 是没有意义的，因为它是如此的标准，”Ghods 说，“我认为 Kubernetes 正在走相同的道路。现在还处于早期阶段 —— 文档仍然需要工作，用于编写和发布 YAML 到 Kubernetes 集群的用户体验仍然很艰难。当你处于潮流最前线时，你可能会做出一些牺牲。但底线是，这是行业发展的方向。从现在开始的三到五年，如果您还会以其他方式运行基础架构，那么真的会让人非常震惊。“\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"9f5ab99e70af662981f582c3310c3270","permalink":"https://lib.jimmysong.io/cloud-native-infra/appendix-c-box-case-study/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/cloud-native-infra/appendix-c-box-case-study/","section":"cloud-native-infra","summary":"以下内容最初由 CNCF 发布在 Kubernetes.io 上，并且在此获得许可。 在 2014 年夏天，Box 对沉淀了十年的硬件和软件基础架构的痛苦，这无法与公司的需求保持一致。 该平台为超过 5000 万用户（包括政府和大型企业如通用电气公司）管理和共享云","tags":["云原生"],"title":"附录 C Box：案例研究","type":"book"},{"authors":null,"categories":["安全"],"content":"下面是一个 Kubernetes Pod 安全策略的例子，它为集群中运行的容器执行了强大的安全要求。这个例子是基于官方的 Kubernetes 文档。我们鼓励管理员对该策略进行修改，以满足他们组织的要求。\napiVersion:policy/v1beta1kind:PodSecurityPolicymetadata:name:restrictedannotations:seccomp.security.alpha.kubernetes.io/allowedProfileNames:\u0026#39;docker/default,runtime/default\u0026#39;apparmor.security.beta.kubernetes.io/allowedProfileNames:\u0026#39;runtime/default\u0026#39;seccomp.security.alpha.kubernetes.io/defaultProfileName:\u0026#39;runtime/default\u0026#39;apparmor.security.beta.kubernetes.io/defaultProfileName:\u0026#39;runtime/default\u0026#39;spec:privileged:false# 需要防止升级到 rootallowPrivilegeEscalation:falserequiredDropCapabilities:- ALLvolumes:- \u0026#39;configMap\u0026#39;- \u0026#39;emptyDir\u0026#39;- \u0026#39;projected\u0026#39;- \u0026#39;secret\u0026#39;- \u0026#39;downwardAPI\u0026#39;- \u0026#39;persistentVolumeClaim\u0026#39;# 假设管理员设置的 persistentVolumes 是安全的hostNetwork:falsehostIPC:falsehostPID:falserunAsUser:rule:\u0026#39;MustRunAsNonRoot\u0026#39;# 要求容器在没有 root 的情况下运行 seLinuxrule:\u0026#39;RunAsAny\u0026#39;# 假设节点使用的是 AppArmor 而不是 SELinuxsupplementalGroups:rule:\u0026#39;MustRunAs\u0026#39;ranges:# 禁止添加到 root 组- min:1max:65535runAsGroup:rule:\u0026#39;MustRunAs\u0026#39;ranges:# 禁止添加到 root 组- min:1max:65535fsGroup:rule:\u0026#39;MustRunAs\u0026#39;ranges:# 禁止添加到 root 组- min:1max:65535readOnlyRootFilesystem:true","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"cb13fd3dd123f131f1380488e55a5deb","permalink":"https://lib.jimmysong.io/kubernetes-hardening-guidance/appendix/c/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/kubernetes-hardening-guidance/appendix/c/","section":"kubernetes-hardening-guidance","summary":"下面是一个 Kubernetes Pod 安全策略的例子，它为集群中运行的容器执行了强大的安全要求。这个例子是基于官方的 Kubernetes 文档。我们鼓励管理员对该策略进行修改，以满足他们组织的要求。 apiVersion:policy/v1beta1kind:PodSecurityPolicymetadata:name:restrictedannotations:seccomp.security.alpha.kubernetes.io/allowedProfileNames:'docker/default,runtime/default'apparmor.security.beta.kubernetes.io/allowedProfileNames:'runtime/default'seccomp.security.alpha.kubernetes.io/defaultProfileName:'runtime/default'apparmor.security.beta.kubernetes.io/defaultProfileName:'runtime/default'spec:privileged:false# 需要防止升级到 rootallowPrivilegeEscalation:falserequiredDropCapabilities:- ALLvolumes:- 'configMap'- 'emptyDir'- 'projected'- 'secret'- 'downwardAPI'- 'persistentVolumeClaim'# 假设管理员设置","tags":["Kubernetes","安全"],"title":"附录 C：Pod 安全策略示例","type":"book"},{"authors":null,"categories":["Envoy"],"content":"在这个实验中，我们将学习如何使用运行时分数和加权集群来配置 Envoy 的流量分割。\n使用运行时分数 当我们只有两个上游集群时，运行时分数是一种很好的流量分割方法。运行时分数的工作原理是提供一个运行时分数（例如分子和分母），代表我们想要路由到一个特定集群的流量的分数。然后，我们使用相同的条件（即，在我们的例子中相同的前缀）提供第二个匹配，但不同的上游集群。\n让我们创建一个 Envoy 配置，对 70% 的流量返回状态为 201 的直接响应。其余的流量返回状态为 202。\nstatic_resources:listeners:- name:listener_0address:socket_address:address:0.0.0.0port_value:10000filter_chains:- filters:- name:envoy.filters.network.http_connection_managertyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManagerstat_prefix:hello_servicehttp_filters:- name:envoy.filters.http.routerroute_config:virtual_hosts:- name:hello_vhostdomains:[\u0026#34;*\u0026#34;]routes:- match:prefix:\u0026#34;/\u0026#34;runtime_fraction:default_value:numerator:70denominator:HUNDREDruntime_key:routing.hello_iodirect_response:status:201body:inline_string:\u0026#34;v1\u0026#34;- match:prefix:\u0026#34;/\u0026#34;direct_response:status:202body:inline_string:\u0026#34;v2\u0026#34;将上述 Envoy 配置保存为 2-lab-2-traffic-splitting-1.yaml，并以该配置运行 Envoy。\nfunc-e run -c 2-lab-2-traffic-splitting-1.yaml 在 Envoy 运行时，我们可以使用 hey 工具向代理发送 200 个请求。\n$ hey http://localhost:10000 ... Status code distribution: [201] 142 responses [202] 58 responses 看一下状态代码的分布，我们会注意到，我们收到 HTTP 201 响应大概占 71%，其余的响应是 HTTP 202 响应。\n使用加权集群 当我们有两个以上的上游集群，我们想把流量分给它们，我们可以使用加权集群的方法。在这里，我们单独给每个上游集群分配权重。我们在以前的方法中使用了多个匹配，而在加权集群中，我们将使用一个路由和多个加权集群。\n对于这种方法，我们必须定义实际的上游集群。我们将运行 httpbin 镜像的三个实例。让我们在 3030、4040 和 5050 端口上运行三个不同的实例；我们将在 Envoy 配置中把它们称为 instance_1、instance_2 和 instance_3。\ndocker run -d -p 3030:80 kennethreitz/httpbin docker run -d -p 4040:80 kennethreitz/httpbin docker run -d -p 5050:80 kennethreitz/httpbin 一个上游集群可以通过以下片段来定义。\nclusters:- name:instance_1connect_timeout:5sload_assignment:cluster_name:instance_1endpoints:- lb_endpoints:- endpoint:address:socket_address:address:127.0.0.1port_value:3030让我们创建 Envoy 配置，将 50% 的流量分给 instance_1，30% 分给 instance_2，20% 分给 instance_3。\n我们还将启用管理接口来检索指标，这些指标将显示向不同集群发出的请求数量。\nstatic_resources:listeners:- name:listener_0address:socket_address:address:0.0.0.0port_value:10000filter_chains:- filters:- name:envoy.filters.network.http_connection_managertyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManagerstat_prefix:listener_httphttp_filters:- name:envoy.filters.http.routerroute_config:name:routevirtual_hosts:- name:vhdomains:[\u0026#34;*\u0026#34;]routes:- match:prefix:\u0026#34;/\u0026#34;route:weighted_clusters:clusters:- name:instance_1weight:50- name:instance_2weight:30- name:instance_3weight:20clusters:- name:instance_1connect_timeout:5sload_assignment:cluster_name:instance_1endpoints:- lb_endpoints:- endpoint:address:socket_address:address:127.0.0.1port_value:3030- name:instance_2connect_timeout:5sload_assignment:cluster_name:instance_1endpoints:- lb_endpoints:- endpoint:address:socket_address:address:127.0.0.1port_value:4040- name:instance_3connect_timeout:5sload_assignment:cluster_name:instance_1endpoints:- lb_endpoints:- endpoint:address:socket_address:address:127.0.0.1port_value:5050admin:address:socket_address:address:127.0.0.1port_value:9901将上述 YAML 保存为 2-lab-2-traffic-splitting-2.yaml 并运行代理程序：func-e run -c 2-lab-2-traffic-splitting-2.yaml。\n一旦代理运行，我们将使用 hey 来发送 200 个请求。\nhey http://localhost:10000 来自 hey 的响应不会帮助我们确定分割，因为每个上游集群的响应都是 HTTP 200。\n要想看到流量的分割，请在 http://localhost:9901/stats/prometheus 上打开统计表。在指标列表中，寻找 envoy_cluster_external_upstream_rq 指标，该指标计算外部上游请求的数量。我们应该看到与此类似的分割。\n# TYPE envoy_cluster_external_upstream_rq counter envoy_cluster_external_upstream_rq{envoy_response_code=\u0026#34;200\u0026#34;,envoy_cluster_name=\u0026#34;instance_1\u0026#34;} 99 envoy_cluster_external_upstream_rq{envoy_response_code=\u0026#34;200\u0026#34;,envoy_cluster_name=\u0026#34;instance_2\u0026#34;} 63 envoy_cluster_external_upstream_rq{envoy_response_code=\u0026#34;200\u0026#34;,envoy_cluster_name=\u0026#34;instance_3\u0026#34;} 38 如果我们计算一下百分比，我们会发现它们与我们在配置中设置的百分比相对应。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"71a7a7b44d49f7a6adccd38aca581f85","permalink":"https://lib.jimmysong.io/envoy-handbook/hcm/lab2/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/envoy-handbook/hcm/lab2/","section":"envoy-handbook","summary":"在这个实验中，我们将学习如何使用运行时分数和加权集群来配置 Envoy 的流量分割。 使用运行时分数 当我们只有两个上游集群时，运行时分数是一种很好的流量分割方法。运行时分数的工作原理是提供一个运行时分数（例如分子和","tags":["Envoy"],"title":"实验2：流量分割","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"Pause 容器，又叫 Infra 容器，本文将探究该容器的作用与原理。\n我们知道在 kubelet 的配置中有这样一个参数：\nKUBELET_POD_INFRA_CONTAINER=--pod-infra-container-image=registry.access.redhat.com/rhel7/pod-infrastructure:latest 上面是 openshift 中的配置参数，kubernetes 中默认的配置参数是：\nKUBELET_POD_INFRA_CONTAINER=--pod-infra-container-image=gcr.io/google_containers/pause-amd64:3.0 Pause 容器，是可以自己来定义，官方使用的 gcr.io/google_containers/pause-amd64:3.0 容器的代码见 Github，使用 C 语言编写。\nPause 容器特点  镜像非常小，目前在 700KB 左右 永远处于 Pause (暂停) 状态  Pause 容器背景 像 Pod 这样一个东西，本身是一个逻辑概念。那在机器上，它究竟是怎么实现的呢？这就是我们要解释的一个问题。\n既然说 Pod 要解决这个问题，核心就在于如何让一个 Pod 里的多个容器之间最高效的共享某些资源和数据。\n因为容器之间原本是被 Linux Namespace 和 cgroups 隔开的，所以现在实际要解决的是怎么去打破这个隔离，然后共享某些事情和某些信息。这就是 Pod 的设计要解决的核心问题所在。\n所以说具体的解法分为两个部分：网络和存储。\nPause 容器就是为解决 Pod 中的网络问题而生的。\nPause 容器实现 Pod 里的多个容器怎么去共享网络？下面是个例子：\n比如说现在有一个 Pod，其中包含了一个容器 A 和一个容器 B，它们两个就要共享 Network Namespace。在 Kubernetes 里的解法是这样的：它会在每个 Pod 里，额外起一个 Infra container 小容器来共享整个 Pod 的 Network Namespace。\nInfra container 是一个非常小的镜像，大概 700KB 左右，是一个 C 语言写的、永远处于 “暂停” 状态的容器。由于有了这样一个 Infra container 之后，其他所有容器都会通过 Join Namespace 的方式加入到 Infra container 的 Network Namespace 中。\n所以说一个 Pod 里面的所有容器，它们看到的网络视图是完全一样的。即：它们看到的网络设备、IP 地址、Mac 地址等等，跟网络相关的信息，其实全是一份，这一份都来自于 Pod 第一次创建的这个 Infra container。这就是 Pod 解决网络共享的一个解法。\n在 Pod 里面，一定有一个 IP 地址，是这个 Pod 的 Network Namespace 对应的地址，也是这个 Infra container 的 IP 地址。所以大家看到的都是一份，而其他所有网络资源，都是一个 Pod 一份，并且被 Pod 中的所有容器共享。这就是 Pod 的网络实现方式。\n由于需要有一个相当于说中间的容器存在，所以整个 Pod 里面，必然是 Infra container 第一个启动。并且整个 Pod 的生命周期是等同于 Infra container 的生命周期的，与容器 A 和 B 是无关的。这也是为什么在 Kubernetes 里面，它是允许去单独更新 Pod 里的某一个镜像的，即：做这个操作，整个 Pod 不会重建，也不会重启，这是非常重要的一个设计。\nPause 容器的作用 我们检查 node 节点的时候会发现每个 node 上都运行了很多的 pause 容器，例如如下。\n$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 2c7d50f1a7be docker.io/jimmysong/heapster-grafana-amd64@sha256:d663759b3de86cf62e64a43b021f133c383e8f7b0dc2bdd78115bc95db371c9a \u0026#34;/run.sh\u0026#34; 3 hours ago Up 3 hours k8s_grafana_monitoring-influxdb-grafana-v4-5697c6b59-76zqs_kube-system_5788a3c5-29c0-11e8-9e88-525400005732_0 5df93dea877a docker.io/jimmysong/heapster-influxdb-amd64@sha256:a217008b68cb49e8f038c4eeb6029261f02adca81d8eae8c5c01d030361274b8 \u0026#34;influxd --config ...\u0026#34; 3 hours ago Up 3 hours k8s_influxdb_monitoring-influxdb-grafana-v4-5697c6b59-76zqs_kube-system_5788a3c5-29c0-11e8-9e88-525400005732_0 9cec6c0ef583 jimmysong/pause-amd64:3.0 \u0026#34;/pause\u0026#34; 3 hours ago Up 3 hours k8s_POD_monitoring-influxdb-grafana-v4-5697c6b59-76zqs_kube-system_5788a3c5-29c0-11e8-9e88-525400005732_0 54d06e30a4c7 docker.io/jimmysong/kubernetes-dashboard-amd64@sha256:668710d034c4209f8fa9a342db6d8be72b6cb5f1f3f696cee2379b8512330be4 \u0026#34;/dashboard --inse...\u0026#34; 3 hours ago Up 3 hours k8s_kubernetes-dashboard_kubernetes-dashboard-65486f5fdf-lshl7_kube-system_27c414a1-29c0-11e8-9e88-525400005732_0 5a5ef33b0d58 jimmysong/pause-amd64:3.0 \u0026#34;/pause\u0026#34; 3 hours ago Up 3 hours k8s_POD_kubernetes-dashboard-65486f5fdf-lshl7_kube-system_27c414a1-29c0-11e8-9e88-525400005732_0 kubernetes 中的 pause 容器主要为每个业务容器提供以下功能：\n 在 pod 中担任 Linux 命名空间共享的基础； 启用 pid 命名空间，开启 init 进程。  这篇文章做出了详细的说明，pause 容器的作用可以从这个例子中看出，首先见下图：\n   Pause 容器示意图  我们首先在节点上运行一个 pause 容器。\ndocker run -d --name pause -p 8880:80 --ipc=shareable jimmysong/pause-amd64:3.0 然后再运行一个 nginx 容器，nginx 将为 localhost:2368 创建一个代理。\n$ cat \u0026lt;\u0026lt;EOF \u0026gt;\u0026gt; nginx.conf error_log stderr; events { worker_connections 1024; } http { access_log /dev/stdout combined; server { listen 80 default_server; server_name example.com www.example.com; location / { proxy_pass http://127.0.0.1:2368; } } } EOF $ docker run -d --name nginx -v `pwd`/nginx.conf:/etc/nginx/nginx.conf --net=container:pause --ipc=container:pause --pid=container:pause nginx 然后再为 ghost 创建一个应用容器，这是一款博客软件。\n$ docker run -d --name ghost --net=container:pause --ipc=container:pause --pid=container:pause ghost 现在访问 http://localhost:8880/ 就可以看到 ghost 博客的界面了。\n解析\npause 容器将内部的 80 端口映射到宿主机的 8880 端口，pause 容器在宿主机上设置好了网络 namespace 后，nginx 容器加入到该网络 namespace 中，我们看到 nginx 容器启动的时候指定了 --net=container:pause，ghost 容器同样加入到了该网络 namespace 中，这样三个容器就共享了网络，互相之间就可以使用 localhost 直接通信，--ipc=contianer:pause --pid=container:pause 就是三个容器处于同一个 namespace 中，init 进程为 pause，这时我们进入到 ghost 容器中查看进程情况。\n# ps aux USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND root 1 0.0 0.0 1024 4 ? Ss 13:49 0:00 /pause root 5 0.0 0.1 32432 5736 ? Ss 13:51 0:00 nginx: master p systemd+ 9 0.0 0.0 32980 3304 ? S 13:51 0:00 nginx: worker p node 10 0.3 2.0 1254200 83788 ? Ssl 13:53 0:03 node current/in root 79 0.1 0.0 4336 812 pts/0 Ss 14:09 0:00 sh root 87 0.0 0.0 17500 2080 pts/0 R+ 14:10 0:00 ps aux 在 ghost 容器中同时可以看到 pause 和 nginx 容器的进程，并且 pause 容器的 PID 是 1。而在 Kubernetes 中容器的 PID=1 的进程即为容器本身的业务进程。\n参考  The Almighty Pause Container - ianlewis.org Kubernetes 之 Pause 容器 - o-my-chenjian.com  ","date":1653062400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"eb37c822f707db021037b2b0948d180c","permalink":"https://lib.jimmysong.io/kubernetes-handbook/objects/pause-container/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/objects/pause-container/","section":"kubernetes-handbook","summary":"Pause 容器，又叫 Infra 容器，本文将探究该容器的作用与原理。 我们知道在 kubelet 的配置中有这样一个参数： KUBELET_POD_INFRA_CONTAINER=--pod-infra-container-image=registry.access.redhat.com/rhel7/pod-infrastructure:latest 上面是 openshift 中的配置参数，kubernetes 中默认的配置参数是： KUBELET_POD_INFRA_CONTAINER=--pod-infra-container-image=gcr.io/google_containers/pause-amd64:3.0 Pause 容器，是可以自己来定义，官方使用的 gcr.io/google_containers/pause-amd64:3.0 容器的代码","tags":["Kubernetes"],"title":"Pause 容器","type":"book"},{"authors":null,"categories":["安全"],"content":"下面的例子是为每个团队或用户组，可以使用 kubectl 命令或 YAML 文件创建一个 Kubernetes 命名空间。应避免使用任何带有 kube 前缀的名称，因为它可能与 Kubernetes 系统保留的命名空间相冲突。\nKubectl 命令来创建一个命名空间。\nkubectl create namespace \u0026lt;insert-namespace-name-here\u0026gt; 要使用 YAML 文件创建命名空间，创建一个名为 my-namespace.yaml 的新文件，内容如下：\napiVersion:v1kind:Namespacemetadata:name:\u0026lt;insert-namespace-name-here\u0026gt;应用命名空间，使用：\nkubectl create –f ./my-namespace.yaml 要在现有的命名空间创建新的 Pod，请切换到所需的命名空间：\nkubectl config use-context \u0026lt;insert-namespace-here\u0026gt; 应用新的 Deployment，使用：\nkubectl apply -f deployment.yaml 另外，也可以用以下方法将命名空间添加到 kubectl 命令中：\nkubectl apply -f deployment.yaml --namespace=\u0026lt;insert-namespace-here\u0026gt; 或在 YAML 声明中的元数据下指定 namespace：\u0026lt;insert-namespace-here\u0026gt;。\n一旦创建，资源不能在命名空间之间移动。必须删除该资源，然后在新的命名空间中创建。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"1f49f13ed35a9a54054e8644205ad23e","permalink":"https://lib.jimmysong.io/kubernetes-hardening-guidance/appendix/d/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/kubernetes-hardening-guidance/appendix/d/","section":"kubernetes-hardening-guidance","summary":"下面的例子是为每个团队或用户组，可以使用 kubectl 命令或 YAML 文件创建一个 Kubernetes 命名空间。应避免使用任何带有 kube 前缀的名称，因为它可能与 Kubernetes 系统保留的命名空间相冲突。 Kubectl 命令来创建一个命名空间。 kubectl create namespace \u003cinsert-namespace-name-here\u003e 要使用 YAML 文件创建命名","tags":["Kubernetes","安全"],"title":"附录 D：命名空间示例","type":"book"},{"authors":null,"categories":["Envoy"],"content":"在这个实验中，我们将学习如何在不同的配置级别上操作请求和响应头。\n我们将使用一个单一的例子，它的作用如下：\n 对所有的请求添加一个响应头 lab: 3 在虚拟主机上添加一个请求头 vh: one 为 /json 路由匹配添加一个名为 json 响应头。响应头有来自请求头 hello 的值  我们将只有一个名为 single_cluster 的上游集群，监听端口为 3030。让我们运行监听该端口的 httpbin 容器：\ndocker run -d -p 3030:80 kennethreitz/httpbin 让我们创建遵循上述规则的 Envoy 配置：\nstatic_resources:listeners:- name:listener_0address:socket_address:address:0.0.0.0port_value:10000filter_chains:- filters:- name:envoy.filters.network.http_connection_managertyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManagerstat_prefix:hello_servicehttp_filters:- name:envoy.filters.http.routerroute_config:response_headers_to_add:- header:key:\u0026#34;lab\u0026#34;value:\u0026#34;3\u0026#34;virtual_hosts:- name:vh_1request_headers_to_add:- header:key:vhvalue:\u0026#34;one\u0026#34;domains:[\u0026#34;*\u0026#34;]routes:- match:prefix:\u0026#34;/json\u0026#34;route:cluster:single_clusterresponse_headers_to_add:- header:key:\u0026#34;json\u0026#34;value:\u0026#34;%REQ(hello)%\u0026#34;- match:prefix:\u0026#34;/\u0026#34;route:cluster:single_clusterclusters:- name:single_clusterconnect_timeout:5sload_assignment:cluster_name:single_clusterendpoints:- lb_endpoints:- endpoint:address:socket_address:address:127.0.0.1port_value:3030将上述 YAML 保存为 2-lab-3-header-manipulation-1.yaml 并运行 Envoy 代理：\nfunc-e run -c 2-lab-3-header-manipulation-1.yaml 让我们先对 /headers 做一个简单的请求，这将匹配 / 前缀：\n$ curl -v localhost:10000/headers ... \u0026lt; x-envoy-upstream-service-time: 2 \u0026lt; lab: 3 \u0026lt; { \u0026#34;headers\u0026#34;: { \u0026#34;Accept\u0026#34;: \u0026#34;*/*\u0026#34;, \u0026#34;Host\u0026#34;: \u0026#34;localhost:10000\u0026#34;, \u0026#34;User-Agent\u0026#34;: \u0026#34;curl/7.64.0\u0026#34;, \u0026#34;Vh\u0026#34;: \u0026#34;one\u0026#34;, \u0026#34;X-Envoy-Expected-Rq-Timeout-Ms\u0026#34;: \u0026#34;15000\u0026#34; } } 我们会注意到响应 Header lab: 3 被设置了。这来自路由配置，并将被添加到我们的所有请求中。\n在响应中，我们可以看到 Vh: one 头（注意这个大写字母来自 httpbin 代码）被添加到请求中。httpbin 的响应显示了所收到的头信息（即请求头信息）。\n让我们试着向 /json 路径发出请求。发送到该路径上的 httpbin 的请求将返回一个 JSON 样本。此外，这一次我们将包括一个名为 hello: world 的请求头。\n$ curl -v -H \u0026#34;hello: world\u0026#34; localhost:10000/json \u0026gt; GET /json HTTP/1.1 \u0026gt; Host: localhost:10000 \u0026gt; User-Agent: curl/7.64.0 \u0026gt; Accept: */* \u0026gt; hello: world \u0026gt; \u0026lt; HTTP/1.1 200 OK \u0026lt; server: envoy \u0026lt; json: world \u0026lt; lab: 3 ... 注意这次我们设置的请求头（hello: world），在响应路径上，我们看到 json: world 头，它的值来自我们设置的请求头。同样地，lab: 3 响应头被设置。请求头 Vh: one 也同时被设置，但这次我们看不到它，因为它没有被输出。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"01ab6cb1e9f882cfe69548eb58de4a99","permalink":"https://lib.jimmysong.io/envoy-handbook/hcm/lab3/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/envoy-handbook/hcm/lab3/","section":"envoy-handbook","summary":"在这个实验中，我们将学习如何在不同的配置级别上操作请求和响应头。 我们将使用一个单一的例子，它的作用如下： 对所有的请求添加一个响应头 lab: 3 在虚拟主机上添加一个请求头 vh: one 为 /json 路由匹配添加一个名为 json 响应头。响","tags":["Envoy"],"title":"实验3：Header操作","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"PodSecurityPolicy 类型的对象能够控制，是否可以向 Pod 发送请求，该 Pod 能够影响被应用到 Pod 和容器的 SecurityContext。\n什么是 Pod 安全策略？ Pod 安全策略 是集群级别的资源，它能够控制 Pod 运行的行为，以及它具有访问什么的能力。 PodSecurityPolicy对象定义了一组条件，指示 Pod 必须按系统所能接受的顺序运行。 它们允许管理员控制如下方面：\n   控制面 字段名称     已授权容器的运行 privileged   为容器添加默认的一组能力 defaultAddCapabilities   为容器去掉某些能力 requiredDropCapabilities   容器能够请求添加某些能力 allowedCapabilities   控制卷类型的使用 volumes   主机网络的使用 hostNetwork   主机端口的使用 hostPorts   主机 PID namespace 的使用 hostPID   主机 IPC namespace 的使用 hostIPC   主机路径的使用 allowedHostPaths   容器的 SELinux 上下文 seLinux   用户 ID runAsUser   配置允许的补充组 supplementalGroups   分配拥有 Pod 数据卷的 FSGroup fsGroup   必须使用一个只读的 root 文件系统 readOnlyRootFilesystem    Pod 安全策略 由设置和策略组成，它们能够控制 Pod 访问的安全特征。这些设置分为如下三类：\n 基于布尔值控制：这种类型的字段默认为最严格限制的值。 基于被允许的值集合控制：这种类型的字段会与这组值进行对比，以确认值被允许。 基于策略控制：设置项通过一种策略提供的机制来生成该值，这种机制能够确保指定的值落在被允许的这组值中。  RunAsUser  MustRunAs - 必须配置一个 range。使用该范围内的第一个值作为默认值。验证是否不在配置的该范围内。 MustRunAsNonRoot - 要求提交的 Pod 具有非零 runAsUser 值，或在镜像中定义了 USER 环境变量。不提供默认值。 RunAsAny - 没有提供默认值。允许指定任何 runAsUser 。  SELinux  MustRunAs - 如果没有使用预分配的值，必须配置 seLinuxOptions。默认使用 seLinuxOptions。验证 seLinuxOptions。 RunAsAny - 没有提供默认值。允许任意指定的 seLinuxOptions ID。  SupplementalGroups  MustRunAs - 至少需要指定一个范围。默认使用第一个范围的最小值。验证所有范围的值。 RunAsAny - 没有提供默认值。允许任意指定的 supplementalGroups ID。  FSGroup  MustRunAs - 至少需要指定一个范围。默认使用第一个范围的最小值。验证在第一个范围内的第一个 ID。 RunAsAny - 没有提供默认值。允许任意指定的 fsGroup ID。  控制卷 通过设置 PSP 卷字段，能够控制具体卷类型的使用。当创建一个卷的时候，与该字段相关的已定义卷可以允许设置如下值：\n azureFile azureDisk flocker flexVolume hostPath emptyDir gcePersistentDisk awsElasticBlockStore gitRepo secret nfs iscsi glusterfs persistentVolumeClaim rbd cinder cephFS downwardAPI fc configMap vsphereVolume quobyte photonPersistentDisk projected portworxVolume scaleIO storageos * (allow all volumes)  对新的 PSP，推荐允许的卷的最小集合包括：configMap、downwardAPI、emptyDir、persistentVolumeClaim、secret 和 projected。\n主机网络  HostPorts， 默认为 empty。HostPortRange 列表通过 min(包含) and max(包含) 来定义，指定了被允许的主机端口。  允许的主机路径  AllowedHostPaths 是一个被允许的主机路径前缀的白名单。空值表示所有的主机路径都可以使用。  许可 包含 PodSecurityPolicy 的 许可控制，允许控制集群资源的创建和修改，基于这些资源在集群范围内被许可的能力。\n许可使用如下的方式为 Pod 创建最终的安全上下文：\n 检索所有可用的 PSP。 生成在请求中没有指定的安全上下文设置的字段值。 基于可用的策略，验证最终的设置。  如果某个策略能够匹配上，该 Pod 就被接受。如果请求与 PSP 不匹配，则 Pod 被拒绝。\nPod 必须基于 PSP 验证每个字段。\n创建 Pod 安全策略 下面是一个 Pod 安全策略的例子，所有字段的设置都被允许：\napiVersion:extensions/v1beta1kind:PodSecurityPolicymetadata:name:permissivespec:seLinux:rule:RunAsAnysupplementalGroups:rule:RunAsAnyrunAsUser:rule:RunAsAnyfsGroup:rule:RunAsAnyhostPorts:- min:8000max:8080volumes:- \u0026#39;*\u0026#39;下载示例文件可以创建该策略，然后执行如下命令：\n$ kubectl create -f ./psp.yaml podsecuritypolicy \u0026#34;permissive\u0026#34; created 获取 Pod 安全策略列表 获取已存在策略列表，使用 kubectl get：\n$ kubectl get psp NAME PRIV CAPS SELINUX RUNASUSER FSGROUP SUPGROUP READONLYROOTFS VOLUMES permissive false [] RunAsAny RunAsAny RunAsAny RunAsAny false [*] privileged true [] RunAsAny RunAsAny RunAsAny RunAsAny false [*] restricted false [] RunAsAny MustRunAsNonRoot RunAsAny RunAsAny false [emptyDir secret downwardAPI configMap persistentVolumeClaim projected] 修改 Pod 安全策略 通过交互方式修改策略，使用 kubectl edit：\n$ kubectl edit psp permissive 该命令将打开一个默认文本编辑器，在这里能够修改策略。\n删除 Pod 安全策略 一旦不再需要一个策略，很容易通过 kubectl 删除它：\n$ kubectl delete psp permissive podsecuritypolicy \u0026#34;permissive\u0026#34; deleted 启用 Pod 安全策略 为了能够在集群中使用 Pod 安全策略，必须确保如下：\n 启用 API 类型 extensions/v1beta1/podsecuritypolicy（仅对 1.6 之前的版本） 启用许可控制器 PodSecurityPolicy 定义自己的策略  使用 RBAC 在 Kubernetes 1.5 或更新版本，可以使用 PodSecurityPolicy 来控制，对基于用户角色和组的已授权容器的访问。访问不同的 PodSecurityPolicy 对象，可以基于认证来控制。基于 Deployment、ReplicaSet 等创建的 Pod，限制访问 PodSecurityPolicy 对象，Controller Manager 必须基于安全 API 端口运行，并且不能够具有超级用户权限。\nPodSecurityPolicy 认证使用所有可用的策略，包括创建 Pod 的用户，Pod 上指定的服务账户（service acount）。当 Pod 基于 Deployment、ReplicaSet 创建时，它是创建 Pod 的 Controller Manager，所以如果基于非安全 API 端口运行，允许所有的 PodSecurityPolicy 对象，并且不能够有效地实现细分权限。用户访问给定的 PSP 策略有效，仅当是直接部署 Pod 的情况。当直接部署 Pod 时，应用 PodSecurityPolicy 控制基于角色和组的已授权容器的访问 。\n","date":1653062400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"d34d254803ad8cf857fe7148a32d00d1","permalink":"https://lib.jimmysong.io/kubernetes-handbook/objects/pod-security-policy/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/objects/pod-security-policy/","section":"kubernetes-handbook","summary":"PodSecurityPolicy 类型的对象能够控制，是否可以向 Pod 发送请求，该 Pod 能够影响被应用到 Pod 和容器的 SecurityContext。 什么是 Pod 安全策略？ Pod 安全策略 是集群级别的资源，它能够控制 Pod 运行的行为，以及它具有访问什么的能力","tags":["Kubernetes"],"title":"Pod 安全策略","type":"book"},{"authors":null,"categories":["安全"],"content":"网络策略根据使用的网络插件而不同。下面是一个网络策略的例子，参考 Kubernetes 文档将 nginx 服务的访问限制在带有标签访问的 Pod 上。\napiVersion:networking.k8s.io/v1kind:NetworkPolicymetadata:name:example-access-nginxnamespace:prod#这可以是任何一个命名空间，或者在不使用命名空间的情况下省略。spec:podSelector:matchLabels:app:nginxingress:- from:- podSelector:matchLabels:access:\u0026#34;true\u0026#34;新的 NetworkPolicy 可以通过以下方式应用：\nkubectl apply -f policy.yaml 一个默认的拒绝所有入口的策略：\napiVersion:networking.k8s.io/v1kind:NetworkPolicymetadata:name:deny-all-ingressspec:podSelector:{}policyType:- Ingress一个默认的拒绝所有出口的策略：\napiVersion:networking.k8s.io/v1kind:NetworkPolicymetadata:name:deny-all-egressspec:podSelector:{}policyType:- Egress","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"4046f068c4fc57190e32fbd2a973829a","permalink":"https://lib.jimmysong.io/kubernetes-hardening-guidance/appendix/e/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/kubernetes-hardening-guidance/appendix/e/","section":"kubernetes-hardening-guidance","summary":"网络策略根据使用的网络插件而不同。下面是一个网络策略的例子，参考 Kubernetes 文档将 nginx 服务的访问限制在带有标签访问的 Pod 上。 apiVersion:networking.k8s.io/v1kind:NetworkPo","tags":["Kubernetes","安全"],"title":"附录 E：网络策略示例","type":"book"},{"authors":null,"categories":["Envoy"],"content":"在这个实验中，我们将学习如何配置不同的重试策略。我们将使用 httpbin Docker 镜像，因为我们可以向不同的路径（例如 /status/[statuscode]）发送请求，而 httpbin 会以该状态码进行响应。\n确保你有 httpbin 容器在 3030 端口监听。\ndocker run -d -p 3030:80 kennethreitz/httpbin 让我们创建 Envoy 配置，定义一个关于 5xx 响应的简单重试策略。我们还将启用管理接口，这样我们就可以在指标中看到重试的报告。\nstatic_resources:listeners:- name:listener_0address:socket_address:address:0.0.0.0port_value:10000filter_chains:- filters:- name:envoy.filters.network.http_connection_managertyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManagerstat_prefix:hello_servicehttp_filters:- name:envoy.filters.http.routerroute_config:virtual_hosts:- name:httpbindomains:[\u0026#34;*\u0026#34;]routes:- match:prefix:\u0026#34;/\u0026#34;route:cluster:httpbinretry_policy:retry_on:\u0026#34;5xx\u0026#34;num_retries:5clusters:- name:httpbinconnect_timeout:5sload_assignment:cluster_name:single_clusterendpoints:- lb_endpoints:- endpoint:address:socket_address:address:127.0.0.1port_value:3030admin:address:socket_address:address:127.0.0.1port_value:9901将上述 YAML 保存为 2-lab-4-retries-1.yaml，然后运行 Envoy 代理：\nfunc-e run -c 2-lab-4-retries-1.yaml 让我们向 /status/500 路径发送一个单一请求。\n$ curl -v localhost:10000/status/500 ... \u0026lt; HTTP/1.1 500 Internal Server Error \u0026lt; server: envoy ... \u0026lt; content-length: 0 \u0026lt; x-envoy-upstream-service-time: 276 正如预期的那样，我们收到了一个 500 响应。另外，注意到 x-envoy-upstream-service-time（上游主机处理该请求所花费的时间，以毫秒为单位）比我们发送 /status/200 请求时要大得多。\n$ curl localhost:10000/status/200 ... \u0026lt; HTTP/1.1 200 OK \u0026lt; server: envoy ... \u0026lt; content-length: 0 \u0026lt; x-envoy-upstream-service-time: 2 这是因为 Envoy 执行了重试，但最后还是失败了。同样，如果我们在管理界面（http://localhost:9901/stats/prometheus\u0026gt;）上打开统计页面，我们会发现代表重试次数的指标（envoy_cluster_retry_upstream_rq）的数值为 5。\n# TYPE envoy_cluster_retry_upstream_rq counter envoy_cluster_retry_upstream_rq{envoy_response_code=\u0026#34;500\u0026#34;,envoy_cluster_name=\u0026#34;httpbin\u0026#34;} 5 ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"3d9f571e6366b7cd755e712db57ad708","permalink":"https://lib.jimmysong.io/envoy-handbook/hcm/lab4/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/envoy-handbook/hcm/lab4/","section":"envoy-handbook","summary":"在这个实验中，我们将学习如何配置不同的重试策略。我们将使用 httpbin Docker 镜像，因为我们可以向不同的路径（例如 /status/[statuscode]）发送请求，而 httpbin 会以该状态码进行响应。 确保你有 httpbin 容器在 3030 端口监","tags":["Envoy"],"title":"实验4:重试","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"本文讲解的是 Kubernetes 中 Pod 的生命周期，包括生命周期的不同阶段、存活和就绪探针、重启策略等。\nPod phase Pod 的 status 字段是一个 PodStatus 对象，PodStatus中有一个 phase 字段。\nPod 的相位（phase）是 Pod 在其生命周期中的简单宏观概述。该字段并不是对容器或 Pod 的综合汇总，也不是为了做为综合状态机。\nPod 相位的数量和含义是严格指定的。除了本文档中列举的状态外，不应该再假定 Pod 有其他的 phase 值。\n下面是 phase 可能的值：\n 挂起（Pending）：Pod 已被 Kubernetes 系统接受，但有一个或者多个容器镜像尚未创建。等待时间包括调度 Pod 的时间和通过网络下载镜像的时间，这可能需要花点时间。 运行中（Running）：该 Pod 已经绑定到了一个节点上，Pod 中所有的容器都已被创建。至少有一个容器正在运行，或者正处于启动或重启状态。 成功（Succeeded）：Pod 中的所有容器都被成功终止，并且不会再重启。 失败（Failed）：Pod 中的所有容器都已终止了，并且至少有一个容器是因为失败终止。也就是说，容器以非0状态退出或者被系统终止。 未知（Unknown）：因为某些原因无法取得 Pod 的状态，通常是因为与 Pod 所在主机通信失败。  下图是Pod的生命周期示意图，从图中可以看到Pod状态的变化。\n   Pod的生命周期示意图（图片来自网络）  Pod 状态 Pod 有一个 PodStatus 对象，其中包含一个 PodCondition 数组。 PodCondition 数组的每个元素都有一个 type 字段和一个 status 字段。type 字段是字符串，可能的值有 PodScheduled、Ready、Initialized、Unschedulable和ContainersReady。status 字段是一个字符串，可能的值有 True、False 和 Unknown。\n容器探针 探针是由 kubelet 对容器执行的定期诊断。要执行诊断，kubelet 调用由容器实现的 Handler。有三种类型的处理程序：\n ExecAction：在容器内执行指定命令。如果命令退出时返回码为 0 则认为诊断成功。 TCPSocketAction：对指定端口上的容器的 IP 地址进行 TCP 检查。如果端口打开，则诊断被认为是成功的。 HTTPGetAction：对指定的端口和路径上的容器的 IP 地址执行 HTTP Get 请求。如果响应的状态码大于等于200 且小于 400，则诊断被认为是成功的。  每次探测都将获得以下三种结果之一：\n 成功：容器通过了诊断。 失败：容器未通过诊断。 未知：诊断失败，因此不会采取任何行动。  Kubelet 可以选择是否执行在容器上运行的两种探针执行和做出反应：\n livenessProbe：指示容器是否正在运行。如果存活探测失败，则 kubelet 会杀死容器，并且容器将受到其 重启策略 的影响。如果容器不提供存活探针，则默认状态为 Success。 readinessProbe：指示容器是否准备好服务请求。如果就绪探测失败，端点控制器将从与 Pod 匹配的所有 Service 的端点中删除该 Pod 的 IP 地址。初始延迟之前的就绪状态默认为 Failure。如果容器不提供就绪探针，则默认状态为 Success。  该什么时候使用存活（liveness）和就绪（readiness）探针? 如果容器中的进程能够在遇到问题或不健康的情况下自行崩溃，则不一定需要存活探针; kubelet 将根据 Pod 的restartPolicy 自动执行正确的操作。\n如果您希望容器在探测失败时被杀死并重新启动，那么请指定一个存活探针，并指定restartPolicy 为 Always 或 OnFailure。\n如果要仅在探测成功时才开始向 Pod 发送流量，请指定就绪探针。在这种情况下，就绪探针可能与存活探针相同，但是 spec 中的就绪探针的存在意味着 Pod 将在没有接收到任何流量的情况下启动，并且只有在探针探测成功后才开始接收流量。\n如果您希望容器能够自行维护，您可以指定一个就绪探针，该探针检查与存活探针不同的端点。\n请注意，如果您只想在 Pod 被删除时能够排除请求，则不一定需要使用就绪探针；在删除 Pod 时，Pod 会自动将自身置于未完成状态，无论就绪探针是否存在。当等待 Pod 中的容器停止时，Pod 仍处于未完成状态。\nreadinessGates 自 Kubernetes 1.14（该版本 readinessGates GA，在1.11 版本是为 alpha）起默认支持 Pod 就绪检测机制扩展。\n应用程序可以向 PodStatus 注入额外的反馈或信号：Pod readiness。要使用这个功能，请在 PodSpec 中设置 readinessGates 来指定 kubelet 评估 Pod readiness 的附加条件列表。\nReadiness gates 由 Pod 的 status.condition 字段的当前状态决定。如果 Kubernetes 在 Pod 的 status.conditions 字段中找不到这样的条件，则该条件的状态默认为 “False”。\n下面是一个例子。\nkind:Pod...spec:readinessGates:- conditionType:\u0026#34;www.example.com/feature-1\u0026#34;status:conditions:- type:Ready # 内置的 Pod 状态status:\u0026#34;False\u0026#34;lastProbeTime:nulllastTransitionTime:2018-01-01T00:00:00Z- type:\u0026#34;www.example.com/feature-1\u0026#34;# 附加的额外的 Pod 状态status:\u0026#34;False\u0026#34;lastProbeTime:nulllastTransitionTime:2018-01-01T00:00:00ZcontainerStatuses:- containerID:docker://abcd...ready:true...您添加的 Pod 条件的名称必须符合 Kubernetes 的 label key 格式。\n只有到 Pod 中的所有容器状态都是 Ready，且 Pod 附加的额外状态检测的 readinessGates 条件也是 Ready 的时候，Pod 的状态才是 Ready。\nPod 和容器状态 有关 Pod 容器状态的详细信息，请参阅 PodStatus 和 ContainerStatus。请注意，报告的 Pod 状态信息取决于当前的 ContainerState。\n重启策略 PodSpec 中有一个 restartPolicy 字段，可能的值为 Always、OnFailure 和 Never。默认为 Always。 restartPolicy 适用于 Pod 中的所有容器。restartPolicy 仅指通过同一节点上的 kubelet 重新启动容器。失败的容器由 kubelet 以五分钟为上限的指数退避延迟（10秒，20秒，40秒…）重新启动，并在成功执行十分钟后重置。如 Pod 文档 中所述，一旦绑定到一个节点，Pod 将永远不会重新绑定到另一个节点。\nPod 的生命 一般来说，Pod 不会消失，直到人为销毁他们。这可能是一个人或控制器。这个规则的唯一例外是成功或失败的 phase 超过一段时间（由 master 确定）的Pod将过期并被自动销毁。\n有三种可用的控制器：\n  使用 Job 运行预期会终止的 Pod，例如批量计算。Job 仅适用于重启策略为 OnFailure 或 Never 的 Pod。\n  对预期不会终止的 Pod 使用 ReplicationController、ReplicaSet 和 Deployment ，例如 Web 服务器。 ReplicationController 仅适用于具有 restartPolicy 为 Always 的 Pod。\n  提供特定于机器的系统服务，使用 DaemonSet 为每台机器运行一个 Pod 。\n  所有这三种类型的控制器都包含一个 PodTemplate。建议创建适当的控制器，让它们来创建 Pod，而不是直接自己创建 Pod。这是因为单独的 Pod 在机器故障的情况下没有办法自动复原，而控制器却可以。\n如果节点死亡或与集群的其余部分断开连接，则 Kubernetes 将应用一个策略将丢失节点上的所有 Pod 的 phase 设置为 Failed。\n示例 高级 liveness 探针示例 存活探针由 kubelet 来执行，因此所有的请求都在 kubelet 的网络命名空间中进行。\napiVersion:v1kind:Podmetadata:labels:test:livenessname:liveness-httpspec:containers:- args:- /serverimage:k8s.gcr.io/livenesslivenessProbe:httpGet:# when \u0026#34;host\u0026#34; is not defined, \u0026#34;PodIP\u0026#34; will be used# host: my-host# when \u0026#34;scheme\u0026#34; is not defined, \u0026#34;HTTP\u0026#34; scheme will be used. Only \u0026#34;HTTP\u0026#34; and \u0026#34;HTTPS\u0026#34; are allowed# scheme: HTTPSpath:/healthzport:8080httpHeaders:- name:X-Custom-Headervalue:AwesomeinitialDelaySeconds:15timeoutSeconds:1name:liveness状态示例  Pod 中只有一个容器并且正在运行。容器成功退出。  记录完成事件。 如果 restartPolicy 为：  Always：重启容器；Pod phase 仍为 Running。 OnFailure：Pod phase 变成 Succeeded。 Never：Pod phase 变成 Succeeded。     Pod 中只有一个容器并且正在运行。容器退出失败。  记录失败事件。 如果 restartPolicy 为：  Always：重启容器；Pod phase 仍为 Running。 OnFailure：重启容器；Pod phase 仍为 Running。 Never：Pod phase 变成 Failed。     Pod 中有两个容器并且正在运行。容器1退出失败。   记录失败事件。\n  如果 restartPolicy 为：\n Always：重启容器；Pod phase 仍为 Running。 OnFailure：重启容器；Pod phase 仍为 Running。 Never：不重启容器；Pod phase 仍为 Running。    如果有容器1没有处于运行状态，并且容器2退出：\n 记录失败事件。 如果 restartPolicy 为：  Always：重启容器；Pod phase 仍为 Running。 OnFailure：重启容器；Pod phase 仍为 Running。 Never：Pod phase 变成 Failed。       Pod 中只有一个容器并处于运行状态。容器运行时内存超出限制：  容器以失败状态终止。 记录 OOM 事件。 如果 restartPolicy 为：  Always：重启容器；Pod phase 仍为 Running。 OnFailure：重启容器；Pod phase 仍为 Running。 Never: 记录失败事件；Pod phase …","date":1653062400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"eadade8131fd7c09397320d864572140","permalink":"https://lib.jimmysong.io/kubernetes-handbook/objects/pod-lifecycle/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/objects/pod-lifecycle/","section":"kubernetes-handbook","summary":"本文讲解的是 Kubernetes 中 Pod 的生命周期，包括生命周期的不同阶段、存活和就绪探针、重启策略等。 Pod phase Pod 的 status 字段是一个 PodStatus 对象，PodStatus中有一个 phase 字段。 Pod 的相位（phase）是 Pod 在其生命周期中的简单宏观概述。","tags":["Kubernetes"],"title":"Pod 的生命周期","type":"book"},{"authors":null,"categories":["安全"],"content":"在 Kubernetes 1.10 和更新版本中，LimitRange 支持被默认启用。下面的 YAML 文件为每个容器指定了一个 LimitRange，其中有一个默认的请求和限制，以及最小和最大的请求。\napiVersion:v1kind:LimitRangemetadata:name:cpu-min-max-demo-lrspec:limits- default:cpu:1defaultRequest:cpu:0.5max:cpu:2min:cpu 0.5type:ContainerLimitRange 可以应用于命名空间，使用：\nkubectl apply -f \u0026lt;example-LimitRange\u0026gt;.yaml --namespace=\u0026lt;Enter-Namespace\u0026gt; 在应用了这个 LimitRange 配置的例子后，如果没有指定，命名空间中创建的所有容器都会被分配到默认的 CPU 请求和限制。命名空间中的所有容器的 CPU 请求必须大于或等于最小值，小于或等于最大 CPU 值，否则容器将不会被实例化。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"672aeaed1d8c5ac68a293a5b8023d7fb","permalink":"https://lib.jimmysong.io/kubernetes-hardening-guidance/appendix/f/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/kubernetes-hardening-guidance/appendix/f/","section":"kubernetes-hardening-guidance","summary":"在 Kubernetes 1.10 和更新版本中，LimitRange 支持被默认启用。下面的 YAML 文件为每个容器指定了一个 LimitRange，其中有一个默认的请求和限制，以及最小和最大的请求。 apiVersion:v1kind:LimitRangemetadata:name:cpu-min-max-demo-lrspec:limits- default:cpu:1defaultRequest:cpu:0.5max:cpu:2min:cpu 0.5type:ContainerLimitRange 可以应用于命名空间，使用： kubectl apply -f \u003cexample-LimitRange\u003e.yaml","tags":["Kubernetes","安全"],"title":"附录 F：LimitRange 示例","type":"book"},{"authors":null,"categories":["Envoy"],"content":"在这个实验中，我们将学习如何配置一个局部速率限制器。我们将使用运行在 3030 端口的 httpbin 容器。\ndocker run -d -p 3030:80 kennethreitz/httpbin 让我们创建一个有五个令牌的速率限制器。每隔 30 秒，速率限制器就会向桶里补充 5 个令牌。\nstatic_resources:listeners:- name:listener_0address:socket_address:address:0.0.0.0port_value:10000filter_chains:- filters:- name:envoy.filters.network.http_connection_managertyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManagerstat_prefix:ingress_httproute_config:name:local_routevirtual_hosts:- name:instance_1domains:[\u0026#34;*\u0026#34;]routes:- match:prefix:/statusroute:cluster:instance_1- match:prefix:/headersroute:cluster:instance_1typed_per_filter_config:envoy.filters.http.local_ratelimit:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.http.local_ratelimit.v3.LocalRateLimitstat_prefix:headers_routetoken_bucket:max_tokens:5tokens_per_fill:5fill_interval:30sfilter_enabled:default_value:numerator:100denominator:HUNDREDfilter_enforced:default_value:numerator:100denominator:HUNDREDresponse_headers_to_add:- append:falseheader:key:x-rate-limitedvalue:OH_NOhttp_filters:- name:envoy.filters.http.local_ratelimittyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.http.local_ratelimit.v3.LocalRateLimitstat_prefix:httpbin_rate_limiter- name:envoy.filters.http.routerclusters:- name:instance_1connect_timeout:0.25stype:STATIClb_policy:ROUND_ROBINload_assignment:cluster_name:instance_1endpoints:- lb_endpoints:- endpoint:address:socket_address:address:127.0.0.1port_value:3030admin:address:socket_address:address:127.0.0.1port_value:9901将上述 YAML 保存为 2-lab-5-local-rate-limiter-1.yaml，用 func-e run -c 2-lab-5-local-rate-limiter-1.yaml 运行 Envoy。\n上述配置为路由 /headers 启用了一个局部速率限制器。此外，一旦达到速率限制，我们将在响应中添加一个头信息（x-rate-limited）。\n如果我们在 30 秒内向 http://localhost:10000/headers 发出超过 5 个请求，我们会得到 HTTP 429 的响应。\n$ curl -v localhost:10000/headers ... \u0026gt; GET /headers HTTP/1.1 \u0026gt; Host: localhost:10000 \u0026gt; User-Agent: curl/7.64.0 \u0026gt; Accept: */* \u0026gt; \u0026lt; HTTP/1.1 429 Too Many Requests \u0026lt; x-rate-limited: OH_NO ... local_rate_limited 另外，注意到 Envoy 设置的 x-rate-limited 头。\n一旦我们被限制了速率，我们将不得不等待 30 秒，让速率限制器再次用令牌把桶填满。我们也可以尝试向 /status/200 发出请求，你会发现我们不会在这个路径上受到速率限制。\n如果我们打开统计页面（localhost:9901/stats/prometheus），我们会发现限速指标是使用我们配置的 headers_route_rate_limiter 统计前缀记录的。\n# TYPE envoy_headers_route_http_local_rate_limit_enabled counter envoy_headers_route_http_local_rate_limit_enabled{} 13 # TYPE envoy_headers_route_http_local_rate_limit_enforced counter envoy_headers_route_http_local_rate_limit_enforced{} 8 # TYPE envoy_headers_route_http_local_rate_limit_ok counter envoy_headers_route_http_local_rate_limit_ok{} 5 # TYPE envoy_headers_route_http_local_rate_limit_rate_limited counter envoy_headers_route_http_local_rate_limit_rate_limited{} 8 ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"468cdcdd427fd80cbe7001d56de6467d","permalink":"https://lib.jimmysong.io/envoy-handbook/hcm/lab5/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/envoy-handbook/hcm/lab5/","section":"envoy-handbook","summary":"在这个实验中，我们将学习如何配置一个局部速率限制器。我们将使用运行在 3030 端口的 httpbin 容器。 docker run -d -p 3030:80 kennethreitz/httpbin 让我们创建一个有五个令牌的速率限制器。每隔 30 秒，速率限制器就会向桶里补充 5 个令牌。 static_resources:listeners:- name:listener_0address:socket_address:address:0.0.0.0port_value:10000filter_chains:- filters:- name:envoy.filters.network.http_connection_managertyped_config:\"@type\": type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManagerstat_prefix:ingress_httproute_config:name:local_routevirtual_hosts:- name:instance_1domains:[\"*\"]routes:- match:prefix:/statusroute:cluster:instance_1- match:prefix:/headersroute:cluster:instance_1typed_per_filter_config:envoy.filters.http.local_ratelimit:\"@type\": type.googleapis.com/envoy.extensions.filters.http.local_ratelimit.v3.LocalRateLimitstat_prefix:headers_routetoken_bucket:max_tokens:5tokens_per_fill:5fill_interval:30sfilter_enabled:default_value:numerator:100denominator:HUNDREDfilter_enforced:default_value:numerator:100denominator:HUNDREDresponse_headers_to_add:- append:falseheader:key:x-rate-limitedvalue:OH_NOhttp_filters:- name:envoy.filters.http.local_ratelimittyped_config:\"@type\":","tags":["Envoy"],"title":"实验5：局部速率限制","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"Pod Hook（钩子）是由 Kubernetes 管理的 kubelet 发起的，当容器中的进程启动前或者容器中的进程终止之前运行，这是包含在容器的生命周期之中。可以同时为 Pod 中的所有容器都配置 hook。\nHook 的类型包括两种：\n exec：执行一段命令 HTTP：发送 HTTP 请求。  参考下面的配置：\napiVersion:v1kind:Podmetadata:name:lifecycle-demospec:containers:- name:lifecycle-demo-containerimage:nginxlifecycle:postStart:exec:command:[\u0026#34;/bin/sh\u0026#34;,\u0026#34;-c\u0026#34;,\u0026#34;echo Hello from the postStart handler\u0026gt; /usr/share/message\u0026#34;]preStop:exec:command:[\u0026#34;/usr/sbin/nginx\u0026#34;,\u0026#34;-s\u0026#34;,\u0026#34;quit\u0026#34;]postStart 在容器创建之后（但并不能保证钩子会在容器 ENTRYPOINT 之前）执行，这时候 Pod 已经被调度到某台 node 上，被某个 kubelet 管理了，这时候 kubelet 会调用 postStart 操作，该操作跟容器的启动命令是在同步执行的，也就是说在 postStart 操作执行完成之前，kubelet 会锁住容器，不让应用程序的进程启动，只有在 postStart 操作完成之后容器的状态才会被设置成为 RUNNING。\nPreStop 在容器终止之前被同步阻塞调用，常用于在容器结束前优雅的释放资源。\n如果 postStart 或者 preStop hook 失败，将会终止容器。\n调试 hook Hook 调用的日志没有暴露给 Pod 的 event，所以只能通过 describe 命令来获取，如果有错误将可以看到 FailedPostStartHook 或 FailedPreStopHook 这样的 event。\n参考  Attach Handlers to Container Lifecycle Events - kuberentes.io Container Lifecycle Hooks - kubernetes.io  ","date":1653062400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"489ef263bc47ccb1a45a442080e04ce4","permalink":"https://lib.jimmysong.io/kubernetes-handbook/objects/pod-hook/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/objects/pod-hook/","section":"kubernetes-handbook","summary":"Pod Hook（钩子）是由 Kubernetes 管理的 kubelet 发起的，当容器中的进程启动前或者容器中的进程终止之前运行，这是包含在容器的生命周期之中。可以同时为 Pod 中的所有容器都配置 hook。 Hook 的类型包括两种： exec：执行一段命令","tags":["Kubernetes"],"title":"Pod Hook","type":"book"},{"authors":null,"categories":["安全"],"content":"通过将 YAML 文件应用于命名空间或在 Pod 的配置文件中指定要求来创建 ResourceQuota 对象，以限制命名空间内的总体资源使用。下面的例子是基于 Kubernetes 官方文档的一个命名空间的配置文件示例：\napiVersion:v1kind:ResourceQuotametadata:name:example-cpu-mem-resourcequotaspec:hard:requests.cpu:\u0026#34;1\u0026#34;requests.memory:1Gilimits.cpu:\u0026#34;2\u0026#34;limits.memory:2Gi可以这样应用这个 ResourceQuota：\nkubectl apply -f example-cpu-mem-resourcequota.yaml -- namespace=\u0026lt;insert-namespace-here\u0026gt; 这个 ResourceQuota 对所选择的命名空间施加了以下限制：\n 每个容器都必须有一个内存请求、内存限制、CPU 请求和 CPU 限制。 所有容器的总内存请求不应超过 1 GiB 所有容器的总内存限制不应超过 2 GiB 所有容器的 CPU 请求总量不应超过 1 个 CPU 所有容器的总 CPU 限制不应超过 2 个 CPU  ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"705c8ab8c0898acfa9093795645c5cfa","permalink":"https://lib.jimmysong.io/kubernetes-hardening-guidance/appendix/g/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/kubernetes-hardening-guidance/appendix/g/","section":"kubernetes-hardening-guidance","summary":"通过将 YAML 文件应用于命名空间或在 Pod 的配置文件中指定要求来创建 ResourceQuota 对象，以限制命名空间内的总体资源使用。下面的例子是基于 Kubernetes 官方文档的一个命名空间的配置文件示例： apiVersion:v1kind:Resou","tags":["Kubernetes","安全"],"title":"附录 G：ResourceQuota 示例","type":"book"},{"authors":null,"categories":["Envoy"],"content":"在这个实验中，我们将学习如何配置一个全局速率限制器。我们将使用速率限制器服务和一个 Redis 实例来跟踪令牌。我们将使用 Docker Compose 来运行 Redis 和速率限制器服务容器。\n让我们首先创建速率限制器服务的配置。\ndomain:my_domaindescriptors:- key:generic_keyvalue:instance_1descriptors:- key:header_matchvalue:get_requestrate_limit:unit:MINUTErequests_per_unit:5我们指定一个通用键（instance_1）和一个名为 header_match 的描述符，速率限制为 5 个请求 / 分钟。\n将上述文件保存到 /config/rl-config.yaml 文件中。\n现在我们可以运行 Docker Compose 文件，它将启动 Redis 和速率限制器服务。\nversion:\u0026#34;3\u0026#34;services:redis:image:redis:alpineexpose:- 6379ports:- 6379:6379networks:- ratelimit-network# Rate limit service configurationratelimit:image:envoyproxy/ratelimit:bd46f11bcommand:/bin/ratelimitports:- 10001:8081- 6070:6070depends_on:- redisnetworks:- ratelimit-networkvolumes:- $PWD/config:/data/config/configenvironment:- USE_STATSD=false- LOG_LEVEL=debug- REDIS_SOCKET_TYPE=tcp- REDIS_URL=redis:6379- RUNTIME_ROOT=/data- RUNTIME_SUBDIRECTORY=confignetworks:ratelimit-network:将上述文件保存为 rl-docker-compose.yaml，并使用下面的命令启动所有容器：\n$ docker-compose -f rl-docker-compose.yaml up 为了确保速率限制器服务正确读取配置，我们可以检查容器的输出或使用速率限制器服务的调试端口。\n$ curl localhost:6070/rlconfig my_domain.generic_key_instance_1.header_match_get_request: unit=MINUTE requests_per_unit=5 随着速率限制器和 Redis 的启动和运行，我们可以启动 httpbin 容器。\ndocker run -d -p 3030:80 kennethreitz/httpbin 接下来，我们将创建 Envoy 配置，定义速率限制动作。我们将设置描述符 instance_1 和 get_request，只要有 GET 请求被发送到 httpbin。\n在 http_filters 下，我们通过指定域名（my_domain）和指向 Envoy 可以用来到达速率限制服务的集群来配置 ratelimit 过滤器。\nstatic_resources:listeners:- name:listener_0address:socket_address:address:0.0.0.0port_value:10000filter_chains:- filters:- name:envoy.filters.network.http_connection_managertyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManagerstat_prefix:ingress_httproute_config:name:local_routevirtual_hosts:- name:namespace.local_servicedomains:[\u0026#34;*\u0026#34;]routes:- match:prefix:/route:cluster:instance_1rate_limits:- actions:- generic_key:descriptor_value:instance_1- header_value_match:descriptor_value:get_requestheaders:- name:\u0026#34;:method\u0026#34;exact_match:GEThttp_filters:- name:envoy.filters.http.ratelimittyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.http.ratelimit.v3.RateLimitdomain:my_domainenable_x_ratelimit_headers:DRAFT_VERSION_03rate_limit_service:transport_api_version:V3grpc_service:envoy_grpc:cluster_name:rate-limit- name:envoy.filters.http.routerclusters:- name:instance_1connect_timeout:0.25stype:STATIClb_policy:ROUND_ROBINload_assignment:cluster_name:instance_1endpoints:- lb_endpoints:- endpoint:address:socket_address:address:127.0.0.1port_value:3030- name:rate-limitconnect_timeout:1stype:STATIClb_policy:ROUND_ROBINprotocol_selection:USE_CONFIGURED_PROTOCOLhttp2_protocol_options:{}load_assignment:cluster_name:rate-limitendpoints:- lb_endpoints:- endpoint:address:socket_address:address:127.0.0.1port_value:10001admin:address:socket_address:address:127.0.0.1port_value:9901将上述 YAML 保存为 2-lab-6-global-rate-limiter-1.yaml，并使用 func-e run -c 2-lab-6-global-rate-limiter-1.yaml 运行代理。\n我们现在可以发送五个以上的请求，我们会得到速率限制。\n$ curl -v localhost:10000 ... \u0026lt; HTTP/1.1 429 Too Many Requests \u0026lt; x-envoy-ratelimited: true \u0026lt; x-ratelimit-limit: 5, 5;w=60 \u0026lt; x-ratelimit-remaining: 0 \u0026lt; x-ratelimit-reset: 25 ... 我们收到了 429 响应，以及表明我们受到速率限制的响应头；在受到速率限制之前我们可以发出多少个请求（x-ratelimit-remaining）以及速率限制何时重置（x-ratelimit-reset）。\n 下一章   ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"fe41f01b0d846ed907240ce823f9ca3e","permalink":"https://lib.jimmysong.io/envoy-handbook/hcm/lab6/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/envoy-handbook/hcm/lab6/","section":"envoy-handbook","summary":"在这个实验中，我们将学习如何配置一个全局速率限制器。我们将使用速率限制器服务和一个 Redis 实例来跟踪令牌。我们将使用 Docker Compose 来运行 Redis 和速率限制器服务容器。 让我们首先创建速率限制器服务的配置。 domain:my_domaindescriptors:- key:generic_keyvalue:instance_1descriptors:- key:head","tags":["Envoy"],"title":"实验6：全局速率限制","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"Preset 就是预设，有时候想要让一批容器在启动的时候就注入一些信息，比如 secret、volume、volume mount 和环境变量，而又不想一个一个的改这些 Pod 的 template，这时候就可以用到 PodPreset 这个资源对象了。\n本页是关于 PodPreset 的概述，该对象用来在 Pod 创建的时候向 Pod 中注入某些特定信息。该信息可以包括 secret、volume、volume mount 和环境变量。\n理解 Pod Preset Pod Preset 是用来在 Pod 被创建的时候向其中注入额外的运行时需求的 API 资源。\n您可以使用 label selector 来指定为哪些 Pod 应用 Pod Preset。\n使用 Pod Preset 使得 pod 模板的作者可以不必为每个 Pod 明确提供所有信息。这样一来，pod 模板的作者就不需要知道关于该服务的所有细节。\n关于该背景的更多信息，请参阅 PodPreset 的设计方案。\n如何工作 Kubernetes 提供了一个准入控制器（PodPreset），当其启用时，Pod Preset 会将应用创建请求传入到该控制器上。当有 Pod 创建请求发生时，系统将执行以下操作：\n 检索所有可用的 PodPresets。 检查 PodPreset 标签选择器上的标签，看看其是否能够匹配正在创建的 Pod 上的标签。 尝试将由 PodPreset 定义的各种资源合并到正在创建的 Pod 中。 出现错误时，在该 Pod 上引发记录合并错误的事件，PodPreset 不会注入任何资源到创建的 Pod 中。 注释刚生成的修改过的 Pod spec，以表明它已被 PodPreset 修改过。注释的格式为 podpreset.admission.kubernetes.io/podpreset-\u0026lt;pod-preset name\u0026gt;\u0026#34;: \u0026#34;\u0026lt;resource version\u0026gt;\u0026#34;。  每个 Pod 可以匹配零个或多个 Pod Prestet；并且每个 PodPreset 可以应用于零个或多个 Pod。 PodPreset 应用于一个或多个 Pod 时，Kubernetes 会修改 Pod Spec。对于 Env、EnvFrom 和 VolumeMounts 的更改，Kubernetes 修改 Pod 中所有容器的容器 spec；对于 Volume 的更改，Kubernetes 修改 Pod Spec。\n 注意：Pod Preset 可以在适当的时候修改 Pod spec 中的 spec.containers 字段。Pod Preset 中的资源定义将不会应用于 initContainers 字段。\n 禁用特定 Pod 的 Pod Preset 在某些情况下，您可能不希望 Pod 被任何 Pod Preset 所改变。在这些情况下，您可以在 Pod 的 Pod Spec 中添加注释：podpreset.admission.kubernetes.io/exclude：\u0026#34;true\u0026#34;。\n启用 Pod Preset 为了在群集中使用 Pod Preset，您必须确保以下内容：\n 您已启用 settings.k8s.io/v1alpha1/podpreset API 类型。例如，可以通过在 API server 的 --runtime-config 选项中包含 settings.k8s.io/v1alpha1=true 来完成此操作。 您已启用 PodPreset 准入控制器。 一种方法是将 PodPreset 包含在为 API server 指定的 --admission-control 选项值中。 您已经在要使用的命名空间中通过创建 PodPreset 对象来定义 PodPreset。  ","date":1653062400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"34523f3dd89ff67664f2f6e013671fcd","permalink":"https://lib.jimmysong.io/kubernetes-handbook/objects/pod-preset/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/objects/pod-preset/","section":"kubernetes-handbook","summary":"Preset 就是预设，有时候想要让一批容器在启动的时候就注入一些信息，比如 secret、volume、volume mount 和环境变量，而又不想一个一个的改这些 Pod 的 template，这时候就可以用到 PodPreset 这个资源对象了。 本","tags":["Kubernetes"],"title":"Pod Preset","type":"book"},{"authors":null,"categories":["安全"],"content":"要对秘密数据进行静态加密，下面的加密配置文件提供了一个例子，以指定所需的加密类型和加密密钥。将加密密钥存储在加密文件中只能稍微提高安全性。Secret 将被加密，但密钥将在 EncryptionConfiguration 文件中被访问。这个例子是基于 Kubernetes 的官方文档。\napiVersion:apiserver.config.k8s.io/v1kind:EncryptionConfigurationresources:- resources:- secretsproviders:- aescbc:keys:- name:key1secret:\u0026lt;base 64 encoded secret\u0026gt;- identity:{}要使用该加密文件进行静态加密，请在重启 API 服务器时设置 --encryption-provider-config 标志，并注明配置文件的位置。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"6ec4cf7b0153d0ffba5a2cb32f0e3f90","permalink":"https://lib.jimmysong.io/kubernetes-hardening-guidance/appendix/h/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/kubernetes-hardening-guidance/appendix/h/","section":"kubernetes-hardening-guidance","summary":"要对秘密数据进行静态加密，下面的加密配置文件提供了一个例子，以指定所需的加密类型和加密密钥。将加密密钥存储在加密文件中只能稍微提高安全性。Secret 将被加密，但密钥将在 EncryptionConfiguration 文件中被访问。这个例子是基于","tags":["Kubernetes","安全"],"title":"附录 H：加密示例","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"这篇文档适用于要构建高可用应用程序的所有者，因此他们需要了解 Pod 可能发生什么类型的中断。也适用于要执行自动集群操作的集群管理员，如升级和集群自动扩容。\n自愿中断和非自愿中断 Pod 不会消失，直到有人（人类或控制器）将其销毁，或者当出现不可避免的硬件或系统软件错误。\n我们把这些不可避免的情况称为应用的非自愿性中断。例如：\n 后端节点物理机的硬件故障 集群管理员错误地删除虚拟机（实例） 云提供商或管理程序故障使虚拟机消失 内核恐慌（kernel panic） 节点由于集群网络分区而从集群中消失 由于节点资源不足而将容器逐出  除资源不足的情况外，大多数用户应该都熟悉以下这些情况；它们不是特定于 Kubernetes 的。\n我们称这些情况为”自愿中断“。包括由应用程序所有者发起的操作和由集群管理员发起的操作。典型的应用程序所有者操作包括：\n 删除管理该 pod 的 Deployment 或其他控制器 更新了 Deployment 的 pod 模板导致 pod 重启 直接删除 pod（意外删除）  集群管理员操作包括：\n 排空（drain）节点进行修复或升级。 从集群中排空节点以缩小集群。 从节点中移除一个 pod，以允许其他 pod 使用该节点。  这些操作可能由集群管理员直接执行，也可能由集群管理员或集群托管提供商自动执行。\n询问您的集群管理员或咨询您的云提供商或发行文档，以确定是否为您的集群启用了任何自动中断源。如果没有启用，您可以跳过创建 Pod Disruption Budget（Pod 中断预算）。\n处理中断 以下是一些减轻非自愿性中断的方法：\n 确保您的 pod 请求所需的资源。 如果您需要更高的可用性，请复制您的应用程序。 （了解有关运行复制的无状态和有状态应用程序的信息。） 为了在运行复制应用程序时获得更高的可用性，请跨机架（使用反亲和性）或跨区域（如果使用多区域集群）分布应用程序。  自愿中断的频率各不相同。在 Kubernetes 集群上，根本没有自愿的中断。但是，您的集群管理员或托管提供商可能会运行一些导致自愿中断的附加服务。例如，节点软件更新可能导致自愿更新。另外，集群（节点）自动缩放的某些实现可能会导致碎片整理和紧缩节点的自愿中断。您的集群管理员或主机提供商应该已经记录了期望的自愿中断级别（如果有的话）。\nKubernetes 提供的功能可以满足在频繁地自动中断的同时运行高可用的应用程序。我们称之为“中断预算”。\n中断预算的工作原理 应用程序所有者可以为每个应用程序创建一个 PodDisruptionBudget 对象（PDB）。 PDB 将限制在同一时间自愿中断的复制应用程序中宕机的 Pod 的数量。例如，基于定额的应用程序希望确保运行的副本数量永远不会低于仲裁所需的数量。Web 前端可能希望确保提供负载的副本的数量永远不会低于总数的某个百分比。\n集群管理器和托管提供商应使用遵循 Pod Disruption Budgets 的工具，方法是调用Eviction API而不是直接删除 Pod。例如 kubectl drain 命令和 Kubernetes-on-GCE 集群升级脚本（cluster/gce/upgrade.sh）。\n当集群管理员想要排空节点时，可以使用 kubectl drain 命令。该命令会试图驱逐机器上的所有 pod。驱逐请求可能会暂时被拒绝，并且该工具会定期重试所有失败的请求，直到所有的 pod 都被终止，或者直到达到配置的超时时间。\nPDB 指定应用程序可以容忍的副本的数量，相对于应该有多少副本。例如，具有 spec.replicas：5 的 Deployment 在任何给定的时间都应该有 5 个 Pod。如果其 PDB 允许在某一时刻有 4 个副本，那么驱逐 API 将只允许仅有一个而不是两个 Pod 自愿中断。\n使用标签选择器来指定应用程序的一组 pod，这与应用程序的控制器（Deployment、StatefulSet 等）使用的相同。\nPod 控制器的 .spec.replicas 计算“预期的” pod 数量。使用对象的 .metadata.ownerReferences 值从控制器获取。\nPDB 不能阻止非自愿中断的发生，但是它们确实会影响预算。\n由于应用程序的滚动升级而被删除或不可用的 Pod 确实会计入中断预算，但控制器（如 Deployment 和 StatefulSet）在进行滚动升级时不受 PDB 的限制——在应用程序更新期间的故障处理是在控制器的规格（spec）中配置（了解更新 Deployment）。\n使用驱逐 API 驱逐 pod 时，pod 会被优雅地终止（请参阅 PodSpec 中的 terminationGracePeriodSeconds）。\nPDB 示例 假设集群有3个节点，node-1 到 node-3。集群中运行了一些应用，其中一个应用有3个副本，分别是 pod-a、pod-b 和 pod-c。另外，还有一个与它相关的不具有 PDB 的 pod，我们称为之为 pod-x。最初，所有 Pod 的分布如下：\n   node-1 node-2 node-3     pod-a available pod-b available pod-c available   pod-x available      所有的3个 pod 都是 Deployment 中的一部分，并且它们共同拥有一个 PDB，要求至少有3个 pod 中的2个始终处于可用状态。\n例如，假设集群管理员想要重启系统，升级内核版本来修复内核中的错误。集群管理员首先使用 kubectl drain 命令尝试排除 node-1。该工具试图驱逐 pod-a 和 pod-x。这立即成功。两个 Pod 同时进入终止状态。这时的集群处于这种状态：\n   node-1 draining node-2 node-3     pod-a terminating pod-b available pod-c available   pod-x terminating      Deployment 注意到其中有一个 pod 处于正在终止，因此会创建了一个 pod-d 来替换。由于 node-1 被封锁（cordon），它落在另一个节点上。同时其它控制器也创建了 pod-y 作为 pod-x 的替代品。\n（注意：对于 StatefulSet，pod-a 将被称为 pod-1，需要在替换之前完全终止，替代它的也称为 pod-1，但是具有不同的 UID，可以创建。否则，示例也适用于 StatefulSet。）\n当前集群的状态如下：\n   node-1 draining node-2 node-3     pod-a terminating pod-b available pod-c available   pod-x terminating pod-d starting pod-y    在某一时刻，pod 被终止，集群看起来像下面这样子：\n   node-1 drained node-2 node-3      pod-b available pod-c available    pod-d starting pod-y    此时，如果一个急躁的集群管理员试图排空（drain）node-2 或 node-3，drain 命令将被阻塞，因为对于 Deployment 只有2个可用的 pod，并且其 PDB 至少需要2个。经过一段时间，pod-d 变得可用。\n   node-1 drained node-2 node-3      pod-b available pod-c available    pod-d available pod-y    现在，集群管理员尝试排空 node-2。drain 命令将尝试按照某种顺序驱逐两个 pod，假设先是 pod-b，然后再 pod-d。它将成功驱逐 pod-b。但是，当它试图驱逐 pod-d 时，将被拒绝，因为这样对 Deployment 来说将只剩下一个可用的 pod。\nDeployment 将创建一个名为 pod-e 的 pod-b 的替代品。但是，集群中没有足够的资源来安排 pod-e。那么，drain 命令就会被阻塞。集群最终可能是这种状态：\n   node-1 drained node-2 drained node-3 no node       pod-c available pod-e pending    pod-d available pod-y     此时，集群管理员需要向集群中添加回一个节点以继续升级操作。\n您可以看到 Kubernetes 如何改变中断发生的速率，根据：\n 应用程序需要多少副本 正常关闭实例需要多长时间 启动新实例需要多长时间 控制器的类型 集群的资源能力  分离集群所有者和应用程序所有者角色 将集群管理者和应用程序所有者视为彼此知识有限的独立角色通常是很有用的。这种责任分离在这些情况下可能是有意义的：\n 当有许多应用程序团队共享一个 Kubernetes 集群，并且有自然的专业角色 使用第三方工具或服务来自动化集群管理  Pod Disruption Budget（Pod 中断预算） 通过在角色之间提供接口来支持这种角色分离。\n如果您的组织中没有这样的职责分离，则可能不需要使用 Pod 中断预算。\n如何在集群上执行中断操作 如果您是集群管理员，要对集群的所有节点执行中断操作，例如节点或系统软件升级，则可以使用以下选择：\n 在升级期间接受停机时间。 故障转移到另一个完整的副本集群。  没有停机时间，但是对于重复的节点和人工协调成本可能是昂贵的。   编写可容忍中断的应用程序和使用 PDB。  没有停机时间。 最小的资源重复。 允许更多的集群管理自动化。 编写可容忍中断的应用程序是很棘手的，但对于可容忍自愿中断，和支持自动调整以容忍非自愿中断，两者在工作上有大量的重叠。    参考   Disruptions - kubernetes.io\n  通过配置Pod Disruption Budget（Pod 中断预算）来执行保护应用程序的步骤。\n  了解更多关于排空节点的信息。\n  ","date":1653062400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"de5435eeeb2452a75e9f2d7a37fd0622","permalink":"https://lib.jimmysong.io/kubernetes-handbook/objects/pod-disruption-budget/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/objects/pod-disruption-budget/","section":"kubernetes-handbook","summary":"这篇文档适用于要构建高可用应用程序的所有者，因此他们需要了解 Pod 可能发生什么类型的中断。也适用于要执行自动集群操作的集群管理员，如升级和集群自动扩容。 自愿中断和非自愿中断 Pod 不会消失，直到有人（人类或控制","tags":["Kubernetes"],"title":"Pod 中断与 PDB（Pod 中断预算）","type":"book"},{"authors":null,"categories":["Istio"],"content":"Envoy sidecar 代理配置中包含以下四个部分：\n bootstrap：Envoy proxy 启动时候加载的静态配置。 listeners：监听器配置，使用 LDS 下发。 clusters：集群配置，静态配置中包括 xds-grpc 和 zipkin 地址，动态配置使用 CDS 下发。 routes：路由配置，静态配置中包括了本地监听的服务的集群信息，其中引用了 cluster，动态配置使用 RDS 下发。  每个部分中都包含静态配置与动态配置，其中 bootstrap 配置又是在集群启动的时候通过 sidecar 启动参数注入的，配置文件在 /etc/istio/proxy/envoy-rev0.json。\nEnvoy 的配置 dump 出来后的结构如下图所示。\n   Envoy 配置  由于 bootstrap 中的配置是来自 Envoy 启动时加载的静态文件，主要配置了节点信息、tracing、admin 和统计信息收集等信息，这不是本文的重点，大家可以自行研究。\n   bootstrap 配置  上图是 bootstrap 的配置信息。\nBootstrap 是 Envoy 中配置的根本来源，Bootstrap 消息中有一个关键的概念，就是静态和动态资源的之间的区别。例如 Listener或 Cluster 这些资源既可以从静态资源配置中获得也可以从动态资源中配置的 LDS 或 CDS 之类的 xDS 服务获取。\nListener Listener 顾名思义，就是监听器，监听 IP 地址和端口，然后根据策略转发。\nListener 的特点\n 每个 Envoy 进程中可以有多个 Listener，Envoy 与 Listener 之间是一对多的关系。 每个 Listener 中可以配置一条 filter 链表（filter_chains），Envoy 会根据 filter 顺序执行过滤。 Listener 可以监听下游的端口，也可以接收来自其他 listener 的数据，形成链式处理。 filter 是可扩展的。 可以静态配置，也可以使用 LDS 动态配置。 目前只能监听 TCP，UDP 还未支持。  Listener 的数据结构\nListener 的数据结构如下，除了 name、address 和 filter_chains 为必须配置之外，其他都为可选的。\n{ \u0026#34;name\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;address\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;filter_chains\u0026#34;: [], \u0026#34;use_original_dst\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;per_connection_buffer_limit_bytes\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;metadata\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;drain_type\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;listener_filters\u0026#34;: [], \u0026#34;transparent\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;freebind\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;socket_options\u0026#34;: [], \u0026#34;tcp_fast_open_queue_length\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;bugfix_reverse_write_filter_order\u0026#34;: \u0026#34;{...}\u0026#34; } 下面是关于上述数据结构中的常用配置解析。\n  name：该 listener 的 UUID，唯一限定名，默认60个字符，例如 10.254.74.159_15011，可以使用命令参数指定长度限制。\n  address：监听的逻辑/物理地址和端口号，例如\n\u0026#34;address\u0026#34;: { \u0026#34;socket_address\u0026#34;: { \u0026#34;address\u0026#34;: \u0026#34;10.254.74.159\u0026#34;, \u0026#34;port_value\u0026#34;: 15011 } }   filter_chains：这是一个列表，Envoy 中内置了一些通用的 filter，每种 filter 都有特定的数据结构，Envoy 会根据该配置顺序执行 filter。\n  use_original_dst：这是一个布尔值，如果使用 iptables 重定向连接，则代理接收的端口可能与原始目的地址的端口不一样。当此标志设置为 true 时，Listener 将重定向的连接切换到与原始目的地址关联的 Listener。如果没有与原始目的地址关联的 Listener，则连接由接收它的 Listener 处理。默认为 false。注意：该参数将被废弃，请使用原始目的地址的 Listener filter 替代。该参数的主要用途是，Envoy 通过监听 15001 端口将应用的流量截取后再由其他 Listener 处理而不是直接转发出去。\n  Cluster Cluster 是指 Envoy 连接的一组逻辑相同的上游主机。Envoy 通过服务发现来发现 cluster 的成员。可以选择通过主动健康检查来确定集群成员的健康状态。Envoy 通过负载均衡策略决定将请求路由到 cluster 的哪个成员。\nCluster 的特点\n 一组逻辑上相同的主机构成一个 cluster。 可以在 cluster 中定义各种负载均衡策略。 新加入的 cluster 需要一个热身的过程才可以给路由引用，该过程是原子的，即在 cluster 热身之前对于 Envoy 及 Service Mesh 的其余部分来说是不可见的。 可以通过多种方式来配置 cluster，例如静态类型、严格限定 DNS、逻辑 DNS、EDS 等。  Cluster 的数据结构\nCluster 的数据结构如下，除了 name 字段，其他都是可选的。\n{ \u0026#34;name\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;alt_stat_name\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;eds_cluster_config\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;connect_timeout\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;per_connection_buffer_limit_bytes\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;lb_policy\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;hosts\u0026#34;: [], \u0026#34;load_assignment\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;health_checks\u0026#34;: [], \u0026#34;max_requests_per_connection\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;circuit_breakers\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;tls_context\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;common_http_protocol_options\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;http_protocol_options\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;http2_protocol_options\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;extension_protocol_options\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;dns_refresh_rate\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;dns_lookup_family\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;dns_resolvers\u0026#34;: [], \u0026#34;outlier_detection\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;cleanup_interval\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;upstream_bind_config\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;lb_subset_config\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;ring_hash_lb_config\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;original_dst_lb_config\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;least_request_lb_config\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;common_lb_config\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;transport_socket\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;metadata\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;protocol_selection\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;upstream_connection_options\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;close_connections_on_host_health_failure\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;drain_connections_on_host_removal\u0026#34;: \u0026#34;...\u0026#34; } 下面是关于上述数据结构中的常用配置解析。\n name：如果你留意到作为 Sidecar 启动的 Envoy 的参数的会注意到 --max-obj-name-len 189，该选项用来用来指定 cluster 的名字，例如 inbound|9080||ratings.default.svc.cluster.local。该名字字符串由 | 分隔成四个部分，分别是 inbound 或 outbound 代表入向流量或出向流量、端口号、subcluster 名称、FQDN，其中 subcluster 名称将对应于 Istio DestinationRule 中配置的 subnet，如果是按照多版本按比例路由的话，该值可以是版本号。 type：即服务发现类型，支持的参数有 STATIC（缺省值）、STRICT_DNS、LOGICAL_DNS、EDS、ORIGINAL_DST。 hosts：这是个列表，配置负载均衡的 IP 地址和端口，只有使用了 STATIC、STRICT_DNS、LOGICAL_DNS 服务发现类型时才需要配置。 eds_cluster_config：如果使用 EDS 做服务发现，则需要配置该项目，其中包括的配置有 service_name 和 ads。  Route 我们在这里所说的路由指的是 HTTP 路由，这也使得 Envoy 可以用来处理网格边缘的流量。HTTP 路由转发是通过路由过滤器实现的。该过滤器的主要职能就是执行路由表中的指令。除了可以做重定向和转发，路由过滤器还需要处理重试、统计之类的任务。\nHTTP 路由的特点\n 前缀和精确路径匹配规则。 可跨越多个上游集群进行基于权重/百分比的路由。 基于优先级的路由。 基于哈希策略的路由。  Route 的数据结构\n{ \u0026#34;name\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;virtual_hosts\u0026#34;: [], \u0026#34;internal_only_headers\u0026#34;: [], \u0026#34;response_headers_to_add\u0026#34;: [], \u0026#34;response_headers_to_remove\u0026#34;: [], \u0026#34;request_headers_to_add\u0026#34;: [], \u0026#34;request_headers_to_remove\u0026#34;: [], \u0026#34;validate_clusters\u0026#34;: \u0026#34;{...}\u0026#34; } 下面是关于上述数据结构中的常用配置解析。\n name：该名字跟 envoy.http_connection_manager filter 中的 http_filters.rds.route_config_name 一致，在 Istio Service Mesh 中为 Envoy 下发的配置中的 Route 是以监听的端口号作为名字，而同一个名字下面的 virtual_hosts 可以有多个值（数组形式）。 virtual_hosts：因为 VirtualHosts 是 Envoy 中引入的一个重要概念，我们在下文将详细说明 virtual_hosts 的数据结构。 validate_clusters：这是一个布尔值，用来设置开启使用 cluster manager 来检测路由表引用的 cluster 是否有效。如果是路由表是通过 route_config 静态配置的则该值默认设置为 true，如果是使用 RDS 动态配置的话，则该值默认设置为 false。  route.VirtualHost VirtualHost 即上文中 Route 配置中的 virtual_hosts，VirtualHost 是路由配置中的顶级元素。每个虚拟主机都有一个逻辑名称以及一组根据传入请求的 host header 路由到它的域。这允许单个 Listener 为多个顶级域路径树提供服务。基于域选择了虚拟主机后 Envoy 就会处理路由以查看要路由到哪个上游集群或是否执行重定向。\nVirtualHost 的数 …","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"3e43ede9cec7b399968d8d614bbf4747","permalink":"https://lib.jimmysong.io/istio-handbook/data-plane/envoy-proxy-config-deep-dive/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/istio-handbook/data-plane/envoy-proxy-config-deep-dive/","section":"istio-handbook","summary":"Envoy sidecar 代理配置中包含以下四个部分： bootstrap：Envoy proxy 启动时候加载的静态配置。 listeners：监听器配置，使用 LDS 下发。 clusters：集群配置，静态配置中包括 xds-grpc 和 zipkin 地址，动态配置使用 CDS","tags":["Istio","Service Mesh"],"title":"Envoy 代理配置详解","type":"book"},{"authors":null,"categories":["Istio"],"content":"让我们以 Web 前端和 customer 服务为例，看看 Envoy 如何确定将请求从 Web 前端发送到 customer 服务（customers.default.svc.cluster.local）的位置。使用 istioctl proxy-config 命令，我们可以列出 web 前端 pod 的所有监听器。\n$ istioctl proxy-config listeners web-frontend-64455cd4c6-p6ft2 ADDRESS PORT MATCH DESTINATION 10.124.0.10 53 ALL Cluster: outbound|53||kube-dns.kube-system.svc.cluster.local 0.0.0.0 80 ALL PassthroughCluster 10.124.0.1 443 ALL Cluster: outbound|443||kubernetes.default.svc.cluster.local 10.124.3.113 443 ALL Cluster: outbound|443||istiod.istio-system.svc.cluster.local 10.124.7.154 443 ALL Cluster: outbound|443||metrics-server.kube-system.svc.cluster.local 10.124.7.237 443 ALL Cluster: outbound|443||istio-egressgateway.istio-system.svc.cluster.local 10.124.8.250 443 ALL Cluster: outbound|443||istio-ingressgateway.istio-system.svc.cluster.local 10.124.3.113 853 ALL Cluster: outbound|853||istiod.istio-system.svc.cluster.local 0.0.0.0 8383 ALL PassthroughCluster 0.0.0.0 15001 ALL PassthroughCluster 0.0.0.0 15006 ALL Inline Route: /* 0.0.0.0 15010 ALL PassthroughCluster 10.124.3.113 15012 ALL Cluster: outbound|15012||istiod.istio-system.svc.cluster.local 0.0.0.0 15014 ALL PassthroughCluster 0.0.0.0 15021 ALL Non-HTTP/Non-TCP 10.124.8.250 15021 ALL Cluster: outbound|15021||istio-ingressgateway.istio-system.svc.cluster.local 0.0.0.0 15090 ALL Non-HTTP/Non-TCP 10.124.7.237 15443 ALL Cluster: outbound|15443||istio-egressgateway.istio-system.svc.cluster.local 10.124.8.250 15443 ALL Cluster: outbound|15443||istio-ingressgateway.istio-system.svc.cluster.local 10.124.8.250 31400 ALL Cluster: outbound|31400||istio-ingressgateway.istio-system.svc.cluster.local 从 Web 前端到客户的请求是一个向外的 HTTP 请求，端口为 80。这意味着它被移交给了0.0.0.0:80的虚拟监听器。我们可以使用 Istio CLI 按地址和端口来过滤监听器。你可以添加-o json来获得监听器的 JSON 表示：\n$ istioctl proxy-config listeners web-frontend-58d497b6f8-lwqkg --address 0.0.0.0 --port 80 -o json ... \u0026#34;rds\u0026#34;: { \u0026#34;configSource\u0026#34;: {\u0026#34;ads\u0026#34;: {}, \u0026#34;resourceApiVersion\u0026#34;: \u0026#34;V3\u0026#34; }, \u0026#34;routeConfigName\u0026#34;: \u0026#34;80\u0026#34; }, ... Listener 使用 RDS（路由发现服务）来寻找路由配置（在我们的例子中是 80）。路由附属于监听器，包含将虚拟主机映射到集群的规则。这允许我们创建流量路由规则，因为 Envoy 可以查看头文件或路径（请求元数据）并对流量进行路由。\n一个路由（route）选择一个集群（cluster）。一个集群是一组接受流量的类似的上游主机 —— 它是一个端点的集合。例如，Web 前端服务的所有实例的集合就是一个集群。我们可以在一个集群内配置弹性功能，如断路器、离群检测和 TLS 配置。\n使用 routes 命令，我们可以通过名称过滤所有的路由来获得路由的详细信息。\n$ istioctl proxy-config routes web-frontend-58d497b6f8-lwqkg --name 80 -o json [ { \u0026#34;name\u0026#34;: \u0026#34;80\u0026#34;, \u0026#34;virtualHosts\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;customers.default.svc.cluster.local:80\u0026#34;, \u0026#34;domains\u0026#34;: [ \u0026#34;customers.default.svc.cluster.local\u0026#34;, \u0026#34;customers.default.svc.cluster.local:80\u0026#34;, \u0026#34;customers\u0026#34;, \u0026#34;customers:80\u0026#34;, \u0026#34;customers.default.svc.cluster\u0026#34;, \u0026#34;customers.default.svc.cluster:80\u0026#34;, \u0026#34;customers.default.svc\u0026#34;, \u0026#34;customers.default.svc:80\u0026#34;, \u0026#34;customers.default\u0026#34;, \u0026#34;customers.default:80\u0026#34;, \u0026#34;10.124.4.23\u0026#34;, \u0026#34;10.124.4.23:80\u0026#34; ], ], \u0026#34;routes\u0026#34;: [ { \u0026#34;match\u0026#34;: {\u0026#34;prefix\u0026#34;: \u0026#34;/\u0026#34;}, \u0026#34;route\u0026#34;: { \u0026#34;cluster\u0026#34;: \u0026#34;outbound|80|v1|customers.default.svc.cluster.local\u0026#34;, \u0026#34;timeout\u0026#34;: \u0026#34;0s\u0026#34;, \u0026#34;retryPolicy\u0026#34;: { \u0026#34;retryOn\u0026#34;: \u0026#34;connect-failure,refused-stream,unavailable,cancelled,retriable-status-codes\u0026#34;, \u0026#34;numRetries\u0026#34;: 2, \u0026#34;retryHostPredicate\u0026#34;: [ {\u0026#34;name\u0026#34;: \u0026#34;envoy.retry_host_predicates.previous_hosts\u0026#34;} ], \u0026#34;hostSelectionRetryMaxAttempts\u0026#34;: \u0026#34;5\u0026#34;, \u0026#34;retriableStatusCodes\u0026#34;: [503] }, \u0026#34;maxGrpcTimeout\u0026#34;: \u0026#34;0s\u0026#34; }, ... 路由80配置为每个服务都有一个虚拟主机。然而，由于我们的请求被发送到customers.default.svc.cluster.local，Envoy 会选择与其中一个域匹配的虚拟主机（customers.default.svc.cluster.local:80）。\n一旦域被匹配，Envoy 就会查看路由，并选择第一个匹配请求的路由。由于我们没有定义任何特殊的路由规则，它匹配第一个（也是唯一的）定义的路由，并指示 Envoy 将请求发送到名为 outbound|80|v1|customers.default.svc.cluster.local 的集群。\n 注意集群名称中的 v1 是因为我们部署了一个 DestinationRule 来创建 v1 子集。如果一个服务没有子集，这部分就留空：outbound|80||customers.default.svc.cluster.local。\n 现在我们有了集群的名称，我们可以查询更多的细节。为了得到一个清楚显示 FQDN、端口、子集和其他信息的输出，你可以省略 -o json 标志。\n$ istioctl proxy-config cluster web-frontend-58d497b6f8-lwqkg --fqdn customers.default.svc.cluster.local SERVICE FQDN PORT SUBSET DIRECTION TYPE DESTINATION RULE customers.default.svc.cluster.local 80 - outbound EDS customers.default customers.default.svc.cluster.local 80 v1 outbound EDS customers.default 最后，使用集群的名称，我们可以查询请求最终将到达的实际端点：\n$ istioctl proxy-config endpoints web-frontend-58d497b6f8-lwqkg --cluster \u0026#34;outbound|80|v1|customers.default.svc.cluster.local\u0026#34; ENDPOINT STATUS OUTLIER CHECK CLUSTER 10.120.0.4:3000 HEALTHY OK outbound|80|v1|customers.default.svc.cluster.local 端点地址等于客户应用程序正在运行的 pod IP。如果我们扩展 customer 的部署，额外的端点会出现在输出中，像这样：\n$ istioctl proxy-config endpoints web-frontend-58d497b6f8-lwqkg --cluster \u0026#34;outbound|80|v1|customers.default.svc.cluster.local\u0026#34; ENDPOINT STATUS OUTLIER CHECK CLUSTER 10.120.0.4:3000 HEALTHY OK outbound|80|v1|customers.default.svc.cluster.local 10.120.3.2:3000 HEALTHY OK outbound|80|v1|customers.default.svc.cluster.local 我们也可以用下图来形象地说明上述流程。\n   Envoy 详情  ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"026696dc7264bbc7e9f645dc2ab572e5","permalink":"https://lib.jimmysong.io/istio-handbook/troubleshooting/envoy-sample/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/istio-handbook/troubleshooting/envoy-sample/","section":"istio-handbook","summary":"让我们以 Web 前端和 customer 服务为例，看看 Envoy 如何确定将请求从 Web 前端发送到 customer 服务（customers.default.svc.cluster.local）的位置。使用 istioctl proxy-config 命令，我们可以列出 web 前端 pod 的所有监听器。 $","tags":["Istio","Service Mesh"],"title":"Envoy 示例","type":"book"},{"authors":null,"categories":["Istio"],"content":"作为 Istio 安装的一部分，我们安装了 Istio 的入口和出口网关。这两个网关都运行一个 Envoy 代理实例，它们在网格的边缘作为负载均衡器运行。入口网关接收入站连接，而出口网关接收从集群出去的连接。\n使用入口网关，我们可以对进入集群的流量应用路由规则。我们可以有一个指向入口网关的单一外部 IP 地址，并根据主机头将流量路由到集群内的不同服务。\n   入口和出口网关  我们可以使用 Gateway 资源来配置网关。网关资源描述了负载均衡器的暴露端口、协议、SNI（服务器名称指示）配置等。\n下面是一个网关资源的例子：\napiVersion:networking.istio.io/v1alpha3kind:Gatewaymetadata:name:my-gatewaynamespace:defaultspec:selector:istio:ingressgatewayservers:- port:number:80name:httpprotocol:HTTPhosts:- dev.example.com- test.example.com上述网关资源设置了一个代理，作为一个负载均衡器，为入口暴露 80 端口。网关配置被应用于 Istio 入口网关代理，我们将其部署到 istio-system 命名空间，并设置了标签 istio: ingressgateway。通过网关资源，我们只能配置负载均衡器。hosts 字段作为一个过滤器，只有以 dev.example.com 和 test.example.com 为目的地的流量会被允许通过。为了控制和转发流量到 Kubernetes 内部运行的实际服务，我们必须将 VirtualService 资源绑定到它。\n   Gateway 和 VirtualService  例如，我们作为 Istio 安装 demo 的一部分而部署的 Ingress 网关创建了一个具有 LoadBalancer 类型的 Kubernetes 服务，并为其分配了一个外部 IP：\n$ kubectl get svc -n istio-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE istio-egressgateway ClusterIP 10.0.146.214 \u0026lt;none\u0026gt; 80/TCP,443/TCP,15443/TCP 7m56s istio-ingressgateway LoadBalancer 10.0.98.7 XX.XXX.XXX.XXX 15021:31395/TCP,80:32542/TCP,443:31347/TCP,31400:32663/TCP,15443:31525/TCP 7m56s istiod ClusterIP 10.0.66.251 \u0026lt;none\u0026gt; 15010/TCP,15012/TCP,443/TCP,15014/TCP,853/TCP 8m6s  LoadBalancer Kubernetes 服务类型的工作方式取决于我们运行 Kubernetes 集群的方式和地点。对于云管理的集群（GCP、AWS、Azure等），在你的云账户中配置了一个负载均衡器资源，Kubernetes LoadBalancer 服务将获得一个分配给它的外部 IP 地址。假设我们正在使用 Minikube 或 Docker Desktop。在这种情况下，外部 IP 地址将被设置为 localhost（Docker Desktop），或者，如果我们使用 Minikube，它将保持待定，我们将不得不使用 minikube tunnel 命令来获得一个IP地址。\n 除了入口网关，我们还可以部署一个出口网关来控制和过滤离开网格的流量。\n就像我们配置入口网关一样，我们可以使用相同的网关资源来配置出口网关。这使我们能够集中管理所有流出的流量、日志和授权。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"fe00d869bdc5d1b09768f644c4e00f9c","permalink":"https://lib.jimmysong.io/istio-handbook/traffic-management/gateway/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/istio-handbook/traffic-management/gateway/","section":"istio-handbook","summary":"作为 Istio 安装的一部分，我们安装了 Istio 的入口和出口网关。这两个网关都运行一个 Envoy 代理实例，它们在网格的边缘作为负载均衡器运行。入口网关接收入站连接，而出口网关接收从集群出去的连接。 使用入口网关，我们可以对进入","tags":["Istio","Service Mesh"],"title":"Gateway","type":"book"},{"authors":null,"categories":["Istio"],"content":"Istio 是最受欢迎和发展最快的开源项目之一。它的发布时间表对企业的生命周期和变更管理实践来说可能非常激进。GetMesh 通过针对不同的 Kubernetes 分发版测试所有 Istio 版本以确保功能的完整性来解决这一问题。GetMesh 的 Istio 版本在安全补丁和其他错误更新方面得到积极的支持，并拥有比上游 Istio 提供的更长的支持期。\n一些服务网格客户需要支持更高的安全要求。GetMesh 通过提供两种 Istio 发行版来解决合规性问题。\n tetrate 发行版，跟踪上游 Istio 并可能应用额外的补丁。 tetratefips 发行版，是符合 FIPS 标准的 tetrate 版本。  如何开始使用？ 第一步是下载 GetMesh CLI。你可以在 macOS 和 Linux 平台上安装 GetMesh。我们可以使用以下命令来下载最新版本的 GetMesh 和认证的 Istio。\n如何开始？ 第一步是下载 GetMesh CLI。你可以在 macOS 和 Linux 平台上安装 GetMesh。我们可以使用以下命令下载最新版本的 GetMesh 并认证 Istio。\ncurl -sL https://istio.tetratelabs.io/getmesh/install.sh | bash 我们可以运行 version 命令以确保 GetMesh 被成功安装。例如：\n$ getmesh version getmesh version: 1.1.1 active istioctl: 1.8.3-tetrate-v0 no running Istio pods in \u0026#34;istio-system\u0026#34; 版本命令输出 GetMesh 的版本、活跃的 Istio CLI 的版本以及 Kubernetes 集群上安装的 Istio 的版本。\n使用 GetMesh 安装 Istio GetMesh 通过 Kubernetes 配置文件与活跃的 Kubernetes 集群进行通信。\n要在当前活动的 Kubernetes 集群上安装 Istio 的演示配置文件，我们可以像这样使用 getmesh istioctl 命令：\ngetmesh istioctl install --set profile=demo 该命令将检查集群，以确保它准备好安装 Istio，一旦你确认，安装程序将继续使用选定的配置文件安装 Istio。\n如果我们现在检查版本，你会注意到输出显示控制平面和数据平面的版本。\n验证配置 config-validate 命令允许你对当前配置和任何尚未应用的 YAML 清单进行验证。\n该命令使用外部资源调用一系列验证，如上游 Istio 验证、Kiali 库和 GetMesh 自定义配置检查。\n下面是一个命令输出的例子，如果没有标记为 Istio 注入的命名空间。\n$ getmesh config-validate Running the config validator. This may take some time... 2021-08-02T19:20:33.873244Z info klog Throttling request took 1.196458809s, request: GET:https://35.185.226.9/api/v1/namespaces/istio-system/configmaps/istio[] NAMESPACE NAME RESOURCE TYPE ERROR CODE SEVERITY MESSAGE default default Namespace IST0102 Info The namespace is not enabled for Istio injection. Run \u0026#39;kubectl label namespace default istio-injection=enabled\u0026#39; to enable it, or \u0026#39;kubectl label namespace default istio-injection=disabled\u0026#39; to explicitly mark it as not needing injection. The error codes of the found issues are prefixed by \u0026#39;IST\u0026#39; or \u0026#39;KIA\u0026#39;. For the detailed explanation, please refer to - https://istio.io/latest/docs/reference/config/analysis/ for \u0026#39;IST\u0026#39; error codes - https://kiali.io/documentation/latest/validations/ for \u0026#39;KIA\u0026#39; error codes 同样，你也可以传入一个 YAML 文件来验证它，然后再将它部署到集群。例如：\n$ getmesh config-validate my-resources.yaml 管理多个 Istio CLI 我们可以使用 show 命令来列出当前下载的 Istio 版本：\ngetmesh show 输出如下所示：\n1.8.2-tetrate-v0 1.8.3-tetrate-v0 (Active) 如果我们想使用的版本在电脑上没有，我们可以使用 getmesh list 命令来列出所有可信的 Istio 版本：\n$ getmesh list ISTIO VERSION FLAVOR FLAVOR VERSION K8S VERSIONS *1.9.5 tetrate 0 1.17,1.18,1.19,1.20 1.9.5 istio 0 1.17,1.18,1.19,1.20 1.9.4 tetrate 0 1.17,1.18,1.19,1.20 1.9.4 istio 0 1.17,1.18,1.19,1.20 1.9.0 tetrate 0 1.17,1.18,1.19,1.20 1.9.0 tetratefips 1 1.17,1.18,1.19,1.20 1.9.0 istio 0 1.17,1.18,1.19,1.20 1.8.6 tetrate 0 1.16,1.17,1.18,1.19 1.8.6 istio 0 1.16,1.17,1.18,1.19 1.8.5 tetrate 0 1.16,1.17,1.18,1.19 1.8.5 istio 0 1.16,1.17,1.18,1.19 1.8.3 tetrate 0 1.16,1.17,1.18,1.19 1.8.3 tetratefips 1 1.16,1.17,1.18,1.19 1.8.3 istio 0 1.16,1.17,1.18,1.19 1.7.8 tetrate 0 1.16,1.17,1.18 1.7.8 istio 0 1.16,1.17,1.18 要获取一个特定的版本（比方说1.8.2 tetratefips），我们可以使用 fetch 命令：\ngetmesh fetch --version 1.9.0 --flavor tetratefips --flavor-version 1 当上述命令完成后，GetMesh 将获取的 Istio CLI 版本设置为 Istio CLI 的活动版本。例如，运行 show 命令现在显示 tetratefips 1.9.0 版本是活跃的：\n$ getmesh show 1.9.0-tetratefips-v1 (Active) 1.9.5-tetrate-v0 同样，如果我们运行 getmesh istioctl version ，我们会发现正在使用的 Istio CLI 的版本：\n$ getmesh istioctl version client version: 1.9.0-tetratefips-v1 control plane version: 1.9.5-tetrate-v0 data plane version: 1.9.5-tetrate-v0 (2 proxies) 要切换到不同版本的 Istio CLI，我们可以运行 getmesh switch 命令：\ngetmesh switch --version 1.9.5 --flavor tetrate --flavor-version 0 CA 集成 我们没有使用自签的根证书，而是从 GCP CAS（证书授权服务）获得一个中间的 Istio 证书授权（CA）来签署工作负载证书。\n假设你已经配置了你的 CAS 实例，你可以用 CA 的参数创建一个 YAML 配置。下面是 YAML 配置的一个例子：\nproviderName:\u0026#34;gcp\u0026#34;providerConfig:gcp:# 这将持有你在 GCP 上创建的证书授权的完整 CA 名称casCAName:\u0026#34;projects/tetrate-io-istio/locations/us-west1/certificateAuthorities/tetrate-example-io\u0026#34;certificateParameters:secretOptions:istioCANamespace:\u0026#34;istio-system\u0026#34;# `cacerts` secrets 所在的命名空间overrideExistingCACertsSecret:true# 重写已存在的 `cacerts` secret，使用新的替换caOptions:validityDays:365# CA 到期前的有效天数keyLength:2048# 创建的 key 的比特数certSigningRequestParams:# x509.CertificateRequest；大部分字段省略subject:commonname:\u0026#34;tetrate.example.io\u0026#34;country:- \u0026#34;US\u0026#34;locality:- \u0026#34;Sunnyvale\u0026#34;organization:- \u0026#34;Istio\u0026#34;organizationunit:- \u0026#34;engineering\u0026#34;emailaddresses:- \u0026#34;youremail@example.io\u0026#34;配置到位后，你可以使用 gen-ca 命令来创建 cacert。\ngetmesh gen-ca --config-file gcp-cas-config.yaml 该命令在 istio-system 中创建 cacerts Kubernetes Secret。为了让 istiod 接受新的 cert，你必须重新启动 istiod。\n如果你创建一个 sample 工作负载，并检查所使用的证书，你会发现 CA 是发布工作负载的那个。\nIstio CA certs 集成可用于 GCP CA 服务和 AWS Private CA 服务。\n发现选择器 发现选择器是 Istio 1.10 中引入的新功能之一。发现选择器允许我们控制 Istio 控制平面观察和发送配置更新的命名空间。\n默认情况下，Istio 控制平面会观察和处理集群中所有 Kubernetes 资源的更新。服务网格中的所有 Envoy代理的配置方式是，它们可以到达服务网格中的每个工作负载，并接受与工作负载相关的所有端口的流量。\n例如，我们在不同的命名空间部署了两个工作负载——foo 和 bar。尽管我们知道 foo 永远不会与 bar 通信，反之亦然，但一个服务的端点将被包含在另一个服务的已发现端点列表中。\n   Foo bar  如果我们运行 istioctl proxy-config 命令，列出 foo 命名空间的 foo 工作负载可以看到的所有端点，你会注意到一个名为 bar 的服务条目：\n$ istioctl proxy-config endpoints deploy/foo.foo ENDPOINT STATUS OUTLIER …","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"4a0a0713a64f92fb4edb014ff64e162f","permalink":"https://lib.jimmysong.io/istio-handbook/setup/getmesh/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/istio-handbook/setup/getmesh/","section":"istio-handbook","summary":"Istio 是最受欢迎和发展最快的开源项目之一。它的发布时间表对企业的生命周期和变更管理实践来说可能非常激进。GetMesh 通过针对不同的 Kubernetes 分发版测试所有 Istio 版本以确保功能的完整性来解决这一问题。GetMesh 的","tags":["Istio","Service Mesh"],"title":"GetMesh","type":"book"},{"authors":null,"categories":["Envoy"],"content":"过滤器链匹配允许我们指定为监听器选择特定过滤器链的标准。\n我们可以在配置中定义多个过滤器链，然后根据目标端口、服务器名称、协议和其他属性来选择和执行它们。例如，我们可以检查哪个主机名正在连接，然后选择不同的过滤器链。如果主机名 hello.com 连接，我们可以选择一个过滤器链来呈现该特定主机名的证书。\n在 Envoy 开始过滤器匹配之前，它需要有一些由监听器过滤器从接收的数据包中提取的数据。之后，Envoy 要选择一个特定的过滤器链，必须满足所有的匹配条件。例如，如果我们对主机名和端口进行匹配，这两个值都需要匹配，Envoy 才能选择该过滤器链。\n匹配顺序如下：\n 目的地端口（当使用 use_original_dst 时） 目的地 IP 地址 服务器名称（TLS 协议的 SNI） 传输协议 应用协议（TLS 协议的 ALPN） 直接连接的源 IP 地址（这只在我们使用覆盖源地址的过滤器时与源 IP 地址不同，例如，代理协议监听器过滤器） 来源类型（例如，任何、本地或外部网络） 源 IP 地址 来源端口  具体标准，如服务器名称 / SNI 或 IP 地址，也允许使用范围或通配符。如果在多个过滤器链中使用通配符标准，最具体的值将被匹配。\n例如，对于 www.hello.com 从最具体到最不具体的匹配顺序是这样的。\n www.hello.com *.hello.com *.com 任何没有服务器名称标准的过滤器链  下面是一个例子，说明我们如何使用不同的属性配置过滤器链匹配。\nfilter_chains:- filter_chain_match:server_names:- \u0026#34;*.hello.com\u0026#34;filters:...- filter_chain_match:source_prefix_ranges:- address_prefix:192.0.0.1prefix_len:32filters:...- filter_chain_match:transport_protocol:tlsfilters:...让我们假设一个 TLS 请求从 IP 地址进来，并且 192.0.0.1 SNI 设置为 v1.hello.com。记住这个顺序，第一个满足所有条件的过滤器链匹配是服务器名称匹配（v1.hello.com）。因此，Envoy 会执行该匹配下的过滤器。\n但是，如果请求是从 IP 192.0.0.1 进来的，那就不是 TLS，而且 SNI 也不符合 *.hello.com 的要求。Envoy 将执行第二个过滤器链——与特定 IP 地址相匹配的那个。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"c9972e80d89af667062f5e4b8f53187c","permalink":"https://lib.jimmysong.io/envoy-handbook/listener/filter-chain-matching/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/envoy-handbook/listener/filter-chain-matching/","section":"envoy-handbook","summary":"过滤器链匹配允许我们指定为监听器选择特定过滤器链的标准。 我们可以在配置中定义多个过滤器链，然后根据目标端口、服务器名称、协议和其他属性来选择和执行它们。例如，我们可以检查哪个主机名正在连接，然后选择不","tags":["Envoy"],"title":"Header 操作","type":"book"},{"authors":null,"categories":["Envoy"],"content":"Envoy 具有一个内置的 HTTP Lua 过滤器，允许在请求和响应流中运行 Lua 脚本。Lua 是一种可嵌入的脚本语言，主要在嵌入式系统和游戏中流行。Envoy 使用 LuaJIT（Lua 的即时编译器）作为运行时。LuaJIT 支持的最高 Lua 脚本版本是 5.1，其中一些功能来自 5.2。\n在运行时，Envoy 为每个工作线程创建一个 Lua 环境。正因为如此，没有真正意义上的全局数据。任何在加载时创建和填充的全局数据都可以从每个独立的工作线程中看到。\nLua 脚本是以同步风格的 coroutines 运行的，即使它们可能执行复杂的异步任务。这使得它更容易编写。Envoy 通过一组 API 执行所有的网络 / 异步处理。当一个异步任务被调用时，Envoy 会暂停脚本的执行，等到异步操作完成就会恢复。\n我们不应该从脚本中执行任何阻塞性操作，因为这会影响 Envoy 的性能。我们应该只使用 Envoy 的 API 来进行所有的 IO 操作。\n我们可以使用 Lua 脚本修改和 / 或检查请求和响应头、正文和 Trailer。我们还可以对上游主机进行出站异步 HTTP 调用，或者执行直接响应，跳过任何进一步的过滤器迭代。例如，在 Lua 脚本中，我们可以进行上游的 HTTP 调用并直接响应，而不继续执行其他过滤器。\n如何配置 Lua 过滤器 Lua 脚本可以使用 inline_code 字段进行内联定义，或者使用过滤器上的 source_codes 字段引用本地文件。\nname:envoy.filters.http.luatyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.http.lua.v3.Luainline_code:|-- Called on the request path. function envoy_on_request(request_handle) -- Do something. end -- Called on the response path. function envoy_on_response(response_handle) -- Do something. endsource_codes:myscript.lua:filename:/scripts/myscript.luaEnvoy 将上述脚本视为全局脚本，对每一个 HTTP 请求都会执行它。在每个脚本中可以定义两个全局函数。\nfunction envoy_on_request(request_handle) end 和\nfunction envoy_on_response(response_handle) end envoy_on_request 函数在请求路径上被调用，而 envoy_on_response 脚本则在响应路径上被调用。每个函数都接收一个句柄，该句柄有不同的定义方法。脚本可以包含响应或请求函数，也可以包含两者。\n我们也有一个选项，可以在虚拟主机、路由或加权集群级别上按路由禁用或改写脚本。\n使用 typed_per_filter_config 字段来禁用或引用主机、路由或加权集群层面上的现有 Lua 脚本。例如，下面是如何使用 typed_per_filter_config 来引用一个现有的脚本（例如：some-script.lua）。\ntyped_per_filter_config:envoy.filters.http.lua。\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions. filters.http.lua.v3.LuaPerRoutename:some-script.lua同样地，我们可以这样定义 source_code 和 inline_string 字段，而不是指定 name 字段。\ntyped_per_filter_config:envoy.filters.http.lua:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.http.lua.v3.LuaPerRoutesource_code:inline_string:|function envoy_on_response(response_handle) -- Do something on response. end流处理 API 我们在前面提到，request_handle 和 response_handle 流句柄会被传递给全局 request 和 response 函数。\n在流句柄上可用的方法包括 headers、body、metadata、各种日志方法（如 logTrace、logInfo、logDebug…）、httpCall、connection等等。你可以在 Lua 过滤器源码中找到完整的方法列表。\n除了流对象外，API 还支持以下对象：\n Header 对象（由 headers() 方法返回） 缓冲区对象（由 body() 方法返回）。 动态元数据对象（由 metadata() 方法返回） Stream 信息对象（由 streamInfo() 方法返回） 连接对象（通过 connection() 方法返回） SSL 连接信息对象（由连接对象的 ssl() 方法返回）  你会在那里看到如何使用 Lua 实验中的一些对象和方法。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"e76fa816c872b5f2c303c616bcbdf37d","permalink":"https://lib.jimmysong.io/envoy-handbook/extending-envoy/lua-filter/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/envoy-handbook/extending-envoy/lua-filter/","section":"envoy-handbook","summary":"Envoy 具有一个内置的 HTTP Lua 过滤器，允许在请求和响应流中运行 Lua 脚本。Lua 是一种可嵌入的脚本语言，主要在嵌入式系统和游戏中流行。Envoy 使用 LuaJIT（Lua 的即时编译器）作为运行时。LuaJIT 支持的最","tags":["Envoy"],"title":"Lua 过滤器","type":"book"},{"authors":null,"categories":["Istio"],"content":"Prometheus 是一个开源的监控系统和时间序列数据库。Istio 使用 Prometheus 来记录指标，跟踪 Istio 和网格中的应用程序的健康状况。\n要安装 Prometheus，我们可以使用 Istio 安装包中 /samples/addons 文件夹中的示例安装。\n$ kubectl apply -f istio-1.9.0/samples/addons/prometheus.yaml serviceaccount/prometheus created configmap/prometheus created clusterrole.rbac.authorization.k8s.io/prometheus created clusterrolebinding.rbac.authorization.k8s.io/prometheus created service/prometheus created deployment.apps/prometheus created 要打开 Prometheus 仪表板，我们可以使用 Istio CLI 中的 dashboard 命令：\n$ istioctl dashboard prometheus http://localhost:9090 现在我们可以在浏览器中打开 http://localhost:9090，进入 Prometheus 仪表板，如下图所示。\n   Prometheus dashboard  部署示例应用 为了看到一些请求和流量，我们将部署一个 Nginx 实例：\n$ kubectl create deploy my-nginx --image=nginx deployment.apps/my-nginx created 为了能够产生一些流量并访问 Nginx Pod，我们需要以某种方式让它被访问。\n最简单的方法是将 Nginx 部署作为 Kubernetes LoadBalancer 服务公开：\nkubectl expose deployment my-nginx --type=LoadBalancer --name=my-nginx --port 80  注意：在课程的后面，我们将学习如何使用 Istio 资源并通过 Istio 的入口网关暴露服务。\n 现在我们可以运行 kubectl get services，获得 my-nginx 服务的外部 IP 地址：\n$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.48.0.1 \u0026lt;none\u0026gt; 443/TCP 73m my-nginx LoadBalancer 10.48.0.94 [IP HERE] 80:31191/TCP 4m6s 让我们把这个 IP 地址存储为一个环境变量，这样我们就可以在整个实验中使用它。\nexport NGINX_IP=$(kubectl get service my-nginx -o jsonpath=\u0026#39;{.status.loadBalancer.ingress[0].ip}\u0026#39;) 现在你可以对上述 IP 运行 curl，你应该得到默认的 Nginx 页面。\n$ curl $NGINX_IP \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt; \u0026lt;style\u0026gt; body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Welcome to nginx!\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;If you see this page, the nginx web server is successfully installed and working. Further configuration is required.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;For online documentation and support please refer to \u0026lt;a href=\u0026#34;http://nginx.org/\u0026#34;\u0026gt;nginx.org\u0026lt;/a\u0026gt;.\u0026lt;br/\u0026gt; Commercial support is available at \u0026lt;a href=\u0026#34;http://nginx.com/\u0026#34;\u0026gt;nginx.com\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;em\u0026gt;Thank you for using nginx.\u0026lt;/em\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 让我们向我们在开始时创建的 $NGINX_IP 环境变量提出几个请求。然后，从 Prometheus UI 中，你可以搜索 Istio 的一个指标（例如 istio_requests_total），以了解哪些数据点正在被收集。\n下面是一个来自 Prometheus 用户界面的示例元素：\nistio_requests_total{app=\u0026#34;my-nginx\u0026#34;, connection_security_policy=\u0026#34;none\u0026#34;, destination_app=\u0026#34;my-nginx\u0026#34;, destination_canonical_revision=\u0026#34;latest\u0026#34;, destination_canonical_service=\u0026#34;my-nginx\u0026#34;, destination_cluster=\u0026#34;Kubernetes\u0026#34;, destination_principal=\u0026#34;unknown\u0026#34;, destination_service=\u0026#34;my-nginx.default.svc.cluster.local\u0026#34;, destination_service_name=\u0026#34;my-nginx\u0026#34;, destination_service_namespace=\u0026#34;default\u0026#34;, destination_version=\u0026#34;unknown\u0026#34;, destination_workload=\u0026#34;my-nginx\u0026#34;, destination_workload_namespace=\u0026#34;default\u0026#34;, instance=\u0026#34;10.92.4.4:15020\u0026#34;, istio_io_rev=\u0026#34;default\u0026#34;, job=\u0026#34;kubernetes-pods\u0026#34;, kubernetes_namespace=\u0026#34;default\u0026#34;, kubernetes_pod_name=\u0026#34;my-nginx-6b74b79f57-r59sf\u0026#34;, pod_template_hash=\u0026#34;6b74b79f57\u0026#34;, reporter=\u0026#34;destination\u0026#34;, request_protocol=\u0026#34;http\u0026#34;, response_code=\u0026#34;200\u0026#34;, response_flags=\u0026#34;-\u0026#34;, security_istio_io_tlsMode=\u0026#34;istio\u0026#34;, service_istio_io_canonical_name=\u0026#34;my-nginx\u0026#34;, service_istio_io_canonical_revision=\u0026#34;latest\u0026#34;, source_app=\u0026#34;unknown\u0026#34;, source_canonical_revision=\u0026#34;latest\u0026#34;, source_canonical_service=\u0026#34;unknown\u0026#34;, source_cluster=\u0026#34;unknown\u0026#34;, source_principal=\u0026#34;unknown\u0026#34;, source_version=\u0026#34;unknown\u0026#34;, source_workload=\u0026#34;unknown\u0026#34;, source_workload_namespace=\u0026#34;unknown\u0026#34;}\t12 ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"c9495ff4fabf6f0fba08dbff741cefb0","permalink":"https://lib.jimmysong.io/istio-handbook/observability/prometheus/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/istio-handbook/observability/prometheus/","section":"istio-handbook","summary":"Prometheus 是一个开源的监控系统和时间序列数据库。Istio 使用 Prometheus 来记录指标，跟踪 Istio 和网格中的应用程序的健康状况。 要安装 Prometheus，我们可以使用 Istio 安装包中 /samples/addons 文件夹中的示例安装。 $ kubectl apply -f istio-1.9.0/samples/addons/prometheus.yaml serviceaccount/prometheus created configmap/prometheus created clusterrole.rbac.authorization.k8s.io/prometheus created clusterrolebinding.rbac.authorization.k8s.io/prometheus","tags":["Istio","Service Mesh"],"title":"Prometheus","type":"book"},{"authors":null,"categories":["Istio"],"content":"RequestAuthentication（请求认证）定义了工作负载支持哪些请求认证方法。如果请求包含无效的认证信息，它将根据配置的认证规则拒绝该请求。不包含任何认证凭证的请求将被接受，但不会有任何认证的身份。\n示例 为了限制只对经过认证的请求进行访问，应该伴随着一个授权规则。\n例如，要求对具有标签 app:httpbin 的工作负载的所有请求使用 JWT 认证。\napiVersion:security.istio.io/v1beta1kind:RequestAuthenticationmetadata:name:httpbinnamespace:foospec:selector:matchLabels:app:httpbinjwtRules:- issuer:\u0026#34;issuer-foo\u0026#34;jwksUri:https://example.com/.well-known/jwks.json---apiVersion:security.istio.io/v1beta1kind:AuthorizationPolicymetadata:name:httpbinnamespace:foospec:selector:matchLabels:app:httpbinrules:- from:- source:requestPrincipals:[\u0026#34;*\u0026#34;]下一个例子展示了如何为不同的 host 设置不同的 JWT 要求。RequestAuthentication 声明它可以接受由 issuer-foo 或 issuer-bar 签发的 JWT（公钥集是由 OpenID Connect 规范隐性设置的）。\napiVersion:security.istio.io/v1beta1kind:RequestAuthenticationmetadata:name:httpbinnamespace:foospec:selector:matchLabels:app:httpbinjwtRules:- issuer:\u0026#34;issuer-foo\u0026#34;- issuer:\u0026#34;issuer-bar\u0026#34;---apiVersion:security.istio.io/v1beta1kind:AuthorizationPolicymetadata:name:httpbinnamespace:foospec:selector:matchLabels:app:httpbinrules:- from:- source:requestPrincipals:[\u0026#34;issuer-foo/*\u0026#34;]to:- operation:hosts:[\u0026#34;example.com\u0026#34;]- from:- source:requestPrincipals:[\u0026#34;issuer-bar/*\u0026#34;]to:- operation:hosts:[\u0026#34;another-host.com\u0026#34;]你可以对授权策略进行微调，为每个路径设置不同的要求。例如，除了 /healthz，所有路径都需要 JWT，可以使用相同的 RequestAuthentication，但授权策略可以是：\napiVersion:security.istio.io/v1beta1kind:AuthorizationPolicymetadata:name:httpbinnamespace:foospec:selector:matchLabels:app:httpbinrules:- from:- source:requestPrincipals:[\u0026#34;*\u0026#34;]- to:- operation:paths:[\u0026#34;/healthz\u0026#34;]关于 RequestAuthentication 配置的详细用法请参考 Istio 官方文档。\n参考  RequestAuthentication - istio.io  ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"2489eadc16d5f1d5dd48b63f9dd2b217","permalink":"https://lib.jimmysong.io/istio-handbook/config-security/request-authentication/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/istio-handbook/config-security/request-authentication/","section":"istio-handbook","summary":"RequestAuthentication（请求认证）定义了工作负载支持哪些请求认证方法。如果请求包含无效的认证信息，它将根据配置的认证规则拒绝该请求。不包含任何认证凭证的请求将被接受，但不会有任何认","tags":["Istio","Service Mesh"],"title":"RequestAuthentication","type":"book"},{"authors":null,"categories":["Istio"],"content":"Slime 是由网易数帆微服务团队开源的一款基于 Istio 的智能网格管理器。Slime 基于 Kubernetes Operator 实现，可作为 Istio 的 CRD 管理器，无须对 Istio 做任何定制化改造，就可以定义动态的服务治理策略，从而达到自动便捷使用 Istio 和 Envoy 高阶功能的目的。\nSlime 试图解决的问题 Slime 项目的诞生主要为了解决以下问题：\n 网格内所有服务配置全量下到所有 Sidecar Proxy，导致其消耗大量资源使得应用性能变差的问题 如何在 Istio 中实现高阶扩展的问题：比如扩展 HTTP 插件；根据服务的资源使用率做到自适应限流  Slime 解决以上问题的答案是构建 Istio 的控制平面，具体做法是：\n 构建可拔插控制器 数据平面监控 CRD 转换  通过以上方式 Slime 可以实现配置懒加载和插件管理器。\nSlime 架构 Slime 内部分为三大模块，其架构图如下所示。\n   Slime 内部架构图  Slime 内部三大组件为：\n slime-boot：在 Kubernetes 上部署 Slime 模块的 operator。 slime-controller：Slime 的核心组件，监听 Slime CRD 并将其转换为Istio CRD。 slime-metric：用于获取服务 metrics 信息的组件，slime-controller 会根据其获取的信息动态调整服务治理规则。  目前 Slime 内置了三个控制器子模块：\n 配置懒加载（按需加载）：用户无须手动配置 SidecarScope，Istio 可以按需加载服务配置和服务发现信息； HTTP 插件管理：使用新的 CRD——pluginmanager/envoyplugin 包装了可读性，摒弃了可维护性较差的 envoyfilter，使得插件扩展更为便捷； 自适应限流：结合监控信息自动调整限流策略；   什么是 SidecarScope？\nSidecarScope 是在 Istio 1.1 版本中引入的，它并不是一个直接面向用户的配置项，而是 Sidecar 资源的包装器，具体来说就是 Sidecar 资源中的 egress 选项。通过该配置可以减少 Istio 向 Sidecar 下发的数据量，例如只向某个命名空间中的某些服务下发某些 hosts 的访问配置，从而提高应用提高性能。\n 使用 Slime 作为 Istio 的控制平面 为了解决这些问题，Slime 在 Istio 之上构建了更高层次的抽象，相当于为 Istio 构建了一层管理平面，其工作流程图如下所示。\n   Slime 工作流程图  具体步骤如下：\n Slime Operator 根据管理员的配置在 Kubernetes 中完成 Slime 组件的初始化； 开发者创建符合 Slime CRD 规范的配置并应用到 Kubernetes 集群中； Slime 查询 Prometheus 中保存的相关服务的监控数据，结合 Slime CRD 中自适应部分的配置，将 Slime CRD 转换为 Istio CRD，同时将其推送到 Global Proxy 中； Istio 监听 Istio CRD 的创建； Istio 将 Sidecar Proxy 的配置信息推送到数据平面相应的 Sidecar Proxy 中；  以上只是一个对 Slime 工作流程的一个笼统的介绍，更多详细信息请参考 Slime GitHub。\n配置懒加载 为了解决数据平面中 Sidecar Proxy 资源消耗过大及网络延迟问题，Slime 使用了配置懒加载（按需加载 Sidecar 配置）的方案。该方案的核心思想是向每个 Sidecar Proxy 中只下发其所 Pod 中服务所需的配置，而不是将网格中的所有服务信息全量下发。所以 Slime 需要获取每个服务的调用关系这样才能得到其所需的 Sidecar Proxy 配置。\nSlime 实现 Sidecar Proxy 配置懒加载的方法是：\n 让数据平面中的所有服务的首次调用都通过一个 Global Proxy，该 Proxy 可以记录所有服务的调用和依赖信息，根据该依赖信息更新 Istio 中 Sidecar 资源的配置； 当某个服务的调用链被 VirtualService 中的路由信息重新定义时， Global Proxy 原有记录就失效了，需要一个新的数据结构来维护该服务的调用关系。Slime 创建了名为 ServiceFence 的 CRD 来维护服务调用关系以解决服务信息缺失问题。  使用 Global Proxy 初始化服务调用拓扑 Slime 在数据平面中部署 Global Proxy（也叫做 Global Sidecar，但其与应用的 Pod 不是一对一的关系，笔者更倾向于称其为 Global Proxy），该代理同样使用 Envoy 构建，在每个需要启动配置懒加载的命名空间中部署一个或在整个网格中只部署一个，所有缺失服务发现信息的调用（你也可以手动配置服务调用关系），都会被兜底路由劫持到 Global Proxy，经过其首次转发后，Slime 便可感知到被调用方的信息，然后根据其对应服务的 VirtualService，找到服务名和真实后端的映射关系，将两者的都加入 SidecarScope，以后该服务的调用就不再需要经过 Global Proxy 了。\n使用 ServiceFence 维护服务调用拓扑 在使用 Global Proxy 初始化服务调用拓扑后，一旦服务调用链有变动的话怎么办？对此 Slime 创建了 ServiceFence 的 CRD。使用 ServiceFence 可以维护服务名和后端服务的映射关系。Slime 根据其对应服务的 VirtualService，找到 Kubernetes 服务名和真实后端（host）的映射关系，将两者的都加入 Sidecar 的配置中。ServiceFence 管理生成的 SidecarScope 的生命周期，自动清理长时间不用的调用关系，从而避免上述问题。\n如何开启配置懒加载 配置懒加载功能对于终端用户是透明的，只需要 Kubernetes Service 上打上 istio.dependency.servicefence/status:\u0026#34;true\u0026#34; 的标签，表明该服务需要开启配置懒加载，剩下的事情交给 Slime Operator 来完成即可。\nHTTP 插件管理 Istio 中的插件扩展只能通过 EnvoyFilter 来实现，因为它是 xDS 层面的配置，管理和维护这样的配置需要耗费大量的精力，也极容易出错。因此，Slime 在 EnvoyFilter 的基础上做了一层面向插件的抽象。\nSlime 共有两个 CRD 用于 HTTP 插件管理，分别是：\n PluginManager：配置为哪些负载开启哪些插件，插件的配置顺序即为执行顺序； EnvoyPlugin：EnvoyPlugin 不关心每个插件的具体配置，具体配置会被放在 EnvoyFilter 资源的 patch.typed_config 结构中透传），EnvoyPlugin 的核心思想是将插件配置在需要的维度中做聚合，从而限定插件的生鲜范围。这样做一方面更加贴合插件使用者的习惯，另一方面也降低了上层配置的冗余，  自适应限流 Envoy 内置的限流组件功能单一，只能以实例维度配置限流值，无法做到根据应用负载的自适应限流。Slime 通过与 Prometheus metric server 对接，实时的获取监控情况，来动态配置限流值。\nSlime 自适应限流的流程图如下所示。\n   Slime 的自适应限流流程图  Slime 的自适应限流的流程分为两部分，一部分为 SmartLimiter 到 EnvoyFilter 的转换，另一部分为获取监控数据。目前 Slime 支持从 Kubernetes Metric Server 获取服务的CPU、内存、副本数等数据。Slime 还对外提供了一套监控数据接口（Metric Discovery Server），通过 MDS，可以将自定义的监控指标同步给限流组件。\nSlime 创建的 CRD SmartLimiter 用于配置自适应限流。其的配置是接近自然语义，例如希望在 CPU 超过 80% 时触发服务 A 的访问限制，限额为 30QPS，对应的SmartLimiter 定义如下：\napiVersion:microservice.netease.com/v1alpha1kind:SmartLimitermetadata:name:anamespace:defaultspec:descriptors:- action:fill_interval:seconds:1quota:\u0026#34;30/{pod}\u0026#34;# 30为该服务的额度，将其均分给每个 pod，加入有 3 个 pod，则每个 pod 的限流为 10condition:\u0026#34;{cpu}\u0026gt;0.8\u0026#34;# 根据监控项{cpu}的值自动填充该模板更多 Slime 开源于 2021 年初，本文发稿时该项目仍处于初级阶段，本文大量参考了杨笛航在云原生社区中的分享 Slime：让 Istio 服务网格变得更加高效与智能 及 Slime 的 GitHub。感兴趣的读者可以关注下这个项目的 GitHub，进一步了解它。\n参考  Slime：让 Istio 服务网格变得更加高效与智能 - cloudnative.to Slime GitHub 文档 - github.com Sidecar - istio.io  ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"e4aec3275c747b431d3b261e6d4ea25e","permalink":"https://lib.jimmysong.io/istio-handbook/ecosystem/slime/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/istio-handbook/ecosystem/slime/","section":"istio-handbook","summary":"Slime 是由网易数帆微服务团队开源的一款基于 Istio 的智能网格管理器。Slime 基于 Kubernetes Operator 实现，可作为 Istio 的 CRD 管理器，无须对 Istio 做任何定制化改造，就可以定义动态的服务治理策略，从而达到自动便捷使用 Istio 和 Envoy 高阶功能的目的。","tags":["Istio","Service Mesh"],"title":"Slime","type":"book"},{"authors":null,"categories":["Istio"],"content":"VirtualService 主要配置流量路由。以下是在流量路由背景下定义的几个有用的术语。\n Service 是与服务注册表（service registry）中的唯一名称绑定的应用行为单元。服务由多个网络端点（endpoint）组成，这些端点由运行在 pod、容器、虚拟机等的工作负载实例实现。 服务版本，又称子集（subset）：在持续部署方案中，对于一个给定的服务，可能有不同的实例子集，运行应用程序二进制的不同变体。这些变体不一定是不同的 API 版本。它们可能是同一服务的迭代变化，部署在不同的环境（prod、staging、dev 等）。发生这种情况的常见场景包括 A/B 测试、金丝雀发布等。一个特定版本的选择可以根据各种标准（header、URL 等）和 / 或分配给每个版本的权重来决定。每个服务都有一个由其所有实例组成的默认版本。 源（source）：下游客户端调用服务。 Host：客户端在尝试连接到服务时使用的地址。 访问模型（access model）：应用程序只针对目标服务（host），而不了解各个服务版本（子集）。版本的实际选择是由代理/sidecar 决定的，使应用程序代码脱离依赖服务。 VirtualService 定义了一套当目标服务（host）被寻址时应用的流量路由规则。每个路由规则定义了特定协议流量的匹配标准。如果流量被匹配，那么它将被发送到注册表中定义的指定目标服务（或它的子集/版本）。  流量的来源也可以在路由规则中进行匹配。这允许为特定的客户环境定制路由。\n示例 以下是 Kubernetes 上的例子，默认情况下，所有的 HTTP 流量都会被路由到标签为 version: v1 的 reviews 服务的 pod 上。此外，路径以 /wpcatalog/ 或 /consumercatalog/ 开头的 HTTP 请求将被重写为 /newcatalog，并被发送到标签为 version: v2 的 pod 上。\napiVersion:networking.istio.io/v1alpha3kind:VirtualServicemetadata:name:reviews-routespec:hosts:- reviews.prod.svc.cluster.localhttp:- name:\u0026#34;reviews-v2-routes\u0026#34;match:- uri:prefix:\u0026#34;/wpcatalog\u0026#34;- uri:prefix:\u0026#34;/consumercatalog\u0026#34;rewrite:uri:\u0026#34;/newcatalog\u0026#34;route:- destination:host:reviews.prod.svc.cluster.localsubset:v2- name:\u0026#34;reviews-v1-route\u0026#34;route:- destination:host:reviews.prod.svc.cluster.localsubset:v1途径目的地的子集/版本是通过对命名的服务子集的引用来识别的，这个子集必须在相应的 DestinationRule 中声明。\napiVersion:networking.istio.io/v1alpha3kind:DestinationRulemetadata:name:reviews-destinationspec:host:reviews.prod.svc.cluster.localsubsets:- name:v1labels:version:v1- name:v2labels:version:v2配置项 下图是 VirtualService 资源的配置拓扑图。\n  VirtualService 资源配置拓扑图  VirtualService 资源的顶级配置项如下：\n  hosts：字符串数组格式。配置流量被发送到的目标主机。可以是一个带有通配符前缀的 DNS 名称或 IP 地址。根据平台的不同，也可以使用短名称来代替 FQDN（即名称中没有点）。在这种情况下，主机的 FQDN 将根据底层平台衍生出来。单个 VirtualService 可以用来描述相应主机的所有流量属性，包括多个 HTTP 和 TCP 端口的属性。另外，可以使用一个以上的 VirtualService 来定义主机的流量属性，但要注意一些问题。详情请参考《流量管理最佳实践》。\nKubernetes 用户的注意事项。当使用短名称时（例如 reviews 而不是 reviews.default.svc.cluster.local），Istio 将根据规则的命名空间而不是服务来解释短名称。在 default 命名空间中包含主机 reviews 的规则将被解释为 reviews.default.svc.cluster.local，而不考虑与 reviews 服务相关的实际命名空间。为了避免潜在的错误配置，建议总是使用完全限定名而不是短名称。\nhosts 字段同时适用于 HTTP 和 TCP 服务。网格内的服务，即那些在服务注册表中发现的服务，必须始终使用它们的字母数字名称来引用。只允许通过网关定义的服务使用 IP 地址来引用。\n注意：对于委托的 VirtualService，必须是空的。\n  gateways：字符串数组格式。配置应用路由的网关和 sidecar 的名称。其他命名空间中的 Gateway 可以通过 \u0026lt;Gateway命名空间\u0026gt;/\u0026lt;Gateway名称\u0026gt; 来引用；指定一个没有命名空间修饰词的 Gateway 默认与 VirtualService 位于同一命名空间。单个 VirtualService 用于网格内的 sidecar，也用于一个或多个网关。这个字段施加的选择条件可以使用协议特定路由的匹配条件中的源字段来覆盖。mesh 这个保留词被用来暗示网格中的所有 sidecar。当这个字段被省略时，将使用默认网关（mesh），这将把规则应用于网格中的所有 sidecar。如果提供了一个网关名称的列表，规则将只应用于网关。要将规则同时应用于网关和 sidecar，请指定 mesh 作为 Gateway 名称。\n  http：HTTP 流量的路由规则的有序列表。HTTP 路由将应用于名为 http-/http2-/grpc-* 的平台服务端口、具有 HTTP/HTTP2/GRPC/TLS 终止的 HTTPS 协议的网关端口以及使用 HTTP/HTTP2/GRPC 协议的 ServiceEntry 端口。请注意该配置是有顺序的，请求第一个匹配的规则将被应用。\n  tls：一个有序的路由规则列表，用于非终止的 TLS 和 HTTPS 流量。路由通常是使用 ClientHello 消息提出的 SNI 值来执行的。TLS 路由将被应用于平台服务端口 https-、tls-，使用 HTTPS/TLS 协议的未终止网关端口（即具有 passthough TLS 模式）和使用 HTTPS/TLS 协议的 ServiceEntry 端口。匹配传入请求的第一条规则被使用。注意：没有相关 VirtualService 的 https- 或 tls- 端口流量将被视为不透明的 TCP 流量。\n  tcp：一个不透明的 TCP 流量的路由规则的有序列表。TCP 路由将被应用于任何不是 HTTP 或 TLS 端口。匹配传入请求的第一个规则被使用。\n  exportTo：VirtualService 被导出的命名空间的列表。导出的 VirtualService 可以被定义在其它命名空间中的 sidecar 和 Gateway 使用。该功能为服务所有者和网格管理员提供了一种机制，以控制 VirtualService 在命名空间边界的可视性。\n如果没有指定命名空间，那么默认情况下，VirtualService 会被输出到所有命名空间。\n值 . 是保留的，它定义了导出到 VirtualService 声明的同一命名空间。同样，值 * 也是保留的，它定义了导出到所有命名空间。\n  完整示例 下面是一个相对完整的 VirtualService 配置的示例，其中包含了所有基本配置，对应的解释请见注释。\napiVersion:networking.istio.io/v1alpha3kind:VirtualServicemetadata:name:reviews-routenamespace:bookinfospec:hosts:- reviews.prod.svc.cluster.local#目标主机gateways:- my-gateway#应用路由的网关和 sidecar 的名称http:#HTTP 路由规则- name:\u0026#34;reviews-v2-routes\u0026#34;match:- uri:prefix:\u0026#34;/wpcatalog\u0026#34;rewrite:uri:\u0026#34;/newcatalog\u0026#34;route:- destination:host:reviews.prod.svc.cluster.localsubset:v1#该子集是在 DestinationRule 中设置的tls:#关于 TLS 的设置mode:MUTUAL#一共有四种模式，DISABLE：关闭 TLS 连接；SIMPLE：发起一个与上游端点的 TLS 连接；MUTUAL：手动配置证书，通过出示客户端证书进行认证，使用双向的 TLS 确保与上游的连接；ISTIO_MUTUAL：该模式使用 Istio 自动生成的证书进行 mTLS 认证。clientCertificate:/etc/certs/myclientcert.pemprivateKey:/etc/certs/client_private_key.pemcaCertificates:/etc/certs/rootcacerts.pemexportTo:#指定 VirtualService 的可视性- \u0026#34;*\u0026#34;#*表示对所有命名空间可见，此为默认值；\u0026#34;.\u0026#34;表示仅对当前命名空间可见关于 VirtualService 配置的详细用法请参考 Istio 官方文档。\n参考  Virtual Service - istio.io  ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"4cdabe2dbbd5ec21dd0678af3df15ff4","permalink":"https://lib.jimmysong.io/istio-handbook/config-networking/virtual-service/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/istio-handbook/config-networking/virtual-service/","section":"istio-handbook","summary":"VirtualService 主要配置流量路由。以下是在流量路由背景下定义的几个有用的术语。 Service 是与服务注册表（service registry）中的唯一名称绑定的应用行为单元。服务由多个网络端点（endpoint）组成，这些端点由","tags":["Istio","Service Mesh"],"title":"VirtualService","type":"book"},{"authors":null,"categories":["Istio"],"content":"在集群和 Istio 准备好后，我们可以克隆在 Online Boutique 应用库。\n1. 克隆仓库\ngit clone https://github.com/GoogleCloudPlatform/microservices-demo.git 2. 前往 microservices-demo 目录\ncd microservices-demo 3. 创建 Kubernetes 资源\nkubectl apply -f release/kubernetes-manifests.yaml 4. 检查所有 Pod 都在运行\n$ kubectl get pods NAME READY STATUS RESTARTS AGE adservice-5c9c7c997f-n627f 2/2 Running 0 2m15s cartservice-6d99678dd6-767fb 2/2 Running 2 2m16s checkoutservice-779cb9bfdf-l2rs9 2/2 Running 0 2m18s currencyservice-5db6c7d559-9drtc 2/2 Running 0 2m16s emailservice-5c47dc87bf-dk7qv 2/2 Running 0 2m18s frontend-5fcb8cdcdc-8c9dk 2/2 Running 0 2m17s loadgenerator-79bff5bd57-q9qkd 2/2 Running 4 2m16s paymentservice-6564cb7fb9-f6dwr 2/2 Running 0 2m17s productcatalogservice-5db9444549-hkzv7 2/2 Running 0 2m17s recommendationservice-ff6878cf5-jsghw 2/2 Running 0 2m18s redis-cart-57bd646894-zb7ch 2/2 Running 0 2m15s shippingservice-f47755f97-dk7k9 2/2 Running 0 2m15s 5. 创建 Istio 资源\nkubectl apply -f ./istio-manifests 部署了一切后，我们就可以得到入口网关的 IP 地址并打开前端服务：\nINGRESS_HOST=\u0026#34;$(kubectl -n istio-system get service istio-ingressgateway \\  -o jsonpath=\u0026#39;{.status.loadBalancer.ingress[0].ip}\u0026#39;)\u0026#34; echo \u0026#34;$INGRESS_HOST\u0026#34; 在浏览器中打开 INGRESS_HOST，你会看到前端服务，如下图所示。\n   前端服务  我们需要做的最后一件事是删除 frontend-external 服务。frontend-external 服务是一个 LoadBalancer 服务，它暴露了前端。由于我们正在使用 Istio 的入口网关，我们不再需要这个 LoadBalancer 服务了。\n要删除服务，运行：\nkubectl delete svc frontend-external Online Boutique 应用清单还包括一个负载发生器，它正在生成对所有服务的请求——这是为了让我们能够模拟网站的流量。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"1f174b95332deded449f609aa57ee521","permalink":"https://lib.jimmysong.io/istio-handbook/practice/online-boutique/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/istio-handbook/practice/online-boutique/","section":"istio-handbook","summary":"在集群和 Istio 准备好后，我们可以克隆在 Online Boutique 应用库。 1. 克隆仓库 git clone https://github.com/GoogleCloudPlatform/microservices-demo.git 2. 前往 microservices-demo 目录 cd microservices-demo 3. 创建 Kubernetes 资源 kubectl apply -f release/kubernetes-manifests.yaml 4. 检查所有 Pod 都在运行 $ kubectl get pods NAME READY STATUS RESTARTS AGE adservice-5c9c7c997f-n627f 2/2 Running 0 2m15s cartservice-6d99678dd6-767fb 2/2 Running 2 2m16s checkoutservice-779cb9bfdf-l2rs9 2/2 Running 0 2m18s currencyservice-5db6c7d559-9drtc 2/2 Running 0 2m16s emailservice-5c47dc87bf-dk7qv 2/2 Running 0 2m18s frontend-5fcb8cdcdc-8c9dk 2/2 Running 0 2m17s loadgenerator-79bff5bd57-q9qkd 2/2","tags":["Istio","Service Mesh"],"title":"部署 Online Boutique 应用","type":"book"},{"authors":null,"categories":["Istio"],"content":"Istio 提供两种类型的认证：对等认证和请求认证。\n对等认证 对等认证用于服务间的认证，以验证建立连接的客户端。\n当两个服务试图进行通信时，相互 TLS 要求它们都向对方提供证书，因此双方都知道它们在与谁交谈。如果我们想在服务之间启用严格的相互 TLS，我们可以使用 PeerAuthentication 资源，将 mTLS 模式设置为 STRICT。\n使用 PeerAuthentication 资源，我们可以打开整个网状结构的相互 TLS（mTLS），而不需要做任何代码修改。\n然而，Istio 也支持一种优雅的模式，我们可以选择在一个工作负载或命名空间的时间内进入相互 TLS。这种模式被称为许可模式。\n当你安装 Istio 时，允许模式是默认启用的。启用允许模式后，如果客户端试图通过相互 TLS 连接到我，我将提供相互 TLS。如果客户端不使用相互 TLS，我也可以用纯文本响应。我是允许客户端做 mTLS 或不做的。使用这种模式，你可以在你的网状网络中逐渐推广相互 TLS。\n简而言之，PeerAuthentication 谈论的是工作负载或服务的通信方式，它并没有说到最终用户。那么，我们怎样才能认证用户呢？\n请求认证 请求认证（RequestAuthentication 资源）验证了附加在请求上的凭证，它被用于终端用户认证。\n请求级认证是通过 JSON Web Tokens（JWT） 验证完成的。Istio 支持任何 OpenID Connect 提供商，如 Auth0、Firebase 或 Google Auth、Keycloak、ORY Hydra。因此，就像我们使用 SPIFFE 身份来验证服务一样，我们可以使用 JWT 令牌来验证用户。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"a412d190f7edfb68570e4a8dabf0faa8","permalink":"https://lib.jimmysong.io/istio-handbook/security/peer-and-request-authn/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/istio-handbook/security/peer-and-request-authn/","section":"istio-handbook","summary":"Istio 提供两种类型的认证：对等认证和请求认证。 对等认证 对等认证用于服务间的认证，以验证建立连接的客户端。 当两个服务试图进行通信时，相互 TLS 要求它们都向对方提供证书，因此双方都知道它们在与谁交谈。如果我们想在","tags":["Istio","Service Mesh"],"title":"对等认证和请求认证","type":"book"},{"authors":null,"categories":["Istio"],"content":"我们在前面看到了通过客户端库来治理服务的架构图，那是我们在改造成服务网格架构前使用微服务架构通常的形式，下图是使用服务网格架构的最终形式。\n   服务网格架构示意图  当然在达到这一最终形态之前我们需要将架构一步步演进，下面给出的是参考的演进路线。\nIngress 或边缘代理 如果你使用的是 Kubernetes 做容器编排调度，那么在进化到服务网格架构之前，通常会使用 Ingress Controller，做集群内外流量的反向代理，如使用 Traefik 或 Nginx Ingress Controller。\n   Ingress 或边缘代理架构示意图  这样只要利用 Kubernetes 的原有能力，当你的应用微服务化并容器化需要开放外部访问且只需要 L7 代理的话这种改造十分简单，但问题是无法管理服务间流量。\n路由器网格 Ingress 或者边缘代理可以处理进出集群的流量，为了应对集群内的服务间流量管理，我们可以在集群内加一个 Router 层，即路由器层，让集群内所有服务间的流量都通过该路由器。\n   路由器网格架构示意图  这个架构无需对原有的单体应用和新的微服务应用做什么改造，可以很轻易的迁移进来，但是当服务多了管理起来就很麻烦。\nProxy per Node 这种架构是在每个节点上都部署一个代理，如果使用 Kubernetes 来部署的话就是使用 DaemonSet 对象，Linkerd 第一代就是使用这种方式部署的，一代的 Linkerd 使用 Scala 开发，基于 JVM 比较消耗资源，二代的 Linkerd 使用 Go 开发。\n   Proxy per node 架构示意图  这种架构有个好处是每个节点只需要部署一个代理即可，比起在每个应用中都注入一个sidecar的方式更节省资源，而且更适合基于物理机/虚拟机的大型单体应用，但是也有一些副作用，比如粒度还是不够细，如果一个节点出问题，该节点上的所有服务就都会无法访问，对于服务来说不是完全透明的。\nSidecar代理/Fabric模型 这个一般不会成为典型部署类型，当企业的服务网格架构演进到这一步时通常只会持续很短时间，然后就会增加控制平面。跟前几个阶段最大的不同就是，应用程序和代理被放在了同一个部署单元里，可以对应用程序的流量做更细粒度的控制。\n   Sidecar代理/Fabric模型示意图  这已经是最接近服务网格架构的一种形态了，唯一缺的就是控制平面了。所有的sidecar都支持热加载，配置的变更可以很容易的在流量控制中反应出来，但是如何操作这么多sidecar就需要一个统一的控制平面了。\nSidecar代理/控制平面 下面的示意图是目前大多数服务网格的架构图，也可以说是整个服务网格架构演进的最终形态。\n   Sidecar 代理/控制平面架构示意图  这种架构将代理作为整个服务网格中的一部分，使用 Kubernetes 部署的话，可以通过以 sidecar 的形式注入，减轻了部署的负担，可以对每个服务的做细粒度权限与流量控制。但有一点不好就是为每个服务都注入一个代理会占用很多资源，因此要想方设法降低每个代理的资源消耗。\n多集群部署和扩展 以上都是单个服务网格集群的架构，所有的服务都位于同一个集群中，服务网格管理进出集群和集群内部的流量，当我们需要管理多个集群或者是引入外部的服务时就需要网格扩展和多集群配置。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"30bdfc05dce3472ac3b0a2d235e161b8","permalink":"https://lib.jimmysong.io/istio-handbook/concepts/service-mesh-patterns/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/istio-handbook/concepts/service-mesh-patterns/","section":"istio-handbook","summary":"我们在前面看到了通过客户端库来治理服务的架构图，那是我们在改造成服务网格架构前使用微服务架构通常的形式，下图是使用服务网格架构的最终形式。 服务网格架构示意图 当然在达到这一最终形态之前我们需要将架构一步","tags":["Istio","Service Mesh"],"title":"服务网格的实现模式","type":"book"},{"authors":null,"categories":["安全"],"content":"要用密钥管理服务（KMS）提供商插件来加密 Secret，可以使用以下加密配置 YAML 文件的例子来为提供商设置属性。这个例子是基于 Kubernetes 的官方文档。\napiVersion:apiserver.config.k8s.io/v1kind:EncryptionConfigurationresources:- resources:- secretsproviders:- kms:name:myKMSPluginendpoint:unix://tmp/socketfile.sockcachesize:100timeout:3s- identity:{}要配置 API 服务器使用 KMS 提供商，请将 --encryption-provider-config 标志与配置文件的位置一起设置，并重新启动 API 服务器。\n要从本地加密提供者切换到 KMS，请将 EncryptionConfiguration 文件中的 KMS 提供者部分添加到当前加密方法之上，如下所示。\napiVersion:apiserver.config.k8s.io/v1kind:EncryptionConfigurationresources:- resources:- secretsproviders:- kms:name:myKMSPluginendpoint:unix://tmp/socketfile.sockcachesize:100timeout:3s- aescbc:keys:- name:key1secret:\u0026lt;base64 encoded secret\u0026gt;重新启动 API 服务器并运行下面的命令来重新加密所有与 KMS 供应商的 Secret。\nkubectl get secrets --all-namespaces -o json | kubectl replace -f - ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"b6d9c1c13d269ee8cfa0a2c22ec0869c","permalink":"https://lib.jimmysong.io/kubernetes-hardening-guidance/appendix/i/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/kubernetes-hardening-guidance/appendix/i/","section":"kubernetes-hardening-guidance","summary":"要用密钥管理服务（KMS）提供商插件来加密 Secret，可以使用以下加密配置 YAML 文件的例子来为提供商设置属性。这个例子是基于 Kubernetes 的官方文档。 apiVersion:apiserver.config.k8s.io/v1kind:EncryptionConfigurationresources:- resources:- secretsproviders:- kms:name:myKMSPluginendpoint:unix://tmp/socketfile.sockcachesize:100timeout:3s- identity:{}要配置 API 服务器使用 KMS 提供商，请将 --encryption-provider-config","tags":["Kubernetes","安全"],"title":"附录 I：KMS 配置实例","type":"book"},{"authors":null,"categories":["Envoy"],"content":"动态提供配置另一种方式是通过指向文件系统上的文件。为了使动态配置发挥作用，我们需要在 node 字段下提供信息。如果我们可能有多个 Envoy 代理指向相同的配置文件，那么 node 字段是用来识别一个特定的 Envoy 实例。\n   动态配置  为了指向动态资源，我们可以使用 dynamic_resources 字段来告诉 Envoy 在哪里可以找到特定资源的动态配置。例如：\nnode:cluster:my-clusterid:some-iddynamic_resources:lds_config:path:/etc/envoy/lds.yamlcds_config:path:/etc/envoy/cds.yaml上面的片段是一个有效的 Envoy 配置。如果我们把 LDS 和 CDS 作为静态资源来提供，它们的单独配置将非常相似。唯一不同的是，我们必须指定资源类型和版本信息。下面是 CDS 配置的一个片段。\nversion_info:\u0026#34;0\u0026#34;resources:- \u0026#34;@type\u0026#34;: type.googleapis.com/envoy.config.cluster.v3.Clustername:instance_1connect_timeout:5sload_assignment:cluster_name:instance_1endpoints:- lb_endpoints:- endpoint:address:socket_address:address:127.0.0.1port_value:3030如果我们想使用 EDS 为集群提供端点，我们可以这样写上面的配置。\nversion_info:\u0026#34;0\u0026#34;resources:- \u0026#34;@type\u0026#34;: type.googleapis.com/envoy.config.cluster.v3.Clustername:instance_1type:EDSeds_cluster_config:eds_config:path:/etc/envoy/eds.yaml另外，注意我们已经把集群的类型设置为 EDS。EDS 的配置会是这样的。\nversion_info:\u0026#34;0\u0026#34;resources:- \u0026#34;@type\u0026#34;: type.googleapis.com/envoy.config.endpoint.v3.ClusterLoadAssignmentcluster_name:instance_1endpoints:- lb_endpoints:- endpoint:address:socket_address:address:127.0.0.1port_value:3030当任何一个文件被更新时，Envoy 会自动重新加载配置。如果配置无效，Envoy 会输出错误，但会保持现有（工作）配置的运行。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"e54422a6e569b4864dab33ce8ed02584","permalink":"https://lib.jimmysong.io/envoy-handbook/dynamic-config/filesystem/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/envoy-handbook/dynamic-config/filesystem/","section":"envoy-handbook","summary":"动态提供配置另一种方式是通过指向文件系统上的文件。为了使动态配置发挥作用，我们需要在 node 字段下提供信息。如果我们可能有多个 Envoy 代理指向相同的配置文件，那么 node 字段是用来识别一个特定的 Envoy 实例。 动态配置 为了指向","tags":["Envoy"],"title":"来自文件系统的动态配置","type":"book"},{"authors":null,"categories":["Envoy"],"content":"我们可以在 HTTP 或 TCP 过滤器级别和监听器级别上配置访问记录器。我们还可以配置多个具有不同日志格式和日志沉积的访问日志。日志沉积（log sink） 是一个抽象的术语，指的是日志写入的位置，例如，写入控制台（stdout、stderr）、文件或网络服务。\n要配置多个访问日志的情况是，我们想在控制台（标准输出）中看到高级信息，并将完整的请求细节写入磁盘上的文件。用于配置访问记录器的字段被称为 access_log。\n让我们看看在 HTTP 连接管理器（HCM）层面上启用访问日志到标准输出（StdoutAccessLog）的例子。\n- filters:- name:envoy.filters.network.http_connection_managertyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManagerstat_prefix:ingress_httpaccess_log:- name:envoy.access_loggers.stdouttyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.access_loggers.stream.v3.StdoutAccessLogEnvoy 的目标是拥有可移植和可扩展的配置：类型化的配置。这样做的一个副作用是配置的名字很冗长。例如，为了启用访问日志，我们找到 HTTP 配置类型的名称，然后找到对应于控制台的类型（StdoutAccessLog）。\nStdoutAccessLog 配置将日志条目写到标准输出（控制台）。其他支持的访问日志沉积有以下几种：\n 文件 (FileAccessLog) gRPC（HttpGrpcAccessLogConfig 和 TcpGrpcAccessLogConfig） 标准错误（StderrAccessLog） Wasm (WasmAccessLog) Open Telemetry  文件访问日志允许我们将日志条目写到配置中指定的文件中。例如：\n- filters:- name:envoy.filters.network.http_connection_managertyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManagerstat_prefix:ingress_httpaccess_log:- name:envoy.access_loggers.filetyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.access_loggers.file.v3.FileAccessLogpath:./envoy-access-logs.log注意名称（envoy.access_loggers.file）和类型（file.v3.FileAccessLog）的变化。此外，我们还提供了我们希望 Envoy 存储访问日志的路径。\ngRPC 访问日志沉积将日志发送到 HTTP 或 TCP gRPC 日志服务。为了使用 gRPC 日志沉积，我们必须建立一个 gRPC 服务器，其端点要实现 MetricsService，特别是 StreamMetrics 函数。然后，Envoy 可以连接到 gRPC 服务器并将日志发送给它。\n在此之前，我们提到了默认的访问日志格式，它是由不同的命令操作符组成的。\n[%start_time%] \u0026#34;%req(:method)%req(x-envoy-original-path?:path)%%protocol%\u0026#34; %response_code% %response_flags% %bytes_received% %bytes_sent% %duration% %。 %resp(x-envoy-upstream-service-time)% \u0026#34;%req(x-forwarded-for)%\u0026#34; \u0026#34;%req(user-agent)%\u0026#34; \u0026#34;%req(x-request-id)%\u0026#34; \u0026#34;%req(:authority)%\u0026#34; \u0026#34;%upstream_host%\u0026#34; 日志条目的格式是可配置的，可以使用 log_format 字段进行修改。使用 log_format，我们可以配置日志条目包括哪些值，并指定我们是否需要纯文本或 JSON 格式的日志。\n例如，我们只想记录开始时间、响应代码和用户代理。我们会这样配置它。\n- filters:- name:envoy.filters.network.http_connection_managertyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManagerstat_prefix:ingress_httpaccess_log:- name:envoy.access_loggers.stdouttyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.access_loggers.stream.v3.StdoutAccessLoglog_format:text_format_source:inline_string:\u0026#34;%START_TIME% %RESPONSE_CODE% %REQ(USER-AGENT)%\u0026#34;一个使用上述格式的日志条目样本看起来是这样的：\n2021-11-01T21:32:27.170Z 404 curl/7.64.0 同样，如果我们希望日志是 JSON 等结构化格式，我们也可以不提供文本格式，而是设置 JSON 格式字符串。\n为了使用 JSON 格式，我们必须提供一个格式字典，而不是像纯文本格式那样提供一个单一的字符串。\n下面是一个使用相同的日志格式的例子，但用 JSON 写日志条目来代替。\n- filter:- name:envoy.filters.network.http_connection_managertyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions. filters.network.http_connection_manager.v3.HttpConnectionManagerstat_prefix:ingress_httpaccess_log:- name:envoy.access_loggers.stdouttyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.access_loggers.stream.v3.StdoutAccessLoglog_format:json_format:start_time:\u0026#34;%START_TIME%\u0026#34;response_code:\u0026#34;%response_code%\u0026#34;user_agent:\u0026#34;%req(user-agent)%\u0026#34;上述片段将产生以下日志条目。\n{\u0026#34;user_agent\u0026#34;:\u0026#34;curl/7.64.0\u0026#34;,\u0026#34;response_code\u0026#34;:404,\u0026#34;start_time\u0026#34;:\u0026#34;2021-11-01T21:37:59.979Z\u0026#34;} 某些命令操作符，如 FILTER_STATE 或 DYNAMIC_METADATA，可能产生嵌套的 JSON 日志条目。\n日志格式也可以使用通过 formatters 字段指定的 formatter 插件。当前版本中有两个已知的格式化插件：元数据（envoy.formatter.metadata）和无查询请求（envoy.formatter.req_without_query）扩展。\n元数据格式化扩展实现了 METADATA 命令操作符，允许我们输出不同类型的元数据（DYNAMIC、CLUSTER 或 ROUTE）。\n同样，req_without_query 格式化允许我们使用 REQ_WITHOUT_QUERY 命令操作符，其工作方式与 REQ 命令操作符相同，但会删除查询字符串。该命令操作符用于避免将任何敏感信息记录到访问日志中。\n下面是一个如何提供格式化器以及如何在 inline_string 中使用它的例子。\n- filters:- name:envoy.filters.network.http_connection_managertyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManagerstat_prefix:ingress_httpaccess_log:- name:envoy.access_loggers.stdouttyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.access_loggers.stream.v3.StdoutAccessLoglog_format:text_format_source:inline_string:\u0026#34;[%START_TIME%] %REQ(:METHOD)% %REQ_WITHOUT_QUERY(X-ENVOY-ORIGINAL-PATH?:PATH)% %PROTOCOL%\u0026#34;formatters:- name:envoy.formatter.req_without_querytyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.formatter.req_without_query.v3.ReqWithoutQuery上述配置中的这个请求 curl localhost:10000/?hello=1234 会产生一个不包括查询参数（hello=1234）的日志条目。\n[2021-11-01t21:48:55.941z] get / http/1.1 ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"aa23c8340b7b9ba50a1e20fc0d1a0734","permalink":"https://lib.jimmysong.io/envoy-handbook/logging/confguring-logging/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/envoy-handbook/logging/confguring-logging/","section":"envoy-handbook","summary":"我们可以在 HTTP 或 TCP 过滤器级别和监听器级别上配置访问记录器。我们还可以配置多个具有不同日志格式和日志沉积的访问日志。日志沉积（log sink） 是一个抽象的术语，指的是日志写入的位置，例如，写入控制台（st","tags":["Envoy"],"title":"配置访问记录器","type":"book"},{"authors":null,"categories":["Envoy"],"content":"/config_dump 端点是一种快速的方法，可以将当前加载的 Envoy 配置显示为 JSON 序列化的 proto 消息。\nEnvoy 输出以下组件的配置，并按照下面的顺序排列。\n 自举（bootstrap） 集群（clusters） 端点（endpoints） 监听器（listeners） 范围路由（scoped routes） 路由（routes） 秘密（secrets）  包括 EDS 配置 为了输出端点发现服务（EDS）的配置，我们可以在查询中加入 ?include_eds 参数。\n筛选输出 同样，我们可以通过提供我们想要包括的资源和一个掩码来过滤输出，以返回一个字段的子集。\n例如，为了只输出静态集群配置，我们可以在资源查询参数中使用 static_clusters 字段，从 ClustersConfigDump proto 在 resource 查询参数中使用。\n$ curl localhost:9901/config_dump?resource=static_clusters{\u0026#34;configs\u0026#34;: [{\u0026#34;@type\u0026#34;: \u0026#34;type.googleapis.com/envoy.admin.v3.ClustersConfigDump.StaticCluster\u0026#34;,\u0026#34;cluster\u0026#34;: {\u0026#34;@type\u0026#34;: \u0026#34;type.googleapis.com/envoy.config.cluster.v3.Cluster\u0026#34;,\u0026#34;name\u0026#34;: \u0026#34;instance_1\u0026#34;,},...{\u0026#34;@type\u0026#34;: \u0026#34;type.googleapis.com/envoy.admin.v3.ClustersConfigDump.StaticCluster\u0026#34;,\u0026#34;cluster\u0026#34;: {\u0026#34;@type\u0026#34;: \u0026#34;type.googleapis.com/envoy.config.cluster.v3.Cluster\u0026#34;,\u0026#34;name\u0026#34;: \u0026#34;instance_2\u0026#34;,...使用mask参数 为了进一步缩小输出范围，我们可以在 mask 参数中指定该字段。例如，只显示每个集群的 connect_timeout 值。\n$ curl localhost:9901/config_dump?resource=static_clusters\u0026amp;mask=cluster.connect_timeout{\u0026#34;configs\u0026#34;: [{\u0026#34;@type\u0026#34;: \u0026#34;type.googleapis.com/envoy.admin.v3.ClustersConfigDump.StaticCluster\u0026#34;,\u0026#34;cluster\u0026#34;: {\u0026#34;@type\u0026#34;: \u0026#34;type.googleapis.com/envoy.config.cluster.v3.Cluster\u0026#34;,\u0026#34;connect_timeout\u0026#34;: \u0026#34;5s\u0026#34;}},{\u0026#34;@type\u0026#34;: \u0026#34;type.googleapis.com/envoy.admin.v3.ClustersConfigDump.StaticCluster\u0026#34;,\u0026#34;cluster\u0026#34;: {\u0026#34;@type\u0026#34;: \u0026#34;type.googleapis.com/envoy.config.cluster.v3.Cluster\u0026#34;,\u0026#34;connect_timeout\u0026#34;: \u0026#34;5s\u0026#34;}},{\u0026#34;@type\u0026#34;: \u0026#34;type.googleapis.com/envoy.admin.v3.ClustersConfigDump.StaticCluster\u0026#34;,\u0026#34;cluster\u0026#34;: {\u0026#34;@type\u0026#34;: \u0026#34;type.googleapis.com/envoy.config.cluster.v3.Cluster\u0026#34;,\u0026#34;connect_timeout\u0026#34;: \u0026#34;1s\u0026#34;}}]}使用正则表达式 另一个过滤选项是指定一个正则表达式来匹配加载的配置的名称。例如，要输出所有名称字段与正则表达式 .*listener.* 相匹配的监听器，我们可以这样写。\n$ curl localhost:9901/config_dump?resource=static_clusters\u0026amp;name_regex=.*listener.* { \u0026#34;configs\u0026#34;: [ { \u0026#34;@type\u0026#34;: \u0026#34;type.googleapis.com/envoy.admin.v3.ListenersConfigDump.StaticListener\u0026#34;, \u0026#34;listener\u0026#34;: { \u0026#34;@type\u0026#34;: \u0026#34;type.googleapis.com/envoy.config.listener.v3.Listener\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;listener_0\u0026#34;, \u0026#34;address\u0026#34;: { \u0026#34;socket_address\u0026#34;: { \u0026#34;address\u0026#34;: \u0026#34;0.0.0.0\u0026#34;, \u0026#34;port_value\u0026#34;: 10000 } }, \u0026#34;filter_chains\u0026#34;: [ {} ] }, \u0026#34;last_updated\u0026#34;: \u0026#34;2021-11-15T20:06:51.208Z\u0026#34; } ] } 同样，/init_dump 端点列出了各种 Envoy 组件的未就绪目标的当前信息。和配置转储一样，我们可以使用 mask 查询参数来过滤特定字段。\n证书 /certs 输出所有加载的 TLS 证书。数据包括证书文件名、序列号、主题候补名称和到期前天数。结果是 JSON 格式的，遵循 admin.v3.Certificates proto。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"7d42322a4629f007648311537acd14fa","permalink":"https://lib.jimmysong.io/envoy-handbook/admin-interface/configuration-dump/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/envoy-handbook/admin-interface/configuration-dump/","section":"envoy-handbook","summary":"/config_dump 端点是一种快速的方法，可以将当前加载的 Envoy 配置显示为 JSON 序列化的 proto 消息。 Envoy 输出以下组件的配置，并按照下面的顺序排列。 自举（bootstrap） 集群（clusters） 端点（endpoints） 监听器（l","tags":["Envoy"],"title":"配置转储","type":"book"},{"authors":null,"categories":["Istio"],"content":"Service Mesh 又译作 “服务网格”，作为服务间通信的基础设施层。Buoyant 公司的 CEO Willian Morgan 在他的这篇文章 WHAT’S A SERVICE MESH? AND WHY DO I NEED ONE? 中解释了什么是 Service Mesh，为什么云原生应用需要 Service Mesh。\n服务网格是用于处理服务间通信的专用基础设施层。它负责通过包含现代云原生应用程序的复杂服务拓扑来可靠地传递请求。实际上，服务网格通常通过一组轻量级网络代理来实现，这些代理与应用程序代码一起部署，而不需要感知应用程序本身。—— Willian Morgan Buoyant CEO\n服务网格（Service Mesh）这个术语通常用于描述构成这些应用程序的微服务网络以及应用之间的交互。随着规模和复杂性的增长，服务网格越来越难以理解和管理。它的需求包括服务发现、负载均衡、故障恢复、指标收集和监控以及通常更加复杂的运维需求，例如 A/B 测试、金丝雀发布、限流、访问控制和端到端认证等。\n服务网格的特点 服务网格有如下几个特点：\n 应用程序间通讯的中间层 轻量级网络代理 应用程序无感知 解耦应用程序的重试/超时、监控、追踪和服务发现  目前两款流行的服务网格开源软件 Linkerd 和 Istio 都可以直接在 Kubernetes 中集成，其中 Linkerd 是 CNCF 成员项目，并在 2021 年 7 月毕业。Istio 在 2018年7月31日宣布 1.0，并在 2020 年 7 月将商标捐献给 Open Usage Commons。\n理解服务网格 如果用一句话来解释什么是服务网格，可以将它比作是应用程序或者说微服务间的 TCP/IP，负责服务之间的网络调用、限流、熔断和监控。对于编写应用程序来说一般无须关心 TCP/IP 这一层（比如通过 HTTP 协议的 RESTful 应用），同样使用服务网格也就无须关系服务之间的那些原来是通过应用程序或者其他框架实现的事情，比如 Spring Cloud、OSS，现在只要交给服务网格就可以了。\nPhil Calçado 在他的这篇博客 Pattern: Service Mesh 中详细解释了服务网格的来龙去脉：\n 从最原始的主机之间直接使用网线相连 网络层的出现 集成到应用程序内部的控制流 分解到应用程序外部的控制流 应用程序的中集成服务发现和断路器 出现了专门用于服务发现和断路器的软件包/库，如 Twitter 的 Finagle 和 Facebook 的 Proxygen，这时候还是集成在应用程序内部 出现了专门用于服务发现和断路器的开源软件，如 Netflix OSS、Airbnb 的 synapse 和 nerve 最后作为微服务的中间层服务网格出现  服务网格的架构如下图所示：\n   Service Mesh 架构图  图片来自：Pattern: Service Mesh\n服务网格作为 sidecar 运行，对应用程序来说是透明，所有应用程序间的流量都会通过它，所以对应用程序流量的控制都可以在 serivce mesh 中实现。\n服务网格如何工作？ 下面以 Istio 为例讲解服务网格如何在 Kubernetes 中工作。\n Istio 将服务请求路由到目的地址，根据中的参数判断是到生产环境、测试环境还是 staging 环境中的服务（服务可能同时部署在这三个环境中），是路由到本地环境还是公有云环境？所有的这些路由信息可以动态配置，可以是全局配置也可以为某些服务单独配置。 当 Istio 确认了目的地址后，将流量发送到相应服务发现端点，在 Kubernetes 中是 service，然后 service 会将服务转发给后端的实例。 Istio 根据它观测到最近请求的延迟时间，选择出所有应用程序的实例中响应最快的实例。 Istio 将请求发送给该实例，同时记录响应类型和延迟数据。 如果该实例挂了、不响应了或者进程不工作了，Istio 将把请求发送到其他实例上重试。 如果该实例持续返回 error，Istio 会将该实例从负载均衡池中移除，稍后再周期性得重试。 如果请求的截止时间已过，Istio 主动失败该请求，而不是再次尝试添加负载。 Istio 以 metric 和分布式追踪的形式捕获上述行为的各个方面，这些追踪信息将发送到集中 metric 系统。  为何使用服务网格？ 服务网格并没有给我们带来新功能，它是用于解决其他工具已经解决过的问题，只不过这次是在云原生的 Kubernetes 环境下的实现。\n在传统的 MVC 三层 Web 应用程序架构下，服务之间的通讯并不复杂，在应用程序内部自己管理即可，但是在现今的复杂的大型网站情况下，单体应用被分解为众多的微服务，服务之间的依赖和通讯十分复杂，出现了 Twitter 开发的 Finagle、Netflix 开发的 Hystrix 和 Google 的 Stubby 这样的 “胖客户端” 库，这些就是早期的服务网格，但是它们都近适用于特定的环境和特定的开发语言，并不能作为平台级的服务网格支持。\n在云原生架构下，容器的使用给予了异构应用程序的更多可行性，Kubernetes 增强的应用的横向扩容能力，用户可以快速的编排出复杂环境、复杂依赖关系的应用程序，同时开发者又无须过分关心应用程序的监控、扩展性、服务发现和分布式追踪这些繁琐的事情而专注于程序开发，赋予开发者更多的创造性。\n参考  Istio: A service mesh for AWS ECS - medium.com 初次了解 Istio - istio.io Application Network Functions With ESBs, API Management, and Now.. Service Mesh? - blog.christianposta.com Pattern: Service Mesh - philcalcado.com Envoy 官方文档中文版 - cloudnative.to Istio 官方文档 - istio.io  ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"a7028f00ff8cebd3881a160a4fe8a049","permalink":"https://lib.jimmysong.io/istio-handbook/intro/what-is-service-mesh/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/istio-handbook/intro/what-is-service-mesh/","section":"istio-handbook","summary":"Service Mesh 又译作 “服务网格”，作为服务间通信的基础设施层。Buoyant 公司的 CEO Willian Morgan 在他的这篇文章 WHAT’S A SERVICE MESH? AND WHY DO I NEED ONE? 中解释了什么是 Service Mesh，为什么云原生应用需要 Service Mesh。 服务网格是用于处理服","tags":["Istio","Service Mesh"],"title":"什么是服务网格？","type":"book"},{"authors":null,"categories":["Istio"],"content":"如果我们有在虚拟机上运行的工作负载，我们可以将它们连接到 Istio 服务网格，使其成为网格的一部分。\n带有虚拟机的 Istio 服务网格有两种架构：单网络架构和多网络架构。\n单网络架构 在这种情况下，有一个单一的网络。Kubernetes 集群和在虚拟机上运行的工作负载都在同一个网络中，它们可以直接相互通信。\n   单网络架构  单网络架构\n控制面的流量（配置更新、证书签署）是通过 Gateway 发送的。\n虚拟机被配置了网关地址，所以它们在启动时可以连接到控制平面。\n多网络架构 多网络架构横跨多个网络。Kubernetes 集群在一个网络内，而虚拟机则在另一个网络内。这使得 Kubernetes 集群中的 Pod 和虚拟机上的工作负载无法直接相互通信。\n   多网络架构  多网络架构\n所有的流量，控制面和 pod 到工作服的流量都流经网关，网关作为两个网络之间的桥梁。\nIstio 中如何表示虚拟机工作负载？ 在 Istio 服务网格中，有两种方式来表示虚拟机工作负载。\n工作负载组（WorkloadGroup 资源）类似于 Kubernetes 中的部署（Deployment），它代表了共享共同属性的虚拟机工作负载的逻辑组。\n描述虚拟机工作负载的第二种方法是使用工作负载条目（WorkloadEntry 资源）。工作负载条目类似于 Pod，它代表了一个虚拟机工作负载的单一实例。\n请注意，创建上述资源将不会提供或运行任何虚拟机工作负载实例。这些资源只是用来参考或指向虚拟机工作负载的。Istio 使用它们来了解如何适当地配置网格，将哪些服务添加到内部服务注册表中，等等。\n为了将虚拟机添加到网格中，我们需要创建一个工作负载组，作为模板。然后，当我们配置并将虚拟机添加到网格中时，控制平面会自动创建一个相应的 WorkloadEntry。\n我们已经提到，WorkloadEntry 的作用类似于 Pod。在添加虚拟机时，会创建 WorkloadEntry 资源，而当虚拟机的工作负载从网格中移除时，该资源会被自动删除。\n除了 WorkloadEntry 资源外，我们还需要创建一个 Kubernetes 服务。创建一个 Kubernetes 服务给了我们一个稳定的主机名和 IP 地址，以便使用选择器字段访问虚拟机工作负载和 pod。这也使我们能够通过 DestinationRule 和 VirtualService 资源使用 Istio 的路由功能。\n 下一章   ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"9886638b7634e816bd7e470cd4e5ff32","permalink":"https://lib.jimmysong.io/istio-handbook/advanced/vm/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/istio-handbook/advanced/vm/","section":"istio-handbook","summary":"如果我们有在虚拟机上运行的工作负载，我们可以将它们连接到 Istio 服务网格，使其成为网格的一部分。 带有虚拟机的 Istio 服务网格有两种架构：单网络架构和多网络架构。 单网络架构 在这种情况下，有一个单一的网络。Kuber","tags":["Istio","Service Mesh"],"title":"虚拟机负载","type":"book"},{"authors":null,"categories":["Envoy"],"content":"Envoy 支持端点上不同的主动健康检查方法。HTTP、TCP、gRPC 和 Redis 健康检查。健康检查方法可以为每个集群单独配置。我们可以通过集群配置中的 health_checks 字段来配置健康检查。\n无论选择哪种健康检查方法，都需要定义几个常见的配置设置。\n超时（timeout）表示分配给等待健康检查响应的时间。如果在这个字段中指定的时间值内没有达到响应，健康检查尝试将被视为失败。间隔（internal）指定健康检查之间的时间节奏。例如，5 秒的间隔将每 5 秒触发一次健康检查。\n其他两个必要的设置可用于确定一个特定的端点何时被认为是健康或不健康的。healthy_threshold 指定在一个端点被标记为健康之前所需的 “健康” 检查（例如，HTTP 200 响应）的数量。unhealthy_threshold 的作用与此相同，但是对于 “不健康” 的健康检查，它指定了在一个端点被标记为不健康之前所需的不健康检查的数量。\n1. HTTP 健康检查\nEnvoy 向端点发送一个 HTTP 请求。如果端点回应的是 HTTP 200，Envoy 认为它是健康的。200 响应是默认的响应，被认为是健康响应。使用 expected_statuses 字段，我们可以通过提供一个被认为是健康的 HTTP 状态的范围来进行自定义。\n如果端点以 HTTP 503 响应，unhealthy_threshold 被忽略，并且端点立即被认为是不健康的。\nclusters:- name:my_cluster_namehealth_checks:- timeout:1sinterval:0.25sunhealthy_threshold:5healthy_threshold:2http_health_check:path:\u0026#34;/health\u0026#34;expected_statuses:- start:200end:299...例如，上面的片段定义了一个 HTTP 健康检查，Envoy 将向集群中的端点发送一个 /health 路径的 HTTP 请求。Envoy 每隔 0.25s（internal）发送一次请求，在超时前等待 1s（timeout）。要被认为是健康的，端点必须以 200 和 299 之间的状态（expected_statuses）响应两次（healthy_threshold）。端点需要以任何其他状态代码响应五次（unhealthy_threshold）才能被认为是不健康的。此外，如果端点以 HTTP 503 响应，它将立即被视为不健康（unhealthy_threshold 设置被忽略）。\n2. TCP 健康检查\n我们指定一个 Hex 编码的有效载荷（例如：68656C6C6F），并将其发送给终端。如果我们设置了一个空的有效载荷，Envoy 将进行仅连接的健康检查，它只尝试连接到端点，如果连接成功就认为是成功的。\n除了被发送的有效载荷外，我们还需要指定响应。Envoy 将对响应进行模糊匹配，如果响应与请求匹配，则认为该端点是健康的。\nclusters:- name:my_cluster_namehealth_checks:- timeout:1sinterval:0.25sunhealthy_threshold:1healthy_threshold:1tcp_health_check:send:text:\u0026#34;68656C6C6F\u0026#34;receive:- text:\u0026#34;68656C6C6F\u0026#34;...3. gRPC 健康检查\n本健康检查遵循 grpc.health.v1.Health 健康检查协议。查看 GRPC 健康检查协议文档以了解更多关于其工作方式的信息。\n我们可以设置的两个可选的配置值是 service_name 和 authority。服务名称是设置在 grpc.health.v1.Health 的 HealthCheckRequest 的 service 字段中的值。授权是 :authority 头的值。如果它是空的，Envoy 会使用集群的名称。\nclusters:- name:my_cluster_namehealth_checks:- timeout:1sinterval:0.25sunhealthy_threshold:1healthy_threshold:1grpc_health_check:{}...4. Redis 健康检查\nRedis 健康检查向端点发送一个 Redis PING 命令，并期待一个 PONG 响应。如果上游的 Redis 端点回应的不是 PONG，就会立即导致健康检查失败。我们也可以指定一个 key，Envoy 会执行 EXIST \u0026lt;key\u0026gt; 命令，而不是 PING 命令。如果 Redis 的返回值是 0（即密钥不存在），那么该端点就是健康的。任何其他响应都被视为失败。\nclusters:- name:my_cluster_namehealth_checks:- timeout:1sinterval:0.25sunhealthy_threshold:1healthy_threshold:1redis_health_check:key:\u0026#34;maintenance\u0026#34;...上面的例子检查键 “维护”（如 EXIST maintainance），如果键不存在，健康检查就通过。\nHTTP 健康检查过滤器 HTTP 健康检查过滤器可以用来限制产生的健康检查流量。过滤器可以在不同的操作模式下运行，控制流量是否被传递给本地服务（即不传递或传递）。\n1. 非穿透模式\n当以非穿透模式运行时，健康检查请求永远不会被发送到本地服务。Envoy 会以 HTTP 200 或 HTTP 503 进行响应，这取决于服务器当前的耗尽状态。\n非穿透模式的一个变种是，如果上游集群中至少有指定比例的端点可用，则返回 HTTP 200。端点的百分比可以用 cluster_min_healthy_percentages 字段来配置。\n...pass_through_mode:falsecluster_min_healthy_percentages:value:15...2. 穿透模式\n在穿透模式下，Envoy 将每个健康检查请求传递给本地服务。该服务可以用 HTTP 200 或 HTTP 503 来响应。\n穿透模式的另一个设置是使用缓存。Envoy 将健康检查请求传递给服务，并将结果缓存一段时间（cache_time）。任何后续的健康检查请求将使用缓存起来的值。一旦缓存失效，下一个健康检查请求会再次传递给服务。\n...pass_through_mode:truecache_time:5m...上面的片段启用了穿透模式，缓存在 5 分钟内到期。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"62c39e59a4266a944fe046b7d886c069","permalink":"https://lib.jimmysong.io/envoy-handbook/cluster/active-health-checking/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/envoy-handbook/cluster/active-health-checking/","section":"envoy-handbook","summary":"Envoy 支持端点上不同的主动健康检查方法。HTTP、TCP、gRPC 和 Redis 健康检查。健康检查方法可以为每个集群单独配置。我们可以通过集群配置中的 health_checks 字段来配置健康检查。 无论选择哪种健康检查方法，都需要定义几个常","tags":["Envoy"],"title":"主动健康检查","type":"book"},{"authors":null,"categories":["云原生"],"content":"众所周知，Kubernetes 是 Google 于 2014 年 6 月基于其内部使用的 Borg 系统开源出来的容器编排调度引擎。其实从 2000 年开始，Google 就开始基于容器研发三个容器管理系统，分别是 Borg、Omega 和 Kubernetes。这篇由 Google 工程师 Brendan Burns、Brian Grant、David Oppenheimer、Eric Brewer 和 John Wilkes 几人在 2016 年发表的《Borg, Omega, and Kubernetes》论文里，阐述了 Google 从 Borg 到 Kubernetes 这个旅程中所获得知识和经验教训。\nBorg、Omega 和 Kubernetes Google 从 2000 年初就开始使用容器（Linux 容器）系统，Google 开发出来的第一个统一的容器管理系统在内部称之为 “Borg”，用来管理长时间运行的生产服务和批处理服务。由于 Borg 的规模、功能的广泛性和超高的稳定性，一直到现在 Borg 在 Google 内部依然是主要的容器管理系统。\nGoogle 的第二套容器管理系统叫做 Omega，作为 Borg 的延伸，它的出现是出于提升 Borg 生态系统软件工程的愿望。Omega 应用到了很多在 Borg 内已经被认证的成功的模式，但是是从头开始来搭建以期更为一致的构架。由于越来越多的应用被开发并运行在 Borg 上，Google 开发了一个广泛的工具和服务的生态系统。它被应用到了很多在 Borg 内已经被认证的成功的模式，但是是从头开始来搭建以期更为一致的构架。这些系统提供了配置和更新 job 的机制，能够预测资源需求，动态地对在运行中的程序推送配置文件、服务发现、负载均衡、自动扩容、机器生命周期管理、额度管理等。许多 Omega 的创新（包括多个调度器）都被收录进了 Borg。\nGoogle 的第三套容器管理系统就是我们所熟知的 Kubernetes，它是针对在 Google 外部的对 Linux 容器感兴趣的开发者以及 Google 在公有云底层商业增长的考虑而研发的。和 Borg、Omega 完全是谷歌内部系统相比，Kubernetes 是开源的。像 Omega 一样，Kubernetes 在其核心有一个被分享的持久存储，有组件来检测相关 object 的变化。跟 Omega 不同的是，Omega 把存储直接暴露给信任的控制平面的组件，而在 Kubernete 中，提供了完全由特定领域更高层面的版本控制、认证、语义、策略的 REST API 接口，以服务更多的用户。更重要的是，Kubernetes 是由一群底层开发能力更强的开发者开发的，他们主要的设计目标是用更容易的方法去部署和管理复杂的分布式系统，同时仍能从容器提升的效率中受益。\n2014 年 Kubernetes 正式开源，2015 年被作为初创项目贡献给了云原生计算基金会（CNCF），从此开启了 Kubernetes 及云原生化的大潮。\n参考  Borg, Omega, and Kubernetes: Lessons learned from three container-management systems over a decade - queue.acm.org Borg、Omega 和 Kubernetes：谷歌十几年来从这三个容器管理系统中得到的经验教训 - dockone.io  ","date":1651532400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"0dbf9fd33c5e6751f26ab31f4d23451f","permalink":"https://lib.jimmysong.io/cloud-native-handbook/kubernetes/history/","publishdate":"2022-05-03T00:00:00+01:00","relpermalink":"/cloud-native-handbook/kubernetes/history/","section":"cloud-native-handbook","summary":"众所周知，Kubernetes 是 Google 于 2014 年 6 月基于其内部使用的 Borg 系统开源出来的容器编排调度引擎。其实从 2000 年开始，Google 就开始基于容器研发三个容器管理系统，分别是 Borg、Omega 和 Kuberne","tags":["云原生"],"title":"Kubernetes 的历史","type":"book"},{"authors":null,"categories":["安全"],"content":"要创建一个 pod-reader 角色，创建一个 YAML 文件，内容如下：\napiVersion:rbac.authorization.k8s.io/v1kind:Rolemetadata:namespace:your-namespace-namename:pod-readerrules:- apiGroups:[\u0026#34;\u0026#34;]# \u0026#34;\u0026#34; 表示核心 API 组resources:[\u0026#34;pods\u0026#34;]verbs:[\u0026#34;get\u0026#34;,\u0026#34;watch\u0026#34;,\u0026#34;list\u0026#34;]应用角色：\nkubectl apply --f role.yaml 要创建一个全局性的 pod-reader ClusterRole：\napiVersion:rbac.authorization.k8s.io/v1kind:ClusterRolemetadata:default# \u0026#34;namespace\u0026#34; 被省略了，因为 ClusterRoles 没有被绑定到一个命名空间上name:global-pod-readerrules:- apiGroups:[\u0026#34;\u0026#34;]# \u0026#34;\u0026#34; 表示核心 API 组resources:[\u0026#34;pods\u0026#34;]verbs:[\u0026#34;get\u0026#34;,\u0026#34;watch\u0026#34;,\u0026#34;list\u0026#34;]应用角色：\nkubectl apply --f clusterrole.yaml ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"eecee7c855abb6e4669480431b8a6ee2","permalink":"https://lib.jimmysong.io/kubernetes-hardening-guidance/appendix/j/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/kubernetes-hardening-guidance/appendix/j/","section":"kubernetes-hardening-guidance","summary":"要创建一个 pod-reader 角色，创建一个 YAML 文件，内容如下： apiVersion:rbac.authorization.k8s.io/v1kind:Rolemetadata:namespace:your-namespace-namename:pod-readerrules:- apiGroups:[\"\"]# \"\" 表示核心 API 组resources:[\"pods\"]verbs:[\"get\",\"watch\",\"l","tags":["Kubernetes","安全"],"title":"附录 J：pod-reader RBAC 角色","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"Node 是 Kubernetes 集群的工作节点，可以是物理机也可以是虚拟机。\nNode 的状态 Node 包括如下状态信息：\n Address  HostName：可以被 kubelet 中的 --hostname-override 参数替代。 ExternalIP：可以被集群外部路由到的 IP 地址。 InternalIP：集群内部使用的 IP，集群外部无法访问。   Condition  OutOfDisk：磁盘空间不足时为 True Ready：Node controller 40 秒内没有收到 node 的状态报告为 Unknown，健康为 True，否则为 False。 MemoryPressure：当 node 有内存压力时为 True，否则为 False。 DiskPressure：当 node 有磁盘压力时为 True，否则为 False。   Capacity  CPU 内存 可运行的最大 Pod 个数   Info：节点的一些版本信息，如 OS、kubernetes、docker 等  Node 管理 禁止 Pod 调度到该节点上。\nkubectl cordon \u0026lt;node\u0026gt; 驱逐该节点上的所有 Pod。\nkubectl drain \u0026lt;node\u0026gt; 该命令会删除该节点上的所有 Pod（DaemonSet 除外），在其他 node 上重新启动它们，通常该节点需要维护时使用该命令。直接使用该命令会自动调用kubectl cordon \u0026lt;node\u0026gt;命令。当该节点维护完成，启动了 kubelet 后，再使用kubectl uncordon \u0026lt;node\u0026gt; 即可将该节点添加到 kubernetes 集群中。\n","date":1653062400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"58d8ffa20906e80b3cfa356076a4a2bc","permalink":"https://lib.jimmysong.io/kubernetes-handbook/cluster/node/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/cluster/node/","section":"kubernetes-handbook","summary":"Node 是 Kubernetes 集群的工作节点，可以是物理机也可以是虚拟机。 Node 的状态 Node 包括如下状态信息： Address HostName：可以被 kubelet 中的 --hostname-override 参数替代。 ExternalIP：可以被集群外部路由到的 IP 地址。 InternalIP：集群","tags":["Kubernetes"],"title":"Node","type":"book"},{"authors":null,"categories":["安全"],"content":"要创建一个 RoleBinding，需创建一个 YAML 文件，内容如下：\napiVersion:rbac.authorization.k8s.io/v1# 这个角色绑定允许 \u0026#34;jane\u0026#34; 读取 \u0026#34;your-namespace-name\u0026#34; 的 Pod 命名空间# 你需要在该命名空间中已经有一个名为 \u0026#34;pod-reader\u0026#34;的角色。kind:RoleBindingmetadata:name:read-podsnamespace:your-namespace-namesubjects:# 你可以指定一个以上的 \u0026#34;subject\u0026#34;- kind:Username:jane# \u0026#34;name\u0026#34; 是大小写敏感的apiGroup:rbac.authorization.k8s.ioroleRef:# \u0026#34;roleRef\u0026#34; 指定绑定到一个 Role/ClusterRolekind:Role# 必须是 Role 或 ClusterRolename:pod-reader# 这必须与你想绑定的 Role 或 ClusterRole 的名字相匹配apiGroup:rbac.authorization.k8s.io应用 RoleBinding：\nkubectl apply --f rolebinding.yaml 要创建一个ClusterRoleBinding，请创建一个 YAML 文件，内容如下：\napiVersion:rbac.authorization.k8s.io/v1# 这个集群角色绑定允许 \u0026#34;manager\u0026#34; 组中的任何人在任何命名空间中读取 Pod 信息。kind:ClusterRoleBindingmetadata:name:global-pod-readersubjects:# 你可以指定一个以上的 \u0026#34;subject\u0026#34;- kind:Groupname:manager# Name 是大小写敏感的apiGroup:rbac.authorization.k8s.ioroleRef:# \u0026#34;roleRef\u0026#34; 指定绑定到一个 Role/ClusterRolekind:ClusterRole# 必须是 Role 或 ClusterRolename:global-pod-reader# 这必须与你想绑定的 Role 或 ClusterRole 的名字相匹配apiGroup:rbac.authorization.k8s.io应用 RoleBinding：\nkubectl apply --f clusterrolebinding.yaml ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"67e8de9e8b00eb09e8a6db1fd06666a2","permalink":"https://lib.jimmysong.io/kubernetes-hardening-guidance/appendix/k/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/kubernetes-hardening-guidance/appendix/k/","section":"kubernetes-hardening-guidance","summary":"要创建一个 RoleBinding，需创建一个 YAML 文件，内容如下： apiVersion:rbac.authorization.k8s.io/v1# 这个角色绑定允许 \"jane\" 读取 \"your-namespace-name\" 的 Pod 命名空间# 你需要在该命名空间中已经有一个名为 \"pod-reader\"的角色。kind:Rol","tags":["Kubernetes","安全"],"title":"附录K：RBAC RoleBinding 和 ClusterRoleBinding 示例","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"在一个 Kubernetes 集群中可以使用 namespace 创建多个 “虚拟集群”，这些 namespace 之间可以完全隔离，也可以通过某种方式，让一个 namespace 中的 service 可以访问到其他的 namespace 中的服务。\n哪些情况下适合使用多个 namespace 因为 namespace 可以提供独立的命名空间，因此可以实现部分的环境隔离。当你的项目和人员众多的时候可以考虑根据项目属性，例如生产、测试、开发划分不同的 namespace。\nNamespace 使用 获取集群中有哪些 namespace\nkubectl get ns\n集群中默认会有 default 和 kube-system 这两个 namespace。\n在执行 kubectl 命令时可以使用 -n 指定操作的 namespace。\n用户的普通应用默认是在 default 下，与集群管理相关的为整个集群提供服务的应用一般部署在 kube-system 的 namespace 下，例如我们在安装 kubernetes 集群时部署的 kubedns、heapseter、EFK 等都是在这个 namespace 下面。\n另外，并不是所有的资源对象都会对应 namespace，node 和 persistentVolume 就不属于任何 namespace。\n","date":1653062400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"97998313c63c211c55d1157c7de13228","permalink":"https://lib.jimmysong.io/kubernetes-handbook/cluster/namespace/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/cluster/namespace/","section":"kubernetes-handbook","summary":"在一个 Kubernetes 集群中可以使用 namespace 创建多个 “虚拟集群”，这些 namespace 之间可以完全隔离，也可以通过某种方式，让一个 namespace 中的 service 可以访问到其他的 namespace 中的服务。 哪些情况下适合使用多个 namespace 因为 namespace 可以提供独立的命名空间，因此可以实现部","tags":["Kubernetes"],"title":"Namespace","type":"book"},{"authors":null,"categories":["安全"],"content":"下面是一个审计策略，它以最高级别记录所有审计事件：\napiVersion:audit.k8s.io/v1kind:Policyrules:- level:RequestResponse# 这个审计策略记录了 RequestResponse 级别的所有审计事件这种审计策略在最高级别上记录所有事件。如果一个组织有可用的资源来存储、解析和检查大量的日志，那么在最高级别上记录所有事件是一个很好的方法，可以确保当事件发生时，所有必要的背景信息都出现在日志中。如果资源消耗和可用性是一个问题，那么可以建立更多的日志规则来降低非关键组件和常规非特权操作的日志级别，只要满足系统的审计要求。如何建立这些规则的例子可以在 Kubernetes 官方文档中找到。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"3ed7fe5e2385b416449f27ff26dd515e","permalink":"https://lib.jimmysong.io/kubernetes-hardening-guidance/appendix/l/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/kubernetes-hardening-guidance/appendix/l/","section":"kubernetes-hardening-guidance","summary":"下面是一个审计策略，它以最高级别记录所有审计事件： apiVersion:audit.k8s.io/v1kind:Policyrules:- level:RequestResponse# 这个审计策略记录了 RequestResponse 级别的所有审计事件这种审计策略在最高级别上记录所有事件。如果一个组织有可用的资源来存储、解析和检查大量的日志，那么在最高级别上","tags":["Kubernetes","安全"],"title":"附录 L：审计策略","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"Label 是附着到 object 上（例如 Pod）的键值对。可以在创建 object 的时候指定，也可以在 object 创建后随时指定。Labels 的值对系统本身并没有什么含义，只是对用户才有意义。\n\u0026#34;labels\u0026#34;: { \u0026#34;key1\u0026#34; : \u0026#34;value1\u0026#34;, \u0026#34;key2\u0026#34; : \u0026#34;value2\u0026#34; } Kubernetes 最终将对 labels 最终索引和反向索引用来优化查询和 watch，在 UI 和命令行中会对它们排序。不要在 label 中使用大型、非标识的结构化数据，记录这样的数据应该用 annotation。\n动机 Label 能够将组织架构映射到系统架构上（就像是康威定律），这样能够更便于微服务的管理，你可以给 object 打上如下类型的 label：\n \u0026#34;release\u0026#34; : \u0026#34;stable\u0026#34;, \u0026#34;release\u0026#34; : \u0026#34;canary\u0026#34; \u0026#34;environment\u0026#34; : \u0026#34;dev\u0026#34;, \u0026#34;environment\u0026#34; : \u0026#34;qa\u0026#34;, \u0026#34;environment\u0026#34; : \u0026#34;production\u0026#34; \u0026#34;tier\u0026#34; : \u0026#34;frontend\u0026#34;, \u0026#34;tier\u0026#34; : \u0026#34;backend\u0026#34;, \u0026#34;tier\u0026#34; : \u0026#34;cache\u0026#34; \u0026#34;partition\u0026#34; : \u0026#34;customerA\u0026#34;, \u0026#34;partition\u0026#34; : \u0026#34;customerB\u0026#34; \u0026#34;track\u0026#34; : \u0026#34;daily\u0026#34;, \u0026#34;track\u0026#34; : \u0026#34;weekly\u0026#34; \u0026#34;team\u0026#34; : \u0026#34;teamA\u0026#34;,\u0026#34;team:\u0026#34; : \u0026#34;teamB\u0026#34;  语法和字符集 Label key 的组成：\n 不得超过 63 个字符 可以使用前缀，使用 / 分隔，前缀必须是 DNS 子域，不得超过 253 个字符，系统中的自动化组件创建的 label 必须指定前缀，kubernetes.io/ 由 kubernetes 保留 起始必须是字母（大小写都可以）或数字，中间可以有连字符、下划线和点  Label value 的组成：\n 不得超过 63 个字符 起始必须是字母（大小写都可以）或数字，中间可以有连字符、下划线和点  Label selector Label 不是唯一的，很多 object 可能有相同的 label。\n通过 label selector，客户端／用户可以指定一个 object 集合，通过 label selector 对 object 的集合进行操作。\nLabel selector 有两种类型：\n equality-based ：可以使用 =、==、!= 操作符，可以使用逗号分隔多个表达式 set-based ：可以使用 in、notin、! 操作符，另外还可以没有操作符，直接写出某个 label 的 key，表示过滤有某个 key 的 object 而不管该 key 的 value 是何值，! 表示没有该 label 的 object  示例 $ kubectl get pods -l environment=production,tier=frontend $ kubectl get pods -l \u0026#39;environment in (production),tier in (frontend)\u0026#39; $ kubectl get pods -l \u0026#39;environment in (production, qa)\u0026#39; $ kubectl get pods -l \u0026#39;environment,environment notin (frontend)\u0026#39; 在 API object 中设置 label selector 在 service、replicationcontroller 等 object 中有对 pod 的 label selector，使用方法只能使用等于操作，例如：\nselector:component:redis在 Job、Deployment、ReplicaSet 和 DaemonSet 这些 object 中，支持 set-based 的过滤，例如：\nselector:matchLabels:component:redismatchExpressions:- {key: tier, operator: In, values:[cache]}- {key: environment, operator: NotIn, values:[dev]}如 Service 通过 label selector 将同一类型的 pod 作为一个服务 expose 出来。\n   Label 示意图  另外在 node affinity 和 pod affinity 中的 label selector 的语法又有些许不同，示例如下：\naffinity:nodeAffinity:requiredDuringSchedulingIgnoredDuringExecution:nodeSelectorTerms:- matchExpressions:- key:kubernetes.io/e2e-az-nameoperator:Invalues:- e2e-az1- e2e-az2preferredDuringSchedulingIgnoredDuringExecution:- weight:1preference:matchExpressions:- key:another-node-label-keyoperator:Invalues:- another-node-label-value","date":1653062400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"b6816fe54c4b39e3ab501b0397ae1d48","permalink":"https://lib.jimmysong.io/kubernetes-handbook/cluster/label/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/cluster/label/","section":"kubernetes-handbook","summary":"Label 是附着到 object 上（例如 Pod）的键值对。可以在创建 object 的时候指定，也可以在 object 创建后随时指定。Labels 的值对系统本身并没有什么含义，只是对用户才有意义。 \"labels\": { \"key1\" : \"value1\", \"key2\" : \"value2\" } Kubernetes 最终将对 labels 最终索引和反向索引用","tags":["Kubernetes"],"title":"Label","type":"book"},{"authors":null,"categories":["安全"],"content":"在控制平面，用文本编辑器打开 kube-apiserver.yaml 文件。编辑 kube-apiserver 配置需要管理员权限。\nsudo vi /etc/kubernetes/manifests/kube-apiserver.yaml 在 kube-apiserver.yaml 文件中添加以下文字：\n--audit-policy-file=/etc/kubernetes/policy/audit-policy.yaml --audit-log-path=/var/log/audit.log --audit-log-maxage=1825 audit-policy-file 标志应该设置为审计策略的路径，而 audit-log-path 标志应该设置为所需的审计日志写入的安全位置。还有一些其他的标志，比如这里显示的 audit-log-maxage 标志，它规定了日志应该被保存的最大天数，还有一些标志用于指定要保留的最大审计日志文件的数量，最大的日志文件大小（兆字节）等等。启用日志记录的唯一必要标志是 audit-policy-file 和 audit-log-path 标志。其他标志可以用来配置日志，以符合组织的政策。\n如果用户的 kube-apiserver 是作为 Pod 运行的，那么就有必要挂载卷，并配置策略和日志文件位置的 hostPath 以保留审计记录。这可以通过在 Kubernetes 文档中指出的 kube-apiserver.yaml 文件中添加以下部分来完成：\nvolumeMounts:- mountPath:/etc/kubernetes/audit-policy.yamlname:auditreadOnly:true- mountPath:/var/log/audit.logname:audit-logreadOnly:falsevolumes:- hostPath:path:/etc/kubernetes/audit-policy.yamltype:Filename:audit- hostPath:path:/var/log/audit.logtype:FileOrCreatename:audit-log","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"d2f2b69d731384bd9b72c7001706ec0a","permalink":"https://lib.jimmysong.io/kubernetes-hardening-guidance/appendix/m/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/kubernetes-hardening-guidance/appendix/m/","section":"kubernetes-hardening-guidance","summary":"在控制平面，用文本编辑器打开 kube-apiserver.yaml 文件。编辑 kube-apiserver 配置需要管理员权限。 sudo vi /etc/kubernetes/manifests/kube-apiserver.yaml 在 kube-apiserver.yaml 文件中添加以下文字： --audit-policy-file=/etc/kubernetes/policy/audit-policy.yaml --audit-log-path=/var/log/audit.log --audit-log-maxage=1825 audit-policy-file 标志应该设置为审计策略的路径，而 audit-log-path 标志应该设置为所需的审计日志写入的安全位置。还有一些其他的标志，比","tags":["Kubernetes","安全"],"title":"附录 M：向 kube-apiserver 提交审计策略文件的标志示例","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"Annotation，顾名思义，就是注解。Annotation 可以将 Kubernetes 资源对象关联到任意的非标识性元数据。使用客户端（如工具和库）可以检索到这些元数据。\n关联元数据到对象 Label 和 Annotation 都可以将元数据关联到 Kubernetes 资源对象。Label 主要用于选择对象，可以挑选出满足特定条件的对象。相比之下，annotation 不能用于标识及选择对象。annotation 中的元数据可多可少，可以是结构化的或非结构化的，也可以包含 label 中不允许出现的字符。\nAnnotation 和 label 一样都是 key/value 键值对映射结构：\n\u0026#34;annotations\u0026#34;: {\u0026#34;key1\u0026#34;:\u0026#34;value1\u0026#34;,\u0026#34;key2\u0026#34;:\u0026#34;value2\u0026#34;} 以下列出了一些可以记录在 annotation 中的对象信息：\n  声明配置层管理的字段。使用 annotation 关联这类字段可以用于区分以下几种配置来源：客户端或服务器设置的默认值，自动生成的字段或自动生成的 auto-scaling 和 auto-sizing 系统配置的字段。\n  创建信息、版本信息或镜像信息。例如时间戳、版本号、git 分支、PR 序号、镜像哈希值以及仓库地址。\n  记录日志、监控、分析或审计存储仓库的指针\n  可以用于 debug 的客户端（库或工具）信息，例如名称、版本和创建信息。\n  用户信息，以及工具或系统来源信息、例如来自非 Kubernetes 生态的相关对象的 URL 信息。\n  轻量级部署工具元数据，例如配置或检查点。\n  负责人的电话或联系方式，或能找到相关信息的目录条目信息，例如团队网站。\n  如果不使用 annotation，您也可以将以上类型的信息存放在外部数据库或目录中，但这样做不利于创建用于部署、管理、内部检查的共享工具和客户端库。\n示例 如 Istio 的 Deployment 配置中就使用到了 annotation：\napiVersion:extensions/v1beta1kind:Deploymentmetadata:name:istio-managerspec:replicas:1template:metadata:annotations:alpha.istio.io/sidecar:ignorelabels:istio:managerspec:serviceAccountName:istio-manager-service-accountcontainers:- name:discoveryimage:harbor-001.jimmysong.io/library/manager:0.1.5imagePullPolicy:Alwaysargs:[\u0026#34;discovery\u0026#34;,\u0026#34;-v\u0026#34;,\u0026#34;2\u0026#34;]ports:- containerPort:8080env:- name:POD_NAMESPACEvalueFrom:fieldRef:apiVersion:v1fieldPath:metadata.namespace- name:apiserverimage:harbor-001.jimmysong.io/library/manager:0.1.5imagePullPolicy:Alwaysargs:[\u0026#34;apiserver\u0026#34;,\u0026#34;-v\u0026#34;,\u0026#34;2\u0026#34;]ports:- containerPort:8081env:- name:POD_NAMESPACEvalueFrom:fieldRef:apiVersion:v1fieldPath:metadata.namespacealpha.istio.io/sidecar 注解就是用来控制是否自动向 pod 中注入 sidecar 的。\n","date":1653062400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"fff5f38baa342c0a63ea1f4a7007b8bc","permalink":"https://lib.jimmysong.io/kubernetes-handbook/cluster/annotation/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/cluster/annotation/","section":"kubernetes-handbook","summary":"Annotation，顾名思义，就是注解。Annotation 可以将 Kubernetes 资源对象关联到任意的非标识性元数据。使用客户端（如工具和库）可以检索到这些元数据。 关联元数据到对象 Label 和 Annotation 都可以将元数据关联到 Kubernetes 资源","tags":["Kubernetes"],"title":"Annotation","type":"book"},{"authors":null,"categories":["安全"],"content":"YAML 文件示例：\napiVersion:v1kind:Configpreferences:{}clusters:- name:example-clustercluster:server:http://127.0.0.1:8080#web endpoint address for the log files to be sent toname:audit-webhook-serviceusers:- name:example-usersuser:username:example-userpassword:example-passwordcontexts:- name:example-contextcontext:cluster:example-clusteruser:example-usercurrent-context:example-context#source: https://dev.bitolog.com/implement-audits-webhook/由 webhook 发送的审计事件是以 HTTP POST 请求的形式发送的，请求体中包含 JSON 审计事件。指定的地址应该指向一个能够接受和解析这些审计事件的端点，无论是第三方服务还是内部配置的端点。\n向 kube-apiserver 提交 webhook 配置文件的标志示例：\n在控制面编辑 kube-apiserver.yaml 文件\nsudo vi /etc/kubernetes/manifests/kube-apiserver.yaml 在 kube-apiserver.yaml 文件中添加以下文字\n--audit-webhook-config-file=/etc/kubernetes/policies/webhook-policy.yaml --audit-webhook-initial-backoff=5 --audit-webhook-mode=batch --audit-webhook-batch-buffer-size=5 audit-webhook-initial-backoff 标志决定了在一个初始失败的请求后要等待多长时间才能重试。可用的 webhook 模式有 batch、block 和 blocking-stric 的。当使用批处理模式时，有可能配置最大等待时间、缓冲区大小等。Kubernetes 官方文档包含了其他配置选项的更多细节审计和 kube-apiserver。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"73c346d2bfa3e04d2c95bdd26d4ad0b5","permalink":"https://lib.jimmysong.io/kubernetes-hardening-guidance/appendix/n/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/kubernetes-hardening-guidance/appendix/n/","section":"kubernetes-hardening-guidance","summary":"YAML 文件示例： apiVersion:v1kind:Configpreferences:{}clusters:- name:example-clustercluster:server:http://127.0.0.1:8080#web endpoint address for the log files to be sent toname:audit-webhook-serviceusers:- name:example-usersuser:username:example-userpassword:example-passwordcontexts:- name:example-contextcontext:cluster:example-clusteruser:example-usercurrent-context:example-context#source: https://dev.bitolog.com/implement-audits-webhook/由 webhook 发送的审计事件是以 HTTP POST 请求的形式发送的，请求体中包含 JSON 审计","tags":["Kubernetes","安全"],"title":"附录 N：webhook 配置","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"Taint（污点）和 Toleration（容忍）可以作用于 node 和 pod 上，其目的是优化 pod 在集群间的调度，这跟节点亲和性类似，只不过它们作用的方式相反，具有 taint 的 node 和 pod 是互斥关系，而具有节点亲和性关系的 node 和 pod 是相吸的。另外还有可以给 node 节点设置 label，通过给 pod 设置 nodeSelector 将 pod 调度到具有匹配标签的节点上。\nTaint 和 toleration 相互配合，可以用来避免 pod 被分配到不合适的节点上。每个节点上都可以应用一个或多个 taint ，这表示对于那些不能容忍这些 taint 的 pod，是不会被该节点接受的。如果将 toleration 应用于 pod 上，则表示这些 pod 可以（但不要求）被调度到具有相应 taint 的节点上。\n示例 以下分别以为 node 设置 taint 和为 pod 设置 toleration 为例。\n为 node 设置 taint 为 node1 设置 taint：\nkubectl taint nodes node1 key1=value1:NoSchedule kubectl taint nodes node1 key1=value1:NoExecute kubectl taint nodes node1 key2=value2:NoSchedule 删除上面的 taint：\nkubectl taint nodes node1 key1:NoSchedule- kubectl taint nodes node1 key1:NoExecute- kubectl taint nodes node1 key2:NoSchedule- 查看 node1 上的 taint：\nkubectl describe nodes node1 为 pod 设置 toleration 只要在 pod 的 spec 中设置 tolerations 字段即可，可以有多个 key，如下所示：\ntolerations:- key:\u0026#34;key1\u0026#34;operator:\u0026#34;Equal\u0026#34;value:\u0026#34;value1\u0026#34;effect:\u0026#34;NoSchedule\u0026#34;- key:\u0026#34;key1\u0026#34;operator:\u0026#34;Equal\u0026#34;value:\u0026#34;value1\u0026#34;effect:\u0026#34;NoExecute\u0026#34;- key:\u0026#34;node.alpha.kubernetes.io/unreachable\u0026#34;operator:\u0026#34;Exists\u0026#34;effect:\u0026#34;NoExecute\u0026#34;tolerationSeconds:6000 value 的值可以为 NoSchedule、 PreferNoSchedule 或 NoExecute。 tolerationSeconds 是当 pod 需要被驱逐时，可以继续在 node 上运行的时间。  详细使用方法请参考官方文档。\n参考  Taints and Tolerations - kuberentes.io  ","date":1653062400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"4ccb0be71bebb6d24198d9f27f24438a","permalink":"https://lib.jimmysong.io/kubernetes-handbook/cluster/taint-and-toleration/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/cluster/taint-and-toleration/","section":"kubernetes-handbook","summary":"Taint（污点）和 Toleration（容忍）可以作用于 node 和 pod 上，其目的是优化 pod 在集群间的调度，这跟节点亲和性类似，只不过它们作用的方式相反，具有 taint 的 node 和 pod 是互斥关系，而具有节点亲和性关系的 node 和 pod 是","tags":["Kubernetes"],"title":"Taint 和 Toleration（污点和容忍）","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"Kubernetes 垃圾收集器的角色是删除指定的对象，这些对象曾经有但以后不再拥有 Owner 了。\nOwner 和 Dependent 一些 Kubernetes 对象是其它一些的 Owner。例如，一个 ReplicaSet 是一组 Pod 的 Owner。具有 Owner 的对象被称为是 Owner 的 Dependent。每个 Dependent 对象具有一个指向其所属对象的 metadata.ownerReferences 字段。\n有时，Kubernetes 会自动设置 ownerReference 的值。例如，当创建一个 ReplicaSet 时，Kubernetes 自动设置 ReplicaSet 中每个 Pod 的 ownerReference 字段值。在 1.6 版本，Kubernetes 会自动为一些对象设置 ownerReference 的值，这些对象是由 ReplicationController、ReplicaSet、StatefulSet、DaemonSet 和 Deployment 所创建或管理。\n也可以通过手动设置 ownerReference 的值，来指定 Owner 和 Dependent 之间的关系。\n这有一个配置文件my-repset.yaml，表示一个具有 3 个 Pod 的 ReplicaSet：\n# k8s \u0026gt;= 1.16 使用下面注释 https://stackoverflow.com/questions/64412740/no-matches-for-kind-replicaset-in-version-extensions-v1beta1/64412990#64412990# apiVersion: apps/v1# k8s \u0026lt; 1.16 使用下面配置apiVersion:extensions/v1beta1kind:ReplicaSetmetadata:name:my-repsetspec:replicas:3selector:matchLabels:pod-is-for:garbage-collection-exampletemplate:metadata:labels:pod-is-for:garbage-collection-examplespec:containers:- name:nginximage:nginx如果创建该 ReplicaSet，然后查看 Pod 的 metadata 字段，能够看到 OwnerReferences 字段：\nkubectl create -f my-repset.yaml kubectl get pods --output=yaml 输出显示了 Pod 的 Owner 是名为 my-repset 的 ReplicaSet：\napiVersion:v1kind:Podmetadata:...ownerReferences:- apiVersion:extensions/v1beta1controller:trueblockOwnerDeletion:truekind:ReplicaSetname:my-repsetuid:d9607e19-f88f-11e6-a518-42010a800195...控制垃圾收集器删除 Dependent 当删除对象时，可以指定是否该对象的 Dependent 也自动删除掉。自动删除 Dependent 也称为 级联删除。Kubernetes 中有两种 级联删除 的模式：background 模式和 foreground 模式。\n如果删除对象时，不自动删除它的 Dependent，这些 Dependent 被称作是原对象的 孤儿。\nBackground 级联删除 在 background 级联删除 模式下，Kubernetes 会立即删除 Owner 对象，然后垃圾收集器会在后台删除这些 Dependent。\nForeground 级联删除 在 foreground 级联删除 模式下，根对象首先进入 “删除中” 状态。在 “删除中” 状态会有如下的情况：\n 对象仍然可以通过 REST API 可见 会设置对象的 deletionTimestamp 字段 对象的 metadata.finalizers 字段包含了值 “foregroundDeletion”  一旦被设置为 “删除中” 状态，垃圾收集器会删除对象的所有 Dependent。垃圾收集器删除了所有 “Blocking” 的 Dependent（对象的 ownerReference.blockOwnerDeletion=true）之后，它会删除 Owner 对象。\n注意，在 “foreground 删除” 模式下，Dependent 只有通过 ownerReference.blockOwnerDeletion 才能阻止删除 Owner 对象。在 Kubernetes 1.7 版本中将增加 admission controller，基于 Owner 对象上的删除权限来控制用户去设置 blockOwnerDeletion 的值为 true，所以未授权的 Dependent 不能够延迟 Owner 对象的删除。\n如果一个对象的ownerReferences 字段被一个 Controller（例如 Deployment 或 ReplicaSet）设置，blockOwnerDeletion 会被自动设置，没必要手动修改这个字段。\n设置级联删除策略 通过为 Owner 对象设置 deleteOptions.propagationPolicy 字段，可以控制级联删除策略。可能的取值包括：“orphan”、“Foreground” 或 “Background”。\n对很多 Controller 资源，包括 ReplicationController、ReplicaSet、StatefulSet、DaemonSet 和 Deployment，默认的垃圾收集策略是 orphan。因此，除非指定其它的垃圾收集策略，否则所有 Dependent 对象使用的都是 orphan 策略。\n注意：本段所指的默认值是指 REST API 的默认值，并非 kubectl 命令的默认值，kubectl 默认为级联删除，后面会讲到。\n下面是一个在后台删除 Dependent 对象的例子：\nkubectl proxy --port=8080 curl -X DELETE localhost:8080/apis/extensions/v1beta1/namespaces/default/replicasets/my-repset \\ -d \u0026#39;{\u0026#34;kind\u0026#34;:\u0026#34;DeleteOptions\u0026#34;,\u0026#34;apiVersion\u0026#34;:\u0026#34;v1\u0026#34;,\u0026#34;propagationPolicy\u0026#34;:\u0026#34;Background\u0026#34;}\u0026#39; \\ -H \u0026#34;Content-Type: application/json\u0026#34; 下面是一个在前台删除 Dependent 对象的例子：\nkubectl proxy --port=8080 curl -X DELETE localhost:8080/apis/extensions/v1beta1/namespaces/default/replicasets/my-repset \\ -d \u0026#39;{\u0026#34;kind\u0026#34;:\u0026#34;DeleteOptions\u0026#34;,\u0026#34;apiVersion\u0026#34;:\u0026#34;v1\u0026#34;,\u0026#34;propagationPolicy\u0026#34;:\u0026#34;Foreground\u0026#34;}\u0026#39; \\ -H \u0026#34;Content-Type: application/json\u0026#34; 下面是一个孤儿 Dependent 的例子：\nkubectl proxy --port=8080 curl -X DELETE localhost:8080/apis/extensions/v1beta1/namespaces/default/replicasets/my-repset \\ -d \u0026#39;{\u0026#34;kind\u0026#34;:\u0026#34;DeleteOptions\u0026#34;,\u0026#34;apiVersion\u0026#34;:\u0026#34;v1\u0026#34;,\u0026#34;propagationPolicy\u0026#34;:\u0026#34;Orphan\u0026#34;}\u0026#39; \\ -H \u0026#34;Content-Type: application/json\u0026#34; kubectl 也支持级联删除。 通过设置 --cascade 为 true，可以使用 kubectl 自动删除 Dependent 对象。设置 --cascade 为 false，会使 Dependent 对象成为孤儿 Dependent 对象。--cascade 的默认值是 true。\n下面是一个例子，使一个 ReplicaSet 的 Dependent 对象成为孤儿 Dependent：\nkubectl delete replicaset my-repset --cascade=false ","date":1653062400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"13db59833c79ba699aa2a48aeae9c597","permalink":"https://lib.jimmysong.io/kubernetes-handbook/cluster/garbage-collection/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/cluster/garbage-collection/","section":"kubernetes-handbook","summary":"Kubernetes 垃圾收集器的角色是删除指定的对象，这些对象曾经有但以后不再拥有 Owner 了。 Owner 和 Dependent 一些 Kubernetes 对象是其它一些的 Owner。例如，一个 ReplicaSet 是一组 Pod 的 Owner。具有 Owner 的对象被称为是 Owner 的 Dependent。每个 Dependent 对象具","tags":["Kubernetes"],"title":"垃圾收集","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"Deployment 为 Pod 和 ReplicaSet 提供了一个声明式定义（declarative）方法，用来替代以前的 ReplicationController 来方便的管理应用。典型的应用场景包括：\n 定义 Deployment 来创建 Pod 和 ReplicaSet 滚动升级和回滚应用 扩容和缩容 暂停和继续 Deployment  比如一个简单的 nginx 应用可以定义为：\napiVersion:extensions/v1beta1kind:Deploymentmetadata:name:nginx-deploymentspec:replicas:3template:metadata:labels:app:nginxspec:containers:- name:nginximage:nginx:1.7.9ports:- containerPort:80扩容：\nkubectl scale deployment nginx-deployment --replicas 10 如果集群支持 horizontal pod autoscaling 的话，还可以为 Deployment 设置自动扩展：\nkubectl autoscale deployment nginx-deployment --min=10 --max=15 --cpu-percent=80 更新镜像也比较简单：\nkubectl set image deployment/nginx-deployment nginx=nginx:1.9.1 回滚：\nkubectl rollout undo deployment/nginx-deployment Deployment 结构示意图    Kubernetes Deployment Cheatsheet  Deployment 是什么？ Deployment 为 Pod 和 Replica Set（下一代 Replication Controller）提供声明式更新。\n您只需要在 Deployment 中描述您想要的目标状态是什么，Deployment controller 就会帮您将 Pod 和 ReplicaSet 的实际状态改变到您的目标状态。您可以定义一个全新的 Deployment 来创建 ReplicaSet 或者删除已有的 Deployment 并创建一个新的来替换。\n注意：您不该手动管理由 Deployment 创建的 ReplicaSet，否则您就篡越了 Deployment controller 的职责！下文罗列了 Deployment 对象中已经覆盖了所有的用例。如果未有覆盖您所有需要的用例，请直接在 Kubernetes 的代码库中提 issue。\n典型的用例如下：\n 使用 Deployment 来创建 ReplicaSet。ReplicaSet 在后台创建 pod。检查启动状态，看它是成功还是失败。 然后，通过更新 Deployment 的 PodTemplateSpec 字段来声明 Pod 的新状态。这会创建一个新的 ReplicaSet，Deployment 会按照控制的速率将 pod 从旧的 ReplicaSet 移动到新的 ReplicaSet 中。 如果当前状态不稳定，回滚到之前的 Deployment revision。每次回滚都会更新 Deployment 的 revision。 扩容 Deployment 以满足更高的负载。 暂停 Deployment 来应用 PodTemplateSpec 的多个修复，然后恢复上线。 根据 Deployment 的状态判断上线是否 hang 住了。 清除旧的不必要的 ReplicaSet。  创建 Deployment 下面是一个 Deployment 示例，它创建了一个 ReplicaSet 来启动 3 个 nginx pod。\n下载示例文件并执行命令：\n$ kubectl create -f https://kubernetes.io/docs/user-guide/nginx-deployment.yaml --record deployment \u0026#34;nginx-deployment\u0026#34; created 将 kubectl 的--record的 flag 设置为true 可以在 annotation 中记录当前命令创建或者升级了该资源。这在未来会很有用，例如，查看在每个 Deployment revision 中执行了哪些命令。\n然后立即执行 get 将获得如下结果：\n$ kubectl get deployments NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE nginx-deployment 3 0 0 0 1s 输出结果表明我们希望的 repalica 数是 3（根据 deployment 中的.spec.replicas配置）当前 replica 数（.status.replicas）是 0, 最新的 replica 数（.status.updatedReplicas）是 0，可用的 replica 数（.status.availableReplicas）是 0。\n过几秒后再执行 get 命令，将获得如下输出：\n$ kubectl get deployments NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE nginx-deployment 3 3 3 3 18s 我们可以看到 Deployment 已经创建了 3 个 replica，所有的 replica 都已经是最新的了（包含最新的 pod template），可用的（根据 Deployment 中的.spec.minReadySeconds声明，处于已就绪状态的 pod 的最少个数）。执行kubectl get rs和kubectl get pods会显示 Replica Set（RS）和 Pod 已创建。\n$ kubectl get rs NAME DESIRED CURRENT READY AGE nginx-deployment-2035384211 3 3 0 18s 您可能会注意到 ReplicaSet 的名字总是\u0026lt;Deployment 的名字\u0026gt;-\u0026lt;pod template 的 hash 值\u0026gt;。\n$ kubectl get pods --show-labels NAME READY STATUS RESTARTS AGE LABELS nginx-deployment-2035384211-7ci7o 1/1 Running 0 18s app=nginx,pod-template-hash=2035384211 nginx-deployment-2035384211-kzszj 1/1 Running 0 18s app=nginx,pod-template-hash=2035384211 nginx-deployment-2035384211-qqcnn 1/1 Running 0 18s app=nginx,pod-template-hash=2035384211 刚创建的 Replica Set 将保证总是有 3 个 nginx 的 pod 存在。\n注意： 您必须在 Deployment 中的 selector 指定正确的 pod template label（在该示例中是 app = nginx），不要跟其他的 controller 的 selector 中指定的 pod template label 搞混了（包括 Deployment、Replica Set、Replication Controller 等）。Kubernetes 本身并不会阻止您任意指定 pod template label，但是如果您真的这么做了，这些 controller 之间会相互打架，并可能导致不正确的行为。\nPod-template-hash label 注意：这个 label 不是用户指定的！\n注意上面示例输出中的 pod label 里的 pod-template-hash label。当 Deployment 创建或者接管 ReplicaSet 时，Deployment controller 会自动为 Pod 添加 pod-template-hash label。这样做的目的是防止 Deployment 的子 ReplicaSet 的 pod 名字重复。通过将 ReplicaSet 的 PodTemplate 进行哈希散列，使用生成的哈希值作为 label 的值，并添加到 ReplicaSet selector 里、 pod template label 和 ReplicaSet 管理中的 Pod 上。\n更新 Deployment 注意：Deployment 的 rollout 当且仅当 Deployment 的 pod template（例如 .spec.template）中的 label 更新或者镜像更改时被触发。其他更新，例如扩容 Deployment 不会触发 rollout。\n假如我们现在想要让 nginx pod 使用 nginx:1.9.1 的镜像来代替原来的 nginx:1.7.9 的镜像。\n$ kubectl set image deployment/nginx-deployment nginx=nginx:1.9.1 deployment \u0026#34;nginx-deployment\u0026#34; image updated 我们可以使用edit命令来编辑 Deployment，修改.spec.template.spec.containers [0].image，将nginx:1.7.9改写成nginx:1.9.1。\n$ kubectl edit deployment/nginx-deployment deployment \u0026#34;nginx-deployment\u0026#34; edited 查看 rollout 的状态，只要执行：\n$ kubectl rollout status deployment/nginx-deployment Waiting for rollout to finish: 2 out of 3 new replicas have been updated... deployment \u0026#34;nginx-deployment\u0026#34; successfully rolled out Rollout 成功后，getDeployment：\n$ kubectl get deployments NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE nginx-deployment 3 3 3 3 36s UP-TO-DATE 的 replica 的数目已经达到了配置中要求的数目。\nCURRENT 的 replica 数表示 Deployment 管理的 replica 数量，AVAILABLE 的 replica 数是当前可用的 replica 数量。\n我们通过执行 kubectl get rs 可以看到 Deployment 更新了 Pod，通过创建一个新的 ReplicaSet 并扩容了 3 个 replica，同时将原来的 ReplicaSet 缩容到了 0 个 replica。\n$ kubectl get rs NAME DESIRED CURRENT READY AGE nginx-deployment-1564180365 3 3 0 6s nginx-deployment-2035384211 0 0 0 36s 执行get pods只会看到当前的新的 pod：\n$ kubectl get pods NAME READY STATUS RESTARTS AGE …","date":1653062400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"e438386d5751692f8c1ca2425df7bb22","permalink":"https://lib.jimmysong.io/kubernetes-handbook/controllers/deployment/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/controllers/deployment/","section":"kubernetes-handbook","summary":"Deployment 为 Pod 和 ReplicaSet 提供了一个声明式定义（declarative）方法，用来替代以前的 ReplicationController 来方便的管理应用。典型的应用场景包括： 定义 Deployment 来创建 Pod 和 ReplicaSet 滚动升级和回滚应用 扩容和缩容 暂停和继续 Deployment 比如一个简单的 nginx 应用可以定","tags":["Kubernetes"],"title":"Deployment","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"StatefulSet 作为 Controller 为 Pod 提供唯一的标识。它可以保证部署和 scale 的顺序。\n使用案例参考：kubernetes contrib - statefulsets，其中包含zookeeper和kakfa的statefulset设置和使用说明。\nStatefulSet是为了解决有状态服务的问题（对应Deployments和ReplicaSets是为无状态服务而设计），其应用场景包括：\n 稳定的持久化存储，即Pod重新调度后还是能访问到相同的持久化数据，基于PVC来实现 稳定的网络标志，即Pod重新调度后其PodName和HostName不变，基于Headless Service（即没有Cluster IP的Service）来实现 有序部署，有序扩展，即Pod是有顺序的，在部署或者扩展的时候要依据定义的顺序依次依次进行（即从0到N-1，在下一个Pod运行之前所有之前的Pod必须都是Running和Ready状态），基于init containers来实现 有序收缩，有序删除（即从N-1到0）  从上面的应用场景可以发现，StatefulSet由以下几个部分组成：\n 用于定义网络标志（DNS domain）的Headless Service 用于创建PersistentVolumes的volumeClaimTemplates 定义具体应用的StatefulSet  StatefulSet中每个Pod的DNS格式为statefulSetName-{0..N-1}.serviceName.namespace.svc.cluster.local，其中\n serviceName为Headless Service的名字 0..N-1为Pod所在的序号，从0开始到N-1 statefulSetName为StatefulSet的名字 namespace为服务所在的namespace，Headless Servic和StatefulSet必须在相同的namespace .cluster.local为Cluster Domain  使用 StatefulSet StatefulSet 适用于有以下某个或多个需求的应用：\n 稳定，唯一的网络标志。 稳定，持久化存储。 有序，优雅地部署和 scale。 有序，优雅地删除和终止。 有序，自动的滚动升级。  在上文中，稳定是 Pod （重新）调度中持久性的代名词。 如果应用程序不需要任何稳定的标识符、有序部署、删除和 scale，则应该使用提供一组无状态副本的 controller 来部署应用程序，例如 Deployment 或 ReplicaSet 可能更适合您的无状态需求。\n限制  StatefulSet 是 beta 资源，Kubernetes 1.5 以前版本不支持。 对于所有的 alpha/beta 的资源，您都可以通过在 apiserver 中设置 --runtime-config 选项来禁用。 给定 Pod 的存储必须由 PersistentVolume Provisioner 根据请求的 storage class 进行配置，或由管理员预先配置。 删除或 scale StatefulSet 将_不会_删除与 StatefulSet 相关联的 volume。 这样做是为了确保数据安全性，这通常比自动清除所有相关 StatefulSet 资源更有价值。 StatefulSets 目前要求 Headless Service 负责 Pod 的网络身份。 您有责任创建此服务。  组件 下面的示例中描述了 StatefulSet 中的组件。\n 一个名为 nginx 的 headless service，用于控制网络域。 一个名为 web 的 StatefulSet，它的 Spec 中指定在有 3 个运行 nginx 容器的 Pod。 volumeClaimTemplates 使用 PersistentVolume Provisioner 提供的 PersistentVolumes 作为稳定存储。  apiVersion:v1kind:Servicemetadata:name:nginxlabels:app:nginxspec:ports:- port:80name:webclusterIP:Noneselector:app:nginx---apiVersion:apps/v1beta1kind:StatefulSetmetadata:name:webspec:serviceName:\u0026#34;nginx\u0026#34;replicas:3template:metadata:labels:app:nginxspec:terminationGracePeriodSeconds:10containers:- name:nginximage:gcr.io/google_containers/nginx-slim:0.8ports:- containerPort:80name:webvolumeMounts:- name:wwwmountPath:/usr/share/nginx/htmlvolumeClaimTemplates:- metadata:name:wwwannotations:volume.beta.kubernetes.io/storage-class:anythingspec:accessModes:[\u0026#34;ReadWriteOnce\u0026#34;]resources:requests:storage:1GiPod 身份 StatefulSet Pod 具有唯一的身份，包括序数，稳定的网络身份和稳定的存储。 身份绑定到 Pod 上，不管它（重新）调度到哪个节点上。\n序数 对于一个有 N 个副本的 StatefulSet，每个副本都会被指定一个整数序数，在 [0,N)之间，且唯一。\n稳定的网络 ID StatefulSet 中的每个 Pod 从 StatefulSet 的名称和 Pod 的序数派生其主机名。构造的主机名的模式是$（statefulset名称)-$(序数)。 上面的例子将创建三个名为web-0，web-1，web-2的 Pod。\nStatefulSet 可以使用 Headless Service 来控制其 Pod 的域。此服务管理的域的格式为：$(服务名称).$(namespace).svc.cluster.local，其中 “cluster.local” 是集群域。\n在创建每个Pod时，它将获取一个匹配的 DNS 子域，采用以下形式：$(pod 名称).$(管理服务域)，其中管理服务由 StatefulSet 上的 serviceName 字段定义。\n以下是 Cluster Domain，服务名称，StatefulSet 名称以及如何影响 StatefulSet 的 Pod 的 DNS 名称的一些示例。\n   Cluster Domain Service (ns/name) StatefulSet (ns/name) StatefulSet Domain Pod DNS Pod Hostname     cluster.local default/nginx default/web nginx.default.svc.cluster.local web-{0..N-1}.nginx.default.svc.cluster.local web-{0..N-1}   cluster.local foo/nginx foo/web nginx.foo.svc.cluster.local web-{0..N-1}.nginx.foo.svc.cluster.local web-{0..N-1}   kube.local foo/nginx foo/web nginx.foo.svc.kube.local web-{0..N-1}.nginx.foo.svc.kube.local web-{0..N-1}    注意 Cluster Domain 将被设置成 cluster.local 除非进行了其他配置。\n稳定存储 Kubernetes 为每个 VolumeClaimTemplate 创建一个 PersistentVolume。上面的 nginx 的例子中，每个 Pod 将具有一个由 anything 存储类创建的 1 GB 存储的 PersistentVolume。当该 Pod （重新）调度到节点上，volumeMounts 将挂载与 PersistentVolume Claim 相关联的 PersistentVolume。请注意，与 PersistentVolume Claim 相关联的 PersistentVolume 在 产出 Pod 或 StatefulSet 的时候不会被删除。这必须手动完成。\n部署和 Scale 保证  对于有 N 个副本的 StatefulSet，Pod 将按照 {0..N-1} 的顺序被创建和部署。 当 删除 Pod 的时候，将按照逆序来终结，从{N-1..0} 对 Pod 执行 scale 操作之前，它所有的前任必须处于 Running 和 Ready 状态。 在终止 Pod 前，它所有的继任者必须处于完全关闭状态。  不应该将 StatefulSet 的 pod.Spec.TerminationGracePeriodSeconds 设置为 0。这样是不安全的且强烈不建议您这样做。进一步解释，请参阅 强制删除 StatefulSet Pod。\n上面的 nginx 示例创建后，3 个 Pod 将按照如下顺序创建 web-0，web-1，web-2。在 web-0 处于 运行并就绪 状态之前，web-1 将不会被部署，同样当 web-1 处于运行并就绪状态之前 web-2也不会被部署。如果在 web-1 运行并就绪后，web-2 启动之前， web-0 失败了，web-2 将不会启动，直到 web-0 成功重启并处于运行并就绪状态。\n如果用户通过修补 StatefulSet 来 scale 部署的示例，以使 replicas=1，则 web-2 将首先被终止。 在 web-2 完全关闭和删除之前，web-1 不会被终止。 如果 web-0 在 web-2 终止并且完全关闭之后，但是在 web-1 终止之前失败，则 web-1 将不会终止，除非 web-0 正在运行并准备就绪。\nPod 管理策略 在 Kubernetes 1.7 和之后版本，StatefulSet 允许您放开顺序保证，同时通过 .spec.podManagementPolicy 字段保证身份的唯一性。\nOrderedReady Pod 管理 StatefulSet 中默认使用的是 OrderedReady pod 管理。它实现了 如上 所述的行为。\n并行 Pod 管理 Parallel pod 管理告诉 StatefulSet controller 并行的启动和终止 Pod，在启动和终止其他 Pod 之前不会等待 Pod 变成 运行并就绪或完全终止状态。\n更新策略 在 kubernetes 1.7 和以上版本中，StatefulSet 的 .spec.updateStrategy 字段允许您配置和禁用 StatefulSet 中的容器、label、resource request/limit、annotation 的滚动更新。\n删除 OnDelete 更新策略实现了遗留（1.6和以前）的行为。 当 spec.updateStrategy 未指定时，这是默认策略。 当StatefulSet 的 .spec.updateStrategy.type 设置为 OnDelete 时，StatefulSet 控制器将不会自动更新 StatefulSet 中的 Pod。 用户必须手动删除 Pod 以使控制器创建新的 Pod，以反映对StatefulSet的 .spec.template 进行 …","date":1653062400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"9219e77fc2c1b2720a5bf7ad2f489c69","permalink":"https://lib.jimmysong.io/kubernetes-handbook/controllers/statefulset/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/controllers/statefulset/","section":"kubernetes-handbook","summary":"StatefulSet 作为 Controller 为 Pod 提供唯一的标识。它可以保证部署和 scale 的顺序。 使用案例参考：kubernetes contrib - statefulsets，其中包含zookeeper和kakfa的statefulset设置和使用说明。 St","tags":["Kubernetes"],"title":"StatefulSet","type":"book"},{"authors":null,"categories":["Istio"],"content":"DestinationRule 定义了在路由发生后适用于服务流量的策略。这些规则指定了负载均衡的配置、来自 sidecar 的连接池大小，以及用于检测和驱逐负载均衡池中不健康主机的异常检测设置。\n示例 例如，ratings 服务的一个简单的负载均衡策略看起来如下。\napiVersion:networking.istio.io/v1alpha3kind:DestinationRulemetadata:name:bookinfo-ratingsspec:host:ratings.prod.svc.cluster.localtrafficPolicy:loadBalancer:simple:LEAST_CONN可以通过定义一个 subset 并覆盖在服务级来配置特定版本的策略。下面的规则对前往由带有标签（version:v3）的端点（如 pod）组成的名为 testversion 的子集的所有流量使用轮询负载均衡策略。\napiVersion:networking.istio.io/v1alpha3kind:DestinationRulemetadata:name:bookinfo-ratingsspec:host:ratings.prod.svc.cluster.localtrafficPolicy:loadBalancer:simple:LEAST_CONNsubsets:- name:testversionlabels:version:v3trafficPolicy:loadBalancer:simple:ROUND_ROBIN注意：只有当路由规则明确地将流量发送到这个子集，为子集指定的策略才会生效。\n流量策略也可以针对特定的端口进行定制。下面的规则对所有到 80 号端口的流量使用最少连接的负载均衡策略，而对 9080 号端口的流量使用轮流负载均衡设置。\napiVersion:networking.istio.io/v1alpha3kind:DestinationRulemetadata:name:bookinfo-ratings-portspec:host:ratings.prod.svc.cluster.localtrafficPolicy:# 应用到所有端口portLevelSettings:- port:number:80loadBalancer:simple:LEAST_CONN- port:number:9080loadBalancer:simple:ROUND_ROBIN配置项 下图是 DestinationRule 资源的配置拓扑图。\n  DestinationRule 资源配置拓扑图  DestinationRule 资源的顶级配置项如下：\n  host：字符串类型。来自服务注册表的服务名称。服务名称从平台的服务注册表（例如，Kubernetes 服务、Consul 服务等）和 ServiceEntry 声明的主机中查找。为服务注册表中不存在的服务定义的规则将被忽略。\nKubernetes 用户的注意事项。当使用短名称时（例如 reviews 而不是 reviews.default.svc.cluster.local），Istio 将根据规则的命名空间而不是服务来解释短名称。在 default 命名空间中包含主机 reviews 的规则将被解释为 reviews.default.svc.cluster.local，而不考虑与 reviews 服务相关的实际命名空间。为了避免潜在的错误配置，建议总是使用完全限定名而不是短名称。\n注意，主机字段适用于 HTTP 和 TCP 服务。\n  trafficPolicy：要应用的流量策略（负载均衡策略、连接池大小、异常值检测）。\n  subsets：一个或多个命名的集合，代表一个服务的单独版本。流量策略可以在子集级别被覆盖。\n  exportTo：DestinationRule 被导出的命名空间的列表。DestinationRule 的解析是在命名空间级别中进行的。导出的 DestinationRule 被包含在其他命名空间的服务的解析层次中。该功能为服务所有者和网格管理员提供了一种机制，以控制 DestinationRule 在命名空间边界的可视性。\n如果没有指定命名空间，那么默认情况下，DestinationRule 会被输出到所有命名空间。\n值 . 是保留的，它定义了导出到 DestinationRule 声明的同一命名空间。同样，值 * 也是保留的，它定义了导出到所有命名空间。\n  下面是一些重要的配置项的使用说明。\n示例配置 下面是一个相对完整的 DestinationRule 配置的示例，其中包含了所有基本配置，对应的解释请见注释。\napiVersion:networking.istio.io/v1alpha3kind:DestinationRulemetadata:name:bookinfo-ratingsnamespace:bookinfospec:host:ratings.prod.svc.cluster.local#来自服务注册表的服务名称trafficPolicy:#要应用的流量策略，默认是应用与所有端口loadBalancer:#配置负载均衡算法simple:ROUND_ROBINportLevelSettings:#对指定端口应用流量策略，可覆盖全局配置- port:number:80loadBalancer:simple:LEAST_CONNconnectionPool:#配置上游服务的连接量，可以配置 TCP 和 HTTPtcp:#同时适用于 TCP 和 HTTP 上游连接maxConnections:100#与上游服务的最大连接数connectTimeout:30ms#TCP 连接的超时时间，默认为 10stcpKeepalive:#如果有该配置，则在套接字上设置 SO_KEEPALIVE，以启用 TCP Keepalive。time:7200s #最后一次探测和第一次探测之间间隔的时间。默认是使用操作系统级别的配置（除非被覆盖，Linux默认为7200s，即2小时。）interval:75s#探测发送间隔，默认是 75sprobs:9# 最大探测次数，使用操作系统级别的默认值，Linux 系统的默认值是 9，如果超过该次数没有得到回复，则意味着连接断开了http:#针对 HTTP 连接池的配置http2MaxRequests:1000#最大请求数maxRequestsPerConnection:10#每个连接中的最大请求数，默认是 0，意味着没有限制maxRetries:5#在给定时间内，集群中的所有主机未完成的最大重试次数outlierDetection:#异常值检测，实际为一个断路器实现，跟踪上游服务中每个独立主机的状态。同时适用于 HTTP 和 TCP 服务。对于 HTTP 服务，持续返回 5xx 错误的 API 调用的主机将在预先定义的时间内从连接池中弹出。对于 TCP 服务，在测量连续错误指标时，对特定主机的连接超时或连接失败算作一个错误。consecutive5xxErrors:7#当达到 7 次 5xx 错误时，该主机将从上游连接中弹出。当通过不透明的 TCP 连接被访问时上游主机时，连接超时、错误/失败和请求失败事件都有资格成为5xx错误。该功能默认为5，但可以通过设置该值为0来禁用。interval:5m#异常值检测的时间间隔，默认是为 10sbaseEjectionTime:15m#主机将保持弹出的时间，等于最小弹出持续时间和主机被弹出次数的乘积。这种技术允许系统自动增加不健康的上游服务器的弹出时间。默认为 30s。tls:#关于 TLS 的设置mode:MUTUAL#一共有四种模式，DISABLE：关闭 TLS 连接；SIMPLE：发起一个与上游端点的 TLS 连接；MUTUAL：手动配置证书，通过出示客户端证书进行认证，使用双向的 TLS 确保与上游的连接；ISTIO_MUTUAL：该模式使用 Istio 自动生成的证书进行 mTLS 认证。clientCertificate:/etc/certs/myclientcert.pemprivateKey:/etc/certs/client_private_key.pemcaCertificates:/etc/certs/rootcacerts.pemsubsets:#服务版本- name:v1labels:version:v1#所有具有该标签的 Pod 被划分为该子集exportTo:#指定DestinationRule 的可视性- \u0026#34;*\u0026#34;#*表示对所有命名空间可见，此为默认值；\u0026#34;.\u0026#34;表示仅对当前命名空间可见关于 DestinationRule 配置的详细用法请参考 Istio 官方文档。\n参考  Destination Rule - istio.io  ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"cee24bb6bd867c31ae74f9a3716a757f","permalink":"https://lib.jimmysong.io/istio-handbook/config-networking/destination-rule/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/istio-handbook/config-networking/destination-rule/","section":"istio-handbook","summary":"DestinationRule 定义了在路由发生后适用于服务流量的策略。这些规则指定了负载均衡的配置、来自 sidecar 的连接池大小，以及用于检测和驱逐负载均衡池中不健康主机的异常检测设置。 示例 例如，ratings 服务的一个简单的负载均衡策略","tags":["Istio","Service Mesh"],"title":"DestinationRule","type":"book"},{"authors":null,"categories":["Istio"],"content":"Grafana 是一个用于分析和监控的开放平台。Grafana 可以连接到各种数据源，并使用图形、表格、热图等将数据可视化。通过强大的查询语言，你可以定制现有的仪表盘并创建更高级的可视化。\n通过 Grafana，我们可以监控 Istio 安装和服务网格中运行的应用程序的健康状况。\n我们可以使用 grafana.yaml 来部署带有预配置仪表盘的 Grafana 示例安装。该 YAML 文件在 Istio 安装包的 /samples/addons 下。\n确保在部署 Grafana 之前部署 Promeheus 插件，因为 Grafana 使用 Prometheus 作为其数据源。\n运行下面的命令来部署 Grafana 和预配置的仪表盘：\n$ kubectl apply -f istio-1.9.0/samples/addons/grafana.yaml serviceaccount/grafana created configmap/grafana created service/grafana created deployment.apps/grafana created configmap/istio-grafana-dashboards created configmap/istio-services-grafana-dashboards created  我们不打算在生产中运行这个 Grafana，因为它没有经过性能或安全方面的优化。\n Kubernetes 将 Grafana 部署在 istio-system 命名空间。要访问 Grafana，我们可以使用 istioctl dashboard 命令。\n$ istioctl dashboard grafana http://localhost:3000 我们可以在浏览器中打开 http://localhost:3000，进入 Grafana。然后，点击首页和 Istio 文件夹，查看已安装的仪表板，如下图所示。\n   Grafana 仪表板  Istio Grafana 安装时预配置了以下仪表板：\n1. Istio 控制平面仪表板\n从 Istio 控制平面仪表板，我们可以监控 Istio 控制平面的健康和性能。\n   Istio 控制平面仪表板  这个仪表板将向我们显示控制平面的资源使用情况（内存、CPU、磁盘、Go routines），以及关于 Pilot 、Envoy 和 Webhook 的信息。\n2. Istio 网格仪表板\n网格仪表盘为我们提供了在网格中运行的所有服务的概览。仪表板包括全局请求量、成功率以及 4xx 和 5xx 响应的数量。\n   Istio 网格仪表板  3. Istio 性能仪表板\n性能仪表盘向我们展示了 Istio 主要组件在稳定负载下的资源利用率。\n   Istio 性能仪表板  4. Istio 服务仪表板\n服务仪表板允许我们在网格中查看关于我们服务的细节。\n我们可以获得关于请求量、成功率、持续时间的信息，以及显示按来源和响应代码、持续时间和大小的传入请求的详细图表。\n   Istio 服务仪表板  5. Istio Wasm 扩展仪表板\nIstio Wasm 扩展仪表板显示与 WebAssembly 模块有关的指标。从这个仪表板，我们可以监控活动的和创建的 Wasm 虚拟机，关于获取删除 Wasm 模块和代理资源使用的数据。\n   Istio Wasm 扩展仪表板  6. Istio 工作负载仪表板\n这个仪表板为我们提供了一个工作负载的详细指标分类。\n   Istio 工作负载仪表板  ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"68511e58832f3b917de513ac54bc1b5e","permalink":"https://lib.jimmysong.io/istio-handbook/observability/grafana/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/istio-handbook/observability/grafana/","section":"istio-handbook","summary":"Grafana 是一个用于分析和监控的开放平台。Grafana 可以连接到各种数据源，并使用图形、表格、热图等将数据可视化。通过强大的查询语言，你可以定制现有的仪表盘并创建更高级的可视化。 通过 Grafana，我们可以","tags":["Istio","Service Mesh"],"title":"Grafana","type":"book"},{"authors":null,"categories":["Envoy"],"content":"HTTP 检查器监听器过滤器（envoy.filters.listener.http_inspector）允许我们检测应用协议是否是 HTTP。如果协议不是 HTTP，监听器过滤器将通过该数据包。\n如果应用协议被确定为 HTTP，它也会检测相应的 HTTP 协议（如 HTTP/1.x 或 HTTP/2）。\n我们可以使用过滤器链匹配中的 application_protocols 字段来检查 HTTP 检查过滤器的结果。\n让我们考虑下面的片段。\n...listener_filters:- name:envoy.filters.listener.http_inspectortyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.listener.http_inspector.v3.HttpInspectorfilter_chains:- filter_chain_match:application_protocols:[\u0026#34;h2\u0026#34;]filters:- name:my_http2_filter... - filter_chain_match:application_protocols:[\u0026#34;http/1.1\u0026#34;]filters:- name:my_http1_filter...我们在 listener_filters 字段下添加了 http_inspector 过滤器来检查连接并确定应用协议。如果 HTTP 协议是 HTTP/2（h2c），Envoy 会匹配第一个网络过滤器链（以 my_http2_filter 开始）。\n另外，如果下游的 HTTP 协议是 HTTP/1.1（http/1.1），Envoy 会匹配第二个过滤器链，并从名为 my_http1_filter 的过滤器开始运行过滤器链。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"f39919523831eabea6144595357e15ee","permalink":"https://lib.jimmysong.io/envoy-handbook/listener/http-inspector-listener-filter/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/envoy-handbook/listener/http-inspector-listener-filter/","section":"envoy-handbook","summary":"HTTP 检查器监听器过滤器（envoy.filters.listener.http_inspector）允许我们检测应用协议是否是 HTTP。如果协议不是 HTTP，监听器过滤器将通过该数据包。 如果应用协议被确","tags":["Envoy"],"title":"HTTP 检查器监听器过滤器","type":"book"},{"authors":null,"categories":["Istio"],"content":"Istio 是一个服务网格的开源实现。从宏观上来看 Istio 支持以下功能。\n1. 流量管理\n利用配置，我们可以控制服务间的流量。设置断路器、超时或重试都可以通过简单的配置改变来完成。\n2. 可观测性\nIstio 通过跟踪、监控和记录让我们更好地了解你的服务，它让我们能够快速发现和修复问题。\n3. 安全性\nIstio 可以在代理层面上管理认证、授权和通信的加密。我们可以通过快速的配置变更在各个服务中执行政策。\nIstio 组件 Istio 服务网格有两个部分：数据平面和控制平面。\n在构建分布式系统时，将组件分离成控制平面和数据平面是一种常见的模式。数据平面的组件在请求路径上，而控制平面的组件则帮助数据平面完成其工作。\nIstio 中的数据平面由 Envoy 代理组成，控制服务之间的通信。网格的控制平面部分负责管理和配置代理。\n   Istio 架构  Envoy（数据平面） Envoy 是一个用 C++ 开发的高性能代理。Istio 服务网格将 Envoy 代理作为一个 sidecar 容器注入到你的应用容器旁边。然后该代理拦截该服务的所有入站和出站流量。注入的代理一起构成了服务网格的数据平面。\nEnvoy 代理也是唯一与流量进行交互的组件。除了前面提到的功能 —— 负载均衡、断路器、故障注入等。Envoy 还支持基于 WebAssembly（WASM）的可插拔扩展模型。这种可扩展性使我们能够执行自定义策略，并为网格中的流量生成遥测数据。\nIstiod（控制平面） Istiod 是控制平面组件，提供服务发现、配置和证书管理功能。Istiod 采用 YAML 编写的高级规则，并将其转换为 Envoy 的可操作配置。然后，它把这个配置传播给网格中的所有 sidecar。\nIstiod 内部的 Pilot 组件抽象出特定平台的服务发现机制（Kubernetes、Consul 或 VM），并将其转换为 sidecar 可以使用的标准格式。\n使用内置的身份和凭证管理，我们可以实现强大的服务间和终端用户认证。通过授权功能，我们可以控制谁可以访问你的服务。\n控制平面的部分以前被称为 Citadel，作为一个证书授权机构，生成证书，允许数据平面中的代理之间进行安全的 mTLS 通信。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"a1dd4248dd574823c7a38e95647a2ce2","permalink":"https://lib.jimmysong.io/istio-handbook/concepts/istio-intro/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/istio-handbook/concepts/istio-intro/","section":"istio-handbook","summary":"Istio 是一个服务网格的开源实现。从宏观上来看 Istio 支持以下功能。 1. 流量管理 利用配置，我们可以控制服务间的流量。设置断路器、超时或重试都可以通过简单的配置改变来完成。 2. 可观测性 Istio 通过跟踪、监控和记录让我们更好地","tags":["Istio","Service Mesh"],"title":"Istio 简介","type":"book"},{"authors":null,"categories":["Istio"],"content":"Istio 中有个 issue #9066 要求将 Istio 中默认使用的 Service Graph 替换成 Kiali。Kiali 最初是由 Red Hat 开源的，用于解决 Service Mesh 中可观测性即微服务的可视性问题。目前已获得 Istio 社区的官方支持。\n关于 Kiali 单体应用使用微服务架构拆分成了许多微服务的组合。服务的数量显著增加，就对需要了解服务之间的通信模式，例如容错（通过超时、重试、断路等）以及分布式跟踪，以便能够看到服务调用的去向。服务网格可以在平台级别上提供这些服务，并使应用程序编写者从以上繁重的通信模式中解放出来。路由决策在网格级别完成。Kiali 与Istio 合作，可视化服务网格拓扑、断路器和请求率等功能。Kiali还包括 Jaeger Tracing，可以提供开箱即用的分布式跟踪功能。\nKiali 提供的功能 Kiali 提供以下功能：\n 服务拓扑图 分布式跟踪 指标度量收集和图标 配置校验 健康检查和显示 服务发现  下图展示了 kiali 中显示的 Bookinfo 示例的服务拓扑图。\n   Kiali 页面  编译安装与试用 Kilia pod 中运行的进程是 /opt/kiali/kiali -config /kiali-configuration/config.yaml -v 4。\n/kiali-configuration/config.yaml 是使用 ConfigMap 挂载进去的，用于配置 Kiali 的 Web 根路径和外部服务地址。\nserver:port:20001web_root:/external_services:jaeger:url:\u0026#34;http://172.17.8.101:31888\u0026#34;grafana:url:\u0026#34;http://grafana.istio-system:3000\u0026#34;Kiali 中的基本概念 在了解 Kiali 如何提供 Service Mesh 中微服务可观测性之前，我们需要先了解下 Kiali 如何划分监控类别的。\n Application：使用运行的工作负载，必须使用 Istio 的将 Label 标记为 app 才算。注意，如果一个应用有多个版本，只要 app 标签的值相同就是属于同一个应用。 Deployment：即 Kubernetes 中的 Deployment。 Label：这个值对于 Istio 很重要，因为 Istio 要用它来标记 metrics。每个 Application 要求包括 app 和 version 两个 label。 Namespace：通常用于区分项目和用户。 Service：即 Kubernetes 中的 Service，不过要求必须有 app label。 Workload：Kubernetes 中的所有常用资源类型如 Deployment、StatefulSet、Job 等都可以检测到，不论这些负载是否加入到 Istio Service Mesh 中。  Application、Workload 与 Service 的关系如下图所示。\n   Kiali 页面  Kilia 的详细 API 使用说明请查看 Swagger API 文档，在 Kiali 的根目录下运行下面的命令可以查看 API 文档。\nmake swagger-serve Swagger UI 如下图。\n   Kiali API文档  架构 Kiali 部署完成后只启动了一个 Pod，前后端都集成在这一个 Pod 中。Kiali 也有一些依赖的组件，例如如果要在 Kiali 的页面中获取到监控 metric 需要使用在 istio-system 中部署 Prometheus。下图是 Kiali 的架构，来自 Kiali 官网。\n   Kiali 架构图  Kiali 使用传统的前后端分离架构：\n 后端使用 Go 编写：https://github.com/kiali/kiali，为前端提供 API，所有消息使用 JSON 编码，使用 ConfigMap 和 Secret 来存储配置。直接与 Kubernetes 和 Istio 通信来获取数据。 前端使用 Typescript 编写：https://github.com/kiali/kiali-ui，无状态，除了一些证书保存在浏览器中。于查询后端 API，可以跳转访问 Jaeger 分布式追踪和 Grafana 监控页面。  Jaeger 和 Grafana 都是可选组件，使用的都是外部服务，不是由 Kiali 部署的，需要在 kiali-configmap.yaml 中配置 URL。注意该 URL 必须是从你本地浏览器中可以直接访问到的地址。\n注意：如果服务之间没有任何请求就不会在 Prometheus 中保存数据也就无法显示服务拓扑图，所以大家在部署完 Bookinfo 服务之后向 productpage 服务发送一些请求用于生成服务拓扑图。\n服务拓扑图 Kiali 中的服务拓扑图比起 Istio 原来默认部署的 ServiceGraph 的效果更炫也更加直观，具有更多选项。\n   Kiali 中的服务拓扑图  例如使用 CURL 模拟请求。\n$ curl -H \u0026#34;Authorization: Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1c2VybmFtZSI6ImFkbWluIiwiZXhwIjoxNTM5NjczOTYyfQ.6gNz4W6yA9Bih4RkTbcSvqdaiRqsyj8c8o6ictM9iDs\u0026#34; http://172.17.8.101:32439/api/namespaces/all/graph?duration=60s\u0026amp;graphType=versionedApp\u0026amp;injectServiceNodes=false\u0026amp;appenders=dead_node,sidecars_check,istio 会得到如下的返回的 JSON 返回值，为了节省篇幅其中省略了部分结果：\n{ \u0026#34;timestamp\u0026#34;: 1539296648, \u0026#34;graphType\u0026#34;: \u0026#34;versionedApp\u0026#34;, \u0026#34;elements\u0026#34;: { \u0026#34;nodes\u0026#34;: [ { \u0026#34;data\u0026#34;: { \u0026#34;id\u0026#34;: \u0026#34;6519157be154675342fb76c41edc731c\u0026#34;, \u0026#34;nodeType\u0026#34;: \u0026#34;app\u0026#34;, \u0026#34;namespace\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;app\u0026#34;: \u0026#34;reviews\u0026#34;, \u0026#34;isGroup\u0026#34;: \u0026#34;version\u0026#34; } }, ... { \u0026#34;data\u0026#34;: { \u0026#34;id\u0026#34;: \u0026#34;6249668dd0a91adb9e62994d36563365\u0026#34;, \u0026#34;nodeType\u0026#34;: \u0026#34;app\u0026#34;, \u0026#34;namespace\u0026#34;: \u0026#34;istio-system\u0026#34;, \u0026#34;workload\u0026#34;: \u0026#34;istio-ingressgateway\u0026#34;, \u0026#34;app\u0026#34;: \u0026#34;istio-ingressgateway\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;unknown\u0026#34;, \u0026#34;rateOut\u0026#34;: \u0026#34;0.691\u0026#34;, \u0026#34;isOutside\u0026#34;: true, \u0026#34;isRoot\u0026#34;: true } } ], \u0026#34;edges\u0026#34;: [ { \u0026#34;data\u0026#34;: { \u0026#34;id\u0026#34;: \u0026#34;d51ca2a95d721427bbe27ed209766ec5\u0026#34;, \u0026#34;source\u0026#34;: \u0026#34;06e488a37fc9aa5b0e0805db4f16ae69\u0026#34;, \u0026#34;target\u0026#34;: \u0026#34;31150e7e5adf85b63f22fbd8255803d7\u0026#34;, \u0026#34;rate\u0026#34;: \u0026#34;0.236\u0026#34;, \u0026#34;percentRate\u0026#34;: \u0026#34;17.089\u0026#34;, \u0026#34;responseTime\u0026#34;: \u0026#34;0.152\u0026#34; } }, ... { \u0026#34;data\u0026#34;: { \u0026#34;id\u0026#34;: \u0026#34;1dda06d9904bcf727d1b6a113be58556\u0026#34;, \u0026#34;source\u0026#34;: \u0026#34;80f71758099020586131c3565075935d\u0026#34;, \u0026#34;target\u0026#34;: \u0026#34;4b64bda48e5a3c7e50ab1c63836c9469\u0026#34;, \u0026#34;rate\u0026#34;: \u0026#34;0.236\u0026#34;, \u0026#34;responseTime\u0026#34;: \u0026#34;0.022\u0026#34; } } ] } } 该值中包含了每个 node 和 edege 的信息，Node 即图中的每个节点，其中包含了节点的配置信息，Edge 即节点间的关系还有流量情况。前端可以根据该信息绘制服务拓扑图，我们下面将查看下 kiali 的后端，看看它是如何生成以上格式的 JSON 信息的。\n注：详细的 REST API 使用和字段说明请查看 swagger 生成的 API 文档。\n代码解析 下面将带大家了解 Kiali 的后端代码基本结构。\n路由配置\n服务拓扑图的路由信息保存在 kiali/routing/routes.go 文件中。\n{ \u0026#34;GraphNamespace\u0026#34;, \u0026#34;GET\u0026#34;, \u0026#34;/api/namespaces/{namespace}/graph\u0026#34;, handlers.GraphNamespace, true, }, { \u0026#34;GraphAppVersion\u0026#34;, \u0026#34;GET\u0026#34;, \u0026#34;/api/namespaces/{namespace}/applications/{app}/versions/{version}/graph\u0026#34;, handlers.GraphNode, true, }, { \u0026#34;GraphApp\u0026#34;, \u0026#34;GET\u0026#34;, \u0026#34;/api/namespaces/{namespace}/applications/{app}/graph\u0026#34;, handlers.GraphNode, true, }, { \u0026#34;GraphService\u0026#34;, \u0026#34;GET\u0026#34;, \u0026#34;/api/namespaces/{namespace}/services/{service}/graph\u0026#34;, handlers.GraphNode, true, }, { \u0026#34;GraphWorkload\u0026#34;, \u0026#34;GET\u0026#34;, \u0026#34;/api/namespaces/{namespace}/workloads/{workload}/graph\u0026#34;, handlers.GraphNode, true, } 直接查看 Swagger 生成的 API 文档也可以。\nPQL 查询语句构建\nkiali/handlers/graph.go 中处理 HTTP 请求，服务拓扑图中所有的指标信息都是从 Prometheus 中查询得到的。\nKiali 的服务状态拓扑是根据 namespace 来查询的，例如 default namespace 下的服务指标查询 PQL：\nround(sum(rate(istio_requests_total{reporter=\u0026#34;source\u0026#34;,source_workload_namespace=\u0026#34;default\u0026#34;,response_code=~\u0026#34;[2345][0-9][0-9]\u0026#34;} [600s])) by (source_workload_namespace,source_workload,source_app,source_version,destination_service_namespace,destination_service_name,destination_workload,destination_app,destination_version,response_code),0.001) 其中的参数都是通过页面选择传入的（构建的 PQL 中的选项在 kiali/graph/options/options.go 中定义）：\n reporter=\u0026#34;source\u0026#34;：metric 报告来源，源服务（source）是 Envoy 代理的下游客户端。在服务网格里，一个源服务通常是一个工作负载，但是入口流量的源服务有可能包含其他客户端，例如浏览器，或者一个移动应用。 …","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"9e5649e22ad4002c5c298dec0a42918d","permalink":"https://lib.jimmysong.io/istio-handbook/setup/istio-observability-tool-kiali/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/istio-handbook/setup/istio-observability-tool-kiali/","section":"istio-handbook","summary":"Istio 中有个 issue #9066 要求将 Istio 中默认使用的 Service Graph 替换成 Kiali。Kiali 最初是由 Red Hat 开源的，用于解决 Service Mesh 中可观测性即微服务的可视性问题。目前已获得 Istio 社区的官方支持。 关于 Kiali 单体应用使用微服务架构拆分成了许多微服","tags":["Istio","Service Mesh"],"title":"安装可观测性工具 Kiali","type":"book"},{"authors":null,"categories":["Istio"],"content":"Merbridge 是由 DaoCloud 在 2022 年初开源的的一款利用 eBPF 加速 Istio 服务网格的插件。使用 Merbridge 可以在一定程度上优化数据平面的网络性能。\n使用条件 要想使用 Merbridge，你的系统必须满足以下条件：\n Istio 网格中的主机使用 Linux 5.7 及以上版本内核  原理 在 Istio 中的透明流量劫持详解中，我们谈到 Istio 默认使用 IPtables 拦截数据平面中的流量到 Envoy 代理，这种拦截方式通用性最强。因为 Pod 中所有的 inbound 和 outbound 流量都会先通过 Envoy 代理，尤其是 Pod 接收的流量，都要先通过 IPtables 将流量劫持到 Envoy 代理后再发往 Pod 中的应用容器的端口。\n   使用 IPtables 劫持流量发到当前 Pod 的应用端口  利用 eBPF 的 sockops 和 redir 能力，可以直接将数据包从 inbound socket 传输到 outbound socket。eBPF 提供了 bpf_msg_redirect_hash 函数可以直接转发应用程序的数据包。\n下图展示的是在不同主机上的 Pod 的利用 eBPF 来劫持流量的示意图。\n   使用 Merbridge 的在不同主机上的 Pod  Pod 内部的流量劫持使用的是 Merbridge，而不同主机间依然需要使用 IPtables 来转发流量。\n如果两个 Pod 位于同一台主机，那么流量转发全程都可以通过 Merbridge 完成。下图展示了的是在同一主机上使用 Merbridge 的示意图。\n   使用 Merbridge 的同一个主机上的 Pod  关于 Merbridge 原理的详细解释请参考 Istio 文档。\n如何使用 只需要在 Istio 集群执行一条命令，即可直接使用 eBPF 代替 iptables 做透明流量拦截，实现网络加速。而且这对 Istio 是无感的，你可以随时安装和卸载 Merbridge。使用下面的命令启用 Merbridge：\nkubectl apply -f https://raw.githubusercontent.com/merbridge/merbridge/main/deploy/all-in-one.yaml Merbridge 是以 DaemonSet 的方式运行在 Istio 网格的每个节点上。它运行在服务网格的下层，对于 Istio 是透明的，要启用它时，无需对 Istio 做任何改动。\n如果你想删除 Merbridge，请运行下面的命令：\nkubectl delete -f https://raw.githubusercontent.com/merbridge/merbridge/main/deploy/all-in-one.yaml 参考  Merbridge - Accelerate your mesh with eBPF - istio.io   下一章   ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"71ca9cb8fa3d38de82d7427a06396ac3","permalink":"https://lib.jimmysong.io/istio-handbook/ecosystem/merbridge/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/istio-handbook/ecosystem/merbridge/","section":"istio-handbook","summary":"Merbridge 是由 DaoCloud 在 2022 年初开源的的一款利用 eBPF 加速 Istio 服务网格的插件。使用 Merbridge 可以在一定程度上优化数据平面的网络性能。 使用条件 要想使用 Merbridge，你的系统必须满足以下条件： Istio 网格中的主机使用 Linux 5.7 及以上版本内核","tags":["Istio","Service Mesh"],"title":"Merbridge","type":"book"},{"authors":null,"categories":["Istio"],"content":"服务中的工作负载之间的通信是通过 Envoy 代理进行的。当一个工作负载使用 mTLS 向另一个工作负载发送请求时，Istio 会将流量重新路由到 sidecar 代理（Envoy）。\n然后，sidecar Envoy 开始与服务器端的 Envoy 进行 mTLS 握手。在握手过程中，调用者会进行安全命名检查，以验证服务器证书中的服务账户是否被授权运行目标服务。一旦 mTLS 连接建立，Istio 就会将请求从客户端的 Envoy 代理转发到服务器端的 Envoy 代理。在服务器端的授权后，sidecar 将流量转发到工作负载。\n我们可以在服务的目标规则中改变 mTLS 行为。支持的 TLS 模式有：DISABLE（无 TLS 连接）、SIMPLE（向上游端点发起 TLS 连接）、MUTUAL（通过出示客户端证书进行认证来使用 mTLS）和 ISTIO_MUTUAL（与 MUTUAL 类似，但使用 Istio 自动生成的证书进行 mTLS）。\n什么是 mTLS？ 在一个典型的 Kubernetes 集群中，加密的流量进入集群，经过一个负载均衡器终止 TLS 连接，从而产生解密的流量。然后，解密的流量被发送到集群内的相关服务。由于集群内的流量通常被认为是安全的，对于许多用例，这是一个可以接受的方法。\n但对于某些用例，如处理个人身份信息（PII），可能需要额外的保护。在这些情况下，我们希望确保 所有的 网络流量，甚至同一集群内的流量，都是加密的。这为防止窥探（读取传输中的数据）和欺骗（伪造数据来源）攻击提供了额外保障。这可以帮助减轻系统中其他缺陷的影响。\n如果手动实现这个完整的数据传输加密系统的话，需要对集群中的每个应用程序进行大规模改造。你需要告诉所有的应用程序终止自己的 TLS 连接，为所有的应用程序颁发证书，并为所有的应用程序添加一个新的证书颁发机构。\nIstio 的 mTLS 在应用程序之外处理这个问题。它安装了一个 sidecar，通过 localhost 连接与你的应用程序进行通信，绕过了暴露的网络流量。它使用复杂的端口转发规则（通过 IPTables）来重定向进出 Pod 的流量，使其通过 sidecar。代理中的 Envoy sidecar 处理所有获取 TLS 证书、刷新密钥、终止等逻辑。\nIstio 的这种工作方式虽然可以让你避免修改应用程序，但是当它可以工作时，能够工作得很好。而当它失败时，它可能是灾难性的，而且还难以调试。Istio 的 mTLS 值得一提的三个具体要点。\n  在严格模式（Strict Mode）下，也就是我们要做的，数据平面 Envoy 会拒绝任何传入的明文通信。\n  通常情况下，如果你对一个不存在的主机进行 HTTP 连接，你会得到一个失败的连接错误。你肯定 不会 得到一个 HTTP 响应。然而，在 Istio 中，你将 总是 成功地发出 HTTP 连接，因为你的连接是给 Envoy 本身的。如果 Envoy 代理不能建立连接，它将像大多数代理一样，返回一个带有 503 错误信息的 HTTP 响应体。\n  Envoy 代理对一些协议有特殊处理。最重要的是，如果你做一个纯文本的 HTTP 外发连接，Envoy 代理有复杂的能力来解析外发请求，了解各种头文件的细节，并做智能路由。\n  允许模式 允许模式（Permissive Mode）是一个特殊的选项，它允许一个服务同时接受纯文本流量和 mTLS 流量。这个功能的目的是为了改善 mTLS 的用户体验。\n默认情况下，Istio 使用允许模式配置目标工作负载。Istio 跟踪使用 Istio 代理的工作负载，并自动向其发送 mTLS 流量。如果工作负载没有代理，Istio 将发送纯文本流量。\n当使用允许模式时，服务器接受纯文本流量和 mTLS 流量，不会破坏任何东西。允许模式给了我们时间来安装和配置 sidecar，以逐步发送 mTLS 流量。\n一旦所有的工作负载都安装了 sidecar，我们就可以切换到严格的 mTLS 模式。要做到这一点，我们可以创建一个 PeerAuthentication 资源。我们可以防止非双向 TLS 流量，并要求所有通信都使用 mTLS。\n我们可以创建 PeerAuthentication 资源，首先在每个命名空间中分别执行严格模式。然后，我们可以在根命名空间（在我们的例子中是 istio-system）创建一个策略，在整个服务网格中执行该策略：\napiVersion:security.istio.io/v1beta1kind:PeerAuthenticationmetadata:name:defaultnamespace:istio-systemspec:mtls:mode:STRICT此外，我们还可以指定selector字段，将策略仅应用于网格中的特定工作负载。下面的例子对具有指定标签的工作负载启用STRICT模式：\napiVersion:security.istio.io/v1beta1kind:PeerAuthenticationmetadata:name:defaultnamespace:my-namespacespec:selector:matchLabels:app:customersmtls:mode:STRICT参考  An Istio/mutual TLS debugging story -fpcomplete.com  ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"4ba8fc9d16268c1984810e82c985e586","permalink":"https://lib.jimmysong.io/istio-handbook/security/mtls/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/istio-handbook/security/mtls/","section":"istio-handbook","summary":"服务中的工作负载之间的通信是通过 Envoy 代理进行的。当一个工作负载使用 mTLS 向另一个工作负载发送请求时，Istio 会将流量重新路由到 sidecar 代理（Envoy）。 然后，sidecar Envoy 开始与服务器端的 Envoy 进行 mTLS 握手。在握","tags":["Istio","Service Mesh"],"title":"mTLS","type":"book"},{"authors":null,"categories":["Istio"],"content":"PeerAuthentication（对等认证）定义了流量将如何被隧道化（或不被隧道化）到 sidecar。\n示例 策略允许命名空间 foo 下所有工作负载的 mTLS 流量。\napiVersion:security.istio.io/v1beta1kind:PeerAuthenticationmetadata:name:defaultnamespace:foospec:mtls:mode:STRICT对于网格级别，根据你的 Istio 安装，将策略放在根命名空间。\n策略允许命名空间 foo 下的所有工作负载的 mTLS 和明文流量，但 finance 的工作负载需要 mTLS。\napiVersion:security.istio.io/v1beta1kind:PeerAuthenticationmetadata:name:defaultnamespace:foospec:mtls:mode:PERMISSIVE---apiVersion:security.istio.io/v1beta1kind:PeerAuthenticationmetadata:name:defaultnamespace:foospec:selector:matchLabels:app:financemtls:mode:STRICT政策允许所有工作负载严格 mTLS，但 8080 端口保留为明文。\napiVersion:security.istio.io/v1beta1kind:PeerAuthenticationmetadata:name:defaultnamespace:foospec:selector:matchLabels:app:financemtls:mode:STRICTportLevelMtls:8080:mode:DISABLE从命名空间（或网格）设置中继承 mTLS 模式的策略，并覆盖 8080 端口的设置。\napiVersion:security.istio.io/v1beta1kind:PeerAuthenticationmetadata:name:defaultnamespace:foospec:selector:matchLabels:app:financemtls:mode:UNSETportLevelMtls:8080:mode:DISABLE关于 PeerAuthentication 配置的详细用法请参考 Istio 官方文档。\n参考  PeerAuthentication- istio.io  ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"746cdff0d3838b0e0986756aef0be42f","permalink":"https://lib.jimmysong.io/istio-handbook/config-security/peer-authentication/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/istio-handbook/config-security/peer-authentication/","section":"istio-handbook","summary":"PeerAuthentication（对等认证）定义了流量将如何被隧道化（或不被隧道化）到 sidecar。 示例 策略允许命名空间 foo 下所有工作负载的 mTLS 流量。 apiVersion:security.isti","tags":["Istio","Service Mesh"],"title":"PeerAuthentication","type":"book"},{"authors":null,"categories":["Envoy"],"content":"Wasm 是一种可执行代码的可移植二进制格式，依赖于一个开放的标准。它允许开发人员用自己喜欢的编程语言编写，然后将代码编译成 Wasm 模块。\n   代码编译成 wasm  Wasm 模块与主机环境隔离，并在一个称为虚拟机（VM） 的内存安全沙盒中执行。Wasm 模块使用一个 API 与主机环境进行通信。\nWasm 的主要目标是在网页上实现高性能应用。例如，假设我们要用 Javascript 构建一个网页应用程序。我们可以用 Go（或其他语言）写一些，并将其编译成一个二进制文件，即 Wasm 模块。然后，我们可以在与 Javascript 网页应用程序相同的沙盒中运行已编译的 Wasm 模块。\n最初，Wasm 被设计为在网络浏览器中运行。然而，我们可以将虚拟机嵌入到其他主机应用程序中，并执行它们。这就是 Envoy 的作用！\nEnvoy 嵌入了 V8 虚拟机的一个子集。V8 是一个用 C++ 编写的高性能 JavaScript 和 WebAssembly 引擎，它被用于 Chrome 和 Node.js 等。\n我们在本课程的前面提到，Envoy 使用多线程模式运行。这意味着有一个主线程，负责处理配置更新和执行全局任务。\n除了主线程之外，还有负责代理单个 HTTP 请求和 TCP 连接的 worker 线程。这些 worker 线程被设计为相互独立。例如，处理一个 HTTP 请求的 worker 线程不会受到其他处理其他请求的 worker 线程的影响。\n   Envoy 线程  每个线程拥有自己的资源副本，包括 Wasm 虚拟机。这样做的原因是为了避免任何昂贵的跨线程同步，即实现更高的内存使用率。\nEnvoy 在运行时将每个独特的 Wasm 模块（所有 *.wasm 文件）加载到一个独特的 Wasm VM。由于 Wasm VM 不是线程安全的（即，多个线程必须同步访问一个 Wasm VM），Envoy 为每个将执行扩展的线程创建一个单独的 Wasm VM 副本。因此，每个线程可能同时有多个 Wasm VM 在使用。\nProxy-Wasm 我们将使用的 SDK 允许我们编写 Wasm 扩展，这些扩展是 HTTP 过滤器，网络过滤器，或称为 Wasm 服务的专用扩展类型。这些扩展在 Wasm 虚拟机内的 worker 线程（HTTP 过滤器，网络过滤器）或主线程（Wasm 服务）上执行。正如我们提到的，这些线程是独立的，它们本质上不知道其他线程上发生的请求处理。\nHTTP 过滤器是处理 HTTP 协议的，它对 HTTP Header、body 等进行操作。同样，网络过滤器处理 TCP 协议，对数据帧和连接进行操作。我们也可以说，这两种插件类型是无状态的。\nEnvoy 还支持有状态的场景。例如，你可以编写一个扩展，将请求数据、日志或指标等统计信息在多个请求之间进行汇总——这意味着跨越了许多 worker 线程。对于这种情况，我们会使用 Wasm 服务类型。Wasm 服务类型运行在单个虚拟机上；这个虚拟机只有一个实例，它运行在 Envoy 主线程上。你可以用它来汇总无状态过滤器的指标或日志。\n下图显示了 Wasm 服务扩展是如何在主线程上执行的，而不是 HTTP 或网络过滤器，后者是在 worker 线程上执行。\n   API  事实上，Wasm 服务扩展是在主线程上执行的，并不影响请求延迟。另一方面，网络或 HTTP 过滤器会影响延迟。\n图中显示了在主线程上运行的 Wasm 服务扩展，它使用消息队列 API 订阅队列并接收由运行在 worker 线程上的 HTTP 过滤器或网络过滤器发送的消息。然后，Wasm 服务扩展可以聚合从 worker 线程上收到的数据。\nWasm 服务扩展并不是持久化数据的唯一方法。你也可以调用 HTTP 或 gRPC API。此外，我们可以使用定时器 API 在请求之外执行行动。\n我们提到的 API、消息队列、定时器和共享数据都是由 Proxy-Wasm 提供的。\nProxy-Wasm 是一个代理无关的 ABI（应用二进制接口）标准，它规定了代理（我们的主机）和 Wasm 模块如何互动。这些互动是以函数和回调的形式实现的。\nProxy-Wasm 中的 API 与代理无关，这意味着它们可以与 Envoy 代理以及任何其他代理（例如 MOSN）一起实现 Proxy-Wasm 标准。这使得你的 Wasm 过滤器可以在不同的代理之间移植，而且它们并不局限于 Envoy。\n   Proxy-wasm  当请求进入 Envoy 时，它们会经过不同的过滤器链，被过滤器处理，在链中的某个点，请求数据会流经本地 Proxy-Wasm 扩展。\n这个扩展使用 Proxy-Wasm 接口与运行在虚拟机内的扩展通信。 过滤器处理完数据后，该链就会继续，或停止，这取决于从扩展返回的结果。\n基于 Proxy-Wasm 规范，我们可以使用一些特定语言的 SDK 实现来编写扩展。\n在其中一个实验中，我们将使用 Go SDK for Proxy-Wasm 来编写 Go 中的 Proxy-Wasm 插件。\nTinyGo 是一个用于嵌入式系统和 WebAssembly 的编译器。它不支持使用所有的标准 Go 包。例如，不支持一些标准包，如 net 和其他。\n你还可以选择使用 Assembly Script、C++、Rust 或 Zig。\n配置 Wasm 扩展 Envoy 中的通用 Wasm 扩展配置看起来像这样。\n- name:envoy.filters.http.wasmtyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/udpa.type.v1.TypedStructtype_url:type.googleapis.com/envoy.extensions.filters.http.wasm.v3.Wasmvalue:config:vm_config:vm_id:\u0026#34;my_vm\u0026#34;runtime:\u0026#34;envoy.wasm.runtime.v8\u0026#34;configuration:\u0026#34;@type\u0026#34;: type.googleapis.com/google.protobuf.StringValuevalue:\u0026#39;{\u0026#34;plugin-config\u0026#34;: \u0026#34;some-value\u0026#34;}\u0026#39;code:local:filename:\u0026#34;my-plugin.wasm\u0026#34;configuration:\u0026#34;@type\u0026#34;: type.googleapis.com/google.protobuf.StringValuevalue:\u0026#39;{\u0026#34;vm-wide-config\u0026#34;: \u0026#34;some-value\u0026#34;}\u0026#39;vm_config 字段用于指定 Wasm 虚拟机、运行时，以及我们要执行的.wasm 扩展的实际指针。\nvm_id 字段在虚拟机之间进行通信时使用。然后这个 ID 可以用来通过共享数据 API 和队列在虚拟机之间共享数据。请注意，要在多个插件中重用虚拟机，你必须使用相同的 vm_id、运行时、配置和代码。\n下一个项目是 runtime。这通常被设置为 envoy.wasm.runtime.v8。例如，如果我们用 Envoy 编译 Wasm 扩展，我们会在这里使用 null 运行时。其他选项是 Wasm micro runtime、Wasm VM 或 Wasmtime；不过，这些在官方 Envoy 构建中都没有启用。\nvm_config 字段下的配置是用来配置虚拟机本身的。除了虚拟机 ID 和运行时外，另一个重要的部分是code字段。\ncode 字段是我们引用编译后的 Wasm 扩展的地方。这可以是一个指向本地文件的指针（例如，/etc/envoy/my-plugin.wasm）或一个远程位置（例如，https://wasm.example.com/my-plugin.wasm）。\nconfiguration 文件，一个在 vm_config 下，另一个在 config 层，用于为虚拟机和插件提供配置。然后当虚拟机或插件启动时，可以从 Wasm 扩展代码中读取这些值。\n要运行一个 Wasm 服务插件，我们必须在 bootstrap_extensions 字段中定义配置，并将 singleton 布尔字段的值设置为真。\nbootstrap_extensions:- name:envoy.bootstrap.wasmtyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.wasm.3.WasmServicesingleton:trueconfig:vm_config:{ ...}开发 Wasm 扩展 - Proxy-Wasm Go SDK API 在开发 Wasm 扩展时，我们将学习上下文、hostcall API 和入口点。\n上下文 上下文是 Proxy-Wasm SDK 中的一个接口集合，并与我们前面解释的概念相匹配。\n   上下文  例如，每个虚拟机中都有一个 VMContext，可以有一个或多个 PluginContexts。这意味着我们可以在同一个虚拟机上下文中运行不同的插件（即使用同一个 vm_id 时）。每个 PluginContext 对应于一个插件实例。那就是 TcpContext（TCP 网络过滤器）或 HttpContext（HTTP 过滤器）。\nVMContext 接口定义了两个函数：OnVMStart 函数和 NewPluginContext 函数。\ntype VMContext interface { OnVMStart(vmConfigurationSize int) OnVMStartStatus NewPluginContext(contextID uint32) PluginContext } 顾名思义，OnVMStart 在虚拟机创建后被调用。在这个函数中，我们可以使用 GetVMConfiguration hostcall 检索可选的虚拟机配置。这个函数的目的是执行任何虚拟机范围的初始化。\n作为开发者，我们需要实现 NewPluginContext 函数，在该函数中我们创建一个 PluginContext 的实例。\nPluginContext 接口定义了与 VMContext 类似的功能。下面是这个接口。\ntype PluginContext interface { OnPluginStart(pluginConfigurationSize int) OnPluginStartStatus OnPluginDone() bool OnQueueReady(queueID uint32) OnTick() NewTcpContext(contextID uint32) TcpContext NewHttpContext(contextID uint32) HttpContext } OnPluginStart 函数与我们前面提到的 OnVMStart 函数类似。它在插件被创建时被调用。在这个函数中，我们也可以使用 GetPluginConfiguration  API 来检索插件的特定配置。我们还必须实现 NewTcpContext 或 NewHttpContext，在代理中响应 HTTP/TCP 流时被调用。这个上下文还包含一些其他的函数，用于设置队列（OnQueueReady）或在流处理的同时做异步任务（OnTick）。\n 参考 Proxy Wasm Go SDK Github 仓库 中的 context.go 文件，以获得最新的接口定义。\n Hostcall API 这里实现的 hostcall API ，为我们提供了与 Wasm 插件的 Envoy 代理互动的方法。\nhostcall API 定义了读取配置的方法；设置共享队列并执行队列操作；调度 HTTP 调用，从请求和响应流中检索 Header、Trailer 和正文并操作这些值；配置指标；以及更多。\n …","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"5d04c0a359b62a4f17735fd766c89720","permalink":"https://lib.jimmysong.io/envoy-handbook/extending-envoy/wasm/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/envoy-handbook/extending-envoy/wasm/","section":"envoy-handbook","summary":"Wasm 是一种可执行代码的可移植二进制格式，依赖于一个开放的标准。它允许开发人员用自己喜欢的编程语言编写，然后将代码编译成 Wasm 模块。 代码编译成 wasm Wasm 模块与主机环境隔离，并在一个称为虚拟机（VM） 的内存安全沙盒中","tags":["Envoy"],"title":"WebAssembly（Wasm）","type":"book"},{"authors":null,"categories":["Istio"],"content":"Envoy 通过查询文件或管理服务器来动态发现资源。概括地讲，对应的发现服务及其相应的 API 被称作 xDS。Envoy 通过订阅（subscription）方式来获取资源，如监控指定路径下的文件、启动 gRPC 流或轮询 REST-JSON URL。后两种方式会发送 DiscoveryRequest 请求消息，发现的对应资源则包含在响应消息 DiscoveryResponse 中。下面，我们将具体讨论每种订阅类型。\n文件订阅 发现动态资源的最简单方式就是将其保存于文件，并将路径配置在 ConfigSource 中的 path 参数中。Envoy 使用 inotify（Mac OS X 上为 kqueue）来监控文件的变化，在文件被更新时，Envoy 读取保存的 DiscoveryResponse 数据进行解析，数据格式可以为二进制 protobuf、JSON、YAML 和协议文本等。\n 译者注：core.ConfigSource 配置格式如下：\n { \u0026#34;path\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;api_config_source\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;ads\u0026#34;: \u0026#34;{...}\u0026#34; } 文件订阅方式可提供统计数据和日志信息，但是缺少 ACK/NACK 更新的机制。如果更新的配置被拒绝，xDS API 则继续使用最后一个的有效配置。\ngRPC 流式订阅 单资源类型发现 每个 xDS API 可以单独配置 ApiConfigSource，指向对应的上游管理服务器的集群地址。每个 xDS 资源类型会启动一个独立的双向 gRPC 流，可能对应不同的管理服务器。API 交付方式采用最终一致性。可以参考后续聚合服务发现（ADS） 章节来了解必要的显式控制序列。\n 译者注：core.ApiConfigSource 配置格式如下：\n { \u0026#34;api_type\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;cluster_names\u0026#34;: [], \u0026#34;grpc_services\u0026#34;: [], \u0026#34;refresh_delay\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;request_timeout\u0026#34;: \u0026#34;{...}\u0026#34; } 类型 URL 每个 xDS API 都与给定的资源的类型存在 1:1 对应。关系如下：\n LDS： envoy.api.v2.Listener RDS： envoy.api.v2.RouteConfiguration CDS： envoy.api.v2.Cluster EDS： envoy.api.v2.ClusterLoadAssignment SDS：envoy.api.v2.Auth.Secret  类型 URL 的概念如下所示，其采用 type.googleapis.com/\u0026lt;resource type\u0026gt; 的形式，例如 CDS 对应于 type.googleapis.com/envoy.api.v2.Cluster。在 Envoy 的请求和管理服务器的响应中，都包括了资源类型 URL。\nACK/NACK 和版本 每个 Envoy 流以 DiscoveryRequest 开始，包括了列表订阅的资源、订阅资源对应的类型 URL、节点标识符和空的 version_info。EDS 请求示例如下：\nversion_info:node:{id:envoy }resource_names:- foo- bartype_url:type.googleapis.com/envoy.api.v2.ClusterLoadAssignmentresponse_nonce:管理服务器可立刻或等待资源就绪时发送 DiscoveryResponse 作为响应，示例如下：\nversion_info:Xresources:- foo ClusterLoadAssignment proto encoding- bar ClusterLoadAssignment proto encodingtype_url:type.googleapis.com/envoy.api.v2.ClusterLoadAssignmentnonce:AEnvoy 在处理 DiscoveryResponse 响应后，将通过流发送一个新的请求，请求包含应用成功的最后一个版本号和管理服务器提供的 nonce。如果本次更新已成功应用，则 version_info 的值设置为 X，如下序列图所示：\n   ACK 后的版本更新  在此序列图及后续中，将统一使用以下缩写格式：\n DiscoveryRequest： (V=version_info，R=resource_names，N=response_nonce，T=type_url) DiscoveryResponse： (V=version_info，R=resources，N=nonce，T=type_url)   译者注：在信息安全中，Nonce是一个在加密通信只能使用一次的数字。在认证协议中，它往往是一个随机或伪随机数，以避免重放攻击。Nonce也用于流密码以确保安全。如果需要使用相同的密钥加密一个以上的消息，就需要Nonce来确保不同的消息与该密钥加密的密钥流不同。（引用自维基百科）在本文中nonce是每次更新的数据包的唯一标识。\n 版本为 Envoy 和管理服务器提供了共享当前应用配置的概念和通过 ACK/NACK 来进行配置更新的机制。如果 Envoy 拒绝配置更新 X，则回复 error_detail 及前一个的版本号，在当前情况下为空的初始版本号，error_detail 包含了有关错误的更加详细的信息：\n   NACK 无版本更新  后续，API 更新可能会在新版本 Y 上成功：\n   ACK 紧接着 NACK  每个流都有自己的版本概念，但不存在跨资源类型的共享版本。在不使用 ADS 的情况下，每个资源类型可能具有不同的版本，因为 Envoy API 允许指向不同的 EDS/RDS 资源配置并对应不同的 ConfigSources。\n何时发送更新 管理服务器应该只向 Envoy 客户端发送上次 DiscoveryResponse 后更新过的资源。Envoy 则会根据接受或拒绝 DiscoveryResponse 的情况，立即回复包含 ACK/NACK 的 DiscoveryRequest 请求。如果管理服务器每次发送相同的资源集结果，而不是根据其更新情况，则会导致 Envoy 和管理服务器通讯效率大打折扣。\n在同一个流中，新的 DiscoveryRequests 将取代此前具有相同的资源类型 DiscoveryRequest 请求。这意味着管理服务器只需要响应给定资源类型最新的 DiscoveryRequest 请求即可。\n资源提示 DiscoveryRequest 中的 resource_names 信息作为资源提示出现。一些资源类型，例如 Cluster 和 Listener 将使用一个空的 resource_names，因为 Envoy 需要获取管理服务器对应于节点标识的所有 Cluster（CDS）和 Listener（LDS）。对于其他资源类型，如 RouteConfigurations（RDS）和 ClusterLoadAssignments（EDS），则遵循此前的 CDS/LDS 更新，Envoy 能够明确地枚举这些资源。\nLDS/CDS 资源提示信息将始终为空，并且期望管理服务器的每个响应都提供 LDS/CDS 资源的完整状态。缺席的 Listener 或 Cluster 将被删除。\n对于 EDS/RDS，管理服务器并不需要为每个请求的资源进行响应，而且还可能提供额外未请求的资源。resource_names 只是一个提示。Envoy 将默默地忽略返回的多余资源。如果请求的资源中缺少相应的 RDS 或 EDS 更新，Envoy 将保留对应资源的最后的值。管理服务器可能会依据 DiscoveryRequest 中 node 标识推断其所需的 EDS/RDS 资源，在这种情况下，提示信息可能会被丢弃。从相应的角度来看，空的 EDS/RDS DiscoveryResponse 响应实际上是表明在 Envoy 中为一个空的资源。\n当 Listener 或 Cluster 被删除时，其对应的 EDS 和 RDS 资源也需要在 Envoy 实例中删除。为使 EDS 资源被 Envoy 已知或跟踪，就必须存在应用过的 Cluster 定义（如通过 CDS 获取）。RDS 和 Listeners 之间存在类似的关系（如通过 LDS 获取）。\n对于 EDS/RDS ，Envoy 可以为每个给定类型的资源生成不同的流（如每个 ConfigSource 都有自己的上游管理服务器的集群）或当指定资源类型的请求发送到同一个管理服务器的时候，允许将多个资源请求组合在一起发送。虽然可以单个实现，但管理服务器应具备处理每个给定资源类型中对单个或多个 resource_names 请求的能力。下面的两个序列图对于获取两个 EDS 资源都是有效的 {foo，bar}：\n   一个流上多个 EDS 请求     不同流上的多个 EDS 请求  资源更新 如上所述，Envoy 可能会更新 DiscoveryRequest 中出现的 resource_names 列表，其中 DiscoveryRequest 是用来 ACK/NACK 管理服务器的特定的 DiscoveryResponse 。此外，Envoy 后续可能会发送额外的 DiscoveryRequests ，用于在特定 version_info 上使用新的资源提示来更新管理服务器。例如，如果 Envoy 在 EDS 版本 X 时仅知道集群 foo，但在随后收到的 CDS 更新时额外获取了集群 bar ，它可能会为版本 X 发出额外的 DiscoveryRequest 请求，并将 {foo，bar} 作为请求的 resource_names 。\n   CDS 响应导致 EDS 资源更新  这里可能会出现竞争状况；如果 Envoy 在版本 X 上发布了资源提示更新请求，但在管理服务器处理该请求之前发送了新的版本号为 Y 的响应，针对 version_info 为 X 的版本，资源提示更新可能会被解释为拒绝 Y 。为避免这种情况，通过使用管理服务器提供的 nonce，Envoy 可用来保证每个 DiscoveryRequest 对应到相应的 DiscoveryResponse ：\n   EDS 更新速率激发 nonces  管理服务器不应该为含有过期 nonce 的 DiscoveryRequest 发送 DiscoveryResponse 响应。在向 Envoy 发送的 DiscoveryResponse 中包含了的新 nonce ，则此前的 nonce 将过期。在资源新版本就绪之前，管理服务器不需要向 Envoy 发送更新。同版本的早期请求将会过期。在新版本就绪时，管理服务器可能会处理同一个版本号的多个 DiscoveryRequests请求。\n   请求变的陈旧  上述资源更新序列表明 Envoy 并不能期待其发出的每个 DiscoveryRequest 都得到 DiscoveryResponse 响应。\n最终一致性考虑 由于 Envoy 的 xDS API 采用最终一致性，因此在更新期间可能导致流量被丢弃。例如，如果通过 CDS/EDS 仅获取到了集群 X，而且 RouteConfiguration 引用了集群 X；在 CDS/EDS 更新集群 Y 配置之前，如果将 RouteConfiguration 将引用的集群调整为 Y ，那么流量将被吸入黑洞而丢弃，直至集群 Y 被 Envoy 实例获取。\n对某些应用程序，可接受临时的流量丢弃，客户端重试或其他 Envoy sidecar 会掩盖流量丢弃。那些对流量丢弃不能容忍的场景，可以通过以下方式避免流量丢失，CDS/EDS 更新同时携带 X 和 Y ，然后发送 RDS 更新从 X 切 …","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"0ca31feed76db86adcc56afa51f9a9fb","permalink":"https://lib.jimmysong.io/istio-handbook/data-plane/envoy-xds-protocol/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/istio-handbook/data-plane/envoy-xds-protocol/","section":"istio-handbook","summary":"Envoy 通过查询文件或管理服务器来动态发现资源。概括地讲，对应的发现服务及其相应的 API 被称作 xDS。Envoy 通过订阅（subscription）方式来获取资源，如监控指定路径下的文件、启动 gRPC 流或轮询 REST-JSON URL","tags":["Istio","Service Mesh"],"title":"xDS 协议解析","type":"book"},{"authors":null,"categories":["Istio"],"content":"接下来，我们将部署可观测性、分布式追踪、数据可视化工具：\nkubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.10/samples/addons/prometheus.yaml kubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.10/samples/addons/grafana.yaml kubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.10/samples/addons/kiali.yaml kubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.10/samples/addons/extras/zipkin.yaml  如果你在安装 Kiali 的时候发现以下错误 No matches for kind \u0026#34;MonitoringDashboard\u0026#34; in version \u0026#34;monitoring.kiali.io/v1alpha1\u0026#34; 请重新运行以上命令。\n 要从 Google Cloud Shell 打开仪表盘，我们可以运行 getmesh istioctl dash kiali 命令，例如，然后点击 Web Preview 按钮，选择仪表盘运行的端口（Kiali 为 20001）。如果你使用你的终端，运行 Istio CLI 命令就可以了。\n下面是 Boutique 图表在 Kiali 中的样子：\n   Boutique 应用在 Kiali 中的样子  ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"38efda69af8234bc574fca0acf70ef33","permalink":"https://lib.jimmysong.io/istio-handbook/practice/observability/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/istio-handbook/practice/observability/","section":"istio-handbook","summary":"接下来，我们将部署可观测性、分布式追踪、数据可视化工具： kubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.10/samples/addons/prometheus.yaml kubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.10/samples/addons/grafana.yaml kubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.10/samples/addons/kiali.yaml kubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.10/samples/addons/extras/zipkin.yaml 如果你在安装 Kiali 的时候发现以下错误 No matches for kind \"MonitoringDashboard\" in version \"monitoring.kiali.io/v1alpha1\" 请重新运行以上命令。 要从 Google Cloud Shell 打开仪表盘，我们可以运行 getmesh istioctl dash kiali 命","tags":["Istio","Service Mesh"],"title":"部署可观测性工具","type":"book"},{"authors":null,"categories":["Istio"],"content":"每当你遇到配置问题时，你可以使用这组步骤来浏览和解决问题。在第一部分，我们要检查配置是否有效。如果配置是有效的，下一步就是看看运行时是如何处理配置的，为此，你需要对 Envoy 配置有基本的了解。\n配置 1. 配置是否有效？\nIstio CLI 有一个叫 validate 的命令，我们可以用它来验证 YAML 配置。YAML 最常见的问题是缩进和数组符号相关的问题。 要验证一个配置，请将 YAML 文件传递给 validate 命令，像这样：\n$ istioctl validate -f myresource.yaml validation succeed 如果资源是无效的，CLI 会给我们一个详细的错误。例如，如果我们拼错了一个字段名：\nunknown field \u0026#34;worloadSelector\u0026#34; in v1alpha3.ServiceEntry 我们可以使用另一个命令istioctl analyze。使用这个命令，我们可以检测 Istio 配置的潜在问题。我们可以针对本地的一组配置文件或实时集群运行它。同时，寻找来自 istiod 的任何警告或错误。\n下面是该命令的一个输出样本，它捕捉到了目的地主机名称中的一个错字：\n$ istioctl analyze Error [IST0101] (VirtualService customers.default) Referenced host not found: \u0026#34;cusomers.default.svc.cluster.local\u0026#34; Error [IST0101] (VirtualService customers.default) Referenced host+subset in destinationrule not found: \u0026#34;cusomers.default.svc.cluster.local+v1\u0026#34; Error: Analyzers found issues when analyzing namespace: default. See https://istio.io/docs/reference/config/analysis for more information about causes and resolutions. 2. 命名是否正确？资源是否在正确的命名空间？\n几乎所有的 Istio 资源都是命名空间范围的。确保它们与你正在处理的服务处于同一命名空间。将 Istio 资源放在同一命名空间中尤其重要，因为选择器也是有命名空间的。\n一个常见的错误配置是在应用程序的命名空间中发布 VirtualService（例如 default），然后使用 istio：ingressgateway 选择器来绑定到 istio-system 命名空间中的 ingress 网关部署。这只有在你的 VirtualService 也在 istio-system 命名空间中时才有效。\n同样地，不要在 istio-system 命名空间中部署引用应用程序命名空间中的 VirtualService 的 Sidecar 资源。相反，为每个需要入口的应用程序部署一组 Envoy 网关。\n3. 资源选择器是否正确？\n验证部署中的 pod 是否有正确的标签设置。正如上一步提到的，资源选择器与资源发布的命名空间绑定。\n在这一点上，我们应该有理由相信，配置是正确的。接下来的步骤是进一步研究运行时系统是如何处理配置的。\n运行时 Istio CLI 的一个实验性功能可以提供信息，帮助我们了解影响 Pod 或服务的配置。下面是一个针对 Pod 运行 describe 命令的例子，这个 Pod 的主机名称中有一个错字：\n$ istioctl x describe pod customers-v1-64455cd4c6-xvjzm.default Pod: customers-v1-64455cd4c6-xvjzm Pod Ports: 3000 (svc), 15090 (istio-proxy) -------------------- Service: customers Port: http 80/HTTP targets pod port 3000 DestinationRule: customers for \u0026#34;customers.default.svc.cluster.local\u0026#34; Matching subsets: v1 No Traffic Policy VirtualService: customers WARNING: No destinations match pod subsets (checked 1 HTTP routes) Route to cusomers.default.svc.cluster.local 1. Envoy 是否接受（ACK）该配置？\n你可以使用 istioctl proxy-status 命令来检查状态，看看 Envoy 是否接受配置。我们希望所有东西的状态都设置为 SYNCHED。任何其他值都可能表明有错误，你应该检查 Pilot 的日志。\n$ istioctl proxy-status NAME CDS LDS EDS RDS ISTIOD VERSION customers-v1... SYNCED SYNCED SYNCED SYNCED istiod-67b4c76c6-8lwxf 1.9.0 customers-v1... SYNCED SYNCED SYNCED SYNCED istiod-67b4c76c6-8lwxf 1.9.0 istio-egress... SYNCED SYNCED SYNCED NOT SENT istiod-67b4c76c6-8lwxf 1.9.0 istio-ingress... SYNCED SYNCED SYNCED SYNCED istiod-67b4c76c6-8lwxf 1.9.0 web-frontend-... SYNCED SYNCED SYNCED SYNCED istiod-67b4c76c6-8lwxf 1.9.0 列表显示所有连接到 Pilot 实例的代理。如果列表中缺少一个代理，这意味着它没有连接到 Pilot，也没有收到任何配置。如果任何一个代理被标记为STALE，可能有网络问题，或者我们需要扩展 Pilot。\n如果 Envoy 接受了配置，但我们仍然看到问题，我们需要确保配置在 Envoy 中的表现符合预期。\n2. 配置在 Envoy 中的表现和预期的一样吗？\n我们可以使用 proxy-config 命令来检索特定 Envoy 实例的信息。请参考下面的表格，我们可以检索不同的代理配置。\n   命令 描述     istioctl proxy-config cluster [POD] -n [NAMESPACE] 检索 cluster 配置   istioctl proxy-config bootstrap [POD] -n [NAMESPACE] 检索 bootstrap 配置   istioctl proxy-config listener [POD] -n [NAMESPACE] 检索 listener 配置   istioctl proxy-config route [POD] -n [NAMESPACE] 检索 route 配置   istioctl proxy-config endpoints [POD] -n [NAMESPACE] 检索 endpoint 配置    该命令从 Envoy 的管理端点（主要是 /config_dump）收集数据，它包含了很多有用的信息。\n另外，请参考显示 Envoy 和 Istio 资源之间映射的图。例如，许多 VirtualService 规则将表现为 Envoy 路由，而 DestinationRules 和 ServiceEntries 则表现为 Cluster。\n DestinationRules 不会出现在配置中，除非其主机的 ServiceEntry 首先存在。\n 让我们以客户的 VirtualService 为例。\napiVersion:networking.istio.io/v1alpha3kind:VirtualServicemetadata:name:customersspec:hosts:- \u0026#39;customers.default.svc.cluster.local\u0026#39;http:- route:- destination:host:customers.default.svc.cluster.localport:number:80subset:v1weight:80- destination:host:customers.default.svc.cluster.localport:number:80subset:v2weight:20timeout:5s如果你运行istioctl proxy-config routes [POD] -o json命令，你会看到加权目的地和超时是如何在配置中体现的：\n.. { \u0026#34;name\u0026#34;: \u0026#34;80\u0026#34;, \u0026#34;virtualHosts\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;customers.default.svc.cluster.local:80\u0026#34;, \u0026#34;domains\u0026#34;: [ \u0026#34;customers.default.svc.cluster.local\u0026#34;, ... ], \u0026#34;routes\u0026#34;: [ { \u0026#34;match\u0026#34;: {\u0026#34;prefix\u0026#34;: \u0026#34;/\u0026#34;}, \u0026#34;route\u0026#34;: { \u0026#34;weightedClusters\u0026#34;: { \u0026#34;clusters\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;outbound|80|v1|customers.default.svc.cluster.local\u0026#34;, \u0026#34;weight\u0026#34;: 80 }, { \u0026#34;name\u0026#34;: \u0026#34;outbound|80|v2|customers.default.svc.cluster.local\u0026#34;, \u0026#34;weight\u0026#34;: 20 } ] }, \u0026#34;timeout\u0026#34;: \u0026#34;5s\u0026#34;, ... 当你评估 VirtualServices 时，你要寻找主机名是否像你写的那样出现在 Envoy 配置中（例如customers.default.svc.cluster.local），以及路由是否存在（见输出中的 80-20 流量分割）。你也可以使用之前的例子，通过监听器、路由和集群（和端点）来追踪调用。\nEnvoy 过滤器会表现在你告诉 Istio 把它们放在哪里（EnvoyFilter 资源中的 applyTo 字段）。通常情况下，一个坏的过滤器会表现为 Envoy 拒绝配置（即不显示 SYNCED 状态）。在这种情况下，你需要检查 Istiod 日志中的错误。\n3. Istiod（Pilot）中是否有错误？\n从 Pilot 查看错误的最快方法是跟踪日志（使用 --follow 标志），然后应用配置。下面是一个来自 Pilot 的错误的例子，这是由于过滤器的内联代码中的一个错字而导致的。\n2020-11-20T21:49:16.017487Z warn ads ADS:LDS: ACK ERROR sidecar~10.120.1.8~web-frontend-58d497b6f8-lwqkg.default~default.svc.cluster.local-4 Internal:Error adding/updating listener (s) virtualInbound: script load error: [string\u0026#34;fction envoy_on_response (response_handle)...\u0026#34;]:1: \u0026#39;=\u0026#39; expected near \u0026#39;envoy_on_response\u0026#39; 如果配置根本没有出现在 Envoy 中（Envoy …","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"750af3c2897e0a1b91d412f4161ac558","permalink":"https://lib.jimmysong.io/istio-handbook/troubleshooting/checklist/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/istio-handbook/troubleshooting/checklist/","section":"istio-handbook","summary":"每当你遇到配置问题时，你可以使用这组步骤来浏览和解决问题。在第一部分，我们要检查配置是否有效。如果配置是有效的，下一步就是看看运行时是如何处理配置的，为此，你需要对 Envoy 配置有基本的了解。 配置 1. 配置是否有","tags":["Istio","Service Mesh"],"title":"调试清单","type":"book"},{"authors":null,"categories":["Envoy"],"content":"Envoy 中访问日志的另一个特点是可以指定过滤器，决定是否需要写入访问日志。例如，我们可以有一个访问日志过滤器，只记录 500 状态代码，只记录超过 5 秒的请求，等等。下表显示了支持的访问日志过滤器。\n   访问日志过滤器名称 描述     status_code_filter 对状态代码值进行过滤。   duration_filter 对总的请求持续时间进行过滤，单位为毫秒。   not_health_check_filter 对非健康检查请求的过滤。   traceable_filter 对可追踪的请求进行过滤。   runtime_filter 对请求进行随机抽样的过滤器。   and_filter 对过滤器列表中每个过滤器的结果进行逻辑 “和” 运算。过滤器是按顺序进行评估的。   or_filter 对过滤器列表中每个过滤器的结果进行逻辑 “或” 运算。过滤器是按顺序进行评估的。   header_filter 根据请求头的存在或值来过滤请求。   response_flag_filter 过滤那些收到设置了 Envoy 响应标志的响应的请求。   grpc_status_filter 根据响应状态过滤 gRPC 请求。   extension_filter 使用一个在运行时静态注册的扩展过滤器。   metadata_filter 基于匹配的动态元数据的过滤器。    每个过滤器都有不同的属性，我们可以选择设置。这里有一个片段，显示了如何使用状态代码、Header 和一个 and 过滤器。\n...access_log:- name:envoy.access_loggers.stdouttyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.access_loggers.stream.v3.StdoutAccessLogfilter:and_filter:filters:header_filter:header:name:\u0026#34;:method\u0026#34;string_match:exact:\u0026#34;GET\u0026#34;status_code_filter:comparison:op:GEvalue:default_value:400...上面的片段为所有响应代码大于或等于 400 的 GET 请求写了一条日志条目到标准输出。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"34c4b0c594cd263697496fe8cd67d0e0","permalink":"https://lib.jimmysong.io/envoy-handbook/logging/access-log-filtering/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/envoy-handbook/logging/access-log-filtering/","section":"envoy-handbook","summary":"Envoy 中访问日志的另一个特点是可以指定过滤器，决定是否需要写入访问日志。例如，我们可以有一个访问日志过滤器，只记录 500 状态代码，只记录超过 5 秒的请求，等等。下表显示了支持的访问日志过滤器。 访问日志过滤器名称","tags":["Envoy"],"title":"访问日志过滤","type":"book"},{"authors":null,"categories":["Istio"],"content":"我们可以使用 VirtualService 资源在 Istio 服务网格中进行流量路由。通过 VirtualService，我们可以定义流量路由规则，并在客户端试图连接到服务时应用这些规则。例如向 dev.example.com 发送一个请求，最终到达目标服务。\n让我们看一下在集群中运行 customers 应用程序的两个版本（v1 和 v2）的例子。我们有两个 Kubernetes 部署，customers-v1 和 customers-v2。属于这些部署的 Pod 有一个标签 version：v1 或一个标签 version：v2 的设置。\n   路由到 Customers  我们想把 VirtualService 配置为将流量路由到应用程序的 V1 版本。70% 的传入流量应该被路由到 V1 版本。30% 的请求应该被发送到应用程序的 V2 版本。\n下面是上述情况下 VirtualService 资源的样子：\napiVersion:networking.istio.io/v1alpha3kind:VirtualServicemetadata:name:customers-routespec:hosts:- customers.default.svc.cluster.localhttp:- name:customers-v1-routesroute:- destination:host:customers.default.svc.cluster.localsubset:v1weight:70- name:customers-v2-routesroute:- destination:host:customers.default.svc.cluster.localsubset:v2weight:30在 hosts 字段下，我们要定义流量被发送到的目标主机。在我们的例子中，这就是 customers.default.svc.cluster.local Kubernetes 服务。\n下一个字段是 http，这个字段包含一个 HTTP 流量的路由规则的有序列表。destination 是指服务注册表中的一个服务，也是路由规则处理后请求将被发送到的目的地。Istio 的服务注册表包含所有的 Kubernetes 服务，以及任何用 ServiceEntry 资源声明的服务。\n我们也在设置每个目的地的权重（weight）。权重等于发送到每个子集的流量的比例。所有权重的总和应该是 100。如果我们有一个单一的目的地，权重被假定为 100。\n通过 gateways 字段，我们还可以指定我们想要绑定这个 VirtualService 的网关名称。比如说：\napiVersion:networking.istio.io/v1alpha3kind:VirtualServicemetadata:name:customers-routespec:hosts:- customers.default.svc.cluster.localgateways:- my-gatewayhttp:...上面的 YAML 将 customers-route VirtualService 绑定到名为 my-gateway 的网关上。这有效地暴露了通过网关的目标路由。\n当一个 VirtualService 被附加到一个网关上时，只允许在网关资源中定义的主机。下表解释了网关资源中的 hosts 字段如何作为过滤器，以及 VirtualService 中的 hosts 字段如何作为匹配。\n   Gateway配置  ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"ab68fcafef2f0c1645ef453852b451d2","permalink":"https://lib.jimmysong.io/istio-handbook/traffic-management/basic-routing/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/istio-handbook/traffic-management/basic-routing/","section":"istio-handbook","summary":"我们可以使用 VirtualService 资源在 Istio 服务网格中进行流量路由。通过 VirtualService，我们可以定义流量路由规则，并在客户端试图连接到服务时应用这些规则。例如向 dev.example.com 发送一个请求，最终到达目标服务。 让我们看一下在","tags":["Istio","Service Mesh"],"title":"基本路由","type":"book"},{"authors":null,"categories":["Envoy"],"content":"使用控制平面来更新 Envoy 比使用文件系统的配置更复杂。我们必须创建自己的控制平面，实现发现服务接口。这里有一个 xDS 服务器实现的简单例子。这个例子显示了如何实现不同的发现服务，并运行 Envoy 连接的 gRPC 服务器的实例来检索配置。\n   Envoy 的动态配置  Envoy 方面的动态配置与文件系统的配置类似。这一次，不同的是，我们提供了实现发现服务的 gRPC 服务器的位置。我们通过静态资源指定一个集群来做到这一点。\n...dynamic_resources:lds_config:resource_api_version:V3api_config_source:api_type:GRPCtransport_api_version:V3grpc_services:- envoy_grpc:cluster_name:xds_clustercds_config:resource_api_version:V3api_config_source:api_type:GRPCtransport_api_version:V3grpc_services:- envoy_grpc:cluster_name:xds_clusterstatic_resources:clusters:- name:xds_clustertype:STATICload_assignment:cluster_name:xds_clusterendpoints:- lb_endpoints:- endpoint:address:socket_address:address:127.0.0.1port_value:9090控制平面不需要在 Envoy 概念上操作。它可以抽象出配置。它也可以使用图形用户界面或不同的 YAML、XML 或任何其他配置文件来收集用户的输入。重要的是，无论高级别配置是如何进入控制平面的，它都需要被翻译成 Envoy xDS API。\n例如，Istio 是 Envoy 代理机群的控制平面，可以通过各种自定义资源定义（VirtualService、Gateway、DestinationRule…）进行配置。除了上层配置外，在 Istio 中，Kubernetes 环境和集群内运行的服务也被用来作为生成 Envoy 配置的输入。上层配置和环境中发现的服务可以一起作为控制平面的输入。控制平面可以接受这些输入，将其转化为 Envoy 可读的配置，并通过 gRPC 将其发送给 Envoy 实例。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"023368edd59b4cf0548e11fbbd1ba66a","permalink":"https://lib.jimmysong.io/envoy-handbook/dynamic-config/control-plane/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/envoy-handbook/dynamic-config/control-plane/","section":"envoy-handbook","summary":"使用控制平面来更新 Envoy 比使用文件系统的配置更复杂。我们必须创建自己的控制平面，实现发现服务接口。这里有一个 xDS 服务器实现的简单例子。这个例子显示了如何实现不同的发现服务，并运行 Envoy 连接的 gRPC 服务器的实例来检索","tags":["Envoy"],"title":"来自控制平面的动态配置","type":"book"},{"authors":null,"categories":["Istio"],"content":"授权是对访问控制问题中访问控制部分的响应。一个（经过认证的）主体是否被允许对一个对象执行动作？用户 A 能否向服务 A 的路径 /hello 发送一个 GET 请求？\n请注意，尽管主体可以被认证，但它可能不被允许执行一个动作。你的公司 ID 卡可能是有效的、真实的，但我不能用它来进入另一家公司的办公室。如果我们继续之前的海关官员的比喻，我们可以说授权类似于你护照上的签证章。\n这就引出了下一个问题 —— 有认证而无授权（反之亦然）对我们没有什么好处。对于适当的访问控制，我们需要两者。让我给你举个例子：如果我们只认证主体而不授权他们，他们就可以做任何他们想做的事，对任何对象执行任何操作。相反，如果我们授权了一个请求，但我们没有认证它，我们就可以假装成其他人，再次对任何对象执行任何操作。\nIstio 允许我们使用 AuthorizationPolicy 资源在网格、命名空间和工作负载层面定义访问控制。AuthorizationPolicy 支持 DENY、ALLOW、AUDIT 和 CUSTOM 操作。\n每个 Envoy 代理实例都运行一个授权引擎，在运行时对请求进行授权。当请求到达代理时，引擎会根据授权策略评估请求的上下文，并返回 ALLOW 或 DENY。AUDIT 动作决定是否记录符合规则的请求。注意，AUDIT 策略并不影响请求被允许或拒绝。\n没有必要明确地启用授权功能。为了执行访问控制，我们可以创建一个授权策略来应用于我们的工作负载。\nAuthorizationPolicy 资源是我们可以利用 PeerAuthentication 策略和 RequestAuthentication 策略中的主体的地方。\n在定义 AuthorizationPolicy 的时候，我们需要考虑三个部分。\n 选择要应用该策略的工作负载 要采取的行动（拒绝、允许或审计） 采取该行动的规则  让我们看看下面这个例子如何与 AuthorizationPolicy 资源中的字段相对应。\napiVersion:security.istio.io/v1beta1kind:AuthorizationPolicymetadata:name:customers-denynamespace:defaultspec:selector:matchLabels:app:customersversion:v2action:DENYrules:- from:- source:notNamespaces:[\u0026#34;default\u0026#34;]使用selector和matchLabels，我们可以选择策略所适用的工作负载。在我们的案例中，我们选择的是所有设置了app: customers和version: v2标签的工作负载。action 字段被设置为DENY。\n最后，我们在规则栏中定义所有规则。我们例子中的规则是说，当请求来自默认命名空间之外时，拒绝对 customers v2 工作负载的请求（action）。\n除了规则中的 from 字段外，我们还可以使用 to 和 when 字段进一步定制规则。让我们看一个使用这些字段的例子。\napiVersion:security.istio.io/v1beta1kind:AuthorizationPolicymetadata:name:customers-denynamespace:defaultspec:selector:matchLabels:app:customersversion:v2action:DENYrules:- from:- source:notNamespaces:[\u0026#34;default\u0026#34;]- to:- operation:methods:[\u0026#34;GET\u0026#34;]- when:- key:request.headers [User-Agent]values:[\u0026#34;Mozilla/*\u0026#34;]我们在规则部分添加了to和when字段。如果我们翻译一下上面的规则，我们可以说，当客户的 GET 请求来自default命名空间之外，并且User Agent头的值与正则表达式Mozilla/* 相匹配时，我们会拒绝 customer v2 的工作负载。\n总的来说，to 定义了策略所允许的行动，from 定义了谁可以采取这些行动，when 定义了每个请求必须具备的属性，以便被策略所允许，selector 定义了哪些工作负载将执行该策略。\n如果一个工作负载有多个策略，则首先评估拒绝的策略。评估遵循这些规则：\n 如果有与请求相匹配的 DENY 策略，则拒绝该请求 如果没有适合该工作负载的 ALLOW 策略，则允许该请求。 如果有任何 ALLOW 策略与该请求相匹配，则允许该请求。 拒绝该请求  来源 我们在上述例子中使用的源是 notNamespaces。我们还可以使用以下任何一个字段来指定请求的来源，如表中所示。\n   来源 示例 释义     principals principals: [\u0026#34;my-service-account\u0026#34;] 任何是有 my-service-account 的工作负载   notPrincipals notPrincipals: [\u0026#34;my-service-account\u0026#34;] 除了 my-service-account 的任何工作负载   requestPrincipals requestPrincipals: [\u0026#34;my-issuer/hello\u0026#34;] 任何具有有效 JWT 和请求主体 my-issuer/hello 的工作负载   notRequestPrincipals notRequestPrincipals: [\u0026#34;*\u0026#34;] 任何没有请求主体的工作负载（只有有效的 JWT 令牌）。   namespaces namespaces: [\u0026#34;default\u0026#34;] 任何来自 default 命名空间的工作负载   notNamespaces notNamespaces: [\u0026#34;prod\u0026#34;] 任何不在 prod 命名空间的工作负载   ipBlocks ipBlocks: [\u0026#34;1.2.3.4\u0026#34;,\u0026#34;9.8.7.6/15\u0026#34;] 任何具有 1.2.3.4 的 IP 地址或来自 CIDR 块的 IP 地址的工作负载   notIpBlock ipBlocks: [\u0026#34;1.2.3.4/24\u0026#34;] Any IP address that’s outside of the CIDR block    操作 操作被定义在 to 字段下，如果多于一个，则使用 AND 语义。就像来源一样，操作是成对的，有正反两面的匹配。设置在操作字段的值是字符串：\n hosts 和 notHosts ports 和 notPorts methods 和 notMethods paths 和 notPath  所有这些操作都适用于请求属性。例如，要在一个特定的请求路径上进行匹配，我们可以使用路径。[\u0026#34;/api/*\u0026#34;,\u0026#34;/admin\u0026#34;] 或特定的端口 ports: [\u0026#34;8080\u0026#34;]，以此类推。\n条件 为了指定条件，我们必须提供一个 key 字段。key 字段是一个 Istio 属性的名称。例如，request.headers、source.ip、destination.port 等等。关于支持的属性的完整列表，请参考 授权政策条件。\n条件的第二部分是 values 或 notValues 的字符串列表。下面是一个 when 条件的片段：\n...- when:- key:source.ipnotValues:[\u0026#34;10.0.1.1\u0026#34;] 下一章   ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"59a8a9c10b0ec83d8c20bdc4ef83c3aa","permalink":"https://lib.jimmysong.io/istio-handbook/security/authz/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/istio-handbook/security/authz/","section":"istio-handbook","summary":"授权是对访问控制问题中访问控制部分的响应。一个（经过认证的）主体是否被允许对一个对象执行动作？用户 A 能否向服务 A 的路径 /hello 发送一个 GET 请求？ 请注意，尽管主体可以被认证，但它可能不被允许执行一个动作。你的公","tags":["Istio","Service Mesh"],"title":"授权","type":"book"},{"authors":null,"categories":["Envoy"],"content":"管理接口的统计输出的主要端点是通过 /stats 端点访问的。这个输入通常是用来调试的。我们可以通过向 /stats 端点发送请求或从管理接口访问同一路径来访问该端点。\n该端点支持使用 filter 查询参数和正则表达式来过滤返回的统计资料。\n另一个过滤输出的维度是使用 usedonly 查询参数。当使用时，它将只输出 Envoy 更新过的统计数据。例如，至少增加过一次的计数器，至少改变过一次的仪表，以及至少增加过一次的直方图。\n默认情况下，统计信息是以 StatsD 格式写入的。每条统计信息都写在单独的一行中，统计信息的名称（例如，cluster_manager.active_clusters）后面是统计信息的值（例如，15）。\n例如：\n... cluster_manager.active_clusters。15 cluster_manager.cluster_added: 3 cluster_manager.cluster_modified:4 ... format 查询参数控制输出格式。设置为 json 将以 JSON 格式输出统计信息。如果我们想以编程方式访问和解析统计信息，通常会使用这种格式。\n第二种格式是 Prometheus 格式（例如， format=prometheus）。这个选项以 Prometheus 格式格式化状态，可以用来与 Prometheus 服务器集成。另外，我们也可以使用 /stats/prometheus 端点来获得同样的输出。\n内存 /memory 端点将输出当前内存分配和堆的使用情况，单位为字节。下面是 /stats 端点打印出来的信息的一个子集。\n$ curl localhost:9901/memory { \u0026#34;allocated\u0026#34;: \u0026#34;5845672\u0026#34;, \u0026#34;heap_size\u0026#34;: \u0026#34;10485760\u0026#34;, \u0026#34;pageheap_unmapped\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;pageheap_free\u0026#34;: \u0026#34;3186688\u0026#34;, \u0026#34;total_thread_cache\u0026#34;: \u0026#34;80064\u0026#34;, \u0026#34;total_physical_bytes\u0026#34;: \u0026#34;12699350\u0026#34; } 重置计数器 向 /reset_counters 发送一个 POST 请求，将所有计数器重置为零。注意，这不会重置或放弃任何发送到 statsd 的数据。它只影响到 /stats  端点的输出。在调试过程中可以使用 /stats 端点和 /reset_counters  端点。\n服务器信息和状态 /server_info 端点输出运行中的 Envoy 服务器的信息。这包括版本、状态、配置路径、日志级别信息、正常运行时间、节点信息等。\n该 admin.v3.ServerInfo proto 解释了由端点返回的不同字段。\n/ready 端点返回一个字符串和一个错误代码，反映 Envoy 的状态。如果 Envoy 是活的，并准备好接受连接，那么它返回 HTTP 200 和字符串 LIVE。否则，输出将是一个 HTTP 503。这个端点可以作为准备就绪检查。\n/runtime 端点以 JSON 格式输出所有运行时值。输出包括活动的运行时覆盖层列表和每个键的层值堆栈。这些值也可以通过向 /runtime_modify 端点发送 POST 请求并指定键 / 值对来修改。例如，POST /runtime_modify?my_key_1=somevalue。\n/hot_restart_version 端点，加上 --hot-restart-version  标志，可以用来确定新的二进制文件和运行中的二进制文件是否热重启兼容。\n热重启是指 Envoy 能够 “热” 或 \u0026#34; 实时 \u0026#34; 重启自己。这意味着 Envoy 可以完全重新加载自己（和配置）而不放弃任何现有的连接。\nHystrix 事件流 /hystrix_event_stream 端点的目的是作为流源用于 Hystrix 仪表盘。向该端点发送请求将触发来自 Envoy 的统计流，其格式是 Hystrix 仪表盘所期望的。\n注意，我们必须在引导配置中配置 Hystrix 统计同步，以使端点工作。\n例如：\nstats_sinks:- name:envoy.stat_sinks.hystrixtyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.config.metrics.v3.HystrixSinknum_buckets:10争用 如果启用了互斥追踪功能，/contention 端点会转储当前 Envoy 互斥内容的统计信息。\nCPU 和堆分析器 我们可以使用 /cpuprofiler 和 /heapprofiler 端点来启用或禁用 CPU / 堆分析器。注意，这需要用 gperftools 编译 Envoy。Envoy 的 GitHub 资源库有文档说明。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"a4a2a6769ada9a0f0473910ee80c65c2","permalink":"https://lib.jimmysong.io/envoy-handbook/admin-interface/statistics/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/envoy-handbook/admin-interface/statistics/","section":"envoy-handbook","summary":"管理接口的统计输出的主要端点是通过 /stats 端点访问的。这个输入通常是用来调试的。我们可以通过向 /stats 端点发送请求或从管理接口访问同一路径来访问该端点。 该端点支持使用 filter 查询参数和正则表达式来过滤返回的统计资料。 另","tags":["Envoy"],"title":"统计","type":"book"},{"authors":null,"categories":["Istio"],"content":"要想说明为什么要使用服务网格，那就要从微服务架构说起，可以说服务网格很大程度上是一种新一代的微服务架构，它解决了微服务中网络层操控性、弹性、可视性的问题。\n微服务架构 开发人员经常将云原生应用程序分解为多个执行特定动作的服务。你可能有一个只处理客户的服务和另一个处理订单或付款的服务。所有这些服务都通过网络相互沟通。如果一个新的付款需要被处理，请求会被发送到付款服务。如果客户数据需要更新，请求会被发送到客户服务，等等。\n   微服务架构  这种类型的架构被称为微服务架构。这种架构有几个好处。你可以有多个较小的团队从事个别服务。这些团队可以灵活地选择他们的技术栈和语言，并且通常有独立部署和发布服务的自主权。这种机制得以运作得益于在其背后通信的网络。随着服务数量的增加，它们之间的通信和网络通信也在增加。服务和团队的数量使得监控和管理通信逻辑变得相当复杂。由于我们也知道网络是不可靠的，它们会失败，所有这些的结合使得微服务的管理和监控相当复杂。\n服务网格概述 服务网格被定义为一个专门的基础设施层，用于管理服务与服务之间的通信，使其可管理、可见、可控制。在某些版本的定义中，你可能还会听到服务网格如何使服务间的通信安全和可靠。如果我必须用一个更直接的句子来描述服务网格，我会说，服务网格是关于服务之间的通信。\n但是，服务网格是如何帮助通信的呢？让我们思考一下通信逻辑和它通常所在的地方。在大多数情况下，开发人员将这种逻辑作为服务的一部分来构建。通信逻辑是处理入站或出站请求的任何代码，重试逻辑，超时，甚至可能是流量路由。因此，无论何时服务 A 调用服务 B，请求都要经过这个通信代码逻辑，这个逻辑决定如何处理这个请求。\n   通信逻辑  我们提到，如果我们采用微服务的方法，最终可能会有大量的服务。我们如何处理所有这些服务的通信逻辑呢？我们可以创建一个包含这种逻辑的共享库，并在多个地方重用它。假设我们对所有的服务都使用相同的堆栈或编程语言，共享库的方法可能会很有效。如果我们不这样做，我们将不得不重新实现这个库，这会带来巨大的工作量而且效率低下。你也可能使用自己本身不拥有代码库的服务。在这种情况下，我们无法控制通信逻辑或监控。\n第二个问题是配置。除了配置你的应用程序外，我们还必须维护通信逻辑配置。如果我们需要同时调整或更新多个服务，我们将不得不为每个服务单独进行调整。\n服务网格所做的是，它将这种通信逻辑、重试、超时等从单个服务中分离出来，并将其移到一个单独的基础设施层。在服务网格的情况下，基础设施层是一个网络代理的阵列。这些网络代理的集合（每个服务实例旁边都有一个）处理你的服务之间的所有通信逻辑。我们称这些代理为 sideecar，因为它们与每个服务并存。\n   Sidecar 代理  以前，我们让 Customer 服务直接与 Payment 服务通信，现在我们有一个 Customer 服务旁边的代理与 Payment 服务旁边的代理通信。服务网格控制平面以这样一种方式配置代理，即它们透明地拦截所有入站和出站请求。这些代理的集合（基础设施层）形成了一个网络网格，称为服务网格。\n将通信逻辑从业务和应用逻辑中分离出来，可以使开发人员专注于业务逻辑，而服务网格运维人员则专注于服务网格配置。\n服务网格的功能 服务网格为我们提供了一种一致的方式来连接、保护和观察微服务。网格内的代理捕获了网格内所有通信的请求和指标。每一次失败、每一次成功的调用、重试或超时都可以被捕获、可视化，并发出警报。此外，可以根据请求属性做出决定。例如，我们可以检查入站（或出站）请求并编写规则，将所有具有特定头值的请求路由到不同的服务版本。\n所有这些信息和收集到的指标使得一些场景可以合理地直接实现。开发人员和运营商可以配置和执行以下方案，而不需要对服务进行任何代码修改。\n mTLS 和自动证书轮换 使用指标识别性能和可靠性问题 在 Grafana 等工具中实现指标的可视化 使用 Jaeger 或 Zipkin（需要对代码进行小的修改，以便在服务之间传播跟踪头信息） 对服务进行调试和追踪 基于权重和请求的流量路由，金丝雀部署，A/B 测试 流量镜像 通过超时和重试提高服务的弹性 通过在服务之间注入故障和延迟来进行混沌测试 检测和弹出不健康的服务实例的断路器  ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"ccfbc4878d965aad540648c9645bf9af","permalink":"https://lib.jimmysong.io/istio-handbook/intro/why-service-mesh/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/istio-handbook/intro/why-service-mesh/","section":"istio-handbook","summary":"要想说明为什么要使用服务网格，那就要从微服务架构说起，可以说服务网格很大程度上是一种新一代的微服务架构，它解决了微服务中网络层操控性、弹性、可视性的问题。 微服务架构 开发人员经常将云原生应用程序分解为多","tags":["Istio","Service Mesh"],"title":"为什么要使用服务网格？","type":"book"},{"authors":null,"categories":["Envoy"],"content":"第二种类型的健康检查被称为被动健康检查。异常点检测（outlier detection） 是一种被动的健康检查形式。说它 “被动” 是因为 Envoy 没有 “主动” 发送任何请求来确定端点的健康状况。相反，Envoy 观察不同端点的性能，以确定它们是否健康。如果端点被认为是不健康的，它们就会被移除或从健康负载均衡池中弹出。\n端点的性能是通过连续失败、时间成功率、延迟等来确定的。\n为了使异常点检测发挥作用，我们需要过滤器来报告错误、超时和重置。目前，有四个过滤器支持异常点检测。HTTP 路由器、TCP 代理、Redis 代理和 Thrift 代理。\n检测到的错误根据起源点分为两类。\n1. 来自外部的错误\n这些错误是针对事务的，发生在上游服务器上，是对收到的请求的回应。这些错误是在 Envoy 成功连接到上游主机后产生的。例如，端点响应的是 HTTP 500。\n2. 本地产生的错误\nEnvoy 产生这些错误是为了应对中断或阻止与上游主机通信的事件，例如超时、TCP 重置、无法连接到指定端口等。\n这些错误也取决于过滤器的类型。例如，HTTP 路由器过滤器可以检测两种错误。相反，TCP 代理过滤器不理解 TCP 层以上的任何协议，只报告本地产生的错误。\n在配置中，我们可以指定是否可以区分本地和外部产生的错误（使用 split_external_local_origin_errors 字段）。这允许我们通过单独的计数器跟踪错误，并配置异常点检测，对本地产生的错误做出反应，而忽略外部产生的错误，反之亦然。默认模式错误将不被分割（即 split_external_local_origin_errors 为 false）。\n端点弹出 当一个端点被确定为异常点时，Envoy 将检查它是否需要从健康负载均衡池中弹出。如果没有端点被弹出，Envoy 会立即弹出异常（不健康的）端点。否则，它会检查 max_ejection_percent 设置，确保被弹出的端点数量低于配置的阈值。如果超过 max_ejection_percent 的主机已经被弹出，该端点就不会被弹出了。\n每个端点被弹出的时间是预先确定的。我们可以使用 base_ejection_time 值来配置弹出时间。这个值要乘以端点连续被弹出的次数。如果端点继续失败，它们被弹出的时间会越来越长。这里的第二个设置叫做 max_ejection_time 。它控制端点被弹出的最长时间——也就是说，端点被弹出的最长时间在 max_ejection_time 值中被指定。\nEnvoy 在 internal 字段中指定的间隔时间内检查每个端点的健康状况。每检查一次端点是否健康，弹出的倍数就会被递减。经历弹出时间后，端点会自动返回到健康的负载均衡池中。\n现在我们了解了异常点检测和端点弹出的基本知识，让我们看看不同的异常点检测方法。\n检测类型 Envoy 支持以下五种异常点检测类型。\n1. 连续的 5xx\n这种检测类型考虑到了所有产生的错误。Envoy 内部将非 HTTP 过滤器产生的任何错误映射为 HTTP 5xx 代码。\n当错误类型被分割时，该检测类型只计算外部产生的错误，忽略本地产生的错误。如果端点是一个 HTTP 服务器，只考虑 5xx 类型的错误。\n如果一个端点返回一定数量的 5xx 错误，该端点会被弹出。consecutive_5xx 值控制连续 5xx 错误的数量。\nclusters:- name:my_cluster_nameoutlier_detection:interval:5sbase_ejection_time:15smax_ejection_time:50smax_ejection_percent:30consecutive_5xx:10...上述异常点检测，一旦它失败 10 次，将弹出一个失败的端点。失败的端点会被弹出 15 秒（base_ejection_time）。在多次弹出的情况下，单个端点被弹出的最长时间是 50 秒（max_ejection_time）。在一个失败的端点被弹出之前，Envoy 会检查是否有超过 30% 的端点已经被弹出（max_ejection_percent），并决定是否弹出这个失败的端点。\n2. 连续的网关故障\n连续网关故障类型与连续 5xx 类型类似。它将 5xx 错误的一个子集，称为 “网关错误”（如 502、503 或 504 状态代码）和本地源故障，如超时、TCP 复位等。\n这种检测类型考虑了分隔模式下的网关错误，并且只由 HTTP 过滤器支持。连续错误的数量可通过 contriable_gateway_failure 字段进行配置。\nclusters:- name:my_cluster_nameoutlier_detection:interval:5sbase_ejection_time:15smax_ejection_time:50smax_ejection_percent:30consecutive_gateway_failure:10...3. 连续的本地源失败\n这种类型只在分隔模式下启用（split_external_local_origin_errors 为 true），它只考虑本地产生的错误。连续失败的数量可以通过 contriable_local_origin_failure 字段进行配置。如果未提供，默认为 5。\nclusters:- name:my_cluster_nameoutlier_detection:interval:5sbase_ejection_time:15smax_ejection_time:50smax_ejection_percent:30consecutive_local_origin_failure:10...4. 成功率\n成功率异常点检测汇总了集群中每个端点的成功率数据。基于成功率，它将在给定的时间间隔内弹出端点。在默认模式下，所有的错误都被考虑，而在分隔模式下，外部和本地产生的错误被分别处理。\n通过 success_rate_request_volume 值，我们可以设置最小请求量。如果请求量小于该字段中指定的请求量，将不计算该主机的成功率。同样地，我们可以使用 success_rate_minimum_hosts 来设置具有最小要求的请求量的端点数量。如果具有最小要求的请求量的端点数量少于 success_rate_minimum_hosts 中设置的值，Envoy 将不会进行异常点检测。\nsuccess_rate_stdev_factor 用于确定弹出阈值。弹出阈值是平均成功率和该系数与平均成功率标准差的乘积之间的差。\n平均值 - (stdev * success_rate_stdev_factor) 这个系数被除以一千，得到一个双数。也就是说，如果想要的系数是 1.9，那么运行时间值应该是 1900。\n5. 故障率\n故障率异常点检测与成功率类似。不同的是，它不依赖于整个集群的平均成功率。相反，它将该值与用户在 failure_percentage_threshold  字段中配置的阈值进行比较。如果某个主机的故障率大于或等于这个值，该主机就会被弹出。\n可以使用 failure_percentage_minimum_hosts 和 failure_percentage_request_volume 配置最小主机和请求量。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"099da0cc49c653e3166ed492bb0c4ee4","permalink":"https://lib.jimmysong.io/envoy-handbook/cluster/outlier-detection/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/envoy-handbook/cluster/outlier-detection/","section":"envoy-handbook","summary":"第二种类型的健康检查被称为被动健康检查。异常点检测（outlier detection） 是一种被动的健康检查形式。说它 “被动” 是因为 Envoy 没有 “主动” 发送任何","tags":["Envoy"],"title":"异常点检测","type":"book"},{"authors":null,"categories":["Istio"],"content":"如果你听说过服务网格，并尝试过 Istio，你可能有以下问题。\n 为什么 Istio 要在 Kubernetes 上运行？ Kubernetes 和服务网格在云原生应用架构中分别扮演什么角色？ Istio 扩展了 Kubernetes 的哪些方面？它解决了哪些问题？ Kubernetes、Envoy 和 Istio 之间是什么关系？  本文将带大家了解 Kubernetes 和 Istio 的内部工作原理。此外，我会介绍 Kubernetes 中的负载均衡方法，并解释为什么有了 Kubernetes 后还需要 Istio。\nKubernetes 本质上是通过声明式配置来实现应用生命周期管理，而服务网格本质上是提供应用间的流量、安全管理和可观测性。如果你已经使用 Kubernetes 搭建了一个稳定的应用平台，那么如何设置服务间调用的负载均衡和流量控制？是否有这样一个通用的工具或者说平台（非 SDK），可以实现？这就需要用到服务网格了。\nEnvoy 引入了 xDS 协议，这个协议得到了各种开源软件的支持，比如 Istio、MOSN 等。Envoy 将 xDS 贡献给服务网格或云原生基础设施。Envoy 本质上是一个现代版的代理，可以通过 API 进行配置，在此基础上衍生出许多不同的使用场景，比如 API Gateway、服务网格中的 sidecar 代理和边缘代理。\n本文包含以下内容。\n kube-proxy 的作用描述。 Kubernetes 在微服务管理方面的局限性。 Istio 服务网格的功能介绍。 Kubernetes、Envoy 和 Istio 服务网格中一些概念的比较。  Kubernetes vs Service Mesh 下图展示的是 Kubernetes 与 Service Mesh 中的的服务访问关系，本文仅针对 sidecar per-pod 模式，详情请参考服务网格的实现模式。\n   kubernetes vs service mesh  流量转发 Kubernetes 集群中的每个节点都部署了一个 kube-proxy 组件，该组件与 Kubernetes API Server 进行通信，获取集群中的服务信息，然后设置 iptables 规则，将服务请求直接发送到对应的 Endpoint（属于同一组服务的 pod）。\n服务发现    服务发现  Istio 可以跟踪 Kubernetes 中的服务注册，也可以在控制平面中通过平台适配器与其他服务发现系统对接；然后生成数据平面的配置（使用 CRD，这些配置存储在 etcd 中），数据平面的透明代理。数据平面的透明代理以 sidecar 容器的形式部署在每个应用服务的 pod 中，这些代理都需要请求控制平面同步代理配置。代理之所以 “透明”，是因为应用容器完全不知道代理的存在。过程中的 kube-proxy 组件也需要拦截流量，只不过 kube-proxy 拦截的是进出 Kubernetes 节点的流量，而 sidecar 代理拦截的是进出 pod 的流量。\n服务网格的劣势 由于 Kubernetes 的每个节点上都运行着很多 pod，所以在每个 pod 中放入原有的 kube-proxy 路由转发功能，会增加响应延迟——由于 sidecar 拦截流量时跳数更多，消耗更多的资源。为了对流量进行精细化管理，将增加一系列新的抽象功能。这将进一步增加用户的学习成本，但随着技术的普及，这种情况会慢慢得到缓解。\n服务网格的优势 kube-proxy 的设置是全局的，无法对每个服务进行细粒度的控制，而 service mesh 通过 sidecar proxy 的方式将 Kubernetes 中的流量控制从服务层中抽离出来–可以实现更大的弹性。\nKube-proxy 的不足之处 首先，如果转发的 pod 不能正常服务，它不会自动尝试其他 pod。每个 pod 都有一个健康检查机制，当一个 pod 出现健康问题时，kubelet 会重启 pod，kube-proxy 会删除相应的转发规则。另外，节点 Port 类型的服务不能添加 TLS 或更复杂的消息路由机制。\nKube-proxy 实现了一个 Kubernetes 服务的多个 pod 实例之间的流量负载均衡，但如何对这些服务之间的流量进行精细化控制–比如将流量按百分比划分给不同的应用版本（这些应用版本都是同一个服务的一部分，但在不同的部署上），或者做金丝雀发布（灰度发布）和蓝绿发布？\nKubernetes 社区给出了一个使用 Deployment 做金丝雀发布的方法，本质上是通过修改 pod 的标签来给部署的服务分配不同的 pod。\nKubernetes Ingress vs Istio Gateway 如上所述，kube-proxy 只能在 Kubernetes 集群内路由流量。Kubernetes 集群的 pods 位于 CNI 创建的网络中。一个 ingress—— 一个在 Kubernetes 中创建的资源对象 - 被创建用于集群外部的通信。它由位于 Kubernetes 边缘节点上的入口控制器驱动，负责管理南北向流量。Ingress 必须与各种 Ingress 控制器对接，比如 nginx ingress 控制器和 traefik。Ingress 只适用于 HTTP 流量，使用简单。它只能通过匹配有限的字段来路由流量–如服务、端口、HTTP 路径等。这使得它无法对 TCP 流量进行路由，如 MySQL、Redis 和各种 RPC。这就是为什么你会看到人们在 ingress 资源注释中写 nginx 配置语言的原因。直接路由南北流量的唯一方法是使用服务的 LoadBalancer 或 NodePort，前者需要云厂商支持，后者需要额外的端口管理。\nIstio Gateway 的功能与 Kubernetes Ingress 类似，它负责进出集群的南北流量。Istio Gateway 描述了一个负载平衡器，用于承载进出网状结构边缘的连接。该规范描述了一组开放端口和这些端口所使用的协议，以及用于负载均衡的 SNI 配置等。Gateway 是一个 CRD 扩展，它也重用了 sidecar 代理的功能；详细配置请参见 Istio 网站。\nEnvoy Envoy 是 Istio 中默认的 sidecar 代理。Istio 基于 Envoy 的 xDS 协议扩展了其控制平面。在讨论 Envoy 的 xDS 协议之前，我们需要先熟悉 Envoy 的基本术语。以下是 Envoy 中的基本术语及其数据结构的列表，更多细节请参考 Envoy 文档。\n   Envoy proxy 架构图  基本术语 下面是您应该了解的 Envoy 里的基本术语：\n Downstream（下游）：下游主机连接到 Envoy，发送请求并接收响应，即发送请求的主机。 Upstream（上游）：上游主机接收来自 Envoy 的连接和请求，并返回响应，即接受请求的主机。 Listener（监听器）：监听器是命名网地址（例如，端口、unix domain socket 等)，下游客户端可以连接这些监听器。Envoy 暴露一个或者多个监听器给下游主机连接。 Cluster（集群）：集群是指 Envoy 连接的一组逻辑相同的上游主机。Envoy 通过服务发现来发现集群的成员。可以选择通过主动健康检查来确定集群成员的健康状态。Envoy 通过负载均衡策略决定将请求路由到集群的哪个成员。  在 Envoy 中可以设置多个监听器，每个监听器可以设置一个过滤链（过滤链表），而且过滤链是可扩展的，这样我们可以更方便地操纵流量的行为–比如设置加密、私有 RPC 等。\nxDS 协议是由 Envoy 提出的，是 Istio 中默认的 sidecar 代理，但只要实现了 xDS 协议，理论上也可以作为 Istio 中的 sidecar 代理 —— 比如蚂蚁集团开源的 MOSN。\n   Envoy proxy 架构图  Istio 是一个功能非常丰富的服务网格，包括以下功能。\n 流量管理。这是 Istio 最基本的功能。 策略控制。实现访问控制系统、遥测采集、配额管理、计费等功能。 可观测性。在 sidecar 代理中实现。 安全认证。由 Citadel 组件进行密钥和证书管理。  Istio 中的流量管理 Istio 中定义了以下 CRD 来帮助用户进行流量管理。\n 网关。网关描述了一个运行在网络边缘的负载均衡器，用于接收传入或传出的 HTTP/TCP 连接。 虚拟服务（VirtualService）。VirtualService 实际上是将 Kubernetes 服务连接到 Istio 网关。它还可以执行额外的操作，例如定义一组流量路由规则，以便在主机寻址时应用。 DestinationRule。DestinationRule 定义的策略决定了流量被路由后的访问策略。简单来说，它定义了流量的路由方式。其中，这些策略可以定义为负载均衡配置、连接池大小和外部检测（用于识别和驱逐负载均衡池中不健康的主机）配置。 EnvoyFilter。EnvoyFilter 对象描述了代理服务的过滤器，可以自定义 Istio Pilot 生成的代理配置。这种配置一般很少被主用户使用。 ServiceEntry。默认情况下，Istio 服务 Mesh 中的服务无法发现 Mesh 之外的服务。ServiceEntry 可以在 Istio 内部的服务注册表中添加额外的条目，从而允许 Mesh 中自动发现的服务访问并路由到这些手动添加的服务。  Kubernetes vs xDS vs Istio 在回顾了 Kubernetes 的 kube-proxy 组件、xDS 和 Istio 对流量管理的抽象后，现在我们仅从流量管理的角度来看看这三个组件 / 协议的比较（注意，三者并不完全等同）。\n   Kubernetes xDS Istio 服务网格     Endpoint Endpoint WorkloadEntry   Service Route VirtualService   kube-proxy Route DestinationRule   kube-proxy Listener EnvoyFilter   Ingress Listener Gateway   Service Cluster ServiceEntry    核心观点  Kubernetes 的本质是应用生命周期管理，具体来说就是部署和管理（伸缩、自动恢复、发布）。 Kubernetes 为微服务提供了一个可扩展、高弹性的部署和管理平台。 服务网格是基于透明代理，通过 sidecar 代理拦截服务之间的流量，然后通过控制平面配置管理它们的行为。 服务网格将流量管理与 Kubernetes 解耦，不需要 kube-proxy 组件来支持服务网格内的流量；通过提供更接近微服务应用层的抽象来管理服务间的流量、安全性和可观测性。 xDS 是服务网格的协议标准之一。 服务网格是 Kubernetes 中服务的一个更高层次的抽象。  总结 如果说 Kubernetes 管理的对象是一个 pod，那么服务网格管理的对象就是一个服务，所以用 Kubernetes 管理微服务，然后应用服务网格就可以了。如果你连服务都不想管理，那就用 Knative 这样的无服务器平台，不过这是后话。\n 下一章   ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"e4380c9314156fe4e15905e62f8ceefa","permalink":"https://lib.jimmysong.io/istio-handbook/intro/cloud-native-application-network/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/istio-handbook/intro/cloud-native-application-network/","section":"istio-handbook","summary":"如果你听说过服务网格，并尝试过 Istio，你可能有以下问题。 为什么 Istio 要在 Kubernetes 上运行？ Kubernetes 和服务网格在云原生应用架构中分别扮演什么角色？ Istio 扩展了 Kubernetes 的哪些方面？它解决了哪些问题？ Kubernetes、Envo","tags":["Istio","Service Mesh"],"title":"云原生应用网络","type":"book"},{"authors":null,"categories":["云原生"],"content":"将应用程序的功能划分为单独的进程运行在同一个最小调度单元中（例如 Kubernetes 中的 Pod）可以被视为 sidecar 模式。如下图所示，sidecar 模式允许您在应用程序旁边添加更多功能，而无需额外第三方组件配置或修改应用程序代码。\n   Sidecar 模式示意图  就像连接了 Sidecar 的三轮摩托车一样，在软件架构中， Sidecar 连接到父应用并且为其添加扩展或者增强功能。Sidecar 应用与主应用程序松散耦合。它可以屏蔽不同编程语言的差异，统一实现微服务的可观测性、监控、日志记录、配置、断路器等功能。\n使用 Sidecar 模式的优势 使用 sidecar 模式部署服务网格时，无需在节点上运行代理，但是集群中将运行多个相同的 sidecar 副本。在 sidecar 部署方式中，每个应用的容器旁都会部署一个伴生容器，这个容器称之为 sidecar 容器。Sidecar 接管进出应用容器的所有流量。在 Kubernetes 的 Pod 中，在原有的应用容器旁边注入一个 Sidecar 容器，两个容器共享存储、网络等资源，可以广义的将这个包含了 sidecar 容器的 Pod 理解为一台主机，两个容器共享主机资源。\n因其独特的部署结构，使得 sidecar 模式具有以下优势：\n 将与应用业务逻辑无关的功能抽象到共同基础设施，降低了微服务代码的复杂度。 因为不再需要编写相同的第三方组件配置文件和代码，所以能够降低微服务架构中的代码重复度。 Sidecar 可独立升级，降低应用程序代码和底层平台的耦合度。  ","date":1651532400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"e27a22a2728b8eaf45084120a82ae478","permalink":"https://lib.jimmysong.io/cloud-native-handbook/kubernetes/sidecar-pattern/","publishdate":"2022-05-03T00:00:00+01:00","relpermalink":"/cloud-native-handbook/kubernetes/sidecar-pattern/","section":"cloud-native-handbook","summary":"将应用程序的功能划分为单独的进程运行在同一个最小调度单元中（例如 Kubernetes 中的 Pod）可以被视为 sidecar 模式。如下图所示，sidecar 模式允许您在应用程序旁边添加更多功能，而无需额外第三方组件配置或修改应用程序代","tags":["云原生"],"title":"Sidecar 模式","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"本文将为您介绍 DaemonSet 的基本概念。\n什么是 DaemonSet？ DaemonSet 确保全部（或者一些）Node 上运行一个 Pod 的副本。当有 Node 加入集群时，也会为他们新增一个 Pod 。当有 Node 从集群移除时，这些 Pod 也会被回收。删除 DaemonSet 将会删除它创建的所有 Pod。\n使用 DaemonSet 的一些典型用法：\n 运行集群存储 daemon，例如在每个 Node 上运行 glusterd、ceph。 在每个 Node 上运行日志收集 daemon，例如fluentd、logstash。 在每个 Node 上运行监控 daemon，例如 Prometheus Node Exporter、collectd、Datadog 代理、New Relic 代理，或 Ganglia gmond。  一个简单的用法是，在所有的 Node 上都存在一个 DaemonSet，将被作为每种类型的 daemon 使用。 一个稍微复杂的用法可能是，对单独的每种类型的 daemon 使用多个 DaemonSet，但具有不同的标志，和/或对不同硬件类型具有不同的内存、CPU要求。\n编写 DaemonSet Spec 必需字段 和其它所有 Kubernetes 配置一样，DaemonSet 需要 apiVersion、kind 和 metadata字段。有关配置文件的通用信息，详见文档 部署应用、配置容器 和资源管理。\nDaemonSet 也需要一个 .spec配置段。\nPod 模板 .spec 唯一必需的字段是 .spec.template。\n.spec.template 是一个 Pod 模板。 它与 Pod 具有相同的 schema，除了它是嵌套的，而且不具有 apiVersion 或 kind 字段。\nPod 除了必须字段外，在 DaemonSet 中的 Pod 模板必须指定合理的标签（查看 pod selector）。\n在 DaemonSet 中的 Pod 模板必需具有一个值为 Always 的 RestartPolicy，或者未指定它的值，默认是 Always。\nPod Selector .spec.selector 字段表示 Pod Selector，它与 Job 或其它资源的 .spec.selector 的原理是相同的。\nspec.selector 表示一个对象，它由如下两个字段组成：\n matchLabels - 与 ReplicationController 的 .spec.selector 的原理相同。 matchExpressions - 允许构建更加复杂的 Selector，可以通过指定 key、value 列表，以及与 key 和 value 列表的相关的操作符。  当上述两个字段都指定时，结果表示的是 AND 关系。\n如果指定了 .spec.selector，必须与 .spec.template.metadata.labels 相匹配。如果没有指定，它们默认是等价的。如果与它们配置的不匹配，则会被 API 拒绝。\n如果 Pod 的 label 与 selector 匹配，或者直接基于其它的 DaemonSet、或者 Controller（例如 ReplicationController），也不可以创建任何 Pod。 否则 DaemonSet Controller 将认为那些 Pod 是它创建的。Kubernetes 不会阻止这样做。一个场景是，可能希望在一个具有不同值的、用来测试用的 Node 上手动创建 Pod。\n仅在相同的 Node 上运行 Pod 如果指定了 .spec.template.spec.nodeSelector，DaemonSet Controller 将在能够匹配上 Node Selector 的 Node 上创建 Pod。 类似这种情况，可以指定 .spec.template.spec.affinity，然后 DaemonSet Controller 将在能够匹配上 Node Affinity 的 Node 上创建 Pod。 如果根本就没有指定，则 DaemonSet Controller 将在所有 Node 上创建 Pod。\n如何调度 Daemon Pod 正常情况下，Pod 运行在哪个机器上是由 Kubernetes 调度器进行选择的。然而，由 Daemon Controller 创建的 Pod 已经确定了在哪个机器上（Pod 创建时指定了 .spec.nodeName），因此：\n DaemonSet Controller 并不关心一个 Node 的 unschedulable 字段。 DaemonSet Controller 可以创建 Pod，即使调度器还没有被启动，这对集群启动是非常有帮助的。  Daemon Pod 关心 Taint 和 Toleration，它们会为没有指定 tolerationSeconds 的 node.alpha.kubernetes.io/notReady 和 node.alpha.kubernetes.io/unreachable 的 Taint，而创建具有 NoExecute 的 Toleration。这确保了当 alpha 特性的 TaintBasedEvictions 被启用，当 Node 出现故障，比如网络分区，这时它们将不会被清除掉（当 TaintBasedEvictions 特性没有启用，在这些场景下也不会被清除，但会因为 NodeController 的硬编码行为而被清除，Toleration 是不会的）。\n与 Daemon Pod 通信 与 DaemonSet 中的 Pod 进行通信，几种可能的模式如下：\n Push：配置 DaemonSet 中的 Pod 向其它 Service 发送更新，例如统计数据库。它们没有客户端。 NodeIP 和已知端口：DaemonSet 中的 Pod 可以使用 hostPort，从而可以通过 Node IP 访问到 Pod。客户端能通过某种方法知道 Node IP 列表，并且基于此也可以知道端口。 DNS：创建具有相同 Pod Selector 的 Headless Service，然后通过使用 endpoints 资源或从 DNS 检索到多个 A 记录来发现 DaemonSet。 Service：创建具有相同 Pod Selector 的 Service，并使用该 Service 访问到某个随机 Node 上的 daemon。（没有办法访问到特定 Node）  更新 DaemonSet 如果修改了 Node Label，DaemonSet 将立刻向新匹配上的 Node 添加 Pod，同时删除新近无法匹配上的 Node 上的 Pod。\n可以修改 DaemonSet 创建的 Pod。然而，不允许对 Pod 的所有字段进行更新。当下次 Node（即使具有相同的名称）被创建时，DaemonSet Controller 还会使用最初的模板。\n可以删除一个 DaemonSet。如果使用 kubectl 并指定 --cascade=false 选项，则 Pod 将被保留在 Node 上。然后可以创建具有不同模板的新 DaemonSet。具有不同模板的新 DaemonSet 将鞥能够通过 Label 匹配识别所有已经存在的 Pod。它不会修改或删除它们，即使是错误匹配了 Pod 模板。通过删除 Pod 或者 删除 Node，可以强制创建新的 Pod。\n在 Kubernetes 1.6 或以后版本，可以在 DaemonSet 上 执行滚动升级。\ninit 脚本 很可能通过直接在一个 Node 上启动 daemon 进程（例如，使用 init、upstartd、或 systemd）。这非常好，然而基于 DaemonSet 来运行这些进程有如下一些好处：\n 像对待应用程序一样，具备为 daemon 提供监控和管理日志的能力。 为 daemon 和应用程序使用相同的配置语言和工具（如 Pod 模板、kubectl）。 Kubernetes 未来版本可能会支持对 DaemonSet 创建 Pod 与 Node升级工作流进行集成。 在资源受限的容器中运行 daemon，能够增加 daemon 和应用容器的隔离性。然而这也实现了在容器中运行 daemon，但却不能在 Pod 中运行（例如，直接基于 Docker 启动）。  裸 Pod 可能要直接创建 Pod，同时指定其运行在特定的 Node 上。 然而，DaemonSet 替换了由于任何原因被删除或终止的 Pod，例如 Node 失败、例行节点维护，比如内核升级。由于这个原因，我们应该使用 DaemonSet 而不是单独创建 Pod。\n静态 Pod 很可能，通过在一个指定目录下编写文件来创建 Pod，该目录受 Kubelet 所监视。这些 Pod 被称为 静态 Pod。 不像 DaemonSet，静态 Pod 不受 kubectl 和 其它 Kubernetes API 客户端管理。静态 Pod 不依赖于 apiserver，这使得它们在集群启动的情况下非常有用。 而且，未来静态 Pod 可能会被废弃掉。\nReplication Controller DaemonSet 与 Replication Controller 非常类似，它们都能创建 Pod，这些 Pod 都具有不期望被终止的进程（例如，Web 服务器、存储服务器）。 为无状态的 Service 使用 Replication Controller，像 frontend，实现对副本的数量进行扩缩容、平滑升级，比之于精确控制 Pod 运行在某个主机上要重要得多。需要 Pod 副本总是运行在全部或特定主机上，并需要先于其他 Pod 启动，当这被认为非常重要时，应该使用 Daemon Controller。\n","date":1653062400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"99dc3ebf56a821f0216c9b61042140df","permalink":"https://lib.jimmysong.io/kubernetes-handbook/controllers/daemonset/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/controllers/daemonset/","section":"kubernetes-handbook","summary":"本文将为您介绍 DaemonSet 的基本概念。 什么是 DaemonSet？ DaemonSet 确保全部（或者一些）Node 上运行一个 Pod 的副本。当有 Node 加入集群时，也会为他们新增一个 Pod 。当有 Node 从集群移除时，这些 Pod 也会被回收。删除 DaemonSet 将会删除它创","tags":["Kubernetes"],"title":"DaemonSet","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"ReplicationController 用来确保容器应用的副本数始终保持在用户定义的副本数，即如果有容器异常退出，会自动创建新的 Pod 来替代；而如果异常多出来的容器也会自动回收。\n在新版本的 Kubernetes 中建议使用 ReplicaSet 来取代 ReplicationController。ReplicaSet 跟 ReplicationController 没有本质的不同，只是名字不一样，并且 ReplicaSet 支持集合式的 selector。\n虽然 ReplicaSet 可以独立使用，但一般还是建议使用 Deployment 来自动管理 ReplicaSet，这样就无需担心跟其他机制的不兼容问题（比如 ReplicaSet 不支持 rolling-update 但 Deployment 支持）。\nReplicaSet 示例：\napiVersion:extensions/v1beta1kind:ReplicaSetmetadata:name:frontend# these labels can be applied automatically# from the labels in the pod template if not set# labels:# app: guestbook# tier: frontendspec:# this replicas value is default# modify it according to your casereplicas:3# selector can be applied automatically# from the labels in the pod template if not set,# but we are specifying the selector here to# demonstrate its usage.selector:matchLabels:tier:frontendmatchExpressions:- {key: tier, operator: In, values:[frontend]}template:metadata:labels:app:guestbooktier:frontendspec:containers:- name:php-redisimage:gcr.io/google_samples/gb-frontend:v3resources:requests:cpu:100mmemory:100Mienv:- name:GET_HOSTS_FROMvalue:dns# If your cluster config does not include a dns service, then to# instead access environment variables to find service host# info, comment out the \u0026#39;value: dns\u0026#39; line above, and uncomment the# line below.# value: envports:- containerPort:80","date":1653062400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"1d0495b86d529b50c5c07b66d3bf251c","permalink":"https://lib.jimmysong.io/kubernetes-handbook/controllers/replicaset/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/controllers/replicaset/","section":"kubernetes-handbook","summary":"ReplicationController 用来确保容器应用的副本数始终保持在用户定义的副本数，即如果有容器异常退出，会自动创建新的 Pod 来替代；而如果异常多出来的容器也会自动回收。 在新版本的 Kubernetes 中建议使用 ReplicaSet 来取代 ReplicationContro","tags":["Kubernetes"],"title":"ReplicationController 和 ReplicaSet","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"Job 负责批处理任务，即仅执行一次的任务，它保证批处理任务的一个或多个 Pod 成功结束。\nJob Spec 格式  spec.template 格式同 Pod RestartPolicy 仅支持 Never 或 OnFailure 单个 Pod 时，默认 Pod 成功运行后 Job 即结束 .spec.completions 标志 Job 结束需要成功运行的 Pod 个数，默认为 1 .spec.parallelism 标志并行运行的 Pod 的个数，默认为 1 spec.activeDeadlineSeconds 标志失败 Pod 的重试最大时间，超过这个时间不会继续重试  一个简单的例子：\napiVersion:batch/v1kind:Jobmetadata:name:pispec:template:metadata:name:pispec:containers:- name:piimage:perlcommand:[\u0026#34;perl\u0026#34;,\u0026#34;-Mbignum=bpi\u0026#34;,\u0026#34;-wle\u0026#34;,\u0026#34;print bpi(2000)\u0026#34;]restartPolicy:Never$ kubectl create -f ./job.yamljob \u0026#34;pi\u0026#34; created$ pods=$(kubectl get pods --selector=job-name=pi --output=jsonpath={.items..metadata.name})$ kubectl logs $pods -c pi3.141592653589793238462643383279502...Bare Pod 所谓 Bare Pod 是指直接用 PodSpec 来创建的 Pod（即不在 ReplicaSet 或者 ReplicationController 的管理之下的 Pod）。这些 Pod 在 Node 重启后不会自动重启，但 Job 则会创建新的 Pod 继续任务。所以，推荐使用 Job 来替代 Bare Pod，即便是应用只需要一个 Pod。\n","date":1653062400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"674af9e8a8f7537dd3ef52e43d76b5b5","permalink":"https://lib.jimmysong.io/kubernetes-handbook/controllers/job/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/controllers/job/","section":"kubernetes-handbook","summary":"Job 负责批处理任务，即仅执行一次的任务，它保证批处理任务的一个或多个 Pod 成功结束。 Job Spec 格式 spec.template 格式同 Pod RestartPolicy 仅支持 Never 或 OnFailure 单个 Pod 时，默认 Pod 成功运行后 Job 即结束 .spec.completions 标志 Job 结束需要成功运行的 Pod 个数，默认为 1 .spec.parallelism 标志并行运行","tags":["Kubernetes"],"title":"Job","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"Cron Job 管理基于时间的 Job，即：\n 在给定时间点只运行一次 周期性地在给定时间点运行  一个 CronJob 对象类似于 crontab （cron table）文件中的一行。它根据指定的预定计划周期性地运行一个 Job，格式可以参考 Cron 。\n前提条件 当前使用的 Kubernetes 集群，版本 \u0026gt;= 1.8（对 CronJob）。对于先前版本的集群，版本 \u0026lt; 1.8，启动 API Server（参考 为集群开启或关闭 API 版本 获取更多信息）时，通过传递选项 --runtime-config=batch/v2alpha1=true 可以开启 batch/v2alpha1 API。\n典型的用法如下所示：\n 在给定的时间点调度 Job 运行 创建周期性运行的 Job，例如：数据库备份、发送邮件。  CronJob Spec   .spec.schedule：调度，必需字段，指定任务运行周期，格式同 Cron\n  .spec.jobTemplate：Job 模板，必需字段，指定需要运行的任务，格式同 Job\n  .spec.startingDeadlineSeconds ：启动 Job 的期限（秒级别），该字段是可选的。如果因为任何原因而错过了被调度的时间，那么错过执行时间的 Job 将被认为是失败的。如果没有指定，则没有期限\n  .spec.concurrencyPolicy：并发策略，该字段也是可选的。它指定了如何处理被 Cron Job 创建的 Job 的并发执行。只允许指定下面策略中的一种：\n Allow（默认）：允许并发运行 Job Forbid：禁止并发运行，如果前一个还没有完成，则直接跳过下一个 Replace：取消当前正在运行的 Job，用一个新的来替换  注意，当前策略只能应用于同一个 Cron Job 创建的 Job。如果存在多个 Cron Job，它们创建的 Job 之间总是允许并发运行。\n  .spec.suspend ：挂起，该字段也是可选的。如果设置为 true，后续所有执行都会被挂起。它对已经开始执行的 Job 不起作用。默认值为 false。\n  .spec.successfulJobsHistoryLimit 和 .spec.failedJobsHistoryLimit ：历史限制，是可选的字段。它们指定了可以保留多少完成和失败的 Job。\n默认情况下，它们分别设置为 3 和 1。设置限制的值为 0，相关类型的 Job 完成后将不会被保留。\n  apiVersion:batch/v1beta1kind:CronJobmetadata:name:hellospec:schedule:\u0026#34;*/1 * * * *\u0026#34;jobTemplate:spec:template:spec:containers:- name:helloimage:busyboxargs:- /bin/sh- -c- date; echo Hello from the Kubernetes clusterrestartPolicy:OnFailure$ kubectl create -f cronjob.yaml cronjob \u0026#34;hello\u0026#34; created 当然，也可以用kubectl run来创建一个CronJob：\nkubectl run hello --schedule=\u0026#34;*/1 * * * *\u0026#34; --restart=OnFailure --image=busybox -- /bin/sh -c \u0026#34;date; echo Hello from the Kubernetes cluster\u0026#34; $ kubectl get cronjob NAME SCHEDULE SUSPEND ACTIVE LAST-SCHEDULE hello */1 * * * * False 0 \u0026lt;none\u0026gt; $ kubectl get jobs NAME DESIRED SUCCESSFUL AGE hello-1202039034 1 1 49s $ pods=$(kubectl get pods --selector=job-name=hello-1202039034 --output=jsonpath={.items..metadata.name}) $ kubectl logs $pods Mon Aug 29 21:34:09 UTC 2016 Hello from the Kubernetes cluster # 注意，删除 cronjob 的时候不会自动删除 job，这些 job 可以用 kubectl delete job 来删除 $ kubectl delete cronjob hello cronjob \u0026#34;hello\u0026#34; deleted Cron Job 限制 Cron Job 在每次调度运行时间内 大概 会创建一个 Job 对象。我们之所以说 大概 ，是因为在特定的环境下可能会创建两个 Job，或者一个 Job 都没创建。我们尝试少发生这种情况，但却不能完全避免。因此，创建 Job 操作应该是 幂等的。\nJob 根据它所创建的 Pod 的并行度，负责重试创建 Pod，并就决定这一组 Pod 的成功或失败。Cron Job 根本就不会去检查 Pod。\n删除 Cron Job 一旦不再需要 Cron Job，简单地可以使用 kubectl 命令删除它：\n$ kubectl delete cronjob hello cronjob \u0026#34;hello\u0026#34; deleted 这将会终止正在创建的 Job。然而，运行中的 Job 将不会被终止，不会删除 Job 或 它们的 Pod。为了清理那些 Job 和 Pod，需要列出该 Cron Job 创建的全部 Job，然后删除它们：\n$ kubectl get jobs NAME DESIRED SUCCESSFUL AGE hello-1201907962 1 1 11m hello-1202039034 1 1 8m ... $ kubectl delete jobs hello-1201907962 hello-1202039034 ... job \u0026#34;hello-1201907962\u0026#34; deleted job \u0026#34;hello-1202039034\u0026#34; deleted ... 一旦 Job 被删除，由 Job 创建的 Pod 也会被删除。注意，所有由名称为 “hello” 的 Cron Job 创建的 Job 会以前缀字符串 “hello-” 进行命名。如果想要删除当前 Namespace 中的所有 Job，可以通过命令 kubectl delete jobs --all 立刻删除它们。\n","date":1653062400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"7ef72b968f87495a8f8067b0c63f54ab","permalink":"https://lib.jimmysong.io/kubernetes-handbook/controllers/cronjob/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/controllers/cronjob/","section":"kubernetes-handbook","summary":"Cron Job 管理基于时间的 Job，即： 在给定时间点只运行一次 周期性地在给定时间点运行 一个 CronJob 对象类似于 crontab （cron table）文件中的一行。它根据指定的预定计划周期性地运行一个 Job，格式可以参考 Cron 。 前提条件 当","tags":["Kubernetes"],"title":"CronJob","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"为了使 Ingress 正常工作，集群中必须运行 Ingress controller。 这与其他类型的控制器不同，其他类型的控制器通常作为 kube-controller-manager 二进制文件的一部分运行，在集群启动时自动启动。 你需要选择最适合自己集群的 Ingress controller 或者自己实现一个。\nKubernetes 社区和众多厂商开发了大量的 Ingress Controller，你可以在 这里 找到。\n使用多个 Ingress 控制器 你可以使用 IngressClass 在集群中部署任意数量的 Ingress 控制器。 请注意你的 Ingress 类资源的 .metadata.name 字段。 当你创建 Ingress 时，你需要用此字段的值来设置 Ingress 对象的 ingressClassName 字段（请参考 IngressSpec v1 reference）。 ingressClassName 是之前的注解做法的替代。\n如果你不为 Ingress 指定 IngressClass，并且你的集群中只有一个 IngressClass 被标记为了集群默认，那么 Kubernetes 会应用此默认 IngressClass。 你可以通过将 ingressclass.kubernetes.io/is-default-class 注解 的值设置为 \u0026#34;true\u0026#34; 来将一个 IngressClass 标记为集群默认。\n理想情况下，所有 Ingress 控制器都应满足此规范，但各种 Ingress 控制器的操作略有不同。\n","date":1653062400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"a142c471566de5f9bab29e16184cc7bb","permalink":"https://lib.jimmysong.io/kubernetes-handbook/controllers/ingress-controller/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/controllers/ingress-controller/","section":"kubernetes-handbook","summary":"为了使 Ingress 正常工作，集群中必须运行 Ingress controller。 这与其他类型的控制器不同，其他类型的控制器通常作为 kube-controller-manager 二进制文件的一部分运行，在集群启动时自动启动。 你需要选择最适合自己集群的 Ingress controller 或者自己实现一个","tags":["Kubernetes"],"title":"Ingress 控制器","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"Kubernetes 中不仅支持 CPU、内存为指标的 HPA，还支持自定义指标的 HPA，例如 QPS。\n设置自定义指标 Kubernetes 1.6\n 在 Kubernetes1.6 集群中配置自定义指标的 HPA 的说明已废弃。\n 在设置定义指标 HPA 之前需要先进行如下配置：\n  将 heapster 的启动参数 --api-server 设置为 true\n  启用 custom metric API\n  将 kube-controller-manager 的启动参数中 --horizontal-pod-autoscaler-use-rest-clients 设置为 true，并指定 --master 为 API server 地址，如 --master=http://172.20.0.113:8080\n  在 Kubernetes1.5 以前很容易设置，参考 1.6 以前版本的 kubernetes 中开启自定义 HPA，而在 1.6 中因为取消了原来的 annotation 方式设置 custom metric，只能通过 API server 和 kube-aggregator 来获取 custom metric，因为只有两种方式来设置了，一是直接通过 API server 获取 heapster 的 metrics，二是部署 kube-aggragator 来实现。\n我们将在 Kubernetes1.8 版本的 Kubernetes 中，使用聚合的 API server 来实现自定义指标的 HPA。\nKuberentes1.7+\n确认您的 Kubernetes 版本在 1.7 或以上，修改以下配置：\n 将 kube-controller-manager 的启动参数中 --horizontal-pod-autoscaler-use-rest-clients 设置为 true，并指定 --master 为 API server 地址，如 --master=http://172.20.0.113:8080 修改 kube-apiserver 的配置文件 apiserver，增加一条配置 --requestheader-client-ca-file=/etc/kubernetes/ssl/ca.pem --requestheader-allowed-names=aggregator --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --proxy-client-cert-file=/etc/kubernetes/ssl/kubernetes.pem --proxy-client-key-file=/etc/kubernetes/ssl/kubernetes-key.pem，用来配置 aggregator 的 CA 证书。  已经内置了 apiregistration.k8s.io/v1beta1 API，可以直接定义 APIService，如：\napiVersion:apiregistration.k8s.io/v1kind:APIServicemetadata:name:v1.custom-metrics.metrics.k8s.iospec:insecureSkipTLSVerify:truegroup:custom-metrics.metrics.k8s.iogroupPriorityMinimum:1000versionPriority:5service:name:apinamespace:custom-metricsversion:v1alpha1部署 Prometheus\n使用 prometheus-operator.yaml 文件部署 Prometheus operator。\n注意： 将镜像修改为你自己的镜像仓库地址。\n这产生一个自定义的 API，可以通过浏览器访问，还可以使用下面的命令可以检查该 API：\n$ kubectl get --raw=apis/custom-metrics.metrics.k8s.io/v1alpha1 {\u0026#34;kind\u0026#34;:\u0026#34;APIResourceList\u0026#34;,\u0026#34;apiVersion\u0026#34;:\u0026#34;v1\u0026#34;,\u0026#34;groupVersion\u0026#34;:\u0026#34;custom-metrics.metrics.k8s.io/v1alpha1\u0026#34;,\u0026#34;resources\u0026#34;:[{\u0026#34;name\u0026#34;:\u0026#34;jobs.batch/http_requests\u0026#34;,\u0026#34;singularName\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;namespaced\u0026#34;:true,\u0026#34;kind\u0026#34;:\u0026#34;MetricValueList\u0026#34;,\u0026#34;verbs\u0026#34;:[\u0026#34;get\u0026#34;]},{\u0026#34;name\u0026#34;:\u0026#34;namespaces/http_requests\u0026#34;,\u0026#34;singularName\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;namespaced\u0026#34;:false,\u0026#34;kind\u0026#34;:\u0026#34;MetricValueList\u0026#34;,\u0026#34;verbs\u0026#34;:[\u0026#34;get\u0026#34;]},{\u0026#34;name\u0026#34;:\u0026#34;jobs.batch/up\u0026#34;,\u0026#34;singularName\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;namespaced\u0026#34;:true,\u0026#34;kind\u0026#34;:\u0026#34;MetricValueList\u0026#34;,\u0026#34;verbs\u0026#34;:[\u0026#34;get\u0026#34;]},{\u0026#34;name\u0026#34;:\u0026#34;pods/up\u0026#34;,\u0026#34;singularName\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;namespaced\u0026#34;:true,\u0026#34;kind\u0026#34;:\u0026#34;MetricValueList\u0026#34;,\u0026#34;verbs\u0026#34;:[\u0026#34;get\u0026#34;]},{\u0026#34;name\u0026#34;:\u0026#34;services/scrape_samples_scraped\u0026#34;,\u0026#34;singularName\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;namespaced\u0026#34;:true,\u0026#34;kind\u0026#34;:\u0026#34;MetricValueList\u0026#34;,\u0026#34;verbs\u0026#34;:[\u0026#34;get\u0026#34;]},{\u0026#34;name\u0026#34;:\u0026#34;namespaces/scrape_samples_scraped\u0026#34;,\u0026#34;singularName\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;namespaced\u0026#34;:false,\u0026#34;kind\u0026#34;:\u0026#34;MetricValueList\u0026#34;,\u0026#34;verbs\u0026#34;:[\u0026#34;get\u0026#34;]},{\u0026#34;name\u0026#34;:\u0026#34;pods/scrape_duration_seconds\u0026#34;,\u0026#34;singularName\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;namespaced\u0026#34;:true,\u0026#34;kind\u0026#34;:\u0026#34;MetricValueList\u0026#34;,\u0026#34;verbs\u0026#34;:[\u0026#34;get\u0026#34;]},{\u0026#34;name\u0026#34;:\u0026#34;services/scrape_duration_seconds\u0026#34;,\u0026#34;singularName\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;namespaced\u0026#34;:true,\u0026#34;kind\u0026#34;:\u0026#34;MetricValueList\u0026#34;,\u0026#34;verbs\u0026#34;:[\u0026#34;get\u0026#34;]},{\u0026#34;name\u0026#34;:\u0026#34;pods/http_requests\u0026#34;,\u0026#34;singularName\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;namespaced\u0026#34;:true,\u0026#34;kind\u0026#34;:\u0026#34;MetricValueList\u0026#34;,\u0026#34;verbs\u0026#34;:[\u0026#34;get\u0026#34;]},{\u0026#34;name\u0026#34;:\u0026#34;pods/scrape_samples_post_metric_relabeling\u0026#34;,\u0026#34;singularName\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;namespaced\u0026#34;:true,\u0026#34;kind\u0026#34;:\u0026#34;MetricValueList\u0026#34;,\u0026#34;verbs\u0026#34;:[\u0026#34;get\u0026#34;]},{\u0026#34;name\u0026#34;:\u0026#34;jobs.batch/scrape_samples_scraped\u0026#34;,\u0026#34;singularName\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;namespaced\u0026#34;:true,\u0026#34;kind\u0026#34;:\u0026#34;MetricValueList\u0026#34;,\u0026#34;verbs\u0026#34;:[\u0026#34;get\u0026#34;]},{\u0026#34;name\u0026#34;:\u0026#34;jobs.batch/scrape_duration_seconds\u0026#34;,\u0026#34;singularName\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;namespaced\u0026#34;:true,\u0026#34;kind\u0026#34;:\u0026#34;MetricValueList\u0026#34;,\u0026#34;verbs\u0026#34;:[\u0026#34;get\u0026#34;]},{\u0026#34;name\u0026#34;:\u0026#34;namespaces/scrape_duration_seconds\u0026#34;,\u0026#34;singularName\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;namespaced\u0026#34;:false,\u0026#34;kind\u0026#34;:\u0026#34;MetricValueList\u0026#34;,\u0026#34;verbs\u0026#34;:[\u0026#34;get\u0026#34;]},{\u0026#34;name\u0026#34;:\u0026#34;namespaces/scrape_samples_post_metric_relabeling\u0026#34;,\u0026#34;singularName\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;namespaced\u0026#34;:false,\u0026#34;kind\u0026#34;:\u0026#34;MetricValueList\u0026#34;,\u0026#34;verbs\u0026#34;:[\u0026#34;get\u0026#34;]},{\u0026#34;name\u0026#34;:\u0026#34;services/scrape_samples_post_metric_relabeling\u0026#34;,\u0026#34;singularName\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;namespaced\u0026#34;:true,\u0026#34;kind\u0026#34;:\u0026#34;MetricValueList\u0026#34;,\u0026#34;verbs\u0026#34;:[\u0026#34;get\u0026#34;]},{\u0026#34;name\u0026#34;:\u0026#34;services/up\u0026#34;,\u0026#34;singularName\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;namespaced\u0026#34;:true,\u0026#34;kind\u0026#34;:\u0026#34;MetricValueList\u0026#34;,\u0026#34;verbs\u0026#34;:[\u0026#34;get\u0026#34;]},{\u0026#34;name\u0026#34;:\u0026#34;pods/scrape_samples_scraped\u0026#34;,\u0026#34;singularName\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;namespaced\u0026#34;:true,\u0026#34;kind\u0026#34;:\u0026#34;MetricValueList\u0026#34;,\u0026#34;verbs\u0026#34;:[\u0026#34;get\u0026#34;]},{\u0026#34;name\u0026#34;:\u0026#34;services/http_requests\u0026#34;,\u0026#34;singularName\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;namespaced\u0026#34;:true,\u0026#34;kind\u0026#34;:\u0026#34;MetricValueList\u0026#34;,\u0026#34;verbs\u0026#34;:[\u0026#34;get\u0026#34;]},{\u0026#34;name\u0026#34;:\u0026#34;jobs.batch/scrape_samples_post_metric_relabeling\u0026#34;,\u0026#34;singularName\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;namespaced\u0026#34;:true,\u0026#34;kind\u0026#34;:\u0026#34;MetricValueList\u0026#34;,\u0026#34;verbs\u0026#34;:[\u0026#34;get\u0026#34;]},{\u0026#34;name\u0026#34;:\u0026#34;namespaces/up\u0026#34;,\u0026#34;singularName\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;namespaced\u0026#34;:false,\u0026#34;kind\u0026#34;:\u0026#34;MetricValueList\u0026#34;,\u0026#34;verbs\u0026#34;:[\u0026#34;get\u0026#34;]}]} 参考  1.6 以前版本的 kubernetes 中开启自定义 HPA - medium.com Horizontal Pod Autoscaler Walkthrough - kubernetes.io Kubernetes 1.8: Now with 100% Daily Value of Custom Metrics - blog.openshift.com Arbitrary/Custom Metrics in the Horizontal Pod Autoscaler#117 - github.com  ","date":1653062400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"dbc2e20994d0b58ba7a74fd91550605d","permalink":"https://lib.jimmysong.io/kubernetes-handbook/controllers/hpa/custom-metrics-hpa/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/controllers/hpa/custom-metrics-hpa/","section":"kubernetes-handbook","summary":"Kubernetes 中不仅支持 CPU、内存为指标的 HPA，还支持自定义指标的 HPA，例如 QPS。 设置自定义指标 Kubernetes 1.6 在 Kubernetes1.6 集群中配置自定义指标的 HPA 的说明已废弃。 在设置定义指标 HPA 之前需要先进行如下配置： 将 heapster 的启动参数 --api-server 设置为","tags":["Kubernetes"],"title":"自定义指标 HPA","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"准入控制器（Admission Controller）位于 API Server 中，在对象被持久化之前，准入控制器拦截对 API Server 的请求，一般用来做身份验证和授权。其中包含两个特殊的控制器：MutatingAdmissionWebhook 和 ValidatingAdmissionWebhook。分别作为配置的变异和验证准入控制 webhook。\n准入控制器包括以下两种：\n 变更（Mutating）准入控制：修改请求的对象 验证（Validating）准入控制：验证请求的对象  准入控制器是在 API Server 的启动参数重配置的。一个准入控制器可能属于以上两者中的一种，也可能两者都属于。当请求到达 API Server 的时候首先执行变更准入控制，然后再执行验证准入控制。\n我们在部署 Kubernetes 集群的时候都会默认开启一系列准入控制器，如果没有设置这些准入控制器的话可以说你的 Kubernetes 集群就是在裸奔，应该只有集群管理员可以修改集群的准入控制器。\n例如我会默认开启如下的准入控制器。\n--admission-control=ServiceAccount,NamespaceLifecycle,NamespaceExists,LimitRanger,ResourceQuota,MutatingAdmissionWebhook,ValidatingAdmissionWebhook 准入控制器列表 Kubernetes 目前支持的准入控制器有：\n AlwaysPullImages：此准入控制器修改每个 Pod 的时候都强制重新拉取镜像。 DefaultStorageClass：此准入控制器观察创建PersistentVolumeClaim时不请求任何特定存储类的对象，并自动向其添加默认存储类。这样，用户就不需要关注特殊存储类而获得默认存储类。 DefaultTolerationSeconds：此准入控制器将Pod的容忍时间notready:NoExecute和unreachable:NoExecute 默认设置为5分钟。 DenyEscalatingExec：此准入控制器将拒绝exec 和附加命令到以允许访问宿主机的升级了权限运行的pod。 EventRateLimit (alpha)：此准入控制器缓解了 API Server 被事件请求淹没的问题，限制时间速率。 ExtendedResourceToleration：此插件有助于创建具有扩展资源的专用节点。 ImagePolicyWebhook：此准入控制器允许后端判断镜像拉取策略，例如配置镜像仓库的密钥。 Initializers (alpha)：Pod初始化的准入控制器，详情请参考动态准入控制。 LimitPodHardAntiAffinityTopology：此准入控制器拒绝任何在 requiredDuringSchedulingRequiredDuringExecution 的 AntiAffinity  字段中定义除了kubernetes.io/hostname 之外的拓扑关键字的 pod 。 LimitRanger：此准入控制器将确保所有资源请求不会超过 namespace 的 LimitRange。 MutatingAdmissionWebhook （1.9版本中为beta）：该准入控制器调用与请求匹配的任何变更 webhook。匹配的 webhook是串行调用的；如果需要，每个人都可以修改对象。 NamespaceAutoProvision：此准入控制器检查命名空间资源上的所有传入请求，并检查引用的命名空间是否存在。如果不存在就创建一个命名空间。 NamespaceExists：此许可控制器检查除 Namespace 其自身之外的命名空间资源上的所有请求。如果请求引用的命名空间不存在，则拒绝该请求。 NamespaceLifecycle：此准入控制器强制执行正在终止的命令空间中不能创建新对象，并确保Namespace拒绝不存在的请求。此准入控制器还防止缺失三个系统保留的命名空间default、kube-system、kube-public。 NodeRestriction：该准入控制器限制了 kubelet 可以修改的Node和Pod对象。 OwnerReferencesPermissionEnforcement：此准入控制器保护对metadata.ownerReferences对象的访问，以便只有对该对象具有“删除”权限的用户才能对其进行更改。 PodNodeSelector：此准入控制器通过读取命名空间注释和全局配置来限制可在命名空间内使用的节点选择器。 PodPreset：此准入控制器注入一个pod，其中包含匹配的PodPreset中指定的字段，详细信息见Pod Preset。 PodSecurityPolicy：此准入控制器用于创建和修改pod，并根据请求的安全上下文和可用的Pod安全策略确定是否应该允许它。 PodTolerationRestriction：此准入控制器首先验证容器的容忍度与其命名空间的容忍度之间是否存在冲突，并在存在冲突时拒绝该容器请求。 Priority：此控制器使用priorityClassName字段并填充优先级的整数值。如果未找到优先级，则拒绝Pod。 ResourceQuota：此准入控制器将观察传入请求并确保它不违反命名空间的ResourceQuota对象中列举的任何约束。 SecurityContextDeny：此准入控制器将拒绝任何试图设置某些升级的SecurityContext字段的pod 。 ServiceAccount：此准入控制器实现serviceAccounts的自动化。 用中的存储对象保护：该StorageObjectInUseProtection插件将kubernetes.io/pvc-protection或kubernetes.io/pv-protection终结器添加到新创建的持久卷声明（PVC）或持久卷（PV）。在用户删除PVC或PV的情况下，PVC或PV不会被移除，直到PVC或PV保护控制器从PVC或PV中移除终结器。有关更多详细信息，请参阅使用中的存储对象保护。 ValidatingAdmissionWebhook（1.8版本中为alpha；1.9版本中为beta）：该准入控制器调用与请求匹配的任何验证webhook。匹配的webhooks是并行调用的；如果其中任何一个拒绝请求，则请求失败。  推荐配置 Kubernetes 1.10+\n对于Kubernetes 1.10及更高版本，我们建议使用--enable-admission-plugins标志运行以下一组准入控制器（顺序无关紧要）。\n 注意： --admission-control在1.10中已弃用并替换为--enable-admission-plugins。\n --enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota 对于Kubernetes 1.9及更早版本，我们建议使用--admission-control标志（顺序有关）运行以下一组许可控制器。\nKubernetes 1.9\n--admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota 值得重申的是，在1.9中，这些发生在变更阶段和验证阶段，并且例如ResourceQuota在验证阶段运行，因此是运行的最后一个准入控制器。 MutatingAdmissionWebhook在此列表中出现在它之前，因为它在变更阶段运行。\n对于早期版本，没有验证准入控制器和变更准入控制器的概念，并且准入控制器以指定的确切顺序运行。\nKubernetes 1.6 - 1.8\n--admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,PersistentVolumeLabel,DefaultStorageClass,ResourceQuota,DefaultTolerationSeconds 参考  Using Admission Controllers - kubernetes.io  ","date":1653062400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"bd0acdf644e4f26ca3dabc77ff676ec9","permalink":"https://lib.jimmysong.io/kubernetes-handbook/controllers/admission-controller/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/controllers/admission-controller/","section":"kubernetes-handbook","summary":"准入控制器（Admission Controller）位于 API Server 中，在对象被持久化之前，准入控制器拦截对 API Server 的请求，一般用来做身份验证和授权。其中包含两个特殊的控制器：MutatingAdmissionW","tags":["Kubernetes"],"title":"准入控制器（Admission Controller）","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"Kubernetes Pod 是有生命周期的，它们可以被创建，也可以被销毁，然而一旦被销毁生命就永远结束。通过 ReplicationController 能够动态地创建和销毁 Pod。 每个 Pod 都会获取它自己的 IP 地址，即使这些 IP 地址不总是稳定可依赖的。这会导致一个问题：在 Kubernetes 集群中，如果一组 Pod（称为 backend）为其它 Pod （称为 frontend）提供服务，那么 frontend Pod 该如何发现和连接哪些 backend Pod 呢？\n关于 Service Kubernetes Service 定义了这样一种抽象：Pod 的逻辑分组，一种可以访问它们的策略 —— 通常称为微服务。这一组 Pod 能够被 Service 访问到，通常是通过 Label Selector（查看下面了解，为什么可能需要没有 selector 的 Service）实现的。\n举个例子，假设有一个用于图片处理的运行了三个副本的 backend。这些副本是可互换的 —— frontend 不需要关心它们调用了哪个 backend 副本。然而组成这一组 backend 程序的 Pod 实际上可能会发生变化，frontend 客户端不应该也没必要知道，而且也不需要跟踪这组 backend 的状态。Service 定义的抽象能够解耦这种关联。\n对 Kubernetes 集群中的应用，Kubernetes 提供了简单的 Endpoints API，只要 Service 中的一组 Pod 发生变更，应用程序就会被更新。对非 Kubernetes 集群中的应用，Kubernetes 提供了基于 VIP 的网桥的方式访问 Service，再由 Service 重定向到 backend Pod。\n定义 Service 一个 Service 在 Kubernetes 中是一个 REST 对象，和 Pod 类似。 像所有的 REST 对象一样， Service 定义可以基于 POST 方式，请求 apiserver 创建新的实例。\n例如，假定有一组 Pod，它们对外暴露了 9376 端口，同时还被打上 \u0026#34;app=MyApp\u0026#34; 标签。\nkind:ServiceapiVersion:v1metadata:name:my-servicespec:selector:app:MyAppports:- protocol:TCPport:80targetPort:9376上述配置将创建一个名称为 “my-service” 的 Service 对象，它会将请求代理到 9376 TCP 端口，具有标签 \u0026#34;app=MyApp\u0026#34; 的 Pod 上。这个 Service 将被指派一个 IP 地址（通常称为 “Cluster IP”），它会被服务的代理使用（见下面）。该 Service 的 selector 将会持续评估，处理结果将被 POST 到一个名称为 “my-service” 的 Endpoints 对象上。\n需要注意的是， Service 能够将一个接收端口映射到任意的 targetPort。默认情况下，targetPort 将被设置为与 port 字段相同的值。targetPort 可以是一个字符串，引用了 backend Pod 的端口的名称。但是，实际指派给该端口名称的端口号，在每个 backend Pod 中可能并不相同。对于部署和设计 Service ，这种方式会提供更大的灵活性。例如，可以在 backend 软件下一个版本中，修改 Pod 暴露的端口，并不会中断客户端的调用。\nKubernetes Service 支持 TCP 和 UDP 协议，默认为 TCP 协议。\n没有 selector 的 Service Service 抽象了该如何访问 Kubernetes Pod，但也能够抽象其它类型的 backend，例如：\n 希望在生产环境中使用外部的数据库集群，但测试环境使用自己的数据库。 希望服务指向另一个 Namespace 中或其它集群中的服务。 正在将工作负载转移到 Kubernetes 集群，和运行在 Kubernetes 集群之外的 backend。  在任何这些场景中，都能够定义没有 selector 的 Service ：\nkind:ServiceapiVersion:v1metadata:name:my-servicespec:ports:- protocol:TCPport:80targetPort:9376由于这个 Service 没有 selector，就不会创建相关的 Endpoints 对象。可以手动将 Service 映射到指定的 Endpoints：\nkind:EndpointsapiVersion:v1metadata:name:my-servicesubsets:- addresses:- ip:1.2.3.4ports:- port:9376注意：Endpoint IP 地址不能是 loopback（127.0.0.0/8）、 link-local（169.254.0.0/16）、或者 link-local 多播（224.0.0.0/24）。\n访问没有 selector 的 Service，与有 selector 的 Service 的原理相同。请求将被路由到用户定义的 Endpoint（该示例中为 1.2.3.4:9376）。\nExternalName Service 是 Service 的特例，它没有 selector，也没有定义任何的端口和 Endpoint。相反地，对于运行在集群外部的服务，它通过返回该外部服务的别名这种方式来提供服务。\nkind:ServiceapiVersion:v1metadata:name:my-servicenamespace:prodspec:type:ExternalNameexternalName:my.database.example.com当查询主机 my-service.prod.svc.CLUSTER时，集群的 DNS 服务将返回一个值为 my.database.example.com 的 CNAME 记录。访问这个服务的工作方式与其它的相同，唯一不同的是重定向发生在 DNS 层，而且不会进行代理或转发。如果后续决定要将数据库迁移到 Kubernetes 集群中，可以启动对应的 Pod，增加合适的 Selector 或 Endpoint，修改 Service 的 type。\nVIP 和 Service 代理 在 Kubernetes 集群中，每个 Node 运行一个 kube-proxy 进程。kube-proxy 负责为 Service 实现了一种 VIP（虚拟 IP）的形式，而不是 ExternalName 的形式。\n在 Kubernetes v1.0 版本，代理完全在 userspace，Service 是 “4层”（TCP/UDP over IP）概念。\n在 Kubernetes v1.1 版本，新增了 iptables 代理，但并不是默认的运行模式。新增了 Ingress API（beta 版），用来表示 “7层”（HTTP）服务。\n从 Kubernetes v1.2 起，默认就是 iptables 代理。\n在 Kubernetes v1.8.0-beta.0 中，添加了ipvs代理。\nuserspace 代理模式 这种模式，kube-proxy 会监视 Kubernetes master 对 Service 对象和 Endpoints 对象的添加和移除。 对每个 Service，它会在本地 Node 上打开一个端口（随机选择）。\n任何连接到“代理端口”的请求，都会被代理到 Service 的backend Pods 中的某个上面（如 Endpoints 所报告的一样）。 使用哪个 backend Pod，是基于 Service 的 SessionAffinity 来确定的。\n最后，它安装 iptables 规则，捕获到达该 Service 的 clusterIP（是虚拟 IP）和 Port 的请求，并重定向到代理端口，代理端口再代理请求到 backend Pod。\n网络返回的结果是，任何到达 Service 的 IP:Port 的请求，都会被代理到一个合适的 backend，不需要客户端知道关于 Kubernetes、Service、或 Pod 的任何信息。\n默认的策略是，通过 round-robin 算法来选择 backend Pod。 实现基于客户端 IP 的会话亲和性，可以通过设置 service.spec.sessionAffinity 的值为 \u0026#34;ClientIP\u0026#34; （默认值为 \u0026#34;None\u0026#34;）。\n   userspace代理模式下Service概览图  iptables 代理模式 这种模式，kube-proxy 会监视 Kubernetes master 对 Service 对象和 Endpoints 对象的添加和移除。 对每个 Service，它会安装 iptables 规则，从而捕获到达该 Service 的 clusterIP（虚拟 IP）和端口的请求，进而将请求重定向到 Service 的一组 backend 中的某个上面。对于每个 Endpoints 对象，它也会安装 iptables 规则，这个规则会选择一个 backend Pod。\n默认的策略是，随机选择一个 backend。实现基于客户端 IP 的会话亲和性，可以将 service.spec.sessionAffinity 的值设置为 \u0026#34;ClientIP\u0026#34; （默认值为 \u0026#34;None\u0026#34;）。\n和 userspace 代理类似，网络返回的结果是，任何到达 Service 的 IP:Port 的请求，都会被代理到一个合适的 backend，不需要客户端知道关于 Kubernetes、Service、或 Pod 的任何信息。\n这应该比 userspace 代理更快、更可靠。然而，不像 userspace 代理，如果初始选择的 Pod 没有响应，iptables 代理不能自动地重试另一个 Pod，所以它需要依赖 readiness probes。\n   iptables代理模式下Service概览图  ipvs 代理模式 这种模式，kube-proxy会监视Kubernetes Service对象和Endpoints，调用netlink接口以相应地创建ipvs规则并定期与Kubernetes Service对象和Endpoints对象同步ipvs规则，以确保ipvs状态与期望一致。访问服务时，流量将被重定向到其中一个后端Pod。\n与iptables类似，ipvs基于netfilter 的 hook 功能，但使用哈希表作为底层数据结构并在内核空间中工作。这意味着ipvs可以更快地重定向流量，并且在同步代理规则时具有更好的性能。此外，ipvs为负载均衡算法提供了更多选项，例如：\n rr：轮询调度 lc：最小连接数 dh：目标哈希 sh：源哈希 sed：最短期望延迟 nq： 不排队调度  注意： ipvs模式假定在运行kube-proxy之前在节点上都已经安装了IPVS内核模块。当kube-proxy以ipvs代理模式启动时，kube-proxy将验证节点上是否安装了IPVS模块，如果未安装，则kube-proxy将回退到iptables代理模式。\n   ipvs代理模式下Service概览图  多端口 Service 很多 Service 需要暴露多个端口。对于这种情况，Kubernetes 支持在 Service 对象中定义多个端口。 当使用多个端口时，必须给出所有的端口的名称，这样 Endpoint 就不会产生歧义，例如： …","date":1653062400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"d31eb030339cc57f8838ca02c3830427","permalink":"https://lib.jimmysong.io/kubernetes-handbook/service-discovery/service/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/service-discovery/service/","section":"kubernetes-handbook","summary":"Kubernetes Pod 是有生命周期的，它们可以被创建，也可以被销毁，然而一旦被销毁生命就永远结束。通过 ReplicationController 能够动态地创建和销毁 Pod。 每个 Pod 都会获取它自己的 IP 地址，即使这些 IP 地址不总是稳定可依赖的。这会导致一个问题：在 Kubernetes","tags":["Kubernetes"],"title":"Service","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"拓扑感知路由指的是客户端对一个服务的访问流量，可以根据这个服务的端点拓扑，优先路由到与该客户端在同一个节点或者可用区的端点上的路由行为。\n先决条件 为了开启服务感知路由，你需要：\n 开启 TopologyAwareHints 智能感知提示门控 开启 EndpointSlice 控制器 安装 kube-proxy  端点切片 我们知道 Endpoint 通常情况下是由 Service 资源自动创建和管理的，但是随着 Kubernetes 集群的规模越来越大和管理的服务越来越多，Endpoint API 的局限性变得越来越明显。 端点切片（EndpointSlices）提供了一种简单的方法来跟踪 Kubernetes 集群中的网络端点。它们为 Endpoint 提供了一种可伸缩和可拓展的替代方案，同时还可以被用到拓扑感知路由中。\nEndpointSlices 示例如下：\napiVersion:discovery.k8s.io/v1kind:EndpointSlicemetadata:name:example-hintslabels:kubernetes.io/service-name:example-svcaddressType:IPv4ports:- name:httpprotocol:TCPport:80endpoints:- addresses:- \u0026#34;10.127.2.3\u0026#34;conditions:ready:truehostname:pod-1nodename:node-azone:zone-aEndpointSlice 中的每个端点都可以包含一定的拓扑信息。 拓扑信息包括端点的位置，对应节点、可用区的信息。 这些信息体现为 EndpointSlices 的如下端点字段：\n nodeName - 端点所在的 Node 名称 zone - 端点所处的可用区 hostname - 端点的 pod 名称  启用拓扑感知 请启用 kube-apiserver、kube-controller-manager、和 kube-proxy 的特性门控 TopologyAwareHints。通过把 Service 中的注解 service.kubernetes.io/topology-aware-hints 的值设置为 auto， 来激活服务的拓扑感知提示功能。 这告诉 EndpointSlice 控制器在它认为安全的时候来设置拓扑提示。kube-proxy 组件依据 EndpointSlice 控制器设置的提示，过滤由它负责路由的端点。\n由 EndpointSlice 控制器提供提示信息后 EndpointSlice 的示例如下：\napiVersion:discovery.k8s.io/v1kind:EndpointSlicemetadata:name:example-hintslabels:kubernetes.io/service-name:example-svcaddressType:IPv4ports:- name:httpprotocol:TCPport:80endpoints:- addresses:- \u0026#34;10.1.2.3\u0026#34;conditions:ready:truehostname:pod-1zone:zone-ahints:forZones:- name:\u0026#34;zone-a\u0026#34;我们看到其中已注入了 hints 信息，对于上面这个示例，zone-a 的客户端访问会优先路由到该端点上。\n管理 在大多数场合下，EndpointSlice 都由某个 Service 所有，因为端点切片正是为该服务跟踪记录其端点。这一属主关系是通过为每个 EndpointSlice 设置一个 属主（owner）引用，同时设置 kubernetes.io/service-name 标签来标明的， 目的是方便查找隶属于某服务的所有 EndpointSlice。\n控制面（尤其是端点切片的控制器） 会创建和管理 EndpointSlice 对象。EndpointSlice 对象还有一些其他使用场景， 例如作为服务网格（Service Mesh）的实现。这些场景都会导致有其他实体 或者控制器负责管理额外的 EndpointSlice 集合。\n为了确保多个实体可以管理 EndpointSlice 而且不会相互产生干扰，Kubernetes 定义了标签 endpointslice.kubernetes.io/managed-by，用来标明哪个实体在管理某个 EndpointSlice。端点切片控制器会在自己所管理的所有 EndpointSlice 上将该标签值设置 为 endpointslice-controller.k8s.io。 管理 EndpointSlice 的其他实体也应该为此标签设置一个唯一值。\n参考  使用拓扑键实现拓扑感知的流量路由 - kubernetes.io 端点切片 - kubernetes.io 拓扑感知提示 - kubernetes.io  ","date":1653062400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"323176a02eeba877a63abb07b09e1b8f","permalink":"https://lib.jimmysong.io/kubernetes-handbook/service-discovery/topology-aware-routing/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/service-discovery/topology-aware-routing/","section":"kubernetes-handbook","summary":"拓扑感知路由指的是客户端对一个服务的访问流量，可以根据这个服务的端点拓扑，优先路由到与该客户端在同一个节点或者可用区的端点上的路由行为。 先决条件 为了开启服务感知路由，你需要： 开启 TopologyAwareHints 智能感知提示门控 开启","tags":["Kubernetes"],"title":"拓扑感知路由","type":"book"},{"authors":null,"categories":["Istio"],"content":"Bookinfo 示例是 Istio 官方为了演示 Istio 功能而开发的一个示例应用，该应用有如下特点：\n 使用微服务方式开发，共有四个微服务 多语言应用，使用了 Java、Python、Ruby 和 NodeJs 语言 为了演示流量管理的高级功能，有的服务同时推出了多个版本  Bookinfo 应用部署架构 以下为 Istio 官方提供的该应用的架构图。\n   Istio 的 Bookinfo 示例应用架构图  Bookinfo 应用分为四个单独的微服务，其中每个微服务的部署的结构中都注入了一个 Sidecar：\n productpage ：productpage 微服务会调用 details 和 reviews 两个微服务，用来生成页面。 details ：这个微服务包含了书籍的信息。 reviews ：这个微服务包含了书籍相关的评论。它还会调用 ratings 微服务。 ratings ：ratings 微服务中包含了由书籍评价组成的评级信息。  reviews 微服务有 3 个版本：\n v1 版本不会调用 ratings 服务。 v2 版本会调用 ratings 服务，并使用 1 到 5 个黑色星形图标来显示评分信息。 v3 版本会调用 ratings 服务，并使用 1 到 5 个红色星形图标来显示评分信息。  使用 kubernetes-vagrant-centos-cluster 部署的 Kubernetes 集群和 Istio 服务的话可以直接运行下面的命令部署 Bookinfo 示例：\n$ kubectl apply -n default -f \u0026lt;(istioctl kube-inject -f yaml/istio-bookinfo/bookinfo.yaml) $ istioctl create -n default -f yaml/istio-bookinfo/bookinfo-gateway.yaml 关于该示例的介绍和详细步骤请参考 Bookinfo 应用。\nBookinfo 示例及 Istio 服务整体架构 从 Bookinfo 应用部署架构中可以看到该应用的几个微服务之间的关系，但是并没有描绘应用与 Istio 控制平面、Kubernetes 平台的关系，下图中描绘的是应用和平台整体的架构。\n   Bookinfo 示例与 Istio 的整体架构图  从图中可以看出 Istio 整体架构的特点：\n 模块化：很多模块可以选择性的开启，如负责证书管理的 istio-citadel 默认就没有启用 可定制化：可观测性的组件可以定制化和替换  参考  Bookinfo 应用 - istio.io   下一章   ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"6eea608c5e7b01ffeb136a6b537618dc","permalink":"https://lib.jimmysong.io/istio-handbook/setup/bookinfo-sample/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/istio-handbook/setup/bookinfo-sample/","section":"istio-handbook","summary":"Bookinfo 示例是 Istio 官方为了演示 Istio 功能而开发的一个示例应用，该应用有如下特点： 使用微服务方式开发，共有四个微服务 多语言应用，使用了 Java、Python、Ruby 和 NodeJs 语言 为了演示流量管理的高级功能，有的服务同时","tags":["Istio","Service Mesh"],"title":"Bookinfo 示例","type":"book"},{"authors":null,"categories":["Envoy"],"content":"到目前为止，我们已经谈到了向 Envoy 发送请求时产生的日志。然而，Envoy 也会在启动时和执行过程中产生日志。\n我们可以在每次运行 Envoy 时看到 Envoy 组件的日志。\n... [2021-11-03 17:22:43.361][1678][info][main] [source/server/server.cc:368] initializing epoch 0 (base id=0, hot restart version=11.104) [2021-11-03 17:22:43.361][1678][info][main] [source/server/server.cc:370] statically linked extensions: [2021-11-03 17:22:43.361][1678][info][main] [source/server/server.cc:372] envoy.filters.network: envoy.client_ssl_auth, envoy.echo, envoy.ext_authz, envoy.filters.network.client_ssl_auth ... 组件日志的默认格式字符串是 [%Y-%m-%d %T.%e][%t][%l][%n] [%g:%#] %v。格式字符串的第一部分代表日期和时间，然后是线程 ID（%t）、消息的日志级别（%l）、记录器名称（%n）、源文件的相对路径和行号（%g:%#），以及实际的日志消息（%v）。\n在启动 Envoy 时，我们可以使用 --log-format 命令行选项来定制格式。例如，如果我们想记录时间记录器名称、源函数名称和日志信息，那么我们可以这样写格式字符串：[%T.%e][%n][%！] %v。\n然后，在启动 Envoy 时，我们可以设置格式字符串，如下所示。\nfunc-e run -c someconfig.yaml --log-format \u0026#39;[%T.%e][%n][%!] %v\u0026#39; 如果我们使用格式字符串，日志条目看起来像这样：\n[17:43:15.963][main][initialize] response trailer map: 160 bytes: grpc-message,grpc-status [17:43:15.965][main][createRuntime] runtime: {} [17:43:15.965][main][initialize] No admin address given, so no admin HTTP server started. [17:43:15.966][config][initializeTracers] loading tracing configuration [17:43:15.966][config][initialize] loading 0 static secret(s) [17:43:15.966][config][initialize] loading 0 cluster(s) [17:43:15.966][config][initialize] loading 1 listener(s) [17:43:15.969][config][initializeStatsConfig] loading stats configuration [17:43:15.969][runtime][onRtdsReady] RTDS has finished initialization [17:43:15.969][upstream][maybeFinishInitialize] cm init: all clusters initialized [17:43:15.969][main][onRuntimeReady] there is no configured limit to the number of allowed active connections. Set a limit via the runtime key overload.global_downstream_max_connections [17:43:15.970][main][operator()] all clusters initialized. initializing init manager [17:43:15.970][config][startWorkers] all dependencies initialized. starting workers [17:43:15.971][main][run] starting main dispatch loop Envoy 具有多个日志记录器，对于每个日志记录器（例如 main、config、http...），我们可以控制日志记录级别（info、debug、trace）。如果我们启用 Envoy 管理界面并向 /logging 路径发送请求，就可以查看所有活动的日志记录器的名称。另一种查看所有可用日志的方法是通过源码。\n下面是 /logging 终端的默认输出的样子。\nactive loggers: admin: info alternate_protocols_cache: info aws: info assert: info backtrace: info cache_filter: info client: info config: info connection: info conn_handler: info decompression: info dns: info dubbo: info envoy_bug: info ext_authz: info rocketmq: info file: info filter: info forward_proxy: info grpc: info hc: info health_checker: info http: info http2: info hystrix: info init: info io: info jwt: info kafka: info key_value_store: info lua: info main: info matcher: info misc: info mongo: info quic: info quic_stream: info pool: info rbac: info redis: info router: info runtime: info stats: info secret: info tap: info testing: info thrift: info tracing: info upstream: info udp: info wasm: info 请注意，每个日志记录器的默认日志级别都被设置为 info。其他的日志级别有以下几种:\n trace debug info warning/warn error critical off  为了配置日志级别，我们可以使用 --log-level 选项或 --component-log-level 来分别控制每个组件的日志级别。组件的日志级别可以用 log_name:log_level 格式来写。如果我们要为多个组件设置日志级别，那么就用逗号来分隔它们。例如：upstream:critical,secret:error,router:trace。\n例如，要将 main 日志级别设置为 trace，config 日志级别设置为 error，并关闭所有其他日志记录器，我们可以键入以下内容。\nfunc-e run -c someconfig.yaml --log-level off --component-log-level main:trace, config:error 默认情况下，所有 Envoy 应用程序的日志都写到标准错误（stderr）。要改变这一点，我们可以使用 --log-path 选项提供一个输出文件。\nfunc-e run -c someconfig.yaml --log-path app-logs.log 在其中一个试验中，我们还将展示如何配置 Envoy，以便将应用日志写入谷歌云操作套件（以前称为 Stackdriver）。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"16d8b51d566d1e775edf177920d76426","permalink":"https://lib.jimmysong.io/envoy-handbook/logging/envoy-component-logs/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/envoy-handbook/logging/envoy-component-logs/","section":"envoy-handbook","summary":"到目前为止，我们已经谈到了向 Envoy 发送请求时产生的日志。然而，Envoy 也会在启动时和执行过程中产生日志。 我们可以在每次运行 Envoy 时看到 Envoy 组件的日志。 ... [2021-11-03 17:22:43.361][1678][info][main] [source/server/server.cc:368] initializing epoch 0 (base id=0, hot restart version=11.104) [2021-11-03 17:22:43.361][1678][info][main] [source/server/server.cc:370] statically linked extensions: [2021-11-03 17:22:43.361][1678][info][main] [source/server/server.cc:372] envoy.filters.network: envoy.client_ssl_auth, envoy.echo, envoy.ext_authz, envoy.filters.network.client_ssl_auth ... 组件","tags":["Envoy"],"title":"Envoy 组件日志","type":"book"},{"authors":null,"categories":["Istio"],"content":"Istio 是目前最流行的服务网格的开源实现，它的架构目前来说有两种，一种是 Sidecar 模式，这也是 Istio 最传统的部署架构，另一种是 Proxyless 模式。\n   Istio 的部署架构  Sidecar 模式 Sidecar 模式是 Istio 开源之初就在使用的模式，这种模式将应用程序的功能划分为单独的进程运行在同一个最小调度单元中，比如 Kubernetes 的 Pod 中。这种架构分为两个部分：控制平面和数据平面，控制平面是一个单体应用 Istiod，数据平面是由注入在每个 Pod 中的 Envoy 代理组成。你可以在 Sidecar 中添加更多功能，而不需要修改应用程序代码。这也是服务网格最大的一个卖点之一，将原先的应用程序 SDK 中的功能转移到了 Sidecar 中，这样开发者就可以专注于业务逻辑，而 sidecar 就交由运维来处理。\n   Sidecar 模式  我们在看下应用程序 Pod 中的结构。Pod 中包含应用容器和 Sidecar 容器，sidecar 容器与控制平面通信，获取该 Pod 上的所有代理配置，其中还有个 Init 容器，它是在注入 Sidecar 之前启动的，用来修改 Pod 的 IPtables 规则，做流量拦截的。\nProxyless 模式 Proxyless 模式是 Istio 1.11 版本中支持的实验特性，Istio 官网中有篇博客介绍了这个特性。可以直接将 gRPC 服务添加到 Istio 中，不需要再向 Pod 中注入 Envoy 代理。这样做可以极大的提升应用性能，降低网络延迟。有人说这种做法又回到了原始的基于 SDK 的微服务模式，其实非也，它依然使用了 Envoy 的 xDS API，但是因为不再需要向应用程序中注入 Sidecar 代理，因此可以减少应用程序性能的损耗。\n   Proxyless 模式  总结 Istio 虽然从诞生之初就是用的是 Sidecar 模式，但是随着网格规模的增大，大量的 Sidecar 对系统造成的瓶颈，及如 eBPF 这里网络技术的演进，未来我们有可能看到部分服务网格的功能，如透明流量劫持、证书配置等下沉到内核层，Istio 的架构也不一定会一成不变。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"cf7b9b7ef51b3c32fd84e2aa25944ea1","permalink":"https://lib.jimmysong.io/istio-handbook/concepts/istio-architecture/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/istio-handbook/concepts/istio-architecture/","section":"istio-handbook","summary":"Istio 是目前最流行的服务网格的开源实现，它的架构目前来说有两种，一种是 Sidecar 模式，这也是 Istio 最传统的部署架构，另一种是 Proxyless 模式。 Istio 的部署架构 Sidecar 模式 Sidecar 模式是 Istio 开源之初就在使用的模式，这种模式将应用程序的功能划分为单","tags":["Istio","Service Mesh"],"title":"Istio 架构解析","type":"book"},{"authors":null,"categories":["Istio"],"content":"本文将概述如何配置 Istio 的开发环境及编译和生成二进制文件和 Kubernetes 的 YAML 文件，更高级的测试、格式规范、原型和参考文档编写等请参考 Istio Dev Guide。\n依赖环境 Istio 开发环境依赖以下软件：\n Docker：测试和运行时 Go 1.11：程序开发 fpm 包构建工具：用来打包 Kubernetes 1.7.3+  设置环境变量 在编译过程中需要依赖以下环境变量，请根据你自己的\nexport ISTIO=$GOPATH/src/istio.io # DockerHub 的用户名 USER=jimmysong export HUB=\u0026#34;docker.io/$USER\u0026#34; # Docker 镜像的 tag，这里为了方便指定成了固定值，也可以使用 install/updateVersion.sh 来生成 tag export TAG=$USER # GitHub 的用户名 export GITHUB_USER=rootsongjc # 指定 Kubernetes 集群的配置文件地址 export KUBECONFIG=${HOME}/.kube/config 全量编译 编译过程中需要下载很多依赖包，请确认你的机器可以科学上网。\n执行下面的命令可以编译 Istio 所有组件的二进制文件。\nmake 以在 Mac 下编译为例，编译完成后所有的二进制文件将位于 $GOPATH/out/darwin_amd64/release。\n执行下面的命令构建镜像。\nmake docker 执行下面的命令将镜像推送到 DockerHub。\nmake push 也可以编译单独组件的镜像，详见开发指南。\n构建 YAML 文件 执行下面的命令可以生成 YAML 文件。\nmake generate_yaml 生成的 YAML 文件位于 repo 根目录的 install/kubernetes 目录下。\n参考  Istio Dev Guide - github.com   下一章   ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"68f63f0c64148df3d07a359f2ea7b499","permalink":"https://lib.jimmysong.io/istio-handbook/troubleshooting/istio-dev-env/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/istio-handbook/troubleshooting/istio-dev-env/","section":"istio-handbook","summary":"本文将概述如何配置 Istio 的开发环境及编译和生成二进制文件和 Kubernetes 的 YAML 文件，更高级的测试、格式规范、原型和参考文档编写等请参考 Istio Dev Guide。 依赖环境 Istio 开发环境依赖以下软件： Docker：测试和运行时 Go 1.11","tags":["Istio","Service Mesh"],"title":"Istio 开发环境配置","type":"book"},{"authors":null,"categories":["Istio"],"content":"用于认证的 JSON Web Token（JWT）令牌格式，由 RFC 7519 定义。参见 OAuth 2.0 和 OIDC 1.0，了解在整个认证流程中如何使用。\n示例 JWT 的规格是由 https://example.com 签发，受众要求必须是 bookstore_android.apps.example.com 或 bookstore_web.apps.example.com。该令牌应呈现在 Authorization header（默认）。Json 网络密钥集（JWKS）将按照 OpenID Connect 协议被发现。\nissuer:https://example.comaudiences:- bookstore_android.apps.example.combookstore_web.apps.example.com这个例子在非默认位置（x-goog-iap-jwt-assertion header）指定了令牌。它还定义了 URI 来明确获取 JWKS。\nissuer:https://example.comjwksUri:https://example.com/.secret/jwks.jsonjwtHeaders:- \u0026#34;x-goog-iap-jwt-assertion\u0026#34;关于 JWTRule 配置的详细用法请参考 Istio 官方文档。\n参考  JWTRule - istio.io   下一章   ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"2eb50efeeff79d05c0544e08adeedc70","permalink":"https://lib.jimmysong.io/istio-handbook/config-security/jwt/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/istio-handbook/config-security/jwt/","section":"istio-handbook","summary":"用于认证的 JSON Web Token（JWT）令牌格式，由 RFC 7519 定义。参见 OAuth 2.0 和 OIDC 1.0，了解在整个认证流程中如何使用。 示例 JWT 的规格是由 https://example.com 签发，受众要求必须是 bookstore_android.apps.example.com 或 bookstore_web.apps.example","tags":["Istio","Service Mesh"],"title":"JWTRule","type":"book"},{"authors":null,"categories":["Istio"],"content":"Listener 发现服务（LDS）是一个可选的 API，Envoy 将调用它来动态获取 Listener。Envoy 将协调 API 响应，并根据需要添加、修改或删除已知的 Listener。\nListener 更新的语义如下：\n 每个 Listener 必须有一个独特的名字。如果没有提供名称，Envoy 将创建一个 UUID。要动态更新的 Listener ，管理服务必须提供 Listener 的唯一名称。 当一个 Listener 被添加，在参与连接处理之前，会先进入“预热”阶段。例如，如果 Listener 引用 RDS 配置，那么在 Listener 迁移到 “active” 之前，将会解析并提取该配置。 Listener 一旦创建，实际上就会保持不变。因此，更新 Listener 时，会创建一个全新的 Listener （使用相同的监听套接字）。新增加的 Listener 都会通过上面所描述的相同“预热”过程。 当更新或删除 Listener 时，旧的 Listener 将被置于 “draining（逐出）” 状态，就像整个服务重新启动时一样。Listener 移除之后，该 Listener 所拥有的连接，经过一段时间优雅地关闭（如果可能的话）剩余的连接。逐出时间通过 --drain-time-s 选项设置。  注意\n Envoy 从 1.9 版本开始已不再支持 v1 API。  统计 LDS 的统计树是以 listener_manager.lds 为根，统计如下：\n   名字 类型 描述     config_reload Counter 因配置不同而导致配置重新加载的总次数   update_attempt Counter 尝试调用配置加载 API 的总次数   update_success Counter 调用配置加载 API 成功的总次数   update_failure Counter 调用配置加载 API 因网络错误的失败总数   update_rejected Counter 调用配置加载 API 因 schema/验证错误的失败总次数   version Gauge 来自上次成功调用配置加载API的内容哈希   control_plane.connected_state Gauge 布尔值，用来表示与管理服务器的连接状态，1表示已连接，0表示断开连接    参考  Listener discovery service(LDS) - envoyproxy.io  ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"1f92372829ad7798a8ff18ac378a14ff","permalink":"https://lib.jimmysong.io/istio-handbook/data-plane/envoy-lds/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/istio-handbook/data-plane/envoy-lds/","section":"istio-handbook","summary":"Listener 发现服务（LDS）是一个可选的 API，Envoy 将调用它来动态获取 Listener。Envoy 将协调 API 响应，并根据需要添加、修改或删除已知的 Listener。 Listener 更新的语义如下： 每个 Listener 必须有一个独特的","tags":["Istio","Service Mesh"],"title":"LDS（监听器发现服务）","type":"book"},{"authors":null,"categories":["Istio"],"content":"目的地指的是不同的子集（subset）或服务版本。通过子集，我们可以识别应用程序的不同变体。在我们的例子中，我们有两个子集，v1 和 v2，它们对应于我们 customer 服务的两个不同版本。每个子集都使用键/值对（标签）的组合来确定哪些 Pod 要包含在子集中。我们可以在一个名为 DestinationRule 的资源类型中声明子集。\n下面是定义了两个子集的 DestinationRule 资源的样子。\napiVersion:networking.istio.io/v1alpha3kind:DestinationRulemetadata:name:customers-destinationspec:host:customers.default.svc.cluster.localsubsets:- name:v1labels:version:v1- name:v2labels:version:v2让我们看看我们可以在 DestinationRule 中设置的流量策略。\nDestinationRule 中的流量策略 通过 DestinationRule，我们可以定义设置，如负载均衡配置、连接池大小、局部异常检测等，在路由发生后应用于流量。我们可以在trafficPolicy字段下设置流量策略设置。以下是这些设置：\n 负载均衡器设置 连接池设置 局部异常点检测 客户端 TLS 设置 端口流量策略  负载均衡器设置 通过负载均衡器设置，我们可以控制目的地使用哪种负载均衡算法。下面是一个带有流量策略的 DestinationRule 的例子，它把目的地的负载均衡算法设置为 round-robin。\napiVersion:networking.istio.io/v1alpha3kind:DestinationRulemetadata:name:customers-destinationspec:host:customers.default.svc.cluster.localtrafficPolicy:loadBalancer:simple:ROUND_ROBINsubsets:- name:v1labels:version:v1- name:v2labels:version:v2我们还可以设置基于哈希的负载均衡，并根据 HTTP 头、cookies 或其他请求属性提供会话亲和性。下面是一个流量策略的片段，它设置了基于哈希的负载均衡，并使用一个叫做 location 的 cookie 来实现亲和力。\ntrafficPolicy:loadBalancer:consistentHash:httpCookie:name:locationttl:4s连接池配置 这些设置可以在 TCP 和 HTTP 层面应用于上游服务的每个主机，我们可以用它们来控制连接量。\n下面是一个片段，显示了我们如何设置对服务的并发请求的限制。\nspec:host:myredissrv.prod.svc.cluster.localtrafficPolicy:connectionPool:http:http2MaxRequests:50异常点检测 异常点检测是一个断路器的实现，它跟踪上游服务中每个主机（Pod）的状态。如果一个主机开始返回 5xx HTTP 错误，它就会在预定的时间内被从负载均衡池中弹出。对于 TCP 服务，Envoy 将连接超时或失败计算为错误。\n下面是一个例子，它设置了 500 个并发的 HTTP2 请求（http2MaxRequests）的限制，每个连接不超过 10 个请求（maxRequestsPerConnection）到该服务。每 5 分钟扫描一次上游主机（Pod）（interval），如果其中任何一个主机连续失败 10 次（contracticalErrors），Envoy 会将其弹出 10 分钟（baseEjectionTime）。\ntrafficPolicy:connectionPool:http:http2MaxRequests:500maxRequestsPerConnection:10outlierDetection:consecutiveErrors:10interval:5mbaseEjectionTime:10m客户端 TLS 设置 包含任何与上游服务连接的 TLS 相关设置。下面是一个使用提供的证书配置 mTLS 的例子。\ntrafficPolicy:tls:mode:MUTUALclientCertificate:/etc/certs/cert.pemprivateKey:/etc/certs/key.pemcaCertificates:/etc/certs/ca.pem其他支持的 TLS 模式有 DISABLE（没有 TLS 连接），SIMPLE（在上游端点发起 TLS 连接），以及 ISTIO_MUTUAL（与 MUTUAL 类似，使用 Istio 的 mTLS 证书）。\n端口流量策略 使用 portLevelSettings 字段，我们可以将流量策略应用于单个端口。比如说：\ntrafficPolicy:portLevelSettings:- port:number:80loadBalancer:simple:LEAST_CONN- port:number:8000loadBalancer:simple:ROUND_ROBIN","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"ca01293c33e8067146ef161e31917973","permalink":"https://lib.jimmysong.io/istio-handbook/traffic-management/subset-and-destinationrule/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/istio-handbook/traffic-management/subset-and-destinationrule/","section":"istio-handbook","summary":"目的地指的是不同的子集（subset）或服务版本。通过子集，我们可以识别应用程序的不同变体。在我们的例子中，我们有两个子集，v1 和 v2，它们对应于我们 customer 服务的两个不同版本。每个子集都使用键/值对（标签","tags":["Istio","Service Mesh"],"title":"Subset 和 DestinationRule","type":"book"},{"authors":null,"categories":["Istio"],"content":"WorkloadEntry 使运维能够描述单个非 Kubernetes 工作负载的属性，如虚拟机或裸机服务器，以加入网格。WorkloadEntry 必须同时配置 Istio ServiceEntry，通过适当的标签选择工作负载，并提供 MESH_INTERNAL 服务的服务定义（主机名、端口属性等）。ServiceEntry 对象可以根据服务条目中指定的标签选择器来选择多个工作负载条目以及 Kubernetes pod。\n当工作负载连接到 istiod 时，自定义资源中的状态字段将被更新，以表明工作负载的健康状况以及其他细节，类似于 Kubernetes 更新 pod 状态的方式。\n示例 下面的例子声明了一个 WorkloadEntry 条目，代表 details.bookinfo.com 服务的一个虚拟机。这个虚拟机安装了 sidecar，并使用 details-legacy 服务账户进行引导。该服务通过 80 端口暴露给网格中的应用程序。通往该服务的 HTTP 流量被 Istio mTLS 封装，并被发送到目标端口 8080 的虚拟机上的 sidecar，后者又将其转发到同一端口的 localhost 上的应用程序。\napiVersion:networking.istio.io/v1alpha3kind:WorkloadEntrymetadata:name:details-svcspec:# 使用服务账户表明工作负载有一个用该服务账户引导的 sidecar 代理。有 sidecar 的 pod 会自动使用 istio mTLS 与工作负载通信。serviceAccount:details-legacyaddress:2.2.2.2labels:app:details-legacyinstance-id:vm1与其相关的 ServiceEntry 如下。\napiVersion:networking.istio.io/v1alpha3kind:ServiceEntrymetadata:name:details-svcspec:hosts:- details.bookinfo.comlocation:MESH_INTERNALports:- number:80name:httpprotocol:HTTPtargetPort:8080resolution:STATICworkloadSelector:labels:app:details-legacy下面的例子使用其完全限定的 DNS 名称声明了同一个虚拟机工作负载。ServiceEntry 的解析模式应改为 DNS，以表明客户端侧设备在转发请求之前应在运行时动态地解析 DNS 名称。\napiVersion:networking.istio.io/v1alpha3kind:WorkloadEntrymetadata:name:details-svcspec:# 使用服务账户表明工作负载有一个用该服务账户引导的 sidecar 代理。有 sidecar 的 pod 会自动使用 istio mTLS 与工作负载通信。serviceAccount:details-legacyaddress:vm1.vpc01.corp.netlabels:app:details-legacyinstance-id:vm1与其相关的 ServiceEntry 如下。\napiVersion:networking.istio.io/v1alpha3kind:ServiceEntrymetadata:name:details-svcspec:hosts:- details.bookinfo.comlocation:MESH_INTERNALports:- number:80name:httpprotocol:HTTPtargetPort:8080resolution:DNSworkloadSelector:labels:app:details-legacy配置项 下图是 WorkloadEntry 资源的配置拓扑图。\n  WorkloadEntry 资源配置拓扑图  WorkloadEntry 资源的顶级配置项如下：\n  address：字符串类型。与网络端点相关的地址，不含端口。当且仅当解析被设置为 DNS 时，可以使用域名，并且必须是完全限定的，没有通配符。对于 Unix 域套接字端点，使用 unix://absolute/path/to/socket 格式。\n  ports：与端点相关的端口集。如果指定了端口映射，必须是servicePortName 到这个端点的端口映射，这样，到服务端口的流量将被转发到映射到服务的 portName 的端点端口。如果省略，而目标端口被指定为服务的端口规范的一部分，那么到服务端口的流量将被转发到指定的目标端口上的一个端点。如果没有指定targetPort和端点的端口映射，到服务端口的流量将被转发到同一端口的端点上。\n注意1：不要用于unix://地址。\n注意2：端点的端口映射优先于 targetPort。\n  labels：与端点相关的一或多个标签。\n  network：字符串类型。network 使Istio能够将位于同一L3域/网络中的端点分组。同一网络中的所有端点都被认为是可以相互直连的。当不同网络中的端点不能直连时，可以使用Istio Gateway来建立连接（通常在Gateway服务器中使用AUTO_PASSTHROUGH模式）。这是一个高级配置，通常用于在多个集群上跨越Istio网格。\n  locality：字符串类型。与端点相关的位置。locality 对应于一个故障域（例如，国家/地区/区域）。例如，在US的一个端点，在US-East-1地区，在可用区az-1内，在数据中心机架r11内，其位置可以表示为us/us-east-1/az-1/r11。Istio将配置sidecar，使其路由到与sidecar相同地域的端点。如果该地区的端点都不可用，将选择父地区的端点（但在同一网络ID内）。例如，如果有两个端点在同一个网络中（networkID n1），比如e1的位置是us/us-east-1/az-1/r11，e2的位置是us/us-east-1/az-2/r12，来自us/us-east-1/az-1/r11位置的sidecar会选择同一位置的e1而不是来自不同位置的e2。端点e2可以是与网关（连接网络n1和n2）相关的IP，或与标准服务端点相关的IP。\n  weight：unit32 类型。与端点相关的负载均衡权重。权重较高的端点将按比例获得较高的流量。\n  serviceAccount：字符串类型。如果工作负载中存在 sidecar，则为与工作负载相关的服务账户。该服务账户必须与配置（WorkloadEntry 或 ServiceEntry）存在于同一命名空间。\n  关于 WorkloadEntry 配置的详细用法请参考 Istio 官方文档。\n参考  WorkloadEntry - istio.io  ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"52366ae0b614527ac9b8bcccaefeee9b","permalink":"https://lib.jimmysong.io/istio-handbook/config-networking/workload-entry/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/istio-handbook/config-networking/workload-entry/","section":"istio-handbook","summary":"WorkloadEntry 使运维能够描述单个非 Kubernetes 工作负载的属性，如虚拟机或裸机服务器，以加入网格。WorkloadEntry 必须同时配置 Istio ServiceEntry，通过适当的标签选择工作负载，并提供 MESH_INTERNAL 服务的服务定义（主机名、","tags":["Istio","Service Mesh"],"title":"WorkloadEntry","type":"book"},{"authors":null,"categories":["Istio"],"content":"分布式追踪是一种监测微服务应用程序的方法。使用分布式追踪，我们可以在请求通过被监控系统的不同部分时追踪它们。\n每当一个请求进入服务网格时，Envoy 都会生成一个唯一的请求 ID 和追踪信息，并将其作为 HTTP 头的一部分来存储。任何应用程序都可以将这些头信息转发给它所调用的其他服务，以便在系统中创建一个完整的追踪。\n分布式追踪是一个跨度（span）的集合。当请求流经不同的系统组件时，每个组件都会贡献一个跨度。每个跨度都有一个名称，开始和结束的时间戳，一组称为标签（tag）和日志（log）的键值对，以及一个跨度上下文。\n标签被应用于整个跨度，并用于查询和过滤。下面是我们在使用 Zipkin 时将看到的几个标签的例子。注意，其中有些是通用的，有些是 Istio 特有的。\n istio.mesh_id istio.canonical_service upstream_cluster http.url http.status_code zone  单个跨度与识别跨度、父跨度、追踪 ID 的上下文头一起被发送到一个叫做采集器的组件。采集器对数据进行验证、索引和存储。\n当请求流经 Envoy 代理时，Envoy 代理会自动发送各个跨度。请注意，Envoy 只能在边缘收集跨度。我们要负责在每个应用程序中生成任何额外的跨度，并确保我们在调用其他服务时转发追踪头信息。这样一来，各个跨度就可以正确地关联到一个单一的追踪中。\n使用 Zipkin 进行分布式追踪 Zipkin 是一个分布式跟踪系统。我们可以轻松地监控服务网格中发生的分布式事务，发现任何性能或延迟问题。\n为了让我们的服务参与分布式跟踪，我们需要在进行任何下游服务调用时传播服务的 HTTP 头信息。尽管所有的请求都要经过 Istio sidecar，但 Istio 没有办法将出站请求与产生这些请求的入站请求联系起来。通过在应用程序中传播相关的头信息可以帮助 Zipkin 将这些跟踪信息拼接起来。\nIstio 依赖于 B3 跟踪头（以 x-b3 开头的 header）和 Envoy 生成的请求 ID（x-request-id）。B3 头信息用于跨服务边界的跟踪上下文传播。\n以下是我们需要在我们的应用程序中对每个发出的请求进行传播的特定头文件名称：\nx-request-id x-b3-traceid x-b3-spanid x-b3-parentspanid x-b3-sampled x-b3-flags b3  如果你使用 Lightstep，你还需要转发名为 x-ot-span-context 的头。\n 传播头信息最常见的方法是从传入的请求中复制它们，并将它们包含在所有从你的应用程序发出的请求中。\n你用 Istio 服务网格得到的跟踪只在服务边界捕获。为了了解应用程序的行为并排除故障，你需要通过创建额外的跨度（span）来正确检测你的应用程序。\n要安装 Zipkin，我们可以使用 addons 文件夹中的 zipkin.yaml 文件。\n$ kubectl apply -f istio-1.9.0/samples/addons/extras/zipkin.yaml deployment.apps/zipkin created service/tracing created service/zipkin created 我们可以通过运行 getmesh istioctl dashboard zipkin 来打开 Zipkin 仪表板。在用户界面上，我们可以选择跟踪查询的标准。点击按钮，从下拉菜单中选择 serviceName，然后选择 customers.default service，点击搜索按钮（或按回车键），就可以搜索到 trace 信息。\n   Zipkin Dashboard  我们可以点击个别 trace 来深入挖掘不同的跨度。详细的视图将显示服务之间的调用时间，以及请求的细节，如方法、协议、状态码等。由于我们只有一个服务在运行（Nginx），所以你不会看到很多细节。稍后，我们将回到 Zipkin，更详细地探索这些 trace。\n   Zipkin trace 详情  ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"c4dab0282d1ea91612ab1a87f0c9f89f","permalink":"https://lib.jimmysong.io/istio-handbook/observability/zipkin/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/istio-handbook/observability/zipkin/","section":"istio-handbook","summary":"分布式追踪是一种监测微服务应用程序的方法。使用分布式追踪，我们可以在请求通过被监控系统的不同部分时追踪它们。 每当一个请求进入服务网格时，Envoy 都会生成一个唯一的请求 ID 和追踪信息，并将其作为 HTTP 头的一","tags":["Istio","Service Mesh"],"title":"Zipkin","type":"book"},{"authors":null,"categories":["Envoy"],"content":"断路是一种重要的模式，可以帮助服务的弹性。断路模式通过控制和管理对故障服务的访问来防止额外的故障。它允许我们快速失败，并尽快向下游反馈。\n让我们看一个定义断路的片段。\n...clusters:- name:my_cluster_name...circuit_breakers:thresholds:- priority:DEFAULTmax_connections:1000- priority:HIGHmax_requests:2000...我们可以为每条路由的优先级分别配置断路器的阈值。例如，较高优先级的路由应该有比默认优先级更高的阈值。如果超过了任何阈值，断路器就会断开，下游主机就会收到 HTTP 503 响应。\n我们可以用多种选项来配置断路器。\n1. 最大连接数（max_connections）\n指定 Envoy 与集群中所有端点的最大连接数。如果超过这个数字，断路器会断开，并增加集群的 upstream_cx_overflow 指标。默认值是 1024。\n2. 最大的排队请求（max_pending_requests）\n指定在等待就绪的连接池连接时被排队的最大请求数。当超过该阈值时，Envoy 会增加集群的 upstream_rq_pending_overflow 统计。默认值是 1024。\n3. 最大请求（max_requests）\n指定 Envoy 向集群中所有端点发出的最大并行请求数。默认值是 1024。\n4. 最大重试（max_retries）\n指定 Envoy 允许给集群中所有终端的最大并行重试次数。默认值是 3，如果这个断路器溢出，upstream_rq_retry_overflow 计数器就会递增。\n另外，我们可以将断路器与重试预算（retry_budget）相结合。通过指定重试预算，我们可以将并发重试限制在活动请求的数量上。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"a4b26c01a24bc115807b636c61d3c6de","permalink":"https://lib.jimmysong.io/envoy-handbook/cluster/circuit-breakers/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/envoy-handbook/cluster/circuit-breakers/","section":"envoy-handbook","summary":"断路是一种重要的模式，可以帮助服务的弹性。断路模式通过控制和管理对故障服务的访问来防止额外的故障。它允许我们快速失败，并尽快向下游反馈。 让我们看一个定义断路的片段。 ...clusters:- name:my_cluster_name...circuit_breakers:thresholds:- priority:DEFAULTmax_connections:1000- priority:HIGHma","tags":["Envoy"],"title":"断路器","type":"book"},{"authors":null,"categories":["Istio"],"content":"我们已经建立了一个新的 Docker 镜像，它使用了与当前运行的前端服务不同的标头。让我们看看如何部署所需的资源并将一定比例的流量路由到不同的前端服务版本。\n在我们创建任何资源之前，让我们删除现有的前端部署（kubectl delete deploy frontend），并创建一个版本标签设置为 original 。\napiVersion:apps/v1kind:Deploymentmetadata:name:frontendspec:selector:matchLabels:app:frontendversion:originaltemplate:metadata:labels:app:frontendversion:originalannotations:sidecar.istio.io/rewriteAppHTTPProbers:\u0026#34;true\u0026#34;spec:containers:- name:serverimage:gcr.io/google-samples/microservices-demo/frontend:v0.2.1ports:- containerPort:8080readinessProbe:initialDelaySeconds:10httpGet:path:\u0026#34;/_healthz\u0026#34;port:8080httpHeaders:- name:\u0026#34;Cookie\u0026#34;value:\u0026#34;shop_session-id=x-readiness-probe\u0026#34;livenessProbe:initialDelaySeconds:10httpGet:path:\u0026#34;/_healthz\u0026#34;port:8080httpHeaders:- name:\u0026#34;Cookie\u0026#34;value:\u0026#34;shop_session-id=x-liveness-probe\u0026#34;env:- name:PORTvalue:\u0026#34;8080\u0026#34;- name:PRODUCT_CATALOG_SERVICE_ADDRvalue:\u0026#34;productcatalogservice:3550\u0026#34;- name:CURRENCY_SERVICE_ADDRvalue:\u0026#34;currencyservice:7000\u0026#34;- name:CART_SERVICE_ADDRvalue:\u0026#34;cartservice:7070\u0026#34;- name:RECOMMENDATION_SERVICE_ADDRvalue:\u0026#34;recommendationservice:8080\u0026#34;- name:SHIPPING_SERVICE_ADDRvalue:\u0026#34;shippingservice:50051\u0026#34;- name:CHECKOUT_SERVICE_ADDRvalue:\u0026#34;checkoutservice:5050\u0026#34;- name:AD_SERVICE_ADDRvalue:\u0026#34;adservice:9555\u0026#34;- name:ENV_PLATFORMvalue:\u0026#34;gcp\u0026#34;resources:requests:cpu:100mmemory:64Milimits:cpu:200mmemory:128Mi将以上文件存储为 frontend-original.yaml 并使用 kubectl apply -f frontend-original.yaml 命令创建部署。\n现在我们准备创建一个 DestinationRule，定义两个版本的前端——现有的（original）和新的（v1）。\napiVersion:networking.istio.io/v1alpha3kind:DestinationRulemetadata:name:frontendspec:host:frontend.default.svc.cluster.localsubsets:- name:originallabels:version:original- name:v1labels:version:1.0.0将上述 YAML 保存为 frontend-dr.yaml，并使用 kubectl apply -f frontend-dr.yaml 创建它。\n接下来，我们将更新 VirtualService，并指定将所有流量路由到的子集。在这种情况下，我们将把所有流量路由到原始版本的前端。\napiVersion:networking.istio.io/v1alpha3kind:VirtualServicemetadata:name:frontend-ingressspec:hosts:- \u0026#39;*\u0026#39;gateways:- frontend-gatewayhttp:- route:- destination:host:frontendport:number:80subset:original将上述 YAML 保存为 frontend-vs.yaml，并使用 kubectl apply -f frontend-vs.yaml 更新 VirtualService 资源。\n现在我们将 VirtualService 配置为将所有进入的流量路由到 original 子集，我们可以安全地创建新的前端部署。\napiVersion:apps/v1kind:Deploymentmetadata:name:frontend-v1spec:selector:matchLabels:app:frontendversion:1.0.0template:metadata:labels:app:frontendversion:1.0.0annotations:sidecar.istio.io/rewriteAppHTTPProbers:\u0026#34;true\u0026#34;spec:containers:- name:serverimage:gcr.io/tetratelabs/boutique-frontend:1.0.0ports:- containerPort:8080readinessProbe:initialDelaySeconds:10httpGet:path:\u0026#34;/_healthz\u0026#34;port:8080httpHeaders:- name:\u0026#34;Cookie\u0026#34;value:\u0026#34;shop_session-id=x-readiness-probe\u0026#34;livenessProbe:initialDelaySeconds:10httpGet:path:\u0026#34;/_healthz\u0026#34;port:8080httpHeaders:- name:\u0026#34;Cookie\u0026#34;value:\u0026#34;shop_session-id=x-liveness-probe\u0026#34;env:- name:PORTvalue:\u0026#34;8080\u0026#34;- name:PRODUCT_CATALOG_SERVICE_ADDRvalue:\u0026#34;productcatalogservice:3550\u0026#34;- name:CURRENCY_SERVICE_ADDRvalue:\u0026#34;currencyservice:7000\u0026#34;- name:CART_SERVICE_ADDRvalue:\u0026#34;cartservice:7070\u0026#34;- name:RECOMMENDATION_SERVICE_ADDRvalue:\u0026#34;recommendationservice:8080\u0026#34;- name:SHIPPING_SERVICE_ADDRvalue:\u0026#34;shippingservice:50051\u0026#34;- name:CHECKOUT_SERVICE_ADDRvalue:\u0026#34;checkoutservice:5050\u0026#34;- name:AD_SERVICE_ADDRvalue:\u0026#34;adservice:9555\u0026#34;- name:ENV_PLATFORMvalue:\u0026#34;gcp\u0026#34;resources:requests:cpu:100mmemory:64Milimits:cpu:200mmemory:128Mi将上述 YAML 保存为 frontend-v1.yaml，然后用 kubectl apply -f frontend-v1.yaml 创建它。\n如果我们在浏览器中打开 INGRESS_HOST，我们仍然会看到原始版本的前端。让我们更新 VirtualService 中的权重，开始将 30% 的流量路由到 v1 的子集。\napiVersion:networking.istio.io/v1alpha3kind:VirtualServicemetadata:name:frontend-ingressspec:hosts:- \u0026#39;*\u0026#39;gateways:- frontend-gatewayhttp:- route:- destination:host:frontendport:number:80subset:originalweight:70- destination:host:frontendport:number:80subset:v1weight:30将上述 YAML 保存为 frontend-30.yaml，然后用 kubectl apply -f frontend-30.yaml 更新 VirtualService。\n如果我们刷新几次网页，我们会注意到来自前端 v1 的更新标头，看起来像下图中的样子。\n   更新后的Header  如果我们打开 Kiali 并点击 Graph，我们会发现有两个版本的前端在运行，如下图所示。\n   在 Kiali 中有两个版本  ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"98434b85c0301170da9e09e4f6b67a12","permalink":"https://lib.jimmysong.io/istio-handbook/practice/traffic-routing/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/istio-handbook/practice/traffic-routing/","section":"istio-handbook","summary":"我们已经建立了一个新的 Docker 镜像，它使用了与当前运行的前端服务不同的标头。让我们看看如何部署所需的资源并将一定比例的流量路由到不同的前端服务版本。 在我们创建任何资源之前，让我们删除现有的前端部署（kube","tags":["Istio","Service Mesh"],"title":"路由流量","type":"book"},{"authors":null,"categories":["Envoy"],"content":"/logging 端点启用或禁用特定组件或所有记录器的不同日志级别。\n要列出所有的记录器，我们可以向 /logging 端点发送一个 POST 请求。\n$ curl -X POST localhost:9901/logging active loggers: admin: info alternate_protocols_cache: info aws: info assert: info backtrace: info cache_filter: info client: info config: info ... 输出将包含记录器的名称和每个记录器的日志级别。要改变所有活动日志记录器的日志级别，我们可以使用 level 参数。例如，我们可以运行下面的程序，将所有日志记录器的日志记录级别改为 debug。\n$ curl -X POST localhost:9901/logging?level=debug active loggers: admin: debug alternate_protocols_cache: debug aws: debug assert: debug backtrace: debug cache_filter: debug client: debug config: debug ... 要改变某个日志记录器的级别，我们可以用日志记录器的名称替换 level 查询参数名称。例如，要将 admin 日志记录器级别改为 warning，我们可以运行以下程序。\n$ curl -X POST localhost:9901/logging?admin=warning active loggers: admin: warning alternate_protocols_cache: info aws: info assert: info backtrace: info cache_filter: info client: info config: info 为了触发所有访问日志的重新开放，我们可以向 /reopen_logs 端点发送一个 POST 请求。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"0e2e0bc1c2e44127b133a36c180cc526","permalink":"https://lib.jimmysong.io/envoy-handbook/admin-interface/logging/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/envoy-handbook/admin-interface/logging/","section":"envoy-handbook","summary":"/logging 端点启用或禁用特定组件或所有记录器的不同日志级别。 要列出所有的记录器，我们可以向 /logging 端点发送一个 POST 请求。 $ curl -X POST localhost:9901/logging active loggers: admin: info alternate_protocols_cache: info aws: info assert: info backtrace: info cache_filter: info client: info config: info ... 输出将包含记录器的名称和每个记录器的日志级别。要","tags":["Envoy"],"title":"日志","type":"book"},{"authors":null,"categories":["Envoy"],"content":"我们将在这个实验中创建一个动态的 Envoy 配置，并通过单独的配置文件配置监听器、集群、路由和端点。\n让我们从最小的 Envoy 配置开始。\nnode:cluster:cluster-1id:envoy-instance-1dynamic_resources:lds_config:path:./lds.yamlcds_config:path:./cds.yamladmin:address:socket_address:address:127.0.0.1port_value:9901access_log:- name:envoy.access_loggers.filetyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.access_loggers.stream.v3.StdoutAccessLog将上述 YAML 保存到 envoy-proxy-1.yaml 文件。我们还需要创建空的（暂时的）cds.yaml 和 lds.yaml 文件。\ntouch {cds,lds}.yaml 我们现在可以用这个配置来运行 Envoy 代理：func-e run -c envoy-proxy-1.yaml。如果我们看一下生成的配置（比如 localhost:9901/config_dump），我们会发现它是空的，因为我们没有提供任何监听器或集群。\n接下来让我们创建监听器和路由配置。\nversion_info:\u0026#34;0\u0026#34;resources:- \u0026#34;@type\u0026#34;: type.googleapis.com/envoy.config.listener.v3.Listenername:listener_0address:socket_address:address:0.0.0.0port_value:10000filter_chains:- filters:- name:envoy.filters.network.http_connection_managertyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManagerstat_prefix:listener_httphttp_filters:- name:envoy.filters.http.routerrds:route_config_name:route_config_1config_source:path:./rds.yaml创建一个空的 rds.yaml 文件（touch rds.yaml），并将上述 YAML 保存为 lds.yaml。因为 Envoy 只关注文件路径的移动，所以保存文件不会触发配置重载。为了触发重载，让我们覆盖 lds.yaml 文件。\nmv lds.yaml tmp; mv tmp lds.yaml 上述命令触发了重载，我们应该从 Envoy 得到以下日志条目：\n[2021-09-07 19:04:06.710][2113][info][upstream] [source/server/lds_api.cc:78] lds: add/update listener \u0026#39;listener_0\u0026#39; 同样，如果我们向 localhost:10000 发送请求，我们会得到一个 HTTP 404。\n接下来让我们创建 rds.yaml 的内容。\nversion_info:\u0026#34;0\u0026#34;resources:- \u0026#34;@type\u0026#34;: type.googleapis.com/envoy.config.route.v3.RouteConfigurationname:route_config_1virtual_hosts:- name:vhdomains:[\u0026#34;*\u0026#34;]routes:- match:prefix:\u0026#34;/headers\u0026#34;route:cluster:instance_1强制重载：\nmv rds.yaml tmp; mv tmp rds.yaml 最后，我们还需要对集群进行配置。在这之前，让我们运行一个 httpbin 容器。\ndocker run -d -p 5050:80 kennethreitz/httpbin 现在我们更新集群（cds.yaml）并强制重载（mv cds.yaml tmp; mv tmp cds.yaml）：\nversion_info:\u0026#34;0\u0026#34;resources:- \u0026#34;@type\u0026#34;: type.googleapis.com/envoy.config.cluster.v3.Clustername:instance_1connect_timeout:5sload_assignment:cluster_name:instance_1endpoints:- lb_endpoints:- endpoint:address:socket_address:address:127.0.0.1port_value:5050当 Envoy 更新配置时，我们会得到以下日志条目：\n$ [2021-09-07 19:09:15.582][2113][info][upstream] [source/common/upstream/cds_api_helper.cc:65] cds: added/updated 1 cluster(s), skipped 0 unmodified cluster(s) 现在我们可以提出请求并验证流量是否到达集群中定义的端点。\n$ curl localhost:10000/headers { \u0026#34;headers\u0026#34;: { \u0026#34;Accept\u0026#34;: \u0026#34;*/*\u0026#34;, \u0026#34;Host\u0026#34;: \u0026#34;localhost:10000\u0026#34;, \u0026#34;User-Agent\u0026#34;: \u0026#34;curl/7.64.0\u0026#34;, \u0026#34;X-Envoy-Expected-Rq-Timeout-Ms\u0026#34;: \u0026#34;15000\u0026#34; } } 注意我们是如何将端点与集群配置在同一个文件中的。我们可以通过单独定义端点（eds.yaml）将两者分开。\n让我们从创建 eds.yaml 文件开始：\nversion_info:\u0026#34;0\u0026#34;resources:- \u0026#34;@type\u0026#34;: type.googleapis.com/envoy.config.endpoint.v3.ClusterLoadAssignmentcluster_name:instance_1endpoints:- lb_endpoints:- endpoint:address:socket_address:address:127.0.0.1port_value:5050将上述 YAML 保存为 eds.yaml。\n为了使用这个端点文件，我们需要更新集群（cds.yaml）以读取 eds.yaml 中的端点。\nversion_info:\u0026#34;0\u0026#34;resources:- \u0026#34;@type\u0026#34;: type.googleapis.com/envoy.config.cluster.v3.Clustername:instance_1connect_timeout:5stype:EDSeds_cluster_config:eds_config:path:./eds.yaml通过运行 mv cds.yaml tmp; mv tmp cds.yaml 来强制重载。Envoy 会重新加载配置，我们就可以像以前一样向 localhost:10000/headers 发送请求。现在的区别是，不同的配置在不同的文件中，可以分别更新。\n 下一章   ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"ce74faf0f212ecc3473896d9723550a4","permalink":"https://lib.jimmysong.io/envoy-handbook/dynamic-config/lab8/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/envoy-handbook/dynamic-config/lab8/","section":"envoy-handbook","summary":"我们将在这个实验中创建一个动态的 Envoy 配置，并通过单独的配置文件配置监听器、集群、路由和端点。 让我们从最小的 Envoy 配置开始。 node:cluster:cluster-1id:envoy-instance-1dynamic_resources:lds_config:path:./lds.yamlcds_config:path:./cds.yamladmin:address:socket_address:address:127.0.0.1port_value:9901access_log:- name:envoy.access_loggers.filetyped_config:\"@type\": type.googleapis.com/envoy.extensions.acc","tags":["Envoy"],"title":"实验 8：来自文件系统的动态配置","type":"book"},{"authors":null,"categories":["Envoy"],"content":"在这个实验中，我们将编写一个 Lua 脚本，为响应头添加一个头，并使用一个文件中定义的全局脚本。\n我们将创建一个 Envoy 配置和一个 Lua 脚本，在响应句柄上添加一个头。由于我们不会使用请求路径，所以我们不需要定义 envoy_on_request 函数。响应函数看起来像这样。\nfunction envoy_on_response(response_handle) response_handle:headers():add(\u0026#34;hello\u0026#34;, \u0026#34;world\u0026#34;) end 我们在从 headers() 函数返回的 header 对象上调用 add(\u0026lt;header-name\u0026gt;, \u0026lt;header-value\u0026gt;) 函数。\n让我们在 Envoy 配置中内联定义这个脚本。为了简化配置，我们将使用 direct_response，而不是集群。\nstatic_resources:listeners:- name:mainaddress:socket_address:address:0.0.0.0port_value:10000filter_chains:- filters:- name:envoy.filters.network.http_connection_managertyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManagerstat_prefix:ingress_httproute_config:name:some_routevirtual_hosts:- name:some_servicedomains:- \u0026#34;*\u0026#34;routes:- match:prefix:\u0026#34;/\u0026#34;direct_response:status:200body:inline_string:\u0026#34;200\u0026#34;http_filters:- name:envoy.filters.http.luatyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.http.lua.v3.Luainline_code:|function envoy_on_response(response_handle) response_handle:headers():add(\u0026#34;hello\u0026#34;, \u0026#34;world\u0026#34;) end- name:envoy.filters.http.router将上述 YAML 保存为 8-lab-1-lua-script.yaml 并运行它。\nfunc-e run -c 8-Lab-1-lua-script.yaml \u0026amp; 为了测试这个功能，我们可以向 localhost:10000 发送一个请求，并检查响应头。\n$ curl -v localhost:10000 ... \u0026gt; GET / HTTP/1.1 \u0026gt; Host: localhost:10000 \u0026gt; User-Agent: curl/7.64.0 \u0026gt; Accept: */* \u0026gt; \u0026lt; HTTP/1.1 200 OK \u0026lt; content-length: 3 \u0026lt; content-type: text/plain \u0026lt; hello: world \u0026lt; date: Tue, 23 Nov 2021 21:37:01 GMT \u0026lt; server: envoy \u0026lt; 200 输出应该包括我们在其他标准头文件中添加的 hello: world header。\n让我们来看看一个更复杂的情况。对于传入的请求，我们要检查它们是否有一个叫做 my-request-id 的头，如果这个头不存在，那么我们要为所有 GET 请求添加一个 my-request-id 头。\n因为我们想在 envoy_on_response 函数中检查方法和一个头，我们将使用动态元数据在 envoy_on_request 函数中存储这些值。然后，在响应函数中，我们可以读取元数据，检查头是否被设置，方法是否为 GET，并添加 my-request-id 头。\n下面是代码的样子。\nfunction envoy_on_request(request_handle) local headers = request_handle:headers() local metadata = request_handle:streamInfo():dynamicMetadata() metadata:set(\u0026#34;envoy.filters.http.lua\u0026#34;, \u0026#34;requestInfo\u0026#34;, { requestId = headers:get(\u0026#34;my-request-id\u0026#34;), method = headers:get(\u0026#34;:method\u0026#34;), }) end function envoy_on_response(response_handle) local requestInfoObj = response_handle:streamInfo():dynamicMetadata():get(\u0026#34;envoy.filters.http.lua\u0026#34;)[\u0026#34;requestInfo\u0026#34;] local requestId = requestInfoObj.requestId local method = requestInfoObj.method if (requestId == nil or requestId == \u0026#39;\u0026#39;) and (method == \u0026#39;GET\u0026#39;) then response_handle:logInfo(\u0026#34;Adding request ID header\u0026#34;) response_handle:headers():add(\u0026#34;my-request-id\u0026#34;, \u0026#34;some_id_here\u0026#34;) end end 注意，目前我们使用 some_id_here 作为 my-request-id的值，以后我们会创建一个函数，为我们生成一个 ID。下面是完整的 Envoy 配置的样子。\nstatic_resources:listeners:- name:mainaddress:socket_address:address:0.0.0.0port_value:10000filter_chains:- filters:- name:envoy.filters.network.http_connection_managertyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManagerstat_prefix:ingress_httproute_config:name:some_routevirtual_hosts:- name:some_servicedomains:- \u0026#34;*\u0026#34;routes:- match:prefix:\u0026#34;/\u0026#34;direct_response:status:200body:inline_string:\u0026#34;200\u0026#34;http_filters:- name:envoy.filters.http.luatyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.http.lua.v3.Luainline_code:|function envoy_on_request(request_handle) local headers = request_handle:headers() local metadata = request_handle:streamInfo():dynamicMetadata() metadata:set(\u0026#34;envoy.filters.http.lua\u0026#34;, \u0026#34;requestInfo\u0026#34;, { requestId = headers:get(\u0026#34;my-request-id\u0026#34;), method = headers:get(\u0026#34;:method\u0026#34;), }) end function envoy_on_response(response_handle) local requestInfoObj = response_handle:streamInfo():dynamicMetadata():get(\u0026#34;envoy.filters.http.lua\u0026#34;)[\u0026#34;requestInfo\u0026#34;] local requestId = requestInfoObj.requestId local method = requestInfoObj.method if (requestId == nil or requestId == \u0026#39;\u0026#39;) and (method == \u0026#39;GET\u0026#39;) then response_handle:logInfo(\u0026#34;Adding request ID header\u0026#34;) response_handle:headers():add(\u0026#34;my-request-id\u0026#34;, \u0026#34;some_id_here\u0026#34;) end end- name:envoy.filters.http.router将上述 YAML 保存为 8-lab-1-lua-script-1.yaml 并运行它。\nfunc-e run -c 8-Lab-1-lua-script-1.yaml \u0026amp; 让我们试一试几种情况。首先，我们将发送一个没有设置 my-request-id 头的 GET 请求。\n$ curl -v localhost:10000 ... [2021-11-23 22:59:35.932][2258][info][lua] [source/extensions/filters/http/lua/lua_filter.cc:795] script log: Adding request ID header \u0026lt; HTTP/1.1 200 OK \u0026lt; content-length: 3 \u0026lt; content-type: text/plain \u0026lt; my-request-id: some_id_here \u0026lt; date: Tue, 23 Nov 2021 22:59:35 GMT \u0026lt; server: envoy \u0026lt; * Connection #0 to host localhost left intact 200 我们知道 Lua 代码运行了，因为我们看到了日志条目和 my-request-id 头的设置。\n让我们试着发送一个 POST 请求。\n$ curl -X POST -v localhost:10000 ... \u0026gt; POST / HTTP/1.1 \u0026gt; Host: localhost:10000 \u0026gt; User-Agent: curl/7.64.0 \u0026gt; Accept: */* \u0026gt; \u0026lt; HTTP/1.1 200 OK \u0026lt; content-length: 3 \u0026lt; content-type: text/plain \u0026lt; date: Mon, 29 Nov 2021 23:49:58 GMT \u0026lt; server: envoy 注意到头信息中没有包括 my-request-id 头信息。最后，让我们也尝试发送一个 GET 请求，但也提供 my-request-id 头。在这种情况下，my-request-id 头信息也不应该被包含在响应中。\n$ curl -v -H \u0026#34;my-request-id: something\u0026#34; localhost:10000 ... \u0026gt; GET / HTTP/1.1 \u0026gt; Host: localhost:10000 \u0026gt; User-Agent: curl/7.64.0 \u0026gt; Accept: */* \u0026gt; my-request-id: …","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"fe6e1226451d8bd7b2653716a7650905","permalink":"https://lib.jimmysong.io/envoy-handbook/extending-envoy/lab16/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/envoy-handbook/extending-envoy/lab16/","section":"envoy-handbook","summary":"在这个实验中，我们将编写一个 Lua 脚本，为响应头添加一个头，并使用一个文件中定义的全局脚本。 我们将创建一个 Envoy 配置和一个 Lua 脚本，在响应句柄上添加一个头。由于我们不会使用请求路径，所以我们不需要定义 envoy_on_request 函数。响","tags":["Envoy"],"title":"实验16：使用 Lua 脚本扩展 Envoy","type":"book"},{"authors":null,"categories":["Envoy"],"content":"原始目的地过滤器（envoy.filters.listener.original_dst）会读取 SO_ORIGINAL_DST 套接字选项。当一个连接被 iptables REDIRECT 或 TPROXY 目标（如果transparent选项被设置）重定向时，这个选项被设置。该过滤器可用于与 ORIGINAL_DST 类型的集群连接。\n当使用 ORIGINAL_DST 集群类型时，请求会被转发到由重定向元数据寻址的上游主机，而不做任何主机发现。因此，在集群中定义任何端点都是没有意义的，因为端点是从原始数据包中提取的，并不是由负载均衡器选择。\n我们可以将 Envoy 作为一个通用代理，使用这种集群类型将所有请求转发到原始目的地。\n要使用 ORIGINAL_DST 集群，流量需要通过 iptables REDIRECT 或 TPROXY 目标到达 Envoy。\n...listener_filters:- name:envoy.filters.listener.original_dsttyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.listener.original_dst.v3.OriginalDst...clusters:- name:original_dst_clusterconnect_timeout:5stype:ORIGNAL_DSTlb_policy:CLUSTER_PROVIDED ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"2896c1931786e37a2e3b0d7a84390621","permalink":"https://lib.jimmysong.io/envoy-handbook/listener/original-destination-listener-filter/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/envoy-handbook/listener/original-destination-listener-filter/","section":"envoy-handbook","summary":"原始目的地过滤器（envoy.filters.listener.original_dst）会读取 SO_ORIGINAL_DST 套接字选项。当一个连接被 iptables REDIRECT 或 TPROXY 目标（如果transparent选项被设置）重定向时，这个选项被设置。","tags":["Envoy"],"title":"原始目的地监听器过滤器","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"Ingress 是从 Kubernetes 集群外部访问集群内部服务的入口，是将 Kubernetes 集群内部服务暴露到外界的几种方式之一。本文将为你详细介绍 Ingress 资源对象。\n术语\n在本篇文章中你将会看到一些在其他地方被交叉使用的术语，为了防止产生歧义，我们首先来澄清下。\n 节点：Kubernetes 集群中的一台物理机或者虚拟机。 集群：位于 Internet 防火墙后的节点，这是 Kubernetes 管理的主要计算资源。 边界路由器：为集群强制执行防火墙策略的路由器。 这可能是由云提供商或物理硬件管理的网关。 集群网络：一组逻辑或物理链接，可根据 Kubernetes 网络模型 实现群集内的通信。 集群网络的实现包括 Overlay 模型的 flannel 和基于 SDN 的 OVS。 服务：使用标签选择器标识一组 pod 成为的 Kubernetes 服务。 除非另有说明，否则服务假定在集群网络内仅可通过虚拟 IP 访问。  什么是 Ingress？ 通常情况下，service 和 pod 仅可在集群内部网络中通过 IP 地址访问。所有到达边界路由器的流量或被丢弃或被转发到其他地方。从概念上讲，可能像下面这样：\n internet | ------------ [Services] Ingress 是授权入站连接到达集群服务的规则集合。\n internet | [Ingress] --|-----|-- [Services] 你可以给 Ingress 配置提供外部可访问的 URL、负载均衡、SSL、基于名称的虚拟主机等。用户通过 POST Ingress 资源到 API server 的方式来请求 ingress。 Ingress controller 负责实现 Ingress，通常使用负载均衡器，它还可以配置边界路由和其他前端，这有助于以高可用的方式处理流量。\n先决条件 在使用 Ingress 资源之前，有必要先了解下面几件事情。\n 你需要一个 Ingress Controller 来实现 Ingress，单纯的创建一个 Ingress 没有任何意义。 GCE/GKE 会在 master 节点上部署一个 ingress controller。你可以在一个 pod 中部署任意个自定义的 ingress controller。你必须正确地注解每个 ingress，比如运行多个 ingress controller 和关闭 glbc。 在非 GCE/GKE 的环境中，你需要在 pod 中部署一个 controller，例如 Nginx Ingress Controller。  Ingress 资源 最简化的 Ingress 配置如下。\n1: apiVersion:extensions/v1beta12: kind:Ingress3:metadata:4: name:test-ingress5:spec:6:rules:7:- http:8:paths:9: - path:/testpath10:backend:11: serviceName:test12: servicePort:80如果你没有配置 Ingress controller 就将其 POST 到 API server 不会有任何用处。\n配置说明\n1-4 行：跟 Kubernetes 的其他配置一样，ingress 的配置也需要 apiVersion，kind 和 metadata 字段。配置文件的详细说明请查看 部署应用，配置容器 和使用资源。\n5-7 行: Ingress spec 中包含配置一个 loadbalancer 或 proxy server 的所有信息。最重要的是，它包含了一个匹配所有入站请求的规则列表。目前 ingress 只支持 http 规则。\n8-9 行：每条 http 规则包含以下信息：一个 host 配置项（比如 for.bar.com，在这个例子中默认是 *），path 列表（比如：/testpath），每个 path 都关联一个 backend(比如 test:80)。在 loadbalancer 将流量转发到 backend 之前，所有的入站请求都要先匹配 host 和 path。\n10-12 行：正如 services doc 中描述的那样，backend 是一个 service:port 的组合。Ingress 的流量被转发到它所匹配的 backend。\n全局参数：为了简单起见，Ingress 示例中没有全局参数，请参阅资源完整定义的 API 参考。 在所有请求都不能跟 spec 中的 path 匹配的情况下，请求被发送到 Ingress controller 的默认后端，可以指定全局缺省 backend。\nIngressClass Ingress 可以由不同的控制器实现，通常使用不同的配置。 每个 Ingress 应当指定一个类，也就是一个对 IngressClass 资源的引用。 IngressClass 资源包含额外的配置，其中包括应当实现该类的控制器名称。下面是一个 IngressClass 示例。\napiVersion:networking.k8s.io/v1kind:IngressClassmetadata:name:external-lbspec:controller:example.com/ingress-controllerparameters:apiGroup:k8s.example.comkind:IngressParametersname:external-lbIngressClass 中的 .spec.parameters 字段可用于引用其他资源以提供额外的相关配置。\n参数（parameters）的具体类型取决于你在 .spec.controller 字段中指定的 Ingress 控制器。\nIngressClass 的作用域 IngressClass 的作用域取决于你的 Ingress 控制器，可以使用集群范围也可以是某个命名空间。\nIngressClass 的默认作用于是集群级别，关于 IngressClass 作用域的详细使用说明请见 Ingress 文档。\n默认 IngressClass 你可以将一个特定的 IngressClass 标记为集群默认 IngressClass。 将一个 IngressClass 资源的 ingressclass.kubernetes.io/is-default-class 注解设置为 true 将确保新的未指定 ingressClassName 字段的 Ingress 能够分配为这个默认的 IngressClass。集群中最多只能有一个 IngressClass 被标记为默认。\nkubernetes.io/ingress.class 注解 在 Kubernetes 1.18 版本引入 IngressClass 资源和 ingressClassName 字段之前，Ingress 类是通过 Ingress 中的一个 kubernetes.io/ingress.class 注解来指定的。 这个注解从未被正式定义过，但是得到了 Ingress 控制器的广泛支持。\ningressClassName 配置项是该注解的替代品，但并不完全等价。 该注解通常用于引用实现该 Ingress 的控制器的名称，而这个新的字段则是对一个包含额外 Ingress 配置的 IngressClass 资源的引用，包括 Ingress 控制器的名称。\nIngress 类型 以下文档描述了 Ingress 资源中公开的一组跨平台功能。 理想情况下，所有的 Ingress controller 都应该符合这个规范，但是目前还没有实现。\n\n  确保您查看控制器特定的文档，以便您了解每个文档的注意事项。   单 Service Ingress Kubernetes 中已经存在一些概念可以暴露单个 service（查看 替代方案），但是你仍然可以通过 Ingress 来实现，通过指定一个没有 rule 的默认 backend 的方式。\ningress.yaml 定义文件：\napiVersion:extensions/v1beta1kind:Ingressmetadata:name:test-ingressspec:backend:serviceName:testsvcservicePort:80使用kubectl create -f命令创建，然后查看 ingress：\n$ kubectl get ing NAME RULE BACKEND ADDRESS test-ingress - testsvc:80 107.178.254.228 107.178.254.228 就是 Ingress controller 为了实现 Ingress 而分配的 IP 地址。RULE 列表示所有发送给该 IP 的流量都被转发到了 BACKEND 所列的 Kubernetes service 上。\n简单展开 如前面描述的那样，kubernetes pod 中的 IP 只在集群网络内部可见，我们需要在边界设置一个东西，让它能够接收 ingress 的流量并将它们转发到正确的端点上。这个东西一般是高可用的 loadbalancer。使用 Ingress 能够允许你将 loadbalancer 的个数降低到最少，例如，假如你想要创建这样的一个设置：\nfoo.bar.com -\u0026gt; 178.91.123.132 -\u0026gt; /foo s1:80 /bar s2:80 你需要一个这样的 ingress：\napiVersion:extensions/v1beta1kind:Ingressmetadata:name:testspec:rules:- host:foo.bar.comhttp:paths:- path:/foobackend:serviceName:s1servicePort:80- path:/barbackend:serviceName:s2servicePort:80使用 kubectl create -f 创建完 ingress 后：\n$ kubectl get ing NAME RULE BACKEND ADDRESS test - foo.bar.com /foo s1:80 /bar s2:80 只要服务（s1，s2）存在，Ingress controller 就会将提供一个满足该 Ingress 的特定 loadbalancer 实现。 这一步完成后，您将在 Ingress 的最后一列看到 loadbalancer 的地址。\n基于名称的虚拟主机 Name-based 的虚拟主机在同一个 IP 地址下拥有多个主机名。\nfoo.bar.com --| |-\u0026gt; foo.bar.com s1:80 | 178.91.123.132 | bar.foo.com --| |-\u0026gt; bar.foo.com s2:80 下面这个 ingress 说明基于 Host header 的后端 loadbalancer 的路由请求：\napiVersion:extensions/v1beta1kind:Ingressmetadata:name:testspec:rules:- host:foo.bar.comhttp:paths:- backend:serviceName:s1servicePort:80- host:bar.foo.comhttp:paths:- backend:serviceName:s2servicePort:80默认 backend：一个没有 rule 的 ingress，如前面章节中所示，所有流量都将发送到一个默认 backend。你可以用该技巧通知 loadbalancer 如何找到你网站的 404 页面，通过制定一些列 rule 和一个默认 backend 的方式。 …","date":1653062400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"9858936957c13fa2c225353805a5b16f","permalink":"https://lib.jimmysong.io/kubernetes-handbook/service-discovery/ingress/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/service-discovery/ingress/","section":"kubernetes-handbook","summary":"Ingress 是从 Kubernetes 集群外部访问集群内部服务的入口，是将 Kubernetes 集群内部服务暴露到外界的几种方式之一。本文将为你详细介绍 Ingress 资源对象。 术语 在本篇文章中你将会看到一些在其他地方被交叉使用的术语，为了防止产生歧义，我们首先来","tags":["Kubernetes"],"title":"Ingress","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"除了直接使用 Service 和 Ingress 之外，Kubernetes 社区还发起了 Gateway API 项目，这是一个 CRD，可以帮助我们将 Kubernetes 中的服务暴露到集群外。\nGateway API 是一个由 SIG-NETWORK 管理的开源项目。该项目的目标是在 Kubernetes 生态系统中发展服务网络 API。Gateway API 提供了暴露 Kubernetes 应用的接口 ——Service、Ingress 等。\n该 API 在 Istio 中也被应用，用于将 Kubernetes 中的服务暴露到服务网格之外。\n目标 Gateway API 旨在通过提供表现性的、可扩展的、面向角色的接口来改善服务网络，这些接口由许多厂商实现，并得到了业界的广泛支持。\nGateway API 是一个 API 资源的集合 —— Service、GatewayClass、Gateway、HTTPRoute、TCPRoute 等。使用这些资源共同为各种网络用例建模。\n下图中展示的是 Kubernetes 集群中四层和七层的网络配置。从图中可以看到通过将这些资源对象分离，可以实现配置上的解耦，由不同角色的人员来管理。\n   Kubernetes Gateway API 简介  Gateway 相较于 Ingress 做了哪些改进？ 更具表现力\nGateway 表达了更多的核心功能，比如基于头的匹配、流量加权和其他功能，而这些功能在 Ingress 中只能通过自定义方式实现。\n更具扩展性\nGateway API 允许在 API 的各个层次上链接自定义资源。这就允许在 API 结构的适当位置进行更精细的定制。\n面向角色\n它们被分离成不同的 API 资源，这些资源映射到 Kubernetes 上运行应用程序的常见角色。\n通用性\n这不是一种改进，而是应该保持不变。正如 Ingress 是一个具有众多实现的通用规范一样，Gateway API 被设计成一个由许多实现支持的可移植规范。\n共享网关\n它们允许独立的路由资源绑定到同一个网关，从而实现负载均衡器和 VIP 的共享。这允许团队安全地共享基础设施，而不需要直接协调。\n类型化后端引用\n通过类型化后端引用，Routes 可以引用 Kubernetes 服务，也可以引用任何一种被设计为 Gateway 后端的 Kubernetes 资源。\n跨命名空间引用\n跨越不同 Namespaces 的路由可以绑定到网关。这样，尽管对工作负载进行了命名空间划分，但仍可共享网络基础设施。\n类\nGatewayClasses 将负载均衡实现的类型形式化。这些类使用户可以很容易和明确地了解资源模型本身有什么样的能力。\n在了解了 Gateway API 的目的后，接下来我们再看下它的资源模型、请求流程、TLS 配置及扩展点等。\n角色 Gateway API 开发者为其使用场景定义四类角色：\n 基础设施提供方：如 AWS、GKE 等 集群运维：管理整个集群的计算、存储、网络、安全等 应用程序开发者：为自己开发的应用负责，管理应用的健壮性 应用管理员：不是所有的公司都有，通常在一些复杂系统中会有专门的应用管理员  资源模型 注意：资源最初将作为 CRD 存在于 networking.x-k8s.io API 组中。未限定的资源名称将隐含在该 API 组中。\nGateway API 的资源模型中，主要有三种类型的对象：\n GatewayClass：定义了一组具有共同配置和行为的网关。 Gateway：请求一个点，在这个点上，流量可以被翻译到集群内的服务。 Route：描述了通过 Gateway 而来的流量如何映射到服务。  GatewayClass GatewayClass 定义了一组共享共同配置和行为的 Gateway，每个 GatewayClass 由一个控制器处理，但控制器可以处理多个 GatewayClass。\nGatewayClass 是一个集群范围的资源。必须至少定义一个 GatewayClass，Gateway 才能够生效。实现 Gateway API 的控制器通过关联的 GatewayClass 资源来实现，用户可以在自己的 Gateway 中引用该资源。\n这类似于 Ingress 的 IngressClass] 和 PersistentVolumes 的 StorageClass。在 Ingress v1beta1 中，最接近 GatewayClass 的是 ingress-class 注解，而在 IngressV1 中，最接近的类似物是 IngressClass 对象。\nGateway Gateway 描述了如何将流量翻译到集群内的服务。也就是说，它定义了一个方法，将流量从不了解 Kubernetes 的地方翻译到了解 Kubernetes 的地方。例如，由云负载均衡器、集群内代理或外部硬件负载均衡器发送到 Kubernetes 服务的流量。虽然许多用例的客户端流量源自集群的 “外部”，但这并不强求。\nGateway 定义了对实现 GatewayClass 配置和行为合同的特定负载均衡器配置的请求。该资源可以由运维人员直接创建，也可以由处理 GatewayClass 的控制器创建。\n由于 Gateway 规范捕获了用户意图，它可能不包含规范中所有属性的完整规范。例如，用户可以省略地址、端口、TLS 设置等字段。这使得管理 GatewayClass 的控制器可以为用户提供这些设置，从而使规范更加可移植。这种行为将通过 GatewayClass 状态对象来明确。\n一个 Gateway 可以包含一个或多个 Route 引用，这些 Route 引用的作用是将一个子集的流量引导到一个特定的服务上。\n{HTTP,TCP,Foo} Route Route 对象定义了特定协议的规则，用于将请求从 Gateway 映射到 Kubernetes 服务。\nHTTPRoute 和 TCPRoute 是目前唯一已定义的 Route 对象。未来可能会添加其他特定协议的 Route 对象。\nBackendPolicy BackendPolicy 提供了一种配置 Gateway 和后端之间连接的方法。在这个 API 中，后端是指路由可以转发流量的任何资源。后端的一个常见例子是 Service。这个级别的配置目前仅限于 TLS，但将来会扩展到支持更高级的策略，如健康检查。\n一些后端配置可能会根据针对后端的 Route 而有所不同。在这些情况下，配置字段将放在 Route 上，而不是 BackendPolicy 上。\n路由绑定 当 Route 绑定到 Gateway 时，代表应用在 Gateway 上的配置，配置了底层的负载均衡器或代理。哪些 Route 如何绑定到 Gateway 是由资源本身控制的。Route 和 Gateway 资源具有内置的控制，以允许或限制它们之间如何相互选择。这对于强制执行组织政策以确定 Route 如何暴露以及在哪些 Gateway 上暴露非常有用。看下下面的例子。\n一个 Kubernetes 集群管理员在 Infra 命名空间中部署了一个名为 shared-gw 的 Gateway，供不同的应用团队使用，以便将其应用暴露在集群之外。团队 A 和团队 B（分别在命名空间 “A” 和 “B” 中）将他们的 Route 绑定到这个 Gateway。它们互不相识，只要它们的 Route 规则互不冲突，就可以继续隔离运行。团队 C 有特殊的网络需求（可能是性能、安全或关键性），他们需要一个专门的 Gateway 来代理他们的应用到集群外。团队 C 在 “C” 命名空间中部署了自己的 Gateway specialive-gw，该 Gateway 只能由 “C” 命名空间中的应用使用。\n不同命名空间及 Gateway 与 Route 的绑定关系如下图所示。\n   路由绑定示意图  在如何将路由与网关绑定以实现不同的组织政策和责任范围方面，有很大的灵活性。下面是网关和路由之间可能的对应关系：\n 一对一：网关和路由可以由一个所有者部署和使用，并具有一对一的关系。团队 C 就是一个例子。 一对多：一个网关可以有许多路由与之绑定，这些路由由来自不同命名空间的不同团队所拥有。团队 A 和 B 就是这样的一个例子。 多对一：路由也可以绑定到多个网关，允许一个路由同时控制不同 IP、负载均衡器或网络上的应用暴露。  总之，网关选择路由，路由控制它们的暴露。当网关选择一个允许自己暴露的路由时，那么该路由将与网关绑定。当路由与网关绑定时，意味着它们的集体路由规则被配置在了由该网关管理的底层负载均衡器或代理服务器上。因此，网关是一个网络数据平面的逻辑表示，可以通过路由进行配置。\n路由选择 Gateway 根据 Route 元数据，特别是 Route 资源的种类、命名空间和标签来选择 Route。Route 实际上被绑定到 Gateway 中的特定监听器上，因此每个监听器都有一个 listener.routes 字段，它通过以下一个或多个标准来选择 Route。\n Label：Gateway 可以通过资源上存在的标签来选择 Route（类似于 Service 通过 Pod 标签选择 Pod 的方式）。 Kind：网关监听器只能选择单一类型的路由资源。可以是 HTTPRoute、TCPRoute 或自定义 Route 类型。 Namespace：Gateway 还可以通过 namespaces.from 字段控制可以从哪些 Namespace、 Route 中选择。它支持三种可能的值。  SameNamespace 是默认选项。只有与该网关相同的命名空间中的路由才会被选择。 All 将选择来自所有命名空间的 Route。 Selector 意味着该网关将选择由 Namespace 标签选择器选择的 Namespace 子集的 Route。当使用 Selector 时，那么 listeners.route.namespaces.selector 字段可用于指定标签选择器。All 或 SameNamespace 不支持该字段。    下面的 Gateway 将在集群中的所有 Namespace 中选择 expose: prod-web-gw 的所有 HTTPRoute 资源。\nkind:Gateway...spec:listeners:- routes:kind:HTTPRouteselector:matchLabels:expose:prod-web-gw namespaces:from:All路由暴露 路由可以决定它们如何通过网关暴露。gateways.allow 字段支持三个值。\n All：如果没有指定，则是默认值。这使得所有的 Route 标签和 Namespace 选择器都绑定在网关上。 SameNamespace 只允许该路由与来自同一 Namespace 的网关绑定。 FromList 允许指定一个明确的网关列表，以便路由与之绑定。  下面的 my-route Route 只选择 foo-namespace 中的 foo-gateway，而不能与其他 Gateway 绑定。注意，foo-gateway 与 my-route 在不同的 Namespace 中。如果 foo-gateway 允许跨 Namespace 绑定，并且也选择了这个 Route，那么 my-route 就会与之绑定。\nkind:HTTPRoutemetadata:name:my-routenamespace:bar-namespacespec:gateways:allow:FromListgatewayRefs:- name:foo-gatewaynamespace:foo-namespace请注意，网关和路由的绑定是双向的。这意味着两个资源必须相互选择才能绑定。如果一个Gateway的Route标签选择器不匹配任何现有 …","date":1653062400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"a812253b3484adb64c3e2ce0da208287","permalink":"https://lib.jimmysong.io/kubernetes-handbook/service-discovery/gateway/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/service-discovery/gateway/","section":"kubernetes-handbook","summary":"除了直接使用 Service 和 Ingress 之外，Kubernetes 社区还发起了 Gateway API 项目，这是一个 CRD，可以帮助我们将 Kubernetes 中的服务暴露到集群外。 Gateway API 是一个由 SIG-NETWORK 管理的开源项目。该项目的目标是在 Kubernetes 生态系统中发展服务网络 API。G","tags":["Kubernetes"],"title":"Gateway","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"ServiceAccount 为 Pod 中的进程提供身份信息。\n注意：本文是关于 Service Account 的用户指南，管理指南另见 Service Account 的集群管理指南 。\n 本文档描述的关于 Service Account 的行为只有当您按照 Kubernetes 项目建议的方式搭建起集群的情况下才有效。您的集群管理员可能在您的集群中有自定义配置，这种情况下该文档可能并不适用。\n 当您（真人用户）访问集群（例如使用kubectl命令）时，apiserver 会将您认证为一个特定的 User Account（目前通常是admin，除非您的系统管理员自定义了集群配置）。Pod 容器中的进程也可以与 apiserver 联系。 当它们在联系 apiserver 的时候，它们会被认证为一个特定的 Service Account（例如default）。\n使用默认的 Service Account 访问 API server 当您创建 pod 的时候，如果您没有指定一个 service account，系统会自动得在与该pod 相同的 namespace 下为其指派一个default service account。如果您获取刚创建的 pod 的原始 json 或 yaml 信息（例如使用kubectl get pods/podename -o yaml命令），您将看到spec.serviceAccountName字段已经被设置为 default。\n您可以在 pod 中使用自动挂载的 service account 凭证来访问 API，如 Accessing the Cluster 中所描述。\nService account 是否能够取得访问 API 的许可取决于您使用的 授权插件和策略。\n在 1.6 以上版本中，您可以选择取消为 service account 自动挂载 API 凭证，只需在 service account 中设置 automountServiceAccountToken: false：\napiVersion:v1kind:ServiceAccountmetadata:name:build-robotautomountServiceAccountToken:false...在 1.6 以上版本中，您也可以选择只取消单个 pod 的 API 凭证自动挂载：\napiVersion:v1kind:Podmetadata:name:my-podspec:serviceAccountName:build-robotautomountServiceAccountToken:false...如果在 pod 和 service account 中同时设置了 automountServiceAccountToken , pod 设置中的优先级更高。\n使用多个Service Account 每个 namespace 中都有一个默认的叫做 default 的 service account 资源。\n您可以使用以下命令列出 namespace 下的所有 serviceAccount 资源。\n$ kubectl get serviceAccounts NAME SECRETS AGE default 1 1d 您可以像这样创建一个 ServiceAccount 对象：\n$ cat \u0026gt; /tmp/serviceaccount.yaml \u0026lt;\u0026lt;EOF apiVersion: v1 kind: ServiceAccount metadata: name: build-robot EOF $ kubectl create -f /tmp/serviceaccount.yaml serviceaccount \u0026#34;build-robot\u0026#34; created 如果您看到如下的 service account 对象的完整输出信息：\n$ kubectl get serviceaccounts/build-robot -o yaml apiVersion: v1 kind: ServiceAccount metadata: creationTimestamp: 2015-06-16T00:12:59Z name: build-robot namespace: default resourceVersion: \u0026#34;272500\u0026#34; selfLink: /api/v1/namespaces/default/serviceaccounts/build-robot uid: 721ab723-13bc-11e5-aec2-42010af0021e secrets: - name: build-robot-token-bvbk5 然后您将看到有一个 token 已经被自动创建，并被 service account 引用。\n您可以使用授权插件来 设置 service account 的权限 。\n设置非默认的 service account，只需要在 pod 的spec.serviceAccountName 字段中将name设置为您想要用的 service account 名字即可。\n在 pod 创建之初 service account 就必须已经存在，否则创建将被拒绝。\n您不能更新已创建的 pod 的 service account。\n您可以清理 service account，如下所示：\n$ kubectl delete serviceaccount/build-robot 手动创建 service account 的 API token 假设我们已经有了一个如上文提到的名为 ”build-robot“ 的 service account，我们手动创建一个新的 secret。\n$ cat \u0026gt; /tmp/build-robot-secret.yaml \u0026lt;\u0026lt;EOF apiVersion: v1 kind: Secret metadata: name: build-robot-secret annotations: kubernetes.io/service-account.name: build-robot type: kubernetes.io/service-account-token EOF $ kubectl create -f /tmp/build-robot-secret.yaml secret \u0026#34;build-robot-secret\u0026#34; created 现在您可以确认下新创建的 secret 取代了 “build-robot” 这个 service account 原来的 API token。\n所有已不存在的 service account 的 token 将被 token controller 清理掉。\n$ kubectl describe secrets/build-robot-secret Name: build-robot-secret Namespace: default Labels: \u0026lt;none\u0026gt; Annotations: kubernetes.io/service-account.name=build-robot,kubernetes.io/service-account.uid=870ef2a5-35cf-11e5-8d06-005056b45392 Type: kubernetes.io/service-account-token Data ==== ca.crt: 1220 bytes token: ... namespace: 7 bytes  注意：该内容中的token被省略了。\n 为 service account 添加 ImagePullSecret 首先，创建一个 imagePullSecret，详见这里。\n然后，确认已创建。如：\n$ kubectl get secrets myregistrykey NAME TYPE DATA AGE myregistrykey kubernetes.io/.dockerconfigjson 1 1d 然后，修改 namespace 中的默认 service account 使用该 secret 作为 imagePullSecret。\nkubectl patch serviceaccount default -p \u0026#39;{\u0026#34;imagePullSecrets\u0026#34;: [{\u0026#34;name\u0026#34;: \u0026#34;myregistrykey\u0026#34;}]}\u0026#39; Vi 交互过程中需要手动编辑：\n$ kubectl get serviceaccounts default -o yaml \u0026gt; ./sa.yaml $ cat sa.yaml apiVersion: v1 kind: ServiceAccount metadata: creationTimestamp: 2015-08-07T22:02:39Z name: default namespace: default resourceVersion: \u0026#34;243024\u0026#34; selfLink: /api/v1/namespaces/default/serviceaccounts/default uid: 052fb0f4-3d50-11e5-b066-42010af0d7b6 secrets: - name: default-token-uudge $ vi sa.yaml [editor session not shown] [delete line with key \u0026#34;resourceVersion\u0026#34;] [add lines with \u0026#34;imagePullSecret:\u0026#34;] $ cat sa.yaml apiVersion: v1 kind: ServiceAccount metadata: creationTimestamp: 2015-08-07T22:02:39Z name: default namespace: default selfLink: /api/v1/namespaces/default/serviceaccounts/default uid: 052fb0f4-3d50-11e5-b066-42010af0d7b6 secrets: - name: default-token-uudge imagePullSecrets: - name: myregistrykey $ kubectl replace serviceaccount default -f ./sa.yaml serviceaccounts/default 现在，所有当前 namespace 中新创建的 pod 的 spec 中都会增加如下内容：\nspec:imagePullSecrets:- name:myregistrykey参考  Configure Service Accounts for Pods - kubernetes.io  ","date":1653062400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"a1c429a30279ddcf2bafc58f82b9fc69","permalink":"https://lib.jimmysong.io/kubernetes-handbook/auth/serviceaccount/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/auth/serviceaccount/","section":"kubernetes-handbook","summary":"ServiceAccount 为 Pod 中的进程提供身份信息。 注意：本文是关于 Service Account 的用户指南，管理指南另见 Service Account 的集群管理指南 。 本文档描述的关于 Service Account 的行为只有当您按照 Kubernetes 项目建议的方式搭建起集群的情况下才有效。您的集群管理员可能在您的集群","tags":["Kubernetes"],"title":"ServiceAccount","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"注意：本文基于 Kubernetes 1.6 撰写，当时 RBAC 模式处于 beta 版本。\n基于角色的访问控制（Role-Based Access Control，即”RBAC”）使用 rbac.authorization.k8s.io API Group 实现授权决策，允许管理员通过 Kubernetes API 动态配置策略。\n要启用 RBAC，请使用 --authorization-mode=RBAC 启动 API Server。\nAPI 概述 本节将介绍 RBAC API 所定义的四种顶级类型。用户可以像使用其他 Kubernetes API 资源一样 （例如通过 kubectl、API 调用等）与这些资源进行交互。例如，命令 kubectl create -f (resource).yml 可以被用于以下所有的例子，当然，读者在尝试前可能需要先阅读以下相关章节的内容。\nRole 与 ClusterRole 在 RBAC API 中，一个角色包含了一套表示一组权限的规则。 权限以纯粹的累加形式累积（没有” 否定” 的规则）。 角色可以由命名空间（namespace）内的 Role 对象定义，而整个 Kubernetes 集群范围内有效的角色则通过 ClusterRole 对象实现。\n一个 Role 对象只能用于授予对某一单一命名空间中资源的访问权限。 以下示例描述了”default” 命名空间中的一个 Role 对象的定义，用于授予对 pod 的读访问权限：\nkind:RoleapiVersion:rbac.authorization.k8s.io/v1beta1metadata:namespace:defaultname:pod-readerrules:- apiGroups:[\u0026#34;\u0026#34;]# 空字符串\u0026#34;\u0026#34; 表明使用 core API groupresources:[\u0026#34;pods\u0026#34;]verbs:[\u0026#34;get\u0026#34;,\u0026#34;watch\u0026#34;,\u0026#34;list\u0026#34;]ClusterRole 对象可以授予与 Role 对象相同的权限，但由于它们属于集群范围对象， 也可以使用它们授予对以下几种资源的访问权限：\n 集群范围资源（例如节点，即 node） 非资源类型 endpoint（例如”/healthz”） 跨所有命名空间的命名空间范围资源（例如 pod，需要运行命令 kubectl get pods --all-namespaces 来查询集群中所有的 pod）  下面示例中的 ClusterRole 定义可用于授予用户对某一特定命名空间，或者所有命名空间中的 secret（取决于其 绑定 方式）的读访问权限：\nkind:ClusterRoleapiVersion:rbac.authorization.k8s.io/v1beta1metadata:# 鉴于 ClusterRole 是集群范围对象，所以这里不需要定义 \u0026#34;namespace\u0026#34; 字段name:secret-readerrules:- apiGroups:[\u0026#34;\u0026#34;]resources:[\u0026#34;secrets\u0026#34;]verbs:[\u0026#34;get\u0026#34;,\u0026#34;watch\u0026#34;,\u0026#34;list\u0026#34;]RoleBinding 与 ClusterRoleBinding 角色绑定将一个角色中定义的各种权限授予一个或者一组用户。 角色绑定包含了一组相关主体（即 subject，包括用户 ——User、用户组 ——Group、或者服务账户 ——Service Account）以及对被授予角色的引用。 在命名空间中可以通过 RoleBinding 对象授予权限，而集群范围的权限授予则通过 ClusterRoleBinding 对象完成。\nRoleBinding 可以引用在同一命名空间内定义的 Role 对象。 下面示例中定义的 RoleBinding 对象在”default” 命名空间中将”pod-reader” 角色授予用户”jane”。 这一授权将允许用户”jane” 从”default” 命名空间中读取 pod。\n以下角色绑定定义将允许用户 “jane” 从 “default” 命名空间中读取 pod。\nkind:RoleBindingapiVersion:rbac.authorization.k8s.io/v1beta1metadata:name:read-podsnamespace:defaultsubjects:- kind:Username:janeapiGroup:rbac.authorization.k8s.ioroleRef:kind:Rolename:pod-readerapiGroup:rbac.authorization.k8s.ioRoleBinding 对象也可以引用一个 ClusterRole 对象用于在 RoleBinding 所在的命名空间内授予用户对所引用的 ClusterRole 中 定义的命名空间资源的访问权限。这一点允许管理员在整个集群范围内首先定义一组通用的角色，然后再在不同的命名空间中复用这些角色。\n例如，尽管下面示例中的 RoleBinding 引用的是一个 ClusterRole 对象，但是用户”dave”（即角色绑定主体）还是只能读取”development” 命名空间中的 secret（即 RoleBinding 所在的命名空间）。\n# 以下角色绑定允许用户 \u0026#34;dave\u0026#34; 读取 \u0026#34;development\u0026#34; 命名空间中的 secret。kind:RoleBindingapiVersion:rbac.authorization.k8s.io/v1beta1metadata:name:read-secretsnamespace:development# 这里表明仅授权读取 \u0026#34;development\u0026#34; 命名空间中的资源。subjects:- kind:Username:daveapiGroup:rbac.authorization.k8s.ioroleRef:kind:ClusterRolename:secret-readerapiGroup:rbac.authorization.k8s.io最后，可以使用ClusterRoleBinding在集群级别和所有命名空间中授予权限。下面示例中所定义的ClusterRoleBinding允许在用户组”manager” 中的任何用户都可以读取集群中任何命名空间中的 secret。\n以下 ClusterRoleBinding 对象允许在用户组 “manager” 中的任何用户都可以读取集群中任何命名空间中的 secret。\nkind:ClusterRoleBindingapiVersion:rbac.authorization.k8s.io/v1beta1metadata:name:read-secrets-globalsubjects:- kind:Groupname:managerapiGroup:rbac.authorization.k8s.ioroleRef:kind:ClusterRolename:secret-readerapiGroup:rbac.authorization.k8s.io对资源的引用 大多数资源由代表其名字的字符串表示，例如”pods”，就像它们出现在相关 API endpoint 的 URL 中一样。然而，有一些 Kubernetes API 还 包含了” 子资源”，比如 pod 的 logs。在 Kubernetes 中，pod logs endpoint 的 URL 格式为：\nGET /api/v1/namespaces/{namespace}/pods/{name}/log\n在这种情况下，”pods” 是命名空间资源，而”log” 是 pods 的子资源。为了在 RBAC 角色中表示出这一点，我们需要使用斜线来划分资源 与子资源。如果需要角色绑定主体读取 pods 以及 pod log，您需要定义以下角色：\nkind:RoleapiVersion:rbac.authorization.k8s.io/v1beta1metadata:namespace:defaultname:pod-and-pod-logs-readerrules:- apiGroups:[\u0026#34;\u0026#34;]resources:[\u0026#34;pods\u0026#34;,\u0026#34;pods/log\u0026#34;]verbs:[\u0026#34;get\u0026#34;,\u0026#34;list\u0026#34;]通过resourceNames列表，角色可以针对不同种类的请求根据资源名引用资源实例。当指定了resourceNames列表时，不同动作 种类的请求的权限，如使用”get”、”delete”、”update” 以及”patch” 等动词的请求，将被限定到资源列表中所包含的资源实例上。 例如，如果需要限定一个角色绑定主体只能”get” 或者”update” 一个 configmap 时，您可以定义以下角色：\nkind:RoleapiVersion:rbac.authorization.k8s.io/v1beta1metadata:namespace:defaultname:configmap-updaterrules:- apiGroups:[\u0026#34;\u0026#34;]resources:[\u0026#34;configmap\u0026#34;]resourceNames:[\u0026#34;my-configmap\u0026#34;]verbs:[\u0026#34;update\u0026#34;,\u0026#34;get\u0026#34;]值得注意的是，如果设置了resourceNames，则请求所使用的动词不能是 list、watch、create 或者 deletecollection。 由于资源名不会出现在 create、list、watch 和 deletecollection 等 API 请求的 URL 中，所以这些请求动词不会被设置了resourceNames的规则所允许，因为规则中的resourceNames 部分不会匹配这些请求。\n一些角色定义的例子 在以下示例中，我们仅截取展示了 rules 部分的定义。\n允许读取 core API Group 中定义的资源”pods”：\nrules:- apiGroups:[\u0026#34;\u0026#34;]resources:[\u0026#34;pods\u0026#34;]verbs:[\u0026#34;get\u0026#34;,\u0026#34;list\u0026#34;,\u0026#34;watch\u0026#34;]允许读写在”extensions” 和”apps” API Group 中定义的”deployments”：\nrules:- apiGroups:[\u0026#34;extensions\u0026#34;,\u0026#34;apps\u0026#34;]resources:[\u0026#34;deployments\u0026#34;]verbs:[\u0026#34;get\u0026#34;,\u0026#34;list\u0026#34;,\u0026#34;watch\u0026#34;,\u0026#34;create\u0026#34;,\u0026#34;update\u0026#34;,\u0026#34;patch\u0026#34;,\u0026#34;delete\u0026#34;]允许读取”pods” 以及读写”jobs”：\nrules:- apiGroups:[\u0026#34;\u0026#34;]resources:[\u0026#34;pods\u0026#34;]verbs:[\u0026#34;get\u0026#34;,\u0026#34;list\u0026#34;,\u0026#34;watch\u0026#34;]- apiGroups:[\u0026#34;batch\u0026#34;,\u0026#34;extensions\u0026#34;]resources:[\u0026#34;jobs\u0026#34;]verbs:[\u0026#34;get\u0026#34;,\u0026#34;list\u0026#34;,\u0026#34;watch\u0026#34;,\u0026#34;create\u0026#34;,\u0026#34;update\u0026#34;,\u0026#34;patch\u0026#34;,\u0026#34;delete\u0026#34;]允许读取一个名为”my-config” 的ConfigMap实例（需要将其通过RoleBinding绑定从而限制针对某一个命名空间中定义的一个ConfigMap实例的访问）：\nrules:- apiGroups:[\u0026#34;\u0026#34;]resources:[\u0026#34;configmaps\u0026#34;]resourceNames:[\u0026#34;my-config\u0026#34;]verbs:[\u0026#34;get\u0026#34;]允许读取 core API Group 中的”nodes” 资源（由于Node是集群级别资源，所以此ClusterRole定义需要与一个ClusterRoleBinding绑定才能有效）：\nrules:- apiGroups:[\u0026#34;\u0026#34;]resources:[\u0026#34;nodes\u0026#34;]verbs:[\u0026#34;get\u0026#34;,\u0026#34;list\u0026#34;,\u0026#34;watch\u0026#34;]允许对非资源 endpoint …","date":1653062400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"172b7086ef78d5821615c0d758a8fa7d","permalink":"https://lib.jimmysong.io/kubernetes-handbook/auth/rbac/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/auth/rbac/","section":"kubernetes-handbook","summary":"注意：本文基于 Kubernetes 1.6 撰写，当时 RBAC 模式处于 beta 版本。 基于角色的访问控制（Role-Based Access Control，即”RBAC”）使用 rbac.authorization.k8s.io API Group 实现授权决策，允许管理员通过 Kubernetes API 动态配置策略。 要启用 RBAC，请使用 --authorization-mode=RBAC","tags":["Kubernetes"],"title":"基于角色的访问控制（RBAC）","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"网络策略说明一组 Pod 之间是如何被允许互相通信，以及如何与其它网络 Endpoint 进行通信。 NetworkPolicy 资源使用标签来选择 Pod，并定义了一些规则，这些规则指明允许什么流量进入到选中的 Pod 上。关于 Network Policy 的详细用法请参考 Kubernetes 官网。\nNetwork Policy 的作用对象是 Pod，也可以应用到 Namespace 和集群的 Ingress、Egress 流量。Network Policy 是作用在 L3/4 层的，即限制的是对 IP 地址和端口的访问，如果需要对应用层做访问限制需要使用如 Istio 这类 Service Mesh。\n前提条件 网络策略通过网络插件来实现，所以必须使用一种支持 NetworkPolicy 的网络方案（如 calico）—— 非 Controller 创建的资源，是不起作用的。\n隔离的与未隔离的 Pod 默认 Pod 是未隔离的，它们可以从任何的源接收请求。 具有一个可以选择 Pod 的网络策略后，Pod 就会变成隔离的。 一旦 Namespace 中配置的网络策略能够选择一个特定的 Pod，这个 Pod 将拒绝任何该网络策略不允许的连接。（Namespace 中其它未被网络策略选中的 Pod 将继续接收所有流量）\nNetworkPolicy 资源 下面是一个 NetworkPolicy 的例子：\napiVersion:networking.k8s.io/v1kind:NetworkPolicymetadata:name:test-network-policynamespace:defaultspec:podSelector:matchLabels:role:dbpolicyTypes:- Ingress- Egressingress:- from:- ipBlock:cidr:172.17.0.0/16except:- 172.17.1.0/24- namespaceSelector:matchLabels:project:myproject- podSelector:matchLabels:role:frontendports:- protocol:TCPport:6379egress:- to:- ipBlock:cidr:10.0.0.0/24ports:- protocol:TCPport:5978将上面配置 POST 到 API Server 将不起任何作用，除非选择的网络方案支持网络策略。\n必选字段：像所有其它 Kubernetes 配置一样， NetworkPolicy 需要 apiVersion、kind 和 metadata 这三个字段，关于如何使用配置文件的基本信息，可以查看 这里。\nspec：NetworkPolicy spec 具有在给定 Namespace 中定义特定网络的全部信息。\npodSelector：每个 NetworkPolicy 包含一个 podSelector，它可以选择一组应用了网络策略的 Pod。由于 NetworkPolicy 当前只支持定义 ingress 规则，这个 podSelector 实际上为该策略定义了一组 “目标Pod”。示例中的策略选择了标签为 “role=db” 的 Pod。一个空的 podSelector 选择了该 Namespace 中的所有 Pod。\ningress：每个NetworkPolicy 包含了一个白名单 ingress 规则列表。每个规则只允许能够匹配上 from 和 ports配置段的流量。示例策略包含了单个规则，它从这两个源中匹配在单个端口上的流量，第一个是通过namespaceSelector 指定的，第二个是通过 podSelector 指定的。\negress：每个NetworkPolicy 包含了一个白名单 ingress 规则列表。每个规则只允许能够匹配上 to 和 ports配置段的流量。示例策略包含了单个规则，它匹配目的地 10.0.0.0/24 单个端口的流量。\n因此，上面示例的 NetworkPolicy：\n 在 “default” Namespace中 隔离了标签 “role=db” 的 Pod（如果他们还没有被隔离） 在 “default” Namespace中，允许任何具有 “role=frontend” 的 Pod，IP 范围在 172.17.0.0–172.17.0.255 和 172.17.2.0–172.17.255.255（整个 172.17.0.0/16 段， 172.17.1.0/24 除外）连接到标签为 “role=db” 的 Pod 的 TCP 端口 6379 允许在 Namespace 中任何具有标签 “project=myproject” ，IP范围在10.0.0.0/24段的 Pod，连接到 “default” Namespace 中标签为 “role=db” 的 Pod 的 TCP 端口 5978  查看 NetworkPolicy 入门指南给出的更进一步的例子。\n默认策略 通过创建一个可以选择所有 Pod 但不允许任何流量的 NetworkPolicy，你可以为一个 Namespace 创建一个 “默认的” 隔离策略，如下所示：\napiVersion:networking.k8s.io/v1kind:NetworkPolicymetadata:name:default-denyspec:podSelector:这确保了即使是没有被任何 NetworkPolicy 选中的 Pod，将仍然是被隔离的。\n可选地，在 Namespace 中，如果你想允许所有的流量进入到所有的 Pod（即使已经添加了某些策略，使一些 Pod 被处理为 “隔离的”），你可以通过创建一个策略来显式地指定允许所有流量：\napiVersion:networking.k8s.io/v1kind:NetworkPolicymetadata:name:allow-allspec:podSelector:ingress:- {}参考  Network Policies - kubernetes.io  ","date":1653062400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"5688757ec94e48940bb089fad8daf368","permalink":"https://lib.jimmysong.io/kubernetes-handbook/auth/network-policy/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/auth/network-policy/","section":"kubernetes-handbook","summary":"网络策略说明一组 Pod 之间是如何被允许互相通信，以及如何与其它网络 Endpoint 进行通信。 NetworkPolicy 资源使用标签来选择 Pod，并定义了一些规则，这些规则指明允许什么流量进入到选中的 Pod 上。关于 Network Policy 的详细用法请参考 Kubernetes 官网。 Network Policy 的","tags":["Kubernetes"],"title":"NetworkPolicy","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"SPIFFE，即每个人的安全生产身份框架（Secure Production Identity Framework for Everyone），是一套开源标准，用于在动态和异构环境中安全地进行身份识别。采用 SPIFFE 的系统无论在哪里运行，都可以轻松可靠地相互认证。\nSPIFFE 开源规范的核心是——通过简单 API 定义了一个短期的加密身份文件 SVID。然后，工作负载进行认证时可以使用该身份文件，例如建立 TLS 连接或签署和验证 JWT 令牌等。\nSPIFFE 已经在云原生应用中得到了大量的应用，尤其是在 Istio 和 Envoy 中。下面将向你介绍 SPIFFE 的一些基本概念。\n工作负载 工作负载是个单一的软件，以特定的配置部署，用于单一目的；它可能包括软件的多个运行实例，所有这些实例执行相同的任务。工作负载这个术语可以包含一系列不同的软件系统定义，包括：\n 一个运行 Python 网络应用程序的网络服务器，在一个虚拟机集群上运行，前面有一个负载均衡器。 一个 MySQL 数据库的实例。 一个处理队列中项目的 worker 程序。 独立部署的系统的集合，它们一起工作，例如一个使用数据库服务的网络应用程序。网络应用程序和数据库也可以单独被视为工作负载。  就 SPIFFE 而言，工作负载往往比物理或虚拟节点更加细化——通常细化到节点上的单个进程。这对工作负载来说至关重要，例如，在容器编排器中托管的工作负载，几个工作负载可能在同一个节点上（但彼此隔离）。\n就 SPIFFE 而言，一个工作负载也可能跨越许多节点。例如，一个可弹性扩展的网络服务器可能同时运行在许多机器上。\n虽然工作负载的粒度会因环境而异，但就 SPIFFE 而言，我们假设工作负载之间有足够好的隔离，这样恶意的工作负载就不能窃取他人的凭证。这种隔离的稳健性和实现的机制超出了 SPIFFE 的范围。\nSPIFFE ID SPIFFE ID 是一个字符串，可以唯一地、具体地标识一个工作负载。SPIFFE ID 也可以分配给工作负载所运行的中间系统（如一组虚拟机）。例如，spiffe://acme.com/billing/payments 是一个有效的SPIFFE ID。\nSPIFFE ID 是一个统一资源标识符（URI），其格式如下：spiffe://信任域/工作负载标识符。\n工作负载标识符是一个信任域内的特定工作负载的唯一标识。\nSPIFFE 规范详细描述了 SPIFFE ID 的格式和使用。\n信任域 信任域对应于系统的信任根。信任域可以代表个人、组织、环境或部门，运行他们自己独立的 SPIFFE 基础设施。在同一信任域中确定的所有工作负载都会被颁发身份文件，它们可以根据信任域的根密钥进行验证。\n通常建议将处于不同物理位置（如不同的数据中心或云区域）或应用不同安全实践的环境（如与生产环境相比的暂存或实验环境）的工作负载放在不同的信任域中。\nSPIFFE 可验证的身份文件（SVID） SVID（SPIFFE Verifiable Identity Document） 是工作负载向资源或调用者证明其身份的文件。如果 SVID 是由 SPIFFE ID 的信任域内的机构签发的，则被认为是有效的。\nSVID 包含一个 SPIFFE ID，代表了服务的身份。它将 SPIFFE ID 编码在一个可加密验证的文件中，目前支持两种格式：X.509 证书或 JWT 令牌。\n由于令牌容易受到重放攻击（replay attack），即在传输过程中获得令牌的攻击者可以使用它来冒充工作负载，因此建议尽可能使用 X.509-SVID。然而，在某些情况下，JWT令牌格式是唯一的选择，例如，当你的架构在两个工作负载之间有一个L7代理或负载均衡器。\n关于 SVID 的详细信息，请参阅 SVID 规范。\nSPIFFE 工作负载 API 工作负载 API 提供以下内容。\n对于X.509格式的身份文件（X.509-SVID）:\n 其身份，描述为 SPIFFE ID。 一个与该 ID 绑定的私钥，可用于代表工作负载签署数据。一个相应的短期 X.509 证书也将被创建，即 X509-SVID。这可以用来建立 TLS 或以其他方式对其他工作负载进行认证。 一组证书——被称为信任包。一个工作负载用来验证另一个工作负载提出的X.509-SVID。  对于 JWT 格式的身份文件（JWT-SVID）：\n 其身份，描述为 SPIFFE ID JWT 令牌 一组证书——被称为信任包，一个工作负载用来验证其他工作负载的身份。  与 AWS EC2 实例元数据 API和 Google GCE 实例元数据 API 类似，工作负载 API 不要求调用的工作负载对自己的身份有任何了解，也不要求调用 API 时拥有任何认证令牌。这意味着你的应用程序不需要与工作负载共同部署任何认证密钥。\n然而，与这些其他 API 不同的是，Workload API 与平台无关，可以在进程级和内核级识别正在运行的服务，这使得它适合与 Kubernetes 等容器调度器一起使用。\n为了最大限度地减少密钥泄露的风险，所有私钥（和相应的证书）都是短期的，经常自动轮换。工作负载可以在相应的密钥过期前从工作负载API请求新的密钥和信任包。\n信任包 当使用 X.509-SVID 时，目标工作负载使用信任包（Trust Bundle）来验证源工作负载的身份。信任包是一个或多个证书机构（CA）根证书的集合，工作负载认为这些证书是可信的。信任包包含 X.509 和 JWT SVID 的公钥材料。\n用来验证 X.509 SVID 的公钥材料是一组证书。用于验证 JWT 的公钥材料是一个原始公钥。信任包的内容是经常轮换的。工作负载在调用工作负载 API 时检索信任包。\n参考  SPIFFE 官网 - spiffe.io  ","date":1653062400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"dcdb7954c9faec61a6583d9cb063dd82","permalink":"https://lib.jimmysong.io/kubernetes-handbook/auth/spiffe/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/auth/spiffe/","section":"kubernetes-handbook","summary":"SPIFFE，即每个人的安全生产身份框架（Secure Production Identity Framework for Everyone），是一套开源标准，用于在动态和异构环境中安全地进行身份识别。采用 SPIFFE 的系统无论在哪里运行，都可以轻松可靠地相互认证。 SPIFFE 开源","tags":["Kubernetes"],"title":"SPIFFE","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"这篇文章将向你介绍 SPIRE 的架构、基本概念及原理。\nSPIRE 是 SPIFFE API 的一个生产就绪的实现，它执行节点和工作负载认证，以便根据一组预先定义的条件，安全地向工作负载发出 SVID，并验证其他工作负载的 SVID。\nSPIRE 架构和组件 SPIRE 部署由一个 SPIRE 服务器和一个或多个 SPIRE 代理组成。服务器充当通过代理向一组工作负载发放身份的签名机构。它还维护一个工作负载身份的注册表，以及为签发这些身份而必须验证的条件。代理在本地向工作负载公开 SPIFFE 工作负载 API，必须安装在工作负载运行的每个节点上。\n   SPIRE 架构图  服务器 SPIRE 服务器负责管理和发布其配置的 SPIFFE 信任域中的所有身份。它存储注册条目（指定决定特定 SPIFFE ID 应被签发的条件的选择器）和签名密钥，使用节点证明来自动验证代理的身份，并在被验证的代理请求时为工作负载创建 SVID。\n   SPIRE 服务器  服务器的行为是通过一系列的插件决定的。SPIRE 包含几个插件，你可以建立额外的插件来扩展 SPIRE 以满足特定的使用情况。插件的类型包括：\n 节点证明器插件：与代理节点证明器一起，验证代理运行的节点的身份。 节点解析器插件：它通过验证关于节点的额外属性来扩展服务器可以用来识别节点的选择器集合。 数据存储插件：服务器用它来存储、查询和更新各种信息，如注册条目、哪些节点已认证、这些节点的选择器是什么。有一个内置的数据存储插件，可以使用 MySQL、SQLite3 或 PostgresSQL 数据库来存储必要的数据。默认情况下，使用 SQLite 3。 密钥管理器插件：控制服务器如何存储用于签署 X.509-SVID 和 JWT-SVID 的私钥。 上游权威机构插件：默认情况下，SPIRE 服务器充当其自身的证书授权机构。但是，你可以使用上游权威机构插件来使用来自不同 PKI 系统的不同 CA。  你可以通过配置插件和其他各种配置变量来定制服务器的行为。详见 SPIRE 服务器配置参考。\n代理 SPIRE 代理在已识别的工作负载所运行的每个节点上运行。该代理：\n 从服务器上请求 SVID，并将其缓存起来，直到工作负载请求其 SVID 为止。 向节点上的工作负载公开 SPIFFE 工作负载 API，并证明调用它的工作负载的身份 为已识别的工作负载提供其 SVID     SPIRE 代理  该代理的主要组成部分包括：\n 节点证明器插件：与服务器节点证明器一起，验证代理运行的节点的身份。 工作负载证明器插件：通过从节点操作系统中查询有关工作负载进程的信息，并将其与你在使用选择器注册工作负载属性时提供给服务器的信息进行比较，来验证节点上工作负载进程的身份。 密钥管理器插件：代理用来生成和使用颁发给工作负载的 X.509-SVID 的私钥。  你可以通过配置插件和其他配置变量来定制代理的行为。详见《SPIRE 代理配置参考》。\n自定义服务器和代理插件 你可以为特定的平台和架构创建自定义的服务器和代理插件，而 SPIRE 并不包括这些插件。例如，你可以为一个架构创建服务器和代理节点验证器，而不是在节点验证下总结的那些。或者你可以创建一个自定义密钥管理器插件，以 SPIRE 目前不支持的方式处理私钥。因为 SPIRE 在运行时加载自定义插件，你不需要重新编译 SPIRE 来启用它们。\n工作负载注册 为了让 SPIRE 识别工作负载，你必须通过注册条目向 SPIRE 服务器注册工作负载。工作负载注册告诉 SPIRE 如何识别工作负载以及为其提供哪个 SPIFFE ID。\n注册条目将身份（以 SPIFFE ID 的形式）映射到一组称为选择器的属性，工作负载必须拥有这些属性才能获得特定身份。在工作负载证明期间，代理使用这些选择器值来验证工作负载的身份。\nSPIRE 文档中详细介绍了工作负载注册。\n证明 SPIRE 上下文中的证明（attestation）是断言工作负载的身份。SPIRE 通过从受信任的第三方收集工作负载进程本身和运行 SPIRE 代理的节点的属性并将它们与工作负载注册时定义的一组选择器进行比较来实现这一点。\n用于执行证明的可信第三方 SPIRE 查询是特定于平台的。\nSPIRE 分两个阶段执行证明：首先是节点证明（其中验证工作负载正在运行的节点的身份），然后是工作负载证明（其中验证节点上的工作负载）。\nSPIRE 有一个灵活的架构，允许它根据工作负载运行的环境，使用许多不同的受信第三方进行节点和工作负载验证。你通过代理和服务器配置文件中的条目告诉 SPIRE 使用哪些受信任的第三方，并通过你在注册工作负载时指定的选择器值告诉 SPIRE 使用哪些类型的信息进行验证。\n节点证明 SPIRE 要求每个代理在首次连接到服务器时进行身份验证和自我验证；这个过程称为节点证明（Node Attestation）。在节点证明期间，代理和服务器一起验证运行代理的节点的身份。他们通过称为节点证明器的插件来做到这一点。所有节点证明器都向节点及其环境询问只有该节点拥有的信息片段，以证明该节点的身份。\n节点证明的成功后，代理收到唯一的 SPIFFE ID。然后，代理的 SPIFFE ID 充当其负责的工作负载的 “父级”。\n节点身份证明的示例包括：\n 通过云平台交付给节点的身份证明文件（例如 AWS Instance 身份证明文件） 验证存储在连接到节点的硬件安全模块或可信平台模块上的私钥 安装代理时通过加入令牌提供的手动验证 多节点软件系统安装在节点上时提供的标识凭据（例如 Kubernetes 服务账户令牌） 其他机器身份证明（例如部署的服务器证书）  节点证明器向服务器返回一组（可选）节点选择器，用于标识特定机器（例如 Amazon 实例 ID）。由于在定义工作负载的身份时，单个机器的特定身份通常没有用处，因此 SPIRE 会查询节点解析器（如果有）以查看可以验证被证明节点的哪些附加属性（例如，如果节点是 AWS 安全组的成员）。来自证明器和解析者的选择器集成为与代理节点的 SPIFFE ID 关联的选择器集。\n\n  节点证明不需要节点选择器，除非你将工作负载映射到多个节点。   下图说明了节点证明中的步骤。在此图中，底层平台是 AWS：\n   SPIRE 节点证明步骤  步骤总结：节点证明  代理 AWS 节点证明器插件向 AWS 查询节点身份证明，并将该信息提供给代理。 代理将此身份证明传递给服务器。服务器将此数据传递给其 AWS 节点证明器。 服务器 AWS 节点证明器独立验证身份证明，或者通过调用 AWS API，使用它在步骤 2 中获得的信息。节点证明器还为代理创建一个 SPIFFE ID，并将其传递回服务器进程，以及它发现的任何节点选择器。 服务器发回代理节点的 SVID。  节点证明器 代理和服务器通过它们各自的节点证明器询问底层平台。SPIRE 支持节点证明器在各种环境中证明节点身份，包括：\n AWS 上的 EC2 实例（使用 EC2 实例身份文档） Microsoft Azure 上的 VM（使用 Azure 托管服务标识） Google Cloud Platform 上的 Google Compute Engine 实例（使用 GCE 实例身份令牌） 作为 Kubernetes 集群成员的节点（使用 Kubernetes 服务账户令牌）  对于没有平台可以直接识别节点的情况，SPIRE 包括用于证明的节点证明器：\n使用服务器生成的加入令牌—— 加入令牌（join token）是 SPIRE 服务器和代理之间的预共享密钥。服务器可以在安装后生成加入令牌，该令牌可用于在代理启动时对其进行验证。为帮助防止滥用，加入令牌在使用后立即过期。\n使用现有的 X.509 证书—— 有关配置节点证明器的信息，请参阅 SPIRE 服务器配置参考和 SPIRE 代理配置参考。\n节点解析 一旦验证了单个节点的身份，“节点解析器” 插件就会扩展一组选择器，这些选择器可用于通过验证节点的其他属性来识别节点（例如，如果节点是特定 AWS 安全组的成员） ，或具有与之关联的特定标签）。只有服务器参与节点解析。SPIRE 在证明之后直接运行一次节点解析器。\n节点解析器 服务器支持以下平台的节点解析器插件：\n 亚马逊网络服务（AWS） 微软 Azure  工作负载证明 工作负载证明提出了一个问题：“这是谁的进程？” 代理通过询问本地可用的权限（例如节点的操作系统内核，或在同一节点上运行的本地 kubelet）来回答这个问题，以确定调用工作负载 API 的进程的属性。\n然后，当你使用选择器注册工作负载的属性时，将这些属性与提供给服务器的信息进行比较。\n这些类型的信息可能包括：\n 底层操作系统如何调度进程。在基于 Unix 的系统上，这可能是用户 ID (uid)、组 ID (gid)、文件系统路径等。） 进程是如何由 Kubernetes 等编排系统调度的。在这种情况下，工作负载可能由运行它的 Kubernetes 服务账户或命名空间来描述。  虽然代理和服务器都在节点证明中发挥作用，但只有代理参与工作负载证明。\n下图说明了工作负载证明的步骤：\n   工作负载证明  步骤摘要：工作负载证明  工作负载调用工作负载 API 以请求 SVID。在 Unix 系统上，这被暴露为 Unix 域套接字。 代理询问节点的内核以识别调用者的进程 ID。然后，它调用任何已配置的工作负载证明器插件，为它们提供工作负载的进程 ID。 工作负载证明者使用进程 ID 来发现有关工作负载的其他信息，并根据需要查询相邻平台特定的组件，例如 Kubernetes kubelet。通常，这些组件也与代理驻留在同一节点上。 证明者将发现的信息以选择器的形式返回给代理。 代理通过将发现的选择器与注册条目进行比较来确定工作负载的身份，并将正确的缓存 SVID 返回给工作负载。  工作负载证明者 SPIRE 包括适用于 Unix、Kubernetes 和 Docker 的工作负载证明器插件。\n参考  SPIRE Concepts - spiffe.io  ","date":1654790400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"ba4c4a81faa53ff384a48f019cfbb8ad","permalink":"https://lib.jimmysong.io/kubernetes-handbook/auth/spire/","publishdate":"2022-06-10T00:00:00+08:00","relpermalink":"/kubernetes-handbook/auth/spire/","section":"kubernetes-handbook","summary":"这篇文章将向你介绍 SPIRE 的架构、基本概念及原理。","tags":["Kubernetes"],"title":"SPIRE","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"SPIRE Kubernetes 工作负载注册器实现了一个 Kubernetes ValidatingAdmissionWebhook，便于在 Kubernetes 内自动注册工作负载。\n配置 命令行配置 注册器有以下命令行选项：\n   标志 描述 默认值     -config 磁盘上 HCL 配置文件的路径 k8s-workload-registrar.conf    HCL 配置 配置文件是注册器所必需的。它包含 HCL 编码的配置项。\n   键 类型 必需的？ 描述 默认     log_level string 必需的 日志级别（panic、 fatal、 error、 warn、 warning、 info、 debug、trace 之一） info   log_path string 可选的 写入日志的磁盘路径    trust_domain string 必需的 SPIRE 服务器的信任域    agent_socket_path string 可选的 SPIRE 代理的 Unix 域套接字的路径。如果 server_address 不是 unix 域套接字地址，则为必需。    server_address string 必需的 SPIRE 服务器的地址。可以使用 unix:///path/to/socket 指定本地套接字。这与代理套接字不同。    server_socket_path string 可选的 SPIRE 服务器的 Unix 域套接字的路径，相当于指定带有 unix:// 前缀的 server_address    cluster string 必需的 用于在其下注册节点 / 工作负载的逻辑集群。必须与 SPIRE SERVER PSAT 节点证明者配置相匹配。    pod_label string 可选的 pod 标签，用于基于标签的工作负载注册    pod_annotation string 可选的 pod 注解，用于基于注解的工作负载注册    mode string 可选的 如何使用 webhook、 reconcile 或运行注册器 crd。 webhook   disabled_namespaces []string 可选的 逗号分隔的命名空间列表，用于禁用自动 SVID 生成 kube-system、 kube-public    以下配置指令是针对 webhook 模式的：\n   键 类型 必需的？ 描述 默认     addr string 必需的 将 HTTPS 监听器绑定到的地址 :8443   cert_path string 必需的 PEM 编码的服务器 TLS 证书的磁盘路径 cert.pem   key_path string 必需的 PEM 编码的服务器 TLS 密钥的磁盘路径 key.pem   cacert_path string 必需的 用于验证客户端（即 API 服务器）的 CA 证书的磁盘路径 cacert.pem   insecure_skip_client_verification boolean 必需的 如果为 true，则跳过客户端证书验证（在这种情况下 cacert_path 被忽略）。 false    以下配置是针对 reconcile 模式的：\n   键 类型 必需的？ 描述 默认     leader_election bool 可选的 启用 / 禁用领导者选举。如果你有多个注册器副本正在运行，请启用 false   leader_election_resource_lock string 可选的 配置用于领导选举锁的资源类型 configmaps   metrics_addr string 可选的 公开指标的地址，0 用于禁用 :8080   controller_name string 可选的 构成用于父 ID 的 spiffe ID 的一部分 spire-k8s-registrar   add_pod_dns_names bool 可选的 启用 / 禁用将 k8s DNS 名称添加到 pod SVID。 错误的   cluster_dns_zone string 可选的 k8s 集群中用于服务的 DNS 区域。 cluster.local    关于 CRD 配置指令，见 CRD 模式配置。\n示例 log_level = \u0026#34;debug\u0026#34; trust_domain = \u0026#34;domain.test\u0026#34; server_socket_path = \u0026#34;/tmp/spire-server/private/api.sock\u0026#34; cluster = \u0026#34;production\u0026#34; 工作负载注册 当在 webhook、reconcile 或 crd 模式下运行时，pod_controller=true 的条目将被自动创建为 Pod。可用的工作负载注册模式包括：\n   注册模式 pod_label pod_annotation identity_template 基于服务账户     webhook 由 pod_label 指定 由 pod_annotation 指定 不可用 服务账户   reconcile 由 pod_label 指定 由 pod_annotation 指定 不可用 服务账户   crd 由 pod_label 指定 由 pod_annotation 指定 由 identity_template 指定 不可用    如果对 基于服务账户的 SPIFFE ID webhook 使用和 reconcile 模式，请不要指定 pod_label 或 pod_annotation。如果你使用基于标签的 SPIFFE ID，请仅指定 pod_label。如果你使用基于注解的 SPIFFE ID，请仅指定 pod_annotation\n对于 crd 模式，如果既不选择 pod_label 也不选择 pod_annotation 工作负载注册模式， identity_template 则作为默认配置： ns/{{.Pod.Namespace}}/sa/{{.Pod.ServiceAccount}}\n新创建的 SVID 可能需要几秒钟才能对工作负载可用。\n联合条目注册 pod 注解 spiffe.io/federatesWith 可用于创建与其他信任域联合的 SPIFFE ID。\n要指定多个信任域，请用逗号分隔它们。\n例子：\napiVersion:v1kind:Podmetadata:annotations:spiffe.io/federatesWith:example.com,example.io,example.ainame:testspec:containers:...基于服务账户的工作负载注册 授予工作负载的 SPIFFE ID 源自：\n 服务账户 可配置的 pod 标签 可配置的 pod 注解  服务账户派生的工作负载注册将服务账户映射到表单的 SPIFFE ID spiffe://\u0026lt;TRUSTDOMAIN\u0026gt;/ns/\u0026lt;NAMESPACE\u0026gt;/sa/\u0026lt;SERVICEACCOUNT\u0026gt;。例如，如果一个 pod 使用命名空间中的服务账户 blog 进入 production，则将创建以下注册条目：\nEntry ID : 200d8b19-8334-443d-9494-f65d0ad64eb5 SPIFFE ID : spiffe://example.org/ns/production/sa/blog Parent ID : ... TTL : default Selector : k8s:ns:production Selector : k8s:pod-name:example-workload-98b6b79fd-jnv5m 基于标签的工作负载注册 基于标签的工作负载注册将 pod 标签值映射到表单的 SPIFFE ID spiffe://\u0026lt;TRUSTDOMAIN\u0026gt;/\u0026lt;LABELVALUE\u0026gt;。例如，如果注册器配置了 spire-workload 标签，并且 pod 带有 spire-workload=example-workload，则将创建以下注册条目：\nEntry ID : 200d8b19-8334-443d-9494-f65d0ad64eb5 SPIFFE ID : spiffe://example.org/example-workload Parent ID : ... TTL : default Selector : k8s:ns:production Selector : k8s:pod-name:example-workload-98b6b79fd-jnv5m 不包含 pod 标签的 pod 将被忽略。\n基于注解的工作负载注册 基于注解的工作负载注册将 pod 注解值映射到表单的 SPIFFE ID spiffe://\u0026lt;TRUSTDOMAIN\u0026gt;/\u0026lt;ANNOTATIONVALUE\u0026gt;。通过使用该模式，可以自由设置 SPIFFE ID 路径。例如，如果注册器配置了 spiffe.io/spiffe-id 注解并且 pod 带有 spiffe.io/spiffe-id: production/example-workload，则将创建以下注册条目：\nEntry ID : 200d8b19-8334-443d-9494-f65d0ad64eb5 SPIFFE ID : spiffe://example.org/production/example-workload Parent ID : ... TTL : default Selector : k8s:ns:production Selector : k8s:pod-name:example-workload-98b6b79fd-jnv5m 不包含 pod 注解的 pod 将被忽略。\n基于身份模板的工作负载注册 这是特定于 crd 模式的。请参阅 crd 模式文档中的基于身份模板的工作负载注册。\n部署 注册器既可以作为独立 Deployment 部署，也可以作为 SPIRE 服务器 pod 中的容器部署。如果它是独立部署的，则需要手动创建与注册器部署相匹配的管理员注册条目。\n如果它被部署为 SPIRE 服务器 pod 中的容器，那么它会通过 Unix 域套接字与 SPIRE 服务器通信。它将需要访问包含套接字文件的共享卷。\n协调模式配置 要使用协调模式（Reconcile Mode），你需要创建适当的角色并将它们绑定到你打算运行控制器的 ServiceAccount。\nCRD 模式配置 请参阅 CRD Kubernetes Workload Registrar 快速入门。\nWebhook 模式配置 注册器将需要访问其服务器密钥对和用于验证客户端的 CA 证书。\n设置验证准入控制器需要以下 K8s 对象：\n Service 指向 spire-server 容器中的注册端口 ValidatingWebhookConfiguration 将注册器配置为验证准入控制器  此外，除非你禁用客户端身份验证 ( insecure_skip_client_verification)，否则你将需要：\n Config 注册器服务客户端的用户条目包含 API 服务器应用于向注册器进行身份验证的客户端证书 / 密钥。 AdmissionConfiguration 描述 API 服务器可以在哪里找到包含 Config。该文件通过 --admission-control-config-file 标志传递给 API 服务器。  Webhook 模式安全注意事项 注册器默认对客户端进行身份验证。这是注册器整体安全性的一个非常重要的方面，因为注册器可用于提供对 SPIRE 服务器 API 的间接访问，尽管范围有限。除非你完全了解风险，否则不建议跳过客户端验证（ …","date":1654675200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"f866238116eb6f4457bce1f41c1f3941","permalink":"https://lib.jimmysong.io/kubernetes-handbook/auth/spire-k8s-workload-registar/","publishdate":"2022-06-08T16:00:00+08:00","relpermalink":"/kubernetes-handbook/auth/spire-k8s-workload-registar/","section":"kubernetes-handbook","summary":"本文介绍了如何在 Kubernetes 中使用 SPIRE 工作负载注册器，包括工作负载注册器部署的方式，注册模式等。","tags":["Kubernetes"],"title":"SPIRE Kubernetes 工作负载注册器","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"本文介绍 SPIRE 如何向工作负载颁发身份的详细步骤，叙述了从代理在节点上启动到同一节点上的工作负载收到 X.509 SVID 形式的有效身份的全过程。请注意，JWT 格式的 SVID 的处理方式是不同的。为了简单演示的目的，工作负载在 AWS EC2 上运行。\n SPIRE 服务器启动。 除非用户配置了 UpstreamAuthority 插件，否则服务器会生成自签名证书（使用自己的私钥签名的证书）；服务器将使用此证书为该服务器信任域中的所有工作负载签署 SVID。 如果是第一次启动，服务器会自动生成一个信任包（trust bundle），其内容存储在你指定的 sql 数据存储中 —— 在服务器插件：DataStore sql 的 “内置插件” 部分中描述。 服务器打开其注册 API，以允许你注册工作负载。 工作负载节点上的 SPIRE 代理启动。 代理执行节点证明，向服务器证明它正在运行的节点的身份。例如，在 AWS EC2 实例上运行时，它通常会通过向服务器提供 AWS 实例身份文档来执行节点证明。 代理通过 TLS 连接向服务器提供此身份证明，该 TLS 连接通过代理配置的引导程序包进行身份验证。  \n  此引导程序包是默认配置，应在生产中替换为客户提供的凭据。   服务器调用 AWS API 来验证证明。 AWS 承认该文件是有效的。 服务器执行节点解析，以验证有关代理节点的其他属性并相应地更新其注册条目。例如，如果节点已使用 Microsoft Azure 托管服务标识 (MSI) 进行了证明。解析器从代理 SPIFFE ID 中提取租户 ID 和主体 ID，并使用各种 Azure 服务获取信息以构建一组额外的选择器。 服务器向代理发出一个 SVID，代表代理本身的身份。 代理联系服务器（使用其 SVID 作为其 TLS 客户端证书）以获取它被授权的注册条目。 服务器使用代理的 SVID 对代理进行身份验证。代理依次完成 mTLS 握手并使用引导程序包对服务器进行身份验证。 然后，服务器从其数据存储中获取所有授权的注册条目并将它们发送给代理。 然后，代理将工作负载 CSR 发送到服务器，服务器对其进行签名并作为工作负载 SVID 返回给客户端。客户端将它们放入缓存中。 现在引导完成，代理开始监听 Workload API 套接字。 工作负载调用工作负载 API 来请求 SVID。 代理通过调用其工作负载证明器来启动工作负载证明过程，并向他们提供工作负载进程的进程 ID。 证明器使用内核和用户空间调用来发现有关工作负载的其他信息。 证明器以工作负载选择器的形式将发现的信息返回给代理。 代理通过将发现的工作负载选择器与注册条目进行比较来确定工作负载的身份，并返回正确的 SVID（已在其缓存中）。  授权注册条目 服务器只向代理发送授权的注册条目。服务器执行以下操作来获取这些授权条目：\n 查询数据库以查找将代理的 SPIFFE ID 列为其 “父 SPIFFE ID” 的任何注册条目。 在数据库中查询特定代理与哪些附加属性相关联（“节点选择器”）。 查询数据库以获取在任何这些节点选择器上声明选择的至少一个注册条目。 递归地查询数据库中的任何注册条目，这些条目声明了迄今为止获得的任何条目作为它们的 “父 SPIFFE ID”（下降到所有子节点）。  另请参阅将工作负载映射到多个节点。\n服务器将生成授权的注册条目集发送给代理。\n","date":1653062400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"b3de22041638ca58d841d4c4feec157f","permalink":"https://lib.jimmysong.io/kubernetes-handbook/auth/svid/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/auth/svid/","section":"kubernetes-handbook","summary":"本文介绍了为工作负载颁发 X.509 SVID 身份的详细步骤。","tags":["Kubernetes"],"title":"SVID 身份颁发过程","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"如果你安装了拥有三个节点的 Kubernetes 集群，节点的状态如下所述。\n[root@node1 ~]# kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME node1 Ready \u0026lt;none\u0026gt; 2d v1.9.1 \u0026lt;none\u0026gt; CentOS Linux 7 (Core) 3.10.0-693.11.6.el7.x86_64 docker://1.12.6 node2 Ready \u0026lt;none\u0026gt; 2d v1.9.1 \u0026lt;none\u0026gt; CentOS Linux 7 (Core) 3.10.0-693.11.6.el7.x86_64 docker://1.12.6 node3 Ready \u0026lt;none\u0026gt; 2d v1.9.1 \u0026lt;none\u0026gt; CentOS Linux 7 (Core) 3.10.0-693.11.6.el7.x86_64 docker://1.12.6 当前 Kubernetes 集群中运行的所有 Pod 信息：\n[root@node1 ~]# kubectl get pods --all-namespaces -o wide NAMESPACE NAME READY STATUS RESTARTS AGE IP NODE kube-system coredns-5984fb8cbb-sjqv9 1/1 Running 0 1h 172.33.68.2 node1 kube-system coredns-5984fb8cbb-tkfrc 1/1 Running 1 1h 172.33.96.3 node3 kube-system heapster-v1.5.0-684c7f9488-z6sdz 4/4 Running 0 1h 172.33.31.3 node2 kube-system kubernetes-dashboard-6b66b8b96c-mnm2c 1/1 Running 0 1h 172.33.31.2 node2 kube-system monitoring-influxdb-grafana-v4-54b7854697-tw9cd 2/2 Running 2 1h 172.33.96.2 node3 当前 etcd 中的注册的宿主机的 pod 地址网段信息：\n[root@node1 ~]# etcdctl ls /kube-centos/network/subnets /kube-centos/network/subnets/172.33.68.0-24 /kube-centos/network/subnets/172.33.31.0-24 /kube-centos/network/subnets/172.33.96.0-24 而每个 node 上的 Pod 子网是根据我们在安装 flannel 时配置来划分的，在 etcd 中查看该配置：\n[root@node1 ~]# etcdctl get /kube-centos/network/config {\u0026#34;Network\u0026#34;:\u0026#34;172.33.0.0/16\u0026#34;,\u0026#34;SubnetLen\u0026#34;:24,\u0026#34;Backend\u0026#34;:{\u0026#34;Type\u0026#34;:\u0026#34;host-gw\u0026#34;}} 我们知道 Kubernetes 集群内部存在三类 IP，分别是：\n Node IP：宿主机的 IP 地址 Pod IP：使用网络插件创建的 IP（如 flannel），使跨主机的 Pod 可以互通 Cluster IP：虚拟 IP，通过 iptables 规则访问服务  在安装 node 节点的时候，节点上的进程是按照 flannel -\u0026gt; docker -\u0026gt; kubelet -\u0026gt; kube-proxy 的顺序启动的，我们下面也会按照该顺序来讲解，flannel 的网络划分和如何与 docker 交互，如何通过 iptables 访问 service。\nFlannel Flannel 是作为一个二进制文件的方式部署在每个 node 上，主要实现两个功能：\n 为每个 node 分配 subnet，容器将自动从该子网中获取 IP 地址 当有 node 加入到网络中时，为每个 node 增加路由配置  下面是使用 host-gw backend 的 flannel 网络架构图：\n   flannel 网络架构（图片来自 openshift）  注意：以上 IP 非本示例中的 IP，但是不影响读者理解。\nNode1 上的 flannel 配置如下:\n[root@node1 ~]# cat /usr/lib/systemd/system/flanneld.service [Unit] Description=Flanneld overlay address etcd agent After=network.target After=network-online.target Wants=network-online.target After=etcd.service Before=docker.service [Service] Type=notify EnvironmentFile=/etc/sysconfig/flanneld EnvironmentFile=-/etc/sysconfig/docker-network ExecStart=/usr/bin/flanneld-start $FLANNEL_OPTIONS ExecStartPost=/usr/libexec/flannel/mk-docker-opts.sh -k DOCKER_NETWORK_OPTIONS -d /run/flannel/docker Restart=on-failure [Install] WantedBy=multi-user.target RequiredBy=docker.service 其中有两个环境变量文件的配置如下：\n[root@node1 ~]# cat /etc/sysconfig/flanneld # Flanneld configuration options FLANNEL_ETCD_ENDPOINTS=\u0026#34;http://172.17.8.101:2379\u0026#34; FLANNEL_ETCD_PREFIX=\u0026#34;/kube-centos/network\u0026#34; FLANNEL_OPTIONS=\u0026#34;-iface=eth2\u0026#34; 上面的配置文件仅供 flanneld 使用。\n[root@node1 ~]# cat /etc/sysconfig/docker-network # /etc/sysconfig/docker-network DOCKER_NETWORK_OPTIONS= 还有一个ExecStartPost=/usr/libexec/flannel/mk-docker-opts.sh -k DOCKER_NETWORK_OPTIONS -d /run/flannel/docker，其中的/usr/libexec/flannel/mk-docker-opts.sh 脚本是在 flanneld 启动后运行，将会生成两个环境变量配置文件：\n /run/flannel/docker /run/flannel/subnet.env  我们再来看下 /run/flannel/docker 的配置。\n[root@node1 ~]# cat /run/flannel/docker DOCKER_OPT_BIP=\u0026#34;--bip=172.33.68.1/24\u0026#34; DOCKER_OPT_IPMASQ=\u0026#34;--ip-masq=true\u0026#34; DOCKER_OPT_MTU=\u0026#34;--mtu=1500\u0026#34; DOCKER_NETWORK_OPTIONS=\u0026#34;--bip=172.33.68.1/24 --ip-masq=true --mtu=1500\u0026#34; 如果你使用systemctl 命令先启动 flannel 后启动 docker 的话，docker 将会读取以上环境变量。\n我们再来看下 /run/flannel/subnet.env 的配置。\n[root@node1 ~]# cat /run/flannel/subnet.env FLANNEL_NETWORK=172.33.0.0/16 FLANNEL_SUBNET=172.33.68.1/24 FLANNEL_MTU=1500 FLANNEL_IPMASQ=false 以上环境变量是 flannel 向 etcd 中注册的。\nDocker Node1 的 docker 配置如下：\n[root@node1 ~]# cat /usr/lib/systemd/system/docker.service [Unit] Description=Docker Application Container Engine Documentation=http://docs.docker.com After=network.target rhel-push-plugin.socket registries.service Wants=docker-storage-setup.service Requires=docker-cleanup.timer [Service] Type=notify NotifyAccess=all EnvironmentFile=-/run/containers/registries.conf EnvironmentFile=-/etc/sysconfig/docker EnvironmentFile=-/etc/sysconfig/docker-storage EnvironmentFile=-/etc/sysconfig/docker-network Environment=GOTRACEBACK=crash Environment=DOCKER_HTTP_HOST_COMPAT=1 Environment=PATH=/usr/libexec/docker:/usr/bin:/usr/sbin ExecStart=/usr/bin/dockerd-current \\  --add-runtime docker-runc=/usr/libexec/docker/docker-runc-current \\  --default-runtime=docker-runc \\  --exec-opt native.cgroupdriver=systemd \\  --userland-proxy-path=/usr/libexec/docker/docker-proxy-current \\  $OPTIONS \\  $DOCKER_STORAGE_OPTIONS \\  $DOCKER_NETWORK_OPTIONS \\  $ADD_REGISTRY \\  $BLOCK_REGISTRY \\  $INSECURE_REGISTRY\\ \t$REGISTRIES ExecReload=/bin/kill -s HUP $MAINPID LimitNOFILE=1048576 LimitNPROC=1048576 LimitCORE=infinity TimeoutStartSec=0 Restart=on-abnormal MountFlags=slave KillMode=process [Install] WantedBy=multi-user.target 查看 Node1 上的 docker 启动参数：\n[root@node1 ~]# systemctl status -l docker ● docker.service - Docker Application Container Engine Loaded: loaded …","date":1653062400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"a44e9d94fcbbc1bc716f9105b71cbbc3","permalink":"https://lib.jimmysong.io/kubernetes-handbook/networking/flannel/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/networking/flannel/","section":"kubernetes-handbook","summary":"如果你安装了拥有三个节点的 Kubernetes 集群，节点的状态如下所述。 [root@node1 ~]# kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME node1 Ready \u003cnone\u003e 2d v1.9.1 \u003cnone\u003e CentOS Linux 7 (Core) 3.10.0-693.11.6.el7.x86_64 docker://1.12.6 node2 Ready \u003cnone\u003e 2d v1.9.1 \u003cnone\u003e CentOS Linux 7 (Core) 3.10.0-693.11.6.el7.x86_64 docker://1.12.6 node3 Ready \u003cnone\u003e 2d v1.9.1 \u003cnone\u003e CentOS Linux 7 (Core) 3.10.0-693.11.6.el7.x86_64 docker://1.12.6 当前 Kubernetes 集群中运行的所有 Pod 信息： [root@node1 ~]# kubectl get pods --all-namespaces","tags":["Kubernetes"],"title":"扁平网络 Flannel","type":"book"},{"authors":null,"categories":["Istio"],"content":"Kiali 是一个基于 Istio 的服务网格的管理控制台。它提供了仪表盘、可观测性，并让我们通过强大的配置和验证能力来操作网格。它通过推断流量拓扑来显示服务网格，并显示网格的健康状况。Kiali 提供了详细的指标，强大的验证，Grafana 访问，以及与 Jaeger 的分布式跟踪的强大集成。\n要安装 Kiali，请使用 addons 文件夹中的 kiali.yaml 文件：\n$ kubectl apply -f istio-1.9.0/samples/addons/kiali.yaml customresourcedefinition.apiextensions.k8s.io/monitoringdashboards.monito ring.kiali.io created serviceaccount/kiali created configmap/kiali created clusterrole.rbac.authorization.k8s.io/kiali-viewer created clusterrole.rbac.authorization.k8s.io/kiali created clusterrolebinding.rbac.authorization.k8s.io/kiali created service/kiali created deployment.apps/kiali created 注意，如果你看到任何错误，例如在版本 monitoringkiali.io/v1alpha 中没有匹配的 MonitoringDashboard，请再次重新运行 kubectl apply 命令。问题是，在安装 CRD（自定义资源定义）和由该 CRD 定义的资源时，可能存在一个匹配条件。\n我们可以用 getmesh istioctl dashboard kiali 打开 Kiali。\nKiali 可以生成一个像下图这样的服务图。\n   Kiali 界面  该图向我们展示了服务的拓扑结构，并将服务的通信方式可视化。它还显示了入站和出站的指标，以及通过连接 Jaeger 和 Grafana（如果安装了）的追踪。图中的颜色代表服务网格的健康状况。颜色为红色或橙色的节点可能需要注意。组件之间的边的颜色代表这些组件之间的请求的健康状况。节点形状表示组件的类型，如服务、工作负载或应用程序。\n节点和边的健康状况会根据用户的偏好自动刷新。该图也可以暂停以检查一个特定的状态，或重放以重新检查一个特定的时期。\nKiali 提供创建、更新和删除 Istio 配置的操作，由向导驱动。我们可以配置请求路由、故障注入、流量转移和请求超时，所有这些都来自用户界面。如果我们有任何现有的 Istio 配置已经部署，Kiali 可以验证它并报告任何警告或错误。\n 下一章   ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"86ac708f4bc6bded5a10295d0247c625","permalink":"https://lib.jimmysong.io/istio-handbook/observability/kiali/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/istio-handbook/observability/kiali/","section":"istio-handbook","summary":"Kiali 是一个基于 Istio 的服务网格的管理控制台。它提供了仪表盘、可观测性，并让我们通过强大的配置和验证能力来操作网格。它通过推断流量拓扑来显示服务网格，并显示网格的健康状况。Kiali 提供了详细的指标，强大的验","tags":["Istio","Service Mesh"],"title":"Kiali","type":"book"},{"authors":null,"categories":["Istio"],"content":"路由发现服务（RDS）是 Envoy 里面的一个可选 API，用于动态获取路由配置。路由配置包括 HTTP header 修改、虚拟主机以及每个虚拟主机中包含的单个路由规则配置。每个 HTTP 连接管理器都可以通过 API 独立地获取自身的路由配置。\n注意：Envoy 从 1.9 版本开始已不再支持 v1 API。\n统计 RDS 的统计树以 http.\u0026lt;stat_prefix\u0026gt;.rds.\u0026lt;route_config_name\u0026gt;.*.为根，route_config_name名称中的任何:字符在统计树中被替换为_。统计树包含以下统计信息：\n   名字 类型 描述     config_reload Counter 因配置不同而导致配置重新加载的总次数   update_attempt Counter 尝试调用配置加载 API 的总次数   update_success Counter 调用配置加载 API 成功的总次数   update_failure Counter 调用配置加载 API 因网络错误的失败总数   update_rejected Counter 调用配置加载 API 因 schema/验证错误的失败总次数   version Gauge 来自上次成功调用配置加载API的内容哈希    ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"6478162900dc59552379fe248a1b5a65","permalink":"https://lib.jimmysong.io/istio-handbook/data-plane/envoy-rds/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/istio-handbook/data-plane/envoy-rds/","section":"istio-handbook","summary":"路由发现服务（RDS）是 Envoy 里面的一个可选 API，用于动态获取路由配置。路由配置包括 HTTP header 修改、虚拟主机以及每个虚拟主机中包含的单个路由规则配置。每个 HTTP 连接管理器都可以通过 API 独立地获取自身的路由配置。 注意","tags":["Istio","Service Mesh"],"title":"RDS（路由发现服务）","type":"book"},{"authors":null,"categories":["Istio"],"content":"Sidecar 模式是 Istio 服务网格采用的模式，在服务网格出现之前该模式就一直存在，尤其是当微服务出现后开始盛行，本文讲解 Sidecar 模式。\n什么是 Sidecar 模式 将应用程序的功能划分为单独的进程可以被视为 Sidecar 模式。Sidecar 设计模式允许你为应用程序添加许多功能，而无需额外第三方组件的配置和代码。\n就如 Sidecar 连接着摩托车一样，类似地在软件架构中， Sidecar 应用是连接到父应用并且为其扩展或者增强功能。Sidecar 应用与主应用程序松散耦合。\n让我用一个例子解释一下。想象一下假如你有6个微服务相互通信以确定一个包裹的成本。\n每个微服务都需要具有可观测性、监控、日志记录、配置、断路器等功能。所有这些功能都是根据一些行业标准的第三方库在每个微服务中实现的。\n但再想一想，这不是多余吗？它不会增加应用程序的整体复杂性吗？如果你的应用程序是用不同的语言编写时会发生什么——如何合并那些特定用于 .Net、Java、Python 等语言的第三方库。\n使用 Sidecar 模式的优势  通过抽象出与功能相关的共同基础设施到一个不同层降低了微服务代码的复杂度。 因为你不再需要编写相同的第三方组件配置文件和代码，所以能够降低微服务架构中的代码重复度。 降低应用程序代码和底层平台的耦合度。  Sidecar 模式如何工作 Sidecar 是容器应用模式的一种，也是在服务网格中发扬光大的一种模式，详见 Service Mesh 架构解析，其中详细描述了节点代理和 Sidecar 模式的服务网格架构。\n使用 Sidecar 模式部署服务网格时，无需在节点上运行代理（因此您不需要基础结构的协作），但是集群中将运行多个相同的 Sidecar 副本。从另一个角度看：我可以为一组微服务部署到一个服务网格中，你也可以部署一个有特定实现的服务网格。在 Sidecar 部署方式中，你会为每个应用的容器部署一个伴生容器。Sidecar 接管进出应用容器的所有流量。在 Kubernetes 的 Pod 中，在原有的应用容器旁边运行一个 Sidecar 容器，可以理解为两个容器共享存储、网络等资源，可以广义的将这个注入了 Sidecar 容器的 Pod 理解为一台主机，两个容器共享主机资源。\n参考  理解 Istio 服务网格中 Envoy 代理 Sidecar 注入及流量劫持 - jimmysong.io 微服务中的 Sidecar 设计模式解析 - cloudnative.to  ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"2bbbc77bbbc6b9ca66e4295bed316d6e","permalink":"https://lib.jimmysong.io/istio-handbook/concepts/sidecar-pattern/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/istio-handbook/concepts/sidecar-pattern/","section":"istio-handbook","summary":"Sidecar 模式是 Istio 服务网格采用的模式，在服务网格出现之前该模式就一直存在，尤其是当微服务出现后开始盛行，本文讲解 Sidecar 模式。 什么是 Sidecar 模式 将应用程序的功能划分为单独的进程可以被视为 Sidecar 模式。Sidecar 设计模式允许","tags":["Istio","Service Mesh"],"title":"Sidecar 模式","type":"book"},{"authors":null,"categories":["Istio"],"content":"WorkloadGroup 描述了工作负载实例的集合。它提供了一个规范，工作负载实例可用于启动其代理，包括元数据和身份。它只适用于虚拟机等非 Kubernetes 工作负载，旨在模仿现有的用于 Kubernetes 工作负载的 sidecar 注入和部署规范模型，以引导 Istio 代理。\n示例 下面的例子声明了一个代表工作负载集合的工作负载组，这些工作负载将在 bookinfo 命名空间的 reviews 下注册。在引导过程中，这组标签将与每个工作负载实例相关联，端口 3550 和 8080 将与工作负载组相关联，并使用 default 服务账户。app.kubernetes.io/version 只是一个标签的例子。\napiVersion:networking.istio.io/v1alpha3kind:WorkloadGroupmetadata:name:reviewsnamespace:bookinfospec:metadata:labels:app.kubernetes.io/name:reviewsapp.kubernetes.io/version:\u0026#34;1.3.4\u0026#34;template:ports:grpc:3550http:8080serviceAccount:defaultprobe:initialDelaySeconds:5timeoutSeconds:3periodSeconds:4successThreshold:3failureThreshold:3httpGet:path:/foo/barhost:127.0.0.1port:3100scheme:HTTPShttpHeaders:- name:Lit-Headervalue:Im-The-Best配置项 下图是 WorkloadGroup 资源的配置拓扑图。\n  WorkloadGroup 资源配置拓扑图  WorkloadGroup 资源的顶级配置项如下：\n metadata：元数据，将用于所有相应的 WorkloadEntry。WorkloadGroup 的用户标签应在 metadata 中而不是在 template 中设置。 template：用于生成属于该 WorkloadGroup 的 WorkloadEntry 资源的模板。请注意，模板中不应设置 address 和 labels 字段，而空的 serviceAccount 的默认值为 default。工作负载身份（mTLS 证书）将使用指定服务账户的令牌进行引导。该组中的 WorkloadEntry 将与 WorkloadGroup 处于同一命名空间，并继承上述 metadata 字段中的标签和注释。 probe：ReadinessProbe 描述了用户必须为其工作负载的健康检查提供的配置。这个配置在语法和逻辑上大部分都与 K8s 一致。  关于 WorkloadGroup 配置的详细用法请参考 Istio 官方文档。\n参考  WorkloadGroup - istio.io  ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"6d76c63f42134e54b3fd856a88beefed","permalink":"https://lib.jimmysong.io/istio-handbook/config-networking/workload-group/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/istio-handbook/config-networking/workload-group/","section":"istio-handbook","summary":"WorkloadGroup 描述了工作负载实例的集合。它提供了一个规范，工作负载实例可用于启动其代理，包括元数据和身份。它只适用于虚拟机等非 Kubernetes 工作负载，旨在模仿现有的用于 Kubernetes 工作负载的 sidecar 注入和部署规范模型，以引导 Istio 代理。 示例 下面","tags":["Istio","Service Mesh"],"title":"WorkloadGroup","type":"book"},{"authors":null,"categories":["Istio"],"content":"在本节中，我们将为推荐服务引入 5 秒的延迟。Envoy 将为 50% 的请求注入延迟。\napiVersion:networking.istio.io/v1alpha3kind:VirtualServicemetadata:name:recommendationservicespec:hosts:- recommendationservicehttp:- route:- destination:host:recommendationservicefault:delay:percentage:value:50fixedDelay:5s将上述 YAML 保存为 recommendation-delay.yaml，然后用 kubectl apply -f recommendation-delay.yaml 创建 VirtualService。\n我们可以在浏览器中打开 INGRESS_HOST，然后点击其中一个产品。推荐服务的结果显示在屏幕底部的”Other Products You Might Light“部分。如果我们刷新几次页面，我们会注意到，该页面要么立即加载，要么有一个延迟加载页面。这个延迟是由于我们注入了 5 秒的延迟。\n我们可以打开 Grafana（getmesh istioctl dash grafana）和 Istio 服务仪表板。确保从服务列表中选择recommendationsservice，在 Reporter 下拉菜单中选择 source，并查看显示延迟的 Client Request Duration，如下图所示。\n   Recommendations 服务延迟  同样地，我们可以注入一个中止。在下面的例子中，我们为发送到产品目录服务的 50% 的请求注入一个 HTTP 500。\napiVersion:networking.istio.io/v1alpha3kind:VirtualServicemetadata:name:productcatalogservicespec:hosts:- productcatalogservicehttp:- route:- destination:host:productcatalogservicefault:abort:percentage:value:50httpStatus:500将上述 YAML 保存为 productcatalogservice-abort.yaml，然后用 kubectl apply -f productcatalogservice-abort.yaml 更新 VirtualService。\n如果我们刷新几次产品页面，我们应该得到如下图所示的错误信息。\n   注入错误  请注意，错误信息说，失败的原因是故障过滤器中止。如果我们打开 Grafana（getmesh istioctl dash grafana），我们也会注意到图中报告的错误。\n我们可以通过运行 kubectl delete vs productcatalogservice 来删除 VirtualService。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"ee50c7c47c0101dbbaae3f0c9dcd0d39","permalink":"https://lib.jimmysong.io/istio-handbook/practice/fault-injection/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/istio-handbook/practice/fault-injection/","section":"istio-handbook","summary":"在本节中，我们将为推荐服务引入 5 秒的延迟。Envoy 将为 50% 的请求注入延迟。 apiVersion:networking.istio.io/v1alpha3kind:VirtualServicemetadata:name:recommendationservicespec:hosts:- recommendationservicehttp:- route:- destination:host:recommendationservicefault:delay:percentage","tags":["Istio","Service Mesh"],"title":"错误注入","type":"book"},{"authors":null,"categories":["Istio"],"content":"弹性（Resiliency）是指在面对故障和对正常运行的挑战时，提供和保持可接受的服务水平的能力。这不是为了避免故障。它是以一种没有停机或数据丢失的方式来应对它们。弹性的目标是在故障发生后将服务恢复到一个完全正常的状态。\n使服务可用的一个关键因素是在提出服务请求时使用超时（timeout）和重试（retry）策略。我们可以在 Istio 的 VirtualService 上配置这两者。\n使用超时字段，我们可以为 HTTP 请求定义一个超时。如果请求的时间超过了超时字段中指定的值，Envoy 代理将放弃请求，并将其标记为超时（向应用程序返回一个 HTTP 408）。连接将保持开放，除非触发了异常点检测。下面是一个为路由设置超时的例子：\n...- route:- destination:host:customers.default.svc.cluster.localsubset:v1timeout:10s...除了超时之外，我们还可以配置更细化的重试策略。我们可以控制一个给定请求的重试次数，每次尝试的超时时间，以及我们想要重试的具体条件。\n例如，我们可以只在上游服务器返回任何 5xx 响应代码时重试请求，或者只在网关错误（HTTP 502、503 或 504）时重试，或者甚至在请求头中指定可重试的状态代码。重试和超时都发生在客户端。当 Envoy 重试一个失败的请求时，最初失败并导致重试的端点就不再包含在负载均衡池中了。假设 Kubernetes 服务有 3 个端点（Pod），其中一个失败了，并出现了可重试的错误代码。当 Envoy 重试请求时，它不会再向原来的端点重新发送请求。相反，它将把请求发送到两个没有失败的端点中的一个。\n下面是一个例子，说明如何为一个特定的目的地设置重试策略。\n...- route:- destination:host:customers.default.svc.cluster.localsubset:v1retries:attempts:10perTryTimeout:2sretryOn:connect-failure,reset...上述重试策略将尝试重试任何连接超时（connect-failure）或服务器完全不响应（reset）的失败请求。我们将每次尝试的超时时间设置为 2 秒，尝试的次数设置为 10 次。注意，如果同时设置重试和超时，超时值将是请求等待的最长时间。如果我们在上面的例子中指定了 10 秒的超时，那么即使重试策略中还剩下一些尝试，我们也只能最多等待 10 秒。\n关于重试策略的更多细节，请参阅 x-envoy-retry-on 文档。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"dfd5d5384615723eaa2ba031c6c50dca","permalink":"https://lib.jimmysong.io/istio-handbook/traffic-management/resiliency/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/istio-handbook/traffic-management/resiliency/","section":"istio-handbook","summary":"弹性（Resiliency）是指在面对故障和对正常运行的挑战时，提供和保持可接受的服务水平的能力。这不是为了避免故障。它是以一种没有停机或数据丢失的方式来应对它们。弹性的目标是在故障发生后将服务恢复到","tags":["Istio","Service Mesh"],"title":"弹性","type":"book"},{"authors":null,"categories":["Envoy"],"content":"负载均衡是一种在单个上游集群的多个端点之间分配流量的方式。在众多端点之间分配流量的原因是为了最好地利用可用资源。\n为了实现资源的最有效利用，Envoy 提供了不同的负载均衡策略，可以分为两组：全局负载均衡和分布式负载均衡。不同的是，在全局负载均衡中，我们使用单一的控制平面来决定端点之间的流量分配。Envoy 决定负载如何分配（例如，使用主动健康检查、分区感知路由、负载均衡策略）。\n在多个端点之间分配负载的技术之一是一致性哈希。服务器使用请求的一部分来创建一个哈希值来选择一个端点。在模数散列中，哈希值被认为是一个巨大的数字。为了得到发送请求的端点索引，我们将哈希值与可用端点的数量取余数（index=hash % endpointCount）。如果端点的数量是稳定的，这种方法效果很好。然而，如果端点被添加或删除（即它们不健康，我们扩大或缩小它们的规模，等等），大多数请求将在一个与以前不同的端点上结束。\n一致性哈希是一种方法，每个端点根据某些属性被分配多个有价值的值。然后，每个请求被分配到具有最接近哈希值的端点。这种方法的价值在于，当我们添加或删除端点时，大多数请求最终会被分配到与之前相同的端点。拥有这种 “粘性” 是有帮助的，因为它不会干扰端点持有的任何缓存。\n负载均衡策略 Envoy 使用其中一个负载均衡策略来选择一个端点发送流量。负载均衡策略是可配置的，可以为每个上游集群分别指定。请注意，负载均衡只在健康的端点上执行。如果没有定义主动或被动的健康检查，则假定所有端点都是健康的。\n我们可以使用 lb_policy 字段和其他针对所选策略的字段来配置负载均衡策略。\n加权轮询（默认） 加权轮询（ROUND_ROBIN）以轮询顺序选择端点。如果端点是加权的，那么就会使用加权的轮询顺序。这个策略给我们提供了一个可预测的请求在所有端点的分布。权重较高的端点将在轮转中出现得更频繁，以实现有效的加权。\n加权的最小请求 加权最小请求（LEAST_REQUEST）算法取决于分配给端点的权重。\n如果所有的端点权重相等，算法会随机选择 N 个可用的端点（choice_count），并挑选出活动请求最少的一个。\n如果端点的权重不相等，该算法就会转入一种模式，即使用加权的循环计划，其中的权重是根据选择时端点的请求负荷动态调整。\n以下公式用于动态计算权重。\nweight = load_balancing_weight / (active_requests + 1)^active_request_bias active_request_bias 是可配置的（默认为 1.0）。主动请求偏差越大，主动请求就越积极地降低有效权重。\n如果 active_request_bias 被设置为 0，那么算法的行为就像轮询一样，在挑选时忽略了活动请求数。\n我们可以使用 least_request_lb_config 字段来设置加权最小请求的可选配置。\n...lb_policy:LEAST_REQUESTleast_request_lb_config:choice_count:5active_request_bias:0.5...环形哈希 环形哈希（或模数散列）算法（RING_HASH）实现了对端点的一致性哈希。每个端点地址（默认设置）都被散列并映射到一个环上。Envoy 通过散列一些请求属性，并在环上顺时针找到最近的对应端点，将请求路由到一个端点。哈希键默认为端点地址；然而，它可以使用 hash_key 字段改变为任何其他属性。\n我们可以通过指定最小（minimum_ring_size）和最大（maximum_ring_size）的环形哈希算法，并使用统计量（min_hashes_per_host 和 max_hashes_per_host）来确保良好的分布。环越大，请求的分布就越能反映出所需的权重。最小环大小默认为 1024 个条目（限制在 8M 个条目），而最大环大小默认为 8M（限制在 8M）。\n我们可以使用 ring_hash_lb_config 字段设置环形哈希的可选配置。\n...lb_policy:RING_HASHring_hash_lb_config:minimum_ring_size:2000maximum_ring_size:10000...Maglev 与环形哈希算法一样，maglev（MAGLEV）算法也实现了对端点的一致性哈希。该算法产生一个查找表，允许在一个恒定的时间内找到一个项目。Maglev 的设计是为了比环形哈希算法的查找速度更快，并且使用更少的内存。你可以在下面这篇文章中阅读更多关于它的内容 Maglev: A Fast and Reliable Software Network Load Balancer。\n我们可以使用 maglev_lb_config 字段来设置 maglev 算法的可选配置。\n...lb_policy:MAGLEVmaglev_lb_config:table_size:69997...默认的表大小是 65537，但它可以被设置为任何素数，只要它不大于 5000011。\n原始目的地 原始目的地（ORIGINAL_DESTINATION）是一个特殊用途的负载均衡器，只能与原始目的地集群一起使用。我们在谈到原始目的地集群类型时已经提到了原始目的地负载均衡器。\n随机 顾名思义，随机（RANDOM）算法会挑选一个可用的随机端点。如果你没有配置主动健康检查策略，随机算法的表现比轮询算法更好。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"e7cb74305697309429cab75c6a1b0ad9","permalink":"https://lib.jimmysong.io/envoy-handbook/cluster/load-balancing/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/envoy-handbook/cluster/load-balancing/","section":"envoy-handbook","summary":"负载均衡是一种在单个上游集群的多个端点之间分配流量的方式。在众多端点之间分配流量的原因是为了最好地利用可用资源。 为了实现资源的最有效利用，Envoy 提供了不同的负载均衡策略，可以分为两组：全局负载均衡","tags":["Envoy"],"title":"负载均衡","type":"book"},{"authors":null,"categories":["Envoy"],"content":"集群端点（/clusters）将显示配置的集群列表，并包括以下信息：\n 每个主机的统计数据 每个主机的健康状态 断路器设置 每个主机的权重和位置信息   这里的主机指的是每个被发现的属于上游集群的主机。\n 下面的片段显示了信息的模样（注意，输出是经过裁剪的）。\n{ \u0026#34;cluster_statuses\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;api_google_com\u0026#34;, \u0026#34;host_statuses\u0026#34;: [ { \u0026#34;address\u0026#34;: { \u0026#34;socket_address\u0026#34;: { \u0026#34;address\u0026#34;: \u0026#34;10.0.0.1\u0026#34;, \u0026#34;port_value\u0026#34;: 8080 } }, \u0026#34;stats\u0026#34;: [ { \u0026#34;value\u0026#34;: \u0026#34;23\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;cx_total\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;rq_error\u0026#34; }, { \u0026#34;value\u0026#34;: \u0026#34;51\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;rq_success\u0026#34; }, ... ], \u0026#34;health_status\u0026#34;: { \u0026#34;eds_health_status\u0026#34;: \u0026#34;HEALTHY\u0026#34; }, \u0026#34;weight\u0026#34;: 1, \u0026#34;locality\u0026#34;: {} } ], \u0026#34;circuit_breakers\u0026#34;: { \u0026#34;thresholds\u0026#34;: [ { \u0026#34;max_connections\u0026#34;: 1024, \u0026#34;max_pending_requests\u0026#34;: 1024, \u0026#34;max_requests\u0026#34;: 1024, \u0026#34;max_retries\u0026#34;: 3 }, { \u0026#34;priority\u0026#34;: \u0026#34;HIGH\u0026#34;, \u0026#34;max_connections\u0026#34;: 1024, \u0026#34;max_pending_requests\u0026#34;: 1024, \u0026#34;max_requests\u0026#34;: 1024, \u0026#34;max_retries\u0026#34;: 3 } ] }, \u0026#34;observability_name\u0026#34;: \u0026#34;api_google_com\u0026#34; }, ...  为了获得 JSON 输出，我们可以在发出请求或在浏览器中打开 URL 时附加 ?format=json。\n 主机统计 输出包括每个主机的统计数据，如下表所解释。\n   指标名称 描述     cx_total 连接总数   cx_active 有效连接总数   cx_connect_fail 连接失败总数   rq_total 请求总数   rq_timeout 超时的请求总数   rq_success 有非 5xx 响应的请求总数   rq_error 有 5xx 响应的请求总数   rq_active 有效请求总数    主机健康状况 主机的健康状况在 health_status 字段下报告。健康状态中的值取决于健康检查是否被启用。假设启用了主动和被动（断路器）健康检查，该表显示了可能包含在 health_status 字段中的布尔字段。\n   字段名称 描述     failed_active_health_check 真，如果该主机目前未能通过主动健康检查。   failed_outlier_check 真，如果该宿主目前被认为是一个异常，并已被弹出。   failed_active_degraded_check 如果主机目前通过主动健康检查被标记为降级，则为真。   pending_dynamic_removal 如果主机已经从服务发现中移除，但由于主动健康检查正在稳定，则为真。   pending_active_hc 真，如果该主机尚未被健康检查。   excluded_via_immediate_hc_fail 真，如果该主机应被排除在恐慌、溢出等计算之外，因为它被明确地通过协议信号从轮换中取出，并且不打算被路由到。   active_hc_timeout 真，如果主机由于超时而导致活动健康检查失败。   eds_health_status 默认情况下，设置为healthy（如果不使用 EDS）。否则，它也可以被设置为unhealthy 或 degraded。    请注意，表中的字段只有在设置为真时才会被报告。例如，如果主机是健康的，那么健康状态将看起来像这样。\n\u0026#34;Health_status\u0026#34;:{ \u0026#34;eds_health_status\u0026#34;:\u0026#34;HEALTHY\u0026#34; } 如果配置了主动健康检查，而主机是失败的，那么状态将看起来像这样。\n\u0026#34;Health_status\u0026#34;:{ \u0026#34;failed_active_health_check\u0026#34;: true, \u0026#34;eds_health_status\u0026#34;:\u0026#34;HEALTHY\u0026#34; }  ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"8c66f48932886efae67d6b9d5d180cba","permalink":"https://lib.jimmysong.io/envoy-handbook/admin-interface/clusters/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/envoy-handbook/admin-interface/clusters/","section":"envoy-handbook","summary":"集群端点（/clusters）将显示配置的集群列表，并包括以下信息： 每个主机的统计数据 每个主机的健康状态 断路器设置 每个主机的权重和位置信息 这里的主机指的是每个被发现的属于上游集群的主机。 下面的片段显示","tags":["Envoy"],"title":"集群","type":"book"},{"authors":null,"categories":["Envoy"],"content":"在这个实验中，我们将学习使用日志过滤器，根据一些请求属性，只记录某些请求。\n让我们想出几个我们想用过滤器来实现的日志要求。\n 将所有带有 HTTP 404 状态码的请求记录到一个名为 not_found.log 的日志文件中。 将所有带有 env=debug 头值的请求记录到一个名为 debug.log 的日志文件中。 将所有 POST 请求记录到标准输出  基于这些要求，我们将有两个访问记录器记录到文件（not_found.log 和 debug.log）和一个写到 stdout 的访问记录器。\naccess_log 字段是一个数组，我们可以在它下面定义多个记录器。在各个日志记录器里面，我们可以使用 filter 字段来指定何时将字符串写到日志中。\n对于第一个要求，我们将使用状态码过滤器，而对于第二个要求，我们将使用 Header 过滤器。下面是配置的样子。\nstatic_resources:listeners:- name:listener_0address:socket_address:address:0.0.0.0port_value:10000filter_chains:- filters:- name:envoy.filters.network.http_connection_managertyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManagerstat_prefix:ingress_httpaccess_log:- name:envoy.access_loggers.filetyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.access_loggers.file.v3.FileAccessLogpath:./debug.logfilter:header_filter:header:name:envstring_match:exact:debug- name:envoy.access_loggers.filetyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.access_loggers.file.v3.FileAccessLogpath:./not_found.logfilter:status_code_filter:comparison:value:default_value:404runtime_key:ingress_http_status_code_filter- name:envoy.access_loggers.stdouttyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.access_loggers.stream.v3.StdoutAccessLogfilter:header_filter:header:name:\u0026#34;:method\u0026#34;string_match:exact:POSThttp_filters:- name:envoy.filters.http.routerroute_config:name:my_first_routevirtual_hosts:- name:direct_response_servicedomains:[\u0026#34;*\u0026#34;]routes:- match:prefix:\u0026#34;/404\u0026#34;direct_response:status:404body:inline_string:\u0026#34;404\u0026#34;- match:prefix:\u0026#34;/\u0026#34;direct_response:status:200body:inline_string:\u0026#34;200\u0026#34;将上述 YAML 保存为 6-lab-1-logging-filters.yaml 并在后台运行 Envoy。\nfunc-e run -c 6-lab-1-logging-filters.yaml \u0026amp; 在 Envoy 运行的情况下，如果我们发送一个请求到 http://localhost:10000，那么我们会注意到没有任何东西被写入标准输出或日志文件。\n接下来让我们试试 POST 请求。\n$ curl -X POST localhost:10000 [2021-11-03T21:52:36.398Z] \u0026#34;POST / HTTP/1.1\u0026#34; 200 - 0 3 0 - \u0026#34;-\u0026#34; \u0026#34;curl/7.64.0\u0026#34; \u0026#34;528335ae-8f0d-4d22-934a-02d4702a9c62\u0026#34; \u0026#34;localhost:10000\u0026#34; \u0026#34;-\u0026#34; 你会注意到，日志条目被写到了配置中定义的标准输出。\n接下来，让我们发送一个头信息 env: debug 与以下请求。\ncurl -H \u0026#34;env: debug\u0026#34; localhost:10000 像第一个例子一样，没有任何东西会被写入标准输出（这不是一个 POST 请求）。然而，如果我们在 debug.log 文件中查看，那么我们会看到日志条目。\n$ cat debug.log [2021-11-03T21:54:49.357Z] \u0026#34;GET / HTTP/1.1\u0026#34; 200 - 0 3 0 - \u0026#34;-\u0026#34; \u0026#34;curl/7.64.0\u0026#34; \u0026#34;ea2a11d6-6ccb-4f13-9686-4d30dbc3136e\u0026#34; \u0026#34;localhost:10000\u0026#34; \u0026#34;-\u0026#34; 同样地，让我们向 /404 发送一个请求，并查看 not_found.log 文件。\n$ curl localhost:10000/404 404 $ cat not_found.log [2021-11-03T21:55:37.891Z] \u0026#34;GET /404 HTTP/1.1\u0026#34; 404 - 0 3 0 - \u0026#34;-\u0026#34; \u0026#34;curl/7.64.0\u0026#34; \u0026#34;59bf1a1a-62b2-49e4-9226-7c49516ec390\u0026#34; \u0026#34;localhost:10000\u0026#34; \u0026#34;-\u0026#34; 在满足多个过滤条件的情况下（例如，我们有一个 POST 请求*，*我们将请求发送到 /404），在这种情况下，日志将被写入标准输出和 not_found.log。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"6751faaa9826cbfb43d9cde63abf01d5","permalink":"https://lib.jimmysong.io/envoy-handbook/logging/lab12/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/envoy-handbook/logging/lab12/","section":"envoy-handbook","summary":"在这个实验中，我们将学习使用日志过滤器，根据一些请求属性，只记录某些请求。 让我们想出几个我们想用过滤器来实现的日志要求。 将所有带有 HTTP 404 状态码的请求记录到一个名为 not_found.log 的日志文件中。 将所有带有 env=debug 头值的请求记","tags":["Envoy"],"title":"实验 12：使用日志过滤器","type":"book"},{"authors":null,"categories":["Envoy"],"content":"在这个实验中，我们将使用 TinyGo、proxy-wasm-go-sdk 和 func-e CLI 来构建和测试一个 Envoy Wasm 扩展。\n我们将写一个简单的 Wasm 模块，为响应头添加一个头。稍后，我们将展示如何读取配置和添加自定义指标。我们将使用 Golang 并使用 TinyGo 编译器进行编译。\n安装 TinyGo 让我们下载并安装 TinyGo。\nwget https://github.com/tinygo-org/tinygo/releases/download/v0.21.0/tinygo_0.21.0_amd64.deb sudo dpkg -i tinygo_0.21.0_amd64.deb 你可以运行 tinygo version 来检查安装是否成功。\n$ tinygo version tinygo version 0.21.0 linux/amd64 (using go version go1.17.2 and LLVM version 11.0.0) 为 Wasm 模块搭建脚手架 我们将首先为我们的扩展创建一个新的文件夹，初始化 Go 模块，并下载 SDK 依赖。\n$ mkdir header-filter \u0026amp;\u0026amp; cd header-filter $ go mod init header-filter $ go mod edit -require=github.com/tetratelabs/proxy-wasm-go-sdk@main $ go mod download github.com/tetratelabs/proxy-wasm-go-sdk 接下来，让我们创建 main.go 文件，其中有我们 WASM 扩展的代码。\npackage main import ( \u0026#34;github.com/tetratelabs/proxy-wasm-go-sdk/proxywasm\u0026#34; \u0026#34;github.com/tetratelabs/proxy-wasm-go-sdk/proxywasm/types\u0026#34; ) func main() { proxywasm.SetVMContext(\u0026amp;vmContext{}) } type vmContext struct { // Embed the default VM context here,  // so that we don\u0026#39;t need to reimplement all the methods.  types.DefaultVMContext } // Override types.DefaultVMContext. func (*vmContext) NewPluginContext(contextID uint32) types.PluginContext { return \u0026amp;pluginContext{} } type pluginContext struct { // Embed the default plugin context here,  // so that we don\u0026#39;t need to reimplement all the methods.  types.DefaultPluginContext } // Override types.DefaultPluginContext. func (*pluginContext) NewHttpContext(contextID uint32) types.HttpContext { return \u0026amp;httpHeaders{contextID: contextID} } type httpHeaders struct { // Embed the default http context here,  // so that we don\u0026#39;t need to reimplement all the methods.  types.DefaultHttpContext contextID uint32 } func (ctx *httpHeaders) OnHttpRequestHeaders(numHeaders int, endOfStream bool) types.Action { proxywasm.LogInfo(\u0026#34;OnHttpRequestHeaders\u0026#34;) return types.ActionContinue } func (ctx *httpHeaders) OnHttpResponseHeaders(numHeaders int, endOfStream bool) types.Action { proxywasm.LogInfo(\u0026#34;OnHttpResponseHeaders\u0026#34;) return types.ActionContinue } func (ctx *httpHeaders) OnHttpStreamDone() { proxywasm.LogInfof(\u0026#34;%d finished\u0026#34;, ctx.contextID) } 将上述内容保存在一个名为 main.go 的文件中。\n让我们建立过滤器，检查是否一切正常。\ntinygo build -o main.wasm -scheduler=none -target=wasi main.go 构建命令应该成功运行并生成一个名为 main.wasm 的文件。\n我们将使用 func-e 来运行一个本地 Envoy 实例来测试我们构建的扩展。\n首先，我们需要一个 Envoy 配置，它将配置扩展。\nstatic_resources:listeners:- name:mainaddress:socket_address:address:0.0.0.0port_value:10000filter_chains:- filters:- name:envoy.http_connection_managertyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManagerstat_prefix:ingress_httpcodec_type:autoroute_config:name:local_routevirtual_hosts:- name:local_servicedomains:- \u0026#34;*\u0026#34;routes:- match:prefix:\u0026#34;/\u0026#34;direct_response:status:200body:inline_string:\u0026#34;hello world\\n\u0026#34;http_filters:- name:envoy.filters.http.wasmtyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/udpa.type.v1.TypedStructtype_url:type.googleapis.com/envoy.extensions.filters.http.wasm.v3.Wasmvalue:config:vm_config:runtime:\u0026#34;envoy.wasm.runtime.v8\u0026#34;code:local:filename:\u0026#34;main.wasm\u0026#34;- name:envoy.filters.http.routeradmin:address:socket_address:address:127.0.0.1port_value:9901将上述内容保存到 8-lab-2-wasm-config.yaml 文件。\nEnvoy 的配置在 10000 端口设置了一个监听器，返回一个直接响应（HTTP 200），正文是 hello world。在 http_filters 部分，我们配置了 envoy.filters.http.wasm 过滤器，并引用了我们之前建立的本地 WASM 文件（main.wasm）。\n让我们在后台用这个配置运行 Envoy。\nfunc-e run -c 8-Lab-2-wasm-config.yaml \u0026amp; Envoy 实例的启动应该没有任何问题。一旦启动，我们就可以向 Envoy 监听的端口（10000）发送一个请求。\n$ curl -v localhost:10000 ... \u0026lt; HTTP/1.1 200 OK \u0026lt; content-length: 13 \u0026lt; content-type: text/plain \u0026lt; my-new-header: some-value-here \u0026lt; date: Mon, 22 Jun 2021 17:02:31 GMT \u0026lt; server: envoy \u0026lt; hello world 输出显示了两个日志条目：一个来自 OnHttpRequestHeaders 处理器，第二个来自 OnHttpResponseHeaders 处理器。最后一行是过滤器中的直接响应配置所返回的响应示例。\n你可以通过用 fg 把进程带到前台，然后按 CTRL+C 停止代理。\n在 HTTP 响应上设置附加头信息 让我们打开 main.go 文件，在响应头信息中添加一个头信息。我们将更新 OnHttpResponseHeaders 函数来做到这一点。\n我们将调用 AddHttpResponseHeader 函数来添加一个新的头。更新 OnHttpResponseHeaders 函数，使其看起来像这样。\nfunc (ctx *httpHeaders) OnHttpResponseHeaders(numHeaders int, endOfStream bool) types.action { proxywasm.LogInfo(\u0026#34;OnHttpResponseHeaders\u0026#34;) err := proxywasm.AddHttpResponseHeader(\u0026#34;my-new-header\u0026#34;, \u0026#34;some-value-here\u0026#34;) if err != nil { proxywasm.LogCriticalf(\u0026#34; failed to add response header: %v\u0026#34;, err) } return types.ActionContinue } 让我们重新建立扩展。\ntinygo build -o main.wasm -scheduler=none -target=wasi main.go 现在我们可以用更新后的扩展来重新运行 Envoy 代理。\nfunc-e run -c 8-Lab-2-wasm-config.yaml \u0026amp; 现在，如果我们再次发送一个请求（确保添加 -v 标志），我们将看到被添加到响应中的头。\n$ curl -v localhost:10000 ... \u0026lt; HTTP/1.1 200 OK \u0026lt; content-length: 13 \u0026lt; content-type: text/plain \u0026lt; my-new-header: some-value-here \u0026lt; date: Mon, 22 Jun 2021 17:02:31 GMT \u0026lt; server: envoy \u0026lt; hello world 从配置中读取数值 在代码中硬编码这样的值从来不是一个好主意。让我们看看我们如何读取额外的头文件。\n1. 将 additionalHeaders 和 contextID 添加到 pluginContext 结构体中：\ntype pluginContext struct { // 在这里嵌入默认的插件上下文。 // 这样我们就不需要重新实现所有的方法了。  types.DefaultPluginContext additionalHeaders map[string]string contextID uint32 } 2. 更新 NewPluginContext 函数以初始化数值。\nfunc …","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"bf0a7b6da2a886a254332d8158c4ebb4","permalink":"https://lib.jimmysong.io/envoy-handbook/extending-envoy/lab17/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/envoy-handbook/extending-envoy/lab17/","section":"envoy-handbook","summary":"在这个实验中，我们将使用 TinyGo、proxy-wasm-go-sdk 和 func-e CLI 来构建和测试一个 Envoy Wasm 扩展。 我们将写一个简单的 Wasm 模块，为响应头添加一个头。稍后，我们将展示如何读取配置和添加自定义指标。我们","tags":["Envoy"],"title":"实验17：使用 Wasm 和 Go 扩展 Envoy","type":"book"},{"authors":null,"categories":["Envoy"],"content":"原始源过滤器（envoy.filters.listener.original_src）在 Envoy 的上游（接收 Envoy 请求的主机）一侧复制了连接的下游（连接到 Envoy 的主机）的远程地址。\n例如，如果我们用 10.0.0.1 发送请求给 Envoy 连接到上游，源 IP 是 10.0.0.1 。这个地址是由代理协议过滤器决定的（接下来解释），或者它可以来自于可信的 HTTP Header。\n- name:envoy.filters.listener.original_srctyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions. filters.listener.original_src.v3.OriginalSrcmark:100该过滤器还允许我们在上游连接的套接字上设置 SO_MARK 选项。SO_MARK 选项用于标记通过套接字发送的每个数据包，并允许我们做基于标记的路由（我们可以在以后匹配标记）。\n上面的片段将该标记设置为 100。使用这个标记，我们可以确保非本地地址在绑定到原始源地址时可以通过 Envoy 代理路由回来。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"a01004c5858d90be8df8318bd9b86128","permalink":"https://lib.jimmysong.io/envoy-handbook/listener/original-source-listener-filter/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/envoy-handbook/listener/original-source-listener-filter/","section":"envoy-handbook","summary":"原始源过滤器（envoy.filters.listener.original_src）在 Envoy 的上游（接收 Envoy 请求的主机）一侧复制了连接的下游（连接到 Envoy 的主机）的远程地址。 例如，如果我们用 10.0.0.1 发送请求给 Envoy 连接","tags":["Envoy"],"title":"原始源监听器过滤器","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"Calico 原意为”有斑点的“，如果说一只猫为 calico cat 的话，就是说这是只花猫，也叫三色猫，所以 calico 的 logo 是只三色猫。\n   Calico logo  概念 Calico 创建和管理一个扁平的三层网络（不需要 overlay），每个容器会分配一个可路由的 IP。由于通信时不需要解包和封包，网络性能损耗小，易于排查，且易于水平扩展。\n小规模部署时可以通过 BGP client 直接互联，大规模下可通过指定的 BGP Route Reflector 来完成，这样保证所有的数据流量都是通过 IP 路由的方式完成互联的。\nCalico 基于 iptables 还提供了丰富而灵活的网络 Policy，保证通过各个节点上的 ACL 来提供 Workload 的多租户隔离、安全组以及其他可达性限制等功能。\nCalico 架构 Calico 由以下组件组成，在部署 Calico 的时候部分组件是可选的。\n Calico API server Felix BIRD confd Dikastes CNI 插件 数据存储插件 IPAM 插件 kube-controllers Typha calicoctl 云编排器插件  Calico 的架构图如下所示：\n   Calico 架构图（图片来自 Calico 官网）  Calico API Server 可以使用 kubectl 直接管理 Calico。\nFelix Felix 以 agent 代理的形式在每台机器端点上运行。对路由和 ACL 以及主机编程，为该主机上的端点提供所需的连接。\n根据具体的编排器环境，Felix负责：\n接口管理\n将有关接口的信息编入内核，以便内核能够正确处理来自该端点的流量。特别是，确保主机响应来自每个工作负载的ARP请求，提供主机的MAC，并为它所管理的接口启用IP转发。它还监控接口，以确保编程在适当的时候应用。\n路由编程\n将其主机上的端点的路由编程到Linux内核的FIB（转发信息库）。这可以确保到达主机上的以这些端点为目的地的数据包被相应地转发。\nACL编程\n在Linux内核中编程ACL，以确保只有有效的流量可以在端点之间发送，并且端点不能规避Calico的安全措施。\n状态报告\n提供网络健康数据。特别是在配置其主机时报告错误和问题。这些数据被写入数据存储，以便对网络的其他组件和运营商可见。\nBIRD BGP Internet Routing Daemon，简称 BIRD。从Felix获取路由，并分发到网络上的BGP peer，用于主机间的路由。在每个Felix代理的节点上运行。\nBGP客户端负责：\n路由分配\n当Felix将路由插入Linux内核的FIB时，BGP客户端将它们分配给部署中的其他节点。这确保了部署中的有效流量路由。\nBGP路由反射器的配置\nBGP路由反射器通常是为大型部署而配置的，而不是一个标准的BGP客户端。BGP路由反射器作为连接BGP客户端的一个中心点。(标准BGP要求每个BGP客户端在网状拓扑结构中与其他每个BGP客户端连接，这很难维护)。\n为了实现冗余，你可以无缝部署多个BGP路由反射器。BGP路由反射器只参与网络的控制：没有终端数据通过它们。当Calico BGP客户端将其FIB中的路由通告给路由反射器时，路由反射器将这些路由通告给部署中的其他节点。\nconfd 开源的、轻量级的配置管理工具。监控Calico数据存储对BGP配置和全局默认的日志变更，如AS号、日志级别和IPAM信息。\nConfd根据存储中的数据更新，动态生成BIRD配置文件。当配置文件发生变化时，confd会触发BIRD加载新的文件。\nDikastes 执行Istio服务网格的网络策略。作为Istio Envoy的一个Sidecar代理，在集群上运行。\nDikastes 是可选的。Calico在Linux内核（使用iptables，在三、四层）和三到七层使用Envoy的Sidecar代理Dikastes为工作负载执行网络策略，对请求进行加密认证。使用多个执行点可以根据多个标准确定远程端点的身份。即使工作负载Pod破坏，Envoy代理被绕过，主机Linux内核的执行也能保护你的工作负载。\nCNI 插件 为Kubernetes集群提供Calico网络。\n向Kubernetes展示该API的Calico二进制文件被称为CNI插件，必须安装在Kubernetes集群的每个节点上。Calico CNI插件允许你为任何使用 CNI 网络规范的编排调度器使用Calico网络。\n数据存储插件 通过减少每个节点对数据存储的影响来增加规模。它是Calico CNI的插件之一。\nKubernetes API datastore（kdd）\n在Calico中使用Kubernetes API数据存储（kdd）的优点是：\n 管理更简单，因为不需要额外的数据存储 使用Kubernetes RBAC来控制对Calico资源的访问 使用Kubernetes审计日志来生成对Calico资源变化的审计日志  etcd\netcd 是一个一致的、高可用的分布式键值存储，为Calico网络提供数据存储，并用于组件之间的通信。etcd仅支持保护非集群主机（从Calico v3.1开始）。etcd的优点是：\n 让你在非Kubernetes平台上运行Calico 分离Kubernetes和Calico资源之间的关注点，例如允许你独立地扩展数据存储。 让你运行的Calico集群不仅仅包含一个Kubernetes集群，例如，让带有Calico主机保护的裸机服务器与Kubernetes集群互通；或者多个Kubernetes集群。  IPAM 插件 使用Calico的IP池资源来控制如何将IP地址分配给集群中的pod。它是大多数Calico安装所使用的默认插件。它是Calico CNI插件之一。\nkube-controller 监控Kubernetes的API，并根据集群状态执行行动。\ntigera/kube-controllers 容器包括以下控制器：\n Policy 控制器 Namespace 控制器 ServiceAccount 控制器 WorkloadEndpoint 控制器 Node 控制器  Typha 通过减少每个节点对数据存储的影响来增加规模。作为数据存储和Felix实例之间的一个守护程序运行。默认安装，但没有配置。\nTypha代表Felix和confd等所有客户端维护一个单一的数据存储连接。它缓存数据存储的状态，并复制事件，以便它们可以被推广到更多监听器。因为一个Typha实例可以支持数百个Felix实例，可以将数据存储的负载降低很多。由于Typha可以过滤掉与Felix无关的更新，它也减少了Felix的CPU使用。在一个大规模（100多个节点）的Kubernetes集群中，这是至关重要的，因为API服务器产生的更新数量随着节点数量的增加而增加。\ncalicoctl Calicoctl命令行作为二进制或容器需要单独安装，可以在任何可以通过网络访问Calico数据存储的主机上使用。\n云编排器插件 将管理网络的编排器API翻译成Calico的数据模型和数据存储。\n对于云供应商，Calico为每个主要的云编排平台提供了一个单独的插件。这使得Calico能够与编排器紧密结合，因此用户可以使用他们的编排器工具来管理Calico网络。当需要时，编排器插件会将Calico网络的反馈信息提供给编排器。例如，提供关于Felix liveness的信息，并在网络设置失败时将特定端点标记为失败。\n参考  Calico 组件架构 - docs.projectcalico.org  ","date":1653062400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"d3fec51dfd68389bbc35f7dc08e499be","permalink":"https://lib.jimmysong.io/kubernetes-handbook/networking/calico/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/networking/calico/","section":"kubernetes-handbook","summary":"Calico 原意为”有斑点的“，如果说一只猫为 calico cat 的话，就是说这是只花猫，也叫三色猫，所以 calico 的 logo 是只三色猫。 Calico logo 概念 Calico 创建和管理一个扁平的三层网络（不需要 overlay），每个容器会分配一个可路由的 IP。由于通","tags":["Kubernetes"],"title":"非 Overlay 扁平网络 Calico","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"Cilium 是一款开源软件，也是 CNCF 的孵化项目，目前已有公司提供商业化支持，还有基于 Cilium 实现的服务网格解决方案。最初它仅是作为一个 Kubernetes 网络组件。Cilium 在 1.7 版本后推出并开源了 Hubble，它是专门为网络可视化设计，能够利用 Cilium 提供的 eBPF 数据路径，获得对 Kubernetes 应用和服务的网络流量的深度可视性。这些网络流量信息可以对接 Hubble CLI、UI 工具，可以通过交互式的方式快速进行问题诊断。除了 Hubble 自身的监控工具，还可以对接主流的云原生监控体系——Prometheus 和 Grafana，实现可扩展的监控策略。\n   Cilium 示意图  本节将带你了解什么是 Cilium 及选择它的原因。\nCilium 是什么？ Cilium 为基于 Kubernetes 的 Linux 容器管理平台上部署的服务，透明地提供服务间的网络和 API 连接及安全。\nCilium 底层是基于 Linux 内核的新技术 eBPF，可以在 Linux 系统中动态注入强大的安全性、可视性和网络控制逻辑。 Cilium 基于 eBPF 提供了多集群路由、替代 kube-proxy 实现负载均衡、透明加密以及网络和服务安全等诸多功能。除了提供传统的网络安全之外，eBPF 的灵活性还支持应用协议和 DNS 请求/响应安全。同时，Cilium 与 Envoy 紧密集成，提供了基于 Go 的扩展框架。因为 eBPF 运行在 Linux 内核中，所以应用所有 Cilium 功能，无需对应用程序代码或容器配置进行任何更改。\n基于微服务的应用程序分为小型独立服务，这些服务使用 HTTP、gRPC、Kafka 等轻量级协议通过 API 相互通信。但是，现有的 Linux 网络安全机制（例如 iptables）仅在网络和传输层（即 IP 地址和端口）上运行，并且缺乏对微服务层的可视性。\nCilium 为 Linux 容器框架（如 Docker 和 Kubernetes） 带来了 API 感知网络安全过滤。使用名为 eBPF 的新 Linux 内核技术，Cilium 提供了一种基于容器 / 容器标识定义和实施网络层和应用层安全策略的简单而有效的方法。\n注：Cilium 中文意思是 “纤毛 “，它十分细小而又无处不在。\n eBPF 扩展的柏克莱封包过滤器（extented Berkeley Packet Filter，缩写 eBPF），是 类 Unix 系统上 数据链路层 的一种原始接口，提供原始链路层 封包 的收发，除此之外，如果网卡驱动支持 洪泛 模式，那么它可以让网卡处于此种模式，这样可以收到 网络 上的所有包，不管他们的目的地是不是所在 主机。参考 维基百科 和 eBPF 简史及BPF、eBPF、XDP 和 Bpfilter 的区别。\n Hubble 是什么？ Hubble 是一个完全分布式的网络和安全可观测性平台。它建立在 Cilium 和 eBPF 之上，以完全透明的方式实现对服务的通信和行为以及网络基础设施的深度可视性（visibility）。\n通过建立在 Cilium 之上，Hubble 可以利用 eBPF 实现可视性。依靠 eBPF，所有的可视性都是可编程的，并允许采用一种动态方法，最大限度地减少开销，同时按照用户的要求提供深入和详细的可视性。Hubble 的创建和专门设计是为了最好地利用 eBPF 的能力。\n特性 以下是 Cilium 的特性。\n基于身份的安全性\nCilium 可视性和安全策略基于容器编排系统的标识（例如，Kubernetes 中的 Label）。在编写安全策略、审计和故障排查时，再也不用担心网络子网或容器 IP 地址了。\n卓越的性能\neBPF 利用 Linux 底层的强大能力，通过提供 Linux 内核的沙盒可编程性来实现数据路径，从而提供卓越的性能。\nAPI 协议可视性 + 安全性\n传统防火墙仅根据 IP 地址和端口等网络标头查看和过滤数据包。Cilium 也可以这样做，但也可以理解并过滤单个 HTTP、gRPC 和 Kafka 请求，这些请求将微服务拼接在一起。\n专为扩展而设计\nCilium 是为扩展而设计的，在部署新 pod 时不需要节点间交互，并且通过高度可扩展的键值存储进行所有协调。\n为什么选择 Cilium 和 Hubble？ 现代数据中心应用程序的开发已经转向面向服务的体系结构（SOA），通常称为微服务，其中大型应用程序被分成小型独立服务，这些服务使用 HTTP 等轻量级协议通过 API 相互通信。微服务应用程序往往是高度动态的，作为持续交付的一部分部署的滚动更新期间单个容器启动或销毁，应用程序扩展 / 缩小以适应负载变化。\n这种向高度动态的微服务的转变过程，给确保微服务之间的连接方面提出了挑战和机遇。传统的 Linux 网络安全方法（例如 iptables）过滤 IP 地址和 TCP/UDP 端口，但 IP 地址经常在动态微服务环境中流失。容器的高度不稳定的生命周期导致这些方法难以与应用程序并排扩展，因为负载均衡表和访问控制列表要不断更新，可能增长成包含数十万条规则。出于安全目的，协议端口（例如，用于 HTTP 流量的 TCP 端口 80）不能再用于区分应用流量，因为该端口用于跨服务的各种消息。\n另一个挑战是提供准确的可视性，因为传统系统使用 IP 地址作为主要识别工具，其在微服务架构中的寿命可能才仅仅几秒钟，被大大缩短。\n利用 Linux eBPF，Cilium 保留了透明地插入安全可视性 + 强制执行的能力，但这种方式基于服务 /pod/ 容器标识（与传统系统中的 IP 地址识别相反），并且可以根据应用层进行过滤 （例如 HTTP）。因此，通过将安全性与寻址分离，Cilium 不仅可以在高度动态的环境中应用安全策略，而且除了提供传统的第 3 层和第 4 层分割之外，还可以通过在 HTTP 层运行来提供更强的安全隔离。\neBPF 的使用使得 Cilium 能够以高度可扩展的方式实现以上功能，即使对于大规模环境也不例外。\n功能概述 透明的保护 API 能够保护现代应用程序协议，如 REST/HTTP、gRPC 和 Kafka。传统防火墙在第 3 层和第 4 层运行，在特定端口上运行的协议要么完全受信任，要么完全被阻止。Cilium 提供了过滤各个应用程序协议请求的功能，例如：\n 允许所有带有方法 GET 和路径 /public/.* 的 HTTP 请求。拒绝所有其他请求。 允许 service1 在 Kafka topic 上生成 topic1，service2 消费 topic1。拒绝所有其他 Kafka 消息。 要求 HTTP 标头 X-Token: [0-9]+ 出现在所有 REST 调用中。  详情请参考 7 层协议。\n基于身份来保护服务间通信 现代分布式应用程序依赖于诸如容器之类的技术来促进敏捷性并按需扩展。这将导致在短时间内启动大量应用容器。典型的容器防火墙通过过滤源 IP 地址和目标端口来保护工作负载。这就要求不论在集群中的哪个位置启动容器时都要操作所有服务器上的防火墙。\n为了避免受到规模限制，Cilium 为共享相同安全策略的应用程序容器组分配安全标识。然后，该标识与应用程序容器发出的所有网络数据包相关联，从而允许验证接收节点处的身份。使用键值存储执行安全身份管理。\n安全访问外部服务 基于标签的安全性是集群内部访问控制的首选工具。为了保护对外部服务的访问，支持入口（ingress）和出口（egress）的传统基于 CIDR 的安全策略。这允许限制对应用程序容器的访问以及对特定 IP 范围的访问。\n简单网络 一个简单的扁平第 3 层网络能够跨越多个集群连接所有应用程序容器。使用主机范围分配器可以简化 IP 分配。这意味着每个主机可以在主机之间没有任何协调的情况下分配 IP。\n支持以下多节点网络模型：\n  Overlay：基于封装的虚拟网络产生所有主机。目前 VXLAN 和 Geneve 已经完成，但可以启用 Linux 支持的所有封装格式。\n何时使用此模式：此模式具有最小的基础架构和集成要求。它几乎适用于任何网络基础架构，唯一的要求是主机之间可以通过 IP 连接。\n  本机路由：使用 Linux 主机的常规路由表。网络必须能够路由应用程序容器的 IP 地址。\n何时使用此模式：此模式适用于高级用户，需要了解底层网络基础结构。此模式适用于：\n 本地 IPv6 网络 与云网络路由器配合使用 如果您已经在运行路由守护进程    负载均衡 应用程序容器和外部服务之间的流量的分布式负载均衡。负载均衡使用 eBPF 实现，允许几乎无限的规模，并且如果未在源主机上执行负载均衡操作，则支持直接服务器返回（DSR）。\n注意：负载均衡需要启用连接跟踪。这是默认值。\n监控和故障排除 可视性和故障排查是任何分布式系统运行的基础。虽然我们喜欢用 tcpdump 和 ping，它们很好用，但我们努力为故障排除提供更好的工具。包括以下工具：\n 使用元数据进行事件监控：当数据包被丢弃时，该工具不仅仅报告数据包的源 IP 和目标 IP，该工具还提供发送方和接收方的完整标签信息等。 策略决策跟踪：为什么丢弃数据包或拒绝请求。策略跟踪框架允许跟踪运行工作负载和基于任意标签定义的策略决策过程。 通过 Prometheus 导出指标：通过 Prometheus 导出关键指标，以便与现有仪表板集成。  集成  网络插件集成：CNI、libnetwork 容器运行时：containerd Kubernetes：NetworkPolicy、Label、Ingress、Service 日志记录：syslog、fluentd  概念 Cilium 要求 Linux kernel 版本在 4.8.0 以上，Cilium 官方建议 kernel 版本至少在 4.9.17 以上，高版本的 Ubuntu 发行版中 Linux 内核版本一般在 4.12 以上，如使用 CentOS7 需要升级内核才能运行 Cilium。\nKV 存储数据库用存储以下状态：\n 策略身份，Label 列表 \u0026lt;=\u0026gt; 服务身份标识 全局的服务 ID，与 VIP 相关联（可选） 封装的 VTEP（Vxlan Tunnel End Point）映射（可选）  为了简单起见，Cilium 一般跟容器编排调度器使用同一个 KV 存储数据库，例如在 Kubernetes 中使用 etcd 存储。\n组成 下图是 Cilium 的组件示意图，Cilium 是位于 Linux kernel 与容器编排系统的中间层。向上可以为容器配置网络，向下可以向 Linux 内核生成 BPF 程序来控制容器的安全性和转发行为。\n   Cilium 组件(来自 Cilium 官网)  管理员通过 Cilium CLI 配置策略信息，这些策略信息将存储在 KV 数据库里，Cilium 使用插件（如 CNI）与容器编排调度系统交互，来实现容器间的联网和容器分配 IP 地址分配，同时 Cilium 还可以获得容器的各种元数据和流量信息，提供监控 API。\nCilium Agent\nCilium Agent 作为守护进程运行在每个节点上，与容器运行时如 Docker，和容器编排系统交互如 Kubernetes。通常是使用插件的形式（如 Docker plugin）或遵从容器编排标准定义的网络接口（如 CNI）。\nCilium Agent 的功能有：\n 暴露 API 给运维和安全团队，可以配置容器间的通信策略。还可以通过这些 API 获取网络监控数据。 收集容器的元数据，例如 Pod 的 Label，可用于 Cilium 安全策略里的 Endpoint 识别，这个跟 Kubernetes 中的 service 里的 Endpoint 类似。 与容器管理平台的网络插件交互，实现 IPAM 的功能，用于给容器分配 IP 地 …","date":1653062400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"d70b2067d2e6e05d39f179b4ee8764a3","permalink":"https://lib.jimmysong.io/kubernetes-handbook/networking/cilium/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/networking/cilium/","section":"kubernetes-handbook","summary":"Cilium 是一款开源软件，也是 CNCF 的孵化项目，目前已有公司提供商业化支持，还有基于 Cilium 实现的服务网格解决方案。最初它仅是作为一个 Kubernetes 网络组件。Cilium 在 1.7 版本后推出并开源了 Hubble，它是专门为网络可视化设计","tags":["Kubernetes"],"title":"基于 eBPF 的网络 Cilium","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"Secret 解决了密码、token、密钥等敏感数据的配置问题，而不需要把这些敏感数据暴露到镜像或者 Pod Spec 中。Secret 可以以 Volume 或者环境变量的方式使用。\nSecret 有三种类型：\n Service Account ：用来访问 Kubernetes API，由 Kubernetes 自动创建，并且会自动挂载到 Pod 的 /run/secrets/kubernetes.io/serviceaccount 目录中； Opaque ：base64 编码格式的 Secret，用来存储密码、密钥等； kubernetes.io/dockerconfigjson ：用来存储私有 docker registry 的认证信息。  Opaque Secret Opaque 类型的数据是一个 map 类型，要求 value 是 base64 编码格式：\n$ echo -n \u0026#34;admin\u0026#34; | base64 YWRtaW4= $ echo -n \u0026#34;1f2d1e2e67df\u0026#34; | base64 MWYyZDFlMmU2N2Rm secrets.yml\napiVersion:v1kind:Secretmetadata:name:mysecrettype:Opaquedata:password:MWYyZDFlMmU2N2Rmusername:YWRtaW4=接着，就可以创建 secret 了：kubectl create -f secrets.yml。\n创建好 secret 之后，有两种方式来使用它：\n 以 Volume 方式 以环境变量方式  将 Secret 挂载到 Volume 中 apiVersion:v1kind:Podmetadata:labels:name:dbname:dbspec:volumes:- name:secretssecret:secretName:mysecretcontainers:- image:gcr.io/my_project_id/pg:v1name:dbvolumeMounts:- name:secretsmountPath:\u0026#34;/etc/secrets\u0026#34;readOnly:trueports:- name:cpcontainerPort:5432hostPort:5432将 Secret 导出到环境变量中 apiVersion:extensions/v1beta1kind:Deploymentmetadata:name:wordpress-deploymentspec:replicas:2strategy:type:RollingUpdatetemplate:metadata:labels:app:wordpressvisualize:\u0026#34;true\u0026#34;spec:containers:- name:\u0026#34;wordpress\u0026#34;image:\u0026#34;wordpress\u0026#34;ports:- containerPort:80env:- name:WORDPRESS_DB_USERvalueFrom:secretKeyRef:name:mysecretkey:username- name:WORDPRESS_DB_PASSWORDvalueFrom:secretKeyRef:name:mysecretkey:passwordkubernetes.io/dockerconfigjson 可以直接用 kubectl 命令来创建用于 docker registry 认证的 secret：\n$ kubectl create secret docker-registry myregistrykey --docker-server=DOCKER_REGISTRY_SERVER --docker-username=DOCKER_USER --docker-password=DOCKER_PASSWORD --docker-email=DOCKER_EMAIL secret \u0026#34;myregistrykey\u0026#34; created. 也可以直接读取 ~/.docker/config.json 的内容来创建：\n$ cat ~/.docker/config.json | base64 $ cat \u0026gt; myregistrykey.yaml \u0026lt;\u0026lt;EOF apiVersion: v1 kind: Secret metadata: name: myregistrykey data: .dockerconfigjson: UmVhbGx5IHJlYWxseSByZWVlZWVlZWVlZWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWFhYWxsbGxsbGxsbGxsbGxsbGxsbGxsbGxsbGxsbGxsbGx5eXl5eXl5eXl5eXl5eXl5eXl5eSBsbGxsbGxsbGxsbGxsbG9vb29vb29vb29vb29vb29vb29vb29vb29vb25ubm5ubm5ubm5ubm5ubm5ubm5ubm5ubmdnZ2dnZ2dnZ2dnZ2dnZ2dnZ2cgYXV0aCBrZXlzCg== type: kubernetes.io/dockerconfigjson EOF $ kubectl create -f myregistrykey.yaml 在创建 Pod 的时候，通过 imagePullSecrets 来引用刚创建的 myregistrykey:\napiVersion:v1kind:Podmetadata:name:foospec:containers:- name:fooimage:janedoe/awesomeapp:v1imagePullSecrets:- name:myregistrykeyService Account Service Account 用来访问 Kubernetes API，由 Kubernetes 自动创建，并且会自动挂载到 Pod 的 /run/secrets/kubernetes.io/serviceaccount 目录中。\n$ kubectl run nginx --image nginx deployment \u0026#34;nginx\u0026#34; created $ kubectl get pods NAME READY STATUS RESTARTS AGE nginx-3137573019-md1u2 1/1 Running 0 13s $ kubectl exec nginx-3137573019-md1u2 ls /run/secrets/kubernetes.io/serviceaccount ca.crt namespace token ","date":1653062400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"c789b891f8b127506d7167e10196b158","permalink":"https://lib.jimmysong.io/kubernetes-handbook/storage/secret/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/storage/secret/","section":"kubernetes-handbook","summary":"Secret 解决了密码、token、密钥等敏感数据的配置问题，而不需要把这些敏感数据暴露到镜像或者 Pod Spec 中。Secret 可以以 Volume 或者环境变量的方式使用。 Secret 有三种类型： Service Account ：用来访问 Kubernetes API，由 Kubernetes 自动创建，并且会自","tags":["Kubernetes"],"title":"Secret","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"其实 ConfigMap 功能在 Kubernetes1.2 版本的时候就有了，许多应用程序会从配置文件、命令行参数或环境变量中读取配置信息。这些配置信息需要与 docker image 解耦，你总不能每修改一个配置就重做一个 image 吧？ConfigMap API 给我们提供了向容器中注入配置信息的机制，ConfigMap 可以被用来保存单个属性，也可以用来保存整个配置文件或者 JSON 二进制大对象。\nConfigMap 概览 ConfigMap API 资源用来保存 key-value pair 配置数据，这个数据可以在 pods 里使用，或者被用来为像 controller 一样的系统组件存储配置数据。虽然 ConfigMap 跟 Secrets 类似，但是 ConfigMap 更方便的处理不含敏感信息的字符串。 注意：ConfigMaps 不是属性配置文件的替代品。ConfigMaps 只是作为多个 properties 文件的引用。你可以把它理解为 Linux 系统中的 /etc 目录，专门用来存储配置文件的目录。下面举个例子，使用 ConfigMap 配置来创建 Kubernetes Volumes，ConfigMap 中的每个 data 项都会成为一个新文件。\nkind:ConfigMapapiVersion:v1metadata:creationTimestamp:2016-02-18T19:14:38Zname:example-confignamespace:defaultdata:example.property.1:helloexample.property.2:worldexample.property.file:|-property.1=value-1 property.2=value-2 property.3=value-3data 一栏包括了配置数据，ConfigMap 可以被用来保存单个属性，也可以用来保存一个配置文件。 配置数据可以通过很多种方式在 Pods 里被使用。ConfigMaps 可以被用来：\n 设置环境变量的值 在容器里设置命令行参数 在数据卷里面创建 config 文件  用户和系统组件两者都可以在 ConfigMap 里面存储配置数据。\n其实不用看下面的文章，直接从 kubectl create configmap -h 的帮助信息中就可以对 ConfigMap 究竟如何创建略知一二了。\nExamples: # Create a new configmap named my-config based on folder bar kubectl create configmap my-config --from-file=path/to/bar # Create a new configmap named my-config with specified keys instead of file basenames on disk kubectl create configmap my-config --from-file=key1=/path/to/bar/file1.txt --from-file=key2=/path/to/bar/file2.txt # Create a new configmap named my-config with key1=config1 and key2=config2 kubectl create configmap my-config --from-literal=key1=config1 --from-literal=key2=config2 创建 ConfigMaps 可以使用该命令，用给定值、文件或目录来创建 ConfigMap。\nkubectl create configmap 使用目录创建 比如我们已经有了一些配置文件，其中包含了我们想要设置的 ConfigMap 的值：\n$ ls docs/user-guide/configmap/kubectl/ game.properties ui.properties $ cat docs/user-guide/configmap/kubectl/game.properties enemies=aliens lives=3 enemies.cheat=true enemies.cheat.level=noGoodRotten secret.code.passphrase=UUDDLRLRBABAS secret.code.allowed=true secret.code.lives=30 $ cat docs/user-guide/configmap/kubectl/ui.properties color.good=purple color.bad=yellow allow.textmode=true how.nice.to.look=fairlyNice 使用下面的命令可以创建一个包含目录中所有文件的 ConfigMap。\n$ kubectl create configmap game-config --from-file=docs/user-guide/configmap/kubectl —from-file 指定在目录下的所有文件都会被用在 ConfigMap 里面创建一个键值对，键的名字就是文件名，值就是文件的内容。\n让我们来看一下这个命令创建的 ConfigMap：\n$ kubectl describe configmaps game-configName:game-configNamespace:defaultLabels:\u0026lt;none\u0026gt;Annotations:\u0026lt;none\u0026gt;Data====game.properties:158bytesui.properties:83bytes我们可以看到那两个 key 是从 kubectl 指定的目录中的文件名。这些 key 的内容可能会很大，所以在 kubectl describe 的输出中，只能够看到键的名字和他们的大小。 如果想要看到键的值的话，可以使用 kubectl get：\n$ kubectl get configmaps game-config -o yaml 我们以 yaml 格式输出配置。\napiVersion:v1data:game.properties:|enemies=aliens lives=3 enemies.cheat=true enemies.cheat.level=noGoodRotten secret.code.passphrase=UUDDLRLRBABAS secret.code.allowed=true secret.code.lives=30ui.properties:|color.good=purple color.bad=yellow allow.textmode=true how.nice.to.look=fairlyNicekind:ConfigMapmetadata:creationTimestamp:2016-02-18T18:34:05Zname:game-confignamespace:defaultresourceVersion:\u0026#34;407\u0026#34;selfLink:/api/v1/namespaces/default/configmaps/game-configuid:30944725-d66e-11e5-8cd0-68f728db1985使用文件创建 刚才使用目录创建的时候我们 —from-file 指定的是一个目录，只要指定为一个文件就可以从单个文件中创建 ConfigMap。\n$ kubectl create configmap game-config-2 --from-file=docs/user-guide/configmap/kubectl/game.properties $ kubectl get configmaps game-config-2 -o yaml apiVersion: v1 data: game-special-key: | enemies=aliens lives=3 enemies.cheat=true enemies.cheat.level=noGoodRotten secret.code.passphrase=UUDDLRLRBABAS secret.code.allowed=true secret.code.lives=30 kind: ConfigMap metadata: creationTimestamp: 2016-02-18T18:54:22Z name: game-config-3 namespace: default resourceVersion: \u0026#34;530\u0026#34; selfLink: /api/v1/namespaces/default/configmaps/game-config-3 uid: 05f8da22-d671-11e5-8cd0-68f728db1985 —from-file 这个参数可以使用多次，你可以使用两次分别指定上个实例中的那两个配置文件，效果就跟指定整个目录是一样的。\n使用字面值创建 使用文字值创建，利用 —from-literal 参数传递配置信息，该参数可以使用多次，格式如下；\n$ kubectl create configmap special-config --from-literal=special.how=very --from-literal=special.type=charm $ kubectl get configmaps special-config -o yaml apiVersion: v1 data: special.how: very special.type: charm kind: ConfigMap metadata: creationTimestamp: 2016-02-18T19:14:38Z name: special-config namespace: default resourceVersion: \u0026#34;651\u0026#34; selfLink: /api/v1/namespaces/default/configmaps/special-config uid: dadce046-d673-11e5-8cd0-68f728db1985 Pod 中使用 ConfigMap 使用 ConfigMap 来替代环境变量\nConfigMap 可以被用来填入环境变量。看下下面的 ConfigMap。\napiVersion:v1kind:ConfigMapmetadata:name:special-confignamespace:defaultdata:special.how:veryspecial.type:charmapiVersion:v1kind:ConfigMapmetadata:name:env-confignamespace:defaultdata:log_level:INFO我们可以在 Pod 中这样使用 ConfigMap：\napiVersion:v1kind:Podmetadata:name:dapi-test-podspec:containers:- name:test-containerimage:gcr.io/google_containers/busyboxcommand:[\u0026#34;/bin/sh\u0026#34;,\u0026#34;-c\u0026#34;,\u0026#34;env\u0026#34;]env:- name:SPECIAL_LEVEL_KEYvalueFrom:configMapKeyRef:name:special-configkey:special.how- …","date":1653062400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"7ba2a85c01522d16d19e7ed2806ce685","permalink":"https://lib.jimmysong.io/kubernetes-handbook/storage/configmap/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/storage/configmap/","section":"kubernetes-handbook","summary":"其实 ConfigMap 功能在 Kubernetes1.2 版本的时候就有了，许多应用程序会从配置文件、命令行参数或环境变量中读取配置信息。这些配置信息需要与 docker image 解耦，你总不能每修改一个配置就重做一个 image 吧？ConfigMap API 给我们提供了向容器中","tags":["Kubernetes"],"title":"ConfigMap","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"ConfigMap 是用来存储配置文件的 kubernetes 资源对象，所有的配置内容都存储在 etcd 中，下文主要是探究 ConfigMap 的创建和更新流程，以及对 ConfigMap 更新后容器内挂载的内容是否同步更新的测试。\n测试示例 假设我们在 default namespace 下有一个名为 nginx-config 的 ConfigMap，可以使用 kubectl 命令来获取：\n$ kubectl get configmap nginx-config NAME DATA AGE nginx-config 1 99d 获取该 ConfigMap 的内容。\nkubectl get configmap nginx-config -o yaml apiVersion: v1 data: nginx.conf: |- worker_processes 1; events { worker_connections 1024; } http { sendfile on; server { listen 80; # a test endpoint that returns http 200s location / { proxy_pass http://httpstat.us/200; proxy_set_header X-Real-IP $remote_addr; } } server { listen 80; server_name api.hello.world; location / { proxy_pass http://l5d.default.svc.cluster.local; proxy_set_header Host $host; proxy_set_header Connection \u0026#34;\u0026#34;; proxy_http_version 1.1; more_clear_input_headers \u0026#39;l5d-ctx-*\u0026#39; \u0026#39;l5d-dtab\u0026#39; \u0026#39;l5d-sample\u0026#39;; } } server { listen 80; server_name www.hello.world; location / { # allow \u0026#39;employees\u0026#39; to perform dtab overrides if ($cookie_special_employee_cookie != \u0026#34;letmein\u0026#34;) { more_clear_input_headers \u0026#39;l5d-ctx-*\u0026#39; \u0026#39;l5d-dtab\u0026#39; \u0026#39;l5d-sample\u0026#39;; } # add a dtab override to get people to our beta, world-v2 set $xheader \u0026#34;\u0026#34;; if ($cookie_special_employee_cookie ~* \u0026#34;dogfood\u0026#34;) { set $xheader \u0026#34;/host/world =\u0026gt; /srv/world-v2;\u0026#34;; } proxy_set_header \u0026#39;l5d-dtab\u0026#39; $xheader; proxy_pass http://l5d.default.svc.cluster.local; proxy_set_header Host $host; proxy_set_header Connection \u0026#34;\u0026#34;; proxy_http_version 1.1; } } } kind: ConfigMap metadata: creationTimestamp: 2017-08-01T06:53:17Z name: nginx-config namespace: default resourceVersion: \u0026#34;14925806\u0026#34; selfLink: /api/v1/namespaces/default/configmaps/nginx-config uid: 18d70527-7686-11e7-bfbd-8af1e3a7c5bd ConfigMap 中的内容是存储到 etcd 中的，然后查询 etcd：\nETCDCTL_API=3 etcdctl get /registry/configmaps/default/nginx-config -w json|python -m json.tool 注意使用 v3 版本的 etcdctl API，下面是输出结果：\n{ \u0026#34;count\u0026#34;: 1, \u0026#34;header\u0026#34;: { \u0026#34;cluster_id\u0026#34;: 12091028579527406772, \u0026#34;member_id\u0026#34;: 16557816780141026208, \u0026#34;raft_term\u0026#34;: 36, \u0026#34;revision\u0026#34;: 29258723 }, \u0026#34;kvs\u0026#34;: [ { \u0026#34;create_revision\u0026#34;: 14925806, \u0026#34;key\u0026#34;: \u0026#34;L3JlZ2lzdHJ5L2NvbmZpZ21hcHMvZGVmYXVsdC9uZ2lueC1jb25maWc=\u0026#34;, \u0026#34;mod_revision\u0026#34;: 14925806, \u0026#34;value\u0026#34;: \u0026#34;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\u0026#34;, \u0026#34;version\u0026#34;: 1 } ] } 其中的 value 就是 nginx.conf 配置文件的内容。\n代码 ConfigMap 结构体的定义：\n// ConfigMap holds configuration data for pods to consume. type ConfigMap struct { metav1.TypeMeta `json:\u0026#34;,inline\u0026#34;` // Standard object\u0026#39;s metadata. \t// More info: http://releases.k8s.io/HEAD/docs/devel/api-conventions.md#metadata \t// +optional \tmetav1.ObjectMeta `json:\u0026#34;metadata,omitempty\u0026#34; protobuf:\u0026#34;bytes,1,opt,name=metadata\u0026#34;` // Data contains the configuration data. \t// Each key must be a valid DNS_SUBDOMAIN with an optional leading dot. \t// +optional \tData map[string]string `json:\u0026#34;data,omitempty\u0026#34; protobuf:\u0026#34;bytes,2,rep,name=data\u0026#34;` } 在 staging/src/k8s.io/client-go/kubernetes/typed/core/v1/configmap.go 中 ConfigMap 的接口定义：\n// ConfigMapInterface has methods to work …","date":1653062400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"b021a4b38b5f83e5e9cc2df06b2cdc06","permalink":"https://lib.jimmysong.io/kubernetes-handbook/storage/configmap-hot-update/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/storage/configmap-hot-update/","section":"kubernetes-handbook","summary":"ConfigMap 是用来存储配置文件的 kubernetes 资源对象，所有的配置内容都存储在 etcd 中，下文主要是探究 ConfigMap 的创建和更新流程，以及对 ConfigMap 更新后容器内挂载的内容是否同步更新的测试。 测试示例 假设我们在 default namespace 下有一个名为 nginx-config 的 ConfigMa","tags":["Kubernetes"],"title":"ConfigMap 的热更新","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"Volume 容器磁盘上的文件的生命周期是短暂的，这就使得在容器中运行重要应用时会出现一些问题。首先，当容器崩溃时，kubelet 会重启它，但是容器中的文件将丢失——容器以干净的状态（镜像最初的状态）重新启动。其次，在 Pod 中同时运行多个容器时，这些容器之间通常需要共享文件。Kubernetes 中的 Volume 抽象就很好的解决了这些问题。\n建议先熟悉 pod。\n背景 Docker 中也有一个 volume 的概念，尽管它稍微宽松一些，管理也很少。在 Docker 中，卷就像是磁盘或是另一个容器中的一个目录。它的生命周期不受管理，直到最近才有了 local-disk-backed 卷。Docker 现在提供了卷驱动程序，但是功能还非常有限（例如Docker1.7只允许每个容器使用一个卷驱动，并且无法给卷传递参数）。\n另一方面，Kubernetes 中的卷有明确的寿命——与封装它的 Pod 相同。所以，卷的生命比 Pod 中的所有容器都长，当这个容器重启时数据仍然得以保存。当然，当 Pod 不再存在时，卷也将不复存在。也许更重要的是，Kubernetes 支持多种类型的卷，Pod 可以同时使用任意数量的卷。\n卷的核心是目录，可能还包含了一些数据，可以通过 pod 中的容器来访问。该目录是如何形成的、支持该目录的介质以及其内容取决于所使用的特定卷类型。\n要使用卷，需要为 pod 指定为卷（spec.volumes 字段）以及将它挂载到容器的位置（spec.containers.volumeMounts 字段）。\n容器中的进程看到的是由其 Docker 镜像和卷组成的文件系统视图。Docker 镜像位于文件系统层次结构的根目录，任何卷都被挂载在镜像的指定路径中。卷无法挂载到其他卷上或与其他卷有硬连接。Pod 中的每个容器都必须独立指定每个卷的挂载位置。\n卷的类型 Kubernetes 支持以下类型的卷：\n awsElasticBlockStore azureDisk azureFile cephfs csi downwardAPI emptyDir fc (fibre channel) flocker gcePersistentDisk gitRepo glusterfs hostPath iscsi local nfs persistentVolumeClaim projected portworxVolume quobyte rbd scaleIO secret storageos vsphereVolume  我们欢迎额外贡献。\nawsElasticBlockStore awsElasticBlockStore 卷将Amazon Web Services（AWS）EBS Volume 挂载到您的容器中。与 emptyDir 类型会在删除 Pod 时被清除不同，EBS 卷的的内容会保留下来，仅仅是被卸载。这意味着 EBS 卷可以预先填充数据，并且可以在数据包之间“切换”数据。\n重要提示：您必须使用 aws ec2 create-volume 或 AWS API 创建 EBS 卷，才能使用它。\n使用 awsElasticBlockStore 卷时有一些限制：\n 运行 Pod 的节点必须是 AWS EC2 实例 这些实例需要与 EBS 卷位于相同的区域和可用区域 EBS 仅支持卷和 EC2 实例的一对一的挂载  创建 EBS 卷 在 pod 中使用的 EBS 卷之前，您需要先创建它。\naws ec2 create-volume --availability-zone=eu-west-1a --size=10 --volume-type=gp2 确保区域与您启动集群的区域相匹配（并且检查大小和 EBS 卷类型是否适合您的使用！）\nAWS EBS 示例配置 apiVersion:v1kind:Podmetadata:name:test-ebsspec:containers:- image:k8s.gcr.io/test-webservername:test-containervolumeMounts:- mountPath:/test-ebsname:test-volumevolumes:- name:test-volume# This AWS EBS volume must already exist.awsElasticBlockStore:volumeID:\u0026lt;volume-id\u0026gt;fsType:ext4azureDisk AzureDisk 用于将 Microsoft Azure Data Disk 挂载到 Pod 中。\nazureFile azureFile 用于将 Microsoft Azure File Volume（SMB 2.1 和 3.0）挂载到 Pod 中。\ncephfs cephfs 卷允许将现有的 CephFS 卷挂载到您的容器中。不像 emptyDir，当删除 Pod 时被删除，cephfs 卷的内容将被保留，卷仅仅是被卸载。这意味着 CephFS 卷可以预先填充数据，并且可以在数据包之间“切换”数据。 CephFS 可以被多个写设备同时挂载。\n重要提示：您必须先拥有自己的 Ceph 服务器，然后才能使用它。\ncsi CSI 代表容器存储接口，CSI 试图建立一个行业标准接口的规范，借助 CSI 容器编排系统（CO）可以将任意存储系统暴露给自己的容器工作负载。有关详细信息，请查看设计方案。\ncsi 卷类型是一种 in-tree 的 CSI 卷插件，用于 Pod 与在同一节点上运行的外部 CSI 卷驱动程序交互。部署 CSI 兼容卷驱动后，用户可以使用 csi 作为卷类型来挂载驱动提供的存储。\nCSI 持久化卷支持是在 Kubernetes v1.9 中引入的，作为一个 alpha 特性，必须由集群管理员明确启用。换句话说，集群管理员需要在 apiserver、controller-manager 和 kubelet 组件的 “--feature-gates =” 标志中加上 “CSIPersistentVolume = true”。\nCSI 持久化卷具有以下字段可供用户指定：\n driver：一个字符串值，指定要使用的卷驱动程序的名称。必须少于 63 个字符，并以一个字符开头。驱动程序名称可以包含 “.”、“- ”、“_” 或数字。 volumeHandle：一个字符串值，唯一标识从 CSI 卷插件的 CreateVolume 调用返回的卷名。随后在卷驱动程序的所有后续调用中使用卷句柄来引用该卷。 readOnly：一个可选的布尔值，指示卷是否被发布为只读。默认是 false。  downwardAPI downwardAPI 卷用于使向下 API 数据（downward API data）对应用程序可用。它挂载一个目录，并将请求的数据写入纯文本文件。\n参考 downwardAPI 卷示例查看详细信息。\nemptyDir 当 Pod 被分配给节点时，首先创建 emptyDir 卷，并且只要该 Pod 在该节点上运行，该卷就会存在。正如卷的名字所述，它最初是空的。Pod 中的容器可以读取和写入 emptyDir 卷中的相同文件，尽管该卷可以挂载到每个容器中的相同或不同路径上。当出于任何原因从节点中删除 Pod 时，emptyDir 中的数据将被永久删除。\n注意：容器崩溃不会从节点中移除 pod，因此 emptyDir 卷中的数据在容器崩溃时是安全的。\nemptyDir 的用法有：\n 暂存空间，例如用于基于磁盘的合并排序 用作长时间计算崩溃恢复时的检查点 Web服务器容器提供数据时，保存内容管理器容器提取的文件  Pod 示例 apiVersion:v1kind:Podmetadata:name:test-pdspec:containers:- image:k8s.gcr.io/test-webservername:test-containervolumeMounts:- mountPath:/cachename:cache-volumevolumes:- name:cache-volumeemptyDir:{}fc (fibre channel) fc 卷允许将现有的 fc 卷挂载到 pod 中。您可以使用卷配置中的 targetWWN 参数指定单个或多个目标全球通用名称（World Wide Name）。如果指定了多个 WWN，则 targetWWN 期望这些 WWN 来自多路径连接。\n重要提示：您必须配置 FC SAN 区域划分，并预先将这些 LUN（卷）分配并屏蔽到目标 WWN，以便 Kubernetes 主机可以访问它们。\n参考 FC 示例获取详细信息。\nflocker Flocker 是一款开源的集群容器数据卷管理器。它提供了由各种存储后端支持的数据卷的管理和编排。\nflocker 允许将 Flocker 数据集挂载到 pod 中。如果数据集在 Flocker 中不存在，则需要先使用 Flocker CLI 或使用 Flocker API 创建数据集。如果数据集已经存在，它将被 Flocker 重新连接到 pod 被调度的节点上。这意味着数据可以根据需要在数据包之间“切换”。\n重要提示：您必须先运行自己的 Flocker 安装程序才能使用它。\n参考 Flocker 示例获取更多详细信息。\ngcePersistentDisk gcePersistentDisk 卷将 Google Compute Engine（GCE）Persistent Disk 挂载到您的容器中。与删除 Pod 时删除的 emptyDir 不同，PD 的内容被保留，只是卸载了卷。这意味着 PD 可以预先填充数据，并且数据可以在 Pod 之间“切换”。\n重要提示：您必须先使用 gcloud 或 GCE API 或 UI 创建一个 PD，然后才能使用它。\n使用 gcePersistentDisk 时有一些限制：\n 运行 Pod 的节点必须是 GCE 虚拟机 那些虚拟机需要在与 PD 一样在 GCE 项目和区域中  PD 的一个特点是它们可以同时被多个用户以只读方式挂载。这意味着您可以预先使用您的数据集填充 PD，然后根据需要给多个 Pod 中并行提供。不幸的是，只能由单个消费者以读写模式挂载 PD，而不允许同时写入。 在由 ReplicationController 控制的 pod 上使用 PD 将会失败，除非 PD 是只读的或者副本数是 0 或 1。\n创建 PD 在您在 pod 中使用 GCE PD 之前，需要先创建它。\ngcloud compute disks create --size=500GB --zone=us-central1-a my-data-disk Pod 示例 apiVersion:v1kind:Podmetadata:name:test-pdspec:containers:- image:k8s.gcr.io/test-webservername:test-containervolumeMounts:- mountPath:/test-pdname:test-volumevolumes:- name:test-volume# This GCE PD must already exist.gcePersistentDisk:pdName:my-data-diskfsType:ext4gitRepo gitRepo 卷是一个可以演示卷插件功能的示例。它会挂载一个空目录并将 git 存储库克隆到您的容器中。将来，这样的卷可能会转移到一个更加分离的模型，而不是为每个这样的用例扩展 Kubernetes API。\n下面是 gitRepo 卷示例：\napiVersion:v1kind:Podmetadata:name:serverspec:containers:- …","date":1653062400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"23c62406fa1b8e323a493ac1d48ed610","permalink":"https://lib.jimmysong.io/kubernetes-handbook/storage/volume/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/storage/volume/","section":"kubernetes-handbook","summary":"Volume 容器磁盘上的文件的生命周期是短暂的，这就使得在容器中运行重要应用时会出现一些问题。首先，当容器崩溃时，kubelet 会重启它，但是容器中的文件将丢失——容器以干净的状态（镜像最初的状态）重新启动。其","tags":["Kubernetes"],"title":"Volume","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"本文档介绍了 Kubernetes 中 PersistentVolume 的当前状态。建议您在阅读本文档前先熟悉 volume。\n介绍 对于管理计算资源来说，管理存储资源明显是另一个问题。PersistentVolume 子系统为用户和管理员提供了一个 API，该 API 将如何提供存储的细节抽象了出来。为此，我们引入两个新的 API 资源：PersistentVolume 和 PersistentVolumeClaim。\nPersistentVolume（PV）是由管理员设置的存储，它是群集的一部分。就像节点是集群中的资源一样，PV 也是集群中的资源。 PV 是 Volume 之类的卷插件，但具有独立于使用 PV 的 Pod 的生命周期。此 API 对象包含存储实现的细节，即 NFS、iSCSI 或特定于云供应商的存储系统。\nPersistentVolumeClaim（PVC）是用户存储的请求。它与 Pod 相似。Pod 消耗节点资源，PVC 消耗 PV 资源。Pod 可以请求特定级别的资源（CPU 和内存）。声明可以请求特定的大小和访问模式（例如，可以以读/写一次或 只读多次模式挂载）。\n虽然 PersistentVolumeClaims 允许用户使用抽象存储资源，但用户需要具有不同性质（例如性能）的 PersistentVolume 来解决不同的问题。集群管理员需要能够提供各种各样的 PersistentVolume，这些PersistentVolume 的大小和访问模式可以各有不同，但不需要向用户公开实现这些卷的细节。对于这些需求，StorageClass 资源可以实现。\n请参阅工作示例的详细过程。\n卷和声明的生命周期 PV 属于集群中的资源。PVC 是对这些资源的请求，也作为对资源的请求的检查。 PV 和 PVC 之间的相互作用遵循这样的生命周期：\n配置（Provision） 有两种方式来配置 PV：静态或动态。\n静态 集群管理员创建一些 PV。它们带有可供群集用户使用的实际存储的细节。它们存在于 Kubernetes API 中，可用于消费。\n动态 根据 StorageClasses，当管理员创建的静态 PV 都不匹配用户的 PersistentVolumeClaim 时，集群可能会尝试动态地为 PVC 创建卷。\n绑定 在动态配置的情况下，用户创建或已经创建了具有特定存储量的 PersistentVolumeClaim 以及某些访问模式。master 中的控制环路监视新的 PVC，寻找匹配的 PV（如果可能），并将它们绑定在一起。如果为新的 PVC 动态调配 PV，则该环路将始终将该 PV 绑定到 PVC。否则，用户总会得到他们所请求的存储，但是容量可能超出要求的数量。一旦 PV 和 PVC 绑定后，PersistentVolumeClaim 绑定是排他性的，不管它们是如何绑定的。 PVC 跟 PV 绑定是一对一的映射。\n如果没有匹配的卷，声明将无限期地保持未绑定状态。随着匹配卷的可用，声明将被绑定。例如，配置了许多 50Gi PV的集群将不会匹配请求 100Gi 的PVC。将100Gi PV 添加到群集时，可以绑定 PVC。\n使用 Pod 使用声明作为卷。集群检查声明以查找绑定的卷并为集群挂载该卷。对于支持多种访问模式的卷，用户指定在使用声明作为容器中的卷时所需的模式。\n用户进行了声明，并且该声明是绑定的，则只要用户需要，绑定的 PV 就属于该用户。用户通过在 Pod 的 volume 配置中包含 persistentVolumeClaim 来调度 Pod 并访问用户声明的 PV。\n持久化卷声明的保护 PVC 保护的目的是确保由 pod 正在使用的 PVC 不会从系统中移除，因为如果被移除的话可能会导致数据丢失。\n注意：当 pod 状态为 Pending 并且 pod 已经分配给节点或 pod 为 Running 状态时，PVC 处于活动状态。\n当启用PVC 保护 alpha 功能时，如果用户删除了一个 pod 正在使用的 PVC，则该 PVC 不会被立即删除。PVC 的删除将被推迟，直到 PVC 不再被任何 pod 使用。\n您可以看到，当 PVC 的状态为 Teminatiing 时，PVC 受到保护，Finalizers 列表中包含 kubernetes.io/pvc-protection：\nkubectl described pvc hostpath Name: hostpath Namespace: default StorageClass: example-hostpath Status: Terminating Volume: Labels: \u0026lt;none\u0026gt; Annotations: volume.beta.kubernetes.io/storage-class=example-hostpath volume.beta.kubernetes.io/storage-provisioner=example.com/hostpath Finalizers: [kubernetes.io/pvc-protection] ... 回收 用户用完 volume 后，可以从允许回收资源的 API 中删除 PVC 对象。PersistentVolume 的回收策略告诉集群在存储卷声明释放后应如何处理该卷。目前，volume 的处理策略有保留、回收或删除。\n保留 保留回收策略允许手动回收资源。当 PersistentVolumeClaim 被删除时，PersistentVolume 仍然存在，volume 被视为“已释放”。但是由于前一个声明人的数据仍然存在，所以还不能马上进行其他声明。管理员可以通过以下步骤手动回收卷。\n 删除 PersistentVolume。在删除 PV 后，外部基础架构中的关联存储资产（如 AWS EBS、GCE PD、Azure Disk 或 Cinder 卷）仍然存在。 手动清理相关存储资产上的数据。 手动删除关联的存储资产，或者如果要重新使用相同的存储资产，请使用存储资产定义创建新的 PersistentVolume。  回收 如果存储卷插件支持，回收策略会在 volume上执行基本擦除（rm -rf / thevolume / *），可被再次声明使用。\n但是，管理员可以使用如此处所述的 Kubernetes controller manager 命令行参数来配置自定义回收站 pod 模板。自定义回收站 pod 模板必须包含 volumes 规范，如下面的示例所示：\napiVersion:v1kind:Podmetadata:name:pv-recyclernamespace:defaultspec:restartPolicy:Nevervolumes:- name:volhostPath:path:/any/path/it/will/be/replacedcontainers:- name:pv-recyclerimage:\u0026#34;k8s.gcr.io/busybox\u0026#34;command:[\u0026#34;/bin/sh\u0026#34;,\u0026#34;-c\u0026#34;,\u0026#34;test -e /scrub \u0026amp;\u0026amp; rm -rf /scrub/..?* /scrub/.[!.]* /scrub/* \u0026amp;\u0026amp; test -z \\\u0026#34;$(ls -A /scrub)\\\u0026#34; || exit 1\u0026#34;]volumeMounts:- name:volmountPath:/scrub但是，volumes 部分的自定义回收站模块中指定的特定路径将被替换为正在回收的卷的特定路径。\n删除 对于支持删除回收策略的卷插件，删除操作将从 Kubernetes 中删除 PersistentVolume 对象，并删除外部基础架构（如 AWS EBS、GCE PD、Azure Disk 或 Cinder 卷）中的关联存储资产。动态配置的卷继承其 StorageClass，默认为 Delete。管理员应该根据用户的期望来配置 StorageClass，否则就必须要在 PV 创建后进行编辑或修补。请参阅更改 PersistentVolume 的回收策略。\n扩展持久化卷声明 Kubernetes 1.8 增加了对扩展持久化存储卷的 Alpha 支持。在 v1.9 中，以下持久化卷支持扩展持久化卷声明：\n gcePersistentDisk awsElasticBlockStore Cinder glusterfs rbd  管理员可以通过将 ExpandPersistentVolumes 特性门设置为true来允许扩展持久卷声明。管理员还应该启用PersistentVolumeClaimResize 准入控制插件来执行对可调整大小的卷的其他验证。\n一旦 PersistentVolumeClaimResize 准入插件已打开，将只允许其 allowVolumeExpansion 字段设置为 true 的存储类进行大小调整。\nkind:StorageClassapiVersion:storage.k8s.io/v1metadata:name:gluster-vol-defaultprovisioner:kubernetes.io/glusterfsparameters:resturl:\u0026#34;http://192.168.10.100:8080\u0026#34;restuser:\u0026#34;\u0026#34;secretNamespace:\u0026#34;\u0026#34;secretName:\u0026#34;\u0026#34;allowVolumeExpansion:true一旦功能门和前述准入插件打开后，用户就可以通过简单地编辑声明以请求更大的 PersistentVolumeClaim 卷。这反过来将触发 PersistentVolume 后端的卷扩展。\n在任何情况下都不会创建新的 PersistentVolume 来满足声明。 Kubernetes 将尝试调整现有 volume 来满足声明的要求。\n对于扩展包含文件系统的卷，只有在 ReadWrite 模式下使用 PersistentVolumeClaim 启动新的 Pod 时，才会执行文件系统调整大小。换句话说，如果正在扩展的卷在 pod 或部署中使用，则需要删除并重新创建要进行文件系统调整大小的pod。此外，文件系统调整大小仅适用于以下文件系统类型：\n XFS Ext3、Ext4  注意：扩展 EBS 卷是一个耗时的操作。另外，每6个小时有一个修改卷的配额。\n持久化卷类型 PersistentVolume 类型以插件形式实现。Kubernetes 目前支持以下插件类型：\n GCEPersistentDisk AWSElasticBlockStore AzureFile AzureDisk FC (Fibre Channel) FlexVolume Flocker NFS iSCSI RBD (Ceph Block Device) CephFS Cinder (OpenStack block storage) Glusterfs VsphereVolume Quobyte Volumes HostPath （仅限于但节点测试—— 不会以任何方式支持本地存储，也无法在多节点集群中工作） VMware Photon Portworx Volumes ScaleIO Volumes StorageOS  原始块支持仅适用于以上这些插件。\n持久化卷 每个 PV 配置中都包含一个 sepc 规格字段和一个 status 卷状态字段。\napiVersion:v1kind:PersistentVolumemetadata:name:pv0003spec:capacity:storage:5GivolumeMode:FilesystemaccessModes:- ReadWriteOncepersistentVolumeReclaimPolicy:RecyclestorageClassName:slowmountOptions:- hard- …","date":1653062400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"9ddd7ab2f97e94295dd5c9178a3ad750","permalink":"https://lib.jimmysong.io/kubernetes-handbook/storage/persistent-volume/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/storage/persistent-volume/","section":"kubernetes-handbook","summary":"本文档介绍了 Kubernetes 中 PersistentVolume 的当前状态。建议您在阅读本文档前先熟悉 volume。 介绍 对于管理计算资源来说，管理存储资源明显是另一个问题。PersistentVolume 子系统为用户和管理员提供了一个 API，该 API","tags":["Kubernetes"],"title":"持久化卷（Persistent Volume）","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"本文介绍了 Kubernetes 中 StorageClass 的概念。在阅读本文之前建议先熟悉 卷 和 Persistent Volume（持久卷）。\n介绍 StorageClass 为管理员提供了描述存储 “class（类）” 的方法。 不同的 class 可能会映射到不同的服务质量等级或备份策略，或由群集管理员确定的任意策略。 Kubernetes 本身不清楚各种 class 代表的什么。这个概念在其他存储系统中有时被称为“配置文件”。\nStorageClass 资源 StorageClass 中包含 provisioner、parameters 和 reclaimPolicy 字段，当 class 需要动态分配 PersistentVolume 时会使用到。\nStorageClass 对象的名称很重要，用户使用该类来请求一个特定的方法。 当创建 StorageClass 对象时，管理员设置名称和其他参数，一旦创建了对象就不能再对其更新。\n管理员可以为没有申请绑定到特定 class 的 PVC 指定一个默认的 StorageClass ： 更多详情请参阅 PersistentVolumeClaim 章节。\nkind:StorageClassapiVersion:storage.k8s.io/v1metadata:name:standardprovisioner:kubernetes.io/aws-ebsparameters:type:gp2reclaimPolicy:RetainmountOptions:- debugProvisioner（存储分配器） Storage class 有一个分配器，用来决定使用哪个卷插件分配 PV。该字段必须指定。\n   Volume Plugin Internal Provisioner Config Example     AWSElasticBlockStore ✓ AWS   AzureFile ✓ Azure File   AzureDisk ✓ Azure Disk   CephFS - -   Cinder ✓ OpenStack Cinder   FC - -   FlexVolume - -   Flocker ✓ -   GCEPersistentDisk ✓ GCE   Glusterfs ✓ Glusterfs   iSCSI - -   PhotonPersistentDisk ✓ -   Quobyte ✓ Quobyte   NFS - -   RBD ✓ Ceph RBD   VsphereVolume ✓ vSphere   PortworxVolume ✓ Portworx Volume   ScaleIO ✓ ScaleIO   StorageOS ✓ StorageOS    您不限于指定此处列出的\u0026#34;内置\u0026#34;分配器（其名称前缀为 kubernetes.io 并打包在 Kubernetes 中）。 您还可以运行和指定外部分配器，这些独立的程序遵循由 Kubernetes 定义的 规范。 外部供应商的作者完全可以自由决定他们的代码保存于何处、打包方式、运行方式、使用的插件（包括Flex）等。 代码仓库 kubernetes-incubator/external-storage 包含一个用于为外部分配器编写功能实现的类库，以及各种社区维护的外部分配器。\n例如，NFS 没有内部分配器，但可以使用外部分配器。一些外部分配器在代码仓库 kubernetes-incubator/external-storage 中。 也有第三方存储供应商提供自己的外部分配器。\n关于内置的 StorageClass 的配置请参考 Storage Classes。\n回收策略 由 storage class 动态创建的 Persistent Volume 会在的 reclaimPolicy 字段中指定回收策略，可以是 Delete 或者 Retain。如果 StorageClass 对象被创建时没有指定 reclaimPolicy ，它将默认为 Delete。\n通过 storage class 手动创建并管理的 Persistent Volume 会使用它们被创建时指定的回收政策。\n挂载选项 由 storage class 动态创建的 Persistent Volume 将使用 class 中 mountOptions 字段指定的挂载选项。\n如果卷插件不支持挂载选项，却指定了该选项，则分配操作失败。 安装选项在 class 和 PV 上都不会做验证，所以如果挂载选项无效，那么这个 PV 就会失败。\n参数 Storage class 具有描述属于 storage class 卷的参数。取决于分配器，可以接受不同的参数。 例如，参数 type 的值 io1 和参数 iopsPerGB 特定于 EBS PV。当参数被省略时，会使用默认值。\n参考  https://kubernetes.io/docs/concepts/storage/storage-classes/  ","date":1653062400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"94ebd6acb6b4689bd7f9f7d2f9ea672d","permalink":"https://lib.jimmysong.io/kubernetes-handbook/storage/storageclass/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/storage/storageclass/","section":"kubernetes-handbook","summary":"本文介绍了 Kubernetes 中 StorageClass 的概念。在阅读本文之前建议先熟悉 卷 和 Persistent Volume（持久卷）。 介绍 StorageClass 为管理员提供了描述存储 “class（类）” 的方法。 不同的 class 可能会映射到不同的服务质量等级或备","tags":["Kubernetes"],"title":"Storage Class","type":"book"},{"authors":null,"categories":["Istio"],"content":"集群发现服务（CDS）是一个可选的 API，Envoy 将调用该 API 来动态获取集群管理器的成员。Envoy 还将根据 API 响应协调集群管理，根据需要完成添加、修改或删除已知的集群。\n关于 Envoy 是如何通过 CDS 从 pilot-discovery 服务中获取的 cluster 配置，请参考 Service Mesh深度学习系列part3—istio源码分析之pilot-discovery模块分析（续）一文中的 CDS 服务部分。\n注意\n 在 Envoy 配置中静态定义的 cluster 不能通过 CDS API 进行修改或删除。 Envoy 从 1.9 版本开始已不再支持 v1 API。  统计 CDS 的统计树以 cluster_manager.cds. 为根，统计如下：\n   名字 类型 描述     config_reload Counter 因配置不同而导致配置重新加载的总次数   update_attempt Counter 尝试调用配置加载 API 的总次数   update_success Counter 调用配置加载 API 成功的总次数   update_failure Counter 调用配置加载 API 因网络错误的失败总数   update_rejected Counter 调用配置加载 API 因 schema/验证错误的失败总次数   version Gauge 来自上次成功调用配置加载API的内容哈希   control_plane.connected_state Gauge 布尔值，用来表示与管理服务器的连接状态，1表示已连接，0表示断开连接    ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"741f9eeedad7f75d0a65ba7c05fdf44c","permalink":"https://lib.jimmysong.io/istio-handbook/data-plane/envoy-cds/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/istio-handbook/data-plane/envoy-cds/","section":"istio-handbook","summary":"集群发现服务（CDS）是一个可选的 API，Envoy 将调用该 API 来动态获取集群管理器的成员。Envoy 还将根据 API 响应协调集群管理，根据需要完成添加、修改或删除已知的集群。 关于 Envoy 是如何通过 CDS 从 pilot-discovery 服务中获取","tags":["Istio","Service Mesh"],"title":"CDS（集群发现服务）","type":"book"},{"authors":null,"categories":["Istio"],"content":"ServiceEntry 可以在 Istio 的内部服务注册表中添加额外的条目，这样网格中自动发现的服务就可以访问 / 路由到这些手动指定的服务。服务条目描述了服务的属性（DNS 名称、VIP、端口、协议、端点）。这些服务可以是网格的外部服务（如 Web API），也可以是不属于平台服务注册表的网格内部服务（如与 Kubernetes 中的服务通信的一组虚拟机）。此外，服务条目的端点也可以通过使用 workloadSelector 字段动态选择。这些端点可以是使用 WorkloadEntry 对象声明的虚拟机工作负载或 Kubernetes pod。在单一服务下同时选择 pod 和 VM 的能力允许将服务从 VM 迁移到 Kubernetes，而不必改变与服务相关的现有 DNS 名称。\n示例 下面的例子声明了一些内部应用程序通过 HTTPS 访问的外部 API。Sidecar 检查了 ClientHello 消息中的 SNI 值，以路由到适当的外部服务。\napiVersion:networking.istio.io/v1alpha3kind:ServiceEntrymetadata:name:external-svc-httpsspec:hosts:- api.dropboxapi.com- www.googleapis.com- api.facebook.comlocation:MESH_EXTERNALports:- number:443name:httpsprotocol:TLSresolution:DNS下面的配置在 Istio 的注册表中添加了一组运行在未被管理的虚拟机上的 MongoDB 实例，因此这些服务也可以被视为网格中的任何其他服务。相关的 DestinationRule 被用来启动与数据库实例的 mTLS 连接。\napiVersion:networking.istio.io/v1alpha3kind:ServiceEntrymetadata:name:external-svc-mongoclusterspec:hosts:- mymongodb.somedomain# 未使用addresses:- 192.192.192.192/24# VIPports:- number:27018name:mongodbprotocol:MONGOlocation:MESH_INTERNALresolution:STATICendpoints:- address:2.2.2.2- address:3.3.3.3相关的 DestinationRule。\napiVersion:networking.istio.io/v1alpha3kind:DestinationRulemetadata:name:mtls-mongoclusterspec:host:mymongodb.somedomaintrafficPolicy:tls:mode:MUTUALclientCertificate:/etc/certs/myclientcert.pemprivateKey:/etc/certs/client_private_key.pemcaCertificates:/etc/certs/rootcacerts.pem下面的例子在一个 VirtualService 中使用 ServiceEntry 和 TLS 路由的组合，根据 SNI 值将流量引导到内部出口（egress）防火墙。\napiVersion:networking.istio.io/v1alpha3kind:ServiceEntrymetadata:name:external-svc-redirectspec:hosts:- wikipedia.org- \u0026#34;*.wikipedia.org\u0026#34;location:MESH_EXTERNALports:- number:443name:httpsprotocol:TLSresolution:NONE相关的 VirtualService，根据 SNI 值进行路由。\napiVersion:networking.istio.io/v1alpha3kind:VirtualServicemetadata:name:tls-routingspec:hosts:- wikipedia.org- \u0026#34;*.wikipedia.org\u0026#34;tls:- match:- sniHosts:- wikipedia.org- \u0026#34;*.wikipedia.org\u0026#34;route:- destination:host:internal-egress-firewall.ns1.svc.cluster.local带有 TLS 匹配的 VirtualService 是为了覆盖默认的 SNI 匹配。在没有 VirtualService 的情况下，流量将被转发到维基百科的域。\n下面的例子演示了专用出口（egress）网关的使用，所有外部服务流量都通过该网关转发。exportTo 字段允许控制服务声明对网格中其他命名空间的可视性。默认情况下，服务会被输出到所有命名空间。下面的例子限制了对当前命名空间的可视性，用 . 表示，所以它不能被其他命名空间使用。\napiVersion:networking.istio.io/v1alpha3kind:ServiceEntrymetadata:name:external-svc-httpbinnamespace :egressspec:hosts:- httpbin.comexportTo:- \u0026#34;.\u0026#34;location:MESH_EXTERNALports:- number:80name:httpprotocol:HTTPresolution:DNS定义一个网关来处理所有的出口（egress）流量。\napiVersion:networking.istio.io/v1alpha3kind:Gatewaymetadata:name:istio-egressgatewaynamespace:istio-systemspec:selector:istio:egressgatewayservers:- port:number:80name:httpprotocol:HTTPhosts:- \u0026#34;*\u0026#34;和相关的 VirtualService，从 Sidecar 路由到网关服务（istio-egressgateway.istio-system.svc.cluster.local），以及从网关路由到外部服务。请注意，VirtualService 被导出到所有命名空间，使它们能够通过网关将流量路由到外部服务。迫使流量通过像这样一个受管理的中间代理是一种常见的做法。\napiVersion:networking.istio.io/v1alpha3kind:VirtualServicemetadata:name:gateway-routingnamespace:egressspec:hosts:- httpbin.comexportTo:- \u0026#34;*\u0026#34;gateways:# 同时应用于网格内的网关和所有 sidecar- mesh- istio-egressgatewayhttp:- match:- port:80gateways:- mesh# 应用于网格内的所有 sidecarroute:- destination:host:istio-egressgateway.istio-system.svc.cluster.local- match:- port:80gateways:- istio-egressgateway# 仅应用于网关route:- destination:host:httpbin.com下面的例子演示了在外部服务的主机中使用通配符。如果连接必须被路由到应用程序请求的 IP 地址（即应用程序解析 DNS 并试图连接到一个特定的 IP），发现模式必须被设置为 NONE。\napiVersion:networking.istio.io/v1alpha3kind:ServiceEntrymetadata:name:external-svc-wildcard-examplespec:hosts:- \u0026#34;*.bar.com\u0026#34;location:MESH_EXTERNALports:- number:80name:httpprotocol:HTTPresolution:NONE下面的例子演示了一个通过客户主机上的 Unix 域套接字提供的服务。解析必须设置为 STATIC 以使用 Unix 地址端点。\napiVersion:networking.istio.io/v1alpha3kind:ServiceEntrymetadata:name:unix-domain-socket-examplespec:hosts:- \u0026#34;example.unix.local\u0026#34;location:MESH_EXTERNALports:- number:80name:httpprotocol:HTTPresolution:STATICendpoints:- address:unix:///var/run/example/socket对于基于 HTTP 的服务，可以创建一个由多个 DNS 可寻址端点支持的 VirtualService。在这种情况下，应用程序可以使用 HTTP_PROXY 环境变量来透明地将 VirtualService 的 API 调用重新路由到所选择的后端。例如，下面的配置创建了一个不存在的外部服务，名为foo.bar.com，由三个域名支持：us.foo.bar.com:8080，uk.foo.bar.com:9080和in.foo.bar.com:7080。\napiVersion:networking.istio.io/v1alpha3kind:ServiceEntrymetadata:name:external-svc-dnsspec:hosts:- foo.bar.comlocation:MESH_EXTERNALports:- number:80name:httpprotocol:HTTPresolution:DNSendpoints:- address:us.foo.bar.comports:http:8080- address:uk.foo.bar.comports:http:9080- address:in.foo.bar.comports:http:7080有了HTTP_PROXY=http://localhost/，从应用程序到 http://foo.bar.com 的调用将在上面指定的三个域中进行负载均衡。换句话说，对 http://foo.bar.com/baz 的调用将被转译成 http://uk.foo.bar.com/baz。\n下面的例子说明了包含主题（subject）替代名称的 ServiceEntry 的用法，其格式符合 SPIFFE 标准。\napiVersion:networking.istio.io/v1alpha3kind:ServiceEntrymetadata:name:httpbinnamespace :httpbin-nsspec:hosts:- httpbin.comlocation:MESH_INTERNALports:- number:80name:httpprotocol:HTTPresolution:STATICendpoints:- address:2.2.2.2- address:3.3.3.3subjectAltNames:- \u0026#34;spiffe://cluster.local/ns/httpbin-ns/sa/httpbin-service-account\u0026#34;下面的例子演示了使用带有workloadSelector的ServiceEntry来处理服务details.bookinfo.com从 VM 到 Kubernetes  …","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"ae4a7837a6f5193d127b732f1a57d428","permalink":"https://lib.jimmysong.io/istio-handbook/config-networking/service-entry/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/istio-handbook/config-networking/service-entry/","section":"istio-handbook","summary":"ServiceEntry 可以在 Istio 的内部服务注册表中添加额外的条目，这样网格中自动发现的服务就可以访问 / 路由到这些手动指定的服务。服务条目描述了服务的属性（DNS 名称、VIP、端口、协议、端点）。这些服务可以是网格的外部服务","tags":["Istio","Service Mesh"],"title":"ServiceEntry","type":"book"},{"authors":null,"categories":["Istio"],"content":"这一节将带大家了解 Istio 为数据平面自动注入 Sidecar 的详细过程。\nSidecar 注入过程概览 如 Istio 官方文档中对 Istio sidecar 注入的描述，你可以使用 istioctl 命令手动注入 Sidecar，也可以为 Kubernetes 集群自动开启 sidecar 注入，这主要用到了 Kubernetes 的准入控制器中的 webhook，参考 Istio 官网中对 Istio sidecar 注入的描述。\n   Sidecar 注入流程图  手动注入 sidecar 与自动注入 sidecar 的区别 不论是手动注入还是自动注入，sidecar 的注入过程有需要遵循如下步骤：\n Kubernetes 需要了解待注入的 sidecar 所连接的 Istio 集群及其配置； Kubernetes 需要了解待注入的 sidecar 容器本身的配置，如镜像地址、启动参数等； Kubernetes 根据 sidecar 注入模板和以上配置填充 sidecar 的配置参数，将以上配置注入到应用容器的一侧；  Istio 和 sidecar 配置保存在 istio 和 istio-sidecar-injector 这两个 ConfigMap 中，其中包含了 Go template，所谓自动 sidecar 注入就是将生成 Pod 配置从应用 YAML 文件期间转移到 mutable webhook 中。\nInit 容器 Init 容器是一种专用容器，它在应用程序容器启动之前运行，用来包含一些应用镜像中不存在的实用工具或安装脚本。\n一个 Pod 中可以指定多个 Init 容器，如果指定了多个，那么 Init 容器将会按顺序依次运行。只有当前面的 Init 容器必须运行成功后，才可以运行下一个 Init 容器。当所有的 Init 容器运行完成后，Kubernetes 才初始化 Pod 和运行应用容器。\nInit 容器使用 Linux Namespace，所以相对应用程序容器来说具有不同的文件系统视图。因此，它们能够具有访问 Secret 的权限，而应用程序容器则不能。\n在 Pod 启动过程中，Init 容器会按顺序在网络和数据卷初始化之后启动。每个容器必须在下一个容器启动之前成功退出。如果由于运行时或失败退出，将导致容器启动失败，它会根据 Pod 的 restartPolicy 指定的策略进行重试。然而，如果 Pod 的 restartPolicy 设置为 Always，Init 容器失败时会使用 RestartPolicy 策略。\n在所有的 Init 容器没有成功之前，Pod 将不会变成 Ready 状态。Init 容器的端口将不会在 Service 中进行聚集。 正在初始化中的 Pod 处于 Pending 状态，但应该会将 Initializing 状态设置为 true。Init 容器运行完成以后就会自动终止。\n关于 Init 容器的详细信息请参考 Init 容器 - Kubernetes 中文指南/云原生应用架构实践手册。\nIstio 中的 sidecar 注入过程详解 Istio 中提供了以下两种 sidecar 注入方式：\n 使用 istioctl 手动注入。 基于 Kubernetes 的 突变 webhook 准入控制器（mutating webhook addmission controller 的自动 sidecar 注入方式。  不论是手动注入还是自动注入，sidecar 的注入过程都需要遵循如下步骤：\n Kubernetes 需要了解待注入的 sidecar 所连接的 Istio 集群及其配置； Kubernetes 需要了解待注入的 sidecar 容器本身的配置，如镜像地址、启动参数等； Kubernetes 根据 sidecar 注入模板和以上配置填充 sidecar 的配置参数，将以上配置注入到应用容器的一侧；  使用下面的命令可以手动注入 sidecar。\nistioctl kube-inject -f ${YAML_FILE} | kuebectl apply -f - 该命令会使用 Istio 内置的 sidecar 配置来注入，下面使用 Istio详细配置请参考 Istio 官网。\n注入完成后您将看到 Istio 为原有 pod template 注入了 initContainer 及 sidecar proxy相关的配置。\nSidecar 注入示例分析 以 Istio 官方提供的 bookinfo 中 productpage 的 YAML 为例，关于 bookinfo 应用的详细 YAML 配置请参考 bookinfo.yaml。\n下文将从以下几个方面讲解：\n Sidecar 容器的注入 iptables 规则的创建 路由的详细过程  apiVersion:apps/v1kind:Deploymentmetadata:name:productpage-v1labels:app:productpageversion:v1spec:replicas:1selector:matchLabels:app:productpageversion:v1template:metadata:labels:app:productpageversion:v1spec:serviceAccountName:bookinfo-productpagecontainers:- name:productpageimage:docker.io/istio/examples-bookinfo-productpage-v1:1.15.0imagePullPolicy:IfNotPresentports:- containerPort:9080volumeMounts:- name:tmpmountPath:/tmpvolumes:- name:tmpemptyDir:{}再查看下 productpage 容器的 Dockerfile。\nFROMpython:3.7.4-slimCOPY requirements.txt ./RUN pip install --no-cache-dir -r requirements.txtCOPY test-requirements.txt ./RUN pip install --no-cache-dir -r test-requirements.txtCOPY productpage.py /opt/microservices/COPY tests/unit/* /opt/microservices/COPY templates /opt/microservices/templatesCOPY static /opt/microservices/staticCOPY requirements.txt /opt/microservices/ARG flood_factorENV FLOOD_FACTOR ${flood_factor:-0}EXPOSE9080WORKDIR/opt/microservicesRUN python -m unittest discoverUSER1CMD [\u0026#34;python\u0026#34;, \u0026#34;productpage.py\u0026#34;, \u0026#34;9080\u0026#34;]我们看到 Dockerfile 中没有配置 ENTRYPOINT，所以 CMD 的配置 python productpage.py 9080 将作为默认的 ENTRYPOINT，记住这一点，再看下注入 sidecar 之后的配置。\n$ istioctl kube-inject -f samples/bookinfo/platform/kube/bookinfo.yaml 我们只截取其中与 productpage 相关的 Deployment 配置中的部分 YAML 配置。\ncontainers:- image:docker.io/istio/examples-bookinfo-productpage-v1:1.15.0# 应用镜像name:productpageports:- containerPort:9080- args:- proxy- sidecar- --domain- $(POD_NAMESPACE).svc.cluster.local- --configPath- /etc/istio/proxy- --binaryPath- /usr/local/bin/envoy- --serviceCluster- productpage.$(POD_NAMESPACE)- --drainDuration- 45s- --parentShutdownDuration- 1m0s- --discoveryAddress- istiod.istio-system.svc:15012- --zipkinAddress- zipkin.istio-system:9411- --proxyLogLevel=warning- --proxyComponentLogLevel=misc:error- --connectTimeout- 10s- --proxyAdminPort- \u0026#34;15000\u0026#34;- --concurrency- \u0026#34;2\u0026#34;- --controlPlaneAuthPolicy- NONE- --dnsRefreshRate- 300s- --statusPort- \u0026#34;15020\u0026#34;- --trust-domain=cluster.local- --controlPlaneBootstrap=falseimage:docker.io/istio/proxyv2:1.5.1# sidecar proxyname:istio-proxyports:- containerPort:15090name:http-envoy-promprotocol:TCPinitContainers:- command:- istio-iptables- -p- \u0026#34;15001\u0026#34;- -z- \u0026#34;15006\u0026#34;- -u- \u0026#34;1337\u0026#34;- -m- REDIRECT- -i- \u0026#39;*\u0026#39;- -x- \u0026#34;\u0026#34;- -b- \u0026#39;*\u0026#39;- -d- 15090,15020image:docker.io/istio/proxyv2:1.5.1# init 容器name:istio-initIstio 给应用 Pod 注入的配置主要包括：\n Init 容器 istio-init：用于 pod 中设置 iptables 端口转发 Sidecar 容器 istio-proxy：运行 sidecar 代理，如 Envoy 或 MOSN。  接下来将分别解析下这两个容器。\nInit 容器解析 Istio 在 pod 中注入的 Init 容器名为 istio-init，我们在上面 Istio 注入完成后的 YAML 文件中看到了该容器的启动命令是：\nistio-iptables -p 15001 -z 15006 -u 1337 -m REDIRECT -i \u0026#39;*\u0026#39; -x \u0026#34;\u0026#34; -b \u0026#39;*\u0026#39; -d 15090,15020 我们再检查下该容器的 Dockerfile 看看 ENTRYPOINT 是怎么确定启动时执行的命令。\n# 前面的内容省略# The pilot-agent will bootstrap Envoy.ENTRYPOINT [\u0026#34;/usr/local/bin/pilot-agent\u0026#34;]我们看到 istio-init 容器的入口是 /usr/local/bin/istio-iptables 命令行，该命令行工具的代码的位置在 Istio 源码仓库的 tools/istio-iptables 目录。\n注意：在 Istio 1.1 版本时还是使用 isito-iptables.sh 命令行来操作 IPtables。\nInit 容器启动入口 Init …","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"701101d65743d205c3818582031017bd","permalink":"https://lib.jimmysong.io/istio-handbook/concepts/istio-sidecar-injector/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/istio-handbook/concepts/istio-sidecar-injector/","section":"istio-handbook","summary":"这一节将带大家了解 Istio 为数据平面自动注入 Sidecar 的详细过程。 Sidecar 注入过程概览 如 Istio 官方文档中对 Istio sidecar 注入的描述，你可以使用 istioctl 命令手动注入 Sidecar，也可以为 Kubernetes 集群自动开启 sidecar 注入，这主要用到了 Kubernetes 的准入控制器中的 w","tags":["Istio","Service Mesh"],"title":"Sidecar 自动注入过程详解","type":"book"},{"authors":null,"categories":["Istio"],"content":"为了帮助我们提高服务的弹性，我们可以使用故障注入功能。我们可以在 HTTP 流量上应用故障注入策略，在转发目的地的请求时指定一个或多个故障注入。\n有两种类型的故障注入。我们可以在转发前延迟（delay）请求，模拟缓慢的网络或过载的服务，我们可以中止（abort） HTTP 请求，并返回一个特定的 HTTP 错误代码给调用者。通过中止，我们可以模拟一个有故障的上游服务。\n下面是一个中止 HTTP 请求并返回 HTTP 404 的例子，针对 30% 的传入请求。\n- route:- destination:host:customers.default.svc.cluster.localsubset:v1fault:abort:percentage:value:30httpStatus:404如果我们不指定百分比，所有的请求将被中止。请注意，故障注入会影响使用该 VirtualService 的服务。它并不影响该服务的所有消费者。\n同样地，我们可以使用 fixedDelay 字段对请求应用一个可选的延迟。\n- route:- destination:host:customers.default.svc.cluster.localsubset:v1fault:delay:percentage:value:5fixedDelay:3s上述设置将对 5% 的传入请求应用 3 秒的延迟。\n注意，故障注入将不会触发我们在路由上设置的任何重试策略。例如，如果我们注入了一个 HTTP 500 的错误，配置为在 HTTP 500 上的重试策略将不会被触发。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"c008da1d29706e8f1960d42914b3b0b6","permalink":"https://lib.jimmysong.io/istio-handbook/traffic-management/fault-injection/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/istio-handbook/traffic-management/fault-injection/","section":"istio-handbook","summary":"为了帮助我们提高服务的弹性，我们可以使用故障注入功能。我们可以在 HTTP 流量上应用故障注入策略，在转发目的地的请求时指定一个或多个故障注入。 有两种类型的故障注入。我们可以在转发前延迟（delay）请求，模拟","tags":["Istio","Service Mesh"],"title":"错误注入","type":"book"},{"authors":null,"categories":["Envoy"],"content":"代理协议监听器过滤器（envoy.filters.listener.proxy_protocol）增加了对 HAProxy 代理协议的支持。\n代理使用其 IP 堆栈连接到远程服务器，并丢失初始连接的源和目的地信息。PROXY 协议允许我们在不丢失客户端信息的情况下链接代理。该协议定义了一种在主 TCP 流之前通过 TCP 通信连接的元数据的方式。元数据包括源 IP 地址。\n使用这个过滤器，Envoy 可以从 PROXY 协议中获取元数据，并将其传播到 x-forwarded-for 头中。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"b173a15f77637d160c7795e5803e3be3","permalink":"https://lib.jimmysong.io/envoy-handbook/listener/proxy-protocol-listener-filter/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/envoy-handbook/listener/proxy-protocol-listener-filter/","section":"envoy-handbook","summary":"代理协议监听器过滤器（envoy.filters.listener.proxy_protocol）增加了对 HAProxy 代理协议的支持。 代理使用其 IP 堆栈连接到远程服务器，并丢失初始连接的源和目的地信息。PROXY","tags":["Envoy"],"title":"代理协议监听器过滤器","type":"book"},{"authors":null,"categories":["Istio"],"content":"为了演示弹性功能，我们将在产品目录服务部署中添加一个名为 EXTRA_LATENCY 的环境变量。这个变量会在每次调用服务时注入一个额外的休眠。\n通过运行 kubectl edit deploy productcatalogservice 来编辑产品目录服务部署。这将打开一个编辑器。滚动到有环境变量的部分，添加 EXTRA_LATENCY 环境变量。\n...spec:containers:- env:- name:EXTRA_LATENCYvalue:6s...保存并推出编辑器。\n如果我们刷新页面，我们会发现页面需要 6 秒的时间来加载）——那是由于我们注入的延迟。\n让我们给产品目录服务添加一个 2 秒的超时。\napiVersion:networking.istio.io/v1alpha3kind:VirtualServicemetadata:name:productcatalogservicespec:hosts:- productcatalogservicehttp:- route:- destination:host:productcatalogservicetimeout:2s将上述 YAML 保存为 productcatalogservice-timeout.yaml，并使用 kubectl apply -f productcatalogservice-timeout.yaml 创建 VirtualService。\n如果我们刷新页面，我们会注意到一个错误信息的出现：\nrpc error: code = Unavailable desc = upstream request timeout could not retrieve products 该错误表明对产品目录服务的请求超时了。我们修改了服务，增加了 6 秒的延迟，并将超时设置为 2 秒。\n让我们定义一个重试策略，有三次尝试，每次尝试的超时为 1 秒。\napiVersion:networking.istio.io/v1alpha3kind:VirtualServicemetadata:name:productcatalogservicespec:hosts:- productcatalogservicehttp:- route:- destination:host:productcatalogserviceretries:attempts:3perTryTimeout:1s由于我们在产品目录服务部署中留下了额外的延迟，我们仍然会看到错误。让我们打开 Zipkin 中的追踪，看看重试策略的作用。\n使用 getmesh istioctl dash zipkin 来打开 Zipkin 仪表盘。点击 + 按钮，选择 serviceName 和 frontend.default。为了只得到至少一秒钟的响应（这就是我们的 perTryTimeout），选择 minDuration，在文本框中输入 1s。点击搜索按钮，显示所有追踪。\n点击 Filter 按钮，从下拉菜单中选择 productCatalogService.default。你应该看到花了 1 秒钟的 trace。这些 trace 对应于我们之前定义的 perTryTimeout。\n   Zipkin 中的 trace  运行 kubectl delete vs productcatalogservice 删除 VirtualService。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"01a3c76d872f3018987b4fc516078572","permalink":"https://lib.jimmysong.io/istio-handbook/practice/resiliency/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/istio-handbook/practice/resiliency/","section":"istio-handbook","summary":"为了演示弹性功能，我们将在产品目录服务部署中添加一个名为 EXTRA_LATENCY 的环境变量。这个变量会在每次调用服务时注入一个额外的休眠。 通过运行 kubectl edit deploy productcatalogservice 来编辑产品目录服务部署。这将打开一个编辑器。滚动到有环境变量的部分，","tags":["Istio","Service Mesh"],"title":"弹性","type":"book"},{"authors":null,"categories":["Envoy"],"content":"/listeners 端点列出了所有配置的监听器。这包括名称以及每个监听器的地址和监听的端口。\n例如：\n$ curl localhost:9901/listeners http_8080::0.0.0.0:8080 http_hello_world_9090::0.0.0.0:9090 对于 JSON 输出，我们可以在 URL 上附加 ?format=json。\n$ curl localhost:9901/listeners?format=json { \u0026#34;listener_statuses\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;http_8080\u0026#34;, \u0026#34;local_address\u0026#34;: { \u0026#34;socket_address\u0026#34;: { \u0026#34;address\u0026#34;: \u0026#34;0.0.0.0\u0026#34;, \u0026#34;port_value\u0026#34;: 8080 } } }, { \u0026#34;name\u0026#34;: \u0026#34;http_hello_world_9090\u0026#34;, \u0026#34;local_address\u0026#34;: { \u0026#34;socket_address\u0026#34;: { \u0026#34;address\u0026#34;: \u0026#34;0.0.0.0\u0026#34;, \u0026#34;port_value\u0026#34;: 9090 } } } ] } 监听器排空 发生排空（draining）的一个典型场景是在热重启排空期间。它涉及到在 Envoy 进程关闭之前，通过指示监听器停止接受传入的请求来减少打开连接的数量。\n默认情况下，如果我们关闭 Envoy，所有的连接都会立即关闭。要进行优雅的关闭（即不关闭现有的连接），我们可以使用 /drain_listeners 端点，并加入一个可选的 graceful 查询参数。\nEnvoy 根据通过 --drain-time-s 和 --drain-strategy 指定的配置来排空连接。\n如果没有提供，排空时间默认为 10 分钟（600 秒）。该值指定了 Envoy 将排空连接的时间——即在关闭它们之前等待多久。\n排空策略参数决定了排空序列中的行为（例如，在热重启期间），连接是通过发送 “Connection:CLOSE”（HTTP/1.1）或 GOAWAY 帧（HTTP/2）。\n有两种支持的策略：渐进（默认）和立即。当使用渐进策略时，随着排空时间的推移，排空的请求的百分比慢慢增加到 100%。即时策略将使所有的请求在排空序列开始后立即排空。\n排空是按监听器进行的。然而，它必须在网络过滤器层面得到支持。目前支持优雅排空的过滤器是 Redis、Mongo 和 HTTP 连接管理器。\n端点的另一个选项是使用 inboundonly 查询参数（例如，/drain_listeners?inboundonly）排空所有入站监听器的能力。这使用监听器上的 traffic_direction 字段来确定流量方向。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"021c0f02913fc89ea7eb9f5d6d291768","permalink":"https://lib.jimmysong.io/envoy-handbook/admin-interface/listeners-and-draining/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/envoy-handbook/admin-interface/listeners-and-draining/","section":"envoy-handbook","summary":"/listeners 端点列出了所有配置的监听器。这包括名称以及每个监听器的地址和监听的端口。 例如： $ curl localhost:9901/listeners http_8080::0.0.0.0:8080 http_hello_world_9090::0.0.0.0:9090 对于 JSON 输出，我们可以在 URL 上附加 ?format=json。 $ curl localhost:9901/listeners?format=json { \"listener_statuses\": [ { \"name\": \"http_8080\", \"local_address\": { \"socket_address\": { \"address\": \"0.0.0.0\", \"port_value\": 8080 } } }, { \"name\": \"http_hello_world_9090\", \"local_address\": { \"socket_address\": {","tags":["Envoy"],"title":"监听器和监听器的排空","type":"book"},{"authors":null,"categories":["Envoy"],"content":"本实验涵盖了如何配置 Envoy 使用独立的 gRPC 访问日志服务（ALS）。我们将使用一个基本的 gRPC 服务器，它实现了 StreamAccessLogs 函数，并将收到的日志从 Envoy 输出到标准输出。\n让我们先把 ALS 服务器作为一个 Docker 容器来运行。\ndocker run -dit -p 5000:5000 gcr.io/tetratelabs/envoy-als:0.1.0 ALS 服务器默认监听端口为 5000，所以如果我们看一下 Docker 容器的日志，它们应该类似于下面的内容。\n$ docker logs [container-id] Creating new ALS server 2021/11/05 20:24:03 Listening on :5000 输出告诉我们，ALS 正在监听 5000 端口。\n在 Envoy 配置中，有两个要求需要配置。首先是使用 access_log 字段的访问日志和一个名为 HttpGrpcAccessLogConfig 的日志类型。其次，在访问日志配置中，我们必须引用 gRPC 服务器。为此定义一个 Envoy 集群。\n下面是配置记录器并指向名为 grpc_als_cluster的 Envoy 集群的片段。\n...access_log:- name:envoy.access_loggers.http_grpctyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.access_loggers.grpc.v3.HttpGrpcAccessLogConfigcommon_config:log_name:\u0026#34;mygrpclog\u0026#34;transport_api_version:V3grpc_service:envoy_grpc:cluster_name:grpc_als_cluster...下一个片段是集群配置，在这一点上我们应该已经很熟悉了。在我们的例子中，我们在同一台机器上运行 gRPC 服务器，端口为 5000。\n...clusters:- name:grpc_als_clusterconnect_timeout:5stype:STRICT_DNShttp2_protocol_options:{}load_assignment:cluster_name:grpc_als_clusterendpoints:- lb_endpoints:- endpoint:address:socket_address:address:127.0.0.1port_value:5000...让我们把这两块放在一起，得出一个使用 gRPC 访问日志服务的 Envoy 配置示例。\nstatic_resources:listeners:- name:listener_0address:socket_address:address:0.0.0.0port_value:10000filter_chains:- filters:- name:envoy.filters.network.http_connection_managertyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManagerstat_prefix:ingress_httpaccess_log:- name:envoy.access_loggers.http_grpctyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.access_loggers.grpc.v3.HttpGrpcAccessLogConfigcommon_config:log_name:\u0026#34;mygrpclog\u0026#34;transport_api_version:V3grpc_service:envoy_grpc:cluster_name:grpc_als_clusterhttp_filters:- name:envoy.filters.http.routerroute_config:name:my_first_routevirtual_hosts:- name:direct_response_servicedomains:[\u0026#34;*\u0026#34;]routes:- match:prefix:\u0026#34;/404\u0026#34;direct_response:status:404body:inline_string:\u0026#34;404\u0026#34;- match:prefix:\u0026#34;/\u0026#34;direct_response:status:200body:inline_string:\u0026#34;200\u0026#34;clusters:- name:grpc_als_clusterconnect_timeout:5stype:STRICT_DNShttp2_protocol_options:{}load_assignment:cluster_name:grpc_als_clusterendpoints:- lb_endpoints:- endpoint:address:socket_address:address:127.0.0.1port_value:5000将上述 YAML 保存为 6-lab-2-grpc-als.yaml，并使用 func-e 启动 Envoy 代理。\nfunc-e run -c 6-lab-2-grpc-als.yaml \u0026amp; 我们正在后台运行 Docker 容器和 Envoy，所以我们现在可以用 curl 向 Envoy 代理发送几个请求。\n$ curl localhost:10000 200 代理的回应是 200，因为那是我们在配置中所定义的。你会注意到没有任何日志输出到标准输出，这是预期的。\n要看到这些日志，我们必须看一下 Docker 容器的日志。你可以使用 docker ps 来获取容器 ID，然后运行 logs 命令。\n$ docker logs 96f Creating new ALS server 2021/11/05 20:24:03 Listening on :5000 2021/11/05 20:33:52 Received value 2021/11/05 20:33:52 {\u0026#34;identifier\u0026#34;:{\u0026#34;node\u0026#34;:{\u0026#34;userAgentName\u0026#34;:\u0026#34;envoy\u0026#34;,\u0026#34;userAgentBuildVersion\u0026#34;:{\u0026#34;version\u0026#34;:{\u0026#34;majorNumber\u0026#34;:1,\u0026#34;minorNumber\u0026#34;:20},\u0026#34;metadata\u0026#34;:{\u0026#34;fields\u0026#34;:{\u0026#34;build.type\u0026#34;:{\u0026#34;stringValue\u0026#34;:\u0026#34;RELEASE\u0026#34;},\u0026#34;revision.sha\u0026#34;:{\u0026#34;stringValue\u0026#34;:\u0026#34;96701cb24611b0f3aac1cc0dd8bf8589fbdf8e9e\u0026#34;},\u0026#34;revision.status\u0026#34;:{\u0026#34;stringValue\u0026#34;:\u0026#34;Clean\u0026#34;},\u0026#34;ssl.version\u0026#34;:{\u0026#34;stringValue\u0026#34;:\u0026#34;BoringSSL\u0026#34;}}}},\u0026#34;extensions\u0026#34;:[{\u0026#34;name\u0026#34;:\u0026#34;envoy.matching.common_inputs.environment_variable\u0026#34;,\u0026#34;category\u0026#34;:\u0026#34;envoy.matching.common_inputs\u0026#34;},{\u0026#34;name\u0026#34;:\u0026#34;envoy.access_loggers.file\u0026#34;,\u0026#34;category\u0026#34;:\u0026#34;envoy.access_loggers\u0026#34;},{\u0026#34;name\u0026#34;:\u0026#34;envoy.access_loggers.http_grpc\u0026#34;,\u0026#34;category\u0026#34;:\u0026#34;envoy.access_loggers\u0026#34;},{\u0026#34;name\u0026#34;:\u0026#34;envoy.access_loggers.open_telemetry\u0026#34;,\u0026#34;category\u0026#34;:\u0026#34;envoy.access_loggers\u0026#34;},{\u0026#34;name\u0026#34;:\u0026#34;envoy.access_loggers.stderr\u0026#34;,\u0026#34;category\u0026#34;:\u0026#34;envoy.access_loggers\u0026#34;},{\u0026#34;name\u0026#34;:\u0026#34;envoy.access_loggers.stdout\u0026#34;,\u0026#34;category\u0026#34;:\u0026#34;envoy.access_loggers\u0026#34;},{\u0026#34;name\u0026#34;:\u0026#34;envoy.acc... ... 然后我们会注意到从 Envoy 代理发送到我们 gRPC 服务器的日志条目。gRPC 服务器中的代码很简单，只是将收到的值转换为一个字符串并输出。\n下面是完整的 StreamAccessLogs 函数的样子。\nfunc (s *server) StreamAccessLogs(stream v3.AccessLogService_StreamAccessLogsServer) error { for { in, err := stream.Recv() log.Println(\u0026#34;Received value\u0026#34;) if err == io.EOF { return nil } if err != nil { return err } str, _ := s.marshaler.MarshalToString(in) log.Println(str) } } 在这一点上，我们可以从收到的数据流中解析具体的数值，并决定如何格式化它们以及将它们发送到哪里。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"7d3c11c657cd0c4fea99b76ace154544","permalink":"https://lib.jimmysong.io/envoy-handbook/logging/lab13/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/envoy-handbook/logging/lab13/","section":"envoy-handbook","summary":"本实验涵盖了如何配置 Envoy 使用独立的 gRPC 访问日志服务（ALS）。我们将使用一个基本的 gRPC 服务器，它实现了 StreamAccessLogs 函数，并将收到的日志从 Envoy 输出到标准输出。 让我们先把 ALS 服务器作为一个 Docker 容器来运行。 docker run -dit -p 5000:5000 gcr.io/tetratelabs/envoy-als:0.1.0 ALS 服务器默","tags":["Envoy"],"title":"实验 13：使用 gRPC 访问日志服务（ALS）记录日志","type":"book"},{"authors":null,"categories":["Envoy"],"content":"在这个实验中，我们将演示如何使用断路器。我们将运行一个 Python HTTP 服务器和一个 Envoy 代理在它前面。要启动在 8000 端口监听的 Python 服务器，请运行：\npython3 -m http.server 8000 接下来，我们将用下面的断路器创建 Envoy 配置：\n...circuit_breakers:thresholds:max_connections:20max_requests:100max_pending_requests:20因此，如果我们超过了 20 个连接或 100 个请求或 20 个待处理请求，断路器就会断开。下面是完整的 Envoy 配置：\nstatic_resources:listeners:- name:listener_0address:socket_address:address:0.0.0.0port_value:10000filter_chains:- filters:- name:envoy.filters.network.http_connection_managertyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManagerstat_prefix:listener_httphttp_filters:- name:envoy.filters.http.routerroute_config:name:routevirtual_hosts:- name:vhdomains:[\u0026#34;*\u0026#34;]routes:- match:prefix:\u0026#34;/\u0026#34;route:cluster:python_serverclusters:- name:python_serverconnect_timeout:5scircuit_breakers:thresholds:max_connections:20max_requests:100max_pending_requests:20load_assignment:cluster_name:python_serverendpoints:- lb_endpoints:- endpoint:address:socket_address:address:127.0.0.1port_value:8000admin:address:socket_address:address:127.0.0.1port_value:9901将上述配置保存为 3-lab-1-circuit-breaker.yaml，然后运行 Envoy 代理。\nfunc-e run -c 3-lab-1-circuit-breaker.yaml 为了向代理发送多个并发请求，我们将使用一个名为 hey 的工具。默认情况下，hey 运行 50 个并发，发送 200 个请求，所以我们在请求 http://localhost:1000，甚至不需要传入任何参数。\nhey http://localhost:10000 ... Status code distribution: [200] 104 responses [503] 96 responses hey 将输出许多统计数字，但我们感兴趣的是状态码的分布。它显示我们在收到了 104 个 HTTP 200 响应，在 96 个 HTTP 503 响应 —— 这就是断路器断开的地方。\n我们可以使用 Envoy 的管理接口（运行在 9901 端口）来查看详细的指标，比如说：\n... envoy_cluster_upstream_cx_overflow{envoy_cluster_name=\u0026#34;python_server\u0026#34;} 104 envoy_cluster_upstream_rq_pending_overflow{envoy_cluster_name=\u0026#34;python_server\u0026#34;} 96  下一章   ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"3ee88c536e27312cf75f3f0bed9eaf05","permalink":"https://lib.jimmysong.io/envoy-handbook/cluster/lab7/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/envoy-handbook/cluster/lab7/","section":"envoy-handbook","summary":"在这个实验中，我们将演示如何使用断路器。我们将运行一个 Python HTTP 服务器和一个 Envoy 代理在它前面。要启动在 8000 端口监听的 Python 服务器，请运行： python3 -m http.server 8000 接下来，我们将用下面的断路器创建 Envoy 配置： ...circuit_bre","tags":["Envoy"],"title":"实验7：断路器","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"本地持久化卷允许用户通过标准 PVC 接口以简单便携的方式访问本地存储。PV 中包含系统用于将 Pod 安排到正确节点的节点亲和性信息。\n一旦配置了本地卷，外部静态配置器（provisioner）可用于帮助简化本地存储管理。请注意，本地存储配置器与大多数配置器不同，并且尚不支持动态配置。相反，它要求管理员预先配置每个节点上的本地卷，并且这些卷应该是：\n Filesystem volumeMode（默认）PV—— 将它们挂载到发现目录下。 Block volumeMode PV——在发现目录下为节点上的块设备创建一个符号链接。  配置器将通过为每个卷创建和清除 PersistentVolumes 来管理发现目录下的卷。\n配置要求  本地卷插件希望路径稳定，包括在重新启动时和添加或删除磁盘时。 静态配置器仅发现挂载点（对于文件系统模式卷）或符号链接（对于块模式卷）。对于基于目录的本地卷必须绑定到发现目录中。  版本兼容性 推荐配置器版本与Kubernetes版本\n   Provisioner version K8s version Reason     2.1.0 1.10 Beta API default, block   2.0.0 1.8, 1.9 Mount propagation   1.0.1 1.7     K8s功能状态 另请参阅已知问题和 CHANGELOG。\n1.10：Beta  添加了新的 PV.NodeAffinity 字段。 重要： Alpha PV NodeAffinity annotation 已弃用。用户必须手动更新其 PV 以使用新的 NodeAffinity字段或运行一次性更新作业。 Alpha：添加了对 raw block 的支持。  1.9：Alpha  新的 StorageClass volumeBindingMode 参数将延迟PVC绑定，直到 pod 被调度。  1.7：Alpha  新的local PersistentVolume 源，允许指定具有 node affinity 的目录或挂载点。 使用绑定到该 PV 的 PVC 的 Pod 将始终调度到该节点。  未来的功能  本地块设备作为卷源，具有分区和 fs 格式化 共享本地持久化存储的动态资源调配 当地 PV 健康监测、污点和容忍 内联 PV（使用专用本地磁盘作为临时存储）  用户指南 这些说明反映了最新版本的代码库。有关旧版本的说明，请参阅版本兼容性下的版本链接。\n步骤1：使用本地磁盘启动集群 启用alpha feature gate 1.10+ 如果需要原始的本地块功能，\nexport KUBE_FEATURE_GATES =\u0026#34;BlockVolume = true\u0026#34; 注意：1.10 之前的 Kubernetes 版本需要几个附加 feature gate，因为持久的本地卷和其他功能处于 alpha 版本。\n选项1：裸金属环境  根据应用程序的要求对每个节点上的磁盘进行分区和格式化。 根据 StorageClass 将所有文件系统挂载到同一个目录下。目录在 configmap 中指定，见下文。 使用 KUBE_FEATURE_GATES 配置 Kubernetes API server、controller manager、scheduler 和所有kubelet，如上所述。 如果不使用默认 Kubernetes 调度程序策略，则必须启用以下谓词：  1.9之前：NoVolumeBindConflict 1.9+：VolumeBindingChecker    选项2：本地测试集群   创建 /mnt/disks 目录并将多个卷挂载到其子目录。下面的示例使用三个 ram 磁盘来模拟真实的本地卷：\nmkdir/mnt/disks vol for vol1 vol2 vol3;do mkdir/mnt/disks/$vol mount -t tmpfs $vol/mnt/disks/$vol DONE   运行本地集群。\n$ALLOW_PRIVILEGED = true LOG_LEVEL = 5 FEATURE_GATES = $KUBE_FEATURE_GATES hack/local-up-cluster.sh   步骤2：创建StorageClass（1.9+） 要延迟卷绑定，直到 pod 被调度，并在单个 pod 中处理多个本地 PV，必须使用设置为 WaitForFirstConsumer 的 volumeBindingMode 创建 StorageClass。\n$kubectl create -f provisioner/deployment/kubernetes/example/default_example_storageclass.yaml 步骤3：创建本地持久卷 选项1：使用本地卷静态配置器   生成 Provisioner 的 ServiceAccount、Role、DaemonSet 和 ConfigMap 规范，并对其进行自定义。\n这一步使用 helm 模板来生成规格。有关安装说明，请参阅helm readme。要使用默认值生成配置器的规格，请运行：\nhelm template ./helm/provisioner \u0026gt; ./provisioner/deployment/kubernetes/provisioner_generated.yaml 您也可以提供一个自定义值文件：\nhelm template ./helm/provisioner --values custom-values.yaml \u0026gt; ./provisioner/deployment/kubernetes/provisioner_generated.yaml   部署配置程序\n如果用户对 Provisioner 的 yaml 文件的内容感到满意，可以用 kubectl 创建 Provisioner 的 DaemonSet 和 ConfigMap。\n$kubectl create -f ./provisioner/deployment/kubernetes/provisioner_generated.yaml   检查发现的本地卷\n一旦启动，外部静态配置器将发现并创建本地 PV。\n例如，如果目录 /mnt/disks/ 包含一个目录 /mnt/disks/vol1，则静态配置器会创建以下本地卷 PV：\n$ kubectl get pv NAME CAPACITY ACCESSMODES RECLAIMPOLICY STATUS CLAIM STORAGECLASS REASON AGE local-pv-ce05be60 1024220Ki RWO Delete Available local-storage 26s $ kubectl describe pv local-pv-ce05be60 Name:\tlocal-pv-ce05be60 Labels:\t\u0026lt;none\u0026gt; Annotations:\tpv.kubernetes.io/provisioned-by=local-volume-provisioner-minikube-18f57fb2-a186-11e7-b543-080027d51893 StorageClass:\tlocal-fast Status:\tAvailable Claim:\tReclaim Policy:\tDelete Access Modes:\tRWO Capacity:\t1024220Ki NodeAffinity: Required Terms: Term 0: kubernetes.io/hostname in [my-node] Message:\tSource: Type:\tLocalVolume (a persistent volume backed by local storage on a node) Path:\t/mnt/disks/vol1 Events:\t\u0026lt;none\u0026gt; 上面描述的 PV 可以通过引用 local-fast storageClassName 声明和绑定到 PVC。\n  选项2：手动创建本地持久化卷 有关示例 PersistentVolume 规范，请参阅Kubernetes文档。\n步骤4：创建本地持久卷声明 kind:PersistentVolumeClaimapiVersion:v1metadata:name:example-local-claimspec:accessModes:- ReadWriteOnceresources:requests:storage:5GistorageClassName:local-storage请替换以下元素以反映您的配置：\n 卷所需的存储容量“5Gi” “local-storage”，与本地 PV 关联的存储类名称应该用于满足此 PVC  对于试图声明 “Block” PV 的 “Block” volumeMode PVC，可以使用以下示例：\nkind:PersistentVolumeClaimapiVersion:v1metadata:name:example-local-claimspec:accessModes:- ReadWriteOnceresources:requests:storage:5GivolumeMode:BlockstorageClassName:local-storage请注意，此处唯一需要注意的字段是 volumeMode，它已被设置为“Block”。\n最佳实践  对于IO隔离，建议每个卷使用整个磁盘 对于容量隔离，建议使用单个分区 避免重新创建具有相同节点名称的节点，而仍然存在指定了该节点亲和性的旧 PV。否则，系统可能认为新节点包含旧的 PV。 对于带有文件系统的卷，建议在 fstab 条目和该挂载点的目录名称中使用它们的 UUID（例如 ls -l/dev/disk/by-uuid  的输出）。这种做法可确保即使设备路径发生变化（例如，如果 /dev/sda1 在添加新磁盘时变为 /dev/sdb1），也不会错误地挂在本地卷。此外，这种做法将确保如果创建具有相同名称的另一个节点，则该节点上的任何卷都是唯一的，而不会误认为是具有相同名称的另一个节点上的卷。 对于没有文件系统的 raw block 卷，使用唯一的 ID 作为符号链接名称。根据您的环境，/dev/disk/by-id/ 中的卷 ID 可能包含唯一的硬件序列号。否则，应该生成一个唯一的 ID。符号链接名称的唯一性将确保如果创建具有相同名称的另一个节点，则该节点上的任何卷都是唯一的，而不会误认为是具有相同名称的另一个节点上的卷。  删除/清理底层卷 当您想要停用本地卷时，以下是可能的工作流程。\n 停止使用卷的 pod 从节点中删除本地卷（即卸载、拔出磁盘等） 删除 PVC 供应商将尝试清理卷，但由于卷不再存在而会失败 手动删除 PV 对象  参考  Local Persistent Storage User Guide  ","date":1653062400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"1d52d9ba5d69815f5f15476df44ab45d","permalink":"https://lib.jimmysong.io/kubernetes-handbook/storage/local-persistent-storage/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/storage/local-persistent-storage/","section":"kubernetes-handbook","summary":"本地持久化卷允许用户通过标准 PVC 接口以简单便携的方式访问本地存储。PV 中包含系统用于将 Pod 安排到正确节点的节点亲和性信息。 一旦配置了本地卷，外部静态配置器（provisioner）可用于帮助简化本地存储管","tags":["Kubernetes"],"title":"本地持久化存储","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"自定义资源是对 Kubernetes API 的扩展，Kubernetes 中的每个资源都是一个 API 对象的集合，例如我们在 YAML 文件里定义的那些 spec 都是对 Kubernetes 中的资源对象的定义，所有的自定义资源可以跟 Kubernetes 中内建的资源一样使用 kubectl 操作。\n自定义资源 Kubernetes 从 1.6 版本开始包含一个内建的资源叫做 TPR（ThirdPartyResource），可以用它来创建自定义资源，但该资源在 Kubernetes 1.7 版本开始已被 CRD（CustomResourceDefinition）取代。\n扩展 API 自定义资源实际上是为了扩展 Kubernetes 的 API，向 Kubernetes API 中增加新类型，可以使用以下三种方式：\n 修改 Kubernetes 的源码，显然难度比较高，也不太合适 创建自定义 API server 并聚合到 API 中  编写自定义资源是扩展 Kubernetes API 的最简单的方式，是否编写自定义资源来扩展 API 请参考 Should I add a custom resource to my Kubernetes Cluster?，行动前请先考虑以下几点：\n 你的 API 是否属于 声明式的 是否想使用 kubectl 命令来管理 是否要作为 Kubernetes 中的对象类型来管理，同时显示在 Kubernetes dashboard 上 是否可以遵守 Kubernetes 的 API 规则限制，例如 URL 和 API group、namespace 限制 是否可以接受该 API 只能作用于集群或者 namespace 范围 想要复用 Kubernetes API 的公共功能，比如 CRUD、watch、内置的认证和授权等  如果这些都不是你想要的，那么你可以开发一个独立的 API。\nCRD 参考下面的 CRD，resourcedefinition.yaml：\napiVersion:apiextensions.k8s.io/v1beta1kind:CustomResourceDefinitionmetadata:# 名称必须符合下面的格式：\u0026lt;plural\u0026gt;.\u0026lt;group\u0026gt;name:crontabs.stable.example.comspec:# REST API 使用的组名称：/apis/\u0026lt;group\u0026gt;/\u0026lt;version\u0026gt;group:stable.example.com# REST API 使用的版本号：/apis/\u0026lt;group\u0026gt;/\u0026lt;version\u0026gt;version:v1# Namespaced 或 Clusterscope:Namespacednames:# URL 中使用的复数名称: /apis/\u0026lt;group\u0026gt;/\u0026lt;version\u0026gt;/\u0026lt;plural\u0026gt;plural:crontabs# CLI 中使用的单数名称singular:crontab# CamelCased 格式的单数类型。在清单文件中使用kind:CronTab# CLI 中使用的资源简称shortNames:- ct创建该 CRD：\nkubectl create -f resourcedefinition.yaml 访问 RESTful API 端点如 http://172.20.0.113:8080 将看到如下 API 端点已创建：\n/apis/stable.example.com/v1/namespaces/*/crontabs/... 创建自定义对象\n如下所示：\napiVersion:\u0026#34;stable.example.com/v1\u0026#34;kind:CronTabmetadata:name:my-new-cron-objectspec:cronSpec:\u0026#34;* * * * /5\u0026#34;image:my-awesome-cron-image引用该自定义资源的 API 创建对象。\n终止器\n可以为自定义对象添加一个终止器，如下所示：\napiVersion:\u0026#34;stable.example.com/v1\u0026#34;kind:CronTabmetadata:finalizers:- finalizer.stable.example.com删除自定义对象前，异步执行的钩子。对于具有终止器的一个对象，删除请求仅仅是为metadata.deletionTimestamp 字段设置一个值，而不是删除它，这将触发监控该对象的控制器执行他们所能处理的任意终止器。\n详情参考：Extend the Kubernetes API with CustomResourceDefinitions\n自定义控制器 单纯设置了自定义资源，并没有什么用，只有跟自定义控制器结合起来，才能将资源对象中的声明式 API 翻译成用户所期望的状态。自定义控制器可以用来管理任何资源类型，但是一般是跟自定义资源结合使用。\n请参考使用 Operator 模式，该模式可以让开发者将自己的领域知识转换成特定的 Kubernetes API 扩展。\nAPI server 聚合 Aggregated（聚合的）API server 是为了将原来的 API server 这个巨石（monolithic）应用给拆分成，为了方便用户开发自己的 API server 集成进来，而不用直接修改 kubernetes 官方仓库的代码，这样一来也能将 API server 解耦，方便用户使用实验特性。这些 API server 可以跟 core API server 无缝衔接，使用 kubectl 也可以管理它们。\n详情参考 Aggregated API Server。\n参考  Custom Resources - kubernetes.io Extend the Kubernetes API with CustomResourceDefinitions - kubernetes.io Introducing Operators: Putting Operational Knowledge into Software - coreos.com  ","date":1653062400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"a012b0780debd60116d3060c5525f6cd","permalink":"https://lib.jimmysong.io/kubernetes-handbook/extend/custom-resource/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/extend/custom-resource/","section":"kubernetes-handbook","summary":"自定义资源是对 Kubernetes API 的扩展，Kubernetes 中的每个资源都是一个 API 对象的集合，例如我们在 YAML 文件里定义的那些 spec 都是对 Kubernetes 中的资源对象的定义，所有的自定义资源可以跟 Kubernetes 中内建的资源一样使用 kubectl 操作。 自定义资源","tags":["Kubernetes"],"title":"使用自定义资源扩展 API","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"本文是如何创建 CRD 来扩展 Kubernetes API 的教程。CRD 是用来扩展 Kubernetes 最常用的方式，在 Service Mesh 和 Operator 中也被大量使用。因此读者如果想在 Kubernetes 上做扩展和开发的话，是十分有必要了解 CRD 的。\n在阅读本文前您需要先了解使用自定义资源扩展 API， 以下内容译自 Kubernetes 官方文档，有删改，推荐阅读如何从零开始编写一个 Kubernetes CRD。\n创建 CRD（CustomResourceDefinition） 创建新的 CustomResourceDefinition（CRD）时，Kubernetes API Server 会为您指定的每个版本创建新的 RESTful 资源路径。CRD 可以是命名空间的，也可以是集群范围的，可以在 CRD scope 字段中所指定。与现有的内置对象一样，删除命名空间会删除该命名空间中的所有自定义对象。CustomResourceDefinition 本身是非命名空间的，可供所有命名空间使用。\n参考下面的 CRD，将其配置保存在 resourcedefinition.yaml 文件中：\napiVersion:apiextensions.k8s.io/v1kind:CustomResourceDefinitionmetadata:# 名称必须符合下面的格式：\u0026lt;plural\u0026gt;.\u0026lt;group\u0026gt;name:crontabs.stable.example.comspec:# REST API使用的组名称：/apis/\u0026lt;group\u0026gt;/\u0026lt;version\u0026gt;group:stable.example.com# REST API使用的版本号：/apis/\u0026lt;group\u0026gt;/\u0026lt;version\u0026gt;versions:- name:v1# 可以通过 served 来开关每个 versionserved:true# 有且仅有一个 version 开启存储storage:trueschema:openAPIV3Schema:type:objectproperties:spec:type:objectproperties:cronSpec:type:stringimage:type:stringreplicas:type:integer# Namespaced或Clusterscope:Namespacednames:# URL中使用的复数名称: /apis/\u0026lt;group\u0026gt;/\u0026lt;version\u0026gt;/\u0026lt;plural\u0026gt;plural:crontabs# CLI中使用的单数名称singular:crontab# CamelCased格式的单数类型。在清单文件中使用kind:CronTab# CLI中使用的资源简称shortNames:- ct 此处使用的 apiVersion 版本是 apiextensions.k8s.io/v1，跟上一版 apiextensions.k8s.io/v1beta1 的最主要区别是改用了 OpenAPI v3.0 validation schema。\n 创建该 CRD：\nkubectl create -f resourcedefinition.yaml 访问 RESTful API 端点如 http://172.20.0.113:8080 将看到如下 API 端点已创建：\n/apis/stable.example.com/v1/namespaces/*/crontabs/... 然后，此端点 URL 可用于创建和管理自定义对象。上面的 CRD 中定义的类型就是 CronTab。\n可能需要几秒钟才能创建端点。您可以监控 CustomResourceDefinition 中 Established 的状态何时为 true，或者查看 API 资源的发现信息中是否显示了您的资源。\n创建自定义对象 创建 CustomResourceDefinition 对象后，您可以创建自定义对象。自定义对象可包含自定义字段。这些字段可以包含任意 JSON。在以下示例中， cronSpec 和 image 自定义字段在自定义对象中设置 CronTab。CronTab 类型来自您在上面创建的 CustomResourceDefinition 对象的规范。\n如果您将以下 YAML 保存到 my-crontab.yaml：\napiVersion:\u0026#34;stable.example.com/v1\u0026#34;kind:CronTabmetadata:name:my-new-cron-objectspec:cronSpec:\u0026#34;* * * * */5\u0026#34;image:my-awesome-cron-image并创建它：\nkubectl create -f my-crontab.yaml 然后，您可以使用 kubectl 管理 CronTab 对象。例如：\nkubectl get crontab 应该打印这样的列表：\nNAME AGE my-new-cron-object 6s 使用 kubectl 时，资源名称不区分大小写，您可以使用 CRD 中定义的单数或复数形式，以及任何短名称。\n您还可以查看原始 YAML 数据：\nkubectl get ct -o yaml 您应该看到它的 yaml 中的自定义 cronSpec 和 image 字段：\napiVersion:v1items:- apiVersion:stable.example.com/v1kind:CronTabmetadata:clusterName:\u0026#34;\u0026#34;creationTimestamp:2017-05-31T12:56:35ZdeletionGracePeriodSeconds:nulldeletionTimestamp:nullname:my-new-cron-objectnamespace:defaultresourceVersion:\u0026#34;285\u0026#34;selfLink:/apis/stable.example.com/v1/namespaces/default/crontabs/my-new-cron-objectuid:9423255b-4600-11e7-af6a-28d2447dc82bspec:cronSpec:\u0026#39;* * * * */5\u0026#39;image:my-awesome-cron-imagekind:Listmetadata:resourceVersion:\u0026#34;\u0026#34;selfLink:\u0026#34;\u0026#34;删除 CustomResourceDefinition 删除 CustomResourceDefinition 时，服务器将删除 RESTful API 端点并删除存储在其中的所有自定义对象。\nkubectl delete -f resourcedefinition.yaml kubectl get crontabs Error from server (NotFound): Unable to list \u0026#34;crontabs\u0026#34;: the server could not find the requested resource (get crontabs.stable.example.com) 如果稍后重新创建相同的 CustomResourceDefinition，它将从空开始。\n高级主题 Finalizer（终结器） Finalizer（终结器）允许控制器实现异步预删除 hook。自定义对象支持终结器，就像内置对象一样。\n您可以将终结器添加到自定义对象，如下所示：\napiVersion:\u0026#34;stable.example.com/v1\u0026#34;kind:CronTabmetadata:finalizers:- finalizer.stable.example.com终结器是任意字符串值，当存在时确保在资源存在时不可能进行硬删除。\n对具有终结器的对象的第一个删除请求设置该metadata.deletionTimestamp字段的值， 但不删除它。设置此值后，finalizer 只能删除列表中的条目。\n如果设置了 metadata.deletionTimestamp 字段，控制器监控对象将执行它们所有的终结器，对该对象轮询更新请求。执行完所有终结器后，将删除该资源。\n值metadata.deletionGracePeriodSeconds控制轮询更新之间的间隔。\n每个控制器都有责任从列表中删除其终结器。\n如果终结器列表为空，Kubernetes 最终只会删除该对象，这意味着所有终结器都已执行。\nValidation（验证） 功能状态： Kubernetes v1.12 beta\n可以通过 OpenAPI v3 schema验证自定义对象是否符合标准 。此外，以下限制适用于 schema：\n 字段default、nullable、discriminator、readOnly、writeOnly、xml、 deprecated 和 $ref 不能设置。 该字段 uniqueItems 不能设置为 true。 该字段 additionalProperties 不能设置为 false。  您可以使用 kube-apiserverCustomResourceValidation 上的功能门（feature gate）禁用此功能：\n--feature-gates=CustomResourceValidation=false 该 schema 在 CustomResourceDefinition 中定义。在以下示例中，CustomResourceDefinition 对自定义对象应用以下验证：\n spec.cronSpec 必须是字符串，并且必须是正则表达式描述的形式。 spec.replicas 必须是整数，且最小值必须为 1，最大值为 10。  将 CustomResourceDefinition 保存到 resourcedefinition.yaml：\napiVersion:apiextensions.k8s.io/v1beta1kind:CustomResourceDefinitionmetadata:name:crontabs.stable.example.comspec:group:stable.example.comversions:- name:v1served:truestorage:trueversion:v1scope:Namespacednames:plural:crontabssingular:crontabkind:CronTabshortNames:- ctvalidation:# openAPIV3Schema 适用于验证自定义对象的 schema。openAPIV3Schema:properties:spec:properties:cronSpec:type:stringpattern:\u0026#39;^(\\d+|\\*)(/\\d+)?(\\s+(\\d+|\\*)(/\\d+)?){4}$\u0026#39;replicas:type:integerminimum:1maximum:10并创建它：\nkubectl create -f resourcedefinition.yaml CronTab 如果其字段中存在无效值，则将拒绝创建类型的自定义对象的请求。在以下示例中，自定义对象包含具有无效值的字段：\n spec.cronSpec 与正则表达式不匹配。 spec.replicas 大于10。  如果您将以下YAML保存到my-crontab.yaml：\napiVersion:\u0026#34;stable.example.com/v1\u0026#34;kind:CronTabmetadata:name:my-new-cron-objectspec:cronSpec:\u0026#34;* * * *\u0026#34;image:my-awesome-cron-imagereplicas:15并创建它：\nkubectl create -f my-crontab.yaml 你会收到一个错误：\nThe …","date":1653062400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"e2eddd9ededdb67e5017732f82a05413","permalink":"https://lib.jimmysong.io/kubernetes-handbook/extend/crd/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/extend/crd/","section":"kubernetes-handbook","summary":"本文是如何创建 CRD 来扩展 Kubernetes API 的教程。CRD 是用来扩展 Kubernetes 最常用的方式，在 Service Mesh 和 Operator 中也被大量使用。因此读者如果想在 Kubernetes 上做扩展和开发的话，是十分有必要了解 CRD 的。 在阅读本文前您需要先了解使用自定义资源扩展 API","tags":["Kubernetes"],"title":"使用 CRD 扩展 Kubernetes API","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"Aggregated（聚合的）API server 是为了将原来的 API server 这个巨石（monolithic）应用给拆分成，为了方便用户开发自己的 API server 集成进来，而不用直接修改 kubernetes 官方仓库的代码，这样一来也能将 API server 解耦，方便用户使用实验特性。这些 API server 可以跟 core API server 无缝衔接，使用 kubectl 也可以管理它们。\n架构 我们需要创建一个新的组件，名为 kube-aggregator，它需要负责以下几件事：\n 提供用于注册 API server 的 API 汇总所有的 API server 信息 代理所有的客户端到 API server 的请求  注意：这里说的 API server 是一组 “API Server”，而不是说我们安装集群时候的那个 API server，而且这组 API server 是可以横向扩展的。\n安装配置聚合的 API server 有两种方式来启用 kube-aggregator：\n 使用 test mode/single-user mode，作为一个独立的进程来运行 使用 gateway mode，kube-apiserver 将嵌入到 kbe-aggregator 组件中，它将作为一个集群的 gateway，用来聚合所有 apiserver。  kube-aggregator 二进制文件已经包含在 Kubernetes release 里面了。\n","date":1653062400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"9cd2a76b1eb5c73a45271e20da8165b5","permalink":"https://lib.jimmysong.io/kubernetes-handbook/extend/aggregated-api-server/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/extend/aggregated-api-server/","section":"kubernetes-handbook","summary":"Aggregated（聚合的）API server 是为了将原来的 API server 这个巨石（monolithic）应用给拆分成，为了方便用户开发自己的 API server 集成进来，而不用直接修改 kubernetes 官方仓库的代码，这样一来也能将 API server 解耦，方便用","tags":["Kubernetes"],"title":"Aggregated API Server","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"APIService 是用来表示一个特定的 GroupVersion 的中的 server，它的结构定义位于代码 staging/src/k8s.io/kube-aggregator/pkg/apis/apiregistration/types.go 中。\n下面是一个 APIService 的示例配置：\napiVersion:apiregistration.k8s.io/v1beta1kind:APIServicemetadata:name:v1alpha1.custom-metrics.metrics.k8s.iospec:insecureSkipTLSVerify:truegroup:custom-metrics.metrics.k8s.iogroupPriorityMinimum:1000versionPriority:5service:name:apinamespace:custom-metricsversion:v1alpha1APIService 详解 使用 apiregistration.k8s.io/v1beta1 版本的 APIService，在 metadata.name 中定义该 API 的名字。\n使用上面的 yaml 的创建 v1alpha1.custom-metrics.metrics.k8s.io APIService。\n insecureSkipTLSVerify：当与该服务通信时，禁用 TLS 证书认证。强加建议不要设置这个参数，默认为 false。应该使用 CABundle 代替。 service：与该 APIService 通信时引用的 service，其中要注明 service 的名字和所属的 namespace，如果为空的话，则所有的服务都会该 API groupversion 将在本地 443 端口处理所有通信。 groupPriorityMinimum：该组 API 的处理优先级，主要排序是基于 groupPriorityMinimum，该数字越大表明优先级越高，客户端就会与其通信处理请求。次要排序是基于字母表顺序，例如 v1.bar 比 v1.foo 的优先级更高。 versionPriority：VersionPriority 控制其组内的 API 版本的顺序。必须大于零。主要排序基于 VersionPriority，从最高到最低（20 大于 10）排序。次要排序是基于对象名称的字母比较。 （v1.foo 在 v1.bar 之前）由于它们都是在一个组内，因此数字可能很小，一般都小于 10。  查看我们使用上面的 yaml 文件创建的 APIService。\nkubectl get apiservice v1alpha1.custom-metrics.metrics.k8s.io -o yaml ​``````yaml apiVersion: apiregistration.k8s.io/v1beta1 kind: APIService metadata: creationTimestamp: 2017-12-14T08:27:35Z name: v1alpha1.custom-metrics.metrics.k8s.io resourceVersion: \u0026#34;35194598\u0026#34; selfLink: /apis/apiregistration.k8s.io/v1beta1/apiservices/v1alpha1.custom-metrics.metrics.k8s.io uid: a31a3412-e0a8-11e7-9fa4-f4e9d49f8ed0 spec: caBundle: null group: custom-metrics.metrics.k8s.io groupPriorityMinimum: 1000 insecureSkipTLSVerify: true service: name: api namespace: custom-metrics version: v1alpha1 versionPriority: 5 status: conditions: - lastTransitionTime: 2017-12-14T08:27:38Z message: all checks passed reason: Passed status: \u0026#34;True\u0026#34; type: Available 查看集群支持的 APISerivce 作为 Kubernetes 中的一种资源对象，可以使用 kubectl get apiservice 来查看。\n例如查看集群中所有的 APIService：\n$ kubectl get apiservice NAME AGE v1. 2d v1.authentication.k8s.io 2d v1.authorization.k8s.io 2d v1.autoscaling 2d v1.batch 2d v1.monitoring.coreos.com 1d v1.networking.k8s.io 2d v1.rbac.authorization.k8s.io 2d v1.storage.k8s.io 2d v1alpha1.custom-metrics.metrics.k8s.io 2h v1beta1.apiextensions.k8s.io 2d v1beta1.apps 2d v1beta1.authentication.k8s.io 2d v1beta1.authorization.k8s.io 2d v1beta1.batch 2d v1beta1.certificates.k8s.io 2d v1beta1.extensions 2d v1beta1.policy 2d v1beta1.rbac.authorization.k8s.io 2d v1beta1.storage.k8s.io 2d v1beta2.apps 2d v2beta1.autoscaling 2d 另外查看当前 kubernetes 集群支持的 API 版本还可以使用kubectl api-versions：\n$ kubectl api-versions apiextensions.k8s.io/v1beta1 apiregistration.k8s.io/v1beta1 apps/v1beta1 apps/v1beta2 authentication.k8s.io/v1 authentication.k8s.io/v1beta1 authorization.k8s.io/v1 authorization.k8s.io/v1beta1 autoscaling/v1 autoscaling/v2beta1 batch/v1 batch/v1beta1 certificates.k8s.io/v1beta1 custom-metrics.metrics.k8s.io/v1alpha1 extensions/v1beta1 monitoring.coreos.com/v1 networking.k8s.io/v1 policy/v1beta1 rbac.authorization.k8s.io/v1 rbac.authorization.k8s.io/v1beta1 storage.k8s.io/v1 storage.k8s.io/v1beta1 v1 ","date":1653062400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"75d22df4ba283c61f56e5fba8482ef86","permalink":"https://lib.jimmysong.io/kubernetes-handbook/extend/apiservice/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/extend/apiservice/","section":"kubernetes-handbook","summary":"APIService 是用来表示一个特定的 GroupVersion 的中的 server，它的结构定义位于代码 staging/src/k8s.io/kube-aggregator/pkg/apis/apiregistration/types.go 中。 下面是一个 APIService 的示例配置： apiVersion:apiregistration.k8s.io/v1beta1kind:APIServicemetadata:name:v1alpha1.custom-metrics.metrics.k8s.iospec:insecureSkipTLSVerify:truegroup:custom-metrics.metrics.k8s.iogroupPriorityMinimum:1000versionPriority:5service:name:apinamespace:custom-metricsversion:v1alpha1APIService 详解 使用 apiregistration.k8s.io/v1beta1 版本的 APIService，在 metadata.name 中定义该 API 的名字。 使用上面的 yaml 的创建 v1alpha1.custom-metrics.metrics.k8s.io APIService。 i","tags":["Kubernetes"],"title":"APIService","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"服务目录（Service Catalog）是 Kubernetes 的扩展 API，它使运行在 Kubernetes 集群中的应用程序可以轻松使用外部托管软件产品，例如由云提供商提供的数据存储服务。\n它提供列表清单、提供 (provision) 和绑定 (binding) 来自服务代理（Service Brokers）的外部托管服务，而不需要关心如何创建或管理这些服务的详细情况。\n由 Open Service Broker API 规范定义的 Service broker 是由第三方提供和维护的一组托管服务的端点 (endpoint)，该第三方可以是 AWS，GCP 或 Azure 等云提供商。\n托管服务可以是 Microsoft Azure Cloud Queue，Amazon Simple Queue Service 和 Google Cloud Pub/Sub 等，它们可以是应用可以使用的提供的各种软件。\n通过 Service Catalog，集群运营者可以浏览由 Service Broker 提供的托管服务列表，提供的托管服务实例，并与其绑定，使其可被 Kubernetes 集群中的应用程序所使用。\n场景样例 应用程序开发人员编写基于 Kubernetes 集群的应用程序，他们希望使用消息队列作为其应用程序的一部分。但是，他们不想自己配置和管理这个服务服务。恰好，有一家云提供商通过其服务代理 (Service Broker) 提供消息队列服务。\n集群运营商可以设置 Service Catalog 并使用它与云提供商的 Service Broker 进行通信，以调配消息排队服务的实例并使其可用于 Kubernetes 集群内的应用程序。因此，应用程序开发人员不需要关心消息队列的实现细节或管理，可以简单地像服务一样使用它。\n架构 Service Catalog 使用 Open Service Broker API 与 Service Broker 进行通信，充当 Kubernetes API 服务器的中介，发起供应并返回应用程序使用托管服务所需的凭据。\nService Catalog 通过扩展 API 服务器和控制器实现，使用 etcd 进行存储。它还使用 Kubernetes 1.7 + 中提供的聚合层来呈现其 API。\n   Service Catalog 架构  API 资源 Service Catalog 安装 servicecatalog.k8s.ioAPI 并提供以以下 Kubernetes 资源：\n ClusterServiceBroker：作为 service broker 的群集内代理，封装其服务器连接详细信息。这些由集群运营者创建和管理，希望使用 broker 服务在其集群中提供新类型的托管服务。 ClusterServiceClass：由特定 service broker 提供的托管服务。将新 ClusterServiceBroker 资源添加到群集时，Service catalog controller 将连接到 service broker 以获取可用托管服务的列表清单。然后它会创建新的 ClusterServiceClass 资源，与每个托管服务相对应。 ClusterServicePlan：托管服务的特定产品。例如，托管服务可能有不同的可用套餐，例如免费套餐或付费套餐，或者可能有不同的配置选项，例如使用 SSD 存储或拥有更多资源。同向群集添加 ClusterServiceClass 一样，当添加一个新的 ClusterServiceBroker 时，Service Catalog 会创建一个新的 ClusterServicePlan 资源，与每个托管服务可用的每个服务套餐对应。 ServiceInstance：一个提供好的 ClusterServiceClass 实例。这些是由集群运营者创建的托管服务的特定实例，供一个或多个集群内应用程序使用。当创建一个新的 ServiceInstance 资源时，Service Catalog controller 连接到相应的服务代理并指示它提供服务实例。 ServiceBinding：访问 ServiceInstance 的凭据。由想让他们的应用利用 ServiceInstance 的集群集运营者创建。创建之后，Service Catalog controller 将创建一个与此服务实例对应的 Kubernetes 的 Secret，包含此服务实例的连接详细信息和凭证 ，可以挂载到 Pod 中。  鉴权认证 Service Catalog 支持这些认证方法：\n Basic (username/password) OAuth 2.0 Bearer Token  用法 群集运营者可以使用 Service Catalog API 资源来提供托管服务，并使其在 Kubernetes 群集中可用。涉及的步骤是：\n 列出 Service Broker 提供的托管服务清单和服务套餐。 提供托管服务的新实例。 绑定到托管服务，该服务返回连接凭证。 将连接凭证映射到应用程序中。  列出托管服务和服务组 首先，群集运营者必须在 servicecatalog.k8s.io 群组内创建 ClusterServiceBroker 资源。此资源包含访问服务代理端点所需的 URL 和连接详细信息。\n这是一个 ClusterServiceBroker 资源的例子：\napiVersion:servicecatalog.k8s.io/v1beta1kind:ClusterServiceBrokermetadata:name:cloud-brokerspec:# Points to the endpoint of a service broker. (This example is not a working URL.)url:https://servicebroker.somecloudprovider.com/v1alpha1/projects/service-catalog/brokers/default###### Additional values can be added here, which may be used to communicate# with the service broker, such as bearer token info or a caBundle for TLS.#####以下是说明从一个 service broker 列出托管服务和套餐所涉及步骤的顺序图：\n   列出服务    将 ClusterServiceBroker 资源添加到 Service catalog 中，它会触发对外部 Service Broker 的调用以获取可用服务的清单。\n  Service Broker 返回可用托管服务的清单和服务套餐的列表，它们分别在本地缓存为 ClusterServiceClass 资源和 ClusterServicePlan 资源。\n  然后，集群运营者可以使用以下命令获取可用托管服务的清单：\nkubectl get clusterserviceclasses -o=custom-columns=SERVICE\\ NAME:.metadata.name,EXTERNAL\\ NAME:.spec.externalName 它应该输出一个类似于以下格式的服务名称列表：\nSERVICE NAME EXTERNAL NAME 4f6e6cf6-ffdd-425f-a2c7-3c9258ad2468 cloud-provider-service ... ... 他们还可以使用以下命令查看可用的服务套餐：\nkubectl get clusterserviceplans -o=custom-columns=PLAN\\ NAME:.metadata.name,EXTERNAL\\ NAME:.spec.externalName 它应该输出一个类似于以下格式的套餐名称列表：\nPLAN NAME EXTERNAL NAME 86064792-7ea2-467b-af93-ac9694d96d52 service-plan-name ... ...   提供新的实例 集群运营者可以通过创建 ServiceInstance 资源来启动新实例的供应。\n如下是一个 ServiceInstance 资源的例子：\napiVersion:servicecatalog.k8s.io/v1beta1kind:ServiceInstancemetadata:name:cloud-queue-instancenamespace:cloud-appsspec:# References one of the previously returned servicesclusterServiceClassExternalName:cloud-provider-serviceclusterServicePlanExternalName:service-plan-name###### Additional parameters can be added here,# which may be used by the service broker.#####以下序列图说明了提供一个新的托管服务的实例所涉及的步骤：\n   启用一个服务   当 ServiceInstance 资源创建后，Service Catalog 发起到外部 service broker 来提供服务的一个实例。 service broker 创建托管服务的新实例并返回 HTTP 响应。 然后，群集运营者可以检查实例的状态，来确认它是否准备就绪。  绑定到托管服务 在提供新实例后，群集运营者必须绑定到托管服务才能获取到应用程序使用服务所需的连接凭证和服务帐户详细信息。这是通过创建 ServiceBinding 资源完成的。\n以下是一个 ServiceBinding 资源的例子：\napiVersion:servicecatalog.k8s.io/v1beta1kind:ServiceBindingmetadata:name:cloud-queue-bindingnamespace:cloud-appsspec:instanceRef:name:cloud-queue-instance###### Additional information can be added here, such as a secretName or# service account parameters, which may be used by the service broker.#####以下序列图说明了绑定到托管服务实例所涉及的步骤：\n   绑定到托管服务    在ServiceBinding创建后，Service Catalog给外部service broker发一个调用请求，获取与服务实例绑定所需的信息。\n  service broker为相应的服务帐户启用应用程序权限/角色。\n  service broker返回连接和访问托管服务实例所需的信息。根据不同的提供商和不同的服务，返回的信息可能在服务提供商和其管理服务之间有所不同。\n  映射连接凭证 绑定后，最后一步是将连接凭证和服务特定的信息映射到应用程序中。这些信息存储在secret中，应用程序可以用来访问并与托管服务连接。\n   映射连接凭证  Pod 配置文件 执行此映射的一种方法是使用声明式 Pod 配置文件。\n以下示例描述了如何将服务帐户凭证映射到应用程序中。被调用的 sa-key 密钥存储在名为 provider-cloud-key 的卷中，并且应用程序将此卷挂载到 /var/secrets/provider/key.json。 …","date":1653062400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"3f37521d50fcfe9b4908e7a2703e9ba6","permalink":"https://lib.jimmysong.io/kubernetes-handbook/extend/service-catalog/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/extend/service-catalog/","section":"kubernetes-handbook","summary":"服务目录（Service Catalog）是 Kubernetes 的扩展 API，它使运行在 Kubernetes 集群中的应用程序可以轻松使用外部托管软件产品，例如由云提供商提供的数据存储服务。 它提供列表清单、提供 (provision) 和绑定 (binding) 来自服务代理（Ser","tags":["Kubernetes"],"title":"服务目录（Service Catalog）","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"2020 年初，Kubernetes 社区提议 Multi-Cluster Services API，旨在解决长久以来就存在的 Kubernetes 多集群服务管理问题。\nKubernetes 用户可能希望将他们的部署分成多个集群，但仍然保留在这些集群中运行的工作负载之间的相互依赖关系，这有很多原因。今天，集群是一个硬边界，一个服务对远程的 Kubernetes 消费者来说是不透明的，否则就可以利用元数据（如端点拓扑结构）来更好地引导流量。为了支持故障转移或在迁移过程中的临时性，用户可能希望消费分布在各集群中的服务，但今天这需要非复杂的定制解决方案。\n多集群服务 API 旨在解决这些问题。\n参考  KEP-1645: Multi-Cluster Services API - github.com  ","date":1653062400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"e9ed381d053b9be6da2704161d1c9f22","permalink":"https://lib.jimmysong.io/kubernetes-handbook/multi-cluster/multi-cluster-services-api/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/multi-cluster/multi-cluster-services-api/","section":"kubernetes-handbook","summary":"2020 年初，Kubernetes 社区提议 Multi-Cluster Services API，旨在解决长久以来就存在的 Kubernetes 多集群服务管理问题。 Kubernetes 用户可能希望将他们的部署分成多个集群，但仍然保留在这些集群中运行的工作负载之间的相互依赖关系，这有很多原","tags":["Kubernetes"],"title":"多集群服务 API（Multi-Cluster Services API）","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"Kubernetes 从 1.8 版本起就声称单集群最多可支持 5000 个节点和 15 万个 Pod，我相信很少有公司会部署如此庞大的一个单集群，总有很多情况下因为各种各样的原因我们可能会部署多个集群，但是有时候有想将他们统一起来管理，这时候就需要用到集群联邦（Federation）。\n为什么要使用集群联邦 Federation 使管理多个集群变得简单。它通过提供两个主要构建模块来实现：\n 跨集群同步资源：Federation 提供了在多个集群中保持资源同步的能力。例如，可以保证同一个 deployment 在多个集群中存在。 跨集群服务发现：Federation 提供了自动配置 DNS 服务以及在所有集群后端上进行负载均衡的能力。例如，可以提供一个全局 VIP 或者 DNS 记录，通过它可以访问多个集群后端。  Federation 还可以提供一些其它用例：\n 高可用：通过在集群间分布负载并自动配置 DNS 服务和负载均衡，federation 最大限度地减少集群故障的影响。 避免厂商锁定：通过更简单的跨集群应用迁移方式，federation 可以防止集群厂商锁定。  Federation 对于单个集群没有用处。基于下面这些原因你可能会需要多个集群：\n 低延迟：通过在多个区域部署集群可以最大限度减少区域近端用户的延迟。 故障隔离：拥有多个小集群可能比单个大集群更利于故障隔离（例如：在云服务提供商的不同可用区中的多个集群）。 可伸缩性：单个集群有可伸缩性限制（对于大多数用户这不是典型场景。更多细节请参考 Kubernetes 弹性伸缩与性能目标）。 混合云：你可以在不同的云服务提供商或本地数据中心中拥有多个集群。  警告 虽然 federation 有很多吸引人的使用案例，但也有一些注意事项：\n 增加网络带宽和成本：federation 控制平面监控所有集群以确保当前状态符合预期。如果集群在云服务提供商的不同区域或者不同的云服务提供商上运行时，这将导致明显的网络成本增加。 减少跨集群隔离：federation 控制平面中的 bug 可能影响所有集群。通过在 federation 中实现最少的逻辑可以缓解这种情况。只要有可能，它就将尽力把工作委托给 kubernetes 集群中的控制平面。这种设计和实现在安全性及避免多集群停止运行上也是错误的。 成熟度：federation 项目相对比较新，还不是很成熟。并不是所有资源都可用，许多仍然处于 alpha 状态。 Issue 88 列举了团队目前正忙于解决的与系统相关的已知问题。  混合云能力 Kubernetes 集群 federation 可以包含运行在不同云服务提供商（例如 Google Cloud、AWS）及本地（例如在 OpenStack 上）的集群。你只需要按需在适合的云服务提供商和 / 或地点上简单的创建集群，然后向 Federation API Server 注册每个集群的 API endpoint 和凭据即可。\n此后，你的 API 资源就可以跨越不同的集群和云服务提供商。\n单个集群范围 在诸如 Google Compute Engine 或者 Amazon Web Services 等 IaaS 服务提供商中，虚拟机存在于 区域（Zone） 或 可用区（Availability Zone） 上。我们建议 Kubernetes 集群中的所有虚拟机应该位于相同的可用区，因为：\n 与拥有单个全局 Kubernetes 集群相比，单点故障更少。 与跨可用区集群相比，推测单区域集群的可用性属性更容易。 当 Kubernetes 开发人员设计系统时（例如对延迟，带宽或相关故障进行假设），它们假设所有的机器都在一个单一的数据中心，或以其它方式紧密连接。  在每个可用区域同时拥有多个集群也是可以的，但总体而言，我们认为少一点更好。 选择较少集群的理由是：\n 某些情况下，在一个集群中拥有更多的节点可以改进 Pod 的装箱打包（较少资源碎片）。 减少运维开销（尽管随着运维工具和流程的成熟，优势已经减少）。 减少每个集群固定资源成本的开销，例如 apiserver 虚拟机（但对于大中型集群的整体集群成本来说百分比很小）。  拥有多个集群的原因包括：\n 严格的安全策略要求将一类工作与另一类工作隔离开来（但是，请参见下面的分区集群（Partitioning Clusters））。 对新的 Kubernetes 发行版或其它集群软件进行灰度测试。  选择正确的集群数量 Kubernetes 集群数量的选择可能是一个相对静态的选择，只是偶尔重新设置。相比之下，依据负载情况和增长，集群的节点数量和 service 的 pod 数量可能会经常变化。\n要选择集群的数量，首先要确定使用哪些区域，以便为所有终端用户提供足够低的延迟，以在 Kubernetes 上运行服务，如果你使用内容分发网络（Content Distribution Network，CDN），则 CDN 托管内容的时延要求不需要考虑。法律问题也可能影响到这一点。例如，拥有全球客户群的公司可能会决定在美国、欧盟、亚太和南亚地区拥有集群。我们将选择的区域数量称为 R。\n其次，决定在整体仍然可用的前提下，可以同时有多少集群不可用。将不可用集群的数量称为 U。如果你不能确定，那么 1 是一个不错的选择。\n如果在集群故障的情形下允许负载均衡将流量引导到任何区域，则至少需要有比 R 或 U + 1 数量更大的集群。如果不行的话（例如希望在集群发生故障时对所有用户确保低延迟），那么你需要有数量为 R * (U + 1) 的集群（R 个区域，每个中有 U + 1 个集群）。无论如何，请尝试将每个集群放在不同的区域中。\n最后，如果你的任何集群需要比 Kubernetes 集群最大建议节点数更多的节点，那么你可能需要更多的集群。 Kubernetes v1.3 支持最多 1000 个节点的集群。 Kubernetes v1.8 支持最多 5000 个节点的集群。\nKubernetes 集群联邦的演进 Kubernetes 官方博客的文章中介绍了 Kubernetes 集群联邦的演进，该项目是在 SIG Multicluster 中进行的，Federation 是 Kubernetes 的一个子项目，社区对这个项目的兴趣很浓，该项目最初重用 Kubernetes API，以消除现有 Kubernetes 用户的任何附加使用复杂性。但由于以下原因，此方式行不通：\n 在集群层面重新实施 Kubernetes API 的困难，因为 Federation 的特定扩展存储在注释中。 由于 Kubernetes API 的1:1仿真，Federation 类型、放置（placement）和调节（reconciliation）的灵活性有限。 没有固定的 GA 路径，API 成熟度普遍混乱；例如，Deployment 在Kubernetes 中是 GA，但在 Federation v1 中甚至不是Beta。  随着 Federation 特定的 API 架构和社区的努力，这些想法有了进一步的发展，改进为 Federation v2。请注意，Federation V1 版本已经归档不再维护和更新，且官方也不再推荐继续使用。如果需要了解更多的 Federation 资料，请参考：Kubernetes Federation v2。\n下面将带你了解 Federation v2 背后的考量。\n将任意资源联合起来 联邦的主要目标之一是能够定义 API 和 API 组，其中包含联邦任何给定 Kubernetes 资源所需的基本原则。这是至关重要的，因为 CRD 已经成为扩展 Kubernetes API 的一种主流方式。\nMulticluster SIG 得出了 Federation API 和 API 组的共同定义，即 “一种将规范的 Kubernetes API 资源分配到不同集群的机制”。最简单的分布形式可以想象为这种’规范的 Kubernetes API 资源’在联邦集群中的简单传播。除了这种简单的 Kubernetes 资源传播之外，有心的读者当然可以看出更复杂的机制。\n在定义 Federation API 的构建模块的历程中，最早期的目标也演化为 “能够创建一个简单的联邦，也就是任何 Kubernetes 资源或 CRD 的简单传播，几乎不需要编写代码”。随后，核心 API 组进一步定义了构件，即每个给定的 Kubernetes 资源有一个 Template 资源、一个 Placement 资源和一个 Override 资源，一个 TypeConfig 来指定给定资源的同步或不同步，以及执行同步的相关控制器。更多细节将在后文中介绍。进一步的章节还将谈到能够遵循分层行为，更高级别的 Federation API 消耗这些核心构件的行为，而用户能够消耗整个或部分 API 和相关控制器。最后，这种架构还允许用户编写额外的控制器或用自己的控制器替换现有的参考控制器（reference controller），以执行所需的行为。\n能够 “轻松地联合任意 Kubernetes 资源”，以及一个解耦的 API，分为构件 API、更高层次的 API 和可能的用户预期类型，这样的呈现方式使得不同的用户可以消费部分和编写控制器组成特定的解决方案，这为 Federation v2 提供了一个令人信服的案例。\n联邦服务与跨集群服务发现 Kubernetes 服务在构建微服务架构时非常有用。人们明显希望跨越集群、可用区、区域和云的边界来部署服务。跨集群的服务提供了地理分布，实现了混合和多云场景，并提高了超越单一集群部署的高可用性水平。希望其服务跨越一个或多个（可能是远程）集群的客户，需要在集群内外以一致的方式提供服务。\nFederated Service 的核心是包含一个 Template（Kubernetes服务的定义）、一个 Placement（部署到哪个集群）、一个 Override（在特定集群中的可选变化）和一个 ServiceDNSRecord（指定如何发现它的细节）。\n注意：联邦服务必须是 LoadBalancer 类型，以便它可以跨集群发现。\nPod 如何发现联邦服务 默认情况下，Kubernetes 集群预先配置了集群本地 DNS 服务器，以及智能构建的 DNS 搜索路径，它们共同确保了由 pod 内部运行的软件发出的 myservice、myservice.mynamespace 或 some-other-service.other-namespace 等 DNS 查询会自动扩展并正确解析到本地集群中运行的服务的相应 IP。\n随着联邦服务和跨集群服务发现的引入，这个概念被扩展到全局覆盖在你的集群联邦中所有集群中运行的 Kubernetes 服务。为了利用这个扩展的范围，我们需要使用一个稍微不同的 DNS 名称（例如 myservice.mynamespace.myfederation）来解析联邦服务。使用不同的 DNS 名还可以避免现有的应用意外地穿越区域网络而会产生不必要的网络费用或延迟。\n让我们看一个例子，使用一个名为 nginx 的服务。\n在 us-central1-a 可用区的集群中的一个 pod 需要联系我们的 nginx 服务。它现在可以使用服务的联邦 DNS 名，即 nginx.mynamespace.myfederation ，而不是使用服务的传统集群本地 DNS 名（即 nginx.mynamespace 自动扩展为 nginx.mynamespace.svc.cluster.local）。这将被自动扩展并解析到离我的 nginx 服务最近的健康 shard。如果本地集群中存在一个健康的 shard，那么该服务的集群本地 IP 地址将被返回（通过集群本地 DNS）。这完全等同于非联邦服务解析。\n如果服务在本地集群中不存在（ …","date":1653062400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"03d3ed1ca610190ae65edffb2ac88564","permalink":"https://lib.jimmysong.io/kubernetes-handbook/multi-cluster/federation/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/multi-cluster/federation/","section":"kubernetes-handbook","summary":"Kubernetes 从 1.8 版本起就声称单集群最多可支持 5000 个节点和 15 万个 Pod，我相信很少有公司会部署如此庞大的一个单集群，总有很多情况下因为各种各样的原因我们可能会部署多个集群，但是有时候有想将他们统一起来管理，这时候就","tags":["Kubernetes"],"title":"集群联邦（Cluster Federation）","type":"book"},{"authors":null,"categories":["Istio"],"content":"EDS 只是 Envoy 中众多的服务发现方式的一种。要想了解 EDS 首先我们需要先知道什么是 Endpoint。\nEndpoint\nEndpoint 即上游主机标识。它的数据结构如下：\n{ \u0026#34;address\u0026#34;: \u0026#34;{...}\u0026#34;, \u0026#34;health_check_config\u0026#34;: \u0026#34;{...}\u0026#34; } 其中包括端点的地址和健康检查配置。详情请参考 Endpoints。\n终端发现服务（EDS）是一个基于 gRPC 或 REST-JSON API 服务器的 xDS 管理服务，在 Envoy 中用来获取集群成员。集群成员在 Envoy 的术语中被称为“终端”。对于每个集群，Envoy 都会通过发现服务来获取成员的终端。由于以下几个原因，EDS 是首选的服务发现机制：\n Envoy 对每个上游主机都有明确的了解（与通过 DNS 解析的负载均衡进行路由相比而言），并可以做出更智能的负载均衡决策。 在每个主机的发现 API 响应中携带的额外属性通知 Envoy 负载均衡权重、金丝雀状态、区域等。这些附加属性在负载均衡、统计信息收集等过程中会被 Envoy 网格全局使用。  Envoy 提供了 Java 和 Go 语言版本的 EDS 和其他发现服务的参考 gRPC 实现。\n通常，主动健康检查与最终一致的服务发现服务数据结合使用，以进行负载均衡和路由决策。\n参考  服务发现 - cloudnative.to  ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"2e51e17d8b6e89a1785330fc9902e5c6","permalink":"https://lib.jimmysong.io/istio-handbook/data-plane/envoy-eds/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/istio-handbook/data-plane/envoy-eds/","section":"istio-handbook","summary":"EDS 只是 Envoy 中众多的服务发现方式的一种。要想了解 EDS 首先我们需要先知道什么是 Endpoint。 Endpoint Endpoint 即上游主机标识。它的数据结构如下： { \"address\": \"{...}\", \"health_check_config\": \"{...}\" } 其中包括端点的地址和健康检查配置。详情请参考 Endpoints","tags":["Istio","Service Mesh"],"title":"EDS（端点发现服务）","type":"book"},{"authors":null,"categories":["Istio"],"content":"在 Pod 中的 init 容器启动之后，就向 Pod 中增加了 iptables 规则，本文将向你介绍 Istio 中的基于 iptables 的透明流量劫持的全过程。\n查看 iptables nat 表中注入的规则 Init 容器通过向 iptables nat 表中注入转发规则来劫持流量的，下图显示的是 productpage 服务中的 iptables 流量劫持的详细过程。\n   Envoy sidecar 流量劫持流程示意图  Init 容器启动时命令行参数中指定了 REDIRECT 模式，因此只创建了 NAT 表规则，接下来我们查看下 NAT 表中创建的规则，这是全文中的重点部分，前面讲了那么多都是为它做铺垫的。\nproductpage 访问 reviews Pod，入站流量处理过程对应于图示上的步骤：1、2、3、4、Envoy Inbound Handler、5、6、7、8、应用容器。\nreviews Pod 访问 rating 服务的出站流量处理过程对应于图示上的步骤是：9、10、11、12、Envoy Outbound Handler、13、14、15。\n上图中关于流量路由部分，包含：\n productpage 服务请求访问 http://reviews.default.svc.cluster.local:9080/，当流量进入 reviews Pod 内部时，流量是如何被 iptables 劫持到 Envoy 代理被 Inbound Handler 处理的； reviews 请求访问 ratings 服务的 Pod，应用程序发出的出站流量被 iptables 劫持到 Envoy 代理的 Outbound Handler 的处理。  在阅读下文时，请大家确立以下已知点：\n 首先，productpage 发出的对 reivews 的访问流量，是在 Envoy 已经通过 EDS 选择出了要请求的 reviews 服务的某个 Pod，知晓了其 IP 地址，直接向该 IP 发送的 TCP 连接请求。 reviews 服务有三个版本，每个版本有一个实例，三个版本中的 sidecar 工作步骤类似，下文只以其中一个 Pod 中的 sidecar 流量转发步骤来说明。 所有进入 reviews Pod 的 TCP 流量都根据 Pod 中的 iptables 规则转发到了 Envoy 代理的 15006 端口，然后经过 Envoy 的处理确定转发给 Pod 内的应用容器还是透传。  iptables 规则注入解析 为了查看 iptables 配置，我们需要登陆到 sidecar 容器中使用 root 用户来查看，因为 kubectl 无法使用特权模式来远程操作 docker 容器，所以我们需要登陆到 productpage pod 所在的主机上使用 docker 命令登陆容器中查看。\n如果您使用 minikube 部署的 Kubernetes，可以直接登录到 minikube 的虚拟机中并切换为 root 用户。查看 iptables 配置，列出 NAT（网络地址转换）表的所有规则，因为在 Init 容器启动的时候选择给 istio-iptables 传递的参数中指定将入站流量重定向到 sidecar 的模式为 REDIRECT，因此在 iptables 中将只有 NAT 表的规格配置，如果选择 TPROXY 还会有 mangle 表配置。iptables 命令的详细用法请参考 iptables 命令。\n我们仅查看与 productpage 有关的 iptables 规则如下，因为这些规则是运行在该容器特定的网络空间下，因此需要使用 nsenter 命令进入其网络空间。进入的时候需要指定进程 ID（PID），因此首先我们需要找到 productpage 容器的 PID。对于在不同平台上安装的 Kubernetes，查找容器的方式会略有不同，例如在 GKE 上，执行 docker ps -a 命令是查看不到任何容器进程的。下面已 minikube 和 GKE 两个典型的平台为例，指导你如何进入容器的网络空间。\n在 minikube 中查看容器中的 iptabes 规则 对于 minikube，因为所有的进程都运行在单个节点上，因此你只需要登录到 minikube 虚拟机，切换为 root 用户然后查找 productpage 进程即可，参考下面的步骤。\n# 进入 minikube 并切换为 root 用户，minikube 默认用户为 docker $ minikube ssh $ sudo -i # 查看 productpage pod 的 istio-proxy 容器中的进程 $ docker top `docker ps|grep \u0026#34;istio-proxy_productpage\u0026#34;|cut -d \u0026#34; \u0026#34; -f1` UID PID PPID C STIME TTY TIME CMD 1337 10576 10517 0 08:09 ? 00:00:07 /usr/local/bin/pilot-agent proxy sidecar --domain default.svc.cluster.local --configPath /etc/istio/proxy --binaryPath /usr/local/bin/envoy --serviceCluster productpage.default --drainDuration 45s --parentShutdownDuration 1m0s --discoveryAddress istiod.istio-system.svc:15012 --zipkinAddress zipkin.istio-system:9411 --proxyLogLevel=warning --proxyComponentLogLevel=misc:error --connectTimeout 10s --proxyAdminPort 15000 --concurrency 2 --controlPlaneAuthPolicy NONE --dnsRefreshRate 300s --statusPort 15020 --trust-domain=cluster.local --controlPlaneBootstrap=false 1337 10660 10576 0 08:09 ? 00:00:33 /usr/local/bin/envoy -c /etc/istio/proxy/envoy-rev0.json --restart-epoch 0 --drain-time-s 45 --parent-shutdown-time-s 60 --service-cluster productpage.default --service-node sidecar~172.17.0.16~productpage-v1-7f44c4d57c-ksf9b.default~default.svc.cluster.local --max-obj-name-len 189 --local-address-ip-version v4 --log-format [Envoy (Epoch 0)] [%Y-%m-%d %T.%e][%t][%l][%n] %v -l warning --component-log-level misc:error --concurrency 2 # 使用 nsenter 进入 sidecar 容器的命名空间（以上任何一个都可以） $ nsenter -n --target 10660 # 查看 NAT 表中规则配置的详细信息。 $ iptables -t nat -L 在 GKE 中查看容器的 iptables 规则 如果你在 GKE 中安装的多节点的 Kubernetes 集群，首先你需要确定这个 Pod 运行在哪个节点上，然后登陆到那台主机，使用下面的命令查找进程的 PID，你会得到类似下面的输出。\n$ ps aux|grep \u0026#34;productpage\u0026#34; chronos 4268 0.0 0.6 43796 24856 ? Ss Apr22 0:00 python productpage.py 9080 chronos 4329 0.9 0.6 117524 24616 ? Sl Apr22 13:43 /usr/local/bin/python /opt/microservices/productpage.py 9080 root 361903 0.0 0.0 4536 812 pts/0 S+ 01:54 0:00 grep --colour=auto productpage 然后在终端中输出 iptables -t nat -L 即可查看 iptables 规则。\niptables 流量劫持过程详解 经过上面的步骤，你已经可以查看到 init 容器向 Pod 中注入的 iptables 规则，如下所示。\n# PREROUTING 链：用于目标地址转换（DNAT），将所有入站 TCP 流量跳转到 ISTIO_INBOUND 链上。 Chain PREROUTING (policy ACCEPT 2701 packets, 162K bytes) pkts bytes target prot opt in out source destination 2701 162K ISTIO_INBOUND tcp -- any any anywhere anywhere # INPUT 链：处理输入数据包，非 TCP 流量将继续 OUTPUT 链。 Chain INPUT (policy ACCEPT 2701 packets, 162K bytes) pkts bytes target prot opt in out source destination # OUTPUT 链：将所有出站数据包跳转到 ISTIO_OUTPUT 链上。 Chain OUTPUT (policy ACCEPT 79 packets, 6761 bytes) pkts bytes target prot opt in out source destination 15 900 ISTIO_OUTPUT tcp -- any any anywhere anywhere # POSTROUTING 链：所有数据包流出网卡时都要先进入 POSTROUTING 链，内核根据数据包目的地判断是否需要转发出去，我们看到此处未做任何处理。 Chain POSTROUTING (policy ACCEPT 79 packets, 6761 bytes) pkts bytes target prot opt in out source destination # ISTIO_INBOUND 链：将所有入站流量重定向到 ISTIO_IN_REDIRECT 链上。目的地为 15090（Prometheus 使用）和 15020（Ingress gateway 使用，用于 Pilot 健康检查）端口的流量除外，发送到以上两个端口的流量将返回 iptables 规则链的调用点，即 PREROUTING 链的后继 POSTROUTING 后直接调用原始目的地。 Chain ISTIO_INBOUND (1 references) pkts bytes target prot opt in out source destination 0 0 RETURN tcp -- any any anywhere anywhere tcp dpt:ssh 2 120 RETURN tcp -- any any anywhere anywhere tcp dpt:15090 2699 162K RETURN tcp -- …","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"8d9d4c7734e7171ccb1f14c1059f0c16","permalink":"https://lib.jimmysong.io/istio-handbook/concepts/transparent-traffic-hijacking/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/istio-handbook/concepts/transparent-traffic-hijacking/","section":"istio-handbook","summary":"在 Pod 中的 init 容器启动之后，就向 Pod 中增加了 iptables 规则，本文将向你介绍 Istio 中的基于 iptables 的透明流量劫持的全过程。 查看 iptables nat 表中注入的规则 Init 容器通过向 iptables nat 表中注入转发规则来劫持流量的，下图显示的是 productpage 服务中的 iptables 流量劫持的详","tags":["Istio","Service Mesh"],"title":"Istio 中的透明流量劫持过程详解","type":"book"},{"authors":null,"categories":["Istio"],"content":"Sidecar 描述了 sidecar 代理的配置，该代理负责协调与它所连接的工作负载实例的 inbound 和 outbound 通信。默认情况下，Istio 会对网格中的所有 sidecar 代理进行必要的配置，以达到网格中的每个工作负载实例，并接受与工作负载相关的所有端口的流量。Sidecar 配置提供了一种方法来微调代理在转发工作负载的流量时将接受的一组端口和协议。此外，还可以限制代理在转发工作负载实例的 outbound 流量时可以到达的服务集。\n网格中的服务和配置被组织到一个或多个命名空间（例如，Kubernetes 命名空间）。命名空间中的 Sidecar 配置将适用于同一命名空间中的一个或多个工作负载实例，并通过 workloadSelector 字段进行选择。在没有 workloadSelector 的情况下，它将适用于同一命名空间的所有工作负载实例。在确定应用于工作负载实例的 Sidecar 配置时，将优先考虑具有选择该工作负载实例的 workloadSelector 的资源，而不是没有任何 workloadSelector 的 Sidecar 配置。\n注意事项 在配置 Sidecar 时需要注意以下事项。\n注意一 每个命名空间只能有一个没有任何 workloadSelector 的 Sidecar 配置，该配置为该命名空间的所有 pod 指定了默认配置。建议对整个命名空间的 sidecar 命名为 defatult。如果在一个给定的命名空间中存在多个无选择器的 Sidecar 配置，则系统的行为未定义。如果两个或更多带有 workloadSelector 的 Sidecar 配置选择了同一个工作负载实例，那么系统的行为将无法定义。\n注意二 MeshConfig 根命名空间中的 Sidecar 配置将被默认应用于所有没有 Sidecar 配置的命名空间。这个全局默认 Sidecar 配置不应该有任何 workloadSelector。\n示例 下面的例子在根命名空间 istio-config 中声明了一个全局默认 Sidecar 配置，该配置将所有命名空间中的 sidecar 配置为只允许向同一命名空间中的其他工作负载以及 istio-system 命名空间中的服务输出流量。\napiVersion:networking.istio.io/v1alpha3kind:Sidecarmetadata:name:defaultnamespace:istio-configspec:egress:- hosts:- \u0026#34;./*\u0026#34;- \u0026#34;istio-system/*\u0026#34;下面的例子在 prod-us1 命名空间中声明了一个 Sidecar 配置，它覆盖了上面定义的全局默认值，并配置了命名空间中的 sidecar，以允许向 prod-us1、prod-apis 和 istio-system 命名空间的公开服务输出流量。\napiVersion:networking.istio.io/v1alpha3kind:Sidecarmetadata:name:defaultnamespace:prod-us1spec:egress:- hosts:- \u0026#34;prod-us1/*\u0026#34;- \u0026#34;prod-apis/*\u0026#34;- \u0026#34;istio-system/*\u0026#34;下面的例子在 prod-us1 命名空间为所有标签为 app: ratings 的 pod 声明了一个 Sidecar 配置，这些 pod 属于 rating.prod-us1 服务。该工作负载接受 9080 端口的入站 HTTP 流量。然后，该流量被转发到在 Unix 域套接字上监听的附加工作负载实例。在出口方向，除了 istio-system 命名空间外，sidecar 只为 prod-us1 命名空间的服务代理绑定在 9080 端口的 HTTP 流量。\napiVersion:networking.istio.io/v1alpha3kind:Sidecarmetadata:name:ratingsnamespace:prod-us1spec:workloadSelector:labels:app:ratingsingress:- port:number:9080protocol:HTTPname:somenamedefaultEndpoint:unix:///var/run/someuds.sockegress:- port:number:9080protocol:HTTPname:egresshttphosts:- \u0026#34;prod-us1/*\u0026#34;- hosts:- \u0026#34;istio-system/*\u0026#34;如果工作负载的部署没有基于 IPTables 的流量捕获，Sidecar 配置是配置连接到工作负载实例的代理上的端口的唯一方法。下面的例子在 prod-us1 命名空间中为属于 productpage.prod-us1 服务的标签为 app: productpage 的所有 pod 声明了一个 Sidecar 配置。假设这些 pod 的部署没有 IPtable 规则（即 istio-init 容器），并且代理元数据 ISTIO_META_INTERCEPTION_MODE 被设置为 NONE，下面的规范允许这样的 pod 在 9080 端口接收 HTTP 流量（在 Istio mutual TLS 内包装），并将其转发给在 127.0.0.1:8080 监听的应用程序。它还允许应用程序与 127.0.0.1:3306 上的 MySQL 数据库进行通信，然后被代理到 mysql.foo.com:3306 上的外部托管 MySQL 服务。\napiVersion:networking.istio.io/v1alpha3kind:Sidecarmetadata:name:no-ip-tablesnamespace:prod-us1spec:workloadSelector:labels:app:productpageingress:- port:number:9080#绑定到 proxy_instance_ip:9080（0.0.0.0:9080，如果实例没有可用的单播 IP）。protocol:HTTPname:somenamedefaultEndpoint:127.0.0.1:8080captureMode:NONE#如果为整个代理设置了元数据，则不需要。egress:- port:number:3306protocol:MYSQLname:egressmysqlcaptureMode:NONE#如果为整个代理设置了元数据，则不需要。bind:127.0.0.1hosts:- \u0026#34;*/mysql.foo.com\u0026#34;以及路由到 mysql.foo.com:3306 的相关 ServiceEntry。\napiVersion:networking.istio.io/v1alpha3kind:ServiceEntrymetadata:name:external-svc-mysqlnamespace:ns1spec:hosts:- mysql.foo.comports:- number:3306name:mysqlprotocol:MYSQLlocation:MESH_EXTERNALresolution:DNS也可以在一个代理中混合和匹配流量捕获模式。例如有一个设置，内部服务在 192.168.0.0/16 子网。因此，在虚拟机上设置 IPTables 以捕获 192.168.0.0/16 子网的所有出站流量。假设虚拟机在 172.16.0.0/16 子网有一个额外的网络接口，用于入站流量。下面的 Sidecar 配置允许虚拟机在 172.16.1.32:80（虚拟机的 IP）上为从 172.16.0.0/16 子网到达的流量暴露一个监听器。\n注意：虚拟机中代理上的 ISTIO_META_INTERCEPTION_MODE 元数据应包含 REDIRECT 或 TPROXY 作为其值，这意味着基于 IPTables 的流量捕获是激活的。\napiVersion:networking.istio.io/v1alpha3kind:Sidecarmetadata:name:partial-ip-tablesnamespace:prod-us1spec:workloadSelector:labels:app:productpageingress:- bind:172.16.1.32port:number:80#绑定到 172.16.1.32:80protocol:HTTPname:somenamedefaultEndpoint:127.0.0.1:8080captureMode:NONEegress:#使用系统检测到的默认值设置配置，以处理到 192.168.0.0/16 子网的服务的出站流量，基于服务注册表提供的信息- captureMode:IPTABLEShosts:- \u0026#34;*/*\u0026#34;配置项 下图是 Sidecar 资源的配置拓扑图。\n  Sidecar 资源配置拓扑图  Sidecar 资源的顶级配置项如下：\n workloadSelector：用于选择应用该 Sidecar 配置的特定 pod/VM 集合的标准。如果省略，Sidecar 配置将被应用于同一命名空间的所有工作负载实例。 ingress：Ingress 指明用于处理连接工作负载实例的入站流量的 sidecar 配置。如果省略，Istio 将根据从编排平台获得的工作负载信息（例如，暴露的端口、服务等）自动配置 sidecar。如果指定，当且仅当工作负载实例与服务相关联时，将配置入站端口。 egress：Egress 指定 sidecar 的配置，用于处理从附加工作负载实例到网格中其他服务的出站流量。如果不指定，则继承系统检测到的全命名空间或全局默认 Sidecar 的默认值。 outboundTrafficPolicy：出站流量策略的配置。如果你的应用程序使用一个或多个事先不知道的外部服务，将策略设置为ALLOW_ANY，将导致 sidecar 将任何来自应用程序的未知流量路由到其请求的目的地。如果没有指定，则继承系统检测到的来自全命名空间或全局默认 Sidecar 的默认值。  关于 Sidecar 配置的详细用法请参考 Istio 官方文档。\n参考  Sidecar - Istio 官网文档  ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"6977824532274584a8d44e8a853a299a","permalink":"https://lib.jimmysong.io/istio-handbook/config-networking/sidecar/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/istio-handbook/config-networking/sidecar/","section":"istio-handbook","summary":"Sidecar 描述了 sidecar 代理的配置，该代理负责协调与它所连接的工作负载实例的 inbound 和 outbound 通信。默认情况下，Istio 会对网格中的所有 sidecar 代理进行必要的配置，以达到网格中的每个工作负载实例，并接受与工作负载相关的所有端口的流","tags":["Istio","Service Mesh"],"title":"Sidecar","type":"book"},{"authors":null,"categories":["Envoy"],"content":"TLS 监听器过滤器让我们可以检测到传输的是 TLS 还是明文。 如果传输是 TLS，它会检测服务器名称指示（SNI）和 / 或客户端的应用层协议协商（ALPN）。\n什么是 SNI？\nSNI 或服务器名称指示（Server Name Indication）是对 TLS 协议的扩展，它告诉我们在 TLS 握手过程的开始，哪个主机名正在连接。我们可以使用 SNI 在同一个 IP 地址和端口上提供多个 HTTPS 服务（使用不同的证书）。如果客户端以主机名 “hello.com” 进行连接，服务器可以出示该主机名的证书。同样地，如果客户以 “example.com” 连接，服务器就会提供该证书。\n什么是 ALPN？\nALPN 或应用层协议协商是对 TLS 协议的扩展，它允许应用层协商应该在安全连接上执行哪种协议，而无需进行额外的往返请求。使用 ALPN，我们可以确定客户端使用的是 HTTP/1.1 还是 HTTP/2。\n我们可以使用 SNI 和 ALPN 值来匹配过滤器链，使用 server_names（对于 SNI）和 / 或 application_protocols（对于 ALPN）字段。\n下面的片段显示了我们如何使用 application_protocols 和 server_names 来执行不同的过滤器链。\n...listener_filters:- name:\u0026#34;envoy.filters.listener.tls_inspector\u0026#34;typed_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.listener.tls_inspector.v3.TlsInspectorfilter_chains:- filter_chain_match:application_protocols:[\u0026#34;h2c\u0026#34;]filters:- name:some_filter... - filter_chain_match:server_names:\u0026#34;something.hello.com\u0026#34;transport_socket:...filters:- name:another_filter... ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"d20eb8299fabbb694460510656f56c9e","permalink":"https://lib.jimmysong.io/envoy-handbook/listener/tls-inspector-listener-filter/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/envoy-handbook/listener/tls-inspector-listener-filter/","section":"envoy-handbook","summary":"TLS 监听器过滤器让我们可以检测到传输的是 TLS 还是明文。 如果传输是 TLS，它会检测服务器名称指示（SNI）和 / 或客户端的应用层协议协商（ALPN）。 什么是 SNI？ SNI 或服务器名称指示（Server Name Indic","tags":["Envoy"],"title":"TLS 检查器监听器过滤器","type":"book"},{"authors":null,"categories":["Envoy"],"content":"分接式过滤器（Tap Filter）的目的是根据一些匹配的属性来记录 HTTP 流量。有两种方法来配置分接式过滤器。\n  使用 Envoy 配置里面的 static_config 字段\n  使用 admin_config 字段并指定配置 ID。\n  不同的是，我们在静态配置（static_config）中一次性提供所有东西——匹配配置和输出配置。当使用管理配置（admin_config）时，我们只提供配置 ID，然后在运行时使用 /tap 管理端点来配置过滤器。\n正如我们所提到的，过滤器的配置被分成两部分：匹配配置和输出配置。\n我们可以用匹配配置指定匹配谓词，告诉分接式过滤器要分接哪些请求并写入配置的输出。\n例如，下面的片段显示了如何使用 any_match 来匹配所有的请求，无论其属性如何。\ncommon_config:static_config:match:any_match:true...我们也有一个选项，可以在请求和响应 Header、Trailer 和正文上进行匹配。\nHeader/Trailer 匹配 Header/Trailer 匹配器使用 HttpHeadersMatch  proto，在这里我们指定一个头数组来匹配。例如，这个片段匹配任何请求头 my-header 被精确设置为 hello 的请求。\ncommon_config:static_config:match:http_request_headers_match:headers:name:\u0026#34;my-header\u0026#34;string_match:exact:\u0026#34;hello\u0026#34;... 请注意，在 string_match 中，我们可以使用其他匹配器（例如 prefix、surffix、safe_regex），正如前面解释的那样。\n 正文匹配 通用请求和响应正文（body）匹配使用 HttpGenericBodyMatch 来指定字符串或二进制匹配。顾名思义，字符串匹配（string_match）是在 HTTP 正文中寻找一个字符串，而二进制匹配（binary_match）是在 HTTP 正文中寻找一串字节的位置。\n例如，如果响应体包含字符串 hello，则下面的片段可以匹配。\ncommon_config:static_config:match:http_response_generic_body_match:patterns:string_match:\u0026#34;hello\u0026#34;...匹配谓词 我们可以用 or_match、and_match 和 not_match 等匹配谓词来组合多个 Header、Trailer 和正文匹配器。\nor_match 和 and_match 使用 MatchSet 原语，描述逻辑 OR 或逻辑 AND。我们在匹配集内的 rules 字段中指定构成一个集合的规则列表。\n下面的例子显示了如何使用 and_match 来确保响应体包含 hello 这个词，以及请求头 my-header 被设置为 hello。\ncommon_config:static_config:match:and_match:rules:- http_response_generic_body_match:patterns:- string_match:\u0026#34;hello\u0026#34;- http_request_headers_match:headers:name:\u0026#34;my-header\u0026#34;string_match:exact:\u0026#34;hello\u0026#34;...如果我们想实现逻辑 OR，那么我们可以用 or_match 字段替换 and_match 字段。字段内的配置将保持不变，因为两个字段都使用 MatchSet  proto。\n让我们使用与之前相同的例子来说明 not_match 是如何工作的。假设我们想过滤所有没有设置头信息 my-header: hello 的请求，以及响应体不包括 hello 这个字符串的请求。\n下面是我们如何写这个配置。\ncommon_config:static_config:match:not_match:and_match:rules:- http_response_generic_body_match:patterns:- string_match:\u0026#34;hello\u0026#34;- http_request_headers_match:headers:name:\u0026#34;my-header\u0026#34;string_match:exact:\u0026#34;hello\u0026#34;...not_match  字段和父 match 字段一样使用 MatchPredicate  原语。匹配字段是一个递归结构，它允许我们创建复杂的嵌套匹配配置。\n这里要提到的最后一个字段是 any_match。这是一个布尔字段，当设置为 true 时，将总是匹配。\n输出配置 一旦请求被过滤出来，我们需要告诉过滤器将输出写入哪里。目前，我们可以配置一个单一的输出沉积。\n下面是一个输出配置示例。\n...output_config:sinks:- format:JSON_BODY_AS_STRINGfile_per_tap:path_prefix:tap...使用 file_per_tap，我们指定要为每个被监听的数据流输出一个文件。path_prefix 指定了输出文件的前缀。文件用以下格式命名：\n\u0026lt;path_prefix\u0026gt;_\u0026lt;id\u0026gt;.\u0026lt;pb | json\u0026gt; id 代表一个标识符，使我们能够区分流实例的记录跟踪。文件扩展名（pb 或 json）取决于格式选择。\n捕获输出的第二个选项是使用 streaming_admin 字段。这指定了 /tap 管理端点将流式传输被捕获的输出。请注意，要使用 /tap 管理端点进行输出，还必须使用 admin_config 字段配置分接式过滤器。如果我们静态地配置了分接式过滤器，我们就不会使用 /tap 端点来获取输出。\n格式选择 我们有多种输出格式的选项，指定消息的书写方式。让我们看看不同的格式，从默认格式开始，JSON_BODY_AS_BYTES。\nJSON_BODY_AS_BYTES 输出格式将消息输出为 JSON，任何响应的 body 数据将在 as_bytes 字段中，其中包含 base64 编码的字符串。\n例如，下面是分接输出的示例。\n{ \u0026#34;http_buffered_trace\u0026#34;: { \u0026#34;request\u0026#34;: { \u0026#34;headers\u0026#34;: [ { \u0026#34;key\u0026#34;: \u0026#34;:authority\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;localhost:10000\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;:path\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;/\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;:method\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;GET\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;:scheme\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;http\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;user-agent\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;curl/7.64.0\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;accept\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;*/*\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;my-header\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;hello\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;x-forwarded-proto\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;http\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;x-request-id\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;67e3e8ac-429a-42fb-945b-ec25927fdcc1\u0026#34; } ], \u0026#34;trailers\u0026#34;: [] }, \u0026#34;response\u0026#34;: { \u0026#34;headers\u0026#34;: [ { \u0026#34;key\u0026#34;: \u0026#34;:status\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;200\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;content-length\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;5\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;content-type\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;text/plain\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;date\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;Mon, 29 Nov 2021 19:31:43 GMT\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;server\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;envoy\u0026#34; } ], \u0026#34;body\u0026#34;: { \u0026#34;truncated\u0026#34;: false, \u0026#34;as_bytes\u0026#34;: \u0026#34;aGVsbG8=\u0026#34; }, \u0026#34;trailers\u0026#34;: [] } } } 注意 body 中的 as_bytes 字段。该值是 body 数据的 base64 编码表示（本例中为 hello）。\n第二种输出格式是 JSON_BODY_AS_STRING。与之前的格式不同的是，在 JSON_BODY_AS_STRING 中，body 数据是以字符串的形式写在 as_string 字段中。当我们知道 body 是人类可读的，并且不需要对数据进行 base64 编码时，这种格式很有用。\n... \u0026#34;body\u0026#34;: { \u0026#34;truncated\u0026#34;: false, \u0026#34;as_string\u0026#34;: \u0026#34;hello\u0026#34; }, ... 其他三种格式类型是 PROTO_BINARY、PROTO_BINARY_LENGTH_DELIMITED 和 PROTO_TEXT。\nPROTO_BINARY 格式以二进制 proto 格式写入输出。这种格式不是自限性的，这意味着如果分接写了多个没有任何长度信息的二进制消息，那么数据流将没有用处。如果我们在每个文件中写一个消息，那么输出格式将更容易解析。\n我们也可以使用 PROTO_BINARY_LENGTH_DELIMITED 格式，其中消息被写成序列元组。每个元组是消息长度（编码为 32 位 protobuf varint 类型），后面是二进制消息。\n最后，我们还可以使用 PROTO_TEXT 格式，在这种格式下，输出结果以下面的 protobuf 格式写入。\nhttp_buffered_trace { request { headers { key: \u0026#34;:authority\u0026#34; value: \u0026#34;localhost:10000\u0026#34; } headers { key: \u0026#34;:path\u0026#34; value: \u0026#34;/\u0026#34; } headers { key: \u0026#34;:method\u0026#34; value: \u0026#34;GET\u0026#34; } headers { key: \u0026#34;:scheme\u0026#34; value: \u0026#34;http\u0026#34; } headers { key: \u0026#34;user-agent\u0026#34; value: \u0026#34;curl/7.64.0\u0026#34; } headers { key: \u0026#34;accept\u0026#34; value: \u0026#34;*/*\u0026#34; } headers { key: \u0026#34;debug\u0026#34; value: \u0026#34;true\u0026#34; } headers { key: \u0026#34;x-forwarded-proto\u0026#34; value: \u0026#34;http\u0026#34; } headers { key: \u0026#34;x-request-id\u0026#34; value: \u0026#34;af6e0879-e057-4efc-83e4-846ff4d46efe\u0026#34; } } response { headers { key: \u0026#34;:status\u0026#34; value: \u0026#34;500\u0026#34; } headers { key: \u0026#34;content-length\u0026#34; value: \u0026#34;5\u0026#34; } headers { key: \u0026#34;content-type\u0026#34; value: \u0026#34;text/plain\u0026#34; } headers { key: \u0026#34;date\u0026#34; value: \u0026#34;Mon, 29 Nov 2021 22:32:40 GMT\u0026#34; } headers { key: \u0026#34;server\u0026#34; value: \u0026#34;envoy\u0026#34; } body { as_bytes: \u0026#34;hello\u0026#34; } }}静态配置分接式过滤器 我们把匹配的配置和输出配置（使用 file_per_tap 字段）结合起来，静态地配置分接式过滤器。\n下面是一个通过静态配置来配置分接式过滤器的片段。\n- name:envoy.filters.http.taptyped_config:\u0026#34;@type\u0026#34;: …","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"882383193bf0edc8142a25a2c72e5c14","permalink":"https://lib.jimmysong.io/envoy-handbook/admin-interface/tap-filter/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/envoy-handbook/admin-interface/tap-filter/","section":"envoy-handbook","summary":"分接式过滤器（Tap Filter）的目的是根据一些匹配的属性来记录 HTTP 流量。有两种方法来配置分接式过滤器。 使用 Envoy 配置里面的 static_config 字段 使用 admin_config 字段并指定配置 ID。 不同的是，我们在静态配置（static_conf","tags":["Envoy"],"title":"分接式过滤器","type":"book"},{"authors":null,"categories":["Envoy"],"content":"在这个实验中，我们将学习如何将 Envoy 应用日志发送到 Google Cloud Logging。我们将在 GCP 中运行的虚拟机（VM）实例上运行 Envoy，配置 Envoy 实例，将应用日志发送到 GCP 中的云日志。配置 Envoy 实例将允许我们在日志资源管理器中查看 Envoy 的日志，获得日志分析，并使用其他谷歌云功能。\n为了使用谷歌云的日志收集、分析和其他工具，我们需要安装云日志代理（Ops Agent）。\n在这个演示中，我们将在一个单独的虚拟机上安装 Ops 代理。另外，请注意，其他云供应商可能使用不同的日志工具和服务。\n安装 Ops 代理 在 Google Cloud，在你的地区创建一个新的虚拟机实例。一旦创建了虚拟机，我们就可以通过 SSH 进入该实例并安装 Ops 代理。\n在虚拟机实例中，运行以下命令来安装 Ops 代理。\ncurl -sSO https://dl.google.com/cloudagents/add-google-cloud-ops-agent-repo.sh sudo bash add-google-cloud-ops-agent-repo.sh --also-install 安装 Ops 代理的另一个选择是按照以下步骤进行。\n 从 GCP 的导航页面，选择监测。 在 \u0026#34;监控 \u0026#34; 导航页面，选择 \u0026#34; 仪表盘 \u0026#34;。 在仪表板表中，找到并点击虚拟机实例。 选择一个没有安装代理的实例旁边的复选框（例如，代理栏显示未检测到）。 点击安装代理按钮，在打开的窗口中，点击在在 Cloud Shell 中运行按钮，开始安装。  安装代理的命令将在 Cloud Shell 中打开。你需要做的最后一件事是按回车键开始安装。\n下面是成功安装的命令和输出在 Cloud Shell 中的样子。\n$ :\u0026gt; agents_to_install.csv \u0026amp;\u0026amp; \\ → echo \u0026#39;\u0026#34;projects/envoy-project/zones/us-west1-a/instances/envoy-instance\u0026#34;,\u0026#34;[{\u0026#34;\u0026#34;type\u0026#34;\u0026#34;:\u0026#34;\u0026#34;ops-agent\u0026#34;\u0026#34;}]\u0026#34;\u0026#39; \u0026gt;\u0026gt; agents_to_install.csv \u0026amp;\u0026amp; \\ → curl -sSO https://dl.google.com/cloudagents/mass-provision-google-cloud-ops-agents.py \u0026amp;\u0026amp; \\ → python3 mass-provision-google-cloud-ops-agents.py --file agents_to_install.csv 2021-11-03T19:04:31.577710Z Processing instance: projects/peterjs-project/zones/us-west1-a/instances/some-instance. ---------------------Getting output------------------------- Progress: |==================================================| 100.0% [1/1] (100.0%) completed; [1/1] (100.0%) succeeded; [0/1] (0.0%) failed; Instance: projects/envoy-project/zones/us-west1-a/instances/envoy-instance successfully runs ops-agent. See log file in: ./google_cloud_ops_agent_provisioning/20211103-190431_576419/envoy-project_us-west1-a_envoy-instance.log SUCCEEDED: [1/1] (100.0%) FAILED: [0/1] (0.0%) COMPLETED: [1/1] (100.0%) See script log file: ./google_cloud_ops_agent_provisioning/20211103-190431_576419/wrapper_script.log 随着安装的进展，虚拟机实例仪表板中的代理列将显示待定。一旦代理安装完成，该值将变为 Ops Agent，这表明 Ops Agent 已成功安装。\n现在我们可以通过 SSH 进入虚拟机实例，安装 func-e（用于运行 Envoy），创建一个基本的 Envoy 配置，并运行它，这样 Envoy 的应用日志就会被发送到 GCP 的云端日志。\n安装 func-e 要在虚拟机上安装 func-e，请运行：\ncurl https://func-e.io/install.sh | sudo bash -s -- -b /usr/local/bin 我们可以运行 func-e --version 来检查安装是否成功。\n发送 Envoy 应用日志到云端日志 让我们创建一个我们将在本实验中使用的原始 Envoy 配置。\nstatic_resources:listeners:- name:listener_0address:socket_address:address:0.0.0.0port_value:10000filter_chains:- filters:- name:envoy.filters.network.http_connection_managertyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManagerstat_prefix:ingress_httphttp_filters:- name:envoy.filters.http.routerroute_config:name:my_first_routevirtual_hosts:- name:direct_response_servicedomains:[\u0026#34;*\u0026#34;]routes:- match:prefix:\u0026#34;/\u0026#34;direct_response:status:200body:inline_string:\u0026#34;200\u0026#34;将上述 YAML 保存为 6-lab-3-gcp-logging.yaml。\n让我们来配置 Ops Agent，为 Envoy 创建一个新的接收器，描述如何检索日志。\nlogging:receivers:envoy:type:filesinclude_paths:- /var/log/envoy.logservice:pipelines:default_pipeline:receivers:[envoy]将上述内容保存到虚拟机实例上的 /etc/google-cloud-ops-agent/config.yaml 文件中。要重新启动 Ops Agent，请运行 sudo service google-cloud-ops-agent restart。\n在 Ops Agent 使用新配置的情况下，我们可以运行 Envoy，并告诉它把日志写到 /var/log/envoy.log 文件中，代理会在那里接收。\nsudo func-e run -c 6-lab-3-gcp-logging.yaml --log-path /var/log/envoy.log 接下来，我们可以点击日志，然后点击 GCP 中的日志资源管理器，查看虚拟机上运行的 Envoy 实例的日志。\n   GCP的日志资源管理器中的 Envoy 日志   下一章   ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"03195933182888e5e89158fd7f67bce7","permalink":"https://lib.jimmysong.io/envoy-handbook/logging/lab14/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/envoy-handbook/logging/lab14/","section":"envoy-handbook","summary":"在这个实验中，我们将学习如何将 Envoy 应用日志发送到 Google Cloud Logging。我们将在 GCP 中运行的虚拟机（VM）实例上运行 Envoy，配置 Envoy 实例，将应用日志发送到 GCP 中的云日志。配置 Envoy 实例将允许我们在日志资源管理器中","tags":["Envoy"],"title":"实验 14：将 Envoy 的日志发送到 Google Cloud Logging","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"Kubernetes 作为一个容器编排调度引擎，资源调度是它的最基本也是最重要的功能，这一节中我们将着重讲解 Kubernetes 中是如何做资源调度的。\nKubernetes 中有一个叫做 kube-scheduler 的组件，该组件就是专门监听 kube-apiserver 中是否有还未调度到 node 上的 pod，再通过特定的算法为 pod 指定分派 node 运行。\nKubernetes 中的众多资源类型，例如 Deployment、DaemonSet、StatefulSet 等都已经定义了 Pod 运行的一些默认调度策略，但是如果我们细心的根据 node 或者 pod 的不同属性，分别为它们打上标签之后，我们将发现 Kubernetes 中的高级调度策略是多么强大。当然如果要实现动态的资源调度，即 pod 已经调度到某些节点上后，因为一些其它原因，想要让 pod 重新调度到其它节点。\n考虑以下两种情况：\n 集群中有新增节点，想要让集群中的节点的资源利用率比较均衡一些，想要将一些高负载的节点上的 pod 驱逐到新增节点上，这是 kuberentes 的 scheduler 所不支持的，需要使用如 descheduler 这样的插件来实现。 想要运行一些大数据应用，设计到资源分片，pod 需要与数据分布达到一致均衡，避免个别节点处理大量数据，而其它节点闲置导致整个作业延迟，这时候可以考虑使用 kube-batch。  ","date":1653062400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"34d47238fcf512c5365268993661f43f","permalink":"https://lib.jimmysong.io/kubernetes-handbook/cluster/scheduling/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/cluster/scheduling/","section":"kubernetes-handbook","summary":"Kubernetes 作为一个容器编排调度引擎，资源调度是它的最基本也是最重要的功能，这一节中我们将着重讲解 Kubernetes 中是如何做资源调度的。 Kubernetes 中有一个叫做 kube-scheduler 的组件，该组件就是专门监听 kube-apiserver 中是否有还未调度到 node 上的 pod，再通过特定的","tags":["Kubernetes"],"title":"资源调度","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"QoS（Quality of Service），大部分译为“服务质量等级”，又译作“服务质量保证”，是作用在 Pod 上的一个配置，当 Kubernetes 创建一个 Pod 时，它就会给这个 Pod 分配一个 QoS 等级，可以是以下等级之一：\n Guaranteed：Pod 里的每个容器都必须有内存/CPU 限制和请求，而且值必须相等。 Burstable：Pod 里至少有一个容器有内存或者 CPU 请求且不满足 Guarantee 等级的要求，即内存/CPU 的值设置的不同。 BestEffort：容器必须没有任何内存或者 CPU 的限制或请求。  该配置不是通过一个配置项来配置的，而是通过配置 CPU/内存的 limits 与 requests 值的大小来确认服务质量等级的。使用 kubectl get pod -o yaml 可以看到 pod 的配置输出中有 qosClass 一项。该配置的作用是为了给资源调度提供策略支持，调度算法根据不同的服务质量等级可以确定将 pod 调度到哪些节点上。\n例如，下面这个 YAML 配置中的 Pod 资源配置部分设置的服务质量等级就是 Guarantee。\nspec:containers:...resources:limits:cpu:100mmemory:128Mirequests:cpu:100mmemory:128Mi下面的 YAML 配置的 Pod 的服务质量等级是 Burstable。\nspec:containers:...resources:limits:memory:\u0026#34;180Mi\u0026#34;requests:memory:\u0026#34;100Mi\u0026#34;参考  配置 Pod 的服务质量 - kubernetes.io  ","date":1653062400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"c220f35c938c5902adfdfdb29427d396","permalink":"https://lib.jimmysong.io/kubernetes-handbook/cluster/qos/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/cluster/qos/","section":"kubernetes-handbook","summary":"QoS（Quality of Service），大部分译为“服务质量等级”，又译作“服务质量保证”，是作用在 Pod 上的一个配置，当 Kubernetes 创建一个 Pod 时，它就会给这个 Pod 分配一个 QoS 等级，可以是以下等级之一： Guarant","tags":["Kubernetes"],"title":"服务质量等级（QoS）","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"当你使用 Kubernetes 的时候，有没有遇到过 Pod 在启动后一会就挂掉然后又重新启动这样的恶性循环？你有没有想过 Kubernetes 是如何检测 pod 是否还存活？虽然容器已经启动，但是 Kubernetes 如何知道容器的进程是否准备好对外提供服务了呢？让我们通过 Kubernetes 官网的这篇文章 Configure Liveness and Readiness Probes，来一探究竟。\n本文将展示如何配置容器的存活和可读性探针。\nKubelet 使用 liveness probe（存活探针）来确定何时重启容器。例如，当应用程序处于运行状态但无法做进一步操作，liveness 探针将捕获到 deadlock，重启处于该状态下的容器，使应用程序在存在 bug 的情况下依然能够继续运行下去（谁的程序还没几个 bug 呢）。\nKubelet 使用 readiness probe（就绪探针）来确定容器是否已经就绪可以接受流量。只有当 Pod 中的容器都处于就绪状态时 kubelet 才会认定该 Pod 处于就绪状态。该信号的作用是控制哪些 Pod 应该作为 service 的后端。如果 Pod 处于非就绪状态，那么它们将会被从 service 的 load balancer 中移除。\n定义 liveness 命令 许多长时间运行的应用程序最终会转换到 broken 状态，除非重新启动，否则无法恢复。Kubernetes 提供了 liveness probe 来检测和补救这种情况。\n在本次练习将基于 gcr.io/google_containers/busybox 镜像创建运行一个容器的 Pod。以下是 Pod 的配置文件 exec-liveness.yaml：\napiVersion:v1kind:Podmetadata:labels:test:livenessname:liveness-execspec:containers:- name:livenessargs:- /bin/sh- -c- touch /tmp/healthy; sleep 30; rm -rf /tmp/healthy; sleep 600image:gcr.io/google_containers/busyboxlivenessProbe:exec:command:- cat- /tmp/healthyinitialDelaySeconds:5periodSeconds:5该配置文件给 Pod 配置了一个容器。periodSeconds 规定 kubelet 要每隔 5 秒执行一次 liveness probe。 initialDelaySeconds 告诉 kubelet 在第一次执行 probe 之前要的等待 5 秒钟。探针检测命令是在容器中执行 cat /tmp/healthy 命令。如果命令执行成功，将返回 0，kubelet 就会认为该容器是活着的并且很健康。如果返回非 0 值，kubelet 就会杀掉这个容器并重启它。\n容器启动时，执行该命令：\n/bin/sh -c \u0026#34;touch /tmp/healthy; sleep 30; rm -rf /tmp/healthy; sleep 600\u0026#34; 在容器生命的最初 30 秒内有一个 /tmp/healthy 文件，在这 30 秒内 cat /tmp/healthy 命令会返回一个成功的返回码。30 秒后， cat /tmp/healthy 将返回失败的返回码。\n创建 Pod：\nkubectl create -f https://k8s.io/docs/tasks/configure-pod-container/exec-liveness.yaml 在 30 秒内，查看 Pod 的 event：\nkubectl describe pod liveness-exec 结果显示没有失败的 liveness probe：\nFirstSeen LastSeen Count From SubobjectPath Type Reason Message --------- -------- ----- ---- ------------- -------- ------ ------- 24s 24s 1 {default-scheduler } Normal Scheduled Successfully assigned liveness-exec to worker0 23s 23s 1 {kubelet worker0} spec.containers{liveness} Normal Pulling pulling image \u0026#34;gcr.io/google_containers/busybox\u0026#34; 23s 23s 1 {kubelet worker0} spec.containers{liveness} Normal Pulled Successfully pulled image \u0026#34;gcr.io/google_containers/busybox\u0026#34; 23s 23s 1 {kubelet worker0} spec.containers{liveness} Normal Created Created container with docker id 86849c15382e; Security:[seccomp=unconfined] 23s 23s 1 {kubelet worker0} spec.containers{liveness} Normal Started Started container with docker id 86849c15382e 启动 35 秒后，再次查看 pod 的 event：\nkubectl describe pod liveness-exec 在最下面有一条信息显示 liveness probe 失败，容器被删掉并重新创建。\nFirstSeen LastSeen Count From SubobjectPath Type Reason Message --------- -------- ----- ---- ------------- -------- ------ ------- 37s 37s 1 {default-scheduler } Normal Scheduled Successfully assigned liveness-exec to worker0 36s 36s 1 {kubelet worker0} spec.containers{liveness} Normal Pulling pulling image \u0026#34;gcr.io/google_containers/busybox\u0026#34; 36s 36s 1 {kubelet worker0} spec.containers{liveness} Normal Pulled Successfully pulled image \u0026#34;gcr.io/google_containers/busybox\u0026#34; 36s 36s 1 {kubelet worker0} spec.containers{liveness} Normal Created Created container with docker id 86849c15382e; Security:[seccomp=unconfined] 36s 36s 1 {kubelet worker0} spec.containers{liveness} Normal Started Started container with docker id 86849c15382e 2s 2s 1 {kubelet worker0} spec.containers{liveness} Warning Unhealthy Liveness probe failed: cat: can\u0026#39;t open \u0026#39;/tmp/healthy\u0026#39;: No such file or directory 再等 30 秒，确认容器已经重启：\nkubectl get pod liveness-exec 从输出结果来 RESTARTS 值加 1 了。\nNAME READY STATUS RESTARTS AGE liveness-exec 1/1 Running 1 1m 定义一个 liveness HTTP 请求 我们还可以使用 HTTP GET 请求作为 liveness probe。下面是一个基于 gcr.io/google_containers/liveness 镜像运行了一个容器的 Pod 的例子 http-liveness.yaml：\napiVersion:v1kind:Podmetadata:labels:test:livenessname:liveness-httpspec:containers:- name:livenessargs:- /serverimage:gcr.io/google_containers/livenesslivenessProbe:httpGet:path:/healthzport:8080httpHeaders:- name:X-Custom-Headervalue:AwesomeinitialDelaySeconds:3periodSeconds:3该配置文件只定义了一个容器，livenessProbe 指定 kubelet 需要每隔 3 秒执行一次 liveness probe。initialDelaySeconds 指定 kubelet 在该执行第一次探测之前需要等待 3 秒钟。该探针将向容器中的 server 的 8080 端口发送一个 HTTP GET 请求。如果 server 的 /healthz 路径的 handler 返回一个成功的返回码，kubelet 就会认定该容器是活着的并且很健康。如果返回失败的返回码，kubelet 将杀掉该容器并重启它。\n任何大于 200 小于 400 的返回码都会认定是成功的返回码。其他返回码都会被认为是失败的返回码。\n最开始的 10 秒该容器是活着的， /healthz handler 返回 200 的状态码。这之后将返回 500 的返回码。\nhttp.HandleFunc(\u0026#34;/healthz\u0026#34;, func(w http.ResponseWriter, r *http.Request) { duration := time.Now().Sub(started) if duration.Seconds() \u0026gt; 10 { w.WriteHeader(500) w.Write([]byte(fmt.Sprintf(\u0026#34;error: %v\u0026#34;, duration.Seconds()))) } else { w.WriteHeader(200) w.Write([]byte(\u0026#34;ok\u0026#34;)) } }) 容器启动 3 秒后，kubelet 开始执行健康检查。第一次健康监测会成功，但是 10 秒后，健康检查将失败，kubelet 将杀掉和重启容器。\n创建一个 Pod 来测试一下 HTTP liveness 检测：\nkubectl create -f https://k8s.io/docs/tasks/configure-pod-container/http-liveness.yaml After 10 seconds, view Pod events to verify that liveness probes have failed and the Container has been restarted:\n10 秒后，查看 Pod 的 event，确认 liveness probe 失败并重启了容器。\nkubectl describe pod liveness-http 定义 TCP liveness …","date":1653062400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"f609f955484ffae64e79fad8614fa09a","permalink":"https://lib.jimmysong.io/kubernetes-handbook/config/liveness-readiness-probes/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/config/liveness-readiness-probes/","section":"kubernetes-handbook","summary":"当你使用 Kubernetes 的时候，有没有遇到过 Pod 在启动后一会就挂掉然后又重新启动这样的恶性循环？你有没有想过 Kubernetes 是如何检测 pod 是否还存活？虽然容器已经启动，但是 Kubernetes 如何知道容器的进程是否准备好对外提供服务了呢？让我们通过 Kubernetes","tags":["Kubernetes"],"title":"配置 Pod 的 liveness 和 readiness 探针","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"Service account 为 Pod 中的进程提供身份信息。\n本文是关于 Service Account 的用户指南，管理指南另见 Service Account 的集群管理指南 。\n注意：本文档描述的关于 Service Account 的行为只有当您按照 Kubernetes 项目建议的方式搭建起集群的情况下才有效。您的集群管理员可能在您的集群中有自定义配置，这种情况下该文档可能并不适用。\n当您（真人用户）访问集群（例如使用kubectl命令）时，apiserver 会将您认证为一个特定的 User Account（目前通常是admin，除非您的系统管理员自定义了集群配置）。Pod 容器中的进程也可以与 apiserver 联系。 当它们在联系 apiserver 的时候，它们会被认证为一个特定的 Service Account（例如default）。\n使用默认的 Service Account 访问 API server 当您创建 pod 的时候，如果您没有指定一个 service account，系统会自动得在与该pod 相同的 namespace 下为其指派一个default service account。如果您获取刚创建的 pod 的原始 json 或 yaml 信息（例如使用kubectl get pods/podename -o yaml命令），您将看到spec.serviceAccountName字段已经被设置为 automatically 。\n您可以在 pod 中使用自动挂载的 service account 凭证来访问 API，如 Accessing the Cluster 中所描述。\nService account 是否能够取得访问 API 的许可取决于您使用的 授权插件和策略。\n在 1.6 以上版本中，您可以选择取消为 service account 自动挂载 API 凭证，只需在 service account 中设置 automountServiceAccountToken: false：\napiVersion:v1kind:ServiceAccountmetadata:name:build-robotautomountServiceAccountToken:false...在 1.6 以上版本中，您也可以选择只取消单个 pod 的 API 凭证自动挂载：\napiVersion:v1kind:Podmetadata:name:my-podspec:serviceAccountName:build-robotautomountServiceAccountToken:false...如果在 pod 和 service account 中同时设置了 automountServiceAccountToken , pod 设置中的优先级更高。\n使用 Service Account 作为用户权限管理配置 kubeconfig 创建服务账号 kubectl create serviceaccount sample-sc 这时候我们将得到一个在 default namespace 的 serviceaccount 账号； 我们运行kubectl get serviceaccount sample-sc 将得到如下结果:\napiVersion:v1kind:ServiceAccountmetadata:creationTimestamp:2018-09-03T02:00:37Zlabels:from:mofangname:mofang-viewer-scnamespace:defaultresourceVersion:\u0026#34;18914458\u0026#34;selfLink:/api/v1/namespaces/default/serviceaccounts/sample-scuid:26e129dc-af1d-11e8-9453-00163e0efab0secrets:- name:sample-sc-token-9x7nk因为我们在使用 serviceaccount 账号配置 kubeconfig 的时候需要使用到 sample-sc 的 token， 该 token 保存在该 serviceaccount 保存的 secret 中；\n我们运行kubectl get secret sample-sc-token-9x7nk 将会得到如下结果:\napiVersion:v1data:ca.crt:Ci0tLS0tQkVHSU4gQ0VSVcvfUNBVEUtLS0tLQpNSUlDL3pDQ0FtaWdBd0lCQWdJREJzdFlNQTBHQ1NxR1NJYjNEUUVCQ3dVQU1HSXhDekFKQmdOVkJBWVRBa05PCk1SRXdEd1lEVlFRSURBaGFhR1ZLYVdGdVp6RVJNQThHQTFVRUJ3d0lTR0Z1WjFwb2IzVXhFREFPQmdOVkJBb00KQjBGc2FXSmhZbUV4RERBS0JnTlZCQXNNQTBGRFV6RU5NQXNHQTFVRUF3d0VjbTl2ZERBZUZ3MHhPREExTWprdwpNelF3TURCYUZ3MHpPREExTWpRd016UTFOVGxhTUdveEtqQW9CZ05WQkFvVElXTTJaVGxqTm1KallUY3pZakUwClkyTTBZV0UzT1RNd1lqTTROREV4TkRaallURVFNQTRHQTFVRUN4TUhaR1ZtWVhWc2RERXFNQ2dHQTFVRUF4TWgKWXpabE9XTTJZbU5oTnpOaU1UUmpZelJoWVRjNU16QmlNemcwTVRFME5tTmhNSUdmTUEwR0NTcUdTSWIzRFFFQgpBUVVBQTRHTkFEQ0JpUUtCZ1FETGNFWmJycCtBa0taNHU4TklVM25jaFU4YkExMnhKR0pJMzlxdTd4aFFsR3lHCmZqQTFqdXV4cVMyaE4vTGpwM21XNkdIaW0zd2lJd2N1WUtUN3RGOW9UejgrTzhBQzZHYnpkWExIL1RQTWtCZ2YKOVNYaEdod1hndklMb3YzbnZlS1MzRElxU3UreS9OK1huMzhOOW53SHF6S0p2WE1ROWtJaUJuTXgwVnlzSFFJRApBUUFCbzRHNk1JRzNNQTRHQTFVZER3RUIvd1FFQXdJQ3JEQVBCZ05WSFJNQkFmOEVCVEFEQVFIL01COEdBMVVkCkl3UVlNQmFBRklWYS85MGp6U1Z2V0VGdm5tMUZPWnRZZlhYL01Ed0dDQ3NHQVFVRkJ3RUJCREF3TGpBc0JnZ3IKQmdFRkJRY3dBWVlnYUhSMGNEb3ZMMk5sY25SekxtRmpjeTVoYkdsNWRXNHVZMjl0TDI5amMzQXdOUVlEVlIwZgpCQzR3TERBcW9DaWdKb1lrYUhSMGNEb3ZMMk5sY25SekxtRmpjeTVoYkdsNWRXNHl0TDNKdmIzUXVZM0pzCk1BMEdDU3FHU0liM0RRRUJDd1VBQTRHQkFKWFRpWElvQ1VYODFFSU5idVRTay9PanRJeDM0ckV0eVBuTytBU2oKakszaTR3d1BRMEt5MDhmTlNuU2Z4Q1EyeEY1NTIxNVNvUzMxU3dMellJVEp1WFpBN2xXT29RU1RZL2lBTHVQQgovazNCbDhOUGNmejhGNXlZNy9uY1N6WDRwTXgxOHIwY29PTE1iZlpUUXJtSHBmQ053bWRjQmVCK0JuRFJMUEpGCmgzSUQKLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQoKCg==namespace:ZGVmYXAsdA==token:ZXlKaGJHY2lPaUpTVXpJMU5pSXNJblI1Y0NJNklrcFhppppppo5LmV5SnBjM01pT2lKcmRXSmxjbTVsZddekwzTmxjblpwWTJWaFkyTnZkVzUwSWl3aWEzVmlaWEp1WlhSbGN5NXBieTl6WlhKMmFXTmxZV05qYjNWdWRDOXVZVzFsYzNCaFkyVWlPaUprWldaaGRXeDBJaXdpYTNWaVpYSnVaWFJsY3k1cGJ5OXpaWEoyYVdObFlXTmpiM1Z1ZEM5elpXTnlaWFF1Ym1GdFpTSTZJbTF2Wm1GdVp5MTJhV1YzWlhJdGMyTXRkRzlyWlc0dE9YZzNibXNpTENKcmRXSmxjbTVsZEdWekxtbHZMM05sY25acFkyVmhZMk52ZFc1MEwzTmxjblpwWTJVdFlXTmpiM1Z1ZEM1dVlXMWxJam9pYlc5bVlXNW5MWFpwWlhkbGNpMXpZeUlzSW10MVltVnlibVYwWlhNdWFXOHZjMlZ5ZG1salpXRmpZMjkxYm5RdmMyVnlkbWxqWlMxaFkyTnZkVzUwTG5WcFpDSTZJakkyWlRFeU9XUmpMV0ZtTVdRdE1URmxPQzA1TkRVekxUQXdNVFl6WlRCbFptRmlNQ0lzSW5OMVlpSTZJbk41YzNSbGJUcHpaWEoyYVdObFlXTmpiM1Z1ZERwa1pXWmhkV3gwT20xdlptRnVaeTEyYVdWM1pYSXRjMk1pZlEuQWpudnZueXRaWHJ1UndSdEJ3S3RFdzZWNzJpWU1vOUI2LVh3VmkzRWhReXNOM21PLXJvdGFJQnZHUFlNMGZNVDlWRzVjTFFKYmJWUmhLR0FyclUyQ1FNVl9JQ3NpbjFzMjFxTXJ5dngzNm9abzFYZkpWZlVVMEtqeWxndEQ0NTNmWU84SlFfWFF3OGNwVWc5NGE2WnZvcDZHcXNGNGhzT0sxTjFMaGRrSFBpWHA4TzRQUzJ6eDFXNklfeGs2ZUNBcjUxRVpwSktEWTZHMmhmZ1A5emxROGdPV21nbS1EVjZPZzNobjJ4UFprZUktVk1nYUF3amdFUGtVZFJDSndYRk9QRG5UcXlqUmtZVzdLVU1GLTVDTHdBNDZMNk5PYjJ6YUR0Uy16Zm5hdVFwLVdIcFNQdDNEdFc0UmRLZDVDZzE3Z3RGaWhRZzI3dnVqYWJNMGpmQmp3kind:Secretmetadata:annotations:kubernetes.io/service-account.name:sample-sckubernetes.io/service-account.uid:26e129dc-af1d-11e8-9453-00163e0efab0creationTimestamp:2018-09-03T02:00:37Zname:mofang-viewer-sc-token-9x7nknamespace:defaultresourceVersion:\u0026#34;18914310\u0026#34;selfLink:/api/v1/namespaces/default/secrets/sample-sc-token-9x7nkuid:26e58b7c-af1d-11e8-9453-00163e0efab0type:kubernetes.io/service-account-token其中 {data.token} 就会是我们的用户 token 的 base64 …","date":1653062400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"55f26129bcc066d03b421ebf75cb374f","permalink":"https://lib.jimmysong.io/kubernetes-handbook/config/service-account/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/config/service-account/","section":"kubernetes-handbook","summary":"Service account 为 Pod 中的进程提供身份信息。 本文是关于 Service Account 的用户指南，管理指南另见 Service Account 的集群管理指南 。 注意：本文档描述的关于 Service Account 的行为只有当您按照 Kubernetes 项目建议的方式搭建起集群的情况下才有效。您的集群管理员可能在您的集","tags":["Kubernetes"],"title":"配置 Pod 的 Service Account","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"Secret 对象类型用来保存敏感信息，例如密码、OAuth 令牌和 ssh key。将这些信息放在 secret 中比放在 pod 的定义中或者 docker 镜像中来说更加安全和灵活。\nSecret 概览 Secret 是一种包含少量敏感信息例如密码、token 或 key 的对象。这样的信息可能会被放在 Pod spec 中或者镜像中；将其放在一个 secret 对象中可以更好地控制它的用途，并降低意外暴露的风险。\n用户可以创建 secret，同时系统也创建了一些 secret。\n要使用 secret，pod 需要引用 secret。Pod 可以用两种方式使用 secret：作为 volume 中的文件被挂载到 pod 中的一个或者多个容器里，或者当 kubelet 为 pod 拉取镜像时使用。\n内置 secret Service Account 使用 API 凭证自动创建和附加 secret Kubernetes 自动创建包含访问 API 凭据的 secret，并自动修改您的 pod 以使用此类型的 secret。\n如果需要，可以禁用或覆盖自动创建和使用API凭据。但是，如果您需要的只是安全地访问 apiserver，我们推荐这样的工作流程。\n参阅 Service Account 文档获取关于 Service Account 如何工作的更多信息。\n创建您自己的 Secret 使用 kubectl 创建 Secret 假设有些 pod 需要访问数据库。这些 pod 需要使用的用户名和密码在您本地机器的 ./username.txt 和 ./password.txt 文件里。\n# Create files needed for rest of example. $ echo -n \u0026#34;admin\u0026#34; \u0026gt; ./username.txt $ echo -n \u0026#34;1f2d1e2e67df\u0026#34; \u0026gt; ./password.txt kubectl create secret 命令将这些文件打包到一个 Secret 中并在 API server 中创建了一个对象。\n$ kubectl create secret generic db-user-pass --from-file=./username.txt --from-file=./password.txt secret \u0026#34;db-user-pass\u0026#34; created 您可以这样检查刚创建的 secret：\n$ kubectl get secrets NAME TYPE DATA AGE db-user-pass Opaque 2 51s $ kubectl describe secrets/db-user-pass Name: db-user-pass Namespace: default Labels: \u0026lt;none\u0026gt; Annotations: \u0026lt;none\u0026gt; Type: Opaque Data ==== password.txt: 12 bytes username.txt: 5 bytes 请注意，默认情况下，get 和 describe 命令都不会显示文件的内容。这是为了防止将 secret 中的内容被意外暴露给从终端日志记录中刻意寻找它们的人。\n请参阅 解码 secret 了解如何查看它们的内容。\n手动创建 Secret 您也可以先以 json 或 yaml 格式在文件中创建一个 secret 对象，然后创建该对象。\n每一项必须是 base64 编码：\n$ echo -n \u0026#34;admin\u0026#34; | base64 YWRtaW4= $ echo -n \u0026#34;1f2d1e2e67df\u0026#34; | base64 MWYyZDFlMmU2N2Rm 现在可以像这样写一个 secret 对象：\napiVersion:v1kind:Secretmetadata:name:mysecrettype:Opaquedata:username:YWRtaW4=password:MWYyZDFlMmU2N2Rm数据字段是一个映射。它的键必须匹配 DNS_SUBDOMAIN，前导点也是可以的。这些值可以是任意数据，使用 base64 进行编码。\n使用 kubectl create创建 secret：\n$ kubectl create -f ./secret.yaml secret \u0026#34;mysecret\u0026#34; created 编码注意： secret 数据的序列化 JSON 和 YAML 值使用 base64 编码成字符串。换行符在这些字符串中无效，必须省略。当在Darwin/OS X上使用 base64 实用程序时，用户应避免使用 -b 选项来拆分长行。另外，对于 Linux用户如果 -w 选项不可用的话，应该添加选项 -w 0 到 base64 命令或管道 base64 | tr -d \u0026#39;\\n\u0026#39; 。\n解码 Secret 可以使用 kubectl get secret 命令获取 secret。例如，获取在上一节中创建的 secret：\n$ kubectl get secret mysecret -o yaml apiVersion: v1 data: username: YWRtaW4= password: MWYyZDFlMmU2N2Rm kind: Secret metadata: creationTimestamp: 2016-01-22T18:41:56Z name: mysecret namespace: default resourceVersion: \u0026#34;164619\u0026#34; selfLink: /api/v1/namespaces/default/secrets/mysecret uid: cfee02d6-c137-11e5-8d73-42010af00002 type: Opaque 解码密码字段：\n$ echo \u0026#34;MWYyZDFlMmU2N2Rm\u0026#34; | base64 --decode 1f2d1e2e67df 使用 Secret Secret 可以作为数据卷被挂载，或作为环境变量暴露出来以供 pod 中的容器使用。它们也可以被系统的其他部分使用，而不直接暴露在 pod 内。例如，它们可以保存凭据，系统的其他部分应该用它来代表您与外部系统进行交互。\n在 Pod 中使用 Secret 文件 在 Pod 中的 volume 里使用 Secret：\n 创建一个 secret 或者使用已有的 secret。多个 pod 可以引用同一个 secret。 修改您的 pod 的定义在 spec.volumes[] 下增加一个 volume。可以给这个 volume 随意命名，它的 spec.volumes[].secret.secretName 必须等于 secret 对象的名字。 将 spec.containers[].volumeMounts[] 加到需要用到该 secret 的容器中。指定 spec.containers[].volumeMounts[].readOnly = true 和 spec.containers[].volumeMounts[].mountPath 为您想要该 secret 出现的尚未使用的目录。 修改您的镜像并且／或者命令行让程序从该目录下寻找文件。Secret 的 data 映射中的每一个键都成为了 mountPath 下的一个文件名。  这是一个在 pod 中使用 volume 挂在 secret 的例子：\napiVersion:v1kind:Podmetadata:name:mypodspec:containers:- name:mypodimage:redisvolumeMounts:- name:foomountPath:\u0026#34;/etc/foo\u0026#34;readOnly:truevolumes:- name:foosecret:secretName:mysecret您想要用的每个 secret 都需要在 spec.volumes 中指明。\n如果 pod 中有多个容器，每个容器都需要自己的 volumeMounts 配置块，但是每个 secret 只需要一个 spec.volumes。\n您可以打包多个文件到一个 secret 中，或者使用的多个 secret，怎样方便就怎样来。\n向特性路径映射 secret 密钥\n我们还可以控制 Secret key 映射在 volume 中的路径。您可以使用 spec.volumes[].secret.items 字段修改每个 key 的目标路径：\napiVersion:v1kind:Podmetadata:name:mypodspec:containers:- name:mypodimage:redisvolumeMounts:- name:foomountPath:\u0026#34;/etc/foo\u0026#34;readOnly:truevolumes:- name:foosecret:secretName:mysecretitems:- key:usernamepath:my-group/my-username将会发生什么呢：\n username secret 存储在 /etc/foo/my-group/my-username 文件中而不是 /etc/foo/username 中。 password secret 没有被影射  如果使用了 spec.volumes[].secret.items，只有在 items 中指定的 key 被影射。要使用 secret 中所有的 key，所有这些都必须列在 items 字段中。所有列出的密钥必须存在于相应的 secret 中。否则，不会创建卷。\nSecret 文件权限\n您还可以指定 secret 将拥有的权限模式位文件。如果不指定，默认使用 0644。您可以为整个保密卷指定默认模式，如果需要，可以覆盖每个密钥。\napiVersion:v1kind:Podmetadata:name:mypodspec:containers:- name:mypodimage:redisvolumeMounts:- name:foomountPath:\u0026#34;/etc/foo\u0026#34;volumes:- name:foosecret:secretName:mysecretdefaultMode:256然后，secret 将被挂载到 /etc/foo 目录，所有通过该 secret volume 挂载创建的文件的权限都是 0400。\n请注意，JSON 规范不支持八进制符号，因此使用 256 值作为 0400 权限。如果您使用 yaml 而不是 json 作为 pod，则可以使用八进制符号以更自然的方式指定权限。\n您还可以是用映射，如上一个示例，并为不同的文件指定不同的权限，如下所示：\napiVersion:v1kind:Podmetadata:name:mypodspec:containers:- name:mypodimage:redisvolumeMounts:- name:foomountPath:\u0026#34;/etc/foo\u0026#34;volumes:- name:foosecret:secretName:mysecretitems:- key:usernamepath:my-group/my-usernamemode:511在这种情况下，导致 /etc/foo/my-group/my-username 的文件的权限值为 0777。由于 JSON 限制，必须以十进制格式指定模式。\n请注意，如果稍后阅读此权限值可能会以十进制格式显示。\n从 Volume 中消费 secret 值\n在挂载的 secret volume 的容器内，secret key 将作为文件，并且 secret 的值使用 base-64 解码并存储在这些文件中。这是在上面的示例容器内执行的命令的结果：\n$ ls /etc/foo/ username password $ cat /etc/foo/username …","date":1653062400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"78b395ee5d6ea6ca5b4955851f9b355e","permalink":"https://lib.jimmysong.io/kubernetes-handbook/config/secret/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/config/secret/","section":"kubernetes-handbook","summary":"Secret 对象类型用来保存敏感信息，例如密码、OAuth 令牌和 ssh key。将这些信息放在 secret 中比放在 pod 的定义中或者 docker 镜像中来说更加安全和灵活。 Secret 概览 Secret 是一种包含少量敏感信息例如密码、token 或 key 的对象。这样的信","tags":["Kubernetes"],"title":"Secret 配置","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"当用多个团队或者用户共用同一个集群的时候难免会有资源竞争的情况发生，这时候就需要对不同团队或用户的资源使用配额做出限制。\n开启资源配额限制功能 目前有两种资源分配管理相关的控制策略插件 ResourceQuota 和 LimitRange。\n要启用它们只要 API Server 的启动配置的 KUBE_ADMISSION_CONTROL 参数中加入了 ResourceQuota 的设置，这样就给集群开启了资源配额限制功能，加入 LimitRange 可以用来限制一个资源申请的范围限制，参考 为 namesapce 配置默认的内存请求与限额 和 在 namespace 中配置默认的CPU请求与限额。\n两种控制策略的作用范围都是对于某一 namespace，ResourceQuota 用来限制 namespace 中所有的 Pod 占用的总的资源 request 和 limit，而 LimitRange 是用来设置 namespace 中 Pod 的默认的资源 request 和 limit 值。\n资源配额分为三种类型：\n 计算资源配额 存储资源配额 对象数量配额  关于资源配额的详细信息请参考 Kubernetes 官方文档 资源配额。\n示例 我们为 spark-cluster 这个 namespace 设置 ResouceQuota 和 LimitRange。\n以下 yaml 文件可以在 kubernetes-handbook 的 manifests/spark-with-kubernetes-native-scheduler 目录下找到。\n配置计算资源配额 配置文件：spark-compute-resources.yaml\napiVersion:v1kind:ResourceQuotametadata:name:compute-resourcesnamespace:spark-clusterspec:hard:pods:\u0026#34;20\u0026#34;requests.cpu:\u0026#34;20\u0026#34;requests.memory:100Gilimits.cpu:\u0026#34;40\u0026#34;limits.memory:200Gi要想查看该配置只要执行：\nkubectl -n spark-cluster describe resourcequota compute-resources 配置对象数量限制 配置文件：spark-object-counts.yaml\napiVersion:v1kind:ResourceQuotametadata:name:object-countsnamespace:spark-clusterspec:hard:configmaps:\u0026#34;10\u0026#34;persistentvolumeclaims:\u0026#34;4\u0026#34;replicationcontrollers:\u0026#34;20\u0026#34;secrets:\u0026#34;10\u0026#34;services:\u0026#34;10\u0026#34;services.loadbalancers:\u0026#34;2\u0026#34;配置CPU和内存LimitRange 配置文件：spark-limit-range.yaml\napiVersion:v1kind:LimitRangemetadata:name:mem-limit-rangespec:limits:- default:memory:50Gicpu:5defaultRequest:memory:1Gicpu:1type:Container default 即 limit 的值 defaultRequest 即 request 的值  参考  资源配额 - kubernetes.io 为命名空间配置默认的内存请求与限额 - kubernetes.io 在命名空间中配置默认的 CPU 请求与限额 - kubernetes.io  ","date":1653062400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"b38f0a22584be0da3e83a3bf526273a3","permalink":"https://lib.jimmysong.io/kubernetes-handbook/config/quota/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/config/quota/","section":"kubernetes-handbook","summary":"当用多个团队或者用户共用同一个集群的时候难免会有资源竞争的情况发生，这时候就需要对不同团队或用户的资源使用配额做出限制。 开启资源配额限制功能 目前有两种资源分配管理相关的控制策略插件 ResourceQuota 和 LimitRan","tags":["Kubernetes"],"title":"管理 namespace 中的资源配额","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"对于没有使用过 Kubernetes 的 Docker 用户，如何快速掌握 kubectl 命令？\n在本文中，我们将向 docker-cli 用户介绍 Kubernetes 命令行如何与 api 进行交互。该命令行工具——kubectl，被设计成 docker-cli 用户所熟悉的样子，但是它们之间又存在一些必要的差异。该文档将向您展示每个 docker 子命令和 kubectl 与其等效的命令。\n在使用 kubernetes 集群的时候，docker 命令通常情况是不需要用到的，只有在调试程序或者容器的时候用到，我们基本上使用 kubectl 命令即可，所以在操作 kubernetes 的时候我们抛弃原先使用 docker 时的一些观念。\ndocker run 如何运行一个 nginx Deployment 并将其暴露出来？ 查看 kubectl run 。\n使用 docker 命令：\n$ docker run -d --restart=always -e DOMAIN=cluster --name nginx-app -p 80:80 nginx a9ec34d9878748d2f33dc20cb25c714ff21da8d40558b45bfaec9955859075d0 $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES a9ec34d98787 nginx \u0026#34;nginx -g \u0026#39;daemon of 2 seconds ago Up 2 seconds 0.0.0.0:80-\u0026gt;80/tcp, 443/tcp nginx-app 使用 kubectl 命令：\n# start the pod running nginx $ kubectl run --image=nginx nginx-app --port=80 --env=\u0026#34;DOMAIN=cluster\u0026#34; deployment \u0026#34;nginx-app\u0026#34; created 在大于等于 1.2 版本 Kubernetes 集群中，使用kubectl run 命令将创建一个名为 “nginx-app” 的 Deployment。如果您运行的是老版本，将会创建一个 replication controller。 如果您想沿用旧的行为，使用 --generation=run/v1 参数，这样就会创建 replication controller。查看 kubectl run 获取更多详细信息。\n# expose a port through with a service $ kubectl expose deployment nginx-app --port=80 --name=nginx-http service \u0026#34;nginx-http\u0026#34; exposed 在 kubectl 命令中，我们创建了一个 Deployment，这将保证有 N 个运行 nginx 的 pod（N 代表 spec 中声明的 replica 数，默认为 1）。我们还创建了一个 service，使用 selector 匹配具有相应的 selector 的 Deployment。查看 快速开始 获取更多信息。\n默认情况下镜像会在后台运行，与docker run -d ... 类似，如果您想在前台运行，使用：\nkubectl run [-i] [--tty] --attach \u0026lt;name\u0026gt; --image=\u0026lt;image\u0026gt; 与 docker run ... 不同的是，如果指定了 --attach ，我们将连接到 stdin，stdout 和 stderr，而不能控制具体连接到哪个输出流（docker -a ...）。\n因为我们使用 Deployment 启动了容器，如果您终止了连接到的进程（例如 ctrl-c），容器将会重启，这跟 docker run -it不同。 如果想销毁该 Deployment（和它的 pod），您需要运行 kubectl delete deployment \u0026lt;name\u0026gt;。\ndocker ps 如何列出哪些正在运行？查看 kubectl get。\n使用 docker 命令：\n$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES a9ec34d98787 nginx \u0026#34;nginx -g \u0026#39;daemon of About an hour ago Up About an hour 0.0.0.0:80-\u0026gt;80/tcp, 443/tcp nginx-app 使用 kubectl 命令：\n$ kubectl get po NAME READY STATUS RESTARTS AGE nginx-app-5jyvm 1/1 Running 0 1h docker attach 如何连接到已经运行在容器中的进程？查看 kubectl attach。\n使用 docker 命令：\n$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES a9ec34d98787 nginx \u0026#34;nginx -g \u0026#39;daemon of 8 minutes ago Up 8 minutes 0.0.0.0:80-\u0026gt;80/tcp, 443/tcp nginx-app $docker attach a9ec34d98787 ... 使用 kubectl 命令：\n$ kubectl get pods NAME READY STATUS RESTARTS AGE nginx-app-5jyvm 1/1 Running 0 10m $ kubectl attach -it nginx-app-5jyvm ... docker exec 如何在容器中执行命令？查看 kubectl exec。\n使用 docker 命令：\n$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES a9ec34d98787 nginx \u0026#34;nginx -g \u0026#39;daemon of 8 minutes ago Up 8 minutes 0.0.0.0:80-\u0026gt;80/tcp, 443/tcp nginx-app $docker exec a9ec34d98787 cat /etc/hostname a9ec34d98787 使用 kubectl 命令：\n$ kubectl get po NAME READY STATUS RESTARTS AGE nginx-app-5jyvm 1/1 Running 0 10m $ kubectl exec nginx-app-5jyvm -- cat /etc/hostname nginx-app-5jyvm 执行交互式命令怎么办？\n使用 docker 命令：\n$ docker exec -ti a9ec34d98787 /bin/sh # exit 使用 kubectl 命令：\n$ kubectl exec -ti nginx-app-5jyvm -- /bin/sh # exit docker logs 如何查看运行中进程的 stdout/stderr？查看 kubectl logs。\n使用 docker 命令：\n$ docker logs -f a9e 192.168.9.1 - - [14/Jul/2015:01:04:02 +0000] \u0026#34;GET / HTTP/1.1\u0026#34; 200 612 \u0026#34;-\u0026#34; \u0026#34;curl/7.35.0\u0026#34; \u0026#34;-\u0026#34; 192.168.9.1 - - [14/Jul/2015:01:04:03 +0000] \u0026#34;GET / HTTP/1.1\u0026#34; 200 612 \u0026#34;-\u0026#34; \u0026#34;curl/7.35.0\u0026#34; \u0026#34;-\u0026#34; 使用 kubectl 命令：\n$ kubectl logs -f nginx-app-zibvs 10.240.63.110 - - [14/Jul/2015:01:09:01 +0000] \u0026#34;GET / HTTP/1.1\u0026#34; 200 612 \u0026#34;-\u0026#34; \u0026#34;curl/7.26.0\u0026#34; \u0026#34;-\u0026#34; 10.240.63.110 - - [14/Jul/2015:01:09:02 +0000] \u0026#34;GET / HTTP/1.1\u0026#34; 200 612 \u0026#34;-\u0026#34; \u0026#34;curl/7.26.0\u0026#34; \u0026#34;-\u0026#34; 现在是时候提一下 pod 和容器之间的细微差别了；默认情况下如果 pod 中的进程退出 pod 也不会终止，相反它将会重启该进程。这类似于 docker run 时的 --restart=always 选项， 这是主要差别。在 docker 中，进程的每个调用的输出都是被连接起来的，但是对于 kubernetes，每个调用都是分开的。要查看以前在 kubernetes 中执行的输出，请执行以下操作：\n$ kubectl logs --previous nginx-app-zibvs 10.240.63.110 - - [14/Jul/2015:01:09:01 +0000] \u0026#34;GET / HTTP/1.1\u0026#34; 200 612 \u0026#34;-\u0026#34; \u0026#34;curl/7.26.0\u0026#34; \u0026#34;-\u0026#34; 10.240.63.110 - - [14/Jul/2015:01:09:02 +0000] \u0026#34;GET / HTTP/1.1\u0026#34; 200 612 \u0026#34;-\u0026#34; \u0026#34;curl/7.26.0\u0026#34; \u0026#34;-\u0026#34; 查看 记录和监控集群活动 获取更多信息。\ndocker stop 和 docker rm 如何停止和删除运行中的进程？查看 kubectl delete。\n使用 docker 命令：\n$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES a9ec34d98787 nginx \u0026#34;nginx -g \u0026#39;daemon of 22 hours ago Up 22 hours 0.0.0.0:80-\u0026gt;80/tcp, 443/tcp nginx-app $docker stop a9ec34d98787 a9ec34d98787 $docker rm a9ec34d98787 a9ec34d98787 使用 kubectl 命令：\n$ kubectl get deployment nginx-app NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE nginx-app 1 1 1 1 2m $ kubectl get po -l run=nginx-app NAME READY STATUS RESTARTS AGE nginx-app-2883164633-aklf7 1/1 Running 0 2m $ kubectl delete deployment nginx-app deployment \u0026#34;nginx-app\u0026#34; deleted $ kubectl get po -l run=nginx-app # Return nothing 请注意，我们不直接删除 pod。使用 kubectl 命令，我们要删除拥有该 pod 的 Deployment。如果我们直接删除pod，Deployment 将会重新创建该 pod。\ndocker login 在 kubectl 中没有对 docker login 的直接模拟。如果您有兴趣在私有镜像仓库中使用 Kubernetes，请参阅 使用私有镜像仓库。\ndocker version 如何查看客户端和服务端的版本？查看 kubectl version。\n使用 docker 命令：\n$ docker version Client version: 1.7.0 Client API version: 1.19 Go version (client): …","date":1653062400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"8ccb4abb43d92bd31cf10279162478b9","permalink":"https://lib.jimmysong.io/kubernetes-handbook/cli/docker-cli-to-kubectl/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/cli/docker-cli-to-kubectl/","section":"kubernetes-handbook","summary":"对于没有使用过 Kubernetes 的 Docker 用户，如何快速掌握 kubectl 命令？ 在本文中，我们将向 docker-cli 用户介绍 Kubernetes 命令行如何与 api 进行交互。该命令行工具——kubectl，被设计成 docker-cli 用户所熟悉的样子，但是它们之间又存在一些必要的差异。该文档","tags":["Kubernetes"],"title":"Docker 用户过渡到 kubectl 命令行指南","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"Kubernetes 提供的 kubectl 命令是与集群交互最直接的方式，v1.6 版本的 kubectl 命令参考图如下：\n   kubectl cheatsheet  Kubectl 的子命令主要分为 8 个类别：\n 基础命令（初学者都会使用的） 基础命令（中级） 部署命令 集群管理命令 故障排查和调试命令 高级命令 设置命令 其他命令  熟悉这些命令有助于大家来操作和管理 kubernetes 集群。\n命令行提示 为了使用 kubectl 命令更加高效，我们可以选择安装一下开源软件来增加操作 kubectl 命令的快捷方式，同时为 kubectl 命令增加命令提示。\n   增加kubeclt命令的工具（图片来自网络）   kubectx：用于切换 Kubernetes context kube-ps1：为命令行终端增加$PROMPT字段 kube-shell：交互式带命令提示的 kubectl 终端  全部配置完成后的 kubectl 终端如下图所示：\n   增强的kubectl命令  开源项目 kube-shell 可以为 kubectl 提供自动的命令提示和补全，使用起来特别方便，推荐给大家。\nKube-shell 有以下特性：\n 命令提示，给出命令的使用说明 自动补全，列出可选命令并可以通过 tab 键自动补全，支持模糊搜索 高亮 使用 tab 键可以列出可选的对象 vim 模式  Mac 下安装\npip install kube-shell --user -U    kube-shell页面  kubectl 的身份认证 Kubernetes 中存在三种安全认证方式：\n CA 证书：API server 与其它几个组件之间都是通过这种方式认证的 HTTP base：即在 API server 的启动参数中指定的 --token-auth-file=/etc/kubernetes/token.csv 文件中明文的用户、组、密码和 UID 配置 bearer token：HTTP 请求中 header 中传递的 Autorization:Bearer token，这个 token 通常保存在创建角色跟 serviceaccount 绑定的时候生成的 secret 中。  kubectl 通过读取 kubeconfig 文件中的配置信息在向 API server 发送请求的时候同时传递认证信息，同时支持 CA 证书和 bearer token 的认证方式。\n终端下 kubectl 命令自动补全 建议使用 oh-my-zsh，增加对 kubectl 命令自动补全支持。\n修改 ~/.zshrc 文件，增加如下两行：\nplugins=(kubectl) source \u0026lt;(kubectl completion zsh) 保存后重启终端即可生效。\n参考  Install and Set Up kubectl - kubernetes.io  ","date":1653062400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"843bf146e5ee4fe8428b89a68840dd0b","permalink":"https://lib.jimmysong.io/kubernetes-handbook/cli/using-kubectl/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/cli/using-kubectl/","section":"kubernetes-handbook","summary":"Kubernetes 提供的 kubectl 命令是与集群交互最直接的方式，v1.6 版本的 kubectl 命令参考图如下： kubectl cheatsheet Kubectl 的子命令主要分为 8 个类别： 基础命令（初学者都会使用的） 基础命令（中级） 部署命令 集群管理命令 故障排查和调试命令 高级命令 设置命","tags":["Kubernetes"],"title":"Kubectl 命令概览","type":"book"},{"authors":null,"categories":["Istio"],"content":"EnvoyFilter 提供了一种机制来定制 Istio Pilot 生成的 Envoy 配置。使用 EnvoyFilter 来修改某些字段的值，添加特定的过滤器，甚至添加全新的 listener、cluster 等。这个功能必须谨慎使用，因为不正确的配置可能破坏整个网格的稳定性。与其他 Istio 网络对象不同，EnvoyFilter 是累加应用。对于特定命名空间中的特定工作负载，可以存在任意数量的 EnvoyFilter。这些 EnvoyFilter 的应用顺序如下：配置根命名空间中的所有 EnvoyFilter，其次是工作负载命名空间中的所有匹配 EnvoyFilter。\n注意一 该 API 的某些方面与 Istio 网络子系统的内部实现以及 Envoy 的 xDS API 有很深的关系。虽然 EnvoyFilter API 本身将保持向后兼容，但通过该机制提供的任何 Envoy 配置应在 Istio 代理版本升级时仔细审查，以确保废弃的字段被适当地删除和替换。\n注意二 当多个 EnvoyFilter 被绑定到特定命名空间的同一个工作负载时，所有补丁将按照创建顺序处理。如果多个 EnvoyFilter 的配置相互冲突，则其行为将无法确定。\n注意三 要将 EnvoyFilter 资源应用于系统中的所有工作负载（sidecar 和 gateway）上，请在 config 根命名空间中定义该资源，不要使用 workloadSelector。\n示例 下面的例子在名为 istio-config 的根命名空间中声明了一个全局默认的 EnvoyFilter 资源，在系统中的所有 sidecar 上添加了一个自定义的协议过滤器，用于 outbound 端口 9307。该过滤器应在终止 tcp_proxy 过滤器之前添加，以便生效。此外，它为 gateway 和 sidecar 的所有 HTTP 连接设置了 30 秒的空闲超时。\napiVersion:networking.istio.io/v1alpha3kind:EnvoyFiltermetadata:name:custom-protocolnamespace:istio-config# 如在 meshConfig 资源中定义的。spec:configPatches:- applyTo:NETWORK_FILTERmatch:context:SIDECAR_OUTBOUND# 将会匹配所有 sidecar 中的所有 outbound listenerlistener:portNumber:9307filterChain:filter:name:\u0026#34;envoy.filters.network.tcp_proxy\u0026#34;patch:operation:INSERT_BEFOREvalue:# 这是完整的过滤器配置，包括名称和 typed_config 部分。name:\u0026#34;envoy.config.filter.network.custom_protocol\u0026#34;typed_config:...- applyTo:NETWORK_FILTER# HTTP 连接管理器是 Envoy 的一个过滤器。match:# 省略了上下文，因此这同时适用于 sidecar 和 gateway。listener:filterChain:filter:name:\u0026#34;envoy.filters.network.http_connection_manager\u0026#34;patch:operation:MERGEvalue:name:\u0026#34;envoy.filters.network.http_connection_manager\u0026#34;typed_config:\u0026#34;@type\u0026#34;: \u0026#34;type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\u0026#34;common_http_protocol_options:idle_timeout:30s下面的例子启用了 Envoy 的 Lua 过滤器，用于处理所有到达 bookinfo 命名空间中的对 reviews 服务 pod 的 8080 端口的 HTTP 调用，标签为 app: reviews。Lua 过滤器调用外部服务internal.org.net:8888，这需要在 Envoy 中定义一个特殊的 cluster。该 cluster 也被添加到 sidecar 中，作为该配置的一部分。\napiVersion:networking.istio.io/v1alpha3kind:EnvoyFiltermetadata:name:reviews-luanamespace:bookinfospec:workloadSelector:labels:app:reviewsconfigPatches:# 第一个补丁将 lua过 滤器添加到监听器/http 连接管理器。- applyTo:HTTP_FILTERmatch:context:SIDECAR_INBOUNDlistener:portNumber:8080filterChain:filter:name:\u0026#34;envoy.filters.network.http_connection_manager\u0026#34;subFilter:name:\u0026#34;envoy.filters.http.router\u0026#34;patch:operation:INSERT_BEFOREvalue:# lua 过滤器配置name:envoy.luatyped_config:\u0026#34;@type\u0026#34;: \u0026#34;type.googleapis.com/envoy.extensions.filters.http.lua.v3.Lua\u0026#34;inlineCode:|function envoy_on_request(request_handle) -- 向上游主机进行 HTTP 调用，header、body 和 time欧特 如下。 local headers, body = request_handle:httpCall( \u0026#34;lua_cluster\u0026#34;, { [\u0026#34;:method\u0026#34;] = \u0026#34;POST\u0026#34;, [\u0026#34;:path\u0026#34;] = \u0026#34;/acl\u0026#34;, [\u0026#34;:authority\u0026#34;] = \u0026#34;internal.org.net\u0026#34; }, \u0026#34;authorize call\u0026#34;, 5000) end# 第二个补丁添加了被 lua 代码引用的 cluster，cds 匹配被省略，因为正在添加一个新的 cluster。- applyTo:CLUSTERmatch:context:SIDECAR_OUTBOUNDpatch:operation:ADDvalue:# cluster 配置name:\u0026#34;lua_cluster\u0026#34;type:STRICT_DNSconnect_timeout:0.5slb_policy:ROUND_ROBINload_assignment:cluster_name:lua_clusterendpoints:- lb_endpoints:- endpoint:address:socket_address:protocol:TCPaddress:\u0026#34;internal.org.net\u0026#34;port_value:8888下面的例子覆盖了 SNI 主机 app.example.com 在 istio-system 命名空间的 ingress gateway 的监听器中的 HTTP 连接管理器的某些字段（HTTP 空闲超时和X-Forward-For信任跳数）。\napiVersion:networking.istio.io/v1alpha3kind:EnvoyFiltermetadata:name:hcm-tweaksnamespace:istio-systemspec:workloadSelector:labels:istio:ingressgatewayconfigPatches:- applyTo:NETWORK_FILTER# HTTP 连接管理器是 Envoy 中的一个过滤器。 match:context:GATEWAYlistener:filterChain:sni:app.example.comfilter:name:\u0026#34;envoy.filters.network.http_connection_manager\u0026#34;patch:operation:MERGEvalue:typed_config:\u0026#34;@type\u0026#34;: \u0026#34;type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\u0026#34;xff_num_trusted_hops:5common_http_protocol_options:idle_timeout:30s下面的例子插入了一个产生 istio_operationId 属性的 attributegen 过滤器，该属性被 istio.stats fiter 消费。filterClass:STATS 对这种依赖关系进行编码。\napiVersion:networking.istio.io/v1alpha3kind:EnvoyFiltermetadata:name:reviews-request-operationnamespace:mynsspec:workloadSelector:labels:app:reviewsconfigPatches:- applyTo:HTTP_FILTERmatch:context:SIDECAR_INBOUNDpatch:operation:ADDfilterClass:STATS# 这个过滤器将在 Istio 统计过滤器之前运行。value:name:istio.request_operationtyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/udpa.type.v1.TypedStructtype_url:type.googleapis.com/envoy.extensions.filters.http.wasm.v3.Wasmvalue:config:configuration:|{ \u0026#34;attributes\u0026#34;: [ { \u0026#34;output_attribute\u0026#34;: \u0026#34;istio_operationId\u0026#34;, \u0026#34;match\u0026#34;: [ { \u0026#34;value\u0026#34;: \u0026#34;ListReviews\u0026#34;, \u0026#34;condition\u0026#34;: \u0026#34;request.url_path == \u0026#39;/reviews\u0026#39; \u0026amp;\u0026amp; request.method == \u0026#39;GET\u0026#39;\u0026#34; }] }] }vm_config:runtime:envoy.wasm.runtime.nullcode:local:{inline_string:\u0026#34;envoy.wasm.attributegen\u0026#34;}下面的例子在 myns 命名空间中插入了一个 http ext_authz 过滤器。\napiVersion:networking.istio.io/v1alpha3kind:EnvoyFiltermetadata:name:myns-ext-authznamespace:mynsspec:configPatches:- applyTo:HTTP_FILTERmatch:context:SIDECAR_INBOUNDpatch:operation:ADDfilterClass:AUTHZ# 该过滤器将在 Istio authz 过滤器之后运行。value:name:envoy.filters.http.ext_authztyped_config:\u0026#34;@type\u0026#34;: …","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"8df10c02c27e968510d23a906da62472","permalink":"https://lib.jimmysong.io/istio-handbook/config-networking/envoy-filter/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/istio-handbook/config-networking/envoy-filter/","section":"istio-handbook","summary":"EnvoyFilter 提供了一种机制来定制 Istio Pilot 生成的 Envoy 配置。使用 EnvoyFilter 来修改某些字段的值，添加特定的过滤器，甚至添加全新的 listener、cluster 等。这个功能必须谨慎使用，因为不正确的配置可能破坏整个网格的稳定性。与","tags":["Istio","Service Mesh"],"title":"EnvoyFilter","type":"book"},{"authors":null,"categories":["Istio"],"content":"本文以 Istio 官方的 bookinfo 示例来讲解在进入 Pod 的流量被 iptables 转交给 Envoy sidecar 后，Envoy 是如何做路由转发的，详述了 Inbound 和 Outbound 处理过程。\n下面是 Istio 官方提供的 bookinfo 的请求流程图，假设 bookinfo 应用的所有服务中没有配置 DestinationRule。\n   Bookinfo 示例  我们将要解析的是 reviews-v1 这个 Pod 中的 Inbound 和 Outbound 流量。\n理解 Inbound Handler Inbound Handler 的作用是将 iptables 拦截到的 downstream 的流量转发给 Pod 内的应用程序容器。在我们的实例中，假设其中一个 Pod 的名字是 reviews-v1-545db77b95-jkgv2，运行 istioctl proxy-config listener reviews-v1-545db77b95-jkgv2 --port 15006 查看该 Pod 中 15006 端口上的监听器情况 ，你将看到下面的输出。\nADDRESS PORT MATCH DESTINATION 0.0.0.0 15006 Addr: *:15006 Non-HTTP/Non-TCP 0.0.0.0 15006 Trans: tls; App: istio-http/1.0,istio-http/1.1,istio-h2; Addr: 0.0.0.0/0 InboundPassthroughClusterIpv4 0.0.0.0 15006 Trans: raw_buffer; App: http/1.1,h2c; Addr: 0.0.0.0/0 InboundPassthroughClusterIpv4 0.0.0.0 15006 Trans: tls; App: TCP TLS; Addr: 0.0.0.0/0 InboundPassthroughClusterIpv4 0.0.0.0 15006 Trans: raw_buffer; Addr: 0.0.0.0/0 InboundPassthroughClusterIpv4 0.0.0.0 15006 Trans: tls; Addr: 0.0.0.0/0 InboundPassthroughClusterIpv4 0.0.0.0 15006 Trans: tls; App: istio,istio-peer-exchange,istio-http/1.0,istio-http/1.1,istio-h2; Addr: *:9080 Cluster: inbound|9080|| 0.0.0.0 15006 Trans: raw_buffer; Addr: *:9080 Cluster: inbound|9080|| 下面列出了以上输出中各字段的含义：\n ADDRESS：下游地址 PORT：Envoy 监听器监听的端口 MATCH：请求使用的传输协议或匹配的下游地址 DESTINATION：路由目的地  reviews Pod 中的 Iptables 将入站流量劫持到 15006 端口上，从上面的输出我们可以看到 Envoy 的 Inbound Handler 在 15006 端口上监听，对目的地为任何 IP 的 9080 端口的请求将路由到 inbound|9080|| Cluster 上。\n从该 Pod 的 Listener 列表的最后两行中可以看到，0.0.0.0:15006/TCP 的 Listener（其实际名字是 virtualInbound）监听所有的 Inbound 流量，其中包含了匹配规则，来自任意 IP 的对 9080 端口的访问流量，将会路由到 inbound|9080|| Cluster，如果你想以 Json 格式查看该 Listener 的详细配置，可以执行 istioctl proxy-config listeners reviews-v1-545db77b95-jkgv2 --port 15006 -o json 命令，你将获得类似下面的输出。\n[ /*省略部分内容*/ { \u0026#34;name\u0026#34;: \u0026#34;virtualInbound\u0026#34;, \u0026#34;address\u0026#34;: { \u0026#34;socketAddress\u0026#34;: { \u0026#34;address\u0026#34;: \u0026#34;0.0.0.0\u0026#34;, \u0026#34;portValue\u0026#34;: 15006 } }, \u0026#34;filterChains\u0026#34;: [ /*省略部分内容*/ { \u0026#34;filterChainMatch\u0026#34;: { \u0026#34;destinationPort\u0026#34;: 9080, \u0026#34;transportProtocol\u0026#34;: \u0026#34;tls\u0026#34;, \u0026#34;applicationProtocols\u0026#34;: [ \u0026#34;istio\u0026#34;, \u0026#34;istio-peer-exchange\u0026#34;, \u0026#34;istio-http/1.0\u0026#34;, \u0026#34;istio-http/1.1\u0026#34;, \u0026#34;istio-h2\u0026#34; ] }, \u0026#34;filters\u0026#34;: [ /*省略部分内容*/ { \u0026#34;name\u0026#34;: \u0026#34;envoy.filters.network.http_connection_manager\u0026#34;, \u0026#34;typedConfig\u0026#34;: { \u0026#34;@type\u0026#34;: \u0026#34;type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\u0026#34;, \u0026#34;statPrefix\u0026#34;: \u0026#34;inbound_0.0.0.0_9080\u0026#34;, \u0026#34;routeConfig\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;inbound|9080||\u0026#34;, \u0026#34;virtualHosts\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;inbound|http|9080\u0026#34;, \u0026#34;domains\u0026#34;: [ \u0026#34;*\u0026#34; ], \u0026#34;routes\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;match\u0026#34;: { \u0026#34;prefix\u0026#34;: \u0026#34;/\u0026#34; }, \u0026#34;route\u0026#34;: { \u0026#34;cluster\u0026#34;: \u0026#34;inbound|9080||\u0026#34;, \u0026#34;timeout\u0026#34;: \u0026#34;0s\u0026#34;, \u0026#34;maxStreamDuration\u0026#34;: { \u0026#34;maxStreamDuration\u0026#34;: \u0026#34;0s\u0026#34;, \u0026#34;grpcTimeoutHeaderMax\u0026#34;: \u0026#34;0s\u0026#34; } }, \u0026#34;decorator\u0026#34;: { \u0026#34;operation\u0026#34;: \u0026#34;reviews.default.svc.cluster.local:9080/*\u0026#34; } } ] } ], \u0026#34;validateClusters\u0026#34;: false }, /*省略部分内容*/ } } ], /*省略部分内容*/ ], \u0026#34;listenerFilters\u0026#34;: [ /*省略部分内容*/ ], \u0026#34;listenerFiltersTimeout\u0026#34;: \u0026#34;0s\u0026#34;, \u0026#34;continueOnListenerFiltersTimeout\u0026#34;: true, \u0026#34;trafficDirection\u0026#34;: \u0026#34;INBOUND\u0026#34; } ] 既然 Inbound Handler 的流量中将来自任意地址的对该 Pod 9080 端口的流量路由到 inbound|9080|| Cluster，那么我们运行 istioctl pc cluster reviews-v1-545db77b95-jkgv2 --port 9080 --direction inbound -o json 查看下该 Cluster 配置，你将获得类似下面的输出。\n[ { \u0026#34;name\u0026#34;: \u0026#34;inbound|9080||\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;ORIGINAL_DST\u0026#34;, \u0026#34;connectTimeout\u0026#34;: \u0026#34;10s\u0026#34;, \u0026#34;lbPolicy\u0026#34;: \u0026#34;CLUSTER_PROVIDED\u0026#34;, \u0026#34;circuitBreakers\u0026#34;: { \u0026#34;thresholds\u0026#34;: [ { \u0026#34;maxConnections\u0026#34;: 4294967295, \u0026#34;maxPendingRequests\u0026#34;: 4294967295, \u0026#34;maxRequests\u0026#34;: 4294967295, \u0026#34;maxRetries\u0026#34;: 4294967295, \u0026#34;trackRemaining\u0026#34;: true } ] }, \u0026#34;cleanupInterval\u0026#34;: \u0026#34;60s\u0026#34;, \u0026#34;upstreamBindConfig\u0026#34;: { \u0026#34;sourceAddress\u0026#34;: { \u0026#34;address\u0026#34;: \u0026#34;127.0.0.6\u0026#34;, \u0026#34;portValue\u0026#34;: 0 } }, \u0026#34;metadata\u0026#34;: { \u0026#34;filterMetadata\u0026#34;: { \u0026#34;istio\u0026#34;: { \u0026#34;services\u0026#34;: [ { \u0026#34;host\u0026#34;: \u0026#34;reviews.default.svc.cluster.local\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;reviews\u0026#34;, \u0026#34;namespace\u0026#34;: \u0026#34;default\u0026#34; } ] } } } } ] 我们看其中的 TYPE 为 ORIGINAL_DST，将流量发送到原始目标地址（Pod IP），因为原始目标地址即当前 Pod，你还应该注意到 upstreamBindConfig.sourceAddress.address 的值被改写为了 127.0.0.6，而且对于 Pod 内流量是通过 lo 网卡发送的，这刚好呼应了上文中的 iptables ISTIO_OUTPUT 链中的第一条规则，根据该规则，流量将被透传到 Pod 内的应用容器。\n理解 Outbound Handler 在本示例中 reviews 会向 ratings 服务发送 HTTP 请求，请求的地址是：http://ratings.default.svc.cluster.local:9080/，Outbound Handler 的作用是将 iptables 拦截到的本地应用程序向外发出的流量，经由 Envoy 代理路由到上游。\nEnvoy 监听在 15001 端口上监听所有 Outbound 流量，Outbound Handler 处理，然后经过 virtualOutbound Listener、0.0.0.0_9080 Listener，然后通过 Route 9080 找到上游的 cluster，进而通过 EDS 找到 Endpoint 执行路由动作。\nratings.default.svc.cluster.local:9080 路由\n运行 istioctl proxy-config routes reviews-v1-545db77b95-jkgv2 --name 9080 -o json 查看 route 配置，因为 sidecar 会根据 HTTP header 中的 domains 来匹配 VirtualHost，所以下面只列举了 ratings.default.svc.cluster.local:9080 这一个 VirtualHost。\n[ { \u0026#34;name\u0026#34;: \u0026#34;9080\u0026#34;, \u0026#34;virtualHosts\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;ratings.default.svc.cluster.local:9080\u0026#34;, \u0026#34;domains\u0026#34;: [ \u0026#34;ratings.default.svc.cluster.local\u0026#34;, \u0026#34;ratings.default.svc.cluster.local:9080\u0026#34;, \u0026#34;ratings\u0026#34;, \u0026#34;ratings:9080\u0026#34;, \u0026#34;ratings.default.svc\u0026#34;, \u0026#34;ratings.default.svc:9080\u0026#34;, \u0026#34;ratings.default\u0026#34;, \u0026#34;ratings.default:9080\u0026#34;, \u0026#34;10.8.8.106\u0026#34;, \u0026#34;10.8.8.106:9080\u0026#34; ], \u0026#34;routes\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;match\u0026#34;: { \u0026#34;prefix\u0026#34;: \u0026#34;/\u0026#34; }, \u0026#34;route\u0026#34;: { \u0026#34;cluster\u0026#34;: …","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"38fa563aab9a11c0704587df31bedca8","permalink":"https://lib.jimmysong.io/istio-handbook/concepts/sidecar-traffic-routing-deep-dive/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/istio-handbook/concepts/sidecar-traffic-routing-deep-dive/","section":"istio-handbook","summary":"本文以 Istio 官方的 bookinfo 示例来讲解在进入 Pod 的流量被 iptables 转交给 Envoy sidecar 后，Envoy 是如何做路由转发的，详述了 Inbound 和 Outbound 处理过程。 下面是 Istio 官方提供的 bookinfo 的请求流程图，假设 bookinfo 应用的所有服务中没有配置 DestinationRu","tags":["Istio","Service Mesh"],"title":"Istio 中的 Sidecar 的流量路由详解","type":"book"},{"authors":null,"categories":["Istio"],"content":"SDS（秘钥发现服务）是 Envoy 1.8.0 版本起开始引入的服务。可以在 bootstrap.static_resource 的 secret 配置中为 Envoy 指定 TLS 证书（secret）。也可以通过秘钥发现服务（SDS）远程获取。Istio 预计将在 1.1 版本中支持 SDS。\nSDS 带来的最大的好处就是简化证书管理。要是没有该功能的话，我们就必须使用 Kubernetes 中的 secret 资源创建证书，然后把证书挂载到代理容器中。如果证书过期，还需要更新 secret 和需要重新部署代理容器。使用 SDS，中央 SDS 服务器将证书推送到所有 Envoy 实例上。如果证书过期，服务器只需将新证书推送到 Envoy 实例，Envoy 可以立即使用新证书而无需重新部署。\n如果 listener server 需要从远程获取证书，则 listener server 不会被标记为 active 状态，在获取证书之前不会打开其端口。如果 Envoy 由于连接失败或错误的响应数据而无法获取证书，则 listener server 将被标记为 active，并且打开端口，但是将重置与端口的连接。\n上游集群的处理方式类似，如果需要通过 SDS 从远程获取集群客户端证书，则不会将其标记为 active 状态，在获得证书之前也它不会被使用。如果 Envoy 由于连接失败或错误的响应数据而无法获取证书，则集群将被标记为 active，可以处理请求，但路由到该集群的请求都将被拒绝。\n使用 SDS 的静态集群需定义 SDS 集群（除非使用不需要集群的 Google gRPC），则必须在使用静态集群之前定义 SDS 集群。\nEnvoy 代理和 SDS 服务器之间的连接必须是安全的。可以在同一主机上运行 SDS 服务器，使用 Unix Domain Socket 进行连接。否则，需要代理和 SDS 服务器之间的 mTLS。在这种情况下，必须静态配置 SDS 连接的客户端证书。\nSDS Server SDS server 需要实现 SecretDiscoveryService 这个 gRPC 服务。遵循与其他 xDS 相同的协议。\nSDS 配置 SDS 支持静态配置也支持动态配置。\n静态配置\n可以在static_resources 的 secrets 中配置 TLS 证书。\n动态配置\n从远程 SDS server 获取 secret。\n 通过 Unix Domain Socket 访问 gRPC SDS server。 通过 UDS 访问 gRPC SDS server。 通过 Envoy gRPC 访问 SDS server。  ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"025de10fb2ee2f3d8fefd8522eeea6e8","permalink":"https://lib.jimmysong.io/istio-handbook/data-plane/envoy-sds/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/istio-handbook/data-plane/envoy-sds/","section":"istio-handbook","summary":"SDS（秘钥发现服务）是 Envoy 1.8.0 版本起开始引入的服务。可以在 bootstrap.static_resource 的 secret 配置中为 Envoy 指定 TLS 证书（secret）。也可以通过秘钥发现服务（SDS）远程获取。Istio 预计将在 1.1 版本中支持 SDS。 SDS 带来的最大的好处就","tags":["Istio","Service Mesh"],"title":"SDS（秘钥发现服务）","type":"book"},{"authors":null,"categories":["Istio"],"content":"在前面，我们了解了如何利用流量的比例（weight 字段）在多个子集之间进行流量路由。在某些情况下，纯粹的基于权重的流量路由或分割已经足够了。然而，在有些场景和情况下，我们可能需要对流量如何被分割和转发到目标服务进行更细化的控制。\nIstio 允许我们使用传入请求的一部分，并将其与定义的值相匹配。例如，我们可以匹配传入请求的 URI前缀，并基于此路由流量。\n   属性 描述     uri 将请求 URI 与指定值相匹配   schema 匹配请求的 schema（HTTP、HTTPS…）   method 匹配请求的 method（GET、POST…）   authority 匹配请求 authority 头   headers 匹配请求头。头信息必须是小写的，并以连字符分隔（例如：x-my-request-id）。注意，如果我们使用头信息进行匹配，其他属性将被忽略（uri、schema、method、authority）。    上述每个属性都可以用这些方法中的一种进行匹配：\n  精确匹配：例如，exact: \u0026#34;value\u0026#34; 匹配精确的字符串\n  前缀匹配：例如，prefix: \u0026#34;value\u0026#34; 只匹配前缀\n  这则匹配：例如，regex：\u0026#34;value\u0026#34; 根据 ECMAscript 风格的正则进行匹配\n  例如，假设请求的 URI 看起来像这样：https://dev.example.com/v1/api。为了匹配该请求的 URI，我们会这样写：\nhttp:- match:- uri:prefix:/v1上述片段将匹配传入的请求，并且请求将被路由到该路由中定义的目的地。\n另一个例子是使用正则并在头上进行匹配。\nhttp:- match:- headers:user-agent:regex:\u0026#39;.*Firefox.*\u0026#39;上述匹配将匹配任何用户代理头与 Regex 匹配的请求。\n重定向和重写请求 在头信息和其他请求属性上进行匹配是有用的，但有时我们可能需要通过请求 URI 中的值来匹配请求。\n例如，让我们考虑这样一种情况：传入的请求使用 /v1/api 路径，而我们想把请求路由到 /v2/api 端点。\n这样做的方法是重写所有传入的请求和与 /v1/api 匹配的 authority/host headers 到 /v2/api。\n例如：\n...http:- match:- uri:prefix:/v1/apirewrite:uri:/v2/apiroute:- destination:host:customers.default.svc.cluster.local...即使目标服务不在 /v1/api 端点上监听，Envoy 也会将请求重写到 /v2/api。\n我们还可以选择将请求重定向或转发到一个完全不同的服务。下面是我们如何在头信息上进行匹配，然后将请求重定向到另一个服务：\n...http:- match:- headers:my-header:exact:helloredirect:uri:/helloauthority:my-service.default.svc.cluster.local:8000...redirect 和 destination 字段是相互排斥的。如果我们使用 redirect，就不需要设置 destination。\nAND 和 OR 语义 在进行匹配时，我们可以使用 AND 和 OR 两种语义。让我们看一下下面的片段：\n...http:- match:- uri:prefix:/v1headers:my-header:exact:hello...上面的片段使用的是 AND 语义。这意味着 URI 前缀需要与 /v1 相匹配，并且头信息 my-header 有一个确切的值 hello。\n要使用 OR 语义，我们可以添加另一个 match 项，像这样：\n...http:- match:- uri:prefix:/v1...- match:- headers:my-header:exact:hello...在上面的例子中，将首先对 URI 前缀进行匹配，如果匹配，请求将被路由到目的地。如果第一个不匹配，算法会转移到第二个，并尝试匹配头。如果我们省略路由上的匹配字段，它将总是评估为 true。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"6ea2e19e7015c60c1d5dd880bc91fa92","permalink":"https://lib.jimmysong.io/istio-handbook/traffic-management/advanced-routing/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/istio-handbook/traffic-management/advanced-routing/","section":"istio-handbook","summary":"在前面，我们了解了如何利用流量的比例（weight 字段）在多个子集之间进行流量路由。在某些情况下，纯粹的基于权重的流量路由或分割已经足够了。然而，在有些场景和情况下，我们可能需要对流量如何被分割和转发","tags":["Istio","Service Mesh"],"title":"高级路由","type":"book"},{"authors":null,"categories":["Envoy"],"content":"/healthcheck/fail 可用于失败的入站健康检查。端点 /healthcheck/ok 用于恢复失败的端点。\n这两个端点都需要使用 HTTP 健康检查过滤器。我们可能会在关闭服务器前或进行完全重启时使用它来排空服务器。当调用失败的健康检查选项时，所有的健康检查都将失败，无论其配置如何。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"388d87ab67b55bde59b0cd25d27ab37b","permalink":"https://lib.jimmysong.io/envoy-handbook/admin-interface/healthchecks/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/envoy-handbook/admin-interface/healthchecks/","section":"envoy-handbook","summary":"/healthcheck/fail 可用于失败的入站健康检查。端点 /healthcheck/ok 用于恢复失败的端点。 这两个端点都需要使用 HTTP 健康检查过滤器。我们可能会在关闭服务器前或进行完全重启时使用它来排空服务器。当调用失败的健康检查选项时，所有的健康检查都将失","tags":["Envoy"],"title":"健康检查","type":"book"},{"authors":null,"categories":["Envoy"],"content":"在这个实验中，我们将学习如何配置原始目的地过滤器。要做到这一点，我们需要启用 IP 转发，然后更新 iptables 规则，以捕获所有流量并将其重定向到 Envoy 正在监听的端口。\n我们将使用一个 Linux 虚拟机，而不是 Google Cloud Shell。\n让我们从启用 IP 转发开始。\n# 启用IP转发功能 sudo sysctl -w net.ipv4.ip_forward=1 接下来，我们需要配置 iptables 来捕获所有发送到 80 端口的流量，并将其重定向到 10000 端口。Envoy 代理将在 10000 端口进行监听。\n首先，我们需要确定我们将在 iptables 命令中使用的网络接口名称。我们可以使用 ip link show 命令列出网络接口。例如：\njimmy@instance-1:~$ ip link show 1: lo: \u0026lt; LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00 2: ens4: \u0026lt; BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1460 qdisc mq state UP mode DEFAULT group default qlen 1000 link/ether 42:01:0a:8a:00:2e brd ff:ff:ff:ff:ff:ff 输出结果告诉我们，我们有两个网络接口：环回接口和一个名为 ens4 的接口。这是我们将在 iptables 命令中使用的接口名称。\n# 捕获所有来自外部的80端口的流量并将其重定向到10000端口 sudo iptables -t nat -A PREROUTING -i ens4 -p tcp --dport 80 -j REDIRECT --to-ports 10000 最后，我们将运行另一条 iptables to 命令，防止从虚拟机发出请求时出现路由循环。设置这个规则将允许我们从虚拟机上运行 curl tetrate.io，并且仍然被重定向到 10000 端口。\n# 使我们能够从同一个实例中运行curl（即防止路由循环）。 sudo iptables -t nat -A OUTPUT -p tcp -m owner ! --uid-owner root --dport 80 --j REDIRECT --to-ports 10000 在修改了 iptables 规则后，我们可以创建以下 Envoy 配置。\nstatic_resources:listeners:- name:inboundaddress:socket_address:address:0.0.0.0port_value:10000listener_filters:- name:envoy.filters.listener.original_dsttyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.listener.original_dst.v3.OriginalDstfilter_chains:- filters:- name:envoy.filters.network.http_connection_managertyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManagerstat_prefix:ingress_httpaccess_log:- name:envoy.access_loggers.filetyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.access_loggers.file.v3.FileAccessLogpath:./envoy.loghttp_filters:- name:envoy.filters.http.routerroute_config:virtual_hosts:- name:proxydomains:[\u0026#34;*\u0026#34;]routes:- match:prefix:\u0026#34;/\u0026#34;route:cluster:original_dst_clusterclusters:- name:original_dst_clustertype:ORIGINAL_DSTconnect_timeout:5slb_policy:CLUSTER_PROVIDEDoriginal_dst_lb_config:use_http_header:true这个配置看起来与我们已经看到的配置相似。我们在 listenener_filters 中添加了 original_dst 过滤器，启用了对一个文件的访问日志，并将所有流量路由到一个叫做 original_dst_cluster 的集群。这个集群的类型设置为 ORIGINAL_DST，将请求发送到原始目的地。\n此外，我们将 use_http_header 字段设置为 true。当设置为 true 时，我们可以使用 x-envoy-original-dst-host 头来覆盖目标地址。请注意，这个标头默认情况下是没有经过处理的，所以启用它允许将流量路由到任意的主机，这可能会产生安全问题。我们在这里只是把它作为一个例子。\n   原始DST过滤器  对于透明代理的情况，这就是我们所需要的。我们不希望做任何解析。我们希望将请求代理到原始目的地。\n将上述 YAML 保存为 5-lab-1-originaldst.yaml。\n为了运行它，我们将使用 func-e CLI。让我们在虚拟机上安装 CLI。\ncurl https://func-e.io/install.sh | sudo bash -s -- -b /usr/local/bin 现在我们可以用我们创建的配置运行 Envoy 代理。\nsudo func-e run -c 5-lab-1-originaldst.yaml  注意，在这种情况下，我们用 sudo 运行 func-e，所以我们可以用同一台机器来测试代理，防止路由循环（见第二条 iptables 规则）。\n 我们可以向 tetrate.io 发送一个请求，如果我们查看 envoy.log 文件，我们会看到以下条目。\n[2021-07-07T21:22:57.294Z] \u0026#34;GET / HTTP/1.1\u0026#34; 301 - 0 227 34 34 \u0026#34;-\u0026#34; \u0026#34;curl/7.64.0\u0026#34; \u0026#34;5fd04969-27b0-4d37-b56c-c273a410da46\u0026#34; \u0026#34;tedrate.io\u0026#34; \u0026#34;75.119.195.116:80\u0026#34; 日志条目显示，iptables 捕获了该请求，并将其重定向到 Envoy 正在监听的端口 10000 。然后，Envoy 将该请求代理到原来的目的地。\n我们也可以从虚拟机的外部提出请求。从第二个终端：这一次，我们使用 Google Cloud Shell，而且我们不在虚拟机中。我们可以向虚拟机的 IP 地址发送请求，并提供 x-envoy-original-dst-host 头，我们希望 Envoy 将请求发送给该 IP 地址。\n 我在这个例子中使用 google.com。要获得 IP 地址，你可以运行 nslookup google.com 并使用该命令中的 IP 地址。\n $ curl -H \u0026#34;x-envoy-original-dst-host: 74.125.199.139\u0026#34; [vm-ip-address] \u0026lt;HTML\u0026gt;\u0026lt;HEAD\u0026gt;\u0026lt;meta http-equiv=\u0026#34;content-type\u0026#34; content=\u0026#34;text/html;charset=utf-8\u0026#34;\u0026gt; \u0026lt;TITLE\u0026gt;301 Moved\u0026lt;/TITLE\u0026gt;\u0026lt;/HEAD\u0026gt;\u0026lt;BODY\u0026gt; \u0026lt;H1\u0026gt;301 Moved\u0026lt;/H1\u0026gt; The document has moved \u0026lt;A HREF=\u0026#34;http://www.google.com/\u0026#34;\u0026gt;here\u0026lt;/A\u0026gt;. \u0026lt;/BODY\u0026gt;\u0026lt;/HTML\u0026gt; 你会注意到响应被代理到了 google.com。我们也可以检查虚拟机上的 envoy.log 来查看日志条目。\n要清理 iptables 规则并禁用 IP 转发，请运行：\n# 禁用IP转发功能 sudo sysctl -w net.ipv4.ip_forward=0 # 从nat表中删除所有规则 sudo iptables -t nat -F ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"3d02a8e2889020e3e0603eb527e9a0fc","permalink":"https://lib.jimmysong.io/envoy-handbook/listener/lab9/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/envoy-handbook/listener/lab9/","section":"envoy-handbook","summary":"在这个实验中，我们将学习如何配置原始目的地过滤器。要做到这一点，我们需要启用 IP 转发，然后更新 iptables 规则，以捕获所有流量并将其重定向到 Envoy 正在监听的端口。 我们将使用一个 Linux 虚拟机，而不是 Google Cloud Shell。 让我们从","tags":["Envoy"],"title":"实验 9：原始目的地过滤器","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"kubectl 命令是操作 Kubernetes 集群的最直接和最高效的途径，这个60多 MB 大小的二进制文件，到底有啥能耐呢？\nKubectl 自动补全 $ source \u0026lt;(kubectl completion bash) # setup autocomplete in bash, bash-completion package should be installed first. $ source \u0026lt;(kubectl completion zsh) # setup autocomplete in zsh Kubectl 上下文和配置 设置 kubectl 命令交互的 kubernetes 集群并修改配置信息。参阅 使用 kubeconfig 文件进行跨集群验证 获取关于配置文件的详细信息。\n$ kubectl config view # 显示合并后的 kubeconfig 配置 # 同时使用多个 kubeconfig 文件并查看合并后的配置 $ KUBECONFIG=~/.kube/config:~/.kube/kubconfig2 kubectl config view # 获取 e2e 用户的密码 $ kubectl config view -o jsonpath=\u0026#39;{.users[?(@.name == \u0026#34;e2e\u0026#34;)].user.password}\u0026#39; $ kubectl config current-context # 显示当前的上下文 $ kubectl config use-context my-cluster-name # 设置默认上下文为 my-cluster-name # 向 kubeconf 中增加支持基本认证的新集群 $ kubectl config set-credentials kubeuser/foo.kubernetes.com --username=kubeuser --password=kubepassword # 使用指定的用户名和 namespace 设置上下文 $ kubectl config set-context gce --user=cluster-admin --namespace=foo \\  \u0026amp;\u0026amp; kubectl config use-context gce 创建对象 Kubernetes 的清单文件可以使用 json 或 yaml 格式定义。可以以 .yaml、.yml、或者 .json 为扩展名。\n$ kubectl create -f ./my-manifest.yaml # 创建资源$ kubectl create -f ./my1.yaml -f ./my2.yaml # 使用多个文件创建资源$ kubectl create -f ./dir # 使用目录下的所有清单文件来创建资源$ kubectl create -f https://git.io/vPieo # 使用 url 来创建资源$ kubectl run nginx --image=nginx # 启动一个 nginx 实例$ kubectl explain pods,svc # 获取 pod 和 svc 的文档# 从 stdin 输入中创建多个 YAML 对象$ cat \u0026lt;\u0026lt;EOF | kubectl create -f -apiVersion:v1kind:Podmetadata:name:busybox-sleepspec:containers:- name:busyboximage:busyboxargs:- sleep- \u0026#34;1000000\u0026#34;---apiVersion:v1kind:Podmetadata:name:busybox-sleep-lessspec:containers:- name:busyboximage:busyboxargs:- sleep- \u0026#34;1000\u0026#34;EOF# 创建包含几个 key 的 Secret$ cat \u0026lt;\u0026lt;EOF | kubectl create -f -apiVersion:v1kind:Secretmetadata:name:mysecrettype:Opaquedata:password:$(echo \u0026#34;s33msi4\u0026#34; | base64)username:$(echo \u0026#34;jane\u0026#34; | base64)EOF显示和查找资源 # Get commands with basic output $ kubectl get services # 列出所有 namespace 中的所有 service $ kubectl get pods --all-namespaces # 列出所有 namespace 中的所有 pod $ kubectl get pods -o wide # 列出所有 pod 并显示详细信息 $ kubectl get deployment my-dep # 列出指定 deployment $ kubectl get pods --include-uninitialized # 列出该 namespace 中的所有 pod 包括未初始化的 # 使用详细输出来描述命令 $ kubectl describe nodes my-node $ kubectl describe pods my-pod $ kubectl get services --sort-by=.metadata.name # List Services Sorted by Name # 根据重启次数排序列出 pod $ kubectl get pods --sort-by=\u0026#39;.status.containerStatuses[0].restartCount\u0026#39; # 获取所有具有 app=cassandra 的 pod 中的 version 标签 $ kubectl get pods --selector=app=cassandra rc -o \\  jsonpath=\u0026#39;{.items[*].metadata.labels.version}\u0026#39; # 获取所有节点的 ExternalIP $ kubectl get nodes -o jsonpath=\u0026#39;{.items[*].status.addresses[?(@.type==\u0026#34;ExternalIP\u0026#34;)].address}\u0026#39; # 列出属于某个 PC 的 Pod 的名字 # “jq”命令用于转换复杂的 jsonpath，参考 https://stedolan.github.io/jq/ $ sel=${$(kubectl get rc my-rc --output=json | jq -j \u0026#39;.spec.selector | to_entries | .[] | \u0026#34;\\(.key)=\\(.value),\u0026#34;\u0026#39;)%?} $ echo $(kubectl get pods --selector=$sel --output=jsonpath={.items..metadata.name}) # 查看哪些节点已就绪 $ JSONPATH=\u0026#39;{range .items[*]}{@.metadata.name}:{range @.status.conditions[*]}{@.type}={@.status};{end}{end}\u0026#39; \\  \u0026amp;\u0026amp; kubectl get nodes -o jsonpath=\u0026#34;$JSONPATH\u0026#34; | grep \u0026#34;Ready=True\u0026#34; # 列出当前 Pod 中使用的 Secret $ kubectl get pods -o json | jq \u0026#39;.items[].spec.containers[].env[]?.valueFrom.secretKeyRef.name\u0026#39; | grep -v null | sort | uniq 更新资源 $ kubectl rolling-update frontend-v1 -f frontend-v2.json # 滚动更新 pod frontend-v1 $ kubectl rolling-update frontend-v1 frontend-v2 --image=image:v2 # 更新资源名称并更新镜像 $ kubectl rolling-update frontend --image=image:v2 # 更新 frontend pod 中的镜像 $ kubectl rolling-update frontend-v1 frontend-v2 --rollback # 退出已存在的进行中的滚动更新 $ cat pod.json | kubectl replace -f - # 基于 stdin 输入的 JSON 替换 pod # 强制替换，删除后重新创建资源。会导致服务中断。 $ kubectl replace --force -f ./pod.json # 为 nginx RC 创建服务，启用本地 80 端口连接到容器上的 8000 端口 $ kubectl expose rc nginx --port=80 --target-port=8000 # 更新单容器 pod 的镜像版本（tag）到 v4 $ kubectl get pod mypod -o yaml | sed \u0026#39;s/\\(image: myimage\\):.*$/\\1:v4/\u0026#39; | kubectl replace -f - $ kubectl label pods my-pod new-label=awesome # 添加标签 $ kubectl annotate pods my-pod icon-url=http://goo.gl/XXBTWq # 添加注解 $ kubectl autoscale deployment foo --min=2 --max=10 # 自动扩展 deployment “foo” 修补资源 使用策略合并补丁并修补资源。\n$ kubectl patch node k8s-node-1 -p \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;unschedulable\u0026#34;:true}}\u0026#39; # 部分更新节点 # 更新容器镜像； spec.containers[*].name 是必须的，因为这是合并的关键字 $ kubectl patch pod valid-pod -p \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;containers\u0026#34;:[{\u0026#34;name\u0026#34;:\u0026#34;kubernetes-serve-hostname\u0026#34;,\u0026#34;image\u0026#34;:\u0026#34;new image\u0026#34;}]}}\u0026#39; # 使用具有位置数组的 json 补丁更新容器镜像 $ kubectl patch pod valid-pod --type=\u0026#39;json\u0026#39; -p=\u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/containers/0/image\u0026#34;, \u0026#34;value\u0026#34;:\u0026#34;new image\u0026#34;}]\u0026#39; # 使用具有位置数组的 json 补丁禁用 deployment 的 livenessProbe $ kubectl patch deployment valid-deployment --type json -p=\u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;remove\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/template/spec/containers/0/livenessProbe\u0026#34;}]\u0026#39; 编辑资源 在编辑器中编辑任何 API 资源。\n$ kubectl edit svc/docker-registry # 编辑名为 docker-registry 的 service $ KUBE_EDITOR=\u0026#34;nano\u0026#34; kubectl edit svc/docker-registry # 使用其它编辑器 Scale 资源 $ kubectl scale --replicas=3 rs/foo # Scale a replicaset named \u0026#39;foo\u0026#39; to 3 $ kubectl …","date":1653062400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"92ee90a6bbb22d64874f4cd33611ca42","permalink":"https://lib.jimmysong.io/kubernetes-handbook/cli/kubectl-cheatsheet/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/cli/kubectl-cheatsheet/","section":"kubernetes-handbook","summary":"kubectl 命令是操作 Kubernetes 集群的最直接和最高效的途径，这个60多 MB 大小的二进制文件，到底有啥能耐呢？ Kubectl 自动补全 $ source \u003c(kubectl completion bash) # setup autocomplete in bash, bash-completion package should be installed first. $ source \u003c(kubectl completion zsh) # setup autocomplete in zsh Kubectl 上下文和配置 设置 kubectl 命令交互的 kubernetes 集群并修改配置信息","tags":["Kubernetes"],"title":"Kubectl 命令技巧大全","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"Kubenretes1.6 中使用 etcd V3 版本的 API，使用 etcdctl 直接 ls 的话只能看到 /kube-centos 一个路径。需要在命令前加上 ETCDCTL_API=3 这个环境变量才能看到 kuberentes 在 etcd 中保存的数据。\nETCDCTL_API=3 etcdctl get /registry/namespaces/default -w=json|python -m json.tool 如果是使用 kubeadm 创建的集群，在 Kubenretes 1.11 中，etcd 默认使用 tls ，这时你可以在 master 节点上使用以下命令来访问 etcd ：\nETCDCTL_API=3 etcdctl --cacert=/etc/kubernetes/pki/etcd/ca.crt \\ --cert=/etc/kubernetes/pki/etcd/peer.crt \\ --key=/etc/kubernetes/pki/etcd/peer.key \\ get /registry/namespaces/default -w=json | jq .  -w 指定输出格式  将得到这样的 json 的结果：\n{ \u0026#34;count\u0026#34;: 1, \u0026#34;header\u0026#34;: { \u0026#34;cluster_id\u0026#34;: 12091028579527406772, \u0026#34;member_id\u0026#34;: 16557816780141026208, \u0026#34;raft_term\u0026#34;: 36, \u0026#34;revision\u0026#34;: 29253467 }, \u0026#34;kvs\u0026#34;: [ { \u0026#34;create_revision\u0026#34;: 5, \u0026#34;key\u0026#34;: \u0026#34;L3JlZ2lzdHJ5L25hbWVzcGFjZXMvZGVmYXVsdA==\u0026#34;, \u0026#34;mod_revision\u0026#34;: 5, \u0026#34;value\u0026#34;: \u0026#34;azhzAAoPCgJ2MRIJTmFtZXNwYWNlEmIKSAoHZGVmYXVsdBIAGgAiACokZTU2YzMzMDgtMWVhOC0xMWU3LThjZDctZjRlOWQ0OWY4ZWQwMgA4AEILCIn4sscFEKOg9xd6ABIMCgprdWJlcm5ldGVzGggKBkFjdGl2ZRoAIgA=\u0026#34;, \u0026#34;version\u0026#34;: 1 } ] } 使用 --prefix 可以看到所有的子目录，如查看集群中的 namespace：\nETCDCTL_API=3 etcdctl get /registry/namespaces --prefix -w=json|python -m json.tool 输出结果中可以看到所有的 namespace。\n{ \u0026#34;count\u0026#34;: 8, \u0026#34;header\u0026#34;: { \u0026#34;cluster_id\u0026#34;: 12091028579527406772, \u0026#34;member_id\u0026#34;: 16557816780141026208, \u0026#34;raft_term\u0026#34;: 36, \u0026#34;revision\u0026#34;: 29253722 }, \u0026#34;kvs\u0026#34;: [ { \u0026#34;create_revision\u0026#34;: 24310883, \u0026#34;key\u0026#34;: \u0026#34;L3JlZ2lzdHJ5L25hbWVzcGFjZXMvYXV0b21vZGVs\u0026#34;, \u0026#34;mod_revision\u0026#34;: 24310883, \u0026#34;value\u0026#34;: \u0026#34;azhzAAoPCgJ2MRIJTmFtZXNwYWNlEmQKSgoJYXV0b21vZGVsEgAaACIAKiQ1MjczOTU1ZC1iMzEyLTExZTctOTcwYy1mNGU5ZDQ5ZjhlZDAyADgAQgsI7fSWzwUQ6Jv1Z3oAEgwKCmt1YmVybmV0ZXMaCAoGQWN0aXZlGgAiAA==\u0026#34;, \u0026#34;version\u0026#34;: 1 }, { \u0026#34;create_revision\u0026#34;: 21387676, \u0026#34;key\u0026#34;: \u0026#34;L3JlZ2lzdHJ5L25hbWVzcGFjZXMvYnJhbmQ=\u0026#34;, \u0026#34;mod_revision\u0026#34;: 21387676, \u0026#34;value\u0026#34;: \u0026#34;azhzAAoPCgJ2MRIJTmFtZXNwYWNlEmEKRwoFYnJhbmQSABoAIgAqJGNkZmQ1Y2NmLWExYzktMTFlNy05NzBjLWY0ZTlkNDlmOGVkMDIAOABCDAjR9qLOBRDYn83XAXoAEgwKCmt1YmVybmV0ZXMaCAoGQWN0aXZlGgAiAA==\u0026#34;, \u0026#34;version\u0026#34;: 1 }, { \u0026#34;create_revision\u0026#34;: 5, \u0026#34;key\u0026#34;: \u0026#34;L3JlZ2lzdHJ5L25hbWVzcGFjZXMvZGVmYXVsdA==\u0026#34;, \u0026#34;mod_revision\u0026#34;: 5, \u0026#34;value\u0026#34;: \u0026#34;azhzAAoPCgJ2MRIJTmFtZXNwYWNlEmIKSAoHZGVmYXVsdBIAGgAiACokZTU2YzMzMDgtMWVhOC0xMWU3LThjZDctZjRlOWQ0OWY4ZWQwMgA4AEILCIn4sscFEKOg9xd6ABIMCgprdWJlcm5ldGVzGggKBkFjdGl2ZRoAIgA=\u0026#34;, \u0026#34;version\u0026#34;: 1 }, { \u0026#34;create_revision\u0026#34;: 18504694, \u0026#34;key\u0026#34;: \u0026#34;L3JlZ2lzdHJ5L25hbWVzcGFjZXMvZGV2\u0026#34;, \u0026#34;mod_revision\u0026#34;: 24310213, \u0026#34;value\u0026#34;: \u0026#34;azhzAAoPCgJ2MRIJTmFtZXNwYWNlEmwKUgoDZGV2EgAaACIAKiQyOGRlMGVjNS04ZTEzLTExZTctOTcwYy1mNGU5ZDQ5ZjhlZDAyADgAQgwI89CezQUQ0v2fuQNaCwoEbmFtZRIDZGV2egASDAoKa3ViZXJuZXRlcxoICgZBY3RpdmUaACIA\u0026#34;, \u0026#34;version\u0026#34;: 4 }, { \u0026#34;create_revision\u0026#34;: 10, \u0026#34;key\u0026#34;: \u0026#34;L3JlZ2lzdHJ5L25hbWVzcGFjZXMva3ViZS1wdWJsaWM=\u0026#34;, \u0026#34;mod_revision\u0026#34;: 10, \u0026#34;value\u0026#34;: \u0026#34;azhzAAoPCgJ2MRIJTmFtZXNwYWNlEmcKTQoLa3ViZS1wdWJsaWMSABoAIgAqJGU1ZjhkY2I1LTFlYTgtMTFlNy04Y2Q3LWY0ZTlkNDlmOGVkMDIAOABCDAiJ+LLHBRDdrsDPA3oAEgwKCmt1YmVybmV0ZXMaCAoGQWN0aXZlGgAiAA==\u0026#34;, \u0026#34;version\u0026#34;: 1 }, { \u0026#34;create_revision\u0026#34;: 2, \u0026#34;key\u0026#34;: \u0026#34;L3JlZ2lzdHJ5L25hbWVzcGFjZXMva3ViZS1zeXN0ZW0=\u0026#34;, \u0026#34;mod_revision\u0026#34;: 2, \u0026#34;value\u0026#34;: \u0026#34;azhzAAoPCgJ2MRIJTmFtZXNwYWNlEmYKTAoLa3ViZS1zeXN0ZW0SABoAIgAqJGU1NmFhMDVkLTFlYTgtMTFlNy04Y2Q3LWY0ZTlkNDlmOGVkMDIAOABCCwiJ+LLHBRDoq9ASegASDAoKa3ViZXJuZXRlcxoICgZBY3RpdmUaACIA\u0026#34;, \u0026#34;version\u0026#34;: 1 }, { \u0026#34;create_revision\u0026#34;: 3774247, \u0026#34;key\u0026#34;: \u0026#34;L3JlZ2lzdHJ5L25hbWVzcGFjZXMvc3BhcmstY2x1c3Rlcg==\u0026#34;, \u0026#34;mod_revision\u0026#34;: 3774247, \u0026#34;value\u0026#34;: \u0026#34;azhzAAoPCgJ2MRIJTmFtZXNwYWNlEoABCmYKDXNwYXJrLWNsdXN0ZXISABoAIgAqJDMyNjY3ZDVjLTM0YWMtMTFlNy1iZmJkLThhZjFlM2E3YzViZDIAOABCDAiA1cbIBRDU3YuAAVoVCgRuYW1lEg1zcGFyay1jbHVzdGVyegASDAoKa3ViZXJuZXRlcxoICgZBY3RpdmUaACIA\u0026#34;, \u0026#34;version\u0026#34;: 1 }, { \u0026#34;create_revision\u0026#34;: 15212191, \u0026#34;key\u0026#34;: \u0026#34;L3JlZ2lzdHJ5L25hbWVzcGFjZXMveWFybi1jbHVzdGVy\u0026#34;, \u0026#34;mod_revision\u0026#34;: 15212191, \u0026#34;value\u0026#34;: \u0026#34;azhzAAoPCgJ2MRIJTmFtZXNwYWNlEn0KYwoMeWFybi1jbHVzdGVyEgAaACIAKiQ2YWNhNjk1Yi03N2Y5LTExZTctYmZiZC04YWYxZTNhN2M1YmQyADgAQgsI1qiKzAUQkoqxDloUCgRuYW1lEgx5YXJuLWNsdXN0ZXJ6ABIMCgprdWJlcm5ldGVzGggKBkFjdGl2ZRoAIgA=\u0026#34;, \u0026#34;version\u0026#34;: 1 } ] } key 的值是经过 base64 编码，需要解码后才能看到实际值，如：\n$ echo L3JlZ2lzdHJ5L25hbWVzcGFjZXMvYXV0b21vZGVs|base64 -d /registry/namespaces/automodel etcd 中 kubernetes 的元数据 我们使用 kubectl 命令获取的 kubernetes 的对象状态实际上是保存在 etcd 中的，使用下面的脚本可以获取 etcd 中的所有 kubernetes 对象的 key：\n 注意，我们使用了 ETCD v3 版本的客户端命令来访问 etcd。\n #!/bin/bash # Get kubernetes keys from etcd export ETCDCTL_API=3 keys=`etcdctl get /registry --prefix -w json|python -m json.tool|grep key|cut -d \u0026#34;:\u0026#34; -f2|tr -d \u0026#39;\u0026#34;\u0026#39;|tr -d \u0026#34;,\u0026#34;` for x in $keys;do echo $x|base64 -d|sort done 通过输出的结果我们可以看到 kubernetes 的原数据是按何种结构包括在 kuberentes 中的，输出结果如下所示：\n/registry/ThirdPartyResourceData/istio.io/istioconfigs/default/route-rule-details-default /registry/ThirdPartyResourceData/istio.io/istioconfigs/default/route-rule-productpage-default /registry/ThirdPartyResourceData/istio.io/istioconfigs/default/route-rule-ratings-default ... /registry/configmaps/default/namerctl-script /registry/configmaps/default/namerd-config /registry/configmaps/default/nginx-config ... /registry/deployments/default/sdmk-page-sdmk /registry/deployments/default/sdmk-payment-web /registry/deployments/default/sdmk-report ... 我们可以看到所有的 Kuberentes 的所有元数据都保存在 /registry 目录下，下一层就是 API 对象类型（复数形式），再下一层是 namespace，最后一层是对象的名字。 …","date":1653062400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"6c9e78b1d92f995c8efbb18445a78e89","permalink":"https://lib.jimmysong.io/kubernetes-handbook/cli/etcdctl/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/cli/etcdctl/","section":"kubernetes-handbook","summary":"Kubenretes1.6 中使用 etcd V3 版本的 API，使用 etcdctl 直接 ls 的话只能看到 /kube-centos 一个路径。需要在命令前加上 ETCDCTL_API=3 这个环境变量才能看到 kuberentes 在 etcd 中保存的数据。 ETCDCTL_API=3 etcdctl get /registry/namespaces/default -w=json|python -m json.tool 如果是使用 kubeadm 创建的集群，在 Kubenretes 1.11 中，etcd 默认使用 tls ，这时你可以在","tags":["Kubernetes"],"title":"使用 etcdctl 访问 Kubernetes 数据","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"在使用二进制文件部署 Kubernetes 集群的时候，很多人在进行到部署证书时遇到各种各样千奇百怪的问题，这一步是创建集群的基础，我们有必要详细了解一下其背后的流程和原理。\n概览 每个 Kubernetes 集群都有一个集群根证书颁发机构（CA）。 集群中的组件通常使用 CA 来验证 API server 的证书，由 API 服务器验证 kubelet 客户端证书等。为了支持这一点，CA 证书包被分发到集群中的每个节点，并作为一个 secret 附加分发到默认 service account 上。 或者，你的 workload 可以使用此 CA 建立信任。 你的应用程序可以使用类似于 ACME 草案的协议，使用 certificates.k8s.io API 请求证书签名。\n集群中的 TLS 信任 让 Pod 中运行的应用程序信任集群根 CA 通常需要一些额外的应用程序配置。 您将需要将 CA 证书包添加到 TLS 客户端或服务器信任的 CA 证书列表中。 例如，您可以使用 golang TLS 配置通过解析证书链并将解析的证书添加到 tls.Config结构中的 Certificates 字段中，CA 证书捆绑包将使用默认服务账户自动加载到 pod 中，路径为 /var/run/secrets/kubernetes.io/serviceaccount/ca.crt。 如果您没有使用默认服务账户，请请求集群管理员构建包含您有权访问使用的证书包的 configmap。\n请求认证 以下部分演示如何为通过 DNS 访问的 Kubernetes 服务创建 TLS 证书。\n下载安装SSL 下载 cfssl 工具。\n创建证书签名请求 通过运行以下命令生成私钥和证书签名请求（或 CSR）：\n$ cat \u0026lt;\u0026lt;EOF | cfssl genkey - | cfssljson -bare server { \u0026#34;hosts\u0026#34;: [ \u0026#34;my-svc.my-namespace.svc.cluster.local\u0026#34;, \u0026#34;my-pod.my-namespace.pod.cluster.local\u0026#34;, \u0026#34;172.168.0.24\u0026#34;, \u0026#34;10.0.34.2\u0026#34; ], \u0026#34;CN\u0026#34;: \u0026#34;my-pod.my-namespace.pod.cluster.local\u0026#34;, \u0026#34;key\u0026#34;: { \u0026#34;algo\u0026#34;: \u0026#34;ecdsa\u0026#34;, \u0026#34;size\u0026#34;: 256 } } EOF 172.168.0.24 是 service 的 cluster IP，my-svc.my-namespace.svc.cluster.local 是 service 的 DNS 名称， 10.0.34.2 是 Pod 的 IP， my-pod.my-namespace.pod.cluster.local 是 pod 的 DNS 名称，你可以看到以下输出：\n2017/03/21 06:48:17 [INFO] generate received request 2017/03/21 06:48:17 [INFO] received CSR 2017/03/21 06:48:17 [INFO] generating key: ecdsa-256 2017/03/21 06:48:17 [INFO] encoded CSR 此命令生成两个文件；它生成包含 PEM 编码的 pkcs #10 认证请求的 server.csr，以及包含仍然要创建的证书的 PEM 编码密钥的 server-key.pem。\n创建证书签名请求对象以发送到 Kubernetes API 使用以下命令创建 CSR yaml 文件，并发送到 API server：\n$ cat \u0026lt;\u0026lt;EOF | kubectl create -f - apiVersion: certificates.k8s.io/v1beta1 kind: CertificateSigningRequest metadata: name: my-svc.my-namespace spec: groups: - system:authenticated request: $(cat server.csr | base64 | tr -d \u0026#39;\\n\u0026#39;) usages: - digital signature - key encipherment - server auth EOF 请注意，在步骤 1 中创建的 server.csr 文件是 base64 编码并存储在.spec.request 字段中。 我们还要求提供 “数字签名”，“密钥加密” 和 “服务器身份验证” 密钥用途的证书。\n在 API server 中可以看到这些 CSR 处于 pending 状态。执行下面的命令你将可以看到：\n$ kubectl describe csr my-svc.my-namespace Name: my-svc.my-namespace Labels: \u0026lt;none\u0026gt; Annotations: \u0026lt;none\u0026gt; CreationTimestamp: Tue, 21 Mar 2017 07:03:51 -0700 Requesting User: yourname@example.com Status: Pending Subject: Common Name: my-svc.my-namespace.svc.cluster.local Serial Number: Subject Alternative Names: DNS Names: my-svc.my-namespace.svc.cluster.local IP Addresses: 172.168.0.24 10.0.34.2 Events: \u0026lt;none\u0026gt; 获取证书签名请求 批准证书签名请求是通过自动批准过程完成的，或由集群管理员一次完成。 有关这方面涉及的更多信息，请参见下文。\n下载签名并使用 一旦 CSR 被签署并获得批准，您应该看到以下内容：\n$ kubectl get csr NAME AGE REQUESTOR CONDITION my-svc.my-namespace 10m yourname@example.com Approved,Issued 你可以通过运行以下命令下载颁发的证书并将其保存到 server.crt 文件中：\n$ kubectl get csr my-svc.my-namespace -o jsonpath=\u0026#39;{.status.certificate}\u0026#39; \\  | base64 -d \u0026gt; server.crt 现在你可以用 server.crt 和 server-key.pem 来做为 keypair 来启动 HTTPS server。\n批准证书签名请求 Kubernetes 管理员（具有适当权限）可以使用 kubectl certificate approve 和 kubectl certificate deny 命令手动批准（或拒绝）证书签名请求。但是，如果您打算大量使用此 API，则可以考虑编写自动化的证书控制器。\n如果上述机器或人类使用 kubectl，批准者的作用是验证 CSR 满足如下两个要求：\n CSR 的主体控制用于签署 CSR 的私钥。这解决了伪装成授权主体的第三方的威胁。在上述示例中，此步骤将验证该 pod 控制了用于生成 CSR 的私钥。 CSR 的主体被授权在请求的上下文中执行。这解决了我们加入群集的我们不期望的主体的威胁。在上述示例中，此步骤将是验证该 pod 是否被允许加入到所请求的服务中。  当且仅当满足这两个要求时，审批者应该批准 CSR，否则拒绝 CSR。\n给集群管理员的一个建议 本教程假设将 signer 设置为服务证书 API。Kubernetes controller manager 提供了一个 signer 的默认实现。 要启用它，请将 --cluster-signing-cert-file 和 --cluster-signing-key-file 参数传递给 controller manager，并配置具有证书颁发机构的密钥对的路径。\n","date":1653062400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"f7f7d4ffbd8c8a2ed2d0d2d0fad91405","permalink":"https://lib.jimmysong.io/kubernetes-handbook/security/managing-tls-in-a-cluster/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/security/managing-tls-in-a-cluster/","section":"kubernetes-handbook","summary":"在使用二进制文件部署 Kubernetes 集群的时候，很多人在进行到部署证书时遇到各种各样千奇百怪的问题，这一步是创建集群的基础，我们有必要详细了解一下其背后的流程和原理。 概览 每个 Kubernetes 集群都有一个集群根证书颁发机构（CA）","tags":["Kubernetes"],"title":"管理集群中的 TLS","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"Kubelet 的 HTTPS 端点对外暴露了用于访问不同敏感程度数据的 API，并允许您在节点或者容器内执行不同权限级别的操作。\n本文档向您描述如何通过认证授权来访问 kubelet 的 HTTPS 端点。\nKubelet 认证 默认情况下，所有未被配置的其他身份验证方法拒绝的，对 kubelet 的 HTTPS 端点的请求将被视为匿名请求，并被授予 system:anonymous 用户名和 system:unauthenticated 组。\n如果要禁用匿名访问并发送 401 Unauthorized 的未经身份验证的请求的响应：\n 启动 kubelet 时指定 --anonymous-auth=false 标志  如果要对 kubelet 的 HTTPS 端点启用 X509 客户端证书身份验证：\n 启动 kubelet 时指定 --client-ca-file 标志，提供 CA bundle 以验证客户端证书 启动 apiserver 时指定 --kubelet-client-certificate 和 --kubelet-client-key 标志 参阅 apiserver 认证文档 获取更多详细信息。  启用 API bearer token（包括 service account token）用于向 kubelet 的 HTTPS 端点进行身份验证：\n 确保在 API server 中开启了 authentication.k8s.io/v1beta1 API 组。 启动 kubelet 时指定 --authentication-token-webhook， --kubeconfig 和 --require-kubeconfig 标志 Kubelet 在配置的 API server 上调用 TokenReview API 以确定来自 bearer token 的用户信息  Kubelet 授权 接着对任何成功验证的请求（包括匿名请求）授权。默认授权模式为 AlwaysAllow，允许所有请求。\n细分访问 kubelet API 有很多原因：\n 启用匿名认证，但匿名用户调用 kubelet API 的能力应受到限制 启动 bearer token 认证，但是 API 用户（如 service account）调用 kubelet API 的能力应受到限制 客户端证书身份验证已启用，但只有那些配置了 CA 签名的客户端证书的用户才可以使用 kubelet API  如果要细分访问 kubelet API，将授权委托给 API server：\n 确保 API server 中启用了 authorization.k8s.io/v1beta1 API 组 启动 kubelet 时指定 --authorization-mode=Webhook、 --kubeconfig 和 --require-kubeconfig 标志 kubelet 在配置的 API server 上调用 SubjectAccessReview API，以确定每个请求是否被授权  kubelet 使用与 apiserver 相同的 请求属性 方法来授权 API 请求。\nVerb（动词）是根据传入的请求的 HTTP 动词确定的：\n   HTTP 动词 request 动词     POST create   GET, HEAD get   PUT update   PATCH patch   DELETE delete    资源和子资源根据传入请求的路径确定：\n   Kubelet API 资源 子资源     /stats/* nodes stats   /metrics/* nodes metrics   /logs/* nodes log   /spec/* nodes spec   all others nodes proxy    Namespace 和 API 组属性总是空字符串，资源的名字总是 kubelet 的 Node API 对象的名字。\n当以该模式运行时，请确保用户为 apiserver 指定了 --kubelet-client-certificate 和 --kubelet-client-key 标志并授权了如下属性：\n verb=*, resource=nodes, subresource=proxy verb=*, resource=nodes, subresource=stats verb=*, resource=nodes, subresource=log verb=*, resource=nodes, subresource=spec verb=*, resource=nodes, subresource=metrics  ","date":1653062400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"3b9191178f294b471fef7a9308ace652","permalink":"https://lib.jimmysong.io/kubernetes-handbook/security/kubelet-authentication-authorization/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/security/kubelet-authentication-authorization/","section":"kubernetes-handbook","summary":"Kubelet 的 HTTPS 端点对外暴露了用于访问不同敏感程度数据的 API，并允许您在节点或者容器内执行不同权限级别的操作。 本文档向您描述如何通过认证授权来访问 kubelet 的 HTTPS 端点。 Kubelet 认证 默认情况下，所有未被配置的其他身份验证方法拒","tags":["Kubernetes"],"title":"Kublet 的认证授权","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"本文档介绍如何为 kubelet 设置 TLS 客户端证书引导（bootstrap）。\nKubernetes 1.4 引入了一个用于从集群级证书颁发机构（CA）请求证书的 API。此 API 的原始目的是为 kubelet 提供 TLS 客户端证书。可以在 这里 找到该提议，在 feature #43 追踪该功能的进度。\nkube-apiserver 配置 您必须提供一个 token 文件，该文件中指定了至少一个分配给 kubelet 特定 bootstrap 组的 “bootstrap token”。\n该组将作为 controller manager 配置中的默认批准控制器而用于审批。随着此功能的成熟，您应该确保 token 被绑定到基于角色的访问控制（RBAC）策略上，该策略严格限制了与证书配置相关的客户端请求（使用 bootstrap token）。使用 RBAC，将 token 范围划分为组可以带来很大的灵活性（例如，当您配置完成节点后，您可以禁用特定引导组的访问）。\nToken 认证文件 Token 可以是任意的，但应该可以表示为从安全随机数生成器（例如大多数现代操作系统中的 /dev/urandom）导出的至少128位熵。生成 token 有很多中方式。例如：\nhead -c 16 /dev/urandom | od -An -t x | tr -d \u0026#39; \u0026#39;\n产生的 token 类似于这样： 02b50b05283e98dd0fd71db496ef01e8。\nToken 文件应该类似于以下示例，其中前三个值可以是任何值，引用的组名称应如下所示：\n02b50b05283e98dd0fd71db496ef01e8,kubelet-bootstrap,10001,system:kubelet-bootstrap 注意：system:kubelet-bootstrap 的配置，当只有一个组时，不需要加引号。\n在 kube-apiserver 命令中添加 --token-auth-file=FILENAME 标志（可能在您的 systemd unit 文件中）来启用 token 文件。\n查看 该文档 获取更多详细信息。\n客户端证书 CA 包 在 kube-apiserver 命令中添加 --client-ca-file=FILENAME 标志启用客户端证书认证，指定包含签名证书的证书颁发机构包（例如 --client-ca-file=/var/lib/kubernetes/ca.pem）。\nkube-controller-manager 配置 请求证书的 API 向 Kubernetes controller manager 中添加证书颁发控制循环。使用磁盘上的 cfssl 本地签名文件的形式。目前，所有发型的证书均为一年有效期和并具有一系列关键用途。\n签名文件 您必须提供证书颁发机构，这样才能提供颁发证书所需的密码资料。\nkube-apiserver 通过指定的 --client-ca-file=FILENAME 标志来认证和采信该 CA。CA 的管理超出了本文档的范围，但建议您为 Kubernetes 生成专用的 CA。\n假定证书和密钥都是 PEM 编码的。\nKube-controller-manager 标志为：\n--cluster-signing-cert-file=\u0026#34;/etc/path/to/kubernetes/ca/ca.crt\u0026#34; --cluster-signing-key-file=\u0026#34;/etc/path/to/kubernetes/ca/ca.key\u0026#34; 审批控制器 在 kubernetes 1.7 版本中，实验性的 “组自动批准” 控制器被弃用，新的 csrapproving 控制器将作为 kube-controller-manager 的一部分，被默认启用。\n控制器使用 SubjectAccessReview API 来确定给定用户是否已被授权允许请求 CSR，然后根据授权结果进行批准。为了防止与其他批准者冲突，内置审批者没有明确地拒绝 CSR，只是忽略未经授权的请求。\n控制器将 CSR 分为三个子资源：\n nodeclient ：用户的客户端认证请求 O=system:nodes， CN=system:node:(node name)。 selfnodeclient：更新具有相同 O 和 CN 的客户端证书的节点。 selfnodeserver：更新服务证书的节点（ALPHA，需要 feature gate）。  当前，确定 CSR 是否为 selfnodeserver 请求的检查与 kubelet 的凭据轮换实现（Alpha 功能）相关联。因此，selfnodeserver 的定义将来可能会改变，并且需要 Controller Manager 上的RotateKubeletServerCertificate feature gate。该功能的进展可以在 kubernetes/feature/#267 上追踪。\n--feature-gates=RotateKubeletServerCertificate=true 以下 RBAC ClusterRoles 代表 nodeClient、selfnodeclient 和 selfnodeserver 功能。在以后的版本中可能会自动创建类似的角色。\n# A ClusterRole which instructs the CSR approver to approve a user requesting# node client credentials.kind:ClusterRoleapiVersion:rbac.authorization.k8s.io/v1beta1metadata:name:approve-node-client-csrrules:- apiGroups:[\u0026#34;certificates.k8s.io\u0026#34;]resources:[\u0026#34;certificatesigningrequests/nodeclient\u0026#34;]verbs:[\u0026#34;create\u0026#34;]---# A ClusterRole which instructs the CSR approver to approve a node renewing its# own client credentials.kind:ClusterRoleapiVersion:rbac.authorization.k8s.io/v1beta1metadata:name:approve-node-client-renewal-csrrules:- apiGroups:[\u0026#34;certificates.k8s.io\u0026#34;]resources:[\u0026#34;certificatesigningrequests/selfnodeclient\u0026#34;]verbs:[\u0026#34;create\u0026#34;]---# A ClusterRole which instructs the CSR approver to approve a node requesting a# serving cert matching its client cert.kind:ClusterRoleapiVersion:rbac.authorization.k8s.io/v1beta1metadata:name:approve-node-server-renewal-csrrules:- apiGroups:[\u0026#34;certificates.k8s.io\u0026#34;]resources:[\u0026#34;certificatesigningrequests/selfnodeserver\u0026#34;]verbs:[\u0026#34;create\u0026#34;]这些权力可以授予给凭证，如 bootstrap token。例如，要复制由已被移除的自动批准标志提供的行为，由单个组批准所有的 CSR：\n# REMOVED: This flag no longer works as of 1.7. --insecure-experimental-approve-all-kubelet-csrs-for-group=\u0026#34;kubelet-bootstrap-token\u0026#34; 管理员将创建一个 ClusterRoleBinding 来定位该组。\n# Approve all CSRs for the group \u0026#34;kubelet-bootstrap-token\u0026#34;kind:ClusterRoleBindingapiVersion:rbac.authorization.k8s.io/v1beta1metadata:name:auto-approve-csrs-for-groupsubjects:- kind:Groupname:kubelet-bootstrap-tokenapiGroup:rbac.authorization.k8s.ioroleRef:kind:ClusterRolename:approve-node-client-csrapiGroup:rbac.authorization.k8s.io要让节点更新自己的凭据，管理员可以构造一个 ClusterRoleBinding 来定位该节点的凭据。\nkind:ClusterRoleBindingapiVersion:rbac.authorization.k8s.io/v1beta1metadata:name:node1-client-cert-renewalsubjects:- kind:Username:system:node:node-1# Let \u0026#34;node-1\u0026#34; renew its client certificate.apiGroup:rbac.authorization.k8s.ioroleRef:kind:ClusterRolename:approve-node-client-renewal-csrapiGroup:rbac.authorization.k8s.io删除该绑定将会阻止节点更新客户端凭据，一旦其证书到期，实际上就会将其从集群中删除。\nkubelet 配置 要向 kube-apiserver 请求客户端证书，kubelet 首先需要一个包含 bootstrap 身份验证 token 的 kubeconfig 文件路径。您可以使用 kubectl config set-cluster，set-credentials 和 set-context 来构建此 kubeconfig 文件。为 kubectl config set-credentials 提供 kubelet-bootstrap 的名称，并包含 --token = \u0026lt;token-value\u0026gt;，如下所示：\nkubectl config set-credentials kubelet-bootstrap --token=${BOOTSTRAP_TOKEN} --kubeconfig=bootstrap.kubeconfig 启动 kubelet 时，如果 --kubeconfig 指定的文件不存在，则使用 bootstrap kubeconfig 向 API server 请求客户端证书。在批准 kubelet 的证书请求和回执时，将包含了生成的密钥和证书的 kubeconfig 文件写入由 -kubeconfig 指定的路径。证书和密钥文件将被放置在由 --cert-dir 指定的目录中。\n启动 kubelet 时启用 bootstrap 用到的标志：\n--experimental-bootstrap-kubeconfig=\u0026#34;/path/to/bootstrap/kubeconfig\u0026#34; 此外，在1.7中，kubelet 实现了 Alpha 功能，使其客户端和/或服务器都能轮转提供证书。\n可以分别通过 kubelet 中的 RotateKubeletClientCertificate …","date":1653062400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"4d0add755ef1a25d07fd42996f91154e","permalink":"https://lib.jimmysong.io/kubernetes-handbook/security/tls-bootstrapping/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/security/tls-bootstrapping/","section":"kubernetes-handbook","summary":"本文档介绍如何为 kubelet 设置 TLS 客户端证书引导（bootstrap）。 Kubernetes 1.4 引入了一个用于从集群级证书颁发机构（CA）请求证书的 API。此 API 的原始目的是为 kubelet 提供 TLS 客户端证书。可以在 这里 找到该提议，在 feature #43 追踪该功","tags":["Kubernetes"],"title":"TLS Bootstrap","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"本文将讲述如何配置和启用 ip-masq-agent。\n创建 ip-masq-agent 要创建 ip-masq-agent，运行下面的 kubectl 命令：\nkubectl create -f https://raw.githubusercontent.com/kubernetes-incubator/ip-masq-agent/master/ip-masq-agent.yaml 关于 ip-masq-agent 的更多信息请参考 该文档。\n在大多数情况下，默认的一套规则应该是足够的；但是，如果内置的规则不适用于您的集群，您可以创建并应用 ConfigMap 来自定义受影响的 IP 范围。例如，为了仅允许 ip-masq-agent 考虑 10.0.0.0/8，您可以在名为 “config” 的文件中创建以下 ConfigMap。\nnonMasqueradeCIDRs:- 10.0.0.0/8resyncInterval:60s注意：重要的是，该文件被命名为 config，因为默认情况下，该文件将被用作 ip-masq-agent 查找的关键字。\n运行下列命令将 ConfigMap 添加到您的集群中：\nkubectl create configmap ip-masq-agent --from-file=config --namespace=kube-system 这将会更新 /etc/config/ip-masq-agent 文件，并每隔 resyscInterval 时间段检查一遍该文件，将配置应用到集群的节点中。\niptables -t nat -L IP-MASQ-AGENT Chain IP-MASQ-AGENT (1 references) target prot opt source destination RETURN all -- anywhere 169.254.0.0/16 /* ip-masq-agent: cluster-local traffic should not be subject to MASQUERADE */ ADDRTYPE match dst-type !LOCAL RETURN all -- anywhere 10.0.0.0/8 /* ip-masq-agent: cluster-local MASQUERADE all -- anywhere anywhere /* ip-masq-agent: outbound traffic should be subject to MASQUERADE (this match must come after cluster-local CIDR matches) */ ADDRTYPE match dst-type !LOCAL 默认情况下，本地链路范围（169.254.0.0/16）也由 ip-masq 代理处理，该代理设置相应的 iptables 规则。想要让 ip-masq-agent 忽略本地链路，您可以在 ConfigMap 中将 masqLinkLocal 设置为 true。\nnonMasqueradeCIDRs:- 10.0.0.0/8resyncInterval:60smasqLinkLocal:trueIP 伪装代理用户指南 ip-masq-agent 用户配置 iptables 规则将 Pod 的 IP 地址隐藏在集群 node 节点的 IP 地址后面。这通常在将流量发送到群集的 pod CIDR 范围之外的目的地时执行。\n关键术语   NAT（网络地址转换）\n是一种通过修改 IP 头中的源和/或目标地址信息来将一个 IP 地址重映射到另一个 IP 地址的方法。通常由执行 IP 路由的设备完成。\n  Masquerading（伪装）\nNAT 的一种形式，通常用于执行多个地址转换，其中多个源 IP 地址被掩盖在单个地址之后，通常是由某设备进行 IP 路由。在 kubernetes 中，这是 Node 的 IP 地址。\n  CIDR（无类域内路由选择）\n基于可变长度子网掩码，允许指定任意长度的前缀。CIDR 引入了一种新的 IP 地址表示方法，现在通常被称为 CIDR 表示法，将地址或路由前缀的比特位数作为后缀，例如 192.168.2.0/24。\n  本地链路\n本地链路地址是仅能在主机连接的网段或广播域内进行有效通信的网络地址。IPv4 的链路本地地址在 CIDR 表示法定义的地址块是 169.254.0.0/16。\n  Ip-masq-agent 在将流量发送到集群 node 节点的 IP 和 Cluster IP 范围之外的目的地时，会配置 iptables 规则来处理伪装的 node/pod IP 地址。这基本上将 pod 的 IP 地址隐藏在了集群 node 节点的 IP 地址后面。在某些环境中，到 “外部” 地址的流量必须来自已知的机器地址。例如，在 Google Cloud 中，到互联网的任何流量必须来自虚拟机的 IP。当使用容器时，如在GKE中，Pod IP 将被拒绝作为出口。为了避免这种情况，我们必须将 Pod IP 隐藏在 VM 自己的 IP 地址之后——通常被称为 “伪装”。默认情况下，配置代理将指定的三个专用 IP 范围视为非伪装 CIDR。范围包括 10.0.0.0/8、172.16.0.0/12 和 192.168.0.0/16。默认情况下，代理还将本地链路（169.254.0.0/16）视为非伪装 CIDR。代理配置为每隔60秒从 /etc/config/ip-masq-agent 位置重新加载其配置，这也是可配置的。\n   IP伪装代理示意图  代理的配置文件必须使用 yaml 或 json 语法，并且包含以下三个可选的 key：\n nonMasqueradeCIDRs：使用 CIDR 表示法指定的非伪装范围的字符串列表。 masqLinkLocal：一个布尔值（true/false），表示是否将流量伪装成本地链路前缀 169.254.0.0/16。默认为false。 resyncInterval：代理尝试从磁盘重新加载配置的时间间隔。例如 ’30s’ 其中 ‘s’ 是秒，’ms’ 是毫秒等…  到 10.0.0.0/8、172.16.0.0/12 和 192.168.0.0/16 范围的流量将不会被伪装。任何其他流量（假定是互联网）将被伪装。这里有个例子，来自 pod 的本地目的地址可以是其节点的 IP 地址、其他节点的地址或 Cluster IP 范围中的一个 IP 地址。其他任何流量都将默认伪装。以下条目显示 ip-masq-agent 应用的默认规则集：\niptables -t nat -L IP-MASQ-AGENT RETURN all -- anywhere 169.254.0.0/16 /* ip-masq-agent: cluster-local traffic should not be subject to MASQUERADE */ ADDRTYPE match dst-type !LOCAL RETURN all -- anywhere 10.0.0.0/8 /* ip-masq-agent: cluster-local traffic should not be subject to MASQUERADE */ ADDRTYPE match dst-type !LOCAL RETURN all -- anywhere 172.16.0.0/12 /* ip-masq-agent: cluster-local traffic should not be subject to MASQUERADE */ ADDRTYPE match dst-type !LOCAL RETURN all -- anywhere 192.168.0.0/16 /* ip-masq-agent: cluster-local traffic should not be subject to MASQUERADE */ ADDRTYPE match dst-type !LOCAL MASQUERADE all -- anywhere anywhere /* ip-masq-agent: outbound traffic should be subject to MASQUERADE (this match must come after cluster-local CIDR matches) */ ADDRTYPE match dst-type !LOCAL 默认情况下，在 GCE/GKE 中将启动 kubernetes 1.7.0 版本，ip-masq-agent 已经在集群中运行。如果您在其他环境中运行 kubernetes，那么您可以将 ip-masq-agent 以 DaemonSet 的方式在集群中运行。\n参考  IP Masquerade Agent User Guide - kubernetes.io  ","date":1653062400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"c84bdf4769395cf4dc8f5d5dbc1c188b","permalink":"https://lib.jimmysong.io/kubernetes-handbook/security/ip-masq-agent/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/security/ip-masq-agent/","section":"kubernetes-handbook","summary":"本文将讲述如何配置和启用 ip-masq-agent。 创建 ip-masq-agent 要创建 ip-masq-agent，运行下面的 kubectl 命令： kubectl create -f https://raw.githubusercontent.com/kubernetes-incubator/ip-masq-agent/master/ip-masq-agent.yaml 关于 ip-masq-agent 的更多信息请参考 该文档。 在大多数情况下，默认的一套规则应该是足够的；但是，如","tags":["Kubernetes"],"title":"IP 伪装代理","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"当我们安装好集群后，如果想要把 kubectl 命令交给用户使用，就不得不对用户的身份进行认证和对其权限做出限制。\n下面以创建一个 devuser 用户并将其绑定到 dev 和 test 两个 namespace 为例说明。\n创建 CA 证书和秘钥 创建 devuser-csr.json 文件\n{ \u0026#34;CN\u0026#34;: \u0026#34;devuser\u0026#34;, \u0026#34;hosts\u0026#34;: [], \u0026#34;key\u0026#34;: { \u0026#34;algo\u0026#34;: \u0026#34;rsa\u0026#34;, \u0026#34;size\u0026#34;: 2048 }, \u0026#34;names\u0026#34;: [ { \u0026#34;C\u0026#34;: \u0026#34;CN\u0026#34;, \u0026#34;ST\u0026#34;: \u0026#34;BeiJing\u0026#34;, \u0026#34;L\u0026#34;: \u0026#34;BeiJing\u0026#34;, \u0026#34;O\u0026#34;: \u0026#34;k8s\u0026#34;, \u0026#34;OU\u0026#34;: \u0026#34;System\u0026#34; } ] } 生成 CA 证书和私钥\n下面我们在 master 节点上为 devuser 创建证书和秘钥，在 /etc/kubernetes/ssl 目录下执行以下命令：\n执行该命令前请先确保该目录下已经包含如下文件：\nca-key.pem ca.pem ca-config.json devuser-csr.json $ cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes devuser-csr.json | cfssljson -bare devuser 2017/08/31 13:31:54 [INFO] generate received request 2017/08/31 13:31:54 [INFO] received CSR 2017/08/31 13:31:54 [INFO] generating key: rsa-2048 2017/08/31 13:31:55 [INFO] encoded CSR 2017/08/31 13:31:55 [INFO] signed certificate with serial number 43372632012323103879829229080989286813242051309 2017/08/31 13:31:55 [WARNING] This certificate lacks a \u0026#34;hosts\u0026#34; field. This makes it unsuitable for websites. For more information see the Baseline Requirements for the Issuance and Management of Publicly-Trusted Certificates, v.1.1.6, from the CA/Browser Forum (https://cabforum.org); specifically, section 10.2.3 (\u0026#34;Information Requirements\u0026#34;). 这将生成如下文件：\ndevuser.csr devuser-key.pem devuser.pem 创建 kubeconfig 文件 # 设置集群参数 export KUBE_APISERVER=\u0026#34;https://172.20.0.113:6443\u0026#34; kubectl config set-cluster kubernetes \\ --certificate-authority=/etc/kubernetes/ssl/ca.pem \\ --embed-certs=true \\ --server=${KUBE_APISERVER} \\ --kubeconfig=devuser.kubeconfig # 设置客户端认证参数 kubectl config set-credentials devuser \\ --client-certificate=/etc/kubernetes/ssl/devuser.pem \\ --client-key=/etc/kubernetes/ssl/devuser-key.pem \\ --embed-certs=true \\ --kubeconfig=devuser.kubeconfig # 设置上下文参数 kubectl config set-context kubernetes \\ --cluster=kubernetes \\ --user=devuser \\ --namespace=dev \\ --kubeconfig=devuser.kubeconfig # 设置默认上下文 kubectl config use-context kubernetes --kubeconfig=devuser.kubeconfig 我们现在查看 kubectl 的 context：\nkubectl config get-contexts CURRENT NAME CLUSTER AUTHINFO NAMESPACE * kubernetes kubernetes admin default-context default-cluster default-admin 显示的用户仍然是 admin，这是因为 kubectl 使用了 $HOME/.kube/config 文件作为了默认的 context 配置，我们只需要将其用刚生成的 devuser.kubeconfig 文件替换即可。\ncp -f ./devuser.kubeconfig /root/.kube/config RoleBinding 如果我们想限制 devuser 用户的行为，需要使用 RBAC创建角色绑定以将该用户的行为限制在某个或某几个 namespace 空间范围内，例如：\nkubectl create rolebinding devuser-admin-binding --clusterrole=admin --user=devuser --namespace=dev kubectl create rolebinding devuser-admin-binding --clusterrole=admin --user=devuser --namespace=test 这样 devuser 用户对 dev 和 test 两个 namespace 具有完全访问权限。\n让我们来验证以下，现在我们在执行：\n# 获取当前的 context kubectl config get-contexts CURRENT NAME CLUSTER AUTHINFO NAMESPACE * kubernetes kubernetes devuser dev * kubernetes kubernetes devuser test # 无法访问 default namespace kubectl get pods --namespace default Error from server (Forbidden): User \u0026#34;devuser\u0026#34; cannot list pods in the namespace \u0026#34;default\u0026#34;. (get pods) # 默认访问的是 dev namespace，您也可以重新设置 context 让其默认访问 test namespace kubectl get pods No resources found. 现在 kubectl 命令默认使用的 context 就是 devuser 了，且该用户只能操作 dev 和 test 这两个 namespace，并拥有完全的访问权限。\n关于角色绑定的更多信息请参考 基于角色的访问控制。\n","date":1653062400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"95918e41a7144722bbc662a72632eb44","permalink":"https://lib.jimmysong.io/kubernetes-handbook/security/kubectl-user-authentication-authorization/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/security/kubectl-user-authentication-authorization/","section":"kubernetes-handbook","summary":"当我们安装好集群后，如果想要把 kubectl 命令交给用户使用，就不得不对用户的身份进行认证和对其权限做出限制。 下面以创建一个 devuser 用户并将其绑定到 dev 和 test 两个 namespace 为例说明。 创建 CA 证书和秘钥 创建 devuser-csr.json 文件 { \"CN\": \"devuser\", \"hosts\": [], \"key\": { \"algo\": \"rsa\", \"size\": 2048","tags":["Kubernetes"],"title":"创建用户认证授权的 kubeconfig 文件","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"在开启了 TLS 的集群中，每当与集群交互的时候少不了的是身份认证，使用 kubeconfig（即证书） 和 token 两种认证方式是最简单也最通用的认证方式，在 dashboard 的登录功能就可以使用这两种登录功能。\n下文分两块以示例的方式来讲解两种登陆认证方式：\n 为 brand 命名空间下的 brand 用户创建 kubeconfig 文件 为集群的管理员（拥有所有命名空间的 amdin 权限）创建 token  使用 kubeconfig 如何生成kubeconfig文件请参考创建用户认证授权的kubeconfig文件。\n 注意我们生成的 kubeconfig 文件中没有 token 字段，需要手动添加该字段。\n 比如我们为 brand namespace 下的 brand 用户生成了名为 brand.kubeconfig 的 kubeconfig 文件，还要再该文件中追加一行 token 的配置（如何生成 token 将在下文介绍），如下所示：\n   kubeconfig 文件  对于访问 dashboard 时候的使用 kubeconfig 文件如brand.kubeconfig 必须追到 token 字段，否则认证不会通过。而使用 kubectl 命令时的用的 kubeconfig 文件则不需要包含 token 字段。\n生成 token 需要创建一个admin用户并授予admin角色绑定，使用下面的yaml文件创建admin用户并赋予他管理员权限，然后可以通过token访问kubernetes，该文件见admin-role.yaml。\n生成kubernetes集群最高权限admin用户的token kind:ClusterRoleBindingapiVersion:rbac.authorization.k8s.io/v1beta1metadata:name:adminannotations:rbac.authorization.kubernetes.io/autoupdate:\u0026#34;true\u0026#34;roleRef:kind:ClusterRolename:cluster-adminapiGroup:rbac.authorization.k8s.iosubjects:- kind:ServiceAccountname:adminnamespace:kube-system---apiVersion:v1kind:ServiceAccountmetadata:name:adminnamespace:kube-systemlabels:kubernetes.io/cluster-service:\u0026#34;true\u0026#34;addonmanager.kubernetes.io/mode:Reconcile然后执行下面的命令创建 serviceaccount 和角色绑定，\nkubectl create -f admin-role.yaml 创建完成后获取secret中token的值。\n# 获取admin-token的secret名字 $ kubectl -n kube-system get secret|grep admin-token admin-token-nwphb kubernetes.io/service-account-token 3 6m # 获取token的值 $ kubectl -n kube-system describe secret admin-token-nwphb Name:\tadmin-token-nwphb Namespace:\tkube-system Labels:\t\u0026lt;none\u0026gt; Annotations:\tkubernetes.io/service-account.name=admin kubernetes.io/service-account.uid=f37bd044-bfb3-11e7-87c0-f4e9d49f8ed0 Type:\tkubernetes.io/service-account-token Data ==== namespace:\t11 bytes token:\t非常长的字符串 ca.crt:\t1310 bytes 也可以使用 jsonpath 的方式直接获取 token 的值，如：\nkubectl -n kube-system get secret admin-token-nwphb -o jsonpath={.data.token}|base64 -d 注意：yaml 输出里的那个 token 值是进行 base64 编码后的结果，一定要将 kubectl 的输出中的 token 值进行 base64 解码，在线解码工具 base64decode，Linux 和 Mac 有自带的 base64 命令也可以直接使用，输入 base64 是进行编码，Linux 中base64 -d 表示解码，Mac 中使用 base64 -D。\n我们使用了 base64 对其重新解码，因为 secret 都是经过 base64 编码的，如果直接使用 kubectl 中查看到的 token 值会认证失败，详见 secret 配置。关于 JSONPath 的使用请参考 JSONPath 手册。\n更简单的方式是直接使用kubectl describe命令获取token的内容（经过base64解码之后）：\nkubectl describe secret admin-token-nwphb 为普通用户生成token 为指定namespace分配该namespace的最高权限，这通常是在为某个用户（组织或者个人）划分了namespace之后，需要给该用户创建token登陆kubernetes dashboard或者调用kubernetes API的时候使用。\n每次创建了新的namespace下都会生成一个默认的token，名为default-token-xxxx。default就相当于该namespace下的一个用户，可以使用下面的命令给该用户分配该namespace的管理员权限。\nkubectl create rolebinding $ROLEBINDING_NAME --clusterrole=admin --serviceaccount=$NAMESPACE:default --namespace=$NAMESPACE  $ROLEBINDING_NAME必须是该namespace下的唯一的 admin表示用户该namespace的管理员权限，关于使用clusterrole进行更细粒度的权限控制请参考RBAC——基于角色的访问控制。 我们给默认的serviceaccount default分配admin权限，这样就不要再创建新的serviceaccount，当然你也可以自己创建新的serviceaccount，然后给它admin权限  参考  JSONPath 手册 - kubernetes.io Kubernetes 中的认证 - kubernetes.io  ","date":1653062400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"f62e9ac51990dda356a16515335ff0f6","permalink":"https://lib.jimmysong.io/kubernetes-handbook/security/auth-with-kubeconfig-or-token/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/security/auth-with-kubeconfig-or-token/","section":"kubernetes-handbook","summary":"在开启了 TLS 的集群中，每当与集群交互的时候少不了的是身份认证，使用 kubeconfig（即证书） 和 token 两种认证方式是最简单也最通用的认证方式，在 dashboard 的登录功能就可以使用这两种登录功能。 下文分两块以示例的方式","tags":["Kubernetes"],"title":"使用 kubeconfig 或 token 进行用户身份认证","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"在安装集群的时候我们在 master 节点上生成了一堆证书、token，还在 kubelet 的配置中用到了 bootstrap token，安装各种应用时，为了能够与 API server 通信创建了各种 service account，在 Dashboard 中使用了 kubeconfig 或 token 登陆，那么这些都属于什么认证方式？如何区分用户的？我特地翻译了下这篇官方文档，想你看了之后你将找到答案。\n重点查看 bearer token 和 HTTP 认证中的 token 使用，我们已经有所应用，如 使用kubeconfig或token进行用户身份认证。\n认识 Kubernetes 中的用户 Kubernetes 集群中包含两类用户：一类是由 Kubernetes 管理的 service account，另一类是普通用户。\n普通用户被假定为由外部独立服务管理。管理员分发私钥，用户存储（如 Keystone 或 Google 帐户），甚至包含用户名和密码列表的文件。在这方面，Kubernetes 没有代表普通用户帐户的对象。无法通过 API 调用的方式向集群中添加普通用户。\n相对的，service account 是由 Kubernetes API 管理的帐户。它们都绑定到了特定的 namespace，并由 API server 自动创建，或者通过 API 调用手动创建。Service account 关联了一套凭证，存储在 Secret，这些凭证同时被挂载到 pod 中，从而允许 pod 与 kubernetes API 之间的调用。\nAPI 请求被绑定到普通用户或 serivce account 上，或者作为匿名请求对待。这意味着集群内部或外部的每个进程，无论从在工作站上输入 kubectl 的人类用户到节点上的 kubelet，到控制平面的成员，都必须在向 API Server 发出请求时进行身份验证，或者被视为匿名用户。\n认证策略 Kubernetes 使用客户端证书、bearer token、身份验证代理或者 HTTP 基本身份验证等身份认证插件来对 API 请求进行身份验证。当有 HTTP 请求发送到 API server 时，插件会尝试将以下属性关联到请求上：\n 用户名：标识最终用户的字符串。常用值可能是 kube-admin 或 jane@example.com。 UID：标识最终用户的字符串，比用户名更加一致且唯一。 组：一组将用户和常规用户组相关联的字符串。 额外字段：包含其他有用认证信息的字符串列表的映射。  所有的值对于认证系统都是不透明的，只有 授权人 才能解释这些值的重要含义。\n您可以一次性启用多种身份验证方式。通常使用至少以下两种认证方式：\n 服务帐户的 service account token 至少一种其他的用户认证的方式  当启用了多个认证模块时，第一个认证模块成功认证后将短路请求，不会进行第二个模块的认证。API server 不会保证认证的顺序。\nsystem:authenticated 组包含在所有已验证用户的组列表中。\n与其他身份验证协议（LDAP、SAML、Kerberos、x509 方案等）的集成可以使用身份验证代理或身份验证 webhook来实现。\nX509 客户端证书 通过将 --client-ca-file=SOMEFILE 选项传递给 API server 来启用客户端证书认证。引用的文件必须包含一个或多个证书颁发机构，用于验证提交给 API server 的客户端证书。如果客户端证书已提交并验证，则使用 subject 的 Common Name（CN）作为请求的用户名。从 Kubernetes 1.4开始，客户端证书还可以使用证书的 organization 字段来指示用户的组成员身份。要为用户包含多个组成员身份，请在证书中包含多个 organization 字段。\n例如，使用 openssl 命令工具生成用于签名认证请求的证书：\nopenssl req -new -key jbeda.pem -out jbeda-csr.pem -subj \u0026#34;/CN=jbeda/O=app1/O=app2\u0026#34; 这将为一个用户名为 ”jbeda“ 的 CSR，属于两个组“app1”和“app2”。\n静态 Token 文件 当在命令行上指定 --token-auth-file=SOMEFILE 选项时，API server 从文件读取 bearer token。目前，token 会无限期地持续下去，并且不重新启动 API server 的话就无法更改令牌列表。\ntoken 文件是一个 csv 文件，每行至少包含三列：token、用户名、用户 uid，其次是可选的组名。请注意，如果您有多个组，则该列必须使用双引号。\ntoken,user,uid,\u0026#34;group1,group2,group3\u0026#34; 在请求中放置 Bearer Token 当使用来自 http 客户端的 bearer token 时，API server 期望 Authorization header 中包含 Bearer token 的值。Bearer token 必须是一个字符串序列，只需使用 HTTP 的编码和引用功能就可以将其放入到 HTTP header 中。例如：如果 bearer token 是 31ada4fd-adec-460c-809a-9e56ceb75269，那么它将出现在 HTTP header 中，如下所示：\nAuthorization: Bearer 31ada4fd-adec-460c-809a-9e56ceb75269 Bootstrap Token 该功能仍处于 alpha 版本。\n为了简化新集群的初始化引导过程，Kubernetes 中包含了一个名为 Bootstrap Token 的动态管理的 bearer token。这些 token 使用 Secret 存储在 kube-system namespace 中，在那里它们可以被动态管理和创建。Controller Manager 中包含了一个 TokenCleaner 控制器，用于在 bootstrap token 过期时删除将其删除。\n这些 token 的形式是 [a-z0-9]{6}.[a-z0-9]{16}。第一部分是 Token ID，第二部分是 Token Secret。您在 HTTP header 中指定的 token 如下所示：\nAuthorization: Bearer 781292.db7bc3a58fc5f07e 在 API server 的启动参数中加上 --experimental-bootstrap-token-auth 标志以启用 Bootstrap Token Authenticator。您必须通过 Controller Manager 上的 --controllers 标志启用 TokenCleaner 控制器，如 --controllers=*,tokencleaner。如果您使用它来引导集群， kubeadm 会为您完成。\n认证者认证为 system:bootstrap:\u0026lt;Token ID\u0026gt; 。被包含在 system:bootstrappers 组中。命名和组是有意限制用户使用过去的 bootstap token。可以使用用户名和组（kubeadm 使用）来制定适当的授权策略以支持引导集群。\n有关 Bootstrap Token 身份验证器和控制器的更深入的文档，以及如何使用 kubeadm 管理这些令牌，请参阅 Bootstrap Token。\n静态密码文件 通过将 --basic-auth-file=SOMEFILE 选项传递给 API server 来启用基本身份验证。目前，基本身份验证凭证将无限期地保留，并且密码在不重新启动API服务器的情况下无法更改。请注意，为了方便起见，目前支持基本身份验证，而上述模式更安全更容易使用。\n基本身份认证是一个 csv 文件，至少包含3列：密码、用户名和用户 ID。在 Kubernetes 1.6 和更高版本中，可以指定包含以逗号分隔的组名称的可选第四列。如果您有多个组，则必须将第四列值用双引号（“）括起来，请参阅以下示例：\npassword,user,uid,\u0026#34;group1,group2,group3\u0026#34; 当使用来自 HTTP 客户端的基本身份验证时，API server 需要 Authorization header 中包含 Basic BASE64ENCODED(USER:PASSWORD) 的值。\nService Account Token Service account 是一个自动启用的验证器，它使用签名的 bearer token 来验证请求。该插件包括两个可选的标志：\n --service-account-key-file 一个包含签名 bearer token 的 PEM 编码文件。如果未指定，将使用 API server 的 TLS 私钥。 --service-account-lookup 如果启用，从 API 中删除掉的 token 将被撤销。  Service account 通常 API server 自动创建，并通过 ServiceAccount 注入控制器 关联到集群中运行的 Pod 上。Bearer token 挂载到 pod 中众所周知的位置，并允许集群进程与 API server 通信。 帐户可以使用 PodSpec 的 serviceAccountName 字段显式地与Pod关联。\n注意： serviceAccountName 通常被省略，因为这会自动生成。\napiVersion:apps/v1beta2kind:Deploymentmetadata:name:nginx-deploymentnamespace:defaultspec:replicas:3template:metadata:# ...spec:containers:- name:nginximage:nginx:1.7.9serviceAccountName:bob-the-botService account bearer token 在集群外使用也是完全有效的，并且可以用于为希望与 Kubernetes 通信的长期运行作业创建身份。要手动创建 service account，只需要使用 kubectl create serviceaccount (NAME) 命令。这将在当前的 namespace 和相关连的 secret 中创建一个 service account。\n$ kubectl create serviceaccount jenkins serviceaccount \u0026#34;jenkins\u0026#34; created $ kubectl get serviceaccounts jenkins -o yaml apiVersion: v1 kind: ServiceAccount metadata: # ... secrets: - name: jenkins-token-1yvwg 创建出的 secret 中拥有 API server 的公共 CA 和前面的 JSON Web Token（JWT）。\n$ kubectl get secret jenkins-token-1yvwg -o yaml apiVersion: v1 data: ca.crt: (APISERVER\u0026#39;S CA BASE64 ENCODED) namespace: ZGVmYXVsdA== token: (BEARER TOKEN BASE64 ENCODED) kind: Secret metadata: # ... type: kubernetes.io/service-account-token 注意：所有值是基于 base64 编码的，因为 secret 总是 …","date":1653062400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"d71bdb7ba4115beb3f1161f8f943d8c7","permalink":"https://lib.jimmysong.io/kubernetes-handbook/security/authentication/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/security/authentication/","section":"kubernetes-handbook","summary":"在安装集群的时候我们在 master 节点上生成了一堆证书、token，还在 kubelet 的配置中用到了 bootstrap token，安装各种应用时，为了能够与 API server 通信创建了各种 service account，在 Dashboard 中使用了 kubeconfig 或 token 登陆，那么这些都属于什么认证","tags":["Kubernetes"],"title":"Kubernetes 中的用户与身份认证授权","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"本文是对 Kubernetes 集群安全性管理的最佳实践。\n端口 请注意管理好以下端口。\n   端口 进程 描述     4149/TCP kubelet 用于查询容器监控指标的 cAdvisor 端口   10250/TCP kubelet 访问节点的 API 端口   10255/TCP kubelet 未认证的只读端口，允许访问节点状态   10256/TCP kube-proxy kube-proxy 的健康检查服务端口   9099/TCP calico-felix calico 的健康检查服务端口（如果使用 calico/canal）   6443/TCP kube-apiserver Kubernetes API 端口    Kubernetes 安全扫描工具 kube-bench kube-bench 可以消除大约 kubernetes 集群中 95％的配置缺陷。通过应用 CIS Kubernetes Benchmark 来检查 master 节点、node 节点及其控制平面组件，从而确保集群设置了特定安全准则。在经历特定的 Kubernetes 安全问题或安全增强功能之前，这应该是第一步。\nAPI 设置 授权模式和匿名认证\n像 kops 这样的一些安装程序会为集群使用 AlwaysAllow 授权模式。这将授予任何经过身份验证的实体拥有完全访问集群的权限。应该使用 RBAC 基于角色的访问控制。检查您的 kube-apiserver 进程的 --authorization-mode 参数。有关该主题的更多信息，请访问认证概览。要强制进行身份验证，请确保通过设置 --anonymous-auth = false 禁用匿名身份验证。\n注意这不影响 Kubelet 授权模式。kubelet 本身公开了一个 API 来执行命令，通过它可以完全绕过 Kubernetes API。\n更多关于使用 kops 等工具自动安装 Kubernetes 集群的安全配置注意事项请参考 Kubernetes Security - Best Practice Guide。\n参考  Kubernetes Security - Best Practice Guide - github.com Kubernetes v1.7 security in practice - acotten.com Isolate containers with a user namespace - docs.docker.com Docker Security – It’s a Layered Approach - logz.io Kubernetes 1.8: Security, Workloads and Feature Depth - blog.kubernetes.io Security Matters: RBAC in Kubernetes - blog.heptio.co  ","date":1653062400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"3e2c8ff64e39e264fa2cad38dd21e662","permalink":"https://lib.jimmysong.io/kubernetes-handbook/security/kubernetes-security-best-practice/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/security/kubernetes-security-best-practice/","section":"kubernetes-handbook","summary":"本文是对 Kubernetes 集群安全性管理的最佳实践。 端口 请注意管理好以下端口。 端口 进程 描述 4149/TCP kubelet 用于查询容器监控指标的 cAdvisor 端口 10250/TCP kubelet 访问节点的 API 端口 10255/TCP kubelet 未认证的只读端口，允许访问节点状态 10256/TCP kube-proxy kube-proxy 的健康检查服务端口 9099/TCP calico-felix calico 的健康","tags":["Kubernetes"],"title":"Kubernetes 集群安全性配置最佳实践","type":"book"},{"authors":null,"categories":["Istio"],"content":"虽然 Envoy 本质上采用了最终一致性模型，但 ADS 提供了对 API 更新推送进行排序的机会，并确保单个管理服务器对 Envoy 节点的 API 更新具有亲和力。ADS 允许管理服务器在单个双向 gRPC 流上传递一个或多个 API 及其资源。否则，一些 API（如 RDS 和 EDS）可能需要管理多个流并连接到不同的管理服务器。\nADS 通过适当得排序 xDS 可以无中断的更新 Envoy 的配置。例如，假设 foo.com 已映射到集群 X。我们希望将路由表中将该映射更改为在集群 Y。为此，必须首先提供 X、Y 这两个集群的 CDS/EDS 更新。\n如果没有 ADS，CDS/EDS/RDS 流可能指向不同的管理服务器，或者位于需要协调的不同 gRPC流连接的同一管理服务器上。EDS 资源请求可以跨两个不同的流分开，一个用于 X，一个用于 Y。ADS 将这些流合并到单个流和单个管理服务器，从而无需分布式同步就可以正确地对更新进行排序。使用 ADS，管理服务器将在单个流上提供 CDS、EDS 和 RDS 更新。\n 下一章   ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"1ba0e1f1da760faba6b2af5cb53e102e","permalink":"https://lib.jimmysong.io/istio-handbook/data-plane/envoy-ads/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/istio-handbook/data-plane/envoy-ads/","section":"istio-handbook","summary":"虽然 Envoy 本质上采用了最终一致性模型，但 ADS 提供了对 API 更新推送进行排序的机会，并确保单个管理服务器对 Envoy 节点的 API 更新具有亲和力。ADS 允许管理服务器在单个双向 gRPC 流上传递一个或多个 API 及其资源。否则，一些 API（","tags":["Istio","Service Mesh"],"title":"ADS（聚合发现服务）","type":"book"},{"authors":null,"categories":["Istio"],"content":"ProxyConfig 暴露了代理级别的配置选项。ProxyConfig 可以在每个工作负载、命名空间或整个网格的基础上进行配置。ProxyConfig 不是一个必要的资源；有默认值。\n注意：ProxyConfig 中的字段不是动态配置的——更改需要重新启动工作负载才能生效。\n对于任何命名空间，包括根配置命名空间，只有一个无工作负载选择器的 ProxyConfig 资源是有效的。\n对于带有工作负载选择器的资源，只有一个选择任何特定工作负载的资源才有效。\n对于网格级别的配置，请将资源放在你的 Istio 安装的根配置命名空间中，而不使用工作负载选择器：\napiVersion:networking.istio.io/v1beta1kind:ProxyConfigmetadata:name:my-proxyconfignamespace:istio-systemspec:concurrency:0image:type:distroless对于命名空间级别的配置，将资源放在所需的命名空间中，不需要工作负载选择器：\napiVersion:networking.istio.io/v1beta1kind:ProxyConfigmetadata:name:my-ns-proxyconfignamespace:user-namespacespec:concurrency:0对于工作负载级别的配置，在 ProxyConfig 资源上设置选择器字段：\napiVersion:networking.istio.io/v1beta1kind:ProxyConfigmetadata:name:per-workload-proxyconfignamespace:examplespec:selector:labels:app:ratingsconcurrency:0image:type:debug如果 ProxyConfig CR 被定义为与工作负载相匹配，它将与 proxy.istio.io/config 注释合并（如果存在的话），对于重叠的字段，CR 优先于注释。同样地，如果定义了一个网格的 ProxyConfig CR，并且设置了 MeshConfig.DefaultConfig，那么这两个资源将被合并，对于重叠的字段，CR 优先。\n配置项 下图是 ProxyConfig 的资源配置拓扑图。\n  ProxyConfig 资源配置拓扑图  ProxyConfig 的顶级配置如下。\n selector：可选的。选择器指定应用此 ProxyConfig 资源的 pod/VM 的集合。如果不设置，ProxyConfig 资源将被应用于定义该资源的命名空间中的所有工作负载。 concurrency：要运行的工作线程的数量。如果没有设置，默认为 2。如果设置为 0，这将被配置为使用机器上的所有内核，使用 CPU 请求和限制来选择一个值，限制优先于请求。 environmentVariables：代理的额外环境变量。以ISTIO_META_ 开头的名字将被包含在生成的引导配置中，并被发送到XDS 服务器。 image：指定代理镜像的细节。  ProxyImage 配置 用于构建代理镜像 URL：$hub/$imagename/$tag-$imagetype 。例如：docker.io/istio/proxyv2:1.11.1 或 docker.io/istio/proxyv2:1.11.1-distroless 。\n imageType：Istio 会发布 default、debug 和 distroless 镜像。如果这些镜像类型（例如：centos）被发布到指定的镜像仓库，则允许使用其他值。支持的值：default、debug、distroless。  参考  ProxyConfig - istio.io   下一章   ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"5237b1fdd7ad24f7bd71b8a75d4c8c35","permalink":"https://lib.jimmysong.io/istio-handbook/config-networking/proxy-config/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/istio-handbook/config-networking/proxy-config/","section":"istio-handbook","summary":"ProxyConfig 暴露了代理级别的配置选项。ProxyConfig 可以在每个工作负载、命名空间或整个网格的基础上进行配置。ProxyConfig 不是一个必要的资源；有默认值。 注意：ProxyConfig 中的字段不是动","tags":["Istio","Service Mesh"],"title":"ProxyConfig","type":"book"},{"authors":null,"categories":["Istio"],"content":"通过 ServiceEntry 资源，我们可以向 Istio 的内部服务注册表添加额外的条目，并使不属于我们网格的外部服务或内部服务看起来像是我们服务网格的一部分。\n当一个服务在服务注册表中时，我们就可以使用流量路由、故障注入和其他网格功能，就像我们对其他服务一样。\n下面是一个 ServiceEntry 资源的例子，它声明了一个可以通过 HTTPS 访问的外部 API（api.external-svc.com）。\napiVersion:networking.istio.io/v1alpha3kind:ServiceEntrymetadata:name:external-svcspec:hosts:- api.external-svc.comports:- number:443name:httpsprotocol:TLSresolution:DNSlocation:MESH_EXTERNALhosts 字段可以包含多个外部 API，在这种情况下，Envoy sidecar 会根据下面的层次结构来进行检查。如果任何一项不能被检查，Envoy 就会转到层次结构中的下一项。\n HTTP Authority 头（在HTTP/2中）和 Host 头（HTTP/1.1中） SNI IP 地址和端口  如果上述数值都无法检查，Envoy 会根据 Istio 的安装配置，盲目地转发请求或放弃该请求。\n与 WorkloadEntry 资源一起，我们可以处理虚拟机工作负载向 Kubernetes 迁移的问题。在 WorkloadEntry 中，我们可以指定在虚拟机上运行的工作负载的细节（名称、地址、标签），然后使用 ServiceEntry 中的 workloadSelector 字段，使虚拟机成为 Istio 内部服务注册表的一部分。\n例如，假设 customers 的工作负载正在两个虚拟机上运行。此外，我们已经有在 Kubernetes 中运行的 Pod，其标签为 app: customers。\n让我们这样来定义 WorkloadEntry 资源：\napiVersion:networking.istio.io/v1alpha3kind:WorkloadEntrymetadata:name:customers-vm-1spec:serviceAccount:customersaddress:1.0.0.0labels:app:customersinstance-id:vm1---apiVersion:networking.istio.io/v1alpha3kind:WorkloadEntrymetadata:name:customers-vm-2spec:serviceAccount:customersaddress:2.0.0.0labels:app:customersinstance-id:vm2现在我们可以创建一个 ServiceEntry 资源，该资源同时跨越 Kubernetes 中运行的工作负载和虚拟机：\napiVersion:networking.istio.io/v1alpha3kind:ServiceEntrymetadata:name:customers-svcspec:hosts:- customers.comlocation:MESH_INTERNALports:- number:80name:httpprotocol:HTTPresolution:STATICworkloadSelector:labels:app:customers在位置字段中设置 MESH_INTERNAL，我们是说这个服务是网格的一部分。这个值通常用于包括未管理的基础设施（VM）上的工作负载的情况。这个字段的另一个值，MESH_EXTERNAL，用于通过 API 消费的外部服务。MESH_INTERNAL 和 MESH_EXTERNAL 设置控制了网格中的 sidecar 如何尝试与工作负载进行通信，包括它们是否会默认使用 Istio 双向 TLS。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"74f6a771085405ded072e6ff614cad5b","permalink":"https://lib.jimmysong.io/istio-handbook/traffic-management/seviceentry/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/istio-handbook/traffic-management/seviceentry/","section":"istio-handbook","summary":"通过 ServiceEntry 资源，我们可以向 Istio 的内部服务注册表添加额外的条目，并使不属于我们网格的外部服务或内部服务看起来像是我们服务网格的一部分。 当一个服务在服务注册表中时，我们就可以使用流量路由、故障注入和其他网格功能","tags":["Istio","Service Mesh"],"title":"ServiceEntry","type":"book"},{"authors":null,"categories":["Envoy"],"content":"在这个实验中，我们将看一个例子，说明我们如何设置 Envoy，以便在一个 IP 地址上用不同的证书为多个网站服务。\n我们将使用自签名的证书进行测试，但如果使用真实签名的证书，其过程是相同的。\n使用 openssl，我们将在 certs 文件夹中为 www.hello.com 和 www.example.com 创建自签名证书。\n$ mkdir certs \u0026amp;\u0026amp; cd certs $ openssl req -nodes -new -x509 -keyout www_hello_com.key -out www_hello_com.cert -subj \u0026#34;/C=US/ST=Washington/L=Seattle/O=Hello LLC/OU=Org/CN=www.hello.com\u0026#34; $ openssl req -nodes -new -x509 -keyout www_example_com.key -out www_example_com.cert -subj \u0026#34;/C=US/ST=Washington/L=Seattle/O=Example LLC/OU=Org/CN=www.example.com\u0026#34; 对于每个通用名称，我们最终会有两个文件：私钥和证书（例如，www_example_com.key 和 www_example_com.cert）。\n我们将为每个过滤链的匹配分别配置 transport_socket 字段。下面是一个如何定义 TLS 传输套接字并提供密钥和证书的片段。\n...transport_socket:name:envoy.transport_sockets.tlstyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.DownstreamTlsContextcommon_tls_context:tls_certificates:- certificate_chain:filename:certs/www_hello_com.certprivate_key:filename:certs/www_hello_com.key...因为我们想根据 SNI 使用不同的证书，我们将在 TLS 监听器中添加一个 TLS 检查器过滤器，使用 filter_chain_match 和 server_names 字段来根据 SNI 进行匹配。\n下面是两个过滤链匹配部分：注意每个过滤链匹配都有自己的 transport_socket，有指向证书和密钥文件的指针。\nlistener_filters:- name:\u0026#34;envoy.filters.listener.tls_inspector\u0026#34;typed_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.listener.tls_inspector.v3.TlsInspectorfilter_chains:- filter_chain_match:server_names:\u0026#34;www.example.com\u0026#34;filters:transport_socket:name:envoy.transport_sockets.tls...http:filters:...- filter_chain_match:server_names:\u0026#34;www.hello.com\u0026#34;filters:transport_socket:name:envoy.transport_sockets.tls...http:filters:...你可以在 5-lab-2-tls_match.yaml 文件中找到完整的配置，由于需要绑定443端口(privileged port)，所以使用 sudo func-e run -c 5-lab-2-tls_match.yaml 运行它。由于我们将只使用 openssl 进行连接，我们不需要集群端点的运行。\n为了检查 SNI 匹配是否正常工作，我们可以使用 openssl 并连接到提供服务器名称的 Envoy 监听器。例如：\n$ openssl s_client -connect 0.0.0.0:443 -servername www.example.comCONNECTED(00000003)depth=0 C = US, ST = Washington, L = Seattle, O = Example LLC, OU = Org, CN = www.example.comverify error:num=18:self signed certificateverify return:1depth=0 C = US, ST = Washington, L = Seattle, O = Example LLC, OU = Org, CN = www.example.comverify return:1---Certificate chain0s:C = US, ST = Washington, L = Seattle, O = Example LLC, OU = Org, CN = www.example.comi:C = US, ST = Washington, L = Seattle, O = Example LLC, OU = Org, CN = www.example.com...该命令将根据所提供的服务器名称返回正确的对等证书。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"2b336ae15a4d4ae3f50794fe972362da","permalink":"https://lib.jimmysong.io/envoy-handbook/listener/lab10/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/envoy-handbook/listener/lab10/","section":"envoy-handbook","summary":"在这个实验中，我们将看一个例子，说明我们如何设置 Envoy，以便在一个 IP 地址上用不同的证书为多个网站服务。 我们将使用自签名的证书进行测试，但如果使用真实签名的证书，其过程是相同的。 使用 openssl，","tags":["Envoy"],"title":"实验 10：TLS 检查器过滤器","type":"book"},{"authors":null,"categories":["Envoy"],"content":"在这个实验中，我们将展示如何使用和配置 HTTP 分接式过滤器。我们将配置一个 /error 路由，它返回一个直接的响应，其主体为 error 值。然后，在分接式过滤器中，我们将配置匹配器，以匹配任何响应主体包含 error 字符串和请求头 debug: true 的请求。如果这两个条件都为真，那么我们就分接该请求并将输出写入一个前缀为 tap_debug 的文件。\n让我们从创建匹配配置开始。我们将使用两个匹配器，一个用来匹配请求头（http_request_headers_match），另一个用来匹配响应体（http_response_generic_body_match）。我们将用逻辑上的 AND 来组合这两个条件。\n下面是匹配配置的样子。\n- name:envoy.filters.http.taptyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.http.tap.v3.Tapcommon_config:static_config:match:and_match:rules:- http_request_headers_match:headers:name:debugstring_match:exact:\u0026#34;true\u0026#34;- http_response_generic_body_match:patterns:- string_match:error我们将使用 JSON_BODY_AS_STRING 格式，并将输出写入以 tap_debug 为前缀的文件。\noutput_config:sinks:- format:JSON_BODY_AS_STRINGfile_per_tap:path_prefix:tap_debug让我们把这两块放在一起，创建一个完整的配置。我们将使用 direct_response，所以我们不需要设置或运行任何额外的服务。\nstatic_resources:listeners:- name:listener_0address:socket_address:address:0.0.0.0port_value:10000filter_chains:- filters:- name:envoy.filters.network.http_connection_managertyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManagerstat_prefix:ingress_httphttp_filters:- name:envoy.filters.http.taptyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.http.tap.v3.Tapcommon_config:static_config:match:and_match:rules:- http_request_headers_match:headers:name:debugstring_match:exact:\u0026#34;true\u0026#34;- http_response_generic_body_match:patterns:- string_match:erroroutput_config:sinks:- format:JSON_BODY_AS_STRINGfile_per_tap:path_prefix:tap_debug- name:envoy.filters.http.routerroute_config:name:local_routevirtual_hosts:- name:local_servicedomains:[\u0026#34;*\u0026#34;]routes:- match:path:\u0026#34;/\u0026#34;direct_response:status:200body:inline_string:hello- match:path:\u0026#34;/error\u0026#34;direct_response:status:500body:inline_string:error将上述 YAML 保存为 7-lab-1-tap-filter-1.yaml 文件，并使用 func-e CLI 运行它：\nfunc-e run -c 7-lab-1-tap-filter-1.yaml \u0026amp; 如果我们发送一个请求到 http://localhost:10000，我们会收到符合第一条路由的响应（HTTP 200 和 body hello）。这个请求不会被分接，因为我们没有提供任何头信息，响应体也没有包含 error 值。\n让我们试着设置 debug 头并向 /error 端点发送一个请求。\n$ curl -H \u0026#34;debug: true\u0026#34; localhost:10000/error error 这一次，在同一个文件夹中创建了一个包含被挖掘的请求内容的 JSON 文件。下面是该文件的内容应该是这样的。\n{ \u0026#34;http_buffered_trace\u0026#34;: { \u0026#34;request\u0026#34;: { \u0026#34;headers\u0026#34;: [ { \u0026#34;key\u0026#34;: \u0026#34;:authority\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;localhost:10000\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;:path\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;/error\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;:method\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;GET\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;:scheme\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;http\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;user-agent\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;curl/7.64.0\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;accept\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;*/*\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;debug\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;true\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;x-forwarded-proto\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;http\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;x-request-id\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;4855ee5d-7798-4c50-8692-a6989e72ca9b\u0026#34; } ], \u0026#34;trailers\u0026#34;: [] }, \u0026#34;response\u0026#34;: { \u0026#34;headers\u0026#34;: [ { \u0026#34;key\u0026#34;: \u0026#34;:status\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;500\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;content-length\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;5\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;content-type\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;text/plain\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;date\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;Mon, 29 Nov 2021 22:38:32 GMT\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;server\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;envoy\u0026#34; } ], \u0026#34;body\u0026#34;: { \u0026#34;truncated\u0026#34;: false, \u0026#34;as_string\u0026#34;: \u0026#34;error\u0026#34; }, \u0026#34;trailers\u0026#34;: [] } } } 输出显示了所有的请求头和 Trailer，以及我们收到的响应。\n我们将在下一个例子中使用同样的场景，但我们将使用 /tap 管理端点来实现它。\n首先，让我们创建 Envoy 配置。\nstatic_resources:listeners:- name:listener_0address:socket_address:address:0.0.0.0port_value:10000filter_chains:- filters:- name:envoy.filters.network.http_connection_managertyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManagerstat_prefix:ingress_httphttp_filters:- name:envoy.filters.http.taptyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.http.tap.v3.Tapcommon_config:admin_config:config_id:my_tap_id- name:envoy.filters.http.routerroute_config:name:local_routevirtual_hosts:- name:local_servicedomains:[\u0026#34;*\u0026#34;]routes:- match:path:\u0026#34;/\u0026#34;direct_response:status:200body:inline_string:hello- match:path:\u0026#34;/error\u0026#34;direct_response:status:500body:inline_string:erroradmin:address:socket_address:address:0.0.0.0port_value:9901这一次，我们使用 admin_config 字段并指定配置 ID。此外，我们要使管理接口使用 /tap 端点。\n将上述 YAML 保存为 7-lab-1-tap-filter-2.yaml，并使用 func-e CLI 运行它。\nfunc-e run -c 7-lab-1-tap-filter-2.yaml \u0026amp; 如果我们尝试向 / 和 error 路径发送请求，我们会得到预期的响应。我们必须用 tap 配置向 /tap 端点发送一个 POST 请求，以启用请求分接。\n让我们使用这个符合任何请求的 tap 配置。\n{ \u0026#34;config_id\u0026#34;: \u0026#34;my_tap_id\u0026#34;, \u0026#34;tap_config\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;any_match\u0026#34;: true }, \u0026#34;output_config\u0026#34;: { \u0026#34;sinks\u0026#34;: [ { \u0026#34;streaming_admin\u0026#34;: {} } ] } } } 注意，我们提供的是与我们在 Envoy 配置中定义的 ID 相匹配的配置 ID。如果我们提供了一个无效的配置 ID，那么在向 /tap 端点发送 POST 请求时，我们会得到一个错误。\nUnknown config id \u0026#39;some_tap_id\u0026#39;. No extension has registered with this id. 我们还使用 streaming_admin 字段作为输出汇，这意味着如果 /tap 的 POST 请求被接受，那么 Envoy 将流式处理序列化的 JSON 信息，直到我们终止请求。\n让我们把上述 JSON 保存到 tap-config-any.json，然后用 cURL 向 /tap 端点发送一个 POST 请求。\ncurl -X POST -d @tap-config-any.json http://localhost:9901/tap 我们将打开第二个终端窗口，向 localhost:10000 发送一个 cURL 请求，以测试配置。由于我们对所有的请求都进行了匹配，我们将在第一个终端窗口中看到流式分接的输出。\n{ \u0026#34;http_buffered_trace\u0026#34;: { \u0026#34;request\u0026#34;: { \u0026#34;headers\u0026#34;: [ { \u0026#34;key\u0026#34;: \u0026#34;:authority\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;localhost:10000\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;:path\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;/\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;:method\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;POST\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;:scheme\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;http\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;user-agent\u0026#34;, \u0026#34;value\u0026#34;: …","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"2bd69459f63f1070de91c6a02f244e18","permalink":"https://lib.jimmysong.io/envoy-handbook/admin-interface/lab15/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/envoy-handbook/admin-interface/lab15/","section":"envoy-handbook","summary":"在这个实验中，我们将展示如何使用和配置 HTTP 分接式过滤器。我们将配置一个 /error 路由，它返回一个直接的响应，其主体为 error 值。然后，在分接式过滤器中，我们将配置匹配器，以匹配任何响应主体包含 error 字符串和请求头 debug: true 的请","tags":["Envoy"],"title":"实验15：使用 HTTP 分接式过滤器","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"本文列举了集中访问 Kubernetes 集群的方式。\n第一次使用 kubectl 访问 如果您是第一次访问 Kubernetes API 的话，我们建议您使用 Kubernetes 命令行工具：kubectl。\n为了访问集群，您需要知道集群的地址，并且需要有访问它的凭证。通常，如果您完成了入门指南那么这些将会自动设置，或者其他人为您部署的集群提供并给您凭证和集群地址。\n使用下面的命令检查 kubectl 已知的集群的地址和凭证：\n$ kubectl config view 直接访问 REST API Kubectl 处理对 apiserver 的定位和认证。如果您想直接访问 REST API，可以使用像 curl、wget 或浏览器这样的 http 客户端，有以下几种方式来定位和认证：\n 以 proxy 模式运行 kubectl。  推荐方法。 使用已保存的 apiserver 位置信息。 使用自签名证书验证 apiserver 的身份。 没有 MITM（中间人攻击）的可能。 认证到 apiserver。 将来，可能会做智能的客户端负载均衡和故障转移。   直接向 http 客户端提供位置和凭据。  替代方法。 适用于通过使用代理而混淆的某些类型的客户端代码。 需要将根证书导入浏览器以防止 MITM。    使用 kubectl proxy 以下命令作为反向代理的模式运行 kubectl。 它处理对 apiserver 的定位并进行认证。\n像这样运行：\n$ kubectl proxy --port=8080 \u0026amp; 然后您可以使用 curl、wget 或者浏览器来访问 API，如下所示：\n$ curl http://localhost:8080/api/ { \u0026#34;versions\u0026#34;: [ \u0026#34;v1\u0026#34; ] } 不使用 kubectl proxy（1.3.x 以前版本） 通过将认证 token 直接传递给 apiserver 的方式，可以避免使用 kubectl proxy，如下所示：\n$ APISERVER=$(kubectl config view | grep server | cut -f 2- -d \u0026#34;:\u0026#34; | tr -d \u0026#34; \u0026#34;) $ TOKEN=$(kubectl config view | grep token | cut -f 2 -d \u0026#34;:\u0026#34; | tr -d \u0026#34; \u0026#34;) $ curl $APISERVER/api --header \u0026#34;Authorization: Bearer $TOKEN\u0026#34; --insecure { \u0026#34;versions\u0026#34;: [ \u0026#34;v1\u0026#34; ] } 不使用 kubectl proxy（1.3.x 以后版本） 在 Kubernetes 1.3 或更高版本中，kubectl config view 不再显示 token。 使用 kubectl describe secret … 获取 default service account 的 token，如下所示：\n$ APISERVER=$(kubectl config view | grep server | cut -f 2- -d \u0026#34;:\u0026#34; | tr -d \u0026#34; \u0026#34;) $ TOKEN=$(kubectl describe secret $(kubectl get secrets | grep default | cut -f1 -d \u0026#39; \u0026#39;) | grep -E \u0026#39;^token\u0026#39; | cut -f2 -d\u0026#39;:\u0026#39; | tr -d \u0026#39;\\t\u0026#39;) $ curl $APISERVER/api --header \u0026#34;Authorization: Bearer $TOKEN\u0026#34; --insecure { \u0026#34;kind\u0026#34;: \u0026#34;APIVersions\u0026#34;, \u0026#34;versions\u0026#34;: [ \u0026#34;v1\u0026#34; ], \u0026#34;serverAddressByClientCIDRs\u0026#34;: [ { \u0026#34;clientCIDR\u0026#34;: \u0026#34;0.0.0.0/0\u0026#34;, \u0026#34;serverAddress\u0026#34;: \u0026#34;10.0.1.149:443\u0026#34; } ] } 以上示例使用--insecure 标志。 这使得它容易受到 MITM 攻击。 当 kubectl 访问集群时，它使用存储的根证书和客户端证书来访问服务器。 （这些安装在~/.kube目录中）。 由于集群证书通常是自签名的，因此可能需要特殊配置才能让您的 http 客户端使用根证书。\n对于某些群集，apiserver 可能不需要身份验证；可以选择在本地主机上服务，或者使用防火墙保护。 对此还没有一个标准。配置对API的访问 描述了群集管理员如何配置此操作。 这种方法可能与未来的高可用性支持相冲突。\n编程访问 API Kubernetes 支持 Go 和 Python 客户端库。\nGo 客户端  要获取该库，请运行以下命令：go get k8s.io/client-go/\u0026lt;version number\u0026gt;/kubernetes 请参阅 https://github.com/kubernetes/client-go 以查看支持哪些版本。 使用 client-go 客户端编程。请注意，client-go 定义了自己的 API 对象，因此如果需要，请从 client-go 而不是从主存储库导入 API 定义，例如导入 k8s.io/client-go/1.4/pkg/api/v1 是正确的。  Go 客户端可以使用与 kubectl 命令行工具相同的 kubeconfig 文件 来定位和验证 apiserver。参考官方 示例 和 client-go 示例。\n如果应用程序在群集中以 Pod 的形式部署，请参考 下一节。\nPython 客户端 要使用 Python client，请运行以下命令：pip install kubernetes。查看 Python 客户端库页面 获取更多的安装选择。\nPython 客户端可以使用与 kubectl 命令行工具相同的 kubeconfig 文件 来定位和验证 apiserver。\n其他语言 还有更多的客户端库可以用来访问 API。有关其他库的验证方式，请参阅文档。\n在 Pod 中访问 API 在 Pod 中访问 API 时，定位和认证到 API server 的方式有所不同。在 Pod 中找到 apiserver 地址的推荐方法是使用kubernetes DNS 名称，将它解析为服务 IP，后者又将被路由到 apiserver。\n向 apiserver 认证的推荐方法是使用 service account 凭据。通过 kube-system，pod 与 service account 相关联，并且将该 service account 的凭据（token）放入该 pod 中每个容器的文件系统树中，位于 /var/run/secrets/kubernetes.io/serviceaccount/token。\n如果可用，证书包将位于每个容器的文件系统树的 /var/run/secrets/kubernetes.io/serviceaccount/ca.crt 位置，并用于验证 apiserver 的服务证书。\n最后，用于 namespace API 操作的默认 namespace 放在每个容器中的 /var/run/secrets/kubernetes.io/serviceaccount/namespace 中。\n在 pod 中，连接到 API 的推荐方法是：\n  将 kubectl proxy 作为 pod 中的一个容器来运行，或作为在容器内运行的后台进程。它将 Kubernetes API 代理到 pod 的本地主机接口，以便其他任何 pod 中的容器内的进程都可以访问它。\n  使用 Go 客户端库，并使用 rest.InClusterConfig() 和 kubernetes.NewForConfig() 函数创建一个客户端。\n他们处理对 apiserver 的定位和认证。示例\n  在以上的几种情况下，都需要使用 pod 的凭据与 apiserver 进行安全通信。\n访问集群中运行的 service 上一节是关于连接到 kubernetes API server。这一节是关于连接到 kubernetes 集群中运行的 service。在 Kubernetes 中，node、 pod 和 services 都有它们自己的 IP。很多情况下，集群中 node 的 IP、Pod 的 IP、service 的 IP 都是不可路由的，因此在集群外面的机器就无法访问到它们，例如从您自己的笔记本电脑。\n连接的方式 您可以选择以下几种方式从集群外部连接到 node、pod 和 service：\n 通过 public IP 访问 service。  使用 NodePort 和 LoadBalancer 类型的 service，以使 service 能够在集群外部被访问到。 根据您的群集环境，这可能会将服务暴露给您的公司网络，或者可能会将其暴露在互联网上。想想暴露的服务是否安全。它是否自己进行身份验证？ 将 pod 放在服务后面。 要从一组副本（例如为了调试）访问一个特定的 pod，请在 pod 上放置一个唯一的 label，并创建一个选择该 label 的新服务。 在大多数情况下，应用程序开发人员不需要通过 node IP 直接访问节点。   通过 Proxy 规则访问 service、node、pod。  在访问远程服务之前，请执行 apiserver 认证和授权。 如果服务不够安全，无法暴露给互联网，或者为了访问节点 IP 上的端口或进行调试，请使用这种方式。 代理可能会导致某些 Web 应用程序出现问题。 仅适用于 HTTP/HTTPS。 见此描述。   在集群内访问 node 和 pod。  运行一个 pod，然后使用 kubectl exec 命令连接到 shell。从该 shell 中连接到其他 node、pod 和 service。 有些集群可能允许 ssh 到集群上的某个节点。 从那个节点您可以访问到集群中的服务。这是一个非标准的方法，它可能将在某些集群上奏效，而在某些集群不行。这些节点上可能安装了浏览器和其他工具也可能没有。群集 DNS 可能无法正常工作。    访问内置服务 通常集群内会有几个在 kube-system 中启动的服务。使用 kubectl cluster-info 命令获取该列表：\n$ kubectl cluster-info Kubernetes master is running at https://104.197.5.247 elasticsearch-logging is running at https://104.197.5.247/api/v1/namespaces/kube-system/services/elasticsearch-logging/proxy kibana-logging is running at https://104.197.5.247/api/v1/namespaces/kube-system/services/kibana-logging/proxy kube-dns is running at https://104.197.5.247/api/v1/namespaces/kube-system/services/kube-dns/proxy grafana is running at https://104.197.5.247/api/v1/namespaces/kube-system/services/monitoring-grafana/proxy heapster is running at https://104.197.5.247/api/v1/namespaces/kube-system/services/monitoring-heapster/proxy 这显示了访问 …","date":1653062400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"25cd81125814542b089d1c6dadb99c36","permalink":"https://lib.jimmysong.io/kubernetes-handbook/access/methods/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/access/methods/","section":"kubernetes-handbook","summary":"本文列举了集中访问 Kubernetes 集群的方式。 第一次使用 kubectl 访问 如果您是第一次访问 Kubernetes API 的话，我们建议您使用 Kubernetes 命令行工具：kubectl。 为了访问集群，您需要知道集群的地址，并且需要有访问它的凭证。通常，如果您完成了入","tags":["Kubernetes"],"title":"访问集群","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"Kubernetes 的认证方式对于不同的人来说可能有所不同。\n 运行 kubelet 可能有一种认证方式（即证书）。 用户可能有不同的认证方式（即令牌）。 管理员可能具有他们为个人用户提供的证书列表。 我们可能有多个集群，并希望在同一个地方将其全部定义——这样用户就能使用自己的证书并重用相同的全局配置。  所以为了能够让用户轻松地在多个集群之间切换，对于多个用户的情况下，我们将其定义在了一个 kubeconfig 文件中。\n此文件包含一系列与昵称相关联的身份验证机制和集群连接信息。它还引入了一个（用户）认证信息元组和一个被称为上下文的与昵称相关联的集群连接信息的概念。\n如果明确指定，则允许使用多个 kubeconfig 文件。在运行时，它们与命令行中指定的覆盖选项一起加载并合并（参见下面的 规则）。\n相关讨论 http://issue.k8s.io/1755\nKubeconfig 文件的组成 Kubeconifg 文件示例 current-context:federal-contextapiVersion:v1clusters:- cluster:api-version:v1server:http://cow.org:8080name:cow-cluster- cluster:certificate-authority:path/to/my/cafileserver:https://horse.org:4443name:horse-cluster- cluster:insecure-skip-tls-verify:trueserver:https://pig.org:443name:pig-clustercontexts:- context:cluster:horse-clusternamespace:chisel-nsuser:green-username:federal-context- context:cluster:pig-clusternamespace:saw-nsuser:black-username:queen-anne-contextkind:Configpreferences:colors:trueusers:- name:blue-useruser:token:blue-token- name:green-useruser:client-certificate:path/to/my/client/certclient-key:path/to/my/client/key各个组件的拆解/释意 Cluster clusters:- cluster:certificate-authority:path/to/my/cafileserver:https://horse.org:4443name:horse-cluster- cluster:insecure-skip-tls-verify:trueserver:https://pig.org:443name:pig-clustercluster 中包含 kubernetes 集群的端点数据，包括 kubernetes apiserver 的完整 url 以及集群的证书颁发机构或者当集群的服务证书未被系统信任的证书颁发机构签名时，设置insecure-skip-tls-verify: true。\ncluster 的名称（昵称）作为该 kubeconfig 文件中的集群字典的 key。 您可以使用 kubectl config set-cluster添加或修改 cluster 条目。\nuser users:- name:blue-useruser:token:blue-token- name:green-useruser:client-certificate:path/to/my/client/certclient-key:path/to/my/client/keyuser 定义用于向 kubernetes 集群进行身份验证的客户端凭据。在加载/合并 kubeconfig 之后，user 将有一个名称（昵称）作为用户条目列表中的 key。 可用凭证有 client-certificate、client-key、token 和 username/password。 username/password 和 token 是二者只能选择一个，但 client-certificate 和 client-key 可以分别与它们组合。\n您可以使用 kubectl config set-credentials 添加或者修改 user 条目。\ncontext contexts:- context:cluster:horse-clusternamespace:chisel-nsuser:green-username:federal-contextcontext 定义了一个命名的 cluster、user、namespace 元组，用于使用提供的认证信息和命名空间将请求发送到指定的集群。 三个都是可选的；仅使用 cluster、user、namespace 之一指定上下文，或指定 none。 未指定的值或在加载的 kubeconfig 中没有相应条目的命名值（例如，如果为上述 kubeconfig 文件指定了 pink-user 的上下文）将被替换为默认值。 有关覆盖/合并行为，请参阅下面的 加载和合并规则。\n您可以使用 kubectl config set-context 添加或修改上下文条目。\ncurrent-context current-context:federal-contextcurrent-context 是昵称或者说是作为 cluster、user、namespace 元组的 ”key“，当 kubectl 从该文件中加载配置的时候会被默认使用。您可以在 kubectl 命令行里覆盖这些值，通过分别传入 —context=CONTEXT、 —cluster=CLUSTER、--user=USER 和 --namespace=NAMESPACE 。\n您可以使用 kubectl config use-context 更改 current-context。\napiVersion:v1kind:Configpreferences:colors:true杂项 apiVersion 和 kind 标识客户端解析器的版本和模式，不应手动编辑。 preferences 指定可选（和当前未使用）的 kubectl 首选项。\n查看 kubeconfig 文件 kubectl config view 命令可以展示当前的 kubeconfig 设置。默认将为您展示所有的 kubeconfig 设置；您可以通过传入 —minify 参数，将视图过滤到与 current-context 有关的配额设置。有关其他选项，请参阅 kubectl config view。\n构建您自己的 kubeconfig 文件 您可以使用上文 示例 kubeconfig 文件 作为\n注意： 如果您是通过 kube-up.sh 脚本部署的 kubernetes 集群，不需要自己创建 kubeconfig 文件——该脚本已经为您创建过了。\n当 api server 启动的时候使用了 —token-auth-file=tokens.csv 选项时，上述文件将会与 API server 相关联，tokens.csv 文件看起来会像这个样子：\nblue-user,blue-user,1 mister-red,mister-red,2 注意： 启动 API server 时有很多 可用选项。请您一定要确保理解您使用的选项。\n上述示例 kubeconfig 文件提供了 green-user 的客户端凭证。因为用户的 current-user 是 green-user ，任何该 API server 的客户端使用该示例 kubeconfig 文件时都可以成功登录。同样，我们可以通过修改 current-context 的值以 blue-user 的身份操作。\n在上面的示例中，green-user 通过提供凭据登录，blue-user 使用的是 token。使用 kubectl config set-credentials 指定登录信息。想了解更多信息，请访问 “示例文件相关操作命令\u0026#34;。\n加载和合并规则 加载和合并 kubeconfig 文件的规则很简单，但有很多。最终的配置按照以下顺序构建：\n  从磁盘中获取 kubeconfig。这将通过以下层次结构和合并规则完成：\n如果设置了 CommandLineLocation （kubeconfig 命令行参数的值），将会只使用该文件，而不会进行合并。该参数在一条命令中只允许指定一次。\n或者，如果设置了 EnvVarLocation （$KUBECONFIG 的值），其将会被作为应合并的文件列表，并根据以下规则合并文件。空文件名被忽略。非串行内容的文件将产生错误。设置特定值或 map key 的第一个文件将优先使用，并且值或 map key 也永远不会更改。 这意味着设置 CurrentContext 的第一个文件将保留其上下文。 这也意味着如果两个文件同时指定一个 red-user，那么将只使用第一个文件中的 red-user 的值。 即使第二个文件的 red-user 中有非冲突条目也被丢弃。\n另外，使用 Home 目录位置（~/.kube/config）将不会合并。\n  根据此链中的第一个命中确定要使用的上下文\n 命令行参数——context 命令行选项的值 来自合并后的 kubeconfig 文件的 current-context 在这个阶段允许空    确定要使用的群集信息和用户。此时，我们可能有也可能没有上下文。他们是基于这个链中的第一次命中。 （运行两次，一次为用户，一次为集群）\n 命令行参数——user 指定用户，cluster 指定集群名称 如果上下文存在，则使用上下文的值 允许空    确定要使用的实际群集信息。此时，我们可能有也可能没有集群信息。根据链条构建每个集群信息（第一次命中胜出）：\n 命令行参数——server，api-version，certificate-authority 和 insecure-skip-tls-verify 如果存在集群信息，并且存在该属性的值，请使用它。 如果没有服务器位置，则产生错误。    确定要使用的实际用户信息。用户使用与集群信息相同的规则构建，除非，您的每个用户只能使用一种认证技术。\n 负载优先级为1）命令行标志 2）来自 kubeconfig 的用户字段 命令行标志是：client-certificate、client-key、username、password 和 token 如果有两种冲突的技术，则失败。    对于任何仍然缺少的信息，将使用默认值，并可能会提示验证信息\n  Kubeconfig 文件中的所有文件引用都相对于 kubeconfig 文件本身的位置进行解析。当命令行上显示文件引用时，它们将相对于当前工作目录进行解析。当路径保存在 ~/.kube/config 中时，相对路径使用相对存储，绝对路径使用绝对存储。\n  Kubeconfig 文件中的任何路径都相对于 kubeconfig 文件本身的位置进行解析。\n使用 kubectl config \u0026lt;subcommand\u0026gt; 操作 kubeconfig kubectl config 有一些列的子命令可以帮助我们更方便的操作 kubeconfig 文件。\n请参阅 kubectl/kubectl_config。\nExample $ kubectl config set-credentials myself --username=admin --password=secret $ kubectl config set-cluster …","date":1653062400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"4ef086b37874885cb00f7efc09b7f05b","permalink":"https://lib.jimmysong.io/kubernetes-handbook/access/authenticate-across-clusters-kubeconfig/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/access/authenticate-across-clusters-kubeconfig/","section":"kubernetes-handbook","summary":"Kubernetes 的认证方式对于不同的人来说可能有所不同。 运行 kubelet 可能有一种认证方式（即证书）。 用户可能有不同的认证方式（即令牌）。 管理员可能具有他们为个人用户提供的证书列表。 我们可能有多个集群，并希望在同一个地方将其","tags":["Kubernetes"],"title":"使用 kubeconfig 文件配置跨集群认证","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"本页向您展示如何使用 kubectl port-forward 命令连接到运行在 Kubernetes 集群中的 Redis 服务器。这种类型的连接对于数据库调试很有帮助。\n创建一个 Pod 来运行 Redis 服务器   创建一个 Pod：\nkubectl create -f https://k8s.io/docs/tasks/access-application-cluster/redis-master.yaml 命令运行成功后将有以下输出验证该 Pod 是否已经创建：\npod \u0026#34;redis-master\u0026#34; created   检查 Pod 是否正在运行且处于就绪状态：\nkubectl get pods 当 Pod 就绪，输出显示 Running 的状态：\nNAME READY STATUS RESTARTS AGE redis-master 2/2 Running 0 41s   验证 Redis 服务器是否已在 Pod 中运行，并监听 6379 端口：\n{% raw %} kubectl get pods redis-master --template=\u0026#39;{{(index (index .spec.containers 0).ports 0).containerPort}}{{\u0026#34;\\n\u0026#34;}}\u0026#39; {% endraw %} 端口输出如下：\n6379   将本地端口转发到 Pod 中的端口   将本地工作站上的 6379 端口转发到 redis-master pod 的 6379 端口：\nkubectl port-forward redis-master 6379:6379 输出类似于：\n I0710 14:43:38.274550 3655 portforward.go:225] Forwarding from 127.0.0.1:6379 -\u0026gt; 6379 I0710 14:43:38.274797 3655 portforward.go:225] Forwarding from [::1]:6379 -\u0026gt; 6379   启动 Redis 命令行界面\nredis-cli   在 Redis 命令行提示符下，输入 ping 命令：\n127.0.0.1:6379\u0026gt;ping Ping 请求成功返回 PONG。\n  讨论 创建连接，将本地的 6379 端口转发到运行在 Pod 中的 Redis 服务器的 6379 端口。有了这个连接您就可以在本地工作站中调试运行在 Pod 中的数据库。\n","date":1653062400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"aea47edac9c47e4c2c9fbd9f0aa7b657","permalink":"https://lib.jimmysong.io/kubernetes-handbook/access/connecting-to-applications-port-forward/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/access/connecting-to-applications-port-forward/","section":"kubernetes-handbook","summary":"本页向您展示如何使用 kubectl port-forward 命令连接到运行在 Kubernetes 集群中的 Redis 服务器。这种类型的连接对于数据库调试很有帮助。 创建一个 Pod 来运行 Redis 服务器 创建一个 Pod： kubectl create -f https://k8s.io/docs/tasks/access-application-cluster/redis-master.yaml 命令运行成功后将有以下输出验证该 Pod 是否已经创建： pod \"redis-master\" created","tags":["Kubernetes"],"title":"通过端口转发访问集群中的应用程序","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"本文向您展示如何创建 Kubernetes Service 对象，外部客户端可以使用它来访问集群中运行的应用程序。该 Service 可以为具有两个运行实例的应用程序提供负载均衡。\n目的  运行 Hello World 应用程序的两个实例。 创建一个暴露 node 节点端口的 Service 对象。 使用 Service 对象访问正在运行的应用程序。  为在两个 pod 中运行的应用程序创建 service   在集群中运行 Hello World 应用程序：\nkubectl run hello-world --replicas=2 --labels=\u0026#34;run=load-balancer-example\u0026#34; --image=gcr.io/google-samples/node-hello:1.0 --port=8080 上述命令创建一个 Deployment 对象和一个相关联的 ReplicaSet 对象。该 ReplicaSet 有两个 Pod，每个 Pod 中都运行一个 Hello World 应用程序。\n  显示关于该 Deployment 的信息：\nkubectl get deployments hello-world kubectl describe deployments hello-world   显示 ReplicaSet 的信息：\nkubectl get replicasets kubectl describe replicasets   创建一个暴露该 Deployment 的 Service 对象：\nkubectl expose deployment hello-world --type=NodePort --name=example-service   显示该 Service 的信息：\nkubectl describe services example-service 输出类似于：\n Name: example-service Namespace: default Labels: run=load-balancer-example Selector: run=load-balancer-example Type: NodePort IP: 10.32.0.16 Port: \u0026lt;unset\u0026gt; 8080/TCP NodePort: \u0026lt;unset\u0026gt; 31496/TCP Endpoints: 10.200.1.4:8080,10.200.2.5:8080 Session Affinity: None No events. 记下服务的 NodePort 值。例如，在前面的输出中，NodePort 值为 31496。\n  列出运行 Hello World 应用程序的 Pod：\nkubectl get pods --selector=\u0026#34;run=load-balancer-example\u0026#34; --output=wide 输出类似于：\n NAME READY STATUS ... IP NODE hello-world-2895499144-bsbk5 1/1 Running ... 10.200.1.4 worker1 hello-world-2895499144-m1pwt 1/1 Running ... 10.200.2.5 worker2   获取正在运行 Hello World 应用程序的 Pod 的其中一个节点的 public IP 地址。如何得到这个地址取决于您的集群设置。例如，如果您使用 Minikube，可以通过运行 kubectl cluster-info 查看节点地址。如果您是使用 Google Compute Engine 实例，可以使用 gcloud compute instances list 命令查看您的公共地址节点。\n  在您选择的节点上，在您的节点端口上例如创建允许 TCP 流量的防火墙规则，如果您的服务 NodePort 值为 31568，创建防火墙规则，允许端口 31568 上的TCP流量。\n  使用节点地址和节点端口访问 Hello World 应用程序：\ncurl http://\u0026lt;public-node-ip\u0026gt;:\u0026lt;node-port\u0026gt; 其中 \u0026lt;public-node-ip\u0026gt; 是您节点的 public IP地址，而 \u0026lt;node-port\u0026gt; 是您服务的 NodePort 值。\n对成功请求的响应是一个 hello 消息：\nHello Kubernetes!   使用 Service 配置文件 作为使用 kubectl expose 的替代方法，您可以使用 service 配置文件 来创建 Service。\n要删除 Service，输入以下命令：\nkubectl delete services example-service 删除 Deployment、ReplicaSet 和正运行在 Pod 中的 Hello World 应用程序，输入以下命令：\nkubectl delete deployment hello-world 了解更多关于 使用 service 连接应用程序。\n","date":1653062400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"38dbc0a49768fc09d6be3ade98e84fa3","permalink":"https://lib.jimmysong.io/kubernetes-handbook/access/service-access-application-cluster/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/access/service-access-application-cluster/","section":"kubernetes-handbook","summary":"本文向您展示如何创建 Kubernetes Service 对象，外部客户端可以使用它来访问集群中运行的应用程序。该 Service 可以为具有两个运行实例的应用程序提供负载均衡。 目的 运行 Hello World 应用程序的两个实例。 创建一个暴露 node 节点端口的 Service 对象。 使用 Service 对","tags":["Kubernetes"],"title":"使用 service 访问群集中的应用程序","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"前面几节讲到如何访问 Kubernetes 集群，本文主要讲解访问 Kubernetes 中的 Pod 和 Serivce 的几种方式，包括如下几种：\n hostNetwork hostPort NodePort LoadBalancer Ingress  说是暴露 Pod 其实跟暴露 Service 是一回事，因为 Pod 就是 Service 的后端。\nhostNetwork: true 这是一种直接定义 Pod 网络的方式。\n如果在 Pod 中使用 hostNotwork:true 配置的话，在这种 pod 中运行的应用程序可以直接看到 pod 启动的主机的网络接口。在主机的所有网络接口上都可以访问到该应用程序。以下是使用主机网络的 pod 的示例定义：\napiVersion:v1kind:Podmetadata:name:influxdbspec:hostNetwork:truecontainers:- name:influxdbimage:influxdb部署该 Pod：\n$ kubectl create -f influxdb-hostnetwork.yml 访问该 pod 所在主机的 8086 端口：\ncurl -v http://$POD_IP:8086/ping 将看到 204 No Content 的 204 返回码，说明可以正常访问。\n注意每次启动这个 Pod 的时候都可能被调度到不同的节点上，所有外部访问 Pod 的 IP 也是变化的，而且调度 Pod 的时候还需要考虑是否与宿主机上的端口冲突，因此一般情况下除非您知道需要某个特定应用占用特定宿主机上的特定端口时才使用 hostNetwork: true 的方式。\n这种 Pod 的网络模式有一个用处就是可以将网络插件包装在 Pod 中然后部署在每个宿主机上，这样该 Pod 就可以控制该宿主机上的所有网络。\nhostPort 这是一种直接定义 Pod 网络的方式。\nhostPort 是直接将容器的端口与所调度的节点上的端口路由，这样用户就可以通过宿主机的 IP 加上来访问 Pod 了，比如：\napiVersion:v1kind:Podmetadata:name:influxdbspec:containers:- name:influxdbimage:influxdbports:- containerPort:8086hostPort:8086这样做有个缺点，因为 Pod 重新调度的时候该 Pod 被调度到的宿主机可能会变动，这样就变化了，用户必须自己维护一个 Pod 与所在宿主机的对应关系。\n这种网络方式可以用来做 Nginx Ingress Controller。外部流量都需要通过 Kubernetes node 节点的 80 和 443 端口。\nNodePort NodePort 在 Kubernetes 里是一个广泛应用的服务暴露方式。Kubernetes 中的 service 默认情况下都是使用的 ClusterIP 这种类型，这样的 service 会产生一个 ClusterIP，这个 IP 只能在集群内部访问，要想让外部能够直接访问 service，需要将 service type 修改为 nodePort。\napiVersion:v1kind:Podmetadata:name:influxdblabels:name:influxdbspec:containers:- name:influxdbimage:influxdbports:- containerPort:8086同时还可以给 service 指定一个 nodePort 值，范围是 30000-32767，这个值在 API server 的配置文件中，用 --service-node-port-range 定义。\nkind:ServiceapiVersion:v1metadata:name:influxdbspec:type:NodePortports:- port:8086nodePort:30000selector:name:influxdb集群外就可以使用 Kubernetes 任意一个节点的 IP 加上 30000 端口访问该服务了。kube-proxy 会自动将流量以 round-robin 的方式转发给该 service 的每一个 pod。\n这种服务暴露方式，无法让你指定自己想要的应用常用端口，不过可以在集群上再部署一个反向代理作为流量入口。\nLoadBalancer LoadBalancer 只能在 service 上定义。这是公有云提供的负载均衡器，如 AWS、Azure、CloudStack、GCE 等。\nkind:ServiceapiVersion:v1metadata:name:influxdbspec:type:LoadBalancerports:- port:8086selector:name:influxdb查看服务：\n$ kubectl get svc influxdb NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE influxdb 10.97.121.42 10.13.242.236 8086:30051/TCP 39s 内部可以使用 ClusterIP 加端口来访问服务，如 10.97.121.42:8086。\n外部可以用以下两种方式访问该服务：\n 使用任一节点的 IP 加 30051 端口访问该服务 使用 EXTERNAL-IP 来访问，这是一个 VIP，是云供应商提供的负载均衡器 IP，如 10.13.242.236:8086  Ingress Ingress 是自 kubernetes1.1 版本后引入的资源类型。必须要部署 Ingress controller 才能创建 Ingress 资源，Ingress controller 是以一种插件的形式提供。Ingress controller 是部署在 Kubernetes 之上的 Docker 容器。它的 Docker 镜像包含一个像 nginx 或 HAProxy 的负载均衡器和一个控制器守护进程。控制器守护程序从 Kubernetes 接收所需的 Ingress 配置。它会生成一个 nginx 或 HAProxy 配置文件，并重新启动负载平衡器进程以使更改生效。换句话说，Ingress controller 是由 Kubernetes 管理的负载均衡器。\nKubernetes Ingress 提供了负载平衡器的典型特性：HTTP 路由，粘性会话，SSL 终止，SSL 直通，TCP 和 UDP 负载平衡等。目前并不是所有的 Ingress controller 都实现了这些功能，需要查看具体的 Ingress controller 文档。\napiVersion:extensions/v1beta1kind:Ingressmetadata:name:influxdbspec:rules:- host:influxdb.kube.example.comhttp:paths:- backend:serviceName:influxdbservicePort:8086外部访问 URL http://influxdb.kube.example.com/ping 访问该服务，入口就是 80 端口，然后 Ingress controller 直接将流量转发给后端 Pod，不需再经过 kube-proxy 的转发，比 LoadBalancer 方式更高效。\n总结 总的来说 Ingress 是一个非常灵活和越来越得到厂商支持的服务暴露方式，包括 Nginx、HAProxy、Traefik，还有各种 服务网格，而其它服务暴露方式可以更适用于服务调试、特殊应用的部署。\n参考  Accessing Kubernetes Pods from Outside of the Cluster - alesnosek.com  ","date":1653062400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"cd3730a227c6a29a89d94d1f4d570990","permalink":"https://lib.jimmysong.io/kubernetes-handbook/access/accessing-kubernetes-pods-from-outside-of-the-cluster/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/access/accessing-kubernetes-pods-from-outside-of-the-cluster/","section":"kubernetes-handbook","summary":"前面几节讲到如何访问 Kubernetes 集群，本文主要讲解访问 Kubernetes 中的 Pod 和 Serivce 的几种方式，包括如下几种： hostNetwork hostPort NodePort LoadBalancer Ingress 说是暴露 Pod 其实跟暴露 Service 是一回事，因为 Pod 就是 Service 的后端。 hostNetwork: true 这是一种直接定义 Pod 网络的方式。 如果在 Pod 中使用 hostNotwork:true 配置的","tags":["Kubernetes"],"title":"从外部访问 Kubernetes 中的 Pod","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"Lens 是一款开源的 Kubenretes IDE，也可以作为桌面客户端，官方网站 https://k8slens.dev，具有以下特性：\n 完全开源，GitHub 地址 https://github.com/lensapp/lens 实时展示集群状态 内置 Prometheus 监控 多集群，多个 namespace 管理 原生 Kubernetes 支持 支持使用 chart 安装应用 使用 kubeconfig 登陆认证 支持多平台，Windows、Mac、Linux Visual Studio Code 友好的风格设计  Lens 界面图下图所示。\n   Lens Kubernetes IDE 界面  参考  Lens, Kubernetes IDE - k8slens.dev  ","date":1653062400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"20ca6f6088debf306dc78fb2729febce","permalink":"https://lib.jimmysong.io/kubernetes-handbook/access/lens/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/access/lens/","section":"kubernetes-handbook","summary":"Lens 是一款开源的 Kubenretes IDE，也可以作为桌面客户端，官方网站 https://k8slens.dev，具有以下特性： 完全开源，GitHub 地址 https://github.com/lensapp/lens 实时展示集群状态 内置 Prometheus 监控 多集群，多个 namespace 管理 原生 Kubernetes 支持 支持使用 chart","tags":["Kubernetes"],"title":"Lens - Kubernetes IDE","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"Kubernator 相较于 Kubernetes Dashboard 来说，是一个更底层的 Kubernetes UI，Dashboard 操作的都是 Kubernetes 的底层对象，而 Kubernator 是直接操作 Kubernetes 各个对象的 YAML 文件。\nKubernator 提供了一种基于目录树和关系拓扑图的方式来管理 Kubernetes 的对象的方法，用户可以在 Web 上像通过 GitHub 的网页版一样操作 Kubernetes 的对象，执行修改、拷贝等操作，详细的使用方式见 https://github.com/smpio/kubernator。\n安装 Kubernator Kubernator 的安装十分简单，可以直接使用 kubectl 命令来运行，它不依赖任何其它组件。\nkubectl create ns kubernator kubectl -n kubernator run --image=smpio/kubernator --port=80 kubernator kubectl -n kubernator expose deploy kubernator kubectl proxy 然后就可以通过 http://localhost:8001/api/v1/namespaces/kubernator/services/kubernator/proxy/ 来访问了。\nCatalog 页面可以看到 Kubernetes 中资源对象的树形结构，还可以在该页面中对资源对象的配置进行更改和操作。\n   Kubernator catalog 页面  RBAC 页面可以看到集群中 RBAC 关系及结构。\n   Kubernator rbac 页面  参考  Kubernator - github.com  ","date":1653062400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"58a8aa537b9a6e8421d735a2c954a5b5","permalink":"https://lib.jimmysong.io/kubernetes-handbook/access/kubernator-kubernetes-ui/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/access/kubernator-kubernetes-ui/","section":"kubernetes-handbook","summary":"Kubernator 相较于 Kubernetes Dashboard 来说，是一个更底层的 Kubernetes UI，Dashboard 操作的都是 Kubernetes 的底层对象，而 Kubernator 是直接操作 Kubernetes 各个对象的 YAML 文件。 Kubernator 提供了一种基于目录树和关系拓扑图的方式来管理 Kubernetes 的对象的方法，用户可以在 Web 上像通过 GitHub","tags":["Kubernetes"],"title":"Kubernator - 更底层的 Kubernetes UI","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"本文讲解了如何开发容器化应用，并使用 Wercker 持续集成工具构建 docker 镜像上传到 docker 镜像仓库中，然后在本地使用 docker-compose 测试后，再使用 kompose 自动生成 kubernetes 的 yaml 文件，再将注入 Envoy sidecar 容器，集成 Istio 服务网格中的详细过程。\n整个过程如下图所示。\n   流程图  为了讲解详细流程，我特意写了用 Go 语言开发的示例程序放在 GitHub 中，模拟监控流程：\n k8s-app-monitor-test：生成模拟的监控数据，在接收到 http 请求返回 json 格式的 metrics 信息 K8s-app-monitor-agent：获取监控 metrics 信息并绘图，访问该服务将获得监控图表  API 文档见 k8s-app-monitor-test 中的 api.html 文件，该文档在 API blueprint 中定义，使用 aglio 生成，打开后如图所示：\n   API  关于服务发现 K8s-app-monitor-agent 服务需要访问 k8s-app-monitor-test 服务，这就涉及到服务发现的问题，我们在代码中直接写死了要访问的服务的内网 DNS 地址（kubedns 中的地址，即 k8s-app-monitor-test.default.svc.cluster.local）。\n我们知道 Kubernetes 在启动 Pod 的时候为容器注入环境变量，这些环境变量在所有的 namespace 中共享（环境变量是不断追加的，新启动的 Pod 中将拥有老的 Pod 中所有的环境变量，而老的 Pod 中的环境变量不变）。但是既然使用这些环境变量就已经可以访问到对应的 service，那么获取应用的地址信息，究竟是使用变量呢？还是直接使用 DNS 解析来发现？\n答案是使用 DNS，详细说明见 Kubernetes中的服务发现与Docker容器间的环境变量传递源码探究。\n持续集成 因为我使用 wercker 自动构建，构建完成后自动打包成 docker 镜像并上传到 docker hub 中（需要现在 docker hub 中创建 repo）。\n构建流程见：https://app.wercker.com/jimmysong/k8s-app-monitor-agent/\n   wercker构建页面  生成了如下两个 docker 镜像：\n jimmysong/k8s-app-monitor-test:9c935dd jimmysong/k8s-app-monitor-agent:234d51c  测试 在将服务发布到线上之前，我们可以先使用 docker-compose 在本地测试一下，这两个应用的 docker-compose.yaml 文件如下：\nversion:\u0026#39;2\u0026#39;services:k8s-app-monitor-agent:image:jimmysong/k8s-app-monitor-agent:234d51ccontainer_name:monitor-agentdepends_on:- k8s-app-monitor-testports:- 8888:8888environment:- SERVICE_NAME=k8s-app-monitor-testk8s-app-monitor-test:image:jimmysong/k8s-app-monitor-test:9c935ddcontainer_name:monitor-testports:- 3000:3000执行下面的命令运行测试。\ndocker-compose up 在浏览器中访问 http://localhost:8888/k8s-app-monitor-test 就可以看到监控页面。\n发布 所有的 kubernetes 应用启动所用的 yaml 配置文件都保存在那两个 GitHub 仓库的 manifest.yaml 文件中。也可以使用 kompose 这个工具，可以将 docker-compose 的 YAML 文件转换成 kubernetes 规格的 YAML 文件。\n分别在两个 GitHub 目录下执行 kubectl create -f manifest.yaml 即可启动服务。也可以直接在 k8s-app-monitor-agent 代码库的 k8s 目录下执行 kubectl apply -f kompose。\n在以上 YAML 文件中有包含了 Ingress 配置，是为了将 k8s-app-monitor-agent 服务暴露给集群外部访问。\n方式一\n服务启动后需要更新 ingress 配置，在 ingress.yaml 文件中增加以下几行：\n- host:k8s-app-monitor-agent.jimmysong.iohttp:paths:- path:/k8s-app-monitor-agentbackend:serviceName:k8s-app-monitor-agentservicePort:8888保存后，然后执行 kubectl replace -f ingress.yaml 即可刷新 ingress。\n修改本机的 /etc/hosts 文件，在其中加入以下一行：\n172.20.0.119 k8s-app-monitor-agent.jimmysong.io 当然你也可以将该域名加入到内网的 DNS 中，为了简单起见我使用 hosts。\n方式二\n或者不修改已有的 Ingress，而是为该队外暴露的服务单独创建一个 Ingress，如下：\napiVersion:extensions/v1beta1kind:Ingressmetadata:name:k8s-app-monitor-agent-ingressannotations:kubernetes.io/ingress.class:\u0026#34;treafik\u0026#34;spec:rules:- host:k8s-app-monitor-agent.jimmysong.iohttp:paths:- path:/backend:serviceName:k8s-app-monitor-agentservicePort:8888集成 Istio 服务网格 上一步中我们生成了 Kubernetes 可读取的应用的 YAML 配置文件，我们可以将所有的 YAML 配置和并到同一个 YAML 文件中假如文件名为 k8s-app-monitor-istio-all-in-one.yaml，如果要将其集成到 Istio 服务网格，只需要执行下面的命令。\nkubectl apply -n default -f \u0026lt;(istioctl kube-inject -f k8s-app-monitor-istio-all-in-one.yaml) 这样就会在每个 Pod 中注入一个 sidecar 容器。\n验证 如果您使用的是 Traefik ingress 来暴露的服务，那么在浏览器中访问 http://k8s-app-monitor-agent.jimmysong.io/k8s-app-monitor-agent，可以看到如下的画面，每次刷新页面将看到新的柱状图。\n   图表  使用 kubernetes-vagrant-centos-cluster 来部署的 kubernetes 集群，该应用集成了 Istio 服务网格后可以通过 http://172.17.8.101:32000/k8s-app-monitor-agent 来访问。\n在对 k8s-app-monitor-agent 服务进行了 N 此访问之后，再访问 http://grafana.istio.jimmysong.io 可以看到 服务网格 的监控信息。\n   Grafana页面  访问 http://servicegraph.istio.jimmysong.io/dotviz 可以看到服务的依赖和 QPS 信息。\n   servicegraph 页面  访问 http://zipkin.istio.jimmysong.io 可以选择查看 k8s-app-monitor-agent 应用的追踪信息。\n   Zipkin页面  至此从代码提交到上线到 Kubernetes 集群上并集成 Istio 服务网格的过程就全部完成了。\n","date":1653062400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"20c9fa8626118a1a31fa82f911aecab2","permalink":"https://lib.jimmysong.io/kubernetes-handbook/devops/deploy-applications-in-kubernetes/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/devops/deploy-applications-in-kubernetes/","section":"kubernetes-handbook","summary":"本文讲解了如何开发容器化应用，并使用 Wercker 持续集成工具构建 docker 镜像上传到 docker 镜像仓库中，然后在本地使用 docker-compose 测试后，再使用 kompose 自动生成 kubernetes 的 yaml 文件，再将注入 Envoy sidecar 容器，集成 Istio 服务网格中的详细过程。 整个过程如下图所示。 流","tags":["Kubernetes"],"title":"适用于 Kubernetes 的应用开发部署流程","type":"book"},{"authors":null,"categories":["Istio"],"content":"默认情况下，注入的 sidecar 代理的配置方式是，它们接受所有端口的流量，并且在转发流量时可以到达网格中的任何服务。\n在某些情况下，你可能想改变这种配置，配置代理，所以它只能使用特定的端口和访问某些服务。要做到这一点，你可以在 Istio 中使用 Sidecar 资源。\nSidecar 资源可以被部署到 Kubernetes 集群内的一个或多个命名空间，但如果没有定义工作负载选择器，每个命名空间只能有一个 sidecar 资源。\nSidecar 资源由三部分组成，一个工作负载选择器、一个入口（ingress）监听器和一个出口（egress）监听器。\n工作负载选择器 工作负载选择器决定了哪些工作负载会受到 sidecar 配置的影响。你可以决定控制一个命名空间中的所有 sidecar，而不考虑工作负载，或者提供一个工作负载选择器，将配置只应用于特定的工作负载。\n例如，这个 YAML 适用于默认命名空间内的所有代理，因为没有定义选择器。\napiVersion:networking.istio.io/v1alpha3kind:Sidecarmetadata:name:default-sidecarnamespace:defaultspec:egress:- hosts:- \u0026#34;default/*\u0026#34;- \u0026#34;istio-system/*\u0026#34;- \u0026#34;staging/*\u0026#34;在 egress 部分，我们指定代理可以访问运行在 default、istio-system 和 staging 命名空间的服务。要将资源仅应用于特定的工作负载，我们可以使用 workloadSelector 字段。例如，将选择器设置为 `version: v1 将只适用于有该标签设置的工作负载。\napiVersion:networking.istio.io/v1alpha3kind:Sidecarmetadata:name:default-sidecarnamespace:defaultspec:workloadSelector:labels:version:v1egress:- hosts:- \u0026#34;default/*\u0026#34;- \u0026#34;istio-system/*\u0026#34;- \u0026#34;staging/*\u0026#34;入口和出口监听器 资源的入口（ingress）监听器部分定义了哪些入站流量被接受。同样地，通过出口（egress）监听器，你可以定义出站流量的属性。\n每个入口监听器都需要一个端口设置，以便接收流量（例如，下面的例子中的 3000）和一个默认的端点。默认端点可以是一个回环 IP 端点或 Unix 域套接字。端点配置了流量将被转发到哪里。\n...ingress:- port:number:3000protocol:HTTPname:somenamedefaultEndpoint:127.0.0.1:8080...上面的片段将入口监听器配置为在端口 3000 上监听，并将流量转发到服务监听的端口 8080 上的回环 IP。此外，我们可以设置 bind 字段，以指定一个 IP 地址或域套接字，我们希望代理监听传入的流量。最后，字段 captureMode 可以用来配置如何以及是否捕获流量。\n出口监听器有类似的字段，但增加了hosts 字段。通过 hosts 字段，你可以用 namespace/dnsName 的格式指定服务主机。例如， myservice.default 或 default/*。在 hosts 字段中指定的服务可以是来自网格注册表的实际服务、外部服务（用 ServiceEntry 定义），或虚拟服务。\negress:- port:number:8080protocol:HTTPhosts:- \u0026#34;staging/*\u0026#34;通过上面的 YAML，sidecar 代理了运行在 staging 命名空间的服务的 8080 端口的流量。\n","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"ee67757be0c111f377df927eead3f0b8","permalink":"https://lib.jimmysong.io/istio-handbook/traffic-management/sidecar/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/istio-handbook/traffic-management/sidecar/","section":"istio-handbook","summary":"默认情况下，注入的 sidecar 代理的配置方式是，它们接受所有端口的流量，并且在转发流量时可以到达网格中的任何服务。 在某些情况下，你可能想改变这种配置，配置代理，所以它只能使用特定的端口和访问某些服务。要做到这一","tags":["Istio","Service Mesh"],"title":"Sidecar","type":"book"},{"authors":null,"categories":["Envoy"],"content":"在这个实验中，我们将学习如何使用 TLS 检查器过滤器来选择一个特定的过滤器链。单个过滤器链将根据 transport_protocol 和 application_protocol将流量分配到不同的上游集群。\n我们将为我们的上游主机使用 mendhak/http-https-echo Docker 镜像。这些容器可以被配置为监听 HTTP/HTTPS 并将响应回传。\n我们将运行镜像的三个实例来代表非 TLS HTTP、TLS HTTP/1.1 和 TLS HTTP/2 协议。\n# non-TLS HTTP docker run -dit -p 8080:8080 -t mendhak/http-https-echo:18 # TLS HTTP1.1 docker run -dit -e HTTPS_PORT=443 -p 443:443 -t mendhak/http-https-echo:18 # TLS HTTP2 docker run -dit -e HTTPS_PORT=8443 -p 8443:8443 -t mendhak/http-https-echo:18 为了确保这三个容器都在运行，我们可以用 curl 发送几个请求，看看是否得到了回应。从输出中，我们还可以检查主机名是否与实际的 Docker 容器 ID 相符。\n# non-TLS HTTP $ curl http://localhost:8080 { \u0026#34;path\u0026#34;: \u0026#34;/\u0026#34;, \u0026#34;headers\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;localhost:8080\u0026#34;, \u0026#34;user-agent\u0026#34;: \u0026#34;curl/7.64.0\u0026#34;, \u0026#34;accept\u0026#34;: \u0026#34;*/*\u0026#34; }, \u0026#34;method\u0026#34;: \u0026#34;GET\u0026#34;, \u0026#34;body\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;fresh\u0026#34;: false, \u0026#34;hostname\u0026#34;: \u0026#34;localhost\u0026#34;, \u0026#34;ip\u0026#34;: \u0026#34;::ffff:172.18.0.1\u0026#34;, \u0026#34;ips\u0026#34;: [], \u0026#34;protocol\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;query\u0026#34;: {}, \u0026#34;subdomains\u0026#34;: [], \u0026#34;xhr\u0026#34;: false, \u0026#34;os\u0026#34;: { \u0026#34;hostname\u0026#34;: \u0026#34;100db0dce742\u0026#34; }, \u0026#34;connection\u0026#34;: {} }  注意在向其他两个容器发送请求时，我们将使用 -k 标志来告诉 curl 跳过对服务器 TLS 证书的验证。此外，我们可以使用 --http1.1 和 --http2 标志来发送 HTTP1.1 或 HTTP2 请求。\n # HTTP1.1 $ curl -k --http1.1 https://localhost:443 { \u0026#34;path\u0026#34;: \u0026#34;/\u0026#34;, \u0026#34;headers\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;localhost\u0026#34;, \u0026#34;user-agent\u0026#34;: \u0026#34;curl/7.64.0\u0026#34;, \u0026#34;accept\u0026#34;: \u0026#34;*/*\u0026#34; }, \u0026#34;method\u0026#34;: \u0026#34;GET\u0026#34;, \u0026#34;body\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;fresh\u0026#34;: false, \u0026#34;hostname\u0026#34;: \u0026#34;localhost\u0026#34;, \u0026#34;ip\u0026#34;: \u0026#34;::ffff:172.18.0.1\u0026#34;, \u0026#34;ips\u0026#34;: [], \u0026#34;protocol\u0026#34;: \u0026#34;https\u0026#34;, \u0026#34;query\u0026#34;: {}, \u0026#34;subdomains\u0026#34;: [], \u0026#34;xhr\u0026#34;: false, \u0026#34;os\u0026#34;: { \u0026#34;hostname\u0026#34;: \u0026#34;51afc40f7506\u0026#34; }, \u0026#34;connection\u0026#34;: { \u0026#34;servername\u0026#34;: \u0026#34;localhost\u0026#34; } } $ curl -k --http2 https://localhost:8443 { \u0026#34;path\u0026#34;: \u0026#34;/\u0026#34;, \u0026#34;headers\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;localhost:8443\u0026#34;, \u0026#34;user-agent\u0026#34;: \u0026#34;curl/7.64.0\u0026#34;, \u0026#34;accept\u0026#34;: \u0026#34;*/*\u0026#34; }, \u0026#34;method\u0026#34;: \u0026#34;GET\u0026#34;, \u0026#34;body\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;fresh\u0026#34;: false, \u0026#34;hostname\u0026#34;: \u0026#34;localhost\u0026#34;, \u0026#34;ip\u0026#34;: \u0026#34;::ffff:172.18.0.1\u0026#34;, \u0026#34;ips\u0026#34;: [], \u0026#34;protocol\u0026#34;: \u0026#34;https\u0026#34;, \u0026#34;query\u0026#34;: {}, \u0026#34;subdomains\u0026#34;: [], \u0026#34;xhr\u0026#34;: false, \u0026#34;os\u0026#34;: { \u0026#34;hostname\u0026#34;: \u0026#34;40e7143e6a55\u0026#34; }, \u0026#34;connection\u0026#34;: { \u0026#34;servername\u0026#34;: \u0026#34;localhost\u0026#34; } } 一旦我们验证了容器的正确运行，我们就可以创建 Envoy 配置。我们将使用 tls_inspector 和 filter_chain_match 字段来检查传输协议是否是 TLS，以及应用协议是否是 HTTP1.1（http/1.1）或 HTTP2（h2）。基于这些信息，我们会有不同的集群，将流量转发到上游主机（Docker 容器）。记住 HTTP 运行在端口 8080，TLS HTTP/1.1 运行在端口 443，TLS HTTP2 运行在端口 8443。\nstatic_resources:listeners:- address:socket_address:address:0.0.0.0port_value:10000listener_filters:- name:\u0026#34;envoy.filters.listener.tls_inspector\u0026#34;typed_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.listener.tls_inspector.v3.TlsInspectorfilter_chains:- filter_chain_match:# Match TLS and HTTP2transport_protocol:tlsapplication_protocols:[h2]filters:- name:envoy.filters.network.tcp_proxytyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.network.tcp_proxy.v3.TcpProxycluster:service-tls-http2stat_prefix:https_passthrough- filter_chain_match:# Match TLS and HTTP1.1transport_protocol:tlsapplication_protocols:[http/1.1]filters:- name:envoy.filters.network.tcp_proxytyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.network.tcp_proxy.v3.TcpProxycluster:service-tls-http1.1stat_prefix:https_passthrough- filter_chain_match:# No matches here, go to HTTP upstreamfilters:- name:envoy.filters.network.tcp_proxytyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.network.tcp_proxy.v3.TcpProxycluster:service-httpstat_prefix:ingress_httpclusters:- name:service-tls-http2type:STRICT_DNSlb_policy:ROUND_ROBINload_assignment:cluster_name:service-tls-http2endpoints:- lb_endpoints:- endpoint:address:socket_address:address:127.0.0.1port_value:8443- name:service-tls-http1.1type:STRICT_DNSlb_policy:ROUND_ROBINload_assignment:cluster_name:service-tls-http1.1endpoints:- lb_endpoints:- endpoint:address:socket_address:address:127.0.0.1port_value:443- name:service-httptype:STRICT_DNSlb_policy:ROUND_ROBINload_assignment:cluster_name:service-httpendpoints:- lb_endpoints:- endpoint:address:socket_address:address:127.0.0.1port_value:8080admin:address:socket_address:address:0.0.0.0port_value:9901将上述 YAML 保存为 tls.yaml，并使用 func-e run -c tls.yaml 运行它。\n为了测试这一点，我们可以像以前一样发出类似的 curl 请求，并检查主机名是否与正在运行的 Docker 容器相符。\n$ curl http://localhost:10000 | jq \u0026#39;.os.hostname\u0026#39; \u0026#34;100db0dce742\u0026#34; $ curl -k --http1.1 https://localhost:10000 | jq \u0026#39;.os.hostname\u0026#39; \u0026#34;51afc40f7506\u0026#34; $ curl -k --http2 https://localhost:10000 | jq \u0026#39;.os.hostname\u0026#39; \u0026#34;40e7143e6a55\u0026#34; 另外，我们可以检查各个容器的日志，看看请求是否被正确发送。\n 下一章   ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"47041317a6b921ba6857beb75e95f601","permalink":"https://lib.jimmysong.io/envoy-handbook/listener/lab11/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/envoy-handbook/listener/lab11/","section":"envoy-handbook","summary":"在这个实验中，我们将学习如何使用 TLS 检查器过滤器来选择一个特定的过滤器链。单个过滤器链将根据 transport_protocol 和 application_protocol将流量分配到不同的上游集群。 我们将为我们的上游主机使用 mendhak/http-https-echo Docker 镜像。这","tags":["Envoy"],"title":"实验 11：匹配传输和应用协议","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"本文档不是说明如何在 kubernetes 中开发和部署应用程序，如果您想要直接开发应用程序在 kubernetes 中运行可以参考 适用于kubernetes的应用开发部署流程。\n本文旨在说明如何将已有的应用程序尤其是传统的分布式应用程序迁移到 kubernetes 中。如果该类应用程序符合云原生应用规范（如12因素法则）的话，那么迁移会比较顺利，否则会遇到一些麻烦甚至是阻碍。具体请参考 迁移至云原生应用架构。\n下图是将单体应用迁移到云原生的步骤。\n   将单体应用迁移到云原生(图片来自DevOpsDay Toronto)  接下来我们将以 Spark on YARN with kubernetes 为例来说明，该例子足够复杂也很有典型性，了解了这个例子可以帮助大家将自己的应用迁移到 kubernetes 集群上去。\n下图即整个架构的示意图，所有的进程管理和容器扩容直接使用 Makefile。\n   spark on yarn with kubernetes  注意： 该例子仅用来说明具体的步骤划分和复杂性，在生产环境应用还有待验证，请谨慎使用。\n术语 对于为曾接触过 kubernetes 或对云平台的技术细节不太了解的人来说，如何将应用迁移到 kubernetes 中可能是个头疼的问题，在行动之前有必要先了解整个过程中需要用到哪些概念和术语，有助于大家在行动中达成共识。\n过程中可能用到的概念和术语初步整理如下：\n   术语  为了讲解整改过程和具体细节，我们所有操作都是通过命令手动完成，不使用自动化工具。当您充分了解到其中的细节后可以通过自动化工具来优化该过程，以使其更加自动和高效，同时减少因为人为操作失误导致的迁移失败。\n迁移应用    分解步骤解析  整个迁移过程分为如下几个步骤：\n  将原有应用拆解为服务\n我们不是一上来就开始做镜像，写配置，而是应该先梳理下要迁移的应用中有哪些可以作为服务运行，哪些是变的，哪些是不变的部分。\n服务划分的原则是最小可变原则，这个同样适用于镜像制作，将服务中不变的部分编译到同一个镜像中。\n对于像 Spark on YARN 这样复杂的应用，可以将其划分为三大类服务：\n ResourceManager NodeManager Spark client    制作镜像\n根据拆解出来的服务，我们需要制作两个镜像：\n Hadoop Spark (From hadoop docker image)  因为我们运行的是 Spark on YARN，因此 Spark 依赖与 Hadoop 镜像，我们在 Spark 的基础上包装了一个 web service 作为服务启动。\n镜像制作过程中不需要在 Dockerfile 中指定 Entrypoint 和 CMD，这些都是在 kubernetes 的 YAML 文件中指定的。\nHadoop YARN 的 Dockerfile 参考如下配置。\nFROMmy-docker-repo/jdk:7u80# Add native libsARG HADOOP_VERSION=2.6.0-cdh5.5.2## Prefer to download from server not use local storageADD hadoop-${HADOOP_VERSION}.tar.gz /usr/localADD ./lib/* /usr/local/hadoop-${HADOOP_VERSION}/lib/native/ADD ./jars/* /usr/local/hadoop-${HADOOP_VERSION}/share/hadoop/yarn/ENV HADOOP_PREFIX=/usr/local/hadoop \\  HADOOP_COMMON_HOME=/usr/local/hadoop \\  HADOOP_HDFS_HOME=/usr/local/hadoop \\  HADOOP_MAPRED_HOME=/usr/local/hadoop \\  HADOOP_YARN_HOME=/usr/local/hadoop \\  HADOOP_CONF_DIR=/usr/local/hadoop/etc/hadoop \\  YARN_CONF_DIR=/usr/local/hadoop/etc/hadoop \\  PATH=${PATH}:/usr/local/hadoop/binRUN \\  cd /usr/local \u0026amp;\u0026amp; ln -s ./hadoop-${HADOOP_VERSION} hadoop \u0026amp;\u0026amp; \\  rm -f ${HADOOP_PREFIX}/logs/*WORKDIR$HADOOP_PREFIX# Hdfs portsEXPOSE50010 50020 50070 50075 50090 8020 9000# Mapred portsEXPOSE19888#Yarn portsEXPOSE8030 8031 8032 8033 8040 8042 8088#Other portsEXPOSE49707 2122  准备应用的配置文件\n因为我们只制作了一个 Hadoop 的镜像，而需要启动两个服务，这就要求在服务启动的时候必须加载不同的配置文件，现在我们只需要准备两个服务中需要同时用的的配置的部分。\nYARN 依赖的配置在 artifacts 目录下，包含以下文件：\nbootstrap.sh capacity-scheduler.xml container-executor.cfg core-site.xml hadoop-env.sh hdfs-site.xml log4j.properties mapred-site.xml nodemanager_exclude.txt slaves start-yarn-nm.sh start-yarn-rm.sh yarn-env.sh yarn-site.xml 其中作为 bootstrap 启动脚本的 bootstrap.sh 也包含在该目录下，该脚本的如何编写请见下文。\n  Kubernetes YAML 文件\n根据业务的特性选择最适合的 kubernetes 的资源对象来运行，因为在 YARN 中 NodeManager 需要使用主机名向 ResourceManger 注册，因此需要沿用 YARN 原有的服务发现方式，使用 headless service 和 StatefulSet 资源。更多资料请参考 StatefulSet。\n所有的 Kubernetes YAML 配置文件存储在 manifest 目录下，包括如下配置：\n yarn-cluster 的 namespace 配置 Spark、ResourceManager、NodeManager 的 headless service 和 StatefulSet 配置 需要暴露到 kubernetes 集群外部的 ingress 配置（ResourceManager 的 Web）  kube-yarn-ingress.yaml spark-statefulset.yaml yarn-cluster-namespace.yaml yarn-nm-statefulset.yaml yarn-rm-statefulset.yaml   Bootstrap 脚本\nBootstrap 脚本的作用是在启动时根据 Pod 的环境变量、主机名或其他可以区分不同 Pod 和将启动角色的变量来修改配置文件和启动服务应用。\n该脚本同时将原来 YARN 的日志使用 stdout 输出，便于使用 kubectl logs 查看日志或其他日志收集工具进行日志收集。\n启动脚本 bootstrap.sh 跟 Hadoop 的配置文件同时保存在 artifacts 目录下。\n该脚本根据 Pod 的主机名，决定如何修改 Hadoop 的配置文件和启动何种服务。bootstrap.sh 文件的部分代码如下：\nif [[ \u0026#34;${HOSTNAME}\u0026#34; =~ \u0026#34;yarn-nm\u0026#34; ]]; then sed -i \u0026#39;/\u0026lt;\\/configuration\u0026gt;/d\u0026#39; $HADOOP_PREFIX/etc/hadoop/yarn-site.xml cat \u0026gt;\u0026gt; $HADOOP_PREFIX/etc/hadoop/yarn-site.xml \u0026lt;\u0026lt;- EOM \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;yarn.nodemanager.resource.memory-mb\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;${MY_MEM_LIMIT:-2048}\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;yarn.nodemanager.resource.cpu-vcores\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;${MY_CPU_LIMIT:-2}\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; EOM echo \u0026#39;\u0026lt;/configuration\u0026gt;\u0026#39; \u0026gt;\u0026gt; $HADOOP_PREFIX/etc/hadoop/yarn-site.xml cp ${CONFIG_DIR}/start-yarn-nm.sh $HADOOP_PREFIX/sbin/ cd $HADOOP_PREFIX/sbin chmod +x start-yarn-nm.sh ./start-yarn-nm.sh fi if [[ $1 == \u0026#34;-d\u0026#34; ]]; then until find ${HADOOP_PREFIX}/logs -mmin -1 | egrep -q \u0026#39;.*\u0026#39;; echo \u0026#34;`date`: Waiting for logs...\u0026#34; ; do sleep 2 ; done tail -F ${HADOOP_PREFIX}/logs/* \u0026amp; while true; do sleep 1000; done fi 从这部分中代码中可以看到，如果 Pod 的主机名中包含 yarn-nm 字段则向 yarn-site.xml 配置文件中增加如下内容：\n\u0026lt;property\u0026gt; \u0026lt;name\u0026gt;yarn.nodemanager.resource.memory-mb\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;${MY_MEM_LIMIT:-2048}\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;yarn.nodemanager.resource.cpu-vcores\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;${MY_CPU_LIMIT:-2}\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; 其中 MY_MEM_LIMIT 和 MY_CPU_LIMIT 是 kubernetes YAML 中定义的环境变量，该环境变量又是引用的 Resource limit。\n所有的配置准备完成后，执行 start-yarn-nm.sh 脚本启动 NodeManager。\n如果 kubernetes YAML 中的 container CMD args 中包含 -d 则在后台运行 NodeManger 并 tail 输出 NodeManager 的日志到标准输出。\n  ConfigMaps\n将 Hadoop 的配置文件和 bootstrap 脚本作为 ConfigMap 资源保存，用作 Pod 启动时挂载的 volume。\nkubectl create configmap hadoop-config \\ \t--from-file=artifacts/hadoop/bootstrap.sh \\ \t--from-file=artifacts/hadoop/start-yarn-rm.sh \\ \t--from-file=artifacts/hadoop/start-yarn-nm.sh \\ \t--from-file=artifacts/hadoop/slaves \\ \t--from-file=artifacts/hadoop/core-site.xml \\ …","date":1653062400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"503ae18ce8bfc17a014967276cd96331","permalink":"https://lib.jimmysong.io/kubernetes-handbook/devops/migrating-hadoop-yarn-to-kubernetes/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/devops/migrating-hadoop-yarn-to-kubernetes/","section":"kubernetes-handbook","summary":"本文档不是说明如何在 kubernetes 中开发和部署应用程序，如果您想要直接开发应用程序在 kubernetes 中运行可以参考 适用于kubernetes的应用开发部署流程。 本文旨在说明如何将已有的应用程序尤其是传统的分布式应用程序迁移到 kubernetes","tags":["Kubernetes"],"title":"迁移传统应用到 Kubernetes 步骤详解——以 Hadoop YARN 为例","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"StatefulSet 这个对象是专门用来部署用状态应用的，可以为 Pod 提供稳定的身份标识，包括 hostname、启动顺序、DNS 名称等。\n下面以在 Kubernetes1.6 版本中部署 zookeeper 和 kafka 为例讲解 StatefulSet 的使用，其中 kafka 依赖于 zookeeper。\nDockerfile 和配置文件见 zookeeper 和 kafka。\n注：所有的镜像基于 CentOS 系统的 JDK 制作，为我的私人镜像，外部无法访问，yaml 中没有配置持久化存储。\n部署 Zookeeper Dockerfile 中从远程获取 zookeeper 的安装文件，然后在定义了三个脚本：\n zkGenConfig.sh：生成 zookeeper 配置文件 zkMetrics.sh：获取 zookeeper 的 metrics zkOk.sh：用来做 ReadinessProb  我们在来看下这三个脚本的执行结果。\nzkMetrics.sh 脚本实际上执行的是下面的命令：\n$ echo mntr | nc localhost $ZK_CLIENT_PORT \u0026gt;\u0026amp; 1 zk_version\t3.4.6-1569965, built on 02/20/2014 09:09 GMT zk_avg_latency\t0 zk_max_latency\t5 zk_min_latency\t0 zk_packets_received\t427879 zk_packets_sent\t427890 zk_num_alive_connections\t3 zk_outstanding_requests\t0 zk_server_state\tleader zk_znode_count\t18 zk_watch_count\t3 zk_ephemerals_count\t4 zk_approximate_data_size\t613 zk_open_file_descriptor_count\t29 zk_max_file_descriptor_count\t1048576 zk_followers\t1 zk_synced_followers\t1 zk_pending_syncs\t0 zkOk.sh 脚本实际上执行的是下面的命令：\n$ echo ruok | nc 127.0.0.1 $ZK_CLIENT_PORT imok zookeeper.yaml\n下面是启动三个 zookeeper 实例的 YAML 配置文件：\n---apiVersion:v1kind:Servicemetadata:name:zk-svclabels:app:zk-svcspec:ports:- port:2888name:server- port:3888name:leader-electionclusterIP:Noneselector:app:zk---apiVersion:v1kind:ConfigMapmetadata:name:zk-cmdata:jvm.heap:\u0026#34;1G\u0026#34;tick:\u0026#34;2000\u0026#34;init:\u0026#34;10\u0026#34;sync:\u0026#34;5\u0026#34;client.cnxns:\u0026#34;60\u0026#34;snap.retain:\u0026#34;3\u0026#34;purge.interval:\u0026#34;0\u0026#34;---apiVersion:policy/v1beta1kind:PodDisruptionBudgetmetadata:name:zk-pdbspec:selector:matchLabels:app:zkminAvailable:2---apiVersion:apps/v1beta1kind:StatefulSetmetadata:name:zkspec:serviceName:zk-svcreplicas:3template:metadata:labels:app:zkspec:affinity:podAntiAffinity:requiredDuringSchedulingIgnoredDuringExecution:- labelSelector:matchExpressions:- key:\u0026#34;app\u0026#34;operator:Invalues:- zktopologyKey:\u0026#34;kubernetes.io/hostname\u0026#34;containers:- name:k8szkimagePullPolicy:Alwaysimage:harbor-001.jimmysong.io/library/zookeeper:3.4.6resources:requests:memory:\u0026#34;2Gi\u0026#34;cpu:\u0026#34;500m\u0026#34;ports:- containerPort:2181name:client- containerPort:2888name:server- containerPort:3888name:leader-electionenv:- name :ZK_REPLICASvalue:\u0026#34;3\u0026#34;- name :ZK_HEAP_SIZEvalueFrom:configMapKeyRef:name:zk-cmkey:jvm.heap- name :ZK_TICK_TIMEvalueFrom:configMapKeyRef:name:zk-cmkey:tick- name :ZK_INIT_LIMITvalueFrom:configMapKeyRef:name:zk-cmkey:init- name :ZK_SYNC_LIMITvalueFrom:configMapKeyRef:name:zk-cmkey:tick- name :ZK_MAX_CLIENT_CNXNSvalueFrom:configMapKeyRef:name:zk-cmkey:client.cnxns- name:ZK_SNAP_RETAIN_COUNTvalueFrom:configMapKeyRef:name:zk-cmkey:snap.retain- name:ZK_PURGE_INTERVALvalueFrom:configMapKeyRef:name:zk-cmkey:purge.interval- name:ZK_CLIENT_PORTvalue:\u0026#34;2181\u0026#34;- name:ZK_SERVER_PORTvalue:\u0026#34;2888\u0026#34;- name:ZK_ELECTION_PORTvalue:\u0026#34;3888\u0026#34;command:- sh- -c- zkGenConfig.sh \u0026amp;\u0026amp; zkServer.sh start-foregroundreadinessProbe:exec:command:- \u0026#34;zkOk.sh\u0026#34;initialDelaySeconds:10timeoutSeconds:5livenessProbe:exec:command:- \u0026#34;zkOk.sh\u0026#34;initialDelaySeconds:10timeoutSeconds:5securityContext:runAsUser:1000fsGroup:1000我们再主要下上面那三个脚本的用途。\n部署 kafka Kafka 的 docker 镜像制作跟 zookeeper 类似，都是从远程下载安装包后，解压安装。\n与 zookeeper 不同的是，只要一个脚本，但是又依赖于我们上一步安装的 zookeeper，kafkaGenConfig.sh 用来生成 kafka 的配置文件。\n我们来看下这个脚本。\n#!/bin/bash HOST=`hostname -s` if [[ $HOST =~ (.*)-([0-9]+)$ ]]; then NAME=${BASH_REMATCH[1]} ORD=${BASH_REMATCH[2]} else echo \u0026#34;Failed to extract ordinal from hostname $HOST\u0026#34; exit 1 fi MY_ID=$((ORD+1)) sed -i s\u0026#34;/broker.id=0/broker.id=$MY_ID/g\u0026#34; /opt/kafka/config/server.properties sed -i s\u0026#39;/zookeeper.connect=localhost:2181/zookeeper.connect=zk-0.zk-svc.brand.svc:2181,zk-1.zk-svc.brand.svc:2181,zk-2.zk-svc.brand.svc:2181/g\u0026#39; /opt/kafka/config/server.properties 该脚本根据 statefulset 生成的 pod 的 hostname 的后半截数字部分作为 broker ID，同时再替换 zookeeper 的地址。\nKafka.yaml\n下面是创建 3 个 kafka 实例的 YAML 配置。\n---apiVersion:v1kind:Servicemetadata:name:kafka-svclabels:app:kafkaspec:ports:- port:9093name:serverclusterIP:Noneselector:app:kafka---apiVersion:policy/v1beta1kind:PodDisruptionBudgetmetadata:name:kafka-pdbspec:selector:matchLabels:app:kafkaminAvailable:2---apiVersion:apps/v1beta1kind:StatefulSetmetadata:name:kafkaspec:serviceName:kafka-svcreplicas:3template:metadata:labels:app:kafkaspec:affinity:podAntiAffinity:requiredDuringSchedulingIgnoredDuringExecution:- labelSelector:matchExpressions:- key:\u0026#34;app\u0026#34;operator:Invalues:- kafkatopologyKey:\u0026#34;kubernetes.io/hostname\u0026#34;podAffinity:preferredDuringSchedulingIgnoredDuringExecution:- weight:1podAffinityTerm:labelSelector:matchExpressions:- key:\u0026#34;app\u0026#34;operator:Invalues:- zktopologyKey:\u0026#34;kubernetes.io/hostname\u0026#34;terminationGracePeriodSeconds:300containers:- name:k8skafkaimagePullPolicy:Alwaysimage:harbor-001.jimmysong.io/library/kafka:2.10-0.8.2.1resources:requests:memory:\u0026#34;1Gi\u0026#34;cpu:500menv:- name:KF_REPLICASvalue:\u0026#34;3\u0026#34;ports:- containerPort:9093name:servercommand:- /bin/bash- -c- \u0026#34;/opt/kafka/bin/kafkaGenConfig.sh \u0026amp;\u0026amp; /opt/kafka/bin/kafka-server-start.sh /opt/kafka/config/server.properties\u0026#34;env:- name:KAFKA_HEAP_OPTSvalue :\u0026#34;-Xmx512M -Xms512M\u0026#34;- name:KAFKA_OPTSvalue:\u0026#34;-Dlogging.level=DEBUG\u0026#34;readinessProbe:tcpSocket:port:9092initialDelaySeconds:15timeoutSeconds:1 …","date":1653062400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"9235b1cc15873e0f7e6c4dfef0e04e25","permalink":"https://lib.jimmysong.io/kubernetes-handbook/devops/using-statefulset/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/devops/using-statefulset/","section":"kubernetes-handbook","summary":"StatefulSet 这个对象是专门用来部署用状态应用的，可以为 Pod 提供稳定的身份标识，包括 hostname、启动顺序、DNS 名称等。 下面以在 Kubernetes1.6 版本中部署 zookeeper 和 kafka 为例讲解 StatefulSet 的使用，其中 kafka 依赖于 zookeeper。 Dockerfile 和配置文件","tags":["Kubernetes"],"title":"使用StatefulSet部署有状态应用","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"Kubernetes 的社区是以 SIG（Special Interest Group 特别兴趣小组）和工作组的形式组织起来的，每个工作组都会定期召开视频会议。\n所有的 SIG 和工作组都使用 slack 和邮件列表沟通。\n   Kubernetes SIG 组织结构  主要 SIG 列表  api-machinery：所有 API 级别的功能，包括了 API server、API 注册和发现、通用的 API CRUD 语义，准入控制，编码 / 解码，转换，默认值，持久化层（etcd），OpenAPI，第三方资源，垃圾回收（gc）和客户端库的方方面面。 aws：如何在 AWS 上支持和使用 kubernetes。 apps：在 kubernetes 上部署和运维应用程序。关注开发者和 DevOps 在 kubernetes 上运行应用程序的体验。 architecture：维持 kubernetes 在架构设计上的一致性和原则。 auth：kubernetes 的认证授权、权限管理和安全性策略。 autoscaling：集群的自动缩放，pod 的水平和垂直自动缩放，pod 的资源初始化，pod 监控和指标收集等主题。 azure：如何在 Azure 上支持和使用 kubernetes。 big-data：在 kubernetes 上部署和运行大数据应用，如 Spark、Kafka、Hadoop、Flink、Storm 等。 CLI：kubectl 和相关工具。 cluster-lifecycle：部署和升级 kubernetes 集群。 cluster-ops：促进 kubernetes 集群本身的可操作性和集群间的互操作性，使不同的运营商之间协调一致。 contributor-experience：维持良好的开发者社区。 docs：文档，流程和出版物。 GCP：在 Google Cloud Platform 上使用 kubernetes。 instrumentation：集群可观测性的最佳实践，包括指标设置、日志收集、事件等。 multicluster：多 kubernetes 集群的用例和工具。 network：kubernetes 集群的网络。 node：node 节点、kubelet。 onprem：在非云供应商的环境下运行 kubernetes，例如 on premise、裸机等环境。 openstack：协调跨 OpenStack 和 Kubernetes 社区的努力。 product-management：侧重于产品管理方面。 release：发布、PR 和 bug 提交等。 scalability：负责回答可伸缩性相关的问题。 scheduling：资源调度。 service-catalog：为 CNCF service broker 和 Kubernetes broker 实现开发 API。 storage：存储和 volume 插件。 testing：测试。 ui：与 UI 相关的话题。 windows：在 kubernets 上运行 Windows Server Container。  工作组列表  App Def：改进 API 中的声明性原语、客户端库、工具的用户体验。 Cloud Provider：云供应商工作组。 Cluster API：定义一个代表 Kubernetes 集群的可移植 API。 API 将包含控制平面及其配置和底层基础设施（节点，节点池等）。 Container Identity：确保容器能够获得安全的身份认证并与外部连通的解决方案。 Kubeadm Adoption：提高 kubeadm 工具的采用率。 Resource ManagementJ：资源隔离和提高资源利用率。  详细信息请参考 https://github.com/kubernetes/community/blob/master/sig-list.md\n","date":1653062400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"81a56c75826a19063b5f1d097d8396a9","permalink":"https://lib.jimmysong.io/kubernetes-handbook/develop/sigs-and-working-group/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/develop/sigs-and-working-group/","section":"kubernetes-handbook","summary":"Kubernetes 的社区是以 SIG（Special Interest Group 特别兴趣小组）和工作组的形式组织起来的，每个工作组都会定期召开视频会议。 所有的 SIG 和工作组都使用 slack 和邮件列表沟通。 Kubernetes SIG 组织结构 主要 SIG 列表 api-machinery：","tags":["Kubernetes"],"title":"SIG 和工作组","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"我们将在 Mac 上使用 docker 环境编译 kuberentes。\n安装依赖 brew install gnu-tar Docker 环境，至少需要给容器分配 4G 内存，在低于 3G 内存的时候可能会编译失败。\n执行编译 切换目录到 kuberentes 源码的根目录下执行：\n./build/run.sh make 可以在 docker 中执行跨平台编译出二进制文件。\n需要用的的 docker 镜像：\ngcr.io/google_containers/kube-cross:v1.7.5-2 该镜像基于 Ubuntu 构建，大小 2.15G，编译环境中包含以下软件：\n Go1.7.5 etcd protobuf g++ 其他 golang 依赖包  在我自己的电脑上的整个编译过程大概要半个小时。\n编译完成的二进制文件在 /_output/local/go/bin/ 目录下。\n","date":1653062400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"f073bc084a3fdfdba29c1f45733985cc","permalink":"https://lib.jimmysong.io/kubernetes-handbook/develop/developing-environment/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/develop/developing-environment/","section":"kubernetes-handbook","summary":"我们将在 Mac 上使用 docker 环境编译 kuberentes。 安装依赖 brew install gnu-tar Docker 环境，至少需要给容器分配 4G 内存，在低于 3G 内存的时候可能会编译失败。 执行编译 切换目录到 kuberentes 源码的根目录下执行： ./build/run.sh make 可以在 docker 中执行跨平台编译出","tags":["Kubernetes"],"title":"配置 Kubernetes 开发环境","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"这篇文章将指导你如何测试 Kubernetes。\n单元测试 单元测试仅依赖于源代码，是测试代码逻辑是否符合预期的最简单方法。\n运行所有的单元测试\nmake test 仅测试指定的 package\n# 单个package make test WHAT=./pkg/api # 多个packages make test WHAT=./pkg/{api,kubelet} 或者，也可以直接用 go test\ngo test -v k8s.io/kubernetes/pkg/kubelet 仅测试指定 package 的某个测试 case\n# Runs TestValidatePod in pkg/api/validation with the verbose flag set make test WHAT=./pkg/api/validation KUBE_GOFLAGS=\u0026#34;-v\u0026#34; KUBE_TEST_ARGS=\u0026#39;-run ^TestValidatePod$\u0026#39; # Runs tests that match the regex ValidatePod|ValidateConfigMap in pkg/api/validation make test WHAT=./pkg/api/validation KUBE_GOFLAGS=\u0026#34;-v\u0026#34; KUBE_TEST_ARGS=\u0026#34;-run ValidatePod\\|ValidateConfigMap$\u0026#34; 或者直接用 go test\ngo test -v k8s.io/kubernetes/pkg/api/validation -run ^TestValidatePod$ 并行测试\n并行测试是 root out flakes 的一种有效方法：\n# Have 2 workers run all tests 5 times each (10 total iterations). make test PARALLEL=2 ITERATION=5 生成测试报告\nmake test KUBE_COVER=y Benchmark 测试 go test ./pkg/apiserver -benchmem -run=XXX -bench=BenchmarkWatch 集成测试 Kubernetes 集成测试需要安装 etcd（只要安装即可，不需要启动），比如\nhack/install-etcd.sh # Installs in ./third_party/etcd echo export PATH=\u0026#34;\\$PATH:$(pwd)/third_party/etcd\u0026#34; \u0026gt;\u0026gt; ~/.profile # Add to PATH 集成测试会在需要的时候自动启动 etcd 和 kubernetes 服务，并运行 test/integration 里面的测试。\n运行所有集成测试\nmake test-integration # Run all integration tests. 指定集成测试用例\n# Run integration test TestPodUpdateActiveDeadlineSeconds with the verbose flag set. make test-integration KUBE_GOFLAGS=\u0026#34;-v\u0026#34; KUBE_TEST_ARGS=\u0026#34;-run ^TestPodUpdateActiveDeadlineSeconds$\u0026#34; End to end (e2e) 测试 End to end (e2e) 测试模拟用户行为操作 Kubernetes，用来保证 Kubernetes 服务或集群的行为完全符合设计预期。\n在开启 e2e 测试之前，需要先编译测试文件，并设置 KUBERNETES_PROVIDER（默认为 gce）：\nmake WHAT=\u0026#39;test/e2e/e2e.test\u0026#39; make ginkgo export KUBERNETES_PROVIDER=local 启动 cluster，测试，最后停止 cluster\n# build Kubernetes, up a cluster, run tests, and tear everything down go run hack/e2e.go -- -v --build --up --test --down 仅测试指定的用例\ngo run hack/e2e.go -v -test --test_args=\u0026#39;--ginkgo.focus=Kubectl\\sclient\\s\\[k8s\\.io\\]\\sKubectl\\srolling\\-update\\sshould\\ssupport\\srolling\\-update\\sto\\ssame\\simage\\s\\[Conformance\\]$\u0026#39; 略过测试用例\ngo run hack/e2e.go -- -v --test --test_args=\u0026#34;--ginkgo.skip=Pods.*env 并行测试\n# Run tests in parallel, skip any that must be run serially GINKGO_PARALLEL=y go run hack/e2e.go --v --test --test_args=\u0026#34;--ginkgo.skip=\\[Serial\\]\u0026#34; # Run tests in parallel, skip any that must be run serially and keep the test namespace if test failed GINKGO_PARALLEL=y go run hack/e2e.go --v --test --test_args=\u0026#34;--ginkgo.skip=\\[Serial\\] --delete-namespace-on-failure=false\u0026#34; 清理测试\ngo run hack/e2e.go -- -v --down 有用的 -ctl\n# -ctl can be used to quickly call kubectl against your e2e cluster. Useful for # cleaning up after a failed test or viewing logs. Use -v to avoid suppressing # kubectl output. go run hack/e2e.go -- -v -ctl=\u0026#39;get events\u0026#39; go run hack/e2e.go -- -v -ctl=\u0026#39;delete pod foobar\u0026#39; Fedaration e2e 测试 export FEDERATION=true export E2E_ZONES=\u0026#34;us-central1-a us-central1-b us-central1-f\u0026#34; # or export FEDERATION_PUSH_REPO_BASE=\u0026#34;quay.io/colin_hom\u0026#34; export FEDERATION_PUSH_REPO_BASE=\u0026#34;gcr.io/${GCE_PROJECT_NAME}\u0026#34; # build container images KUBE_RELEASE_RUN_TESTS=n KUBE_FASTBUILD=true go run hack/e2e.go -- -v -build # push the federation container images build/push-federation-images.sh # Deploy federation control plane go run hack/e2e.go -- -v --up # Finally, run the tests go run hack/e2e.go -- -v --test --test_args=\u0026#34;--ginkgo.focus=\\[Feature:Federation\\]\u0026#34; # Don\u0026#39;t forget to teardown everything down go run hack/e2e.go -- -v --down 可以用 cluster/log-dump.sh \u0026lt;directory\u0026gt; 方便的下载相关日志，帮助排查测试中碰到的问题。\nNode e2e 测试 Node e2e 仅测试 Kubelet 的相关功能，可以在本地或者集群中测试\nexport KUBERNETES_PROVIDER=local make test-e2e-node FOCUS=\u0026#34;InitContainer\u0026#34; make test_e2e_node TEST_ARGS=\u0026#34;--experimental-cgroups-per-qos=true\u0026#34; 补充说明 借助 kubectl 的模版可以方便获取想要的数据，比如查询某个 container 的镜像的方法为\nkubectl get pods nginx-4263166205-ggst4 -o template \u0026#39;--template={{if (exists . \u0026#34;status\u0026#34; \u0026#34;containerStatuses\u0026#34;)}}{{range .status.containerStatuses}}{{if eq .name \u0026#34;nginx\u0026#34;}}{{.image}}{{end}}{{end}}{{end}}\u0026#39; kubernetes 测试工具集 test-infra test-infra 是由 kubernetes 官方开源的测试框架，其中包括了 Kubernetes 测试工具集和测试结果展示。下图展示了 test-infra 的架构：\n   test-infra架构图（图片来自官方GitHub）  该测试框架主要是真多 Google 公有云做的，支持 kubernetes1.6 以上版本的测试。详见 https://github.com/kubernetes/test-infra。\n参考文档  https://github.com/kubernetes/test-infra  ","date":1653062400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"41df6ddd3931c7b1ecb5c27759ed9246","permalink":"https://lib.jimmysong.io/kubernetes-handbook/develop/testing/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/develop/testing/","section":"kubernetes-handbook","summary":"这篇文章将指导你如何测试 Kubernetes。 单元测试 单元测试仅依赖于源代码，是测试代码逻辑是否符合预期的最简单方法。 运行所有的单元测试 make test 仅测试指定的 package # 单个package make test WHAT=./pkg/api # 多个package","tags":["Kubernetes"],"title":"测试 Kubernetes","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"访问 kubernetes 集群有以下几种方式：\n   方式 特点 支持者     Kubernetes dashboard 直接通过 Web UI 进行操作，简单直接，可定制化程度低 官方支持   kubectl 命令行操作，功能最全，但是比较复杂，适合对其进行进一步的分装，定制功能，版本适配最好 官方支持   client-go 从 kubernetes 的代码中抽离出来的客户端包，简单易用，但需要小心区分 kubernetes 的 API 版本 官方支持   client-python python 客户端，kubernetes-incubator 官方支持   Java client fabric8 中的一部分，kubernetes 的 java 客户端 Red Hat    下面，我们基于 client-go，对 Deployment 升级镜像的步骤进行了定制，通过命令行传递一个 Deployment 的名字、应用容器名和新 image 名字的方式来升级。\nkubernetes-client-go-sample 项目的 main.go 代码如下：\npackage main import ( \u0026#34;flag\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;os\u0026#34; \u0026#34;path/filepath\u0026#34; \u0026#34;k8s.io/apimachinery/pkg/api/errors\u0026#34; metav1 \u0026#34;k8s.io/apimachinery/pkg/apis/meta/v1\u0026#34; \u0026#34;k8s.io/client-go/kubernetes\u0026#34; \u0026#34;k8s.io/client-go/tools/clientcmd\u0026#34; ) func main() { var kubeconfig *string if home := homeDir(); home != \u0026#34;\u0026#34; { kubeconfig = flag.String(\u0026#34;kubeconfig\u0026#34;, filepath.Join(home, \u0026#34;.kube\u0026#34;, \u0026#34;config\u0026#34;), \u0026#34;(optional) absolute path to the kubeconfig file\u0026#34;) } else { kubeconfig = flag.String(\u0026#34;kubeconfig\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;absolute path to the kubeconfig file\u0026#34;) } deploymentName := flag.String(\u0026#34;deployment\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;deployment name\u0026#34;) imageName := flag.String(\u0026#34;image\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;new image name\u0026#34;) appName := flag.String(\u0026#34;app\u0026#34;, \u0026#34;app\u0026#34;, \u0026#34;application name\u0026#34;) flag.Parse() if *deploymentName == \u0026#34;\u0026#34; { fmt.Println(\u0026#34;You must specify the deployment name.\u0026#34;) os.Exit(0) } if *imageName == \u0026#34;\u0026#34; { fmt.Println(\u0026#34;You must specify the new image name.\u0026#34;) os.Exit(0) } // use the current context in kubeconfig \tconfig, err := clientcmd.BuildConfigFromFlags(\u0026#34;\u0026#34;, *kubeconfig) if err != nil { panic(err.Error()) } // create the clientset \tclientset, err := kubernetes.NewForConfig(config) if err != nil { panic(err.Error()) } deployment, err := clientset.AppsV1beta1().Deployments(\u0026#34;default\u0026#34;).Get(*deploymentName, metav1.GetOptions{}) if err != nil { panic(err.Error()) } if errors.IsNotFound(err) { fmt.Printf(\u0026#34;Deployment not found\\n\u0026#34;) } else if statusError, isStatus := err.(*errors.StatusError); isStatus { fmt.Printf(\u0026#34;Error getting deployment%v\\n\u0026#34;, statusError.ErrStatus.Message) } else if err != nil { panic(err.Error()) } else { fmt.Printf(\u0026#34;Found deployment\\n\u0026#34;) name := deployment.GetName() fmt.Println(\u0026#34;name -\u0026gt;\u0026#34;, name) containers := \u0026amp;deployment.Spec.Template.Spec.Containers found := false for i := range *containers { c := *containers if c[i].Name == *appName { found = true fmt.Println(\u0026#34;Old image -\u0026gt;\u0026#34;, c[i].Image) fmt.Println(\u0026#34;New image -\u0026gt;\u0026#34;, *imageName) c[i].Image = *imageName } } if found == false { fmt.Println(\u0026#34;The application container not exist in the deployment pods.\u0026#34;) os.Exit(0) } _, err := clientset.AppsV1beta1().Deployments(\u0026#34;default\u0026#34;).Update(deployment) if err != nil { panic(err.Error()) } } } func homeDir() string { if h := os.Getenv(\u0026#34;HOME\u0026#34;); h != \u0026#34;\u0026#34; { return h } return os.Getenv(\u0026#34;USERPROFILE\u0026#34;) // windows } 我们使用 kubeconfig 文件认证连接 Kubernetes 集群，该文件默认的位置是 $HOME/.kube/config。\n该代码编译后可以直接在 Kubernetes 集群之外，任何一个可以连接到 API server 的机器上运行。\n编译运行\n$ go get github.com/rootsongjc/kubernetes-client-go-sample $ cd $GOPATH/src/github.com/rootsongjc/kubernetes-client-go-sample $ go build main.go 该命令的用法如下。\n$ ./main -app string application name (default \u0026#34;app\u0026#34;) -deployment string deployment name -image string new image name -kubeconfig string (optional) absolute path to the kubeconfig file (default \u0026#34;/Users/jimmy/.kube/config\u0026#34;) 使用不存在的 image 更新\n$ ./main -deployment filebeat-test -image harbor-001.jimmysong.io/library/analytics-docker-test:Build_9 Found deployment name -\u0026gt; filebeat-test Old image -\u0026gt; harbor-001.jimmysong.io/library/analytics-docker-test:Build_8 New image -\u0026gt; harbor-001.jimmysong.io/library/analytics-docker-test:Build_9 查看 Deployment 的 event。\n$ kubectl describe deployment filebeat-test Name:\tfilebeat-test Namespace:\tdefault CreationTimestamp:\tFri, 19 May 2017 15:12:28 +0800 Labels:\tk8s-app=filebeat-test Selector:\tk8s-app=filebeat-test Replicas:\t2 updated | 3 total | 2 available | 2 unavailable StrategyType:\tRollingUpdate MinReadySeconds:\t0 RollingUpdateStrategy:\t1 max unavailable, 1 max surge Conditions: Type\tStatus\tReason ----\t------\t------ Available True\tMinimumReplicasAvailable Progressing True\tReplicaSetUpdated OldReplicaSets:\tfilebeat-test-2365467882 (2/2 replicas created) NewReplicaSet:\tfilebeat-test-2470325483 (2/2 replicas created) Events: FirstSeen\tLastSeen\tCount\tFrom\tSubObjectPath\tType\tReasoMessage ---------\t--------\t-----\t----\t-------------\t--------\t------------ 2h\t1m\t3\t{deployment-controller }\tNormal\tScalingReplicaSet\tScaled down replica set filebeat-test-2365467882 to 2 1m\t1m\t1\t{deployment-controller }\tNormal\tScalingReplicaSet\tScaled up replica set filebeat-test-2470325483 to 1 1m\t1m\t1\t{deployment-controller }\tNormal\tScalingReplicaSet\tScaled up replica set filebeat-test-2470325483 to 2 可以看到老的 ReplicaSet 从 3 个 replica 减少到了 2 个，有 2 个使用新配置的 replica 不可用，目前可用的 replica 是 2 个。\n这是因为我们指定的镜像不存在，查看 Deployment 的 pod 的状态。\n$ kubectl get pods -l k8s-app=filebeat-test NAME READY STATUS RESTARTS AGE filebeat-test-2365467882-4zwx8 2/2 Running 0 33d filebeat-test-2365467882-rqskl 2/2 Running 0 33d filebeat-test-2470325483-6vjbw 1/2 ImagePullBackOff 0 4m …","date":1653062400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"6f289e757e47a901d625bdb573774a84","permalink":"https://lib.jimmysong.io/kubernetes-handbook/develop/client-go-sample/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/develop/client-go-sample/","section":"kubernetes-handbook","summary":"访问 kubernetes 集群有以下几种方式： 方式 特点 支持者 Kubernetes dashboard 直接通过 Web UI 进行操作，简单直接，可定制化程度低 官方支持 kubectl 命令行操作，功能最全，但是比较复杂，适合对其进行进一步的分装，定制功能，版本适配最好 官方支持 client-go 从 kubernetes 的","tags":["Kubernetes"],"title":"client-go 示例","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"Operator 是由 CoreOS 开发的，用来扩展 Kubernetes API，特定的应用程序控制器，它用来创建、配置和管理复杂的有状态应用，如数据库、缓存和监控系统。Operator 基于 Kubernetes 的资源和控制器概念之上构建，但同时又包含了应用程序特定的领域知识。创建 Operator 的关键是 CRD（自定义资源）的设计。awesome-operators 中罗列了目前已知的 Operator。\n工作原理 Operator 是将运维人员对软件操作的知识给代码化，同时利用 Kubernetes 强大的抽象来管理大规模的软件应用。\nOperator 使用了 Kubernetes 的自定义资源扩展 API 机制，如使用 CRD（CustomResourceDefinition）来创建。Operator 通过这种机制来创建、配置和管理应用程序。\nOperator 基于 Kubernetes 的以下两个概念构建：\n 资源：对象的状态定义 控制器：观测、分析和行动，以调节资源的分布  Operator 用途 若您有以下需求，可能会需要用到 Operator：\n 按需部署一个应用程序 需要备份和恢复应用程序的状态（如数据库） 处理应用程序代码的升级以及相关更改，例如数据库架构或额外的配置设置 发布一个服务，要让不支持 Kubernetes API 的应用程序能够发现 模拟整个或部分集群中的故障以测试其弹性 在没有内部成员选举程序的情况下为分布式应用程序选择领导者  一个 Operator 用途的详细示例 下面是一个使用 Operator 的详细示例。\n 一个名为 SampleDB 的自定义资源，您可以将其配置到集群中。 确保正在运行的 Deployment 的 Pod 中包含 Operator 的控制器部分。 Operator 代码的容器镜像。 查询控制平面以找出配置了哪些 SampleDB 资源的控制器代码。 Operator 的核心是告诉 API Server 如何使现实与代码里已配置的资源匹配。  如果添加新的 SampleDB，Operator 将设置 PersistentVolumeClaims 以提供持久的数据库存储，设置 StatefulSet 以运行 SampleDB，并设置 Job 来处理初始配置。 如果删除它，Operator 将建立快照，然后确保删除了 StatefulSet 和卷。   Operator 还管理常规数据库备份。对于每个 SampleDB 资源，Operator 确定何时创建可以连接到数据库并进行备份的 Pod。这些 Pod 将依赖于 ConfigMap 和 / 或具有数据库连接详细信息和凭据的 Secret。 由于 Operator 旨在为其管理的资源提供强大的自动化功能，因此会有其他支持代码。对于此示例，代码将检查数据库是否正在运行旧版本，如果是，则创建 Job 对象为您升级数据库。  创建 Operator Operator 本质上是与应用息息相关的，因为这是特定领域的知识的编码结果，这其中包括了资源配置的控制逻辑。下面是创建 Operator 的基本步骤：\n 在单个 Deployment 中定义 Operator，如：https://coreos.com/operators/etcd/latest/deployment.yaml 需要为 Operator 创建一个新的自定义类型 CRD，这样用户就可以使用该对象来创建实例 Operator 应该利用 Kubernetes 中内建的原语，如 Deployment、Service 这些经过充分测试的对象，这样也便于理解 Operator 应该向后兼容，始终了解用户在之前版本中创建的资源 当 Operator 被停止或删除时，Operator 创建的应用实例应该不受影响 Operator 应该让用户能够根据版本声明来选择所需版本和编排应用程序升级。不升级软件是操作错误和安全问题的常见来源，Operator 可以帮助用户更加自信地解决这一问题。 Operator 应该进行 “Chaos Monkey” 测试，以模拟 Pod、配置和网络故障的情况下的行为。  参考  Operators - coreos.com awesome-operators - github.com Writing a Kubernetes Operator in Golang Introducing Operators: Putting Operational Knowledge into Software - coreos.com Automating Kubernetes Cluster Operations with Operators - thenewstack.io Operator pattern - kubernetes.io  ","date":1653062400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"d407bde8c2c0ec6ddf002223f522d555","permalink":"https://lib.jimmysong.io/kubernetes-handbook/develop/operator/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/develop/operator/","section":"kubernetes-handbook","summary":"Operator 是由 CoreOS 开发的，用来扩展 Kubernetes API，特定的应用程序控制器，它用来创建、配置和管理复杂的有状态应用，如数据库、缓存和监控系统。Operator 基于 Kubernetes 的资源和控制器概念之上构建，但同时又包含了应用程序特定的","tags":["Kubernetes"],"title":"Operator","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"Operator SDK 由 CoreOS 开源，它是用于构建 Kubernetes 原生应用的 SDK，它提供更高级别的 API、抽象和项目脚手架。在阅读本文前请先确认您已经了解 Operator是什么。\n使用 Kubernetes 中原生的对象来部署和管理复杂的应用程序不是那么容易，尤其是要管理整个应用的生命周期、组件的扩缩容，我们之前通常是编写各种脚本，通过调用 Kubernetes 的命令行工具来管理 Kubernetes 上的应用。现在可以通过 CRD（CustomResourceDefinition）来自定义这些复杂操作，通过将运维的知识封装在自定义 API 里来减轻运维人员的负担。同时我们还可以像操作 Kubernetes 的原生资源对象一样，使用 kubectl 来操作 CRD。\n下面我们将安装和试用一下 Operator SDK。\n安装 Operator SDK $ mkdir -p $GOPATH/src/github.com/operator-framework $ cd $GOPATH/src/github.com/operator-framework/operator-sdk $ dep ensure Create kubernetes-operator-sdk-tutorial/cmd/kubernetes-operator-sdk-tutorial/main.go Create kubernetes-operator-sdk-tutorial/config/config.yaml Create kubernetes-operator-sdk-tutorial/deploy/rbac.yaml Create kubernetes-operator-sdk-tutorial/deploy/cr.yaml Create kubernetes-operator-sdk-tutorial/pkg/apis/jimmysong/v1alpha1/doc.go Create kubernetes-operator-sdk-tutorial/pkg/apis/jimmysong/v1alpha1/register.go Create kubernetes-operator-sdk-tutorial/pkg/apis/jimmysong/v1alpha1/types.go Create kubernetes-operator-sdk-tutorial/pkg/stub/handler.go Create kubernetes-operator-sdk-tutorial/tmp/build/build.sh Create kubernetes-operator-sdk-tutorial/tmp/build/docker_build.sh Create kubernetes-operator-sdk-tutorial/tmp/build/Dockerfile Create kubernetes-operator-sdk-tutorial/tmp/codegen/boilerplate.go.txt Create kubernetes-operator-sdk-tutorial/tmp/codegen/update-generated.sh Create kubernetes-operator-sdk-tutorial/Gopkg.toml Create kubernetes-operator-sdk-tutorial/Gopkg.lock Run dep ensure ... Root project is \u0026#34;github.com/rootsongjc/kubernetes-operator-sdk-tutorial\u0026#34; 3 transitively valid internal packages 12 external packages imported from 4 projects (0) ✓ select (root) (1)\t? attempt k8s.io/api with 1 pkgs; at least 1 versions to try (1)\ttry k8s.io/api@kubernetes-1.9.3 (1)\t✓ select k8s.io/api@kubernetes-1.9.3 w/1 pkgs (2)\t? attempt k8s.io/apimachinery with 4 pkgs; at least 1 versions to try (2)\ttry k8s.io/apimachinery@kubernetes-1.9.3 (2)\t✓ select k8s.io/apimachinery@kubernetes-1.9.3 w/22 pkgs ... $ go install github.com/operator-framework/operator-sdk/commands/operator-sdk 该过程需要几分钟，请耐心等待。确认 $GOPATH/bin/operator-sdk 文件位于您的 $PATH 目录下。\n创建项目 $ cd $GOPATH/src/github.com/\u0026lt;your-github-repo\u0026gt;/ $ operator-sdk new \u0026lt;operator-project-name\u0026gt; --api-version=\u0026lt;your-api-group\u0026gt;/\u0026lt;version\u0026gt; --kind=\u0026lt;custom-resource-kind\u0026gt; $ cd \u0026lt;operator-project-name\u0026gt;  operator-project-name：创建的项目的名称 your-api-group：Kubernetes 自定义 API 的组名，一般用域名如 jimmysong.io version：Kubernetes 自定义资源的 API 版本 custom-resource-kind：CRD 的名称  operator-sdk new kubernetes-operator-sdk-tutorial --api-version=jimmysong.io/v1alpha1 --kind=operator-sdk 参考  A complete guide to Kubernetes Operator SDK  ","date":1653062400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"bdfdff09cfd9d4914d369b75c3191d4e","permalink":"https://lib.jimmysong.io/kubernetes-handbook/develop/operator-sdk/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/develop/operator-sdk/","section":"kubernetes-handbook","summary":"Operator SDK 由 CoreOS 开源，它是用于构建 Kubernetes 原生应用的 SDK，它提供更高级别的 API、抽象和项目脚手架。在阅读本文前请先确认您已经了解 Operator是什么。 使用 Kubernetes 中原生的对象来部署和管理复杂的应用程序不是那么容易，","tags":["Kubernetes"],"title":"Operator SDK","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"Kubebuilder 是一个基于 CRD 来构建 Kubernetes API 的框架，可以使用 CRD 来构建 API、Controller 和 Admission Webhook。\n动机 目前扩展 Kubernetes 的 API 的方式有创建 CRD、使用 Operator SDK 等方式，都需要写很多的样本文件（boilerplate），使用起来十分麻烦。为了能够更方便构建 Kubernetes API 和工具，就需要一款能够事半功倍的工具，与其他 Kubernetes API 扩展方案相比，kubebuilder 更加简单易用，并获得了社区的广泛支持。\n工作流程 Kubebuilder 的工作流程如下：\n 创建一个新的工程目录 创建一个或多个资源 API CRD 然后将字段添加到资源 在控制器中实现协调循环（reconcile loop），watch 额外的资源 在集群中运行测试（自动安装 CRD 并自动启动控制器） 更新引导集成测试测试新字段和业务逻辑 使用用户提供的 Dockerfile 构建和发布容器  设计哲学 Kubebuilder 提供基于简洁的精心设计的示例 godoc 来提供整洁的库抽象。\n 能使用 go 接口和库，就不使用代码生成 能使用代码生成，就不用使用多于一次的存根初始化 能使用一次存根，就不 fork 和修改 boilerplate 绝不 fork 和修改 boilerplate  示例 下面是一个使用 kubebuilder 创建 Kubernetes Operator 的示例。\n准备 本文中的示例运行环境及相关软件版本如下：\n Kubernetes MiniKube v1.9.2 Kubernetes v1.18.0 Go 1.14 Kubebuilder 2.3.1 kustomize 3.6.1 Docker 19.03.8  使用 Minikube 安装 Kubernetes 集群，Kubernetes 安装好后，检查集群是否可用。\nMinikube 的 DNS 解析问题 如果遇到 Kubernetes 集群无法拉取镜像，DNS 解析出现问题，解决方式见 DNS lookup not working when starting minikube with –dns-domain #1674。\n使用 minikube ssh 进入 minikube 主机，修改 /etc/systemd/resolved.conf 文件，将其中的 DNS 配置字段修改为 DNS=8.8.8.8，然后执行 sudo systemctl restart systemd-resolved 即可更改 DNS，切勿直接修改 /etc/resolv.conf 文件。\n修正 Minikube 的 DNS 配置，请执行下面的命令。\nminikube ssh sudo sed -i \u0026#39;s/#DNS=/DNS=8.8.8.8/g\u0026#39; /etc/systemd/resolved.conf sudo systemctl restart systemd-resolved 名词解释 在阅读下面的文章前，需要先明确以下两个名词的含义。\n CRD：自定义资源定义，Kubernetes 中的资源类型。 CR：Custom Resource，对使用 CRD 创建出来的自定义资源的统称。  安装 kubebuilder 到 kubebuilder 的 GitHub release 页面上下载与您操作系统对应的 kubebuilder 安装包。\nMacOS\n对于 Mac 系统，将下载好的安装包解压后将其移动到 /usr/local/kubebuilder 目录下，并将 /usr/local/kubebuilder/bin 添加到您的 $PATH 路径下。\n创建项目 我们首先将使用自动配置创建一个项目，该项目在创建 CR 时不会触发任何资源生成。\n初始化和创建 API 创建的项目路径位于 $GOPATH/jimmysong.io/kubebuilder-example。下文中的操作没有明确说明的话都是在该项目路径下运行。\n在项目路径下使用下面的命令初始化项目。\n$ kubebuilder init --domain jimmysong.io 在项目根目录下执行下面的命令创建 API。\n$ kubebuilder create api --group webapp --version v1 --kind Guestbook Create Resource under pkg/apis [y/n]? y Create Controller under pkg/controller [y/n]? y Writing scaffold for you to edit... api/v1/guestbook_types.go controllers/guestbook_controller.go Running make: $ make /Users/jimmysong/Workspace/go/bin/controller-gen object:headerFile=\u0026#34;hack/boilerplate.go.txt\u0026#34; paths=\u0026#34;./...\u0026#34; go fmt ./... go vet ./... go: finding github.com/onsi/ginkgo v1.11.0 go: finding github.com/onsi/gomega v1.8.1 go: finding github.com/hpcloud/tail v1.0.0 go: finding gopkg.in/tomb.v1 v1.0.0-20141024135613-dd632973f1e7 go build -o bin/manager main.go API 创建完成后，在项目根目录下查看目录结构。\n. ├── Dockerfile # 用于构建 Operator 镜像 ├── Makefile # 构建时使用 ├── PROJECT # 项目配置 ├── api │ └── v1 │ ├── groupversion_info.go │ ├── guestbook_types.go │ └── zz_generated.deepcopy.go ├── bin │ └── manager ├── config │ ├── certmanager │ │ ├── certificate.yaml │ │ ├── kustomization.yaml │ │ └── kustomizeconfig.yaml │ ├── crd # 新增 CRD 定义 │ │ ├── kustomization.yaml │ │ ├── kustomizeconfig.yaml │ │ └── patches │ ├── default │ │ ├── kustomization.yaml │ │ ├── manager_auth_proxy_patch.yaml │ │ ├── manager_webhook_patch.yaml │ │ └── webhookcainjection_patch.yaml │ ├── manager │ │ ├── kustomization.yaml │ │ └── manager.yaml │ ├── prometheus │ │ ├── kustomization.yaml │ │ └── monitor.yaml │ ├── rbac │ │ ├── auth_proxy_client_clusterrole.yaml │ │ ├── auth_proxy_role.yaml │ │ ├── auth_proxy_role_binding.yaml │ │ ├── auth_proxy_service.yaml │ │ ├── guestbook_editor_role.yaml │ │ ├── guestbook_viewer_role.yaml │ │ ├── kustomization.yaml │ │ ├── leader_election_role.yaml │ │ ├── leader_election_role_binding.yaml │ │ └── role_binding.yaml │ ├── samples │ │ └── webapp_v1_guestbook.yaml # CRD 示例 │ └── webhook │ ├── kustomization.yaml │ ├── kustomizeconfig.yaml │ └── service.yaml ├── controllers # 新增 controller │ ├── guestbook_controller.go │ └── suite_test.go ├── go.mod ├── go.sum ├── hack │ └── boilerplate.go.txt └── main.go # 新增处理逻辑 15 directories, 40 files 以上就是自动初始化出来的文件。\n安装 CRD 执行下面的命令安装 CRD。\n$ make install /Users/jimmysong/Workspace/go/bin/controller-gen \u0026#34;crd:trivialVersions=true\u0026#34; rbac:roleName=manager-role webhook paths=\u0026#34;./...\u0026#34; output:crd:artifacts:config=config/crd/bases kustomize build config/crd | kubectl apply -f - customresourcedefinition.apiextensions.k8s.io/guestbooks.webapp.jimmysong.io created $ kubectl get crd |grep jimmysong.io guestbooks.webapp.jimmysong.io 2020-06-06T21:58:17Z 部署 controller 在开始部署 controller 之前，我们需要先检查 kubebuilder 自动生成的 YAML 文件。\n修改使用 gcr.io 镜像仓库的镜像地址\n对于中国大陆用户，可能无法访问 Google 镜像仓库 gcr.io，因此需要修改 config/default/manager_auth_proxy_patch.yaml 文件中的镜像地址，将其中 gcr.io/kube-rbac-proxy:v0.5.0 修改为 jimmysong/kubebuilder-kube-rbac-proxy:v0.5.0。\n有两种方式运行 controller：\n 本地运行，用于调试 部署到 Kubernetes 上运行，作为生产使用  本地运行 controller\n要想在本地运行 controller，只需要执行下面的命令。\nmake run 你将看到 controller 启动和运行时输出。\n将 controller 部署到 Kubernetes\n执行下面的命令部署 controller 到 Kubernetes 上，这一步将会在本地构建 controller 的镜像，并推送到 DockerHub 上，然后在 Kubernetes 上部署 Deployment 资源。\nmake docker-build docker-push IMG=jimmysong/kubebuilder-example:latest make deploy IMG=jimmysong/kubebuilder-example:latest 在初始化项目时，kubebuilder 会自动根据项目名称创建一个 Namespace，如 …","date":1653062400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"c42a83a9c9c30f0cb68f4423e20796b0","permalink":"https://lib.jimmysong.io/kubernetes-handbook/develop/kubebuilder/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/develop/kubebuilder/","section":"kubernetes-handbook","summary":"Kubebuilder 是一个基于 CRD 来构建 Kubernetes API 的框架，可以使用 CRD 来构建 API、Controller 和 Admission Webhook。 动机 目前扩展 Kubernetes 的 API 的方式有创建 CRD、使用 Operator SDK 等方式，都需要写很多的样本文件（boilerplate），","tags":["Kubernetes"],"title":"Kubebuilder","type":"book"},{"authors":null,"categories":["Istio"],"content":"EnvoyFilter 资源允许你定制由 Istio Pilot 生成的 Envoy 配置。使用该资源，你可以更新数值，添加特定的过滤器，甚至添加新的监听器、集群等等。小心使用这个功能，因为不正确的定制可能会破坏整个网格的稳定性。\n过滤器是叠加应用的，这意味着对于特定命名空间中的特定工作负载，可以有任何数量的过滤器。根命名空间（例如 istio-system）中的过滤器首先被应用，然后是工作负载命名空间中的所有匹配过滤器。\n下面是一个 EnvoyFilter 的例子，它在请求中添加了一个名为 api-version 的头。\napiVersion:networking.istio.io/v1alpha3kind:EnvoyFiltermetadata:name:api-header-filternamespace:defaultspec:workloadSelector:labels:app:web-frontendconfigPatches:- applyTo:HTTP_FILTERmatch:context:SIDECAR_INBOUNDlistener:portNumber:8080filterChain:filter:name:\u0026#34;envoy.http_connection_manager\u0026#34;subFilter:name:\u0026#34;envoy.router\u0026#34;patch:operation:INSERT_BEFOREvalue:name:envoy.luatyped_config:\u0026#34;@type\u0026#34;: \u0026#34;type.googleapis.com/envoy.extensions.filters.http.lua.v3.Lua\u0026#34;inlineCode:|function envoy_on_response(response_handle) response_handle:headers():add(\u0026#34;api-version\u0026#34;, \u0026#34;v1\u0026#34;) end如果你向 $GATEWAY_URL 发送一个请求，你可以注意到 api-version 头被添加了，如下所示：\n$ curl -s -I -X HEAD http://$GATEWAY_URL HTTP/1.1 200 OK x-powered-by: Express content-type: text/html; charset=utf-8 content-length: 2471 etag: W/\u0026#34;9a7-hEXE7lJW5CDgD+e2FypGgChcgho\u0026#34; date: Tue, 17 Nov 2020 00:40:16 GMT x-envoy-upstream-service-time: 32 api-version: v1 server: istio-envoy  下一章   ","date":1652803200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"5c8612f44ebc7041c3dcd55d9e7464f7","permalink":"https://lib.jimmysong.io/istio-handbook/traffic-management/envoyfilter/","publishdate":"2022-05-18T00:00:00+08:00","relpermalink":"/istio-handbook/traffic-management/envoyfilter/","section":"istio-handbook","summary":"EnvoyFilter 资源允许你定制由 Istio Pilot 生成的 Envoy 配置。使用该资源，你可以更新数值，添加特定的过滤器，甚至添加新的监听器、集群等等。小心使用这个功能，因为不正确的定制可能会破坏整个网格的稳定性。 过滤器是叠加应用的，这意味","tags":["Istio","Service Mesh"],"title":"EnvoyFilter","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"本页假定您已经熟悉 Kubernetes 的核心概念并可以轻松的部署自己的应用程序。在浏览了本页面及其链接的内容后，您将会更好的理解如下部分：\n 可以在应用程序中使用的高级功能 扩展 Kubernetes API 的各种方法  使用高级功能部署应用 现在您知道了 Kubernetes 中提供的一组 API 对象。理解了 daemonset 和 deployment 之间的区别对于应用程序部署通常是足够的。也就是说，熟悉 Kubernetes 中其它的鲜为人知的功能也是值得的。因为这些功能有时候对于特别的用例是非常强大的。\n容器级功能 如您所知，将整个应用程序（例如容器化的 Rails 应用程序，MySQL 数据库以及所有应用程序）迁移到单个 Pod 中是一种反模式。这就是说，有一些非常有用的模式超出了容器和 Pod 之间的 1:1 的对应关系：\n Sidecar 容器：虽然 Pod 中依然需要有一个主容器，你还可以添加一个副容器作为辅助（见 日志示例)。单个 Pod 中的两个容器可以通过共享卷进行通信。 Init 容器：Init 容器在 Pod 的应用容器（如主容器和 sidecar 容器）之前运行。  Pod 配置 通常，您可以使用 label 和 annotation 将元数据附加到资源上。将数据注入到资源，您可以会创建 ConfigMap（用于非机密数据）或 Secret（用于机密数据）。\n下面是一些其他不太为人所知的配置资源 Pod 的方法：\n Taint（污点）和 Toleration（容忍）：这些为节点“吸引”或“排斥” Pod 提供了一种方法。当需要将应用程序部署到特定硬件（例如用于科学计算的 GPU）时，经常使用它们。阅读更多。 向下 API：这允许您的容器使用有关自己或集群的信息，而不会过度耦合到 Kubernetes API server。这可以通过环境变量 或者 DownwardAPIVolumeFiles。 Pod 预设：通常，要将运行时需求（例如环境变量、ConfigMap 和 Secret）安装到资源中，可以在资源的配置文件中指定它们。PodPresets 允许您在创建资源时动态注入这些需求。例如，这允许团队 A 将任意数量的新Secret 安装到团队 B 和 C 创建的资源中，而不需要 B 和 C 的操作。  其他 API 对象 在设置以下资源之前，请检查这是否属于您组织的集群管理员的责任。\n Horizontal Pod Autoscaler (HPA) ：这些资源是在CPU使用率或其他自定义度量标准“秒杀”时自动化扩展应用程序的好方法。*查看示例*以了解如何设置HPA。 联合集群对象：如果使用 federation 在多个 Kubernetes 集群上运行应用程序，则需要部署标准 Kubernetes API 对象的联合版本。  扩展 Kubernetes API Kubernetes 在设计之初就考虑到了可扩展性。如果上面提到的 API 资源和功能不足以满足您的需求，则可以自定义其行为，而无需修改核心 Kubernetes 代码。\n理解 Kubernetes 的默认行为 在进行任何自定义之前，了解 Kubernetes API 对象背后的一般抽象很重要。虽然 Deployment 和 Secret 看起来可能完全不同，但对于任何对象来说，以下概念都是正确的：\n  Kubernetes对象是存储有关您的集群的结构化数据的一种方式。\n在 Deployment 的情况下，该数据代表期望的状态（例如“应该运行多少副本？”），但也可以是通用的元数据（例如数据库凭证）。\n  Kubernetes 对象通过 Kubernetes API 修改。\n换句话说，您可以对特定的资源路径（例如 \u0026lt;api-server-url\u0026gt;/api/v1/namespaces/default/deployments ）执行 GET 和 POST 请求来读取或修改对应的对象类型。\n  利用 Controller 模式，Kubernetes 对象可被确保达到期望的状态。为了简单起见，您可以将 Controller 模式看作以下连续循环：\n 检查当前状态（副本数、容器镜像等） 对比当前状态和期望状态 如果不匹配则更新当前状态  这些状态是通过 Kubernetes API 来获取的。\n并非所有的 Kubernetes 对象都需要一个 Controller。尽管 Deployment 触发群集进行状态更改，但 ConfigMaps 纯粹作为存储。\n  创建自定义资源 基于上述想法，您可以定义与 Deployment 一样合法的自定义资源。例如，如果 CronJobs 不能提供所有您需要的功能，您可能需要定义 Backup 对象以进行定期备份。\n创建自定义资源有以下两种方式：\n 自定义资源定义（CRD）：这种实现方式的工作量最小。参考示例。 API 聚合：在实际设置单独的扩展 API server 之前，此方法需要一些预配置  请注意，与依赖内置的 kube-controller-manager 不同，您需要编写并运行自定义控制器。\n下面是一些有用的链接：\n 如何才知道自定义资源是否符合您的使用场景 CRD 还是 API 聚合，如何选择？  Service Catalog 如果您想要使用或提供完整的服务（而不是单个资源），Service Catalog 为此提供了一个规范。这些服务使用 Service Broker注册（请参阅 示例）。\n如果您没有集群管理员来管理 Service Catalog 的安装，您可以使用 Helm 或 二进制安装器。\n探索其他资源 参考 以下主题对构建更复杂的应用程序也很有用：\n Kubernetes 中的其他扩展点 - 在哪里可以挂勾到 Kubernetes 架构的概念性的概述 Kubernetes 客户端库 - 用于构建需要与 Kubernetes API 大量交互的应用程序。  下一步 恭喜您完成了应用开发者之旅！您已经了解了 Kubernetes 提供的大部分功能。现在怎么办？\n 如果您想推荐新功能或跟上Kubernetes应用开发的最新进展，请考虑加入 SIG，如 SIG Apps。  ","date":1653062400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"9cb0e32492a781c2be9da1b8b393f57a","permalink":"https://lib.jimmysong.io/kubernetes-handbook/develop/advance-developer/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/develop/advance-developer/","section":"kubernetes-handbook","summary":"本页假定您已经熟悉 Kubernetes 的核心概念并可以轻松的部署自己的应用程序。在浏览了本页面及其链接的内容后，您将会更好的理解如下部分： 可以在应用程序中使用的高级功能 扩展 Kubernetes API 的各种方法 使用高级功能部署应用 现在您知道了","tags":["Kubernetes"],"title":"高级开发指南","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"如果您想参与 Kubernetes 社区，请先阅读下Kubernetes Community这个 GitHub Repo中的文档，该文档中包括社区的治理形式、社区成员资格申请、提交 Issue、查找问题和提交 PR 的指导等。\n参考  Kubernetes Community Kubernetes Developer Guide Enhencement Tracking and Backlog Kubernetes 官方网站项目  ","date":1653062400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"95993365e5dcc82c170dab084c3351bb","permalink":"https://lib.jimmysong.io/kubernetes-handbook/develop/contribute/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/develop/contribute/","section":"kubernetes-handbook","summary":"如果您想参与 Kubernetes 社区，请先阅读下Kubernetes Community这个 GitHub Repo中的文档，该文档中包括社区的治理形式、社区成员资格申请、提交 Issue、查找问题和提交 PR 的指导等。 参考 Kubernetes Community Kubernetes Developer Guide Enhencement Tracking and","tags":["Kubernetes"],"title":"参与 Kubernetes 社区贡献","type":"book"},{"authors":null,"categories":["Kubernetes"],"content":"Minikube 用于在本地运行 kubernetes 环境，用来开发和测试。\n安装 Minikube 到 GitHub 下载 minikube，我安装的是 minikube v1.11.0。\n下载完成后修改文件名为 minikube，然后 chmod +x minikube，移动到 $PATH 目录下：\nsudo mv ~/Download/minikube-darwin-adm64 /usr/local/bin/ sudo chmod +x /usr/local/bin/minikube 安装 kubectl 方式一\n参考 Install and Set Up kubectl，直接使用二进制文件安装即可。\ncurl -LO https://storage.googleapis.com/kubernetes-release/release/`curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt`/bin/darwin/amd64/kubectl 方式二\n先访问 https://storage.googleapis.com/kubernetes-release/release/stable.txt 得到返回值，假设为 v1.18.4，然后拼接网址，直接在浏览器访问： https://storage.googleapis.com/kubernetes-release/release/v1.18.4/bin/darwin/amd64/kubectl 直接下载 kubectl 文件。\n若第一种方式访问多次超时，可以使用上述的第二种方式访问。\n启动 Minikube 对于 macOS，执行 minikube start --vm-driver=hyperkit （使用 hyperkit 作为虚拟机，不需要安装 docker）即可自动下载依赖文件，开始安装和启动 minikube。该过程中将自动执行以下步骤：\n 下载 docker-machine-driver-hyperkit（10.9 M） 下载虚拟机镜像（近 200M） 下载 Kubernetes 安装包（500 多 M）  安装完成后将生成默认的 ~/.kube/config 文件，自动指向 minikube 集群。\n注意：在安装过程中建议配置代理，否则将会有的镜像无法下载。\n常用命令 下面是 minkube 的常用命令。\n# 进入集群节点 minikube ssh # 查看节点 IP minikube ip # 停止集群 minikube stop # 删除集群 minikube delete 参考  Install minikube - kubernetes.io  ","date":1653062400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"c91c6acf7fde1e4c955801291dd2bf4e","permalink":"https://lib.jimmysong.io/kubernetes-handbook/develop/minikube/","publishdate":"2022-05-21T00:00:00+08:00","relpermalink":"/kubernetes-handbook/develop/minikube/","section":"kubernetes-handbook","summary":"Minikube 用于在本地运行 kubernetes 环境，用来开发和测试。 安装 Minikube 到 GitHub 下载 minikube，我安装的是 minikube v1.11.0。 下载完成后修改文件名为 minikube，然后 chmod +x minikube，移动到 $PATH 目录下： sudo mv ~/Download/minikube-darwin-adm64 /usr/local/bin/ sudo chmod +x /usr/local/bin/minikube 安","tags":["Kubernetes"],"title":"Minikube","type":"book"},{"authors":["刘晗","吴晟"],"categories":["可观测性"],"content":"背景介绍 Apache SkyWalking 观察部署在服务网格中的服务的度量、日志、追踪和事件。在进行故障排除时，SkyWalking 错误分析是一个宝贵的工具，可以帮助确定错误发生的位置。然而，确定性能问题更加困难：利用预先存在的观察数据往往不可能找到性能问题的根本原因。为此，动态调试和故障排除在进行服务性能剖析时就必不可少。在这篇文章中，我们将讨论如何使用 eBPF 技术来改进 SkyWalking 中的剖析功能，并用于分析服务网格中的性能影响。\nSkyWalking 中的追踪剖析 自 SkyWalking 7.0.0 以来，Trace Profiling 通过定期对线程堆栈进行采样，让开发者知道运行哪行代码花费更多时间，从而帮助开发者发现性能问题。然而，Trace Profiling 不适合以下情况：\n 线程模型：Trace Profiling 对于剖析在单线程中执行的代码最有用。它对严重依赖异步执行模式的中间件不太有用。例如，Go 中的 Goroutines 或 Kotlin Coroutines。 语言：目前，Trace Profiling 只支持 Java 和 Python，因为在 Go 和 Node.js 等一些语言的运行时中不容易获得线程栈。 Agent 绑定：Trace Profiling 需要安装 Agent，根据语言的不同，这可能很麻烦（例如，PHP 必须依赖其 C 内核；Rust 和 C/C++ 需要的仪器需要手动安装）。 关联性：由于追踪剖析只与单个请求相关，所以当无法确认哪个请求产生问题时则变得难已处理。 生命周期短的服务：由于（至少）两个原因，Trace Profiling 不支持短声明周期的服务：  在启动阶段，很难区分系统性能和类代码操作。 Trace Profiling 与一个端点相连，以识别性能影响，但没有端点来匹配这些短生命周期的服务。    幸运的是，有一些技术比 Trace Profiling 更实用。\neBPF 简介 我们发现，eBPF —— 一种可以在操作系统内核中运行沙盒程序的技术，从而安全有效地扩展内核的功能，而不需要修改内核或加载内核模块，可以帮助我们填补 Trace Profiling 留下的空白。eBPF 技术正在流行，因为它打破了传统上的用户和内核空间之间的障碍。现在我们可以将程序作为字节码注入到内核中运行，而不需要定制和重新编译内核。可观测可以很好地利用这一点。\n在下图中，我们可以看到，当系统执行 execve 系统调用时，eBPF 程序被触发，通过使用函数调用获得当前进程的运行时信息。\n   eBPF 程序调用流程图  使用 eBPF 技术，可以将 SkyWalking 的剖析能力范围扩大到：\n 全局性能剖析：在 eBPF 之前，数据收集被限制在代理可以观察的范围内。由于 eBPF 程序在内核中运行，它们可以观察到所有的线程。当你不确定某个性能问题是否是由一个特定的请求引起的，这一点特别有用。 数据内容：eBPF 可以转储用户和内核空间的线程栈，所以如果性能问题发生在内核空间就更容易被发现。 代理绑定：所有现代 Linux 内核都支持 eBPF，所以不需要安装任何东西。这意味着它是一个免编排与代理的模型。这减少了由内置软件引起的摩擦，这些软件可能没有安装正确的代理，如服务网格中的 Envoy。 采样类型：与追踪剖析不同，eBPF 是事件驱动的，因此，不受间隔轮询的限制。例如，eBPF 可以触发事件，并根据传输大小的阈值收集更多的数据。这可以让系统在极端负载下分流和优先收集数据。  eBPF 的局限性 虽然 eBPF 为发掘性能瓶颈提供了显著的优势，但没有任何技术是完美的。eBPF 有一些限制，如下所述（幸运的是，由于 SkyWalking 不依赖 eBPF，其影响是有限的）：\n Linux 版本要求：eBPF 程序需要的 Linux 内核版本要 4.4 以上，更新的内核版本可以提供更多的数据收集。BCC 记录了 不同 Linux 内核版本所支持的功能，不同版本之间的差异在于 eBPF 收集的数据集。 需要特权权限：所有打算将 eBPF 程序加载到 Linux 内核的进程必须在特权模式下运行。因此，代码中的错误或其他问题可能对安全有很大的影响。 对动态语言的支持较弱：eBPF 对基于 JIT 的动态语言，如 Java，支持较弱。这也取决于你想收集什么数据。对于 Profiling，eBPF 不支持解析程序的字符表（symbol），这就是为什么大多数基于 eBPF 的剖析技术只支持静态语言如 C、C++、Go 和 Rust。然而，字符表映射有时可以通过语言所提供的工具来解决。例如，在 Java 中，可以使用 perf-map-agent 来生成字符表映射。然而，动态语言不支持附加（uprobe）功能，而这种功能可以让我们通过符号追踪执行事件。  SkyWalking Rover 简介 SkyWalking Rover 是 SkyWalking 生态系统中引入的 eBPF 剖析功能。下图显示了 SkyWalking Rover 的整体架构。SkyWalking Rover 目前支持 Kubernetes 环境，必须部署在 Kubernetes 集群内。与 SkyWalking 后端服务器建立连接后，它将当前机器上的进程信息保存到 SkyWalking。当用户通过用户界面创建 eBPF 剖析任务时，SkyWalking Rover 会接收任务并在相关的基于 C、C++、Golang 和 Rust 语言的程序中执行。\n除了需要具有 eBPF 功能的内核外，部署 SkyWalking Rover 没有其他先决条件。\n   SkyWalking Rover 架构图  使用 Rover 进行 CPU 剖析 CPU 剖析是显示服务性能的最直观方式。受 Brendan Gregg 的博客文章 的启发，我们将 CPU 剖析分为两种类型，并在 Rover 中加以实施：\n  CPU 剖析：线程在 CPU 上的运行时间。\n  off-CPU 剖析：线程在 I/O、锁、定时器、分页 / 交换等方面被阻塞时的等待时间。\n  用 eBPF 对 Envoy 进行剖析 Envoy 是一个流行的代理，在 Istio 服务网格中被用作为数据平面。在 Kubernetes 集群中，Istio 将 Envoy 作为 sidecar 注入到每个服务的 pod 中，在那里透明地拦截和处理传入和传出的流量。作为数据平面，Envoy 的任何性能问题都会影响到网格中的所有服务流量。在这种情况下，使用 eBPF 剖析来分析生产中由服务网格引起的问题是比较有力的。\n演示环境 如果你想看到详细过程，我们已经建立了一个演示环境，在那里我们部署了一个 Nginx 服务进行压力测试。流量被 Envoy 拦截并转发到 Nginx。安装整个环境的命令可以在 GitHub 上获取。\nCPU 剖析 当服务的 CPU 使用率很高时，CPU 剖析适用于分析线程堆栈。如果堆栈被转储的次数较多，意味着线程堆栈占据了更多的 CPU 资源。\n在使用演示配置文件安装 Istio 时，我们发现有两个地方的性能可以优化：\n Zipkin 追踪：不同的 Zipkin 采样百分比对 QPS 有直接影响。 访问日志格式：减少 Envoy 访问日志的字段可以提高 QPS。  Zipkin 追踪 Zipkin 100% 采样 在默认的演示配置文件中，Envoy 使用 100% 采样作为默认的追踪策略。这对性能有什么影响？\n如下图所示，使用 CPU 剖析，我们发现它大约需要 16% 的 CPU 开销。在固定消耗 2 个 CPU 的情况下，其 QPS 可以达到 5.7K。\n   Zipkin 100% 采样 CPU 剖析的火焰图  禁用 Zipkin 追踪 此时，我们发现，如果没有必要，可以降低 Zipkin 采样比例，甚至可以禁用追踪。根据 Istio 文档，我们可以在安装 Istio 时使用以下命令禁用追踪。\nistioctl install -y --set profile=demo \\  --set \u0026#39;meshConfig.enableTracing=false\u0026#39; \\  --set \u0026#39;meshConfig.defaultConfig.tracing.sampling=0.0\u0026#39; 禁用追踪后，我们再次进行 CPU 剖析。根据下图，我们发现 Zipkin 已经从火焰图中消失了。在与前面的例子相同的 2 个 CPU 消耗下，QPS 达到 9K，几乎增加了 60%。\n   禁用 Zipkin 追踪的 CPU 剖析火焰图  追踪吞吐量 在 CPU 使用率相同的情况下，我们发现，当追踪功能被禁用时，Envoy 的性能会大大提升。当然，这需要我们在 Zipkin 收集的样本数量和 Envoy 的预期性能（QPS）之间做出权衡。\n下表说明了在相同的 CPU 使用率下，不同的 Zipkin 采样比例对 QPS 的影响。\n   Zipkin 采样比例 QPS CPU 备注     100% （默认） 5.7K 2 Zipkin 占用 16%   1% 8.1K 2 Zipkin 占用 0.3%   禁用 9.2K 2 Zipkin 占用 0%    访问日志格式 默认访问日志格式 在默认的演示配置文件中，默认的访问日志格式包含大量的数据。下面的火焰图显示了在解析数据时涉及的各种功能，如请求头、响应头和流媒体主体。\n   默认访问日志格式的 CPU 剖析火焰图  简化访问日志格式 通常情况下，我们不需要访问日志中的所有信息，所以我们通常可以简化它来获得我们需要的信息。下面的命令简化了访问日志的格式，只显示基本信息。\nistioctl install -y --set profile=demo \\  --set meshConfig.accessLogFormat=\u0026#34;[% START_TIME%] \\\u0026#34;% REQ (:METHOD)% % REQ (X-ENVOY-ORIGINAL-PATH?:PATH)% % PROTOCOL%\\\u0026#34;% RESPONSE_CODE%\\n\u0026#34; 简化访问日志格式后，我们发现 QPS 从 5.7K 增加到 5.9K。当再次执行 CPU 剖析时，日志格式化的 CPU 使用率从 2.4% 下降到 0.7%。\n简化日志格式帮助我们提高了性能。\nOff-CPU 剖析 Off-CPU 剖析适用于由非高 CPU 使用率引起的性能问题。例如，当一个服务中有太多的线程时，使用 off-CPU 剖析可以揭示出哪些线程花费了更多的时间进行上下文切换。\n我们提供两个维度的数据汇总。\n 切换次数：一个线程切换上下文的次数。当线程返回到 CPU 时，它完成了一次上下文切换。开关次数较多的线程栈会花费更多时间进行上下文切换。 切换持续时间：一个线程切换上下文所需的时间。切换持续时间较长的线程栈在 off-CPU 花费的时间较多。  写入访问日志 启用写入 使用与之前 CPU 测试相同的环境和设置，我们进行了 off-CPU 剖析。如下图所示，我们发现访问日志的写入占总上下文切换的 28% 左右。下图中的 __write 也表明这是 Linux 内核中的方法。\n   启用写入的 off-CPU 剖析火焰图  禁用写入 SkyWalking 实现了 Envoy 的访问日志服务（ALS）功能，允许我们使用 gRPC 协议将访问日志发送到 SkyWalking 可观察性分析平台（OAP）。即使禁用访问日志，我们仍然可以使用 ALS 来捕获 / 汇总日志。我们使用以下命令禁用了对访问日志的写入。\nistioctl install -y --set profile=demo --set meshConfig.accessLogFile=\u0026#34;\u0026#34; 禁用访问日志功能后，我们进行了 off-CPU 剖析。如下图所示，文件写入条目已经消失了。Envoy 的吞吐量也从 5.7K …","date":1657011600,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"2855c0e827fd48a1a2088ce3bdb5003c","permalink":"https://lib.jimmysong.io/blog/pinpoint-service-mesh-critical-performance-impact-by-using-ebpf/","publishdate":"2022-07-05T17:00:00+08:00","relpermalink":"/blog/pinpoint-service-mesh-critical-performance-impact-by-using-ebpf/","section":"blog","summary":"在这篇文章中，我们将讨论如何使用 eBPF 技术来改进 SkyWalking 中的剖析功能，并用于分析服务网格中的性能影响。","tags":["SkyWalking","可观测性","性能分析","eBPF","Service Mesh"],"title":"使用 eBPF 准确定位服务网格的关键性能问题","type":"post"},{"authors":["Petr Mcallister"],"categories":["网关"],"content":"编者的话 本文是来自 Tetrate 工程师的分享，Tetrate 的拳头产品是 Tetrate Service Bridge（下文简称 TSB），它是在开源的 Istio 和 Envoy 基础上构建的，但为其增加了管理平面。\n简介 Tetrate 的应用连接平台 Tetrate Service Bridge（TSB）提供两种网关类型，分别为一级网关（Tier-1）和二级网关（Tier-2），它们都基于 Envoy 构建，但是目的有所不同。本文将探讨这两种类型网关的功能，以及何时选用哪种网关。\n关于两级网关的简要介绍：\n 一级网关（下文简称 T1）位于应用边缘，用于多集群环境。同一应用会同时托管在不同的集群上，T1网关将对该应用的请求流量在这些集群之间路由。 二级网关（下文简称 T2）位于一个的集群边缘，用于将流量路由到该集群内由服务网格管理的服务。  两级网关释义 托管在 TSB 管理的集群中的应用部署的设计与开源的 Istio 模型非常相似。它们的结构相同，使用入口网关来路由传入的流量。T2 网关相当于 Istio 的入口网关（Ingress Gateway），在逻辑上与 Istio 开源模型相同，如图 1 所示。\n   图 1：Istio 网关vs Tetrate T2 网关  Tetrate Service Bridge 使用 Istio 和 Envoy 构建的服务网格管理集群的控制平面和数据平面，其本身不存在于应用数据路径中。比较开源 Istio 管理的集群和 TSB 管理的集群之间的数据包路径，你会发现两者之间没有区别。TSB 的配置清单（manifest）被 Istio 消费和使用。通过这种方式，TSB 的作用类似于 CI/CD 自动化逻辑，其中部署过程会影响应用程序的行为，但不会影响应用程序逻辑本身。\nTSB 在开源 Istio 的增加了一些组件，以管理每组应用程序网关范围的安装和配置，以加快开发和运维人员的工作进度，将基础设施和应用程序之间的职责分离，将错误配置网关的影响与其他应用程序/业务组隔离。\n何时使用 T1 网关？ 当你有两个或更多的 Kubernetes 集群为同一个应用服务时，为了增加容量、蓝绿部署、故障转移等，问题总是出现：入站流量如何在这些集群之间分配？ 在每个集群边缘的 T2 网关允许直接访问应用程序 —— 例如，集群 A 将监听 service1A.example.com，集群 X 将监听 service1X.example.com。反过来，T2 网关提供跨集群的全局负载均衡。跨集群的流量路由分配基于 1 到 100 之间的权重值，指定发送到特定集群的流量的百分比。\n下面是一个简单的 T1 网关配置的例子。这个例子表示了一个完整的 T1 网关清单，以证明该解决方案的简单性。关于具体设置的细节，请参考Tetrate API 文档。\napiVersion:gateway.tsb.tetrate.io/v2kind:Tier1Gatewaymetadata:name:service1-tier1group:demo-gw-grouporganization:demo-orgtenant:demo-tenantworkspace:demo-wsspec:workloadSelector:namespace:service1-tier1labels:app:tsb-gateway-service1-tier1istio:ingressgatewayexternalServers:- name:service1hostname:service1.cx.example.comport:80tls:{}clusters:- name:site-1-gcpweight:75- name:site-2-awsweight:25在这个例子中，到 service1.cx.example.com 的 75% 用户请求被转发到 GCP 中的 site-1，其余的转发到 AWS 中的 site-2。这个例子中的流量到达明文端口 80，之后 T1 网关和应用集群之间的所有通信都经过 mTLS 加密。\n   图 2：T1 和 T2 网关网络路径图  云供应网关整合 Istio 用户通常按应用模型实施入口网关。这种方法保证了对一个应用程序及其工件的安全、独立管理。\n这里注意到的最常见的痛点 —— 每个应用都需要使用云供应商的负载均衡器。这样做使得用户需要维护位于 Envoy 入口网关 Pod 前大量的负载均衡器，这带来资金开销和管理成本。\nTSB 允许通过 NodePort 服务类型而不是 LoadBalancer 进行服务发现和通信，这意味着不再需要云供应商的负载均衡器；TSB 集群内的服务可以通过 NodePort 直接到达。T1 网关允许我们将云供应商负载均衡器的使用压缩到一个单一的入口点。\n   图 3：云供应商网关整合  图 3 展示了通过将集群内的服务连接转移到 TSB，而不使用云供应商的负载均衡器来简化云设置。在没有 TSB 的情况下，要实现上述设置，需要使用外部负载均衡器。TSB 还维护 Kubernetes 节点的列表。\n资源要求 就 T2 网关所需的资源而言，开源 Istio 和 TSB 的要求没有什么不同。事实上，实现方式是一样的 —— Gateway 和 VirtualService 清单可以手动创建，也可以通过开源的自动化工具创建。在 Tetrate 的用例中，TSB 为 Istio 创建清单。\nT1 网关确实需要一个专门的控制平面，这意味着网格管理的应用程序和 T1 网关不能在同一个集群中运行，尽管承载 T1 网关的 Kubernetes 集群也可以承载服务网格以外的应用程序。不过，Tetrate 的有些客户将 T1 网关放在与 TSB 管理平面相同的集群上。\n架构考虑因素 随着应用环境的发展和成熟，出现新的需求是很常见的。T1 网关可以作为初始服务网格架构实施的一部分进行规划和实施，也可以在以后添加。增加一级网关只影响入口点的入站流量，但不需要对现有集群做任何改变。\n图 4 展示了一个没有 T1 网关的部署配置。\n   图 4：没有 T1 网关的部署配置  当引入 T1 网关时（图5），必须更新 DNS 记录以指向一级网关，而不需要对应用集群的设置进行修改。\n   图 5：带有一级网关的多集群部署实例  \n  注意：TSB 不是 DNS 管理工具，DNS 记录的更改是在 TSB 之外进行的（有多种自动化工具和技术可用于该操作)。   然而，在添加 T2 网关时，从使用 LoadBalancer 切换到 NodePort 架构，确实需要对应用集群进行轻微的改变。\n虽然 T1 网关作为应用边缘传入流量的前端，但它可以部署在一个高可用性的配置中（图 6）。\n   图 6：高可用性配置中的T1网关  在高可用性方面，可以使用 T1 网关的数量没有限制。这种灵活的架构允许用户建立强大的设计以满足广泛的要求。\n总结 本文涵盖了服务网格架构师在企业环境中设计 TSB 部署时最常见的架构问题。以下是最重要的收获：\n TSB T1 和 T2 网关使用 Istio 入口网关 Pod 和服务。这里没有引入额外的专有组件。 TSB 支持开源 Istio 中的网关模式。仅仅是名称上的改变，如 TSB 的入口网关被称为 T2 网关。 单一的网关可以用于所有的应用，也可以采用按应用划分的网关模式。 TSB 可以通过利用 Kubernetes NodePort 而不是 LoadBalancer 进行集群内通信，减少使用的云厂商负载均衡器的数量，从而降低云计算成本。 TSB T1 网关提供跨集群负载均衡功能。 由于在实施的早期阶段可能不需要跨集群负载均衡，因此 T1 网关不需要成为初始部署的一部分，可以在以后添加，对现有的应用程序没有重大影响。 多个 T1 网关可以部署在同一个应用程序前面，以实现高可用性。  ","date":1656471600,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"298d006800f64f6fd6bef89570897c16","permalink":"https://lib.jimmysong.io/blog/designing-traffic-flow-via-tier1-and-tier2-ingress-gateways/","publishdate":"2022-06-29T11:00:00+08:00","relpermalink":"/blog/designing-traffic-flow-via-tier1-and-tier2-ingress-gateways/","section":"blog","summary":"在 Kubernetes 中我们不能仅依靠网络层加密，还需要 mTLS 来对客户端和服务端进行双向的传输层认证。本文将聚焦于 TLS 的真实性，以及证书管理的难题，说明服务网格对于在 Kubernetes 中开启 mTLS 带来的便利。","tags":["网关","Envoy","Istio"],"title":"通过两级网关设计来路由服务网格流量","type":"post"},{"authors":null,"categories":null,"content":"Cilium 中文指南，节选译自 Cilium 官方文档。\n许可证 您可以使用署名 - 非商业性使用 - 相同方式共享 4.0 (CC BY-NC-SA 4.0) 协议共享。\n","date":1655683200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"dd9c85c805ba7fa50cae180ae824e5c3","permalink":"https://lib.jimmysong.io/book/cilium-handbook/","publishdate":"2022-06-20T00:00:00Z","relpermalink":"/book/cilium-handbook/","section":"book","summary":"节选译自 Cilium 官方文档。","tags":["Handbook系列"],"title":"Cilium 中文指南","type":"publication"},{"authors":["William Morgan"],"categories":["安全"],"content":"编者的话 本文翻译节选自 A Kubernetes engineer’s guide to mTLS，为了便于读者理解，笔者对原文做了一点修改 1。因为笔者最近在研究 Istio 中的身份认证及 SPIFFE，对如何在 Kubernetes 中应用 mTLS 以及为什么要使用 mTLS 产生了浓厚的兴趣，再回想起五年前手动安装 Kubernetes 时，因为给集群开启 TLS 问题而导致安装停滞不前。\n本文的主要观点是：在 Kubernetes 中我们不能仅依靠网络层加密，还需要 mTLS 来对客户端和服务端进行双向的传输层认证。本文将聚焦于 TLS 的真实性，以及证书管理的难题，说明服务网格对于在 Kubernetes 中开启 mTLS 带来的便利。\n 简介 Mutual TLS（双向 TLS），或称 mTLS，是 Kubernetes 中的一个热门话题，尤其是对于那些负责为应用程序提供传输层加密的人来说。但是，你有没有考虑过，什么是 mTLS，它提供什么样的安全，为什么需要 mTLS？\n本指南我将介绍什么是 mTLS，它与常规 TLS 的关系，以及为什么它与 Kubernetes 有关。我还会谈论 mTLS 的一些优缺点及替代方案。\n什么是 mTLS？ 对于常规 TLS，只需要服务端认证，mTLS 相对来说有一个额外的规定：客户端也要经过认证。但这意味着什么，为什么要这样做呢？\n在回答这些问题之前，我们需要先对 TLS 有一个基本的了解。TLS 是一个传输层协议，旨在为 TCP 连接提供安全保障（我们将在下面看到安全的确切含义）。TLS 在传输层工作，可以与任何使用 TCP 的应用层协议结合使用。例如，HTTPS 是 HTTP 与 TLS 的结合（HTTPS 中的 S 指的是 SSL，即 TLS 的前身），HTTP 不需要做任何改变来适应 TLS 2。\n因为 TLS 中存在各种各样的问题，使得其从安全的角度来看是不理想的。TLS 规范复杂，而且没有得到充分的说明，有些地方并没有真正的意义，而且不管怎样，实现起来也不会 100% 符合 TLS 规范。\n尽管有这些担忧，TLS 还是无处不在。你现在就在使用 TLS：这个页面是通过 HTTPS 提供的，你可能在浏览器的 URL 栏中看到一个小锁图标。\nTLS 提供什么样的安全性？ 大多数人把 TLS 与加密联系起来。但 TLS 不仅仅是这样。TLS 为连接提供了三种安全保证：\n 真实性：任何一方都能证明他们是自己所声称的身份。 保密性：其他人无法看到正在交换的数据。 完整性：收到的数据与发送的数据相同。  因此，虽然 TLS 确实给你提供了加密 —— 这就是它实现保密的方式 —— 但从 TLS 的角度来看，这对安全通信来说是不够的：你需要所有这三种属性。如果你没有真实性，那么有人就可以在连接的另一端进行欺骗。如果你没有完整性，那么有人可以修改通信中的关键信息。如果你没有保密性，那么任何人都可以监听。\n在这三种保证中，本文主要关注真实性。\nmTLS 什么时候有用？ 回到我们最初的定义：mTLS 是简单的常规 TLS，其中有一个额外的规定，即客户端也要经过认证。有了对 TLS 的基本了解，我们现在可以解析这个声明了。TLS 保证了真实性，但默认情况下，这只发生在一个方向上：客户端对服务器进行认证，但服务器并不对客户端进行认证。\n为什么 TLS 的默认只在一个方向进行认证？因为客户端的身份往往是不相关的。例如，在加载这个页面时，你的浏览器已经验证了要访问的网站服务端的身份，但服务端并没有验证你的浏览器的身份。它实际上并不关心你的浏览器的身份。\n当然，不验证客户端身份对于提供网页服务是有意义的，但有很多类型的通信，客户端的身份也很重要。例如 API 调用：如果你调用像 GitHub 这样的服务，那么 GitHub 需要知道你是谁 —— 除其他原因外，这样他们就可以给你发送账单。如果不向 GitHub 提供某种客户端身份，你就不能对 GitHub 的 API 进行调用。\n但 GitHub 并不使用 mTLS。相反，你通过给 GitHub 一个秘密的认证令牌（token）来认证自己，这个令牌是创建账户时分配给你的。坦率地说，mTLS 设置起来很烦人（后面会有很多这方面的内容），所以如果你提供像 GitHub 这样的公共 API，你可能会只使用 auth token。\n然而，使用 mTLS 的认证有一些非常强大的 auth token 方法没有的优势。首先，mTLS 认证可以完全在应用程序之外完成，不需要任何应用程序级别的功能来创建、注册或管理身份。使用 auth token 时，在你进行第一次 GitHub API 调用之前，你需要登录网站，创建一个账户，获得令牌 3。GitHub 的 API 必须知道这个 auth token，并提供将其传递给 API 调用和管理它的方法。但有了 mTLS，一个全新的客户端就可以直接认证自己，即使从没有人见过它。而应用程序不需要知道任何关于认证的事情，也不需要提供端点来管理认证。\n综上所述，我们看到 mTLS 非常适合以下情况：\n 你需要安全通信； 你关心客户端的身份； 不想为管理身份建立应用级流程； 你可以管理实际实施的复杂性。  有种场景具有所有这些特征，那就是微服务！\n使用 mTLS 来保护微服务的安全 mTLS 是保证微服务之间跨服务通信安全的好方法，原因就在上面。\n首先，你想要安全的通信。当我们把我们的应用程序拆分为多个服务时，我们最终会在这些服务之间的网络上发送敏感数据。任何能够进入网络的人都有可能读取这些敏感数据并伪造请求。\n第二，你关心客户端的身份。首先，你要确保你能知道调用是什么时候发生的，以便进行诊断，并正确记录指标等事项。此外，你可能想对这些身份进行授权（允许 A 调用 B 吗）。我们将在后面讨论更多关于授权的问题。\n第三，你并不真的想为管理服务身份建立应用级的流程。这不是业务逻辑，开发人员的时间最好用在其他地方。\n最后，如果你控制了平台，你实际上可以管理实施 mTLS 的复杂性。或者至少，比 GitHub 做得更好。在我们的 GitHub 例子中，每个用户都必须解决对 GitHub 进行身份验证的难题。这个挑战越难，对用户就越不利（对 GitHub 的底线也越不利）。但是，如果我们能在平台层面上实现 mTLS，我们就能一次性支付成本，而不是为每个服务或每个用户支付。\n综上所述，mTLS 非常适用于确保微服务之间的通信安全。但是有一个问题。\n实施 TLS 的难点：证书管理 到目前为止，我们已经为 mTLS 描绘了一幅美好的图景。客户端和服务器愉快地相互认证，然后它们之间的通信就安全了。在实践中，阻碍 mTLS 工作的最大挑战是证书管理。\nTLS 中的认证是通过 公钥密码学和公钥基础设施（PKI）进行的 。这两者本身就是一个巨大的话题，在这篇文章中我们不会去讨论这些细节。但简而言之，它们涉及大量的证书。\nTLS 认证基于 X.509 证书。X.509 证书中包含身份和公钥。公钥有一个相应的私钥，它不是证书的一部分。TLS 认证分两步，第一步是向对方展示你的证书，然后用私钥来证明证书中包含的身份属于你（公钥密码学的神奇之处在于，任何复制证书的人都无法进行这种证明，因为他们没有私钥。因此，你可以非常自由地使用证书，包括通过明文渠道发送证书或将其存储在公开场合）。\nX.509 证书是由一个 证书授权机构（Certificate Authority，简称 CA） 签署，其中包括受 CA 信任该的身份。证书用于 TLS 认证的第二步：如果有人向你展示他们的身份并证明他们拥有该身份，你现在必须决定是否信任该身份。TLS 在这里使用了一个简单的规则：如果证书是由 CA 签署的，且你信任该 CA，那么你就应该信任该身份。如何验证 CA 对证书的签名？通过使用该 CA 本身的 X.509 证书。怎么知道是否应该信任该 CA？嗯，这个就与 TLS 协议本身无关，你会被外界告知应该信任它 4。\nCA 也签发证书。要获得证书，你首先要创建公钥和私钥对。你保留私钥，嗯，私钥 —— 千万不要在网络上发送私钥 —— 你向 CA 发送一个包含公钥和你身份的证书签名请求（Certificate Signing Request，简称 CSR）。如果 CA 批准了这个请求，它就会创建和签署证书，并把证书发送给你。\n所以，证书管理就是就成了证书创建和分发流程中的挑战。我们需要确保有一个 CA，每个服务都可以向其发送 CSR，而且 CA 可以把证书发送给服务。我们还需要确保 CA 的安全，没有人能够访问任何服务的私钥，而且每个服务都知道自己的身份，而且不能被改变。\n在 Kubernetes 这样的环境中，服务实际上是一组不断变化的副本，可以随时创建或销毁，每个副本都需要自己的证书，这使得证书分发的挑战更加严峻。\n而且，由于在实践中，减少证书暴露损失（即当有人未经授权获得秘钥时）的最好方法是证书轮换：缩短证书的寿命，在证书过期前重新颁发。这意味着我们需要每隔 n 小时为每个副本重复整个证书请求和签名的流程。\n如果我们想在多个集群之间扩展安全通信，需要一种方法来确保在一个集群中产生的身份可以被其他集群所使用，而且如果某个集群被破坏，我们可以禁用该集群而不禁用其他集群，这就进一步增加了证书管理的复杂性，因为这将产生更多的证书。\n总之，实施 mTLS 涉及到管理大量的证书，消耗大量的时间。这一挑战的复杂性令人生畏。但尽管如此，mTLS 在 Kubernetes 的世界里已经看到了一些复兴的趋势。这是因为有一门技术使 mTLS 变得可行：服务网格。\nKubernetes、mTLS 和服务网格 服务网格是为集群开启 mTLS 的一个绝佳的机制。它不仅可以处理证书管理的挑战，还可以处理建立和接收 TLS 连接本身。它使得为集群添加 mTLS 成为一个零配置的操作：当你在 Kubernetes 集群上安装服务网格的时候，网格化的 pod 之间的所有通信都自动被 mTLS 化。对于像 mTLS 这样复杂的东西来说，这是很不可思议的。\n这一切之所以能够实现，是因为 Kubernetes 使一些本来非常复杂的事情，如 sidecar 模式，变得简单易行。得益于 Kubernetes，服务网格可以：\n 透明地将一个 sidecar 代理注入到每个应用程序的 pod 中，并通过该代理路由所有进出 pod 的 TCP 通信。 将一个内部 CA 作为其控制平面的一部分，签发 TLS 证书，并将该 CA 的证书安全地分配给所有代理。 使用这个 CA 向每个代理发放短期的证书，与 pod 的 Kubernetes ServiceAccount 身份相联系。 每隔 N 小时重新签发这些证书。 让每个代理对所有使用这些证书的 pod 的连接执行 mTLS，确保客户端和服务器双方都有有效的身份。 在连接进入应用程序之前，使用这些身份应用授权策略。  当然，这只是一种简化的描述。例如，Linkerd 实际上使用了两级 CA，一个在集群层面，一个在全局层面，以便允许跨集群通信。Linkerd 可以使用多个信任根，所以你也可以轮流使用 CA。\n常见问题：mTLS 实际上能保护什么？ 事实上，mTLS 只能用于防止特定的攻击：未经授权的网络访问。阻止入侵者嗅探网络请求中的内容，阻止冒充服务进行访问。\n但是有很多东西是 mTLS 不能保护的，例如未经授权主机访问。如果黑客入侵进了主机，mTLS 保护就无济于事了：入侵者可以读取密匙，嗅探或欺骗连接，颠覆 CA 并造成破坏，或任何其他恶意活动。\n确保 Kubernetes 的安全并不容易，实际上 mTLS 只解决了 Kubernetes 的一小部分安全漏洞。\nmTLS 与 IPSec 或 Wireguard 等网络层加密相比怎么样？ 在 Kubernetes 中，一些 CNI 插件如 Calico 和 Cilium 可以通过 IPSec 或 Wireguard  …","date":1655348400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"c06dd15e10691d7e0901607eb080963f","permalink":"https://lib.jimmysong.io/blog/mtls-guide/","publishdate":"2022-06-16T11:00:00+08:00","relpermalink":"/blog/mtls-guide/","section":"blog","summary":"在 Kubernetes 中我们不能仅依靠网络层加密，还需要 mTLS 来对客户端和服务端进行双向的传输层认证。本文将聚焦于 TLS 的真实性，以及证书管理的难题，说明服务网格对于在 Kubernetes 中开启 mTLS 带来的便利。","tags":["mTLS","安全"],"title":"写给 Kubernetes 工程师的 mTLS 指南","type":"post"},{"authors":["Istio"],"categories":["Istio"],"content":"编者的话 Istio 1.14 版本增加了对 SPIRE 集成的支持，这篇文章将指导你如何在 Istio 中集成 SPIRE。\nSPIRE 是 SPIFFE 规范的一个生产就绪的实现，它可以执行节点和工作负载证明，以便安全地将加密身份发给在异构环境中运行的工作负载。通过与 Envoy 的 SDS API 集成，SPIRE 可以被配置为 Istio 工作负载的加密身份来源。Istio 可以检测到一个 UNIX 域套接字的存在，该套接字在定义的套接字路径上实现了 Envoy SDS API，允许 Envoy 直接从它那里进行通信和获取身份。\n这种与 SPIRE 的集成提供了灵活的认证选项，这是默认的 Istio 身份管理所不具备的，同时利用了 Istio 强大的服务管理。例如，SPIRE 的插件架构能够提供多样化的工作负载认证选项，超越 Istio 提供的 Kubernetes 命名空间和服务账户认证。SPIRE 的节点认证将认证扩展到工作负载运行的物理或虚拟硬件上。\n关于这种 SPIRE 与 Istio 集成的快速演示，请参阅通过 Envoy 的 SDS API 将 SPIRE 作为 CA 进行集成。\n请注意，这个集成需要 1.14 版本的 istioctl 和数据平面。\n该集成与 Istio 的升级兼容。\n安装 SPIRE 选项 1: 快速启动 Istio 提供了一个基本的安装示例，以快速启动和运行 SPIRE。\n$ kubectl apply -f samples/security/spire/spire-quickstart.yaml 这将把 SPIRE 部署到你的集群中，同时还有两个额外的组件：SPIFFE CSI 驱动 —— 用于与整个节点的其他 pod 共享 SPIRE Agent 的 UNIX 域套接字，以及 SPIRE Kubernetes 工作负载注册器，这是一个在 Kubernetes 内执行自动工作负载注册的促进器。参见安装 Istio 以配置 Istio 并与 SPIFFE CSI 驱动集成。\n选项 2：配置一个自定义的 SPIRE 安装 请参阅 SPIRE 的 Kubernetes 快速入门指南，将 SPIRE 部署到 Kubernetes 环境中。请参阅 SPIRE CA 集成先决条件，了解有关配置 SPIRE 以与 Istio 部署集成的更多信息。\nSPIRE CA 集成的先决条件 将 SPIRE 部署与 Istio 集成，配置 SPIRE：\n  访问 SPIRE 代理参考，配置 SPIRE 代理套接字路径，以匹配 Envoy SDS 定义的套接字路径。\nsocket_path = \u0026#34;/run/secrets/workload-spiffe-uds/socket\u0026#34;   通过部署 SPIFFE CSI 驱动，与节点内的 pod 共享 SPIRE 代理套接字。\n  参见安装 Istio 以配置 Istio 与 SPIFFE CSI 驱动集成。\n注意，你必须在将 Istio 安装到你的环境中之前部署 SPIRE，以便 Istio 可以检测到它是一个 CA。\n安装 Istio   下载 Istio 1.14 + 版本。\n  在将 SPIRE 部署到你的环境中，并验证所有的部署都处于 Ready 状态后，为 Ingress-gateway 以及 istio-proxy 安装 Istio 的定制补丁。\n$ istioctl install --skip-confirmation -f - \u0026lt;\u0026lt;EOFapiVersion:install.istio.io/v1alpha1kind:IstioOperatormetadata:namespace:istio-systemspec:profile:defaultmeshConfig:trustDomain:example.orgvalues:global:# This is used to customize the sidecar templatesidecarInjectorWebhook:templates:spire:|spec: containers: - name: istio-proxy volumeMounts: - name: workload-socket mountPath: /run/secrets/workload-spiffe-uds readOnly: true volumes: - name: workload-socket csi: driver: \u0026#34;csi.spiffe.io\u0026#34; components:ingressGateways:- name:istio-ingressgatewayenabled:truelabel:istio:ingressgatewayk8s:overlays:- apiVersion:apps/v1kind:Deploymentname:istio-ingressgatewaypatches:- path:spec.template.spec.volumes.[name:workload-socket]value:name:workload-socketcsi:driver:\u0026#34;csi.spiffe.io\u0026#34;- path:spec.template.spec.containers.[name:istio-proxy].volumeMounts.[name:workload-socket]value:name:workload-socketmountPath:\u0026#34;/run/secrets/workload-spiffe-uds\u0026#34;readOnly:trueEOF这将与 Ingress Gateway 和将被注入工作负载 pod 的 sidecars 共享 spiffe-csi-driver，允许它们访问 SPIRE Agent 的 UNIX 域套接字。\n  使用 sidecar 注入，将 istio-proxy 容器注入到网格内的 pod 中。关于如何将自定义的 spire 模板应用到 istio-proxy 中的信息，请参见自定义模板。这使得 CSI 驱动能够在 sidecar 上安装 UDS。\n检查 Ingress-gateway pod 状态。\n$ kubectl get pods -n istio-system NAME READY STATUS RESTARTS AGE istio-ingressgateway-5b45864fd4-lgrxs 0/1 Running 0 17s istiod-989f54d9c-sg7sn 1/1 Running 0 23s   只有在 SPIRE 服务器上为它们创建了相应的注册条目时，数据平面容器才会到达 Ready。然后，Envoy 将能够从 SPIRE 获取加密身份。请参阅注册工作负载 ，为你的网格中的服务注册条目。\n注册工作负载 本节介绍在 SPIRE 服务器中注册工作负载的可用选项。\n选项 1：使用 SPIRE 工作负载注册器自动登记 通过将 SPIRE Kubernetes Workload Registrar 与 SPIRE 服务器一起部署，每创建一个新的 pod，就会自动注册新的条目。\n请参阅” 验证身份是否为工作负载创建 “，以检查已发布的身份。\n请注意，在快速启动部分使用了 SPIRE工作负载注册器。\n选项 2：手动注册 为了提高工作负载证明的安全稳健性，SPIRE 能够根据不同的参数，针对一组选择器的值进行验证。如果你按照快速启动安装 SPIRE，则跳过这些步骤，因为它使用自动注册。\n  为 Ingress Gateway 生成一个条目，其中有一组选择器，如 pod 名称和 pod UID：\n$ INGRESS_POD=$(kubectl get pod -l istio=ingressgateway -n istio-system -o jsonpath=\u0026#34;{.items[0].metadata.name}\u0026#34; ) $ INGRESS_POD_UID=$(kubectl get pods -n istio-system $INGRESS_POD -o jsonpath=\u0026#39;{.metadata.uid}\u0026#39;)   获取 spire-server pod：\n$ SPIRE_SERVER_POD=$(kubectl get pod -l app=spire-server -n spire -o jsonpath=\u0026#34;{.items[0].metadata.name}\u0026#34; )   为节点上运行的 SPIRE 代理注册一个条目。\n$ kubectl exec -n spire $SPIRE_SERVER_POD -- \\/opt/spire/bin/spire-server entry create \\-spiffeID spiffe://example.org/ns/spire/sa/spire-agent \\-selector k8s_psat:cluster:demo-cluster \\-selector k8s_psat:agent_ns:spire \\-selector k8s_psat:agent_sa:spire-agent \\-node -socketPath /run/spire/sockets/server.sockEntry ID :d38c88d0-7d7a-4957-933c-361a0a3b039cSPIFFE ID :spiffe://example.org/ns/spire/sa/spire-agentParent ID :spiffe://example.org/spire/serverRevision :0TTL :defaultSelector :k8s_psat:agent_ns:spireSelector :k8s_psat:agent_sa:spire-agentSelector :k8s_psat:cluster:demo-cluster  为 Ingress-gateway pod 注册一个条目。\n$ kubectl exec -n spire $SPIRE_SERVER_POD -- \\ /opt/spire/bin/spire-server entry create \\  -spiffeID spiffe://example.org/ns/istio-system/sa/istio-ingressgateway-service-account \\  -parentID spiffe://example.org/ns/spire/sa/spire-agent \\  -selector k8s:sa:istio-ingressgateway-service-account \\  -selector k8s:ns:istio-system \\  -selector k8s:pod-uid:$INGRESS_POD_UID \\  -dns $INGRESS_POD \\  -dns istio-ingressgateway.istio-system.svc \\  -socketPath /run/spire/sockets/server.sock Entry ID : 6f2fe370-5261-4361-ac36-10aae8d91ff7 SPIFFE ID : spiffe://example.org/ns/istio-system/sa/istio-ingressgateway-service-account Parent ID : spiffe://example.org/ns/spire/sa/spire-agent Revision : 0 TTL : default Selector : k8s:ns:istio-system Selector : …","date":1654484400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"57fcfb35b46d6eddc60802ec12c1bb95","permalink":"https://lib.jimmysong.io/blog/istio-spire-integration/","publishdate":"2022-06-06T11:00:00+08:00","relpermalink":"/blog/istio-spire-integration/","section":"blog","summary":"Istio 1.14 版本增加了对 SPIRE 集成的支持，这篇文章将指导你如何在 Istio 中集成 SPIRE。","tags":["Istio","SPIRE"],"title":"如何在 Istio 中集成 SPIRE","type":"post"},{"authors":null,"categories":null,"content":"《深入理解 Istio —— 云原生服务网格进阶实战》，云原生社区著，电子工业出版社出版，2022 年 6 月。\n  《深入理解 Istio —— 云原生服务网格进阶实战》  以社区之名成就开源 2018 年 5 月，在蚂蚁金服的支持下，ServiceMesher 社区成立。随后，国内刮起了服务网格的旋风，由社区领导的 Istio 官方文档翻译工作也进入白热化阶段。\n随着时间的推移，我感受到系统介绍 Istio 的中文资料匮乏，于是在 2018 年 9 月开始构思写一本关于 Istio 的图书，并在 GitHub 上发起了 Istio Handbook 的开源电子书项目。几个月后，随着服务网格技术的推广及 ServiceMesher 社区规模的扩大，我在社区的线上线下活动中结识了很多同样热衷于 Istio 和服务网格技术的朋友。我们一致决定，一起写一本 Istio 的开源电子书，将社区积累的宝贵文章和经验集结成系统的文字，分享给广大开发者。\n2019 年 3 月，在社区管理委员会的组织下，几十位成员自愿参与并开始共同撰写此书。2020 年 5 月，为了更好地推广云原生技术，丰富社区分享的技术内容，我们成立了云原生社区，并将原有的 ServiceMesher 社区纳入其中，社区运营的内容也从服务网格技术扩展到更加全面的云原生技术。\n2020 年 10 月，这本书主要的内容贡献者组成了编委会，成员分别有我、马若飞、王佰平、王炜、罗广明、赵化冰、钟华和郭旭东。我们在出版社的指导与帮助下，对本书进行了后续的版本升级、完善、优化等工作。经过反复的迭代，这本《深入理解 Isito：云原生服务网格进阶实战》终于和大家见面了。\n  《深入理解 Istio —— 云原生服务网格进阶实战》封面  关于本书 Istio 在 1.5 版本后有了重大的架构变化，同时引入或改进了多项功能，例如，引入了智能 DNS 代理、新的资源对象，改进了对虚拟机的支持等。\n本书以 Istio 新版本为基础编写而成，在持续追踪 Istio 社区最新动向的基础上，力求为读者提 供最新、最全面的内容。另外，本书的多位作者都是一线的开发或运维工程师，具有丰富的 Istio 实战经验， 为本书提供了翔实、宝贵的参考案例。\n","date":1654041600,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"850e895c9c8df5e6dbbeaf175999df4b","permalink":"https://lib.jimmysong.io/book/istio-in-depth/","publishdate":"2022-06-01T00:00:00Z","relpermalink":"/book/istio-in-depth/","section":"book","summary":"云原生社区著","tags":["出版物"],"title":"深入理解 Istio —— 云原生服务网格进阶实战","type":"publication"},{"authors":null,"categories":null,"content":"关于本书 本书译自 O’Reilly Report “What is eBPF”，作者 Liz Rice，英文原版可以在 O’Reilly 网站上获取，英文版发布于 2022 年 4 月，中文版发布于 2022 年 6 月。\n","date":1654041600,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"b72896688941bd8eda190a6e0b6da39c","permalink":"https://lib.jimmysong.io/book/what-is-ebpf/","publishdate":"2022-06-01T00:00:00Z","relpermalink":"/book/what-is-ebpf/","section":"book","summary":"新一代网络、安全和可观测性工具介绍","tags":["翻译电子书"],"title":"什么是 eBPF","type":"publication"},{"authors":["James Blackwood-Sewell"],"categories":["可观测性"],"content":"前言 OpenTelemetry 追踪包含了理解分布式系统和排除故障的信息宝库 —— 但你的服务必须首先被指标化，以发射 OpenTelemetry 追踪来实现这一价值。然后，这些追踪信息需要被发送到一个可观察的后端，使你能够获得关于这些数据的任意问题的答案。可观测性是一个分析问题。\n本周早些时候，我们部分解决了这个问题，宣布在 Promscale 中普遍提供 OpenTelemetry 追踪支持，将由 SQL 驱动的可观测性带给所有开发者。随着对分析语言 ——SQL 的全面支持，我们解决了分析的问题。但我们仍然需要解决第一部分的问题：测量。\n为了让你的服务发出追踪数据，你必须手动添加 OpenTelemetry 测量工具到代码中。而且你必须针对所有服务和你使用的所有框架来做，否则你将无法看到每个请求的执行情况。你还需要部署 OpenTelemetry 收集器来接收所有新的追踪，处理它们，批处理它们，并最终将它们发送到你的可观测性后端。这需要花费大量的时间和精力。\n如果你不需要做所有这些手工工作，并且可以在几分钟内而不是几小时甚至几天内启动和运行呢？如果你还能建立一个完整的可观测性技术栈并自动连接所有的组件呢？如果我告诉你，你可以用一个命令完成所有这些工作呢？\n我不是疯子。我只是一个 Tobs 用户😎。\nTobs 是 Kubernetes 的可观测性技术栈，是一个可以用来在几分钟内在 Kubernetes 集群中部署一个完整的可观测性技术栈的工具。该栈包括 OpenTelemetry Operator、OpenTelemetry Collector、Promscale 和 Grafana。它还部署了其他几个工具，如 Prometheus，以收集 Kubernetes 集群的指标，并将其发送到 Promscale。在我们的最新版本中，tobs 包括支持通过 OpenTelemetry Operator 用 OpenTelemetry 追踪自动检测你的 Python、Java 和 Node.js 服务。\n是的，你没看错：自动！你不需要改变服务中的任何一行代码，就可以让它们被检测出来。锦上添花的是什么？你可以通过执行 helm 命令来部署一切。\n有了 tobs，你可以安装你的可观测性技术栈，只需几步就能搞定你的 OpenTelemetry 指标化的第一层。告别繁琐的配置工作，因为你的框架会自己检测。\n如果你想了解如何做到这一点，请继续阅读本博文。首先，我们将解释一切是如何运作的，剖析 OpenTelemetry Operator 在内部的真正作用。接下来，我们将通过一个例子演示如何将其直接付诸实践。\n 我们将通过 tobs 在我们的 Kubernetes 集群中安装一个完整的可观测性技术栈。 我们将部署一个云原生 Python 应用程序。 我们将检查我们的应用程序是如何被 OpenTelemetry 追踪器自动检测到的，这要归功于 tobs 和 OpenTelemetry Operator 所做的魔术🪄。  OpenTelemetry Operator OpenTelemetry 是一个开源的框架，可以捕获、转换和路由所有类型的信号（追踪、日志和指标）。在大多数情况下，你会使用 OpenTelemetry SDK 来在你的应用程序代码中生成这些信号。但是，在某些情况下，OpenTelemetry 可以自动检测你的代码 —— 也就是说，当你的应用框架被支持，并且你使用的语言是 OpenTelemetry 可以注入代码的。在这种情况下，你的系统将开始产生遥测，而不需要手动工作。\n要了解 OpenTelemetry 是如何做到这一点的，我们首先需要熟悉 OpenTelemetry Operator。OpenTelemetry Operator 是一个实现 Kubernetes Operator 模式的应用程序，与 Kubernetes 集群中的两个 CustomResourceDefinitions（CRD）互动。\n   图示说明 OpenTelemetry Opertator 如何与 Kubernetes互动  基于 CustomResourceDefinitions（CRD）实例的变化，Operator 为我们管理以下两点：\n 创建和删除 OpenTelemetry Collector 实例 将 OpenTelemetry 自动测量所需的库和二进制文件直接注入到你的 pod 中  让我们更详细地解读这两项任务。\n管理 OpenTelemetry Collector OpenTelemetry Operator 的首要任务是部署 OpenTelemetry Collector 实例。这些实例将被用来把信号从源头（你的工作负载和 Kubernetes 本身）路由到它们的目标（支持 OpenTelemetry 协议的存储系统或集群外的另一个采集器）。\n采集器可以以三种不同的方式部署：\n 作为 Kubernetes Deployment：这是默认选项，它允许采集器根据需要在节点之间移动，支持向上和向下扩展。 作为 Kubernetes Daemonset：这个选项将在每个节点上部署一个采集器，当你想确保你的信号在没有任何网络开销的情况下被处理时，它可能很有用。 作为一个 Sidecar：被注入到任何新的注释的 pod 中（使用 sidecar.opentelemetry.io/inject: true）。当采集器需要一个 pod 的特定配置时，这可能是很好的（例如，也许它需要一些专门的转换）。  如果你愿意，你可以混合和匹配这些收集器模式。例如，你可以设置一个 sidecar，为部署中的 pod 做一些转换，然后将它们发送到一个全局收集器，与你的其他工作负载共享。\n定义这些收集器实例的配置在收集器 CRD（opentelemetrycollectors.opentelemetry.io）中进行建模。允许多个实例来实现更复杂的模式。部署类型是通过 mode 设置来选择的，伴随着一个原始的配置字符串，它被逐字传递给控制器，并作为配置加载。下面是使用 Deployment 模式创建 Operator 的 CRD 的例子。\napiVersion:opentelemetry.io/v1alpha1kind:OpenTelemetryCollectormetadata:name:tobs-tobs-opentelemetrynamespace:defaultSpec:mode:deploymentconfig:|receivers: jaeger: protocols: grpc: thrift_http: otlp: protocols: grpc: http: exporters: logging: otlp: endpoint: \u0026#34;tobs-promscale-connector.default.svc:9202\u0026#34; compression: none tls: insecure: true prometheusremotewrite: endpoint: \u0026#34;tobs-promscale-connector.default.svc:9201/write\u0026#34; tls: insecure: true processors: batch: service: pipelines: traces: receivers: [jaeger, otlp] exporters: [logging, otlp] processors: [batch] metrics: receivers: [otlp] processors: [batch] exporters: [prometheusremotewrite] 正如我们在后面的例子中看到的，当你使用 tobs 时，你不需要担心所有这些配置细节。tobs 的好处之一是它会为你安装一个采集器，它将直接把数据发送到本地的 Promscale 实例。\n在 Kubernetes 中添加 OpenTelemetry 自动监测系统 Operator 的第二个关注点是将 OpenTelemetry 自动测量所需的库和二进制文件注入到 pod 中。要做到这一点，这些 pod 需要容纳 Java、Python 或 Node.js 应用程序（OpenTelemetry 将来会支持更多语言）。\n用于部署这些 pod 的 Kubernetes 清单文件必须包括一个注释，以指示 OpenTelemetry Operator 对其进行检测。\ninstrumentation.opentelemetry.io/inject-\u0026lt;language\u0026gt;: \u0026#34;true\u0026#34; 其中 language 可以是 python、java 或 nodejs。\n当注解的 pod 启动时，会创建一个 init 容器，注入所需的代码并改变 pod 运行代码的方式，使用正确的 OpenTelemetry 自动探测方法。实际上，这意味着在使用 Kubernetes 时，不需要修改任何代码就可以获得自动监测的好处。该配置还定义了 OpenTelemetry Collector 端点，这些追踪将被发送到该端点，传播的信息类型，以及我们用来采样追踪的方法（如果有的话）(关于 CRD 的全部细节，请看文档）。\n为 Python、Java 和 Node.js 应用程序提供自动测量的自定义资源的例子是这样的。\napiVersion:opentelemetry.io/v1alpha1kind:Instrumentationmetadata:name:tobs-auto-instrumentationnamespace:defaultspec:exporter:endpoint:http://tobs-opentelemetry-collector.default.svc:4318 propagators:- tracecontext- baggage- b3sampler:argument:\u0026#34;0.25\u0026#34;type:parentbased_traceidratio再一次，如果你使用 tobs，你将不需要自己创建这些自定义资源。Tobs 将确保集群被自动配置成对任何有注释的 pod 进行检测，而不需要你做任何操作。你所需要做的就是在你想收集追踪的 pod 中添加以下注释之一。\ninstrumentation.opentelemetry.io/inject-java: \u0026#34;true\u0026#34; instrumentation.opentelemetry.io/inject-nodejs: \u0026#34;true\u0026#34; instrumentation.opentelemetry.io/inject-python:\u0026#34;true\u0026#34; 让我们通过一个例子看看这在实践中是如何运作的。\n使用 OpenTelemetry Operator 和 Tobs 在本节中，我们将使用我们的微服务演示应用程序，它由一个过度工程化的密码生成器应用程序组成。在 repo 中，你可以找到一个已测量的版本和一个未测量的版本，这就是我们在这个例子中要使用的版本。\n要运行这个，你首先需要一个 Kubernetes 集群，安装了 cert-manager，配置了通过 kubectl（至少需要 1.21.0 版本）的访问，并安装了 helm。为了部署和运行所有不同的组件，你将需要在你的 Kubernetes 集群中提供大约 4 核 CPU 和 8GB 的内存。\n如果你的集群中没有 cert-manager，你将需要使用这个命令来安装它。\nkubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.8.0/cert-manager.yaml 准备好后，让我们使用 Timescale Helm Chart 来安装 tobs。在命令提示符下运行以下命令。\nhelm …","date":1653879600,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"7fd7c6b0b07d595abfadeee75bb9e1ec","permalink":"https://lib.jimmysong.io/blog/generate-and-store-opentelemetry-traces-automatically/","publishdate":"2022-05-30T11:00:00+08:00","relpermalink":"/blog/generate-and-store-opentelemetry-traces-automatically/","section":"blog","summary":"首先，我们将解释一下如何在 Kubernetes 自动生成和存储 OpenTelemetry 追踪，剖析 OpenTelemetry Operator 在内部的真正作用。接下来，我们将通过一个例子演示如何将其直接付诸实践。","tags":["tobs","可观测性","Kubernetes"],"title":"一键开启 Kubernetes 可观测性——如何自动生成和存储 OpenTelemetry 追踪","type":"post"},{"authors":null,"categories":null,"content":"《云原生导览》一书是随本站同步开始建设的，起始于 2022 年 5 月，目前仍在编写中。本书将 Jimmy 原有的众多分散在不同的 Handbook 中的关于云原生核心技术的精华篇章整理归纳，着重于概念体系的建设，帮助你一站式浏览云原生技术的全貌。\n","date":1653523200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"f328dac5c4ee20e6355d73ad9242ec4c","permalink":"https://lib.jimmysong.io/book/cloud-native-handbook/","publishdate":"2022-05-26T00:00:00Z","relpermalink":"/book/cloud-native-handbook/","section":"book","summary":"一站式了解云原生技术体系","tags":["Handbook系列"],"title":"云原生导览","type":"publication"},{"authors":["Yury Niño Roa"],"categories":["可观测性"],"content":"主要收获   对于现代软件系统来说，可观测性不是关于数学方程。它是关于人类如何与复杂的系统互动并试图理解它们。\n  混沌工程利用了可观测性，因为它可以检测到系统稳定状态的偏差。混沌工程借助可观测性可以发现和克服系统的弱点。\n  可观测性依赖于系统所发出的信号，这些信号提供了关于系统行为的原始数据。然而，可观测性不仅受限于这些信号的质量，还受限于这些信号的可视化和解释的方式。\n  考虑到混沌工程、可观测性和可视化涉及到人类自我的解释，仪表盘的设计者可能会对这些解释产生偏差，这是一个事实。在这个意义上，视觉隐喻并不能保证我们以正确的方式解释这些数据。\n  基于视觉隐喻的仪表盘可以提供比经典的可视化更有用的数据。然而，这两种策略都很容易产生偏差；例如，在一项研究中，大多数参与者都注意到，由于显示了糟糕的柱状图和线状图，没有在图中显示出重要的分界点，因此整体结果是有偏差的。\n  自从 Netflix、Slack 和 Linkedin 等领先的技术公司采用混沌工程来抵御生产中的意外中断后，这门学科在近来已经成为主流。在这条道路上，可观测性发挥了关键作用，为工程师们带来了数据和监控的力量，他们现在有了了解自己系统的策略，确定当某些东西发生故障时它们将如何表现，并增加复原力和可靠性。\n混沌工程和可观测性是两个密切相关的学科。根据 Russ Miles 的说法，\u0026#34;可观测性原则将系统变成了可检查和可调试的案发现场，而混沌工程鼓励并利用可观测性，因为它试图帮助预先发现并克服系统的弱点 \u0026#34;。混沌工程鼓励并要求可观测性，因为要自信地执行混沌实验，可观测性必须检测系统何时正常，以及在执行方法实验时如何偏离该稳定状态。见图 1 中的说明。\n   图 1. 混沌工程和可观测性  学术界和科技界都做出了巨大的努力，为实践混沌工程和可观测性提供了工具。然而，指标的可视化和视觉策略的适当选择仍然是有限的。本文引入了一个新的角色：视觉隐喻。具体来说，它提供了混沌工程和可观测性的概念基础，介绍了市场上可用的可视化技术的现状，并展示了树状图、仪表图、地理图和城市隐喻是如何丰富观察混沌的视觉策略的。\n混沌工程和可观测性的基础 关于混沌工程：混沌、弹性和可靠性是关键的概念，而关于可观测性，当人类想要观察他们的系统时，监控、度量和仪表盘是至关重要的。因此，在深入研究混沌工程和可观测性之间的关系之前，明确这些定义很重要。\n混沌工程根据混沌原理定义为一门在系统上进行实验的学科，以建立对系统在生产中承受动荡条件能力的信心。为了具体解决大规模分布式系统的不确定性，混沌工程提供了一种基于实验的方法，包括四个步骤：第一个步骤包括定义稳定状态，这是系统的一个可测量的输出，表示正常行为。第二步是与假设相关的，它提出了一个改变稳态的后果的句子。有了这个假设，就该引入现实世界的事件，如服务器崩溃或硬盘故障，以证实或反驳这个假设。最后，目标是建立控制组和实验组之间稳定状态的差异分析。\n可观测性是指能够完全理解一个系统。在控制理论中，它被定义为衡量一个系统的内部状态可以从其外部输出的知识中推断出来的程度。特别是在软件工程中，可观测性可以被描述为提出适当的问题、提供正确的答案以及用收集到的数据建立知识的艺术。\n监控与可观测性是不同的，理解两者的区别很重要。监控是关于收集、处理、汇总和显示系统的实时定量数据；而可观测性是关于处理和分析这些数据，让团队主动理解和调试系统的行为。对于现代软件系统来说，可观测性不是关于数学公式。它是关于人们如何与复杂的系统互动并试图理解它们。\n在这个意义上，监控涉及到通过数字读取系统发出的信号，这些数字被命名为度量。指标是一个单一的数字，可以选择附加标签进行分组和搜索，比如查询次数和类型、错误次数和类型、处理时间或服务器寿命。这些数值在仪表盘中被可视化，仪表盘是提供服务核心指标的摘要视图的应用程序。\n传统的仪表盘是建立在折线图、饼图或柱状图上的。考虑到可观测性取决于系统发出的信号以及这些信号被可视化和解释的质量，提供最好的工具和设计是很重要的。如果颜色、图例和比例使用不当，一些可视化可能会对操作者造成限制和困惑。下一节提供了监控和可观测性的技术状况，并更详细地描述了其中的一些限制。\n监控和可观测性 监控和可观测性已经成为工程团队和一般现代数字企业最基本的能力之一，他们希望在他们的解决方案中提供卓越。由于监控和观察系统有很多原因，谷歌记录了四个黄金信号或指标，它们定义了系统健康的含义，是可观测性和监控平台现状的基础。这四个指标描述如下。\n延迟（Latency）是指一个服务为一个请求提供服务所需的时间。它包括由于与数据库或其他关键后端失去连接而触发的 HTTP 500 错误，这些错误可能不会很快得到服务。延迟是一个基本指标，因为慢的错误甚至比快的错误更糟糕。\n流量（Traffic）是衡量对系统的需求有多大。它决定了系统在某一特定时间内从用户或通过服务运行的事务中承受多大的压力。以网络服务为例，这种测量通常是每秒的 HTTP 请求。通过监控应用程序或服务中的真实用户互动和流量，工程团队可以看到系统如何支持需求的变化，以及他们应该如何扩展资源以满足需求。\n错误（Error）与请求失败的比率有关，无论是显性的还是隐性的。根据系统和发生故障的组件，监控错误情况可能会有很大的不同。这就是为什么工程团队需要监控整个系统的错误发生率，但也需要监控单个服务层面的错误发生率的原因。同样重要的是，要优先考虑哪些错误是关键的，哪些错误是不太危险的。\n最后，饱和度（Saturation）是系统对资源利用的信号，如内存、I/O 或 CPU。考虑到许多系统在达到 100% 的利用率之前就会出现性能下降的情况，拥有一个饱和度目标是非常重要的。它允许我们回答这样的问题：服务还有多少容量？什么水平的饱和度能保证客户的服务性能和可用性？\n用于监控的传统可视化方法 现在，上一节所述的四个黄金信号是用传统方法监控的，如折线图、柱状图或饼状图。\n如图 2 所示，折线图是将系统的四个黄金信号的行为在时间上可视化的最常用策略。\n   图 2. 虚构项目中的折线图  线形图在颜色、图例、轴和系列的标题方面提出了不同的挑战，因为变量会聚、交叉，而且通常会纠缠在一起。如果仪表盘的创建者没有使用适当的视觉资产，这种类型的图形可能会变成最令人困惑的图表之一。\n另一个常见的图表是柱状图，它是用高度或长度与所代表的数值成正比的矩形条来表示分类数据。如图 3 所示，一些云供应商用它们来表示日志的分类数据。\n   图 3. 一个虚构的项目中的柱状图  最后，尽管较少使用，饼图是表示和比较数据分布中的比例的一种简单方法。当一个比例占主导地位——一半或四分之三时，它们是最有效的。超过几种颜色的楔形图会在楔形图之间产生同一性，使其难以比较数值。\n考虑到这些局限性，下一节将介绍一种不同的方式来可视化这四个黄金指标。由于本文是关于混沌工程的，这种技术是在报告事件的情况下进行分析的。\n视觉隐喻作为视觉化混沌的一种建议 为了克服前面提到的局限性，本文提出了一种新的策略，将生产中的混乱现象可视化。这个建议是基于其他科学领域的一个概念：视觉隐喻。视觉隐喻是一种策略，将一个应用领域的概念和对象映射到一个相似性和类比的系统中。计算机隐喻是交互式视觉对象和模型对象之间同化的基本思想。它的作用是促进对对象的语义的更好理解。一个熟悉的例子可以是在跑车的图片前使用一只豹子，暗示该产品具有速度、力量和耐力等可比性。\n一些例子包括：地图、城市和几何场景，如图 3 所示。该图显示了城市隐喻，这是一种用于可视化程序代码属性的流行方法。许多项目都采用了这种隐喻来可视化软件库的属性，比如说。现有的研究已经被用来用包来映射街区，用建筑来映射类。\n   图 4. 一个虚构项目中的城市隐喻，来自这里  在这种情况下，这个隐喻把类表示为建筑物，把包表示为建筑物所处的邻域。建筑物的每个边缘都被用来映射类的属性。\n提出一个将事件可视化的实验 为了确定参与运营活动的工程团队的看法，我们对他们中的 28 人进行了关于传统仪表盘和视觉隐喻的调查。具体来说，他们被问及使用经典仪表盘和视觉隐喻对四个黄金指标（错误、延迟、流量和饱和度）进行可视化的事件。\n这项研究由关于一个事件的具体问题组成，其中提供了两种可视化：一种是传统的图表，另一种是视觉隐喻。对于每一种情况，都分析了每种可视化的价值。在接下来的段落中，将介绍每个问题和分析。\n关于人口统计学，共有 28 名参与者，其背景分布在后端、前端和全栈工程师、软件架构师、数据工程师和网站可靠性工程师。如图 5 所示，参与人数最多的是后端开发工程师。\n   图 5. 人口统计学数据  第一个问题是关于 饱和信号。基本上，使用了两个仪表盘——一个折线图和一个城市隐喻来询问五个微服务的状态：ms_authentication、ms_patients、ms_payments、ms_medications 和 ms_appointments。这些微服务是一个虚构的医疗系统的一部分。\n具体来说，问题是：使用传统的仪表板（见图 6）和视觉隐喻（见图 7），哪个微服务受到影响？正确答案是 ms_authentication。\n   图 6. 传统的折线图     图 7. 可视化的城市比喻  如图 8 所示，当他们使用视觉隐喻时，一些参与者的答案改变了，选择了正确的答案。\n   图 8. 使用传统图表与视觉隐喻的参与者的回答  所有参与者都认为，受 CPU 高利用率影响的微服务是认证。在这种情况下，视觉隐喻比传统的图表更有用，因为图表线很混乱，而且颜色、形状和大小都不好，改变了参与者的看法。\n关于错误信号，我们用经典的柱状图和树状图来要求参与者计算每个微服务的平均错误，如图 9 所示。\n   图 9. 用于可视化误差的传统柱状图     图 10. 用于可视化错误的可视化树状图隐喻  正确的答案是 ms_appointments，虽然有些参与者没有选择它，但当他们使用视觉隐喻时，许多人改变了他们的答案。图 11 说明了这一点。\n   图 11. 参与者使用传统图表与视觉隐喻来可视化错误的答案  关于流量信号，我们用一个经典的柱状图和一个地理中心的隐喻来询问参与者哪个第三方服务的流量更大。在这种情况下，原有的微服务和新的四个第三方服务：srv_ldap、srv_goverment、srv_assurance 和 srv_authentication 之间的互动被分析了。图 12 用柱状图显示了这种整合，图 13 用地理中心的隐喻显示了相同的流量值。在这个比喻中，圆圈代表服务和微服务，线条连接它们之间的关系。\n   图 12. 用于可视化微服务和第三方服务之间流量的传统柱状图     图 13. 用于可视化微服务和第三方服务之间流量的可视化地理中心隐喻  尽管有线条和大小来代表微服务和第三方服务之间的连接和流量负载，但这个比喻对参与者来说是混乱的。圆圈的大小可能与 srv_ldap 的最小百分比有关，而 srv_ldap 是正确的答案，它在饼中由绿色部分表示（见图 14）。\n   图 14. 参与者使用传统图表与视觉隐喻对微服务和第三方服务之间的流量进行可视化的答案。  最后，我们用柱状图的视觉化和仪表的比喻来分析延迟信号。图 15 和图 16 分别说明了这两种可视化方式。\n   图 15. 用于可视化延迟信号的传统柱状图。     图 16. 用于可视化微服务的延迟的可视化仪表隐喻。  对于这种情况，这个隐喻肯定没有为参与者提供价值，因为正确的答案是 ms_patients，这在图 17 中有说明。\n   图 17. 参与者使用传统图表与视觉隐喻来可视化微服务和第三方服务之间的延迟的答案。  引入视觉隐喻的结论 混沌的可视化，特别是生产事故的可视化，给专注于可观测性的工业和学术界带来了一些挑战。正如我们在这篇文章中所展示的，由于混沌工程、可观测性和可视化涉及到人与机器的互动，解释中的偏差是一个持续的风险。通过一项研究，28 位工程师回答了与经典仪表盘和视觉隐喻 …","date":1653357600,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"b1980d7c12dc9ba9d46d9a93a6d65f70","permalink":"https://lib.jimmysong.io/blog/chaos-engineering-observability-visual-metaphors/","publishdate":"2022-05-24T10:00:00+08:00","relpermalink":"/blog/chaos-engineering-observability-visual-metaphors/","section":"blog","summary":"本文为可观测性引入了一个新的角色：视觉隐喻，并进行了一项研究，对比了视觉隐喻与传统表示的结果。","tags":["混沌工程","可观测性"],"title":"混沌工程和视觉隐喻的可观测性","type":"post"},{"authors":null,"categories":null,"content":"Envoy 基础教程，本手册梳理了 Envoy 基础知识，适用于初学者，帮你快速掌握 Envoy 代理。\n关于本书 Envoy 是一个开源的边缘和服务代理，专为云原生应用而设计。Envoy 与每个应用程序一起运行，通过提供网络相关的功能，如重试、超时、流量路由和镜像、TLS 终止等，以一种平台无关的方式抽象出网络。由于所有的网络流量都流经 Envoy 代理，因此很容易观察到流量和问题区域，调整性能，并准确定位延迟来源。\n本书为 Tetrate 出品的《Envoy 基础教程》的文字内容，其配套的 8 节实验及 19 个测试，请访问 Tetrate 学院。\n关于作者 宋净超（Jimmy Song），CNCF Ambassador，云原生社区创始人，个人网站 jimmysong.io。\n许可证 您可以使用署名 - 非商业性使用 - 相同方式共享 4.0 (CC BY-NC-SA 4.0) 协议共享。\n","date":1652832000,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"4a01e92eddc6da383ffd966fd08ce545","permalink":"https://lib.jimmysong.io/book/envoy-handbook/","publishdate":"2022-05-18T00:00:00Z","relpermalink":"/book/envoy-handbook/","section":"book","summary":"从零开始学习 Envoy 网络代理","tags":["Handbook系列"],"title":"Envoy 基础教程","type":"publication"},{"authors":null,"categories":null,"content":"关于本书 本书的主题包括：\n 服务网格概念解析 控制平面和数据平面的原理 Istio 架构详解 基于 Istio 的自定义扩展 迁移到 Istio 服务网格 构建云原生应用网络  书中部分内容来自 Tetrate 出品的 Istio 基础教程，请访问 Tetrate 学院，解锁全部教程及测试，获得 Tetrate 认证的 Istio 认证。\n致谢 感谢 ServiceMesher 及云原生社区先后负责翻译了 Envoy 及 Istio 官方文档，为本书的成书提供了大量参考资料。\n许可证 本书所有内容支持使用署名 - 非商业性使用 - 相同方式共享 4.0 (CC BY-NC-SA 4.0) 协议共享。\n参与本书 请参考 Istio 文档样式指南。\n","date":1652832000,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"cdeb6daaa562734b2afb9bbddb006311","permalink":"https://lib.jimmysong.io/book/istio-handbook/","publishdate":"2022-05-18T00:00:00Z","relpermalink":"/book/istio-handbook/","section":"book","summary":"云原生应用网络构建指南","tags":["Handbook系列"],"title":"Istio 基础教程","type":"publication"},{"authors":["Varun Talwar"],"categories":["Envoy"],"content":"前言 今天，Envoy 社区宣布了一个令人兴奋的新项目。 Envoy Gateway。该项目将行业领导者联合起来，精简由 Envoy 驱动的应用网关的好处。这种方法使 Envoy Gateway 能够立即为快速创新打下坚实的基础。该项目将提供一套服务来管理 Envoy 代理机群，通过易用性来推动采用，并通过定义明确的扩展机制来支持众多的用例。\n我们为什么要这样做？ Tetrate 是 Envoy Proxy 的第一贡献者（按提交量计算），也是 Envoy Gateway 指导小组的成员，其贡献者涵盖技术和管理领域。我们相信，我们强大的伙伴关系和在开源软件方面的深厚经验将有助于确保 Envoy Gateway 的成功。Tetrate 推动了 EG 计划，因为我们致力于上游项目，因为我们相信这将降低 Envoy Proxy 用户的进入门槛，也因为这与我们开发服务网格作为零信任架构基础的使命相一致。Tetrate 将大力投资建设 Envoy Gateway 的安全功能，包括支持 OAuth2 和 Let’s Encrypt 集成等 API 功能。\n对上游项目的承诺 Tetrate 从第一天起就站在服务网格的最前沿，始终相信上游项目和它们的社区。因此，我们一直在为 Istio 和 Envoy 的上游项目提供帮助和支持。我们看到不同的人在使用 Envoy，并创建他们自己的控制平面和 API 网关实现，导致碎片化，创新速度慢，功能差距大，以及缺乏对一个代码库的支持。由于我们与 Matt Klein 和 Envoy 社区长期以来关系密切，当我们提议将其纳入 Envoy 的标准化实现，并将其整合到一个官方的上游实现中时，我们得到了 Matt 和其他 CNCF 项目的强烈支持。我们一直在幕后与其他指导委员会成员（Ambassador Labs、Fidelity Investments、Project Contour 和 VMware 辛勤工作，以定义 Envoy Gateway。\n我们知道，艰苦的工作才刚刚开始，我们致力于这个项目以及 CNCF 内其他几个项目的长期成功。\n实现控制平面的标准化 在很短的时间内，Envoy 已经成为现代云原生应用的首选网络层。随着 Envoy 获得关注，大量的上游项目开始利用它来实现服务网格、入口、出口和 API 网关功能。这些项目中有许多能力重叠、功能差距、专有特性，或者缺乏社区多样性。这种支离破碎的状态是由于 Envoy 社区没有提供控制平面的实现而产生的副作用。\n因此，创新的速度降低了，企业被要求辨别利用 Envoy 作为其应用网络数据平面的最佳方法。现在，社区正在提供 Envoy Gateway，更多的用户可以享受 Envoy 的好处，而无需决定控制平面。Envoy Gateway 的目标是：\n “…… 通过支持众多入口和 L7/L4 流量路由使用案例的表达式、可扩展、面向角色的 API，降低采用障碍，吸引更多用户使用 Envoy；并为供应商建立增值产品提供共同基础，而无需重新设计基本交互。”\n 易用性和运营效率 Envoy Proxy 是由 xDS API 驱动的，这些 APIs 暴露了大量的功能，并被控制平面广泛采用。虽然这些 API 功能丰富，但对于用户来说，要快速学习并开始利用 Envoy 的功能是非常困难的。Envoy Gateway 将为用户抽象出这些复杂的功能，同时支持现有的运营和应用管理模式。\nEnvoy Gateway 将利用 Gateway API 来实现这些目标，而不是开发一个新的项目专用 API。Gateway API 是一个由 Kubernetes 网络特别兴趣小组管理的项目，正在迅速成为提供用户接口以管理应用网络基础设施和流量路由的首选方法。这个开源项目有一个丰富、多样的社区，有几个知名的实施方案。我们期待着作为社区的一部分开展工作，使 Envoy Gateway 成为业界首选的网关 API 实现。\n为什么这比传统的 API 网关更好？ 传统的代理不是轻量级的、开放的，也不是动态可编程的、类似 xDS 的 API，因此 Envoy 很适合成为当今动态后端的 API 网关 —— 尤其是在增加安全功能的情况下。我们设想将 Envoy 网关作为不断发展的 API 管理领域的一个关键组成部分。API 网关是 API 管理的核心组件，提供透明地策略执行和生成详细遥测数据的功能。这种遥测技术提供了强大的可观测性，为企业提供了更好的洞察力，以排除故障、维护和优化其 API。\n在我们看来，由于 Envoy 的设计、功能设置、安装基础和社区，它是业内最好的 API 网关。有了 Envoy Gateway，企业可以在将 Envoy 嵌入其 API 管理策略方面增加信心。\n无边界的零信任 当你的所有应用服务都在一个服务网格中运行时，实现零信任架构就不那么难了。然而，现实中不都是服务网格。服务在虚拟机上运行，在无代理容器中运行，作为无服务器函数运行，等等。Envoy Gateway 将突破这些运行时的界限，为跨异构环境的统一策略执行提供基础。\n这一基础的关键是 Envoy Gateway 的可扩展性，它提供了暴露 Envoy 和非 Envoy 安全功能的灵活性。这些扩展点将被用来提供实现零信任架构所需的功能，包括用户和应用认证、授权、加密和速率限制。Envoy Gateway 将很快成为寻求实现零信任架构的组织的一个关键组件。\n同样，Tetrate 致力于上游项目和它们的长期可行性。这一举措又一次证明了这一点，并表明上游的 Envoy 和 Istio 现在正成为构建服务网格的事实上的支柱。Envoy Gateway 将使服务网格成为主流，架构师们应该把网格看作是 ZTA 的基础。为了帮助架构师进行论证，我们最近出版了《服务网格手册》。我们很快就会发布一种带有上游 Envoy Gateway 和 Istio 的架构方法，可以看作是你的应用网络的基础。\n探索 Envoy Gateway 在 Tetrate，我们正在领导基于 Envoy Gateway 和 Istio 的零信任架构的定义，并将在后续博文中阐述设想的架构。如果你想和我们一起讨论架构，并了解更多关于如何为传统和云原生应用程序进行架构，请加入 tetrate-community Slack 频道。\n要了解更多关于 Tetrate 的信息，请访问 tetrate.io。\n","date":1652756400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"abbebe61974909337e0b32ac513f284e","permalink":"https://lib.jimmysong.io/blog/the-gateway-to-a-new-frontier/","publishdate":"2022-05-17T11:00:00+08:00","relpermalink":"/blog/the-gateway-to-a-new-frontier/","section":"blog","summary":"在我们看来，由于Envoy的设计、功能设置、安装基础和社区，它是业内最好的API网关。有了Envoy Gateway，企业可以在将Envoy嵌入其API管理策略方面增加信心。","tags":["Envoy","开源","网关","Tetrate"],"title":"Envoy API Gateway——推动网关的进一步发展","type":"post"},{"authors":["Matt Klein"],"categories":["Envoy"],"content":"前言 今天，我们很高兴地宣布 Envoy Gateway 成为 Envoy 代理家族的新成员，该项目旨在大幅降低将 Envoy 作为 API 网关的使用门槛。\n历史 Envoy 在 2016 年秋天开源，令我们惊讶的是，它很快就引领了整个行业。用户被这个项目的许多不同方面所吸引，包括它的包容性社区、可扩展性、API 驱动的配置模型、强大的可观测性输出和越来越广泛的功能集。\n尽管在其早期历史中，Envoy 成为了服务网格的代名词，但它在 Lyft 的首次使用实际上是作为 API 网关 / 边缘代理，提供深入的可观测性输出，帮助 Lyft 从单体架构迁移到微服务架构。\n在过去的 5 年多时间里，我们看到 Envoy 被大量的终端用户采用，既可以作为 API 网关，也可以作为服务网格中的 sidecar 代理。同时，我们看到围绕 Envoy 出现了一个庞大的供应商生态系统，在开源和专有领域提供了大量的解决方案。Envoy 的供应商生态系统对项目的成功至关重要；如果没有对所有在 Envoy 上兼职或全职工作的员工的资助，这个项目肯定不会有今天的成就。\nEnvoy 作为许多不同的架构类型和供应商解决方案的组成部分，其成功的另一面是它本质上位于架构底层；Envoy 并不是一个容易学习的软件。虽然该项目在世界各地的大型工程组织中取得了巨大的成功，但在较小和较简单的用例中，它只被轻度采用，在这些用例中，nginx 和 HAProxy 仍占主导地位。\nEnvoy Gateway 项目的诞生是出于这样的信念：将 Envoy 作为 API 网关的角色推向大众需要两个主要条件：\n 一个简化的部署模型和 API 层，旨在满足轻量级使用 将现有的 CNCF API 网关项目（Contour 和 Emissary）合并为一个共同的核，可以提供最好的用户体验，同时仍然允许供应商在 Envoy Proxy 和 Envoy Gateway 的基础上建立增值解决方案。  我们坚信，如果社区汇聚在单一的以 Envoy 为核心的 API 网关周围，它将会：\n 减少围绕安全、控制平面技术细节和其他共同关切的重复工作。 允许供应商专注于在 Envoy Proxy 和 Envoy Gateway 的基础上以扩展、管理平面 UI 等形式分层提供增值功能。 众人拾柴火焰高，让全世界更多的用户享受到 Envoy 的好处，无论组织的大小。更多的用户为更多的潜在客户提供了良性循环，为 Envoy 的核心项目提供了更多的支持，也为所有人提供了更好的整体体验。  项目概要 总得来说，Envoy Gateway 可以被认为是 Envoy Proxy 核心的一个封装器。它不会以任何方式改变核心代理、xDS、go-control-plane 等（除了潜在的驱动功能、bug 修复和一般改进以外）。它将提供以下功能：\n 为网关用例提供简化的 API。该 API 将是带有一些 Envoy 特定扩展的 Kubernetes Gateway API。之所以选择这个 API，是因为在 Kubernetes 上作为 Ingress Controller 部署是该项目最初的重点，而且该 API 得到了业界的广泛认可。 开箱即用，让用户能够尽可能快地启动和运行。这包括提供控制器资源、控制平面资源、代理实例等的生命周期管理功能。 可扩展的 API 平面。虽然该项目将致力于使常见的 API 网关功能开箱即用（例如，速率限制、认证、Let’s Encrypt 集成等），但供应商将能够提供所有 API 的 SaaS 版本，提供额外的 API 和增值功能，如 WAF、增强的可观测性、混乱工程等。 高质量的文档和入门指南。我们对 Envoy Gateway 的主要目标是使最常见的网关用例对普通用户来说可以信手拈来。  关于 API，我们认为导致混乱的主要是在针对高级用例时，在其他项目中有效地重新实现 Envoy 的 xDS API。这种模式导致用户不得不学习多个复杂的 API（最终转化为 xDS），才能完成工作。因此，Envoy Gateway 致力于 “硬性规定”，即 Kubernetes Gateway API（以及该 API 中任何允许的扩展）是唯一被支持的额外 API。更高级的用例将由 “xDS 模式” 提供服务，其中现有的 API 资源将为最终用户自动翻译，然后他们可以切换到直接利用 xDS API。这将导致一个更清晰的主 API，同时为那些可能超越主 API 的表达能力并希望通过 xDS 利用 Envoy 的全部功能的组织提供了路径。\n关于 API 的标准化 虽然 Envoy Gateway 的目标是提供一个参考实现，以便在 Kubernetes 中作为 Ingress Controller 轻松运行 Envoy，但这项工作最主要的目的是 API 标准化。随着行业在特定的 Envoy Kubernetes Gateway API 扩展上的趋同，它将允许供应商轻松地提供替代的 SaaS 实现，如果用户超越了参考实现，想要额外的支持和功能等，这可能是最好的。显然，围绕定义 API 扩展，确定哪些 API 是必需的，哪些是可选的，等等，还有很多工作要做。这是我们标准化之旅的开始，我们渴望与所有感兴趣的人一起深入研究。\n接下来的计划 今天，我们感谢 Envoy Gateway 的最初赞助商（Ambassador Labs、Fidelity、Tetrate 和 VMware），很高兴能与大家一起开始这个新的旅程。该项目是非常早期的，到目前为止的重点是商定 目标和高水平的设计，所以现在是参与的好时机，无论是作为终端用户还是作为系统集成商。\n我们还想非常清楚地说明，Contour 和 Emissary 的现有用户不会被抛在后面。该项目（以及 VMware 和 Ambassador Labs）完全致力于确保这些项目的用户最终能够顺利地迁移到 Envoy Gateway，无论是通过翻译和替换，还是通过这些项目成为 Envoy Gateway 核心的包装物。\n我们对通过 Envoy Gateway 项目将 Envoy 带给更大的用户群感到非常兴奋，我们希望你能加入我们的旅程。\n","date":1652677200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"f52e27fea3bc85bc650b9954f605a3e5","permalink":"https://lib.jimmysong.io/blog/introducing-envoy-gateway/","publishdate":"2022-05-16T13:00:00+08:00","relpermalink":"/blog/introducing-envoy-gateway/","section":"blog","summary":"今天，我们很高兴地宣布 Envoy Gateway 成为 Envoy 代理家族的新成员，该项目旨在大幅降低将 Envoy 作为 API 网关的使用门槛。","tags":["Envoy","开源","网关"],"title":"开源项目 Envoy Gateway 简介","type":"post"},{"authors":["Liam Byrne"],"categories":["Service Mesh"],"content":"什么是 Wasm 插件？ 你可以使用 Wasm 插件在数据路径上添加自定义代码，轻松地扩展服务网格的功能。可以用你选择的语言编写插件。目前，有 AssemblyScript（TypeScript-ish）、C++、Rust、Zig 和 Go 语言的 Proxy-Wasm SDK。\n在这篇博文中，我们描述了如何使用 Wasm 插件来验证一个请求的有效载荷。这是 Wasm 与 Istio 的一个重要用例，也是你可以使用 Wasm 扩展 Istio 的许多方法的一个例子。您可能有兴趣阅读我们关于在 Istio 中使用 Wasm 的博文，并观看我们关于在 Istio 和 Envoy 中使用 Wasm 的免费研讨会的录音。\n何时使用 Wasm 插件？ 当你需要添加 Envoy 或 Istio 不支持的自定义功能时，你应该使用 Wasm 插件。使用 Wasm 插件来添加自定义验证、认证、日志或管理配额。\n在这个例子中，我们将构建和运行一个 Wasm 插件，验证请求 body 是 JSON，并包含两个必要的键 ——id 和 token。\n编写 Wasm 插件 这个示例使用 tinygo 来编译成 Wasm。确保你已经安装了 tinygo 编译器。\n配置 Wasm 上下文 首先配置 Wasm 上下文，这样 tinygo 文件才能操作 HTTP 请求：\npackage main import ( \u0026#34;github.com/tetratelabs/proxy-wasm-go-sdk/proxywasm\u0026#34; \u0026#34;github.com/tetratelabs/proxy-wasm-go-sdk/proxywasm/types\u0026#34; \u0026#34;github.com/tidwall/gjson\u0026#34; ) func main() { // SetVMContext 是配置整个 Wasm VM 的入口。请确保该入口在 main 函数中调用，否则 VM 将启动失败。 \tproxywasm.SetVMContext(\u0026amp;vmContext{}) } // vmContext 实现 proxy-wasm-go SDK 的 types.VMContext 接口。 type vmContext struct { // 在这里嵌入默认的虚拟机环境，我们不需要实现所有方法。 \ttypes.DefaultVMContext } // 复写 types.DefaultVMContext func (*vmContext) NewPluginContext(contextID uint32) types.PluginContext { return \u0026amp;pluginContext{} } // pluginContext 实现 proxy-wasm-go SDK 的 types.PluginContext 接口 type pluginContext struct { // 在这里侵入默认的插件上下文，我们不需要实现所有方法。 \ttypes.DefaultPluginContext } // 复写 types.DefaultPluginContext func (ctx *pluginContext) NewHttpContext(contextID uint32) types.HttpContext { return \u0026amp;payloadValidationContext{} } // payloadValidationContext 实现 proxy-wasm-go SDK 的 types.HttpContext 接口 type payloadValidationContext struct { // 在这里嵌入默认的根 http 上下文，我们不需要实现所有方法。 \ttypes.DefaultHttpContext totalRequestBodySize int } 验证负载 内容类型头是通过实现 OnHttpRequestHeaders 来验证的，一旦从客户端收到请求头，就会调用该头。\nproxywasm.SendHttpResponse 用于响应 403 forbidden 的错误代码和信息，如果内容类型丢失的话。\nfunc (ctx *payloadValidationContext) OnHttpRequestHeaders(numHeaders int, endOfStream bool) types.Action { contentType, err := proxywasm.GetHttpRequestHeader(\u0026#34;content-type\u0026#34;) if err != nil || contentType != \u0026#34;application/json\u0026#34; { // 如果 header 没有期望的 content type，返回 403 响应 \tif err := proxywasm.SendHttpResponse(403, nil, []byte(\u0026#34;content-type must be provided\u0026#34;), -1); err != nil { proxywasm.LogErrorf(\u0026#34;failed to send the 403 response: %v\u0026#34;, err) } // 终止 ActionPause 对流量的进一步处理 \treturn types.ActionPause } // ActionContinue 让主机继续处理 body \treturn types.ActionContinue } 请求主体是通过实现 OnHttpRequestBody 来验证的，每次从客户端接收到请求的一个块时，都会调用该请求。这是通过等待直到 endOfStream 为真并记录所有收到的块的总大小来完成的。一旦收到整个主体，就会使用 proxywasm.GetHttpRequestBody 读取，然后可以使用 golang 进行验证。\n这个例子使用 gjson，因为 tinygo 不支持 golang 的默认 JSON 库。它检查有效载荷是否是有效的 JSON，以及键 id 和 token 是否存在。\nfunc (ctx *payloadValidationContext) OnHttpRequestBody(bodySize int, endOfStream bool) types.Action { ctx.totalRequestBodySize += bodySize if !endOfStream { // OnHttpRequestBody 等待收到到 body 的全部才开始处理。 \treturn types.ActionPause } body, err := proxywasm.GetHttpRequestBody(0, ctx.totalRequestBodySize) if err != nil { proxywasm.LogErrorf(\u0026#34;failed to get request body: %v\u0026#34;, err) return types.ActionContinue } if !validatePayload(body) { // 如果验证失败，发送 403 响应。 \tif err := proxywasm.SendHttpResponse(403, nil, []byte(\u0026#34;invalid payload\u0026#34;), -1); err != nil { proxywasm.LogErrorf(\u0026#34;failed to send the 403 response: %v\u0026#34;, err) } // 终止流量 \treturn types.ActionPause } return types.ActionContinue } // validatePayload 验证给定的 json 负载 // 注意该函数使用 gjson 解析 json，因为 TinyGo 不支持 encoding/json func validatePayload(body []byte) bool { if !gjson.ValidBytes(body) { proxywasm.LogErrorf(\u0026#34;body is not a valid json: %v\u0026#34;, body) return false } jsonData := gjson.ParseBytes(body) // 验证 json。检查示例中是否存在必须的键 \tfor _, requiredKey := range []string{\u0026#34;id\u0026#34;, \u0026#34;token\u0026#34;} { if !jsonData.Get(requiredKey).Exists() { proxywasm.LogErrorf(\u0026#34;required key (%v) is missing: %v\u0026#34;, requiredKey, jsonData) return false } } return true } 编译成 Wasm 使用 tinygo 编译器编译成 Wasm：\ntinygo build -o main.wasm -scheduler=none -target=wasi main.go 部署 Wasm 插件 打包到 Docker 中部署到 Envoy 对于开发，这个插件可以在 Docker 中部署到 Envoy。下面的 Envoy 配置文件将设置 Envoy 监听 localhost:18000，运行所提供的 Wasm 插件，并在成功后响应 HTTP 200 和文本 hello from server。突出显示的部分是配置 Wasm 插件。\nstatic_resources:listeners:- name:mainaddress:socket_address:address:0.0.0.0port_value:18000filter_chains:- filters:- name:envoy.http_connection_managertyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManagerstat_prefix:ingress_httpcodec_type:autoroute_config:name:local_routevirtual_hosts:- name:local_servicedomains:- \u0026#34;*\u0026#34;routes:- match:prefix:\u0026#34;/\u0026#34;route:cluster:web_servicehttp_filters:- name:envoy.filters.http.wasmtyped_config:\u0026#34;@type\u0026#34;: type.googleapis.com/udpa.type.v1.TypedStructtype_url:type.googleapis.com/envoy.extensions.filters.http.wasm.v3.Wasmvalue:config:vm_config:runtime:\u0026#34;envoy.wasm.runtime.v8\u0026#34;code:local:filename:\u0026#34;./main.wasm\u0026#34;- name:envoy.filters.http.router- name:staticreplyaddress:socket_address:address:127.0.0.1port_value:8099filter_chains:- filters:- name:envoy.http_connection_managertyped_config:\u0026#34;@type\u0026#34;: …","date":1652418000,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"73000808ae2139e4323c3fce9d45f880","permalink":"https://lib.jimmysong.io/blog/validating-a-request-payload-with-wasm/","publishdate":"2022-05-13T13:00:00+08:00","relpermalink":"/blog/validating-a-request-payload-with-wasm/","section":"blog","summary":"本文是一个使用 Go 语言开发 Wasm 插件验证请求负载，并将其部署到 Istio 或 Envoy 上的教程。","tags":["wasm","istio","envoy"],"title":"使用 WebAssembly 验证请求负载","type":"post"},{"authors":null,"categories":null,"content":"本报告译自 O’Reilly 出品的 The Future of Observablity with OpeTelemetry，作者 Ted Young，译者 Jimmy Song。\n关于本书 本书内容包括：\n OpenTelemetry 如何满足库作者、应用程序拥有者、运维和响应者的需求 应用程序的不同角色如何围绕 OpenTelemetry 来协同和独立工作 关于在组织中采用和管理 OpenTelemetry 的实用建议  关于作者 Ted Young 是 OpenTelemetry 项目的联合创始人之一。在过去的二十年里，他设计并建立了各种大规模的分布式系统，包括可视化 FX 管道和容器调度系统。他目前在 Lightstep 公司担任开发者教育总监，住在俄勒冈州波特兰的一个小农场里。\n许可证 您可以使用署名 - 非商业性使用 - 相同方式共享 4.0 (CC BY-NC-SA 4.0) 协议共享。\n","date":1651795200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"3d78837fcc0503a24f74c332111a3100","permalink":"https://lib.jimmysong.io/book/opentelemetry-obervability/","publishdate":"2022-05-06T00:00:00Z","relpermalink":"/book/opentelemetry-obervability/","section":"book","summary":"利用格式化数据从根本上改变可观测性实践","tags":["翻译电子书"],"title":"OpenTelemetry 可观测性指南","type":"publication"},{"authors":["Ariane van der Steldt","Radha Kumari"],"categories":["Envoy"],"content":"前言 Slack 有一个全球客户群，在高峰期有数百万同时连接的用户。用户之间的大部分通信涉及到向对方发送大量的微小信息。在 Slack 的大部分历史中，我们一直使用 HAProxy 作为所有传入流量的负载均衡器。今天，我们将讨论我们在使用 HAProxy 时所面临的问题，我们如何用 Envoy Proxy 来解决这些问题，迁移所涉及的步骤，以及结果是什么。让我们开始吧！\nSlack 的 Websockets 为了即时传递信息，我们使用 websocket 连接，这是一种双向的通信链接，负责让你看到 “有几个人在打字……\u0026#34;，然后是他们打的东西，速度几乎是光速的。websocket 连接被摄取到一个叫做 “wss”（WebSocket 服务）的系统中，可以通过 wss-primary.slack.com 和 wss-backup.slack.com（这不是网站，如果去访问，只会得到一个 HTTP 404）从互联网上访问。\n   显示websockets工作原理的图表  Websocket 连接一开始是普通的 HTTPS 连接，然后客户端发出协议切换请求，将连接升级为 Websocket。在 Slack，我们有不同的 websocket 服务，专门用于消息、在线（列出哪些联系人在线）和其他服务。其中一个 websocket 端点是专门为需要与 Slack 互动的应用程序制作的（因为应用程序也想要实时通信）。\n   解释流量如何被路由到后端服务的流程图  过去，我们在多个 AWS Region 有一组专门用于 websockets 的 HAProxy 实例，以终止靠近用户的 websocket 连接，并将请求转发给相应的后端服务。\n迁移到 Envoy Proxy 的动机 虽然我们从 Slack 开始就一直在使用 HAproxy，并且知道如何大规模地操作它，但有一些操作上的挑战让我们考虑替代方案，比如 Envoy Proxy。\n热重启 在 Slack，后端服务端点列表的变化是一个常见的事件（由于实例被添加或删除）。HAProxy 提供两种方法来更新其配置，以适应端点列表的变化。一种是使用 HAProxy Runtime API。我们在其中一套 HAProxy 实例中使用了这种方法，我们的经验在另一篇博文中有所描述 —— 在 Slack 的可怕的、恐怖的、没有好处的、非常糟糕的一天。另一种方法，我们用于 websockets 负载均衡器（LB），是将后端渲染到 HAProxy 配置文件中，然后重新加载 HAProxy。\n每次 HAProxy 重载时，都会创建一组新的进程来处理新进入的连接。我们会让旧的进程持续运行很多小时，以便让长寿的 websocket 连接耗尽，避免用户频繁断开连接。然而，我们不能有太多的 HAProxy 进程，每个进程都运行着它自己 “当时” 的配置副本 —— 我们希望实例能更快地汇聚到新版本的配置上。我们不得不定期收割旧的 HAProxy 进程，并限制 HAProxy 重新加载的频率，以防底层后端出现混乱。\n无论我们使用哪种方法，都需要一些额外的基础设施来管理 HAProxy 的重新加载。\nEnvoy 允许我们使用动态配置的集群和端点，这意味着如果端点列表发生变化，它不需要重新加载。如果代码或配置确实发生了变化，Envoy 有能力在不放弃任何连接的情况下热重启自己。Envoy 通过 inotify 观察文件系统配置的更新。在热重启过程中，Envoy 还将统计数据从父进程复制到子进程中，因此仪表和计数器不会被重置。\n这一切都使 Envoy 的运营开销大大减少，而且不需要额外的服务来管理配置变化或重新启动。\n负载均衡功能 Envoy 提供了一些先进的负载均衡功能，如：\n 内置支持区域感知路由的功能 通过异常值检测进行被动健康检查 恐慌路由：Envoy 通常只将流量路由到健康的后端，但是如果健康主机的百分比低于某个阈值，它可以被配置为将流量发送到所有的后端，不管是健康的还是不健康的。这在我们 2021 年 1 月 4 日的故障中非常有帮助，这次故障是由我们基础设施中的一个广泛的网络问题引起的。  由于上述原因，在 2019 年，我们决定将我们的入口负载均衡层从 HAproxy 迁移到 Envoy Proxy，从 websockets 堆栈开始。迁移的主要目标是提高可操作性，获得 Envoy 提供的新功能，以及更加标准化。通过在整个 Slack 中从 HAProxy 迁移到 Envoy，我们的团队将不再需要了解两个软件的怪异之处，不再需要维护两种不同的配置，不再需要管理两个构建和发布管道，诸如此类。那时，我们已经在使用 Envoy Proxy 作为我们服务网格中的数据平面。我们内部也有经验丰富的 Envoy 开发人员，所以我们可以随时获得 Envoy 的专业知识。\n生成 Envoy 配置 这次迁移的第一步是审查我们现有的 websocket 层配置，并生成一个同等的 Envoy 配置。在迁移过程中，管理 Envoy 配置是我们最大的挑战之一。Envoy 有丰富的功能集，其配置与 HAProxy 的配置有很大的不同。Envoy 配置涉及四个主要概念：\n Listener，接收请求，又称 TCP 套接字、SSL 套接字或 unix 域套接字。 Cluster，代表我们发送请求的内部服务，如消息服务器和存在服务器 Route，将 Listener 和 Cluster 连接在一起 Filter，它对请求进行操作  Slack 的配置管理主要是通过 Chef 完成的。当我们开始使用 Envoy 时，我们把 Envoy 配置作为 chef 模板文件来部署，但它的管理变得很麻烦，而且容易出错。为了解决这个问题，我们建立了 chef 库和自定义资源来生成 Envoy 配置。\n   Chef 资源的结构和流程图  在 Chef 内部，配置是一个单例，模拟了每个主机只有一个 Envoy 配置的情况。所有的 Chef 资源都在这个单例上操作，添加监听器、路由或集群。在 Chef 运行的最后，envoy.yaml 被生成、验证，然后安装 —— 我们从不写中间配置，因为这些配置可能是无效的。\n这个例子展示了我们如何创建一个有两条路由的 HTTP 监听器，将流量路由到两个动态集群。\n   调用Chef资源以创建带有集群和路由的监听器的例子  要在 Envoy 中复制我们复杂的 HAProxy 配置需要一些努力。大部分需要的功能在 Envoy 中已经有了，所以只需要在 chef 库中加入对它的支持就可以了。我们实现了一些缺失的 Envoy 功能（有些是上游贡献的，有些是内部维护的扩展）。\n对我们的新配置进行测试和验证 测试新的 Envoy websockets 层是一个迭代的过程。我们经常用手工编码的 Envoy 配置做原型，并在本地的开发机器上测试，每个监听器、路由和集群都有一个。手工编码的修改一旦成功，就会被移到 chef 库中。\nHTTP 路由是用 curl 测试的：\n 基于头和 cookie 的特定路由到特定后端 基于路径、前缀和查询参数的路由到特定后端 SSL 证书  当事情没有达到预期效果时，我们在机器上使用 Envoy 调试日志。调试日志清楚地解释了为什么 Envoy 选择将一个特定的请求路由到一个特定的集群。Envoy 的调试日志非常有用，但也很冗长，而且很昂贵（你真的不想在生产环境中启用这个功能）。调试日志可以通过 Curl 启用，如下所示。\ncurl -X POST http://localhost:\u0026lt;envoy_admin_port\u0026gt;/logging?level=debug Envoy 管理接口在初始调试时也很有用，特别是这些端点：\n clusters：显示所有配置的集群，包括每个集群中所有上游主机的信息以及每个主机的统计数据。 /certs：以 JSON 格式显示所有加载的 TLS 证书，包括文件名、序列号、主体替代名称和到期前的天数。 /listeners：显示所有配置的监听器及其名称和地址。  我们的 Chef 库使用 -mode validate 命令行选项运行 Envoy，作为一个验证步骤，以防止安装无效的配置。这也可以手动完成。\nsudo /path/to/envoy/binary -c \u0026lt;/path/to/envoy.yaml\u0026gt; --mode validate Envoy 提供 JSON 格式的监听器日志。我们将这些日志录入我们的日志管道（当然是在对日志进行 PII 处理后），这对调试工作经常很有帮助。\n一旦对开发环境中的配置有信心，我们就准备做一些更多的测试 – 在生产中！\u0026#34;。\n迁移至生产 为了将迁移过程中的风险降到最低，我们建立了一个新的 Envoy websocket 栈，其配置与现有的 HAProxy 层相当。这意味着我们可以逐步、有控制地将流量转移到新的 Envoy 堆栈，并且在必要时可以快速切换回 HAProxy。缺点是我们的 AWS 成本 —— 我们在迁移过程中使用了双倍的资源，但我们愿意花费时间和资源为我们的客户透明地进行迁移。\n我们通过 NS1 管理我们的 DNS 记录 wss-primary.slack.com 和 wss-backup.slack.com。我们使用加权路由将流量从 haproxy-wss 转移到 envoy-wss NLB DNS 名称。第一批区域是以 10%、25%、50%、75% 和 100% 的步骤单独上线的。由于我们对新的 Envoy 层和上线过程有信心，所以最后的区域上线速度更快（25%、50%、75%、100% 只需两天，而之前的一个区域需要一周的时间）。\n尽管迁移工作很顺利，没有出现故障，但还是出现了一些小问题，比如超时值和 header 的差异。在迁移过程中，我们多次恢复、修复，并再次上线。\n   流程图显示DNS迁移过程中涉及的组件和步骤  经过漫长而激动人心的 6 个月，迁移完成了，整个 HAProxy websocket 堆栈在全球范围内被 Envoy Proxy 取代，对客户的影响为零。\n哪些进展顺利，哪些不顺利 迁移本身是相对平淡和无聊的。枯燥是一件好事：刺激意味着事情的中断，枯燥意味着一切顺利。\n我们发现，旧的 HAProxy 配置随着时间的推移而有机地增长。它在很大程度上是由 HAProxy 使用的模型形成的 —— 一个包括所有监听器的大型配置。Envoy 的配置模型比 HAProxy 的模型使用更多的定义范围。一旦一个监听器被输入，只有该监听器内的规则适用于请求。一旦输入一个路由，只有该路由上的规则适用。这使得将规则与相关的请求联系起来更加容易。\n我们花了很长时间从旧的 HAProxy 配置中提取重要的东西，这实际上是技术债务。通常很难弄清楚为什么会有某个规则，哪些是有意的，哪些是无意的，以及其他服务所依赖的行为是什么。例如，有些服务应该只在两个虚拟主机（vhosts）中的一个下，但实际上在 HAProxy 的两个 vhosts 下都可用。我们不得不复制这个错误，因为现有的代码依赖于这种行为。\n我们在 HAProxy 堆栈中错过了一些细微的东西。有时这些是很重要的 —— 我们破坏了 Slack 的每日活跃用户（DAU）指标（哎呀！）。也有很多小问题需要解决。负载均衡器的行为很复杂，除了花时间调试外，没有真正的办法解决这个问题。\n我们开始迁移时，没有为负载均衡器的配置提供测试框架。我们没有自动测试来验证测试的 URL 路由到正确的端点以及与请求和响应头相关的行为，而是有…… 一个 HAProxy 配置。在迁移过程中，测试是很有帮助的，因为它们可以提供很多关于预期行为的原因的背景。因为我们缺乏测试，所以我们经常不得不向服务所有者询问，以了解他们所依赖的行为。\n我们建立的 Chef 资源有意只支持 Envoy 功能的一个子集。这使我们的库更简单 —— 我们只需要考虑我们实际使用的功能。缺点是，每次我们想使用新的 Envoy 功能时，都必 …","date":1649394000,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"2bc7c863ec60248a8ceb5cc931139689","permalink":"https://lib.jimmysong.io/blog/migrating-millions-of-concurrent-websockets-to-envoy/","publishdate":"2022-04-08T13:00:00+08:00","relpermalink":"/blog/migrating-millions-of-concurrent-websockets-to-envoy/","section":"blog","summary":"本文是 Slack 花半年时间从 HAProxy 迁移到 Envoy 上的经验分享。","tags":["Envoy","slcak","haproxy","WebSockets"],"title":"Slack 将数百万个并发的 Websockets 迁移到 Envoy 上经验分享","type":"post"},{"authors":null,"categories":null,"content":"本书译自 The Developer Advocacy Handbook，副标题「开发者布道师的自我修养」为译者自拟。\n 作者：Christian Heilmann 译者：宋净超（Jimmy Song） 开始阅读 下载电子书  译者序 随着开源软件的流行，企业的开发者布道（Developer Advocacy）的需求不断增长，开发者布道师（Developer Advocate）的职业前景也越来越广阔。这本书将揭开大众口中的“布道师”的神秘面纱，让你了解这一角色的工作内容；对于各位布道师来说，这本书也将成为你整个布道生涯的得力助手，指导你布道的方方面面。\n作者 Christian Heilmann 是一个德国人，从事开发者布道工作已有十余年，译者本身也有多年从事开发者布道工作的经验。目前在中文互联网上，对“布道师”这一角色还存在很多猜疑和误解，相信这本书可以化解开发者对布道师这一角色的疑虑。虽然书中提到的一些技术在中国可能并不适用，但是中国也有同类出色的替代品，我相信人情总是相通的，技术只是手段而已。\n由于笔者完全凭个人兴趣，利用业余时间翻译此书，翻译过程中难免遗漏和错误，欢迎提交 Issue 评论和 PR 修改。如果你有关于开发者布道、开源、云原生及开源相关的问题，欢迎与我交流。\n许可证 本书所有内容支持使用署名 - 非商业性使用 - 相同方式共享 4.0 (CC BY-NC-SA 4.0) 协议共享。\n","date":1648080000,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"6e2457fbeab76e72de47a3f208acdd81","permalink":"https://lib.jimmysong.io/book/developer-advocacy-handbook/","publishdate":"2022-03-24T00:00:00Z","relpermalink":"/book/developer-advocacy-handbook/","section":"book","summary":"开发者布道师的自我修养","tags":["电子书"],"title":"开发者布道手册","type":"publication"},{"authors":null,"categories":null,"content":"本书译自美国国家标准标准与技术研究院（NIST）Special Publication 800-204C。\n关于本书 作者：Ramaswamy Chandramouli\n审阅\n 计算机安全司信息技术实验室 美国商务部 Gina M. Raimondo，秘书 国家标准和技术研究所 James K. Olthoff，履行负责标准和技术的商务部副部长兼国家标准和技术研究所所长的非专属职能和职责  原出版物可在：https://doi.org/10.6028/NIST.SP.800-204C 免费获取，中文版请在此阅读。\n","date":1647216000,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"77e3afee877672a563686411fc92a285","permalink":"https://lib.jimmysong.io/book/service-mesh-devsecops/","publishdate":"2022-03-14T00:00:00Z","relpermalink":"/book/service-mesh-devsecops/","section":"book","summary":"NIST 特别发布","tags":["翻译电子书"],"title":"利用服务网格实施微服务的 DevSecOps","type":"publication"},{"authors":["Thomas Graf"],"categories":["service mesh"],"content":"编者的话 本文作者是 Isovalent 联合创始人\u0026amp;CTO，原文标题 How eBPF will solve Service Mesh - Goodbye Sidecars，作者回顾了Linux 内核的连接性，实现服务网格的几种模式，以及如何使用 eBPF 实现无 Sidecar 的服务网格。\n什么是服务网格？ 随着分布式应用的引入，额外的可视性、连接性和安全性要求也浮出水面。应用程序组件通过不受信任的网络跨越云和集群边界进行通信，负载均衡、弹性变得至关重要，安全必须发展到发送者和接收者都可以验证彼此的身份的模式。在分布式应用的早期，这些要求是通过直接将所需的逻辑嵌入到应用中来解决的。服务网格将这些功能从应用程序中提取出来，作为基础设施的一部分提供给所有应用程序使用，因此不再需要修改每个应用程序。\n   服务网格示意图  纵观今天服务网格的功能设置，可以总结为以下几点：\n 弹性连接：服务与服务之间的通信必须能够跨越边界，如云、集群和场所。通信必须是有弹性的和容错的。 L7 流量管理：负载均衡、速率限制和弹性必须是 L7 感知的（HTTP、REST、gRPC、WebSocket 等）。 基于身份的安全：依靠网络标识符来实现安全已经不够了，发送和接收服务都必须能够根据身份而不是网络标识符来验证对方。 可观测性和跟踪：追踪和指标形式的可观测性对于理解、监控和排除应用程序的稳定性、性能和可用性至关重要。 透明：该功能必须以透明的方式提供给应用程序，即不需要改变应用程序代码。  在早期，服务网格的功能通常是以库的形式实现的，要求网格中的每个应用程序都要链接到以应用程序的语言框架编写的库。类似的事情也发生在互联网的早期：曾几何时，应用程序还需要运行自己的 TCP/IP 协议栈！正如我们将在这篇文章中讨论的那样，服务网格正在发展成为一种内核责任，就像网络堆栈一样。\n   基于库的服务网格模型  今天，服务网格通常使用一种叫做 sidecar 模型的架构来实现。这种架构将实现上述功能的代码封装到第四层代理中，服务间的流量被重定向到这个所谓的 sidecar 代理。它之所以被称为 “挎斗”，是因为每个应用程序都有一个代理，就像挎斗附着在摩托车上一样。\n   基于 Sidecar 的服务网格模型  这种架构的优点是，服务不再需要自己实现服务网格的功能。如果许多服务是用不同的语言编写部署的，或者如果你正在运行不可变的第三方应用程序，这就很有好处。\n这种模式的缺点是有大量的代理，许多额外的网络连接，以及复杂的重定向逻辑，将网络流量输入代理。除此之外，在什么类型的网络流量可以被重定向到第四层代理上也有限制。代理（Proxy）在其能支持的网络协议方面是有限的。\n连接性转移到内核中的历史 几十年来，在应用程序之间提供安全可靠的连接一直是操作系统的责任。有些人可能还记得早期 Unix 和 Linux 时代的 TCP 包装器和 tcpd。tcpd 允许用户在不修改应用程序的情况下透明地添加日志、访问控制、主机名验证和欺骗保护。它使用了 libwrap，而且，在一个有趣的平行于服务网格的故事中，这个库也是以前应用程序提供这些功能的链接对象。tcpd 所带来的是能够在不修改现有应用程序的情况下将这些功能透明地添加到现有应用程序中。最终，所有这些功能都进入了 Linux 本身，并以一种更有效、更强大的方式提供给所有应用程序。今天，这已经发展到了我们所知道的 iptables。\n然而，iptables 显然不适合解决现代应用的连接性、安全性和可观测性要求，因为它只在网络层面上操作，对应用协议层缺乏任何了解。自然，阻力最小的路径是回到库模型，然后是 sidecar 模型。现在，我们正处于这样一个阶段：为了最佳的透明度、效率和安全性，在操作系统中原生地支持这种模式是有意义的。\n   服务网格的进化  在 tcpd 时代，曾经的连接记录现在是追踪。IP 层面的访问控制已经演变成应用协议层面的授权，例如使用 JWT。主机名验证已被更强大的认证所取代，如 mTLS。网络负载均衡已经扩展到 L7 流量管理。HTTP 重试是新的 TCP 重传。过去用黑洞路由解决的问题今天被称为断路。这些都不是根本性的新问题，但所需的环境和控制已经发生了变化。\n扩展内核命名空间概念 Linux 内核已经有一个概念，可以共享共同的功能，并使其对系统上运行的许多应用程序可用。这个概念被称为命名空间（Namespace），它构成了我们今天所知的容器技术的基础。命名空间（内核的那种，不是 Kubernetes 的命名空间）存在于各种抽象中，包括文件系统、用户管理、挂载设备、进程、网络等。这就是允许单个容器呈现不同的文件系统视图、不同的用户集，以及允许多个容器绑定到单个主机上的同一网络端口。在 cgroups 的帮助下，这个概念得到了扩展，可以对 CPU、内存和网络等资源进行管理和优先排序。从云原生应用开发者的角度来看，cgroups 和资源被紧密地整合到我们所知的 “容器” 概念中。\n符合逻辑的是，如果我们认为服务网格是操作系统的责任，那么它必须符合并整合命名空间和 cgroup 的概念。这看起来会是这样的。\n   Service Mesh Namespace  不出所料，这看起来非常自然，而且可能是大多数用户从简单的角度所期望的。应用程序保持不变，它们继续使用套接字进行通信，就像以前那样。理想的服务网格是作为 Linux 的一部分透明地提供的。它就在那里，就像今天的 TCP 一样。\n注入 Sidecar 的成本 如果我们仔细研究一下 sidecar 模型，我们会发现它实际上是在试图模仿这种模型。应用程序继续使用套接字，一切都被塞进 Linux 内核的网络命名空间。然而，这比它看起来要复杂得多，需要许多额外的步骤来透明地注入 sidecar 代理。\n   注入 Sidecar 的成本  这种额外的复杂性在延迟和额外资源消耗方面付出了巨大的代价。早期的基准测试表明，这对延迟的影响高达 3-4 倍，而且所有代理都需要大量的额外内存。在这篇文章的后面，我们将研究这两点，因为我们将其与基于 eBPF 的模型进行比较。\n用 eBPF 解锁内核服务网格 为什么我们以前没有在内核中创建一个服务网格？有些人半开玩笑地说，kube-proxy 是最初的服务网格（见我们已经构建了相当多的服务网格 - Tim Hockin, Google）。这句话是有一定道理的。Kube-proxy 是一个很好的例子，说明了 Linux 内核在依靠传统的基于网络的 iptables 功能实现服务网格时，可以达到多么接近。然而，这还不够，L7 上下文是缺失的。Kube-proxy 完全在网络数据包层面运作。现代应用需要 L7 流量管理、跟踪、认证和额外的可靠性保证。Kube-proxy 不能在网络层面上提供这些。\neBPF 改变了这个模式。它允许动态地扩展 Linux 内核的功能。我们一直在使用 eBPF 为 Cilium 建立一个高效的网络、安全和可观测性数据通路，并将其直接嵌入到 Linux 内核。应用这个相同的概念，我们也可以在内核层面上解决服务网格的要求。事实上，Cilium 已经实现了各种所需的概念，如基于身份的安全、L3-L7 可观测性和授权、加密和负载均衡。缺少的部分现在正在向 Cilium 涌来。在本博客的末尾，你会发现如何加入由 Cilium 社区推动的 Cilium 服务网格测试项目的细节。\n   eBPF 服务网格架构  有人可能想知道为什么 Linux 内核社区不直接解决这些需求。eBPF 有一个巨大的优势，eBPF 代码可以在运行时插入到现有的 Linux 内核中，类似于 Linux 内核模块，但与内核模块不同，它可以以安全和可移植的方式进行。这使得 eBPF 的实现能够随着服务网格社区的发展而继续发展。新的内核版本需要几年时间才能进入用户手中。eBPF 是一项关键技术，它使 Linux 内核能够跟上快速发展的云原生技术栈。\n无 Sidecar 的基于 eBPF 的 L7 追踪和度量 让我们看看 L7 追踪和指标可观测性，作为一个具体的例子，说明基于 eBPF 的服务网格对保持低延迟和提高观察性有巨大的影响。应用程序团队依靠应用程序的可视性和监控作为基本要求这些，这包括请求跟踪、HTTP 响应率和服务延迟信息等能力。然而，这种可观测性应该没有明显的成本（延迟、复杂性、资源…）。\n   基于 eBPF 的可视性  在下面的基准测试中，我们可以看到早期的测量结果，即通过 eBPF 或 sidecar 方法实现 HTTP 可视性对延迟的影响。该设置是在两个不同节点上运行的两个 pod 之间通过固定数量的连接每秒稳定运行 10K 个 HTTP 请求，并测量请求的平均延时。\n   基于 eBPF 的延迟基准测试 vs 基于 Sidecar 的 L7 可视性  我们故意不提这些测量中使用的具体代理，因为它并不重要。对于我们测试过的所有代理，结果几乎都是一样的。要明确的是，这不是关于 Envoy、Linkerd、Nginx 或其他代理是否更快。所提到的代理有差异，但与首先注入代理的成本相比，它们是微不足道的。几乎没有开销是来自代理本身的逻辑。开销是通过注入代理，将网络流量重定向到它，终止连接和启动新的连接而增加的。\n这些早期的测量结果表明，基于 eBPF 的内核方法是非常有前途的，可以实现完全透明的服务网格的愿望，而且没有明显的开销。\n使用 eBPF 加速的 per-node 代理 越来越多的用例可以用这种仅有 eBPF 的方法来覆盖，从而完全取消 L4 代理。有些用例，仍然需要代理。例如，当连接需要拼接时，当 TLS 终止被执行时，或对于某些形式的 HTTP 授权。\n我们的 eBPF 服务网格工作将继续关注那些从性能角度可以获得最大收益的领域。如果你必须执行 TLS 终止，你可能不介意在流量流入集群时用代理终止一次连接。然而，你会更关心在每个连接的路径中注入两个代理的影响，以提取 HTTP 指标和跟踪数据。\n当一个用例不能用纯 eBPF 的方法来实现时，网格可以回退到每个节点的代理模型，直接将代理与内核的套接字层结合起来。\n   eBPF per-node Proxy  eBPF 不依赖网络级的重定向，而是直接在套接字级别注入代理，保持路径短。在 Cilium 的案例中，正在使用 Envoy 代理，尽管从架构的角度来看，任何代理都可以被整合到这个模型。从概念上讲，这允许将 Linux 内核网络命名空间的概念直接扩展到 Envoy 监听器配置的概念，并将 Envoy 变成一个多用户代理。\nSidecar 与 per-Node 代理 即使需要代理，代理的成本也会根据部署的架构而有所不同。让我们来看看每个节点的代理模式与 sidecar 模式的比较。\n每个连接的代理 所需的网络连接数将因是否有代理而不同。最简单的情况是无 sidecar 模式，这意味着网络连接的数量没有变化。一个单一的连接将为请求提供服务，eBPF 将提供服务网格功能，如跟踪或现有连接上的负载均衡。\n   基于 eBPF 的模型  用 sidecar 模型提供同样的功能需要在连接中注入两次代理，这导致需要维护三个连接。这导致了开销的增加和所有额外的套接字缓冲区所需内存的倍增，表现为更高的服务间延迟。这就是我们之前在无 sidecar L7 可视性部分看到的 sidecar 开销。\n   基于 Sidecar 代理的模型  切换到 per-node 的代理模式使我们能够摆脱其中一个代理，因为我们不再依赖在每个工作负载中运行一个 sidecar。比起不需要额外的连接，这还是不够理想，但比起总是需要两个额外的连接要好。\n   Per-node 代理模式  所需的代理总数 在每个工作负载中运行一个 sidecar 会导致大量的代理。即使每个单独的代理实例在其内存占用方面是相当优化的，但实例的数量之多将导致总的影响很大。此外，每个代理维护的数据结构，如路由和端点 …","date":1639054980,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"b400a332a4fa3e3f21eab757d0c7953d","permalink":"https://lib.jimmysong.io/blog/ebpf-solve-service-mesh-sidecar/","publishdate":"2021-12-09T21:03:00+08:00","relpermalink":"/blog/ebpf-solve-service-mesh-sidecar/","section":"blog","summary":"本文回顾了Linux 内核的连接性，实现服务网格的几种模式，以及如何使用 eBPF 实现无 Sidecar 的服务网格。","tags":["Istio","ebpf","service mesh"],"title":"告别 Sidecar——使用 eBPF 解锁内核级服务网格","type":"post"},{"authors":["胡渐飞"],"categories":["Istio"],"content":"前言 Istio 1.12 中新的 WebAssembly 基础设施使其能够轻松地将额外的功能注入网格部署中。\n经过三年的努力，Istio 现在有了一个强大的扩展机制，可以将自定义和第三方 Wasm 模块添加到网格中的 sidecar。Tetrate 工程师米田武（Takeshi Yoneda）和周礼赞（Lizan Zhou）在实现这一目标方面发挥了重要作用。这篇文章将介绍 Istio 中 Wasm 的基础知识，以及为什么它很重要，然后是关于建立自己的 Wasm 插件并将其部署到网格的简短教程。\n为什么 Istio 中的 Wasm 很重要 使用 Wasm，开发人员可以更容易的扩展网格和网关。在 Tetrate，我们相信这项技术正在迅速成熟，因此我们一直在投资上游的 Istio，使配置 API、分发机制和从 Go 开始的可扩展性体验更加容易。我们认为这将使 Istio 有一个全新的方向。\n有何期待：新的插件配置 API，可靠的获取和安装机制 有一个新的顶级 API，叫做 WasmPlugin，可以让你配置要安装哪些插件，从哪里获取它们（OCI 镜像、容器本地文件或远程 HTTP 资源），在哪里安装它们（通过 Workload 选择器），以及一个配置结构体来传递给插件实例。\nistio-agent 中的镜像提取机制（在 Istio 1.9 中引入），从远程 HTTP 源可靠地检索 Wasm 二进制文件，已被扩展到支持从任何 OCI 注册处检索 Wasm OCI 镜像，包括 Docker Hub、Google Container Registry（GCR）、Amazon Elastic Container Registry（Amazon ECR）和其他地方。\n这意味着你可以创建自己的 Wasm 插件，或者从任何注册处选择现成的插件，只需几行配置就可以扩展 Istio 的功能。Istio 会在幕后做所有的工作，为你获取、验证、安装和配置它们。\nIstio Wasm 扩展 Istio 的扩展机制使用 Proxy-Wasm 应用二进制接口（ABI）规范，该规范由周礼赞和米田武带头制定，提供了一套代理无关的流媒体 API 和实用功能，可以用任何有合适 SDK 的语言来实现。截至目前，Proxy-Wasm 的 SDK 有 AssemblyScript（类似 TypeScript）、C++、Rust、Zig 和 Go（使用 TinyGo WebAssembly 系统接口「WASI」，米田武也是其主要贡献者）。\n如何获取：Tetrate Istio Distro 获得 Istio 的最简单方法是使用 Tetrate 的开源 get-mesh CLI 和 Tetrate Istio Distro，这是一个简单、安全的上游 Istio 的企业级发行版。\nWasm 实战：构建你自己的速率限制 WebAssembly 插件 在我们之前关于 Envoy 中的 Wasm 扩展的博客中，我们展示了如何开发 WebAssembly 插件来增强服务网格的能力。新的 Wasm 扩展 API 让它变得更加简单。本教程将解释如何使用 Istio Wasm 扩展 API 来实现 Golang 中的速率限制。\n先决条件  熟悉 Istio 和 Envoy 中的 Wasm。 安装 TinyGo 0.21.0 并使用 Golang 构建 Wasm 扩展。  说明 在这个例子中，我们将在集群中部署两个应用程序（sleep 和 httpbin）。我们将从一个容器向另一个容器发送几个请求，而不部署任何 Wasm 扩展。\n接下来，我们将在 Go 中创建一个 Wasm 模块，为响应添加一个自定义头，并拒绝任何请求率超过每秒两个的请求。\n我们将把 Wasm 模块推送到 Docker 镜像仓库，并使用新的 WasmPlugin 资源，告诉 Istio 从哪里下载 Wasm 模块，以及将该模块应用于哪些工作负载。\n第 1 步：安装 Istio 并部署应用程序 首先，我们将下载并安装 Istio 1.12，并标记 Kubernetes 的 default 命名空间，以便自动注入 sidecar。\ncurl -L https://git.io/getLatestIstio | ISTIO_VERSION=1.12 sh cd istio-1.12/ ./bin/istioctl install --set profile=demo -y kubectl label namespace default istio-injection=enabled --overwrite 接下来，我们将部署 httpbin 和 sleep 应用程序的示例。\nkubectl apply -f samples/httpbin/httpbin.yaml kubectl apply -f samples/sleep/sleep.yaml 应用程序部署并运行后，我们将每秒从 sleep 容器向 httpbin 容器发送 4 个请求。\n$ SLEEP_POD=$(kubectl get pod -l app=sleep -o jsonpath={.items..metadata.name}) $ kubectl exec ${SLEEP_POD} -c sleep -- sh -c \u0026#39;for i in $(seq 1 3); do curl --head -s httpbin:8000/headers; sleep 0.25; done\u0026#39; HTTP/1.1 200 OK server: envoy date: Tue, 16 Nov 2021 22:18:32 GMT content-type: application/json content-length: 523 access-control-allow-origin: * access-control-allow-credentials: true x-envoy-upstream-service-time: 2 HTTP/1.1 200 OK server: envoy date: Tue, 16 Nov 2021 22:18:32 GMT content-type: application/json content-length: 523 access-control-allow-origin: * access-control-allow-credentials: true x-envoy-upstream-service-time: 4 HTTP/1.1 200 OK server: envoy date: Tue, 16 Nov 2021 22:18:32 GMT content-type: application/json content-length: 523 access-control-allow-origin: * access-control-allow-credentials: true x-envoy-upstream-service-time: 1 你会发现所有的请求都成功了，并返回了 HTTP 200。\n第 2 步：开发、编译和推送 Wasm 模块 我们将使用 Golang 和 Proxy Wasm Golang SDK 来开发 Wasm 模块。我们将使用 SDK 资源库中的一个现有例子，叫做 istio-rate-limiting。要开始，请先克隆 Github 仓库。\ngit clone https://github.com/tetratelabs/wasm-rate-limiting cd wasm-rate-limiting/ 我们来看看 main.go 中的代码。这就是我们使用 Proxy Wasm Golang SDK 实现速率限制逻辑的地方。Wasm 模块做了两件事。\n 在响应中添加一个自定义的头。 执行 2 个请求 / 秒的速率限制，拒绝超额的请求。  下面是 main.go 的片段，显示了功能是如何实现的。\n// Modify the header func (ctx *httpHeaders) OnHttpResponseHeaders(numHeaders int, endOfStream bool) types.Action { for key, value := range additionalHeaders { proxywasm.AddHttpResponseHeader(key, value) } return types.ActionContinue } // Perform rate limiting func (ctx *httpHeaders) OnHttpRequestHeaders(int, bool) types.Action { current := time.Now().UnixNano() // We use nanoseconds() rather than time.Second() because the proxy-wasm has the known limitation. \t// TODO(incfly): change to time.Second() once https://github.com/proxy-wasm/proxy-wasm-cpp-host/issues/199 \t// is resolved and released. \tif current \u0026gt; ctx.pluginContext.lastRefillNanoSec+1e9 { ctx.pluginContext.remainToken = 2 ctx.pluginContext.lastRefillNanoSec = current } proxywasm.LogCriticalf(\u0026#34;Current time %v, last refill time %v, the remain token %v\u0026#34;, current, ctx.pluginContext.lastRefillNanoSec, ctx.pluginContext.remainToken) if ctx.pluginContext.remainToken == 0 { if err := proxywasm.SendHttpResponse(403, [][2]string{ {\u0026#34;powered-by\u0026#34;, \u0026#34;proxy-wasm-go-sdk!!\u0026#34;}, }, []byte(\u0026#34;rate limited, wait and retry.\u0026#34;), -1); err != nil { proxywasm.LogErrorf(\u0026#34;failed to send local response: %v\u0026#34;, err) proxywasm.ResumeHttpRequest() } return types.ActionPause } ctx.pluginContext.remainToken -= 1 return types.ActionContinue } 在 OnHttpResponseHeaders 函数中，我们正在迭代 extraHeaders 变量，并将头文件添加到响应中。\n在 OnHttpRequestHeaders 函数中，我们得到当前的时间戳，将其与最后一次补给时间的时间戳进行比较（对于速率限制器），如果需要的话，就补给令牌。\n如果没有剩余的令牌，我们就发送一个带有额外头的 403 响应（由：proxy-wasm-go-sdk！！）。\n让我们用 tinygo 将 Golang 程序编译成 Wasm 模块，并将其打包成一个 Docker 镜像。\ntinygo build -o main.wasm -scheduler=none -target=wasi main.go 我们构建一个 Docker 镜像，并将其推送到镜像仓库（用 …","date":1637748180,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"d3575dbc379211b78ddb89305ae024f3","permalink":"https://lib.jimmysong.io/blog/istio-wasm-extensions-and-ecosystem/","publishdate":"2021-11-24T18:03:00+08:00","relpermalink":"/blog/istio-wasm-extensions-and-ecosystem/","section":"blog","summary":"Istio 1.12 中新的 WebAssembly 基础设施使其能够轻松地将额外的功能注入网格部署中。","tags":["Envoy","Wasm","Istio","Tetrate"],"title":"Istio 1.12 引入 Wasm 插件配置 API 用于扩展 Istio 生态","type":"post"},{"authors":["Steven Landow"],"categories":["Istio"],"content":"编者的话 本文译自 Istio 官方博客，博客原标题 gRPC Proxyless Service Mesh，其实是 Istio 1.11 版本中支持的实验特性，可以直接将 gRPC 服务添加到 Istio 中，而不需要再向 Pod 中注入 Envoy 代理。本文中还给出了一个 Demo 性能测试数据，这种做法可以极大的提升应用性能，降低网络延迟。\nIstio 使用一组发现 API（统称为 xDS API 来动态配置其 Envoy sidecar 代理。这些 API 的目标是成为一个 通用的数据平面 API。gRPC 项目对 xDS API 有很好的支持，也就是说你可以管理 gRPC 工作负载，而不需要同时部署 Envoy sidecar。你可以在 Megan Yahya 的 KubeCon EU 2021 演讲中了解更多关于该集成的信息。关于 gRPC 支持的最新情况，可以在他们的提案中找到，还有实现状态。\nIstio 1.11 增加了实验性支持，可以直接将 gRPC 服务添加到网格中。我们支持基本的服务发现，一些基于 VirtualService 的流量策略，以及双向 TLS。\n支持的功能 与 Envoy 相比，目前 gRPC 内的 xDS API 的实现在某些方面是有限的。以下功能应该可以使用，尽管这不是一个详尽的列表，其他功能可能部分可用。\n 基本的服务发现。你的 gRPC 服务可以接触到在网格中注册的其他 pod 和虚拟机。 DestinationRule  Subset：你的 gRPC 服务可以根据标签选择器将流量分割到不同的实例组。 目前唯一支持的 Istio loadBalancer 是 ROUND_ROBIN，consistentHash 将在未来的 Istio 版本中加入（支持 gRPC）。 tls 设置被限制为 DISABLE 或 ISTIO_MUTUAL。其他模式将被视为 DISABLE。   VirtualService  Header 匹配和 URI 匹配的格式为 /ServiceName/RPCName。 覆盖目标主机和子集。 加权的流量转移。   PeerAuthentication  只支持 DISABLE 和 STRICT。其他模式将被视为 DISABLE。 在未来的版本中可能会有对 auto-mTLS 的支持。    其他功能包括故障、重试、超时、镜像和重写规则，可能会在未来的版本中支持。其中一些功能正等待在 gRPC 中实现，而其他功能则需要在 Istio 中支持。gRPC 中 xDS 功能的状态可以在这里找到。Istio 的支持状况将存在于未来的官方文档中。\n这个功能是实验性的。标准的 Istio 功能将随着时间的推移和整体设计的改进而得到支持。\n架构概述    gRPC服务如何与istiod通信的示意图  虽然不使用 proxy 进行数据面通信，但它仍然需要一个 agent 来进行初始化和与控制面的通信。首先，agent 在启动时生成一个引导文件，与为 Envoy 生成引导文件的方式相同。这告诉 gRPC 库如何连接到 istiod，在哪里可以找到数据面通信的证书，以及向控制面发送什么元数据。接下来，agent 作为一个 xDS proxy，代表应用程序与 istiod 进行连接和认证。最后，agent 获取并轮换数据平面通信中使用的证书。\n对应用程序代码的修改 本节介绍了 gRPC 在 Go 中的 xDS 支持。其他语言也有类似的 API。\n为了启用 gRPC 中的 xDS 功能，你的应用程序必须做一些必要的修改。你的 gRPC 版本应该至少是 1.39.0。\n客户端 下面的导入将在 gRPC 中注册 xDS 解析器和均衡器。它应该被添加到你的主包或调用 grpc.Dial 的同一个包中。\nimport _ \u0026#34;google.golang.org/grpc/xds\u0026#34; 当创建一个 gRPC 连接时，URL 必须使用 xds:/// scheme。\nconn, err := grpc.DialContext(ctx, \u0026#34;xds:///foo.ns.svc.cluster.local:7070\u0026#34;) 此外，为了支持（m）TLS，必须向 DialContext 传递一个特殊的 TransportCredentials 选项。FallbackCreds 允许我们在 istiod 不发送安全配置时成功。\nimport \u0026#34;google.golang.org/grpc/credentials/xds\u0026#34; ... creds, err := xds.NewClientCredentials(xds.ClientOptions{ FallbackCreds: insecure.NewCredentials() }) // handle err conn, err := grpc.DialContext( ctx, \u0026#34;xds:///foo.ns.svc.cluster.local:7070\u0026#34;, grpc.WithTransportCredentials(creds), ) 服务端 为了支持服务器端的配置，如 mTLS，必须做一些修改。\n首先，我们使用一个特殊的构造函数来创建 GRPCServer。\nimport \u0026#34;google.golang.org/grpc/xds\u0026#34; ... server = xds.NewGRPCServer() RegisterFooServer(server, \u0026amp;fooServerImpl) 如果你的 protoc 生成的 Go 代码已经过期，你可能需要重新生成，以便与 xDS 服务器兼容。你生成的 RegisterFooServer 函数应该像下面这样。\nfunc RegisterFooServer(s grpc.ServiceRegistrar, srv FooServer) { s.RegisterService(\u0026amp;FooServer_ServiceDesc, srv) } 最后，与客户端的变化一样，我们必须启用安全支持。\ncreds, err := xds.NewServerCredentials(xdscreds.ServerOptions{FallbackCreds: insecure.NewCredentials()}) // handle err server = xds.NewGRPCServer(grpc.Creds(creds)) 在你的 Kubernetes 部署中 假设你的应用代码是兼容的，Pod 只需要注释 inject.istio.io/templates：grpc-agent。这增加了一个运行上述代理的 sidecar 容器，以及一些环境变量，gRPC 使用这些变量来寻找引导文件并启用某些功能。\n对于 gRPC 服务端，你的 Pod 也应该用 proxy.istio.io/config: \u0026#39;{\u0026#34;holdApplicationUntilProxyStarts\u0026#34;: true}\u0026#39; 来注释，以确保在你的 gRPC 服务端初始化之前，代理中的 xDS 代理和引导文件已经准备就绪。\n例子 在本指南中，你将部署 echo，一个已经支持服务器端和客户端无代理的 gRPC 的应用。通过这个应用程序，你可以尝试一些支持的流量策略，启用 mTLS。\n先决条件 本指南要求在进行之前安装 Istio（1.11+）控制平面。\n部署应用程序 创建一个支持注入的命名空间 echo-grpc。接下来部署两个 echo 应用程序的实例以及服务。\n$ kubectl create namespace echo-grpc $ kubectl label namespace echo-grpc istio-injection=enabled $ kubectl -n echo-grpc apply -f samples/grpc-echo/grpc-echo.yaml 确保两个 Pod 正在运行。\n$ kubectl -n echo-grpc get pods NAME READY STATUS RESTARTS AGE echo-v1-69d6d96cb7-gpcpd 2/2 Running 0 58s echo-v2-5c6cbf6dc7-dfhcb 2/2 Running 0 58s 测试 gRPC 解析器 首先，将 17171 端口转发到其中一个 Pod 上。这个端口是一个非 xDS 支持的 gRPC 服务端，允许从端口转发的 Pod 发出请求。\n$ kubectl -n echo-grpc port-forward $(kubectl -n echo-grpc get pods -l version=v1 -ojsonpath=\u0026#39;{.items[0].metadata.name}\u0026#39;) 17171 \u0026amp; 接下来，我们可以发送一批 5 个请求。\n$ grpcurl -plaintext -d \u0026#39;{\u0026#34;url\u0026#34;: \u0026#34;xds:///echo.echo-grpc.svc.cluster.local:7070\u0026#34;, \u0026#34;count\u0026#34;: 5}\u0026#39; :17171 proto.EchoTestService/ForwardEcho | jq -r \u0026#39;.output | join(\u0026#34;\u0026#34;)\u0026#39; | grep Hostname Handling connection for 17171 [0 body] Hostname=echo-v1-7cf5b76586-bgn6t [1 body] Hostname=echo-v2-cf97bd94d-qf628 [2 body] Hostname=echo-v1-7cf5b76586-bgn6t [3 body] Hostname=echo-v2-cf97bd94d-qf628 [4 body] Hostname=echo-v1-7cf5b76586-bgn6t 你也可以使用类似 Kubernetes 名称解析的短名称。\n$ grpcurl -plaintext -d \u0026#39;{\u0026#34;url\u0026#34;: \u0026#34;xds:///echo:7070\u0026#34;}\u0026#39; :17171 proto.EchoTestService/ForwardEcho | jq -r \u0026#39;.output | join (\u0026#34;\u0026#34;)\u0026#39; | grep Hostname [0 body] Hostname=echo-v1-7cf5b76586-ltr8q $ grpcurl -plaintext -d \u0026#39;{\u0026#34;url\u0026#34;: \u0026#34;xds:///echo.echo-grpc:7070\u0026#34;}\u0026#39; :17171 proto.EchoTestService/ForwardEcho | jq -r \u0026#39;.output | join(\u0026#34;\u0026#34;)\u0026#39; | grep Hostname [0 body] Hostname=echo-v1-7cf5b76586-ltr8q $ grpcurl -plaintext -d \u0026#39;{\u0026#34;url\u0026#34;: \u0026#34;xds:///echo.echo-grpc.svc:7070\u0026#34;}\u0026#39; :17171 proto.EchoTestService/ForwardEcho | jq -r \u0026#39;.output | join(\u0026#34;\u0026#34;)\u0026#39; | grep Hostname [0 body] Hostname=echo-v2-cf97bd94d-jt5mf 用目的地规则创建子集 首先，为每个版本的工作负载创建一个子集。\n$ cat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: echo-versions namespace: echo-grpc spec: host: echo.echo-grpc.svc.cluster.local subsets: - name: v1 labels: version: v1 …","date":1637632980,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"ebc0030af70ad52bba7c388c2d327abe","permalink":"https://lib.jimmysong.io/blog/grpc-proxyless-service-mesh/","publishdate":"2021-11-23T10:03:00+08:00","relpermalink":"/blog/grpc-proxyless-service-mesh/","section":"blog","summary":"编者的话 本文译自 Istio 官方博客，博客原标题 gRPC Proxyless Service Mesh，其实是 Istio 1.11 版本中支持的实验特性，可以直接将 gRPC 服务添加到 Istio 中，而不需要再向 Pod 中注入 Envoy 代理。本文中还给出了一个 Demo 性能测试数据，这种做法可以极大的提升应","tags":["Istio","grpc","service mesh"],"title":"基于 gRPC 和 Istio 的无 sidecar 代理的服务网格","type":"post"},{"authors":["Liz Rice"],"categories":["Service Mesh"],"content":"前言 今天有几个服务网格的产品和项目，承诺简化应用微服务之间的连接，同时提供额外的功能，如安全连接、可观测性和流量管理。但正如我们在过去几年中反复看到的那样，对服务网格的兴奋已经被对额外的复杂性和开销的实际担忧所抑制。让我们来探讨一下 eBPF 是如何让我们精简服务网格，使服务网格的数据平面更有效率，更容易部署。\nSidecar 问题 今天的 Kubernetes 服务网格解决方案要求你在每一个应用 pod 上添加一个代理 sidecar 容器，如 Envoy 或 Linkerd-proxy。这是正确的：即使在一个非常小的环境中，比如说有 20 个服务，每个服务运行五个 pod，分布在三个节点上，你也有 100 个代理容器。无论代理的实现多么小和有效，这种纯粹的重复都会耗费资源。\n每个代理使用的内存与它需要能够通信的服务数量有关。Pranay Singhal 写了他配置 Istio 的经验，将每个代理的消耗从 1GB 左右减少到更合理的 60-70MB。但是，即使在我们的小环境中，在三个节点上有 100 个代理，这种优化配置仍然需要每个节点 2GB 左右。\n   来自redhat.com/architect/why-when-service-mesh——每个微服务都有自己的代理sidecar  为什么我们需要所有这些 sidecar？这种模式允许代理容器与 pod 中的应用容器共享一个网络命名空间。网络命名空间是 Linux 内核的结构，它允许容器和 pod 拥有自己独立的网络堆栈，将容器化的应用程序相互隔离。这使得应用之间互不相干，这就是为什么你可以让尽可能多的 pod 在 80 端口上运行一个 web 应用 —— 网络命名空间意味着它们各自拥有自己的 80 端口。代理必须共享相同的网络命名空间，这样它就可以拦截和处理进出应用容器的流量。\n引入 eBPF eBPF 是一种内核技术，允许自定义程序在内核中运行。这些程序在响应事件时运行，有成千上万个可能的事件，eBPF 程序可以被附加到这些事件上。这些事件包括轨迹点、进入或退出任何功能（在内核或用户空间）或对服务网格来说很重要的 —— 抵达的网络数据包。\n重要的是，每个节点只有一个内核；在一个节点上运行的所有容器（也就是所有的 pod）共享同一个内核。如果你在内核中添加一个 eBPF 程序到一个事件中，它将被触发，无论哪个进程引起该事件，无论它是在应用容器中运行还是直接运行在主机上。\n   每台主机一个内核  这就是为什么 eBPF 对于 Kubernetes 中的任何一种 instrumentation 来说都是如此令人兴奋的技术 —— 你只需要在每个节点上添加一次 instrumentation ，所有的应用程序 pod 都会被覆盖。无论你是在寻求可观测性、安全性还是网络，由 eBPF 驱动的解决方案都可以在不需要 sidecar 的情况下对应用进行检测。\n基于 eBPF 的 Cilium 项目（最近 以孵化级别加入云计算基金会）将这种 “无 sidecar\u0026#34; 模式带到了服务网格的世界。除了传统的 sidecar 模型，Cilium 还支持每个节点使用一个 Envoy 代理实例运行服务网格的数据平面。使用我们前面的例子，这就把代理实例的数量从 100 个减少到只有 3 个。\n   用无sidecar代理模式减少代理实例  减少 YAML 在 sidecar 模型中，指定每个应用 pod 的 YAML 需要被修改以添加 sidecar 容器。这通常是自动化的 —— 例如，使用一个 mutating webhook，在每个应用 pod 部署的时候注入 sidecar。\n以 Istio 为例，这需要标记 Kubernetes 命名空间和 / 或 pod，以定义是否应该注入 sidecar—— 当然也需要为集群启用 mutating webhook。\n但如果出了问题怎么办？如果命名空间或 pod 的标签不正确，那么 sidecar 将不会被注入，pod 将不会被连接到服务网格。更糟糕的是，如果攻击者破坏了集群，并能够运行一个恶意的工作负载 —— 例如，一个加密货币矿工，他们将不太可能标记它，以便它加入服务网格。它不会通过服务网格提供的流量观察能力而被发现。\n相比之下，在支持 eBPF 的无 sidecar 代理模型中，pod 不需要任何额外的 YAML 就可以被检测。相反，一个 CRD 被用来在集群范围内配置服务网格。即使是已经存在的 pod 也可以成为服务网格的一部分，而不需要重新启动。\n如果攻击者试图通过直接在主机上运行工作负载来绕过 Kubernetes 编排，eBPF 程序可以检测并控制这一活动，因为这一切都可以从内核看到。\neBPF 支持的网络效率 支持 eBPF 的网络允许数据包走捷径，绕过内核的部分网络堆栈，这可以使 Kubernetes 网络的性能得到显著改善。让我们看看这在服务网格数据平面中是如何应用的。\n   在eBPF加速、无sidecar的服务网格模型中，网络数据包通过的路径要短得多  在服务网格的情况下，代理在传统网络中作为 sidecar 运行，数据包到达应用程序的路径相当曲折：入站数据包必须穿越主机 TCP/IP 栈，通过虚拟以太网连接到达 pod 的网络命名空间。从那里，数据包必须穿过 pod 的网络堆栈到达代理，代理将数据包通过回环接口转发到应用程序。考虑到流量必须在连接的两端流经代理，与非服务网格流量相比，这将导致延迟的显著增加。\n基于 eBPF 的 Kubernetes CNI 实现，如 Cilium，可以使用 eBPF 程序，明智地钩住内核中的特定点，沿着更直接的路线重定向数据包。这是可能的，因为 Cilium 知道所有的 Kubernetes 端点和服务的身份。当数据包到达主机时，Cilium 可以将其直接分配到它所要去的代理或 Pod 端点。\n网络中的加密 如果一个网络解决方案能够意识到 Kubernetes 服务，并在这些服务的端点之间提供网络连接，那么它能够提供服务网格数据平面的能力就不足为奇。但这些能力可以超越基本的连接。一个例子是透明加密。\n通常使用服务网格来确保所有的应用流量都是经过认证和加密的。这是通过双向 TLS（mTLS）实现的；服务网格代理组件作为网络连接的端点，并与其远程对等物协商一个安全的 TLS 连接。这种连接对代理之间的通信进行加密，而不需要对应用程序做任何改变。\n但在应用层管理的 TLS 并不是实现组件间认证和加密流量的唯一方法。另一个选择是在网络层加密流量，使用 IPSec 或 WireGuard。因为它在网络层操作，这种加密不仅对应用程序完全透明，而且对代理也是透明的 —— 它可以在有或没有服务网格时启用。如果你使用服务网格的唯一原因是提供加密，你可能想考虑网络级加密。它不仅更简单，而且还可以用来验证和加密节点上的任何流量 —— 它不只限于那些启用了 sidecar 的工作负载。\neBPF 是服务网格的数据平面 现在，eBPF 在 Linux 生产发行版使用的内核版本中得到广泛支持，企业可以利用它来获得更有效的网络解决方案，并作为服务网格的更有效的数据平面。\n去年，我代表 CNCF 的技术监督委员会，对服务网格领域的整合和清晰化做了一些 预测。在同一主题演讲中，我谈到 eBPF 有可能成为更多项目和更广泛部署能力的基础。这两个想法现在正结合在一起，因为 eBPF 似乎是服务网格数据平面的自然路径。\n","date":1635310800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"e0f9521495f99f03108aa8cab4088bb7","permalink":"https://lib.jimmysong.io/blog/how-ebpf-streamlines-the-service-mesh/","publishdate":"2021-10-27T13:00:00+08:00","relpermalink":"/blog/how-ebpf-streamlines-the-service-mesh/","section":"blog","summary":"本文探讨一下eBPF是如何让我们精简服务网格，使服务网格的数据平面更有效率，更容易部署。","tags":["Service Mesh","ebpf"],"title":"eBPF 如何简化服务网格","type":"post"},{"authors":["宣超","胡凯","谭崇康","唐博","林文炜","张凯豪"],"categories":null,"content":"讲师分享  云原生社区 meetup 第八期上海站开场，郭旭东 云原生 2.0 华为云赋能 “新云原生企业”，张凯豪 蚂蚁万级规模 K8s 集群 etcd 架构优化实践 —ETCD on OceanBase，宣超 新一代开源 HCI 底层原理剖析，胡凯 攀登规模化的高峰 —— 蚂蚁集团大规模 Sigma 集群 ApiServer 优化实践，唐博，谭崇康 云原生分布式存储 Rook 及其在企业中应用的未来，林文炜  ","date":1634965200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"93a357e6fddc8d2c7fa611b7e08c92b2","permalink":"https://lib.jimmysong.io/event/cloud-native-meetup-shanghai-08/","publishdate":"2021-10-23T20:00:00+08:00","relpermalink":"/event/cloud-native-meetup-shanghai-08/","section":"event","summary":"本次活动聚焦于 OceanBase、Rook、Sigma 等。","tags":["OceanBase","Rook"],"title":"云原生社区 meetup 第八期上海站","type":"event"},{"authors":["Srini Penchikala"],"categories":["service mesh"],"content":"主要收获  了解采用服务网格技术的新兴架构趋势，特别是多云、多集群和多租户模式，如何在异构基础设施（裸机、虚拟机和 Kubernetes）中部署服务网格解决方案，以及从边缘计算层到网格的应用 / 服务连接。 了解服务网格生态系统中的一些新模式，如多集群服务网格、媒体服务网格（Media Service Mesh）和混沌网格，以及经典的微服务反模式，如 “死星（Death Star） “架构。 获取最新的关于在部署领域使用服务网格的创新总结，在 Pod（K8s 集群）和 VM（非 K8s 集群）之间进行快速实验、混乱工程和金丝雀部署。 探索服务网格扩展领域的创新，包括：增强身份管理，以确保微服务连接的安全性，包括自定义证书授权插件，自适应路由功能，以提高服务的可用性和可扩展性，以及增强 sidecar 代理。 了解操作方面即将出现的情况，如配置多集群功能和将 Kubernetes 工作负载连接到托管在虚拟机基础设施上的服务器，以及管理多集群服务网格中所有功能和 API 的开发者门户。  在过去的几年里，服务网格技术有了长足的发展。服务网格在各组织采用云原生技术方面发挥着重要作用。通过提供四种主要能力 —— 连接性、可靠性、可观测性和安全性，服务网格已经成为 IT 组织的技术和基础设施现代化工作的核心组成部分。服务网格使开发和运维团队能够在基础设施层面实现这些能力，因此，当涉及到跨领域的非功能需求时，应用团队不需要重新发明轮子。\n自本文第一版于 2020 年 2 月发表以来，服务网格技术经历了重大创新，在不断发展的服务网格领域出现了一些新的架构趋势、技术能力和服务网格项目。\n在过去的一年里，服务网格产品的发展远远超过了原有的 Kubernetes 解决方案，没有托管在 Kubernetes 平台上的应用无法利用服务网格。并非所有的组织都将其所有的业务和 IT 应用程序过渡到 Kubernetes 云平台。因此，自服务网格诞生以来，一直需要这项技术在不同的 IT 基础设施环境中工作。\n随着微服务架构的不断采用，应用系统在云供应商、基础设施（Kubernetes、虚拟机、裸机服务器）、地域，甚至在服务网格集成环境中要管理的工作负载类型方面，都已实现解耦和分布式。\n让我们从服务网格的历史开始说起，了解服务网格是如何产生的。\n2016 年前后，“服务网格 \u0026#34; 这个词出现在微服务、云计算和 DevOps 的领域。Buoyant 团队在 2016 年用这个词来解释他们的产品 Linkerd。和云计算领域的许多概念一样，相关的模式和技术其实有很长的历史。\n服务网格的到来主要是由于 IT 领域内的一场风暴。开发人员开始使用多语言（polyglot）方法构建分布式系统，并需要动态服务发现。运维部门开始使用短暂的基础设施，并希望优雅地处理不可避免的通信故障和执行网络策略。平台团队开始接受像 Kubernetes 这样的容器编排系统，并希望使用现代 API 驱动的网络代理（如 Envoy）在系统中和周围动态地路由流量。\n本文旨在回答软件架构师和技术负责人的相关问题，如：什么是服务网格？我是否需要服务网格？如何评估不同的服务网格产品？\n服务网格模式 服务网格模式专注于管理分布式软件系统中所有服务之间的通信。\n背景介绍 该模式的背景有两个方面。首先，工程师们已经采用了微服务架构模式，并通过将多个（理想情况下是单一用途且可独立部署的）服务组合在一起构建他们的应用。第二，组织已经接受了云原生平台技术，如容器（如 Docker）、编排器（如 Kubernetes）和网关。\n意图 服务网格模式试图解决的问题包括：\n 消除了将特定语言的通信库编译到单个服务中的需求，以处理服务发现、路由和应用层（第 7 层）非功能通信要求。 外部化服务通信配置，包括外部服务的网络位置、安全凭证和服务质量目标。 提供对其他服务的被动和主动监测。 在整个分布式系统中分布式地执行策略。 提供可观测性的默认值，并使相关数据的收集标准化。  启用请求记录 配置分布式追踪 收集指标    结构 服务网格模式主要侧重于处理传统上被称为 “东西向 “的基于远程过程调用（RPC）的流量：请求 / 响应类型的通信，源自数据中心内部，在服务之间传播。这与 API 网关或边缘代理相反，后者被设计为处理 “南北 “流量。来自外部的通信，进入数据中心内的一个终端或服务。\n服务网格的特点 服务网格的实施通常会提供以下一个或多个功能：\n 规范化命名并增加逻辑路由，（例如，将代码级名称 “用户服务 \u0026#34; 映射到平台特定位置 “AWS-us-east-1a/prod/users/v4”。 提供流量整形和流量转移 保持负载均衡，通常采用可配置的算法 提供服务发布控制（例如，金丝雀释放和流量分割） 提供按请求的路由（例如，影子流量、故障注入和调试重新路由）。 增加基线可靠性，如健康检查、超时 / 截止日期、断路和重试（预算）。 通过透明的双向传输级安全（TLS）和访问控制列表（ACL）等策略，提高安全性 提供额外的可观测性和监测，如顶线指标（请求量、成功率和延迟），支持分布式追踪，以及 “挖掘” 和检查实时服务间通信的能力。 使得平台团队能够配置 \u0026#34; 理智的默认值”，以保护系统免受不良通信的影响。  服务网格的能力可分为以下四个方面：\n 连接性 可靠性 安全性 可观测性  让我们看看服务网格技术在这些领域都能提供哪些功能。\n连接性\n 流量控制（路由，分流） 网关（入口、出口） 服务发现 A/B 测试、金丝雀 服务超时、重试  可靠性\n 断路器 故障注入 / 混沌测试  安全性\n 服务间认证（mTLS） 证书管理 用户认证（JWT） 用户授权（RBAC） 加密  可观测性\n 监测 遥测、仪表、计量 分布式追踪 服务图表  服务网格架构：内部原理 服务网格由两部分组成：数据平面和控制平面。Matt Klein，Envoy Proxy 的作者，写了一篇关于 “ 服务网格数据平面与控制平面 “的深入探讨。\n广义上讲，数据平面 “执行工作”，负责 “有条件地翻译、转发和观察流向和来自 [网络终端] 的每个网络数据包”。在现代系统中，数据平面通常以代理的形式实现，（如 Envoy、HAProxy 或 MOSN），它作为 “sidecar” 与每个服务一起在进程外运行。Linkerd 使用了一种 微型代理方法，该方法针对服务网格的使用情况进行了优化。\n控制平面 “监督工作”，并将数据平面的所有单个实例 —— 一组孤立的无状态 sidecar 代理变成一个分布式系统。控制平面不接触系统中的任何数据包 / 请求，相反，它允许人类运维人员为网格中所有正在运行的数据平面提供策略和配置。控制平面还能够收集和集中数据平面的遥测数据，供运维人员使用。\n控制平面和数据平面的结合提供了两方面的优势，即策略可以集中定义和管理，同时，同样的政策可以以分散的方式，在 Kubernetes 集群的每个 pod 中本地执行。这些策略可以与安全、路由、断路器或监控有关。\n下图取自 Istio 架构文档，虽然标注的技术是 Istio 特有的，但这些组件对所有服务网格的实现都是通用的。\n   Istio 架构  Istio 架构，展示了控制平面和代理数据平面的交互方式（由 Istio 文档提供）。\n使用案例 服务网格可以实现或支持多种用例。\n动态服务发现和路由 服务网格提供动态服务发现和流量管理，包括用于测试的流量影子（复制），以及用于金丝雀发布和 A/B 实验的流量分割。\n服务网格中使用的代理通常是 “应用层 \u0026#34; 感知的（在 OSI 网络堆栈的第 7 层运行）。这意味着流量路由决策和指标的标记可以利用 HTTP 头或其他应用层协议元数据。\n服务间通信可靠性 服务网格支持跨领域的可靠性要求的实施和执行，如请求重试、超时、速率限制和断路。服务网格经常被用来补偿（或封装）处理分布式计算的八个谬误。应该注意的是，服务网格只能提供 wire-level 的可靠性支持（如重试 HTTP 请求），最终服务应该对相关的业务影响负责，如避免多个（非幂等的）HTTP POST 请求。\n流量的可观测性 由于服务网格处于系统内处理的每个请求的关键路径上，它还可以提供额外的 “可观测性”，例如请求的分布式追踪、HTTP 错误代码的频率以及全局和服务间的延迟。虽然在企业领域是一个被过度使用的短语，但服务网格经常被提议作为一种方法来捕获所有必要的数据，以实现整个系统内流量的统一界面视图。\n通信安全 服务网格还支持跨领域安全要求的实施和执行，如提供服务身份（通过 x509 证书），实现应用级服务 / 网络分割（例如，“服务 A\u0026#34; 可以与 “服务 B “通信，但不能与 “服务 C “通信），确保所有通信都经过加密（通过 TLS），并确保存在有效的用户级身份令牌或 “护照 “。\n反模式 当反模式的使用出现时，这往往是一个技术成熟的标志。服务网格也不例外。\n太多的流量管理层次 当开发人员不与平台或运维团队协商，并在现在通过服务网格实现的代码中重复现有的通信处理逻辑时，就会出现这种反模式。例如，除了服务网格提供的 wire-level 重试策略外，应用程序还在代码中还实现了重试策略。这种反模式会导致重复的事务等问题。\n服务网格银弹 在 IT 领域没有 “银弹 “这样的东西，但供应商有时会被诱惑给新技术贴上这个标签。服务网格不会解决微服务、Kubernetes 等容器编排器或云网络的所有通信问题。服务网格的目的只是促进服务件（东西向）的通信，而且部署和运行服务网格有明显的运营成本。\n企业服务总线（ESB）2.0 在前微服务面向服务架构（SOA）时代，企业服务总线（ESB）实现了软件组件之间的通信系统。有些人担心 ESB 时代的许多错误会随着服务网格的使用而重演。\n通过 ESB 提供的集中的通信控制显然有价值。然而，这些技术的发展是由供应商推动的，这导致了多种问题，例如：ESB 之间缺乏互操作性，行业标准的定制扩展（例如，将供应商的特定配置添加到 WS-* 兼容模式中），以及高成本。ESB 供应商也没有做任何事情来阻止业务逻辑与通信总线的集成和紧耦合。\n大爆炸部署 在整个 IT 界有一种诱惑，认为大爆炸式的部署方法是最容易管理的方法，但正如 Accelerate 和 DevOps 报告的研究，事实并非如此。由于服务网格的全面推广意味着这项技术处于处理所有终端用户请求的关键路径上，大爆炸式的部署是非常危险的。\n死星建筑 当企业采用微服务架构，开发团队开始创建新的微服务或在应用中利用现有的服务时，服务间的通信成为架构的一个关键部分。如果没有一个良好的治理模式，这可能会导致不同服务之间的紧密耦合。当整个系统在生产中出现问题时，也将很难确定哪个服务出现了问题。\n如果缺乏服务沟通战略和治理模式，该架构就会变成所谓的 “死星架构”。\n关于这种架构反模式的更多信息，请查看关于云原生架构采用的第一部分、第二部分和第三部分的文章。\n特定领域的服务网格 服务网格的本地实现和过度优化有时会导致服务网格部署范围过窄。开发人员可能更喜欢针对自己的业务领域的服务网格，但这种方法弊大于利。我们不希望实现过于细化的服务网格范围，比如为组织中的每个业务或功能域（如财务、人力资源、会计等）提供专用的服务网格。这就违背了拥有像服务网格这样的通用服务协调解决方案的目的，即企业级服务发现或跨域服务路由等功能。\n服务网格的实现和产品 以下是一份非详尽的当前服务网格实施清单。\n Linkerd (CNCF 毕业项目) Istio Consul Kuma（CNCF 沙盒项目） AWS App Mesh NGINX Service Mesh AspenMesh Kong Solo Gloo Mesh Tetrate Service Bridge Traefik Mesh（原名 Maesh） Meshery Open Service MEsh（CNCF 沙盒项目）  另外，像 DataDog …","date":1633917600,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"e9357e83d1bf9c408dd94002f661f3e1","permalink":"https://lib.jimmysong.io/blog/service-mesh-ultimate-guide-e2/","publishdate":"2021-10-11T10:00:00+08:00","relpermalink":"/blog/service-mesh-ultimate-guide-e2/","section":"blog","summary":"本文是 InfoQ 自 2020 年 2 月发表的服务网格终极指南后的第二版，发布于 2021 年 9 月。","tags":["service mesh"],"title":"服务网格终极指南第二版——下一代微服务开发","type":"post"},{"authors":["董志勇","汪晟杰","黄金浩","田甜","邓洪超"],"categories":null,"content":"讲师分享  云原生社区 meetup 第七期深圳站开场致辞 - 宋净超 使用 IAST 构建高效的 DevSecOps 流程 - 董志勇 云原生场景下的开发和调试-汪晟杰，黄金浩 Envoy 在腾讯游戏云原生平台应用 - 田甜 使用 KubeVela 构建混合云应用管理平台 - 邓洪超  ","date":1632546000,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"d1047a20179fc46964691ba7b24a0887","permalink":"https://lib.jimmysong.io/event/cloud-native-meetup-shenzhen-07/","publishdate":"2021-09-25T18:00:00+08:00","relpermalink":"/event/cloud-native-meetup-shenzhen-07/","section":"event","summary":"本次活动关注于 DevSecOps、Envoy、KubeVela、Nocalhost 等。","tags":["DevSecOps","Envoy","KubeVela","Nocalhost"],"title":"云原生社区 meetup 第七期深圳站","type":"event"},{"authors":["Matt Klein"],"categories":["envoy"],"content":"编者的话 本文译自 Envoy 代理的创始人 Matt Klein 于昨晚在个人博客上发布的文章 5 year of Envoy OSS。他在 Twitter 因为自己的程序 bug 造成重大事故而离职，后加入 Lyft，在开源 Envoy 之前几乎没有贡献和管理开源项目的经验，这篇文章分享了他个人及 Envoy 开源的心路历程，在投身开源 Envoy 还是为雇主 Lyft 效命，该如何抉择？看完本文，相信对于开源项目的维护者、创业者及投资人都会大有收获。\n前言 今天是 Envoy Proxy 开源的 5 周年。毫不夸张地说，在专业方面，过去的 5 年是一个史诗般的过山车，我的情绪介于兴奋、自豪、焦虑、尴尬、无聊、倦怠之间。我想分享一下这个项目的前传和历史，以及我在发展大型开源软件项目的过程中所学到的一些经验教训。\n前传和历史 前传 除了一些小的弯路，我在技术行业二十年的职业生涯一直专注于底层系统：嵌入式系统，操作系统，虚拟化，文件系统，以及最近的分布式系统网络。我的分布式系统网络之旅始于 2010 年初在亚马逊，我有幸帮助开发了第一批高性能计算（HPC）EC2 实例类型。我学到了大量的底层高性能计算机网络知识，尽管我对分布式系统的概念接触有限。\n2012 年，我加入了 Twitter，在经历了几次错误的开始后，我最终加入了边缘网络团队。这是我第一次真正接触到分布式系统应用网络概念。我领导了一个新的 HTTP 边缘代理的开发，称为 Twitter 流式聚合器（TSA），它在 2013 年首次推出，以扩大 Twitter 的 “firehose” API（流式所有推文）的交付。在 2014 年世界杯前夕，我们决定将 TSA 作为一个通用的 HTTP/HTTP2/TLS 边缘代理，在靠近巴西赛事的存在点（POPs）推出。这样做的主要原因是不可能在 POP 的少量主机托管机架上部署现有的基于 JVM 的资源匮乏的边缘代理。项目周期特别紧张，我的团队成功地完成了一届没有事故的世界杯。（我还清楚地记得有一段时间，当软件崩溃时，不管是什么时候，我都会给自己打上一页，修复错误，然后重新进行金丝雀部署，继续测试）。在 Twitter 工作期间，我还接触到了该公司通过 Finagle 库进行服务间网络通信的方式，并取得了巨大成功。\n2015 年元旦前后，我在 Twitter 的日子里，因为我写的一个 bug，TSA 系统故障导致数百万 Twitter 的安卓用户被下线，这将是我在 Twitter 工作的尾声。\n   Matt Klein 发表的推特  加入 Lyft 和创建 “Lyft 代理” 我在 2015 年春天离开了 Twitter，部分原因是下线事件的影响，部分原因是对没有得到晋升的挫败感，部分原因是想尝试新的东西。我跟着我的老板从 Twitter 到了 Lyft，还有我在 Twitter 的其他同事。\n当我加入 Lyft 时，公司规模相对较小（少于 100 名工程师），并且正在努力从单体架构迁移到微服务架构。我已经多次谈到了 Envoy 的这部分历程，所以我不会再重述，在此简短的总结下，Lyft 遇到了所有典型的微服务迁移问题，主要是源于网络和可观测性。此外，Lyft 已经是 “多面手”（使用多种语言和框架），所以使用基于库的解决方案来解决这些问题似乎不切实际。因此，根据我以前建立 TSA 的经验和观察服务间通信在 Twitter 的工作方式，由于得到在 Lyft 的前 Twitter 同事们的信任，我提议建立一个新的应用网络系统，称为 “Lyft 代理”。\n经过一些激烈的讨论，包括新的代理是否应该用 Python 构建（是的，真的），我们就项目的大致轮廓达成一致，并决定使用 C++ 作为实现语言。在当时，C++ 似乎是唯一合理的选择。今天我还会选择 C++ 吗？然而，如今已经不是 2015 年初了。\n如果不说 “Envoy\u0026#34; 这个名字的由来，这部分的历史就不完整了。我们正在为这个项目建立最初的开发脚手架的时候，一个有远见的同事（Ryan Lane）说，我们不能把这个新项目叫做 “Lyft 代理”，我们必须选择一个更好的名字。我总是很实际，就去找辞典，查了一下 “代理”，然后决定用 Envoy 作为新名字。\n在 Lyft 上线 直到 2015 年夏天，我才开始认真地研究 Envoy 的源代码。那几个月是我职业生涯中最有趣的几个月。我们应该珍惜这段初创时期，因为它不会持续很久。我花了很长时间，争取在合理的时间内（根据我的定义，这种类型的项目需要 3-4 个月的时间）做出能给 Lyft 带来价值的东西。俗话说，Lyft 给了我大量的绳子来吊死自己，而我致力于确保这种吊死不会发生。\n当然，我的效率主要归功于刚从压缩的开发时间表和许多错误（主要是我自己的）中走出来，在 Twitter 的 TSA。我知道哪些错误是不能犯的，哪些抽象是需要的，哪些测试有效，哪些无效，等等。\n2015 年秋天准备投入生产的 Envoy 的最初版本只包含了该项目今天所包含的功能和复杂性的一小部分。它不支持 TLS，只支持 HTTP/1，并且有极其简单的路由和弹性功能。它所拥有的是你今天所看到的东西的骨架。在这个项目的历史上，很少有重大的重构，主要是因为，正如我之前所说的，我知道将要发生什么，以及为了支持这些功能，需要有哪些抽象。Envoy 从一开始就拥有一流的可观测性输出，以指标和日志的形式。在 2021 年，这种类型的网络可观测性是桌面上的赌注（这在很大程度上要归功于 Envoy 的成功），但在当时却不是这样。\nEnvoy 最初是作为边缘代理在 Lyft 上线的，位于提供 TLS 终止的 AWS ELB 后面。到 2015 年秋末，Envoy 为 Lyft 的 100% 流量提供服务，该系统产生的边缘仪表盘立即得到了回报（例如，提供 API 调用百分点延迟直方图，每个终端的成功率和请求率等）。\n在最初推出后不久，另一位 Twitter 同事（Bill Gallagher）加入了我的项目，我们迅速增加了一些功能，如 TLS 终止、HTTP/2 支持、更多路由和负载平衡功能等。\n与此同时，Lyft 基于 Envoy 的 “服务网格 \u0026#34; 也开始成形了。首先，Envoy 被部署在 PHP 单片机旁边，以取代 HAProxy 及其一些固有的运维问题（例如，当时 HAProxy 仍然是单线程的），以帮助 MongoDB 的代理。可以毫不夸张地说，Envoy 的早期开发有很大一部分是针对 MongoDB 的稳定性（负载均衡、速率限制、可观测性等）。\n基于 Envoy 的边缘机群和单体之间的直接观察能力的好处是非常明显的。不久之后，我们在一些高 RPS 分解的微服务旁边部署了 Envoy，以帮助排除网络问题。这方面的价值也得到了证明。随着时间的推移，我们超越了对可观测性的关注，增加了帮助系统可靠性的功能，如直接连接和服务发现（跳过内部 ELB）、异常值检测、健康检查、重试、断路等。Lyft 的基于负载的重大事件的数量从每 1-2 周一次慢慢减少。当然，Envoy 不能将所有此类事件的减少归功于此，但它提供的网络抽象确实有很大的帮助。\n2016 年初，我们决定推动一个 100% 覆盖的服务网格。最初，我们认为这将是一个艰难的过程，需要自上而下的授权。在实践中，团队报名参加了迁移，因为他们将得到的好处是显而易见的。“胡萝卜 “式的迁移几乎总是成功的。而 “大棒” 式的迁移则很少成功，或者即使成功了，也会在组织内留下眼泪和愤怒。\n到 2016 年中期，Envoy 被用于 Lyft 的所有网络通信，包括边缘服务、服务间通信、数据库、外部合作伙伴等。无论从哪个角度来看，该项目都取得了巨大的成功，帮助 Lyft 完成了微服务的迁移，提高了整体的可靠性，并对网络进行了抽象，使大多数工程师不需要了解真实的系统拓扑结构。此后，Bill 离开了这个项目，在 Lyft 从事其他工作，接替他的是 Roman Dzhabarov 和 Constance Caramanolis 加入我的团队。我们的小团队为整个 Lyft 开发和运维 Envoy。\n开放源码 到 2016 年夏天，我们开始认真讨论开源 Envoy 的问题。早期的 Lyft 员工对开源和它为公司所做的事情很欣赏。很明显，Envoy 并不是 Lyft 的主要业务，那么为什么不把它放在那里并给予回报呢？我可以坦率地说，我们都带着不同的目标和期望来对待开放源代码的过程，以及对项目获得巨大成功后会发生什么感到非常天真。\n在加入 Envoy 之前，我已经使用了相当多的开源软件，但我几乎没有开源贡献的经验，也没有维护者的经验。（虽然我在 Linux 内核中有过一次提交！）开源 Envoy 似乎是一个很好的机会，可以扩展我的技能组合，学习新的东西，可能会促进我的职业生涯，坦率地说，我不希望有一个 TSA v3 在第三家公司出现。对于 Lyft 来说，Envoy 是一个重要的工程项目，领导层认为，开放源代码将使 Lyft 作为一个工程组织具有可信度，并有助于招聘工作。正如我之前所说，我们所有人都对创建成功的开源，更重要的是在它获得成功的情况下培育它所需要的东西感到天真。\n但是，我们决定给它一个机会。我们在 2016 年夏天花了很大一部分时间来编写文档（Jose Nino 在这个时候加入了团队，他的第一个任务就是阅读并帮助改进所有的文档），清理存储库，使其 \u0026#34; 不那么尴尬”，制作网站，发布博文等等。我真的很感谢这段时间里我在 Lyft 的同事，他们不仅支持我们，还帮助我们完成了无数的任务，包括网站设计、logo 等等。即使在这个早期阶段，我们也觉得第一印象很重要，如果我们要在开源领域有所作为，就必须通过高质量的文档、网站等给人留下良好的第一印象。\n在此期间，我们还利用我们的行业关系，与 Lyft 的一些 “同行公司”（湾区的 “独角兽 \u0026#34; 互联网创业公司）会面，向他们展示我们在 Envoy 方面所做的工作，并获得他们的反馈，我们认为如果我们在正式开源前成功获得一个启动合作伙伴，这将是对项目的一个重大帮助。所有这些会议都非常友好，总的来说，所有与我们会面的公司都对我们所取得的成就印象深刻。但是，事后看来，他们都表示，以他们的小型基础设施团队，不可能马上采用 Envoy。他们祝愿我们在开放源代码方面取得最好的成绩，并说他们以后会回来看看。我们不禁对这些会议的结果感到沮丧，但我们还是向前推进了。\n2015 年 8 月，我与谷歌进行了第一次友好的会面。一个 Lyft 的同事（Chris Burnett）在一个 gRPC 聚会上发言，提到了 Envoy，因为它与 Envoy 的 gRPC 桥接支持有关。我不知道的是，谷歌在发现 Envoy 的时候，正准备在 NGINX 的基础上推出 Istio。一次会议引出了另一次会议，然后是更多的会议，在 Envoy 开源之前，大量的谷歌员工已经看到了源代码和文档。(稍后会有更多关于这方面的内容）。\n到 9 月初，我们已经准备好了，并将开源日定为 9 月 14 日。总的来说，我是一个（过度？）自信的人，但在我的生活中，有几次我对自己成功的能力有很大的焦虑。我立即想到的是：开始上高中，开始上大学，以及大学毕业后在微软工作。而开源的 Envoy 就是其中之一。我记得我被公众的反应吓坏了。人们会怎么说？反馈会是积极的还是恶毒的？虽然我们在开源时是一个小团队，但我仍然写了 90% 或更多的代码，并且觉得把它放到公共领域是对我自己和我的能力的一种反映。\n如期而至，Envoy 在 2016 年 9 月 14 日 成为开源产品。我记得我和妻子一起庆祝，并说了一些话。“如果我们能让其他公司像 Lyft 一样使用 Envoy，我就会很高兴。”\n对开放源码发布的反应几乎是普遍的积极。令我们惊讶的是，几乎是立刻，我们开始听到大公司的声音，而不是小公司。在几周内，我们与苹果、微软进行了交谈，与谷歌的对话也不断加快。大公司在现有的解决方案中存在 …","date":1631673714,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"aed91e434b355488d56272bc98a1b7e9","permalink":"https://lib.jimmysong.io/blog/envoy-oss-5-year/","publishdate":"2021-09-15T10:41:54+08:00","relpermalink":"/blog/envoy-oss-5-year/","section":"blog","summary":"开源网络代理 Envoy 的创始人 Matt Klein，在 Twitter 因为自己的程序 bug 造成重大事故而离职，后加入 Lyft，在开源 Envoy 之前几乎没有贡献和管理开源项目的经验，这篇文章分享了他个人及 Envoy 开源的心路历程，在投身开源 Envoy 还是为雇主 Lyft 效命，该如何抉择？","tags":["envoy","proxy"],"title":"网络代理 Envoy 开源五周年，创始人 Matt Klein 亲述开源心路历程及经验教训","type":"post"},{"authors":["林杨","白西原","张卫滨","何昌钦"],"categories":null,"content":"话题  欢迎来到云原生社区大连站 Connecting, Controlling and Observing Dubbo Microservices with Flomesh，林杨 基于云原生技术的服务最大化可用性，白西原（乐天创研） Jutopia 一站式云原生机器学习平台，何昌钦 分布式系统的发展与趋势分析，张卫滨  ","date":1629608400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"a801c020c0b65a1030b6aa97ee9a11b7","permalink":"https://lib.jimmysong.io/event/cloud-native-meetup-dalian-06/","publishdate":"2021-08-22T18:00:00+08:00","relpermalink":"/event/cloud-native-meetup-dalian-06/","section":"event","summary":"本次活动关注服务网格、机器学习。","tags":["Flomesh","Jutopia"],"title":"云原生社区 meetup 第六期大连站","type":"event"},{"authors":null,"categories":null,"content":"近日美国国家安全局（NSA）和网络安全与基础设施安全署（CISA）发布了一份网络安全技术报告 Kubernetes Hardening Guidance（查看英文原版 PDF）。\n笔者将本资料翻译为《Kubernetes 加固指南》（或译作《Kubernetes 强化指南》）中文版。\n","date":1628380800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"ff33cba030ab96ff0c0640954bf14026","permalink":"https://lib.jimmysong.io/book/kubernetes-hardening-guidance/","publishdate":"2021-08-08T00:00:00Z","relpermalink":"/book/kubernetes-hardening-guidance/","section":"book","summary":"NSA 和 CISA 联合出品","tags":["翻译电子书"],"title":"Kubernetes 加固指南","type":"publication"},{"authors":["Istio"],"categories":["Istio"],"content":"编者的话 本文译自 Istio 官方博客 Security Best Practices。\nIstio 的安全功能提供了强大的身份、策略、透明的 TLS 加密以及认证、授权和审计（AAA）工具来保护你的服务和数据。然而，为了充分安全地利用这些功能，必须注意遵循最佳实践。建议在继续阅读之前，先回顾一下安全概述。\n双向 TLS Istio 将尽可能使用双向 TLS 对流量进行自动加密。然而，代理在默认情况下被配置为许可模式（Permissive Mode），这意味着他们将接受双向 TLS 和明文流量。\n虽然这是为了增量采用或允许来自没有 Istio sidecar 的客户端的流量的需要，但它也削弱了安全立场。建议在可能的情况下迁移到严格模式（Strict Mode），以强制使用双向 TLS。\n然而，仅靠双向 TLS 并不足以保证流量的安全，因为它只提供认证，而不是授权。这意味着，任何拥有有效证书的人仍然可以访问一个服务。\n为了完全锁定流量，建议配置授权策略。这允许创建细粒度的策略来允许或拒绝流量。例如，你可以只允许来自 app 命名空间的请求访问 hello-world 服务。\n授权策略 Istio 授权在 Istio 安全中起着关键作用。它需要努力配置正确的授权策略，以最好地保护你的集群。了解这些配置的影响是很重要的，因为 Istio 无法确定所有用户的正确授权。请全程关注本节内容。\n应用默认拒绝的授权策略 我们建议你按照 default-deny 模式定义你的 Istio 授权策略，以增强集群的安全态势。默认拒绝授权模式意味着你的系统默认拒绝所有请求，而你定义了允许请求的条件。如果你错过了一些条件，流量将被意外地拒绝，而不是流量被意外地允许。后者通常是一个安全事件，而前者可能会导致糟糕的用户体验、服务中断或不符合你的 SLO/SLA。\n例如，在 HTTP 流量的授权任务中，名为 allow-nothing 的授权策略确保所有流量在默认情况下被拒绝。从这里开始，其他授权策略根据特定条件允许流量。\n在路径规范化上定制你的系统 Istio 授权策略可以基于 HTTP 请求中的 URL 路径。路径规范化（又称 URI 规范化）对传入请求的路径进行修改和标准化，从而使规范化后的路径能够以标准方式进行处理。语法上不同的路径在路径规范化后可能是等同的。\nIstio 支持以下请求路径的规范化方案，然后再根据授权策略进行评估和路由请求：\n   选项 描述 示例     NONE 不做任何规范化处理。Envoy 收到的任何信息都会被原封不动地转发给任何后端服务。 ../%2Fa../b 由授权政策评估并发送给你的服务。   BASE 这是目前 Istio 默认安装中使用的选项。这在 Envoy 代理上应用了 normalize_path选项，该选项遵循 RFC 3986，有额外的规范化处理，将反斜线转换成正斜线。 /a/../b 被规范化为 /b。\\da 被规范化微 /da。   MERGE_SLASHES 斜线在 BASE 规范化之后被合并。 /a//b 被规范化为 /a/b。   DECODE_AND_MERGE_SLASHES 最严格的设置，当你默认允许所有流量。这个设置是推荐的，但要注意的是，你需要彻底测试你的授权策略路径。百分比编码的斜线和反斜线字符（%2F、%2f、%5C 和 %5c）在 MERGE_SLASHES 被规范化之前被解码为 / 或 \\。 /a%2fb 被规范化为 /a/b。    该配置是通过 mesh 配置中的 pathNormalization字段指定的。\n为了强调这一点，规范化算法是按照以下顺序进行的：\n 百分比解码 %2F、%2f、%5C 和 %5c。 RFC 3986 和其他由 Envoy 的 normalize_path选项实现的规范化。 合并斜线   虽然这些规范化选项代表了来自 HTTP 标准和常见行业惯例的建议，但应用程序可以以它选择的任何方式解释一个 URL。当使用拒绝策略时，请确保你了解你的应用程序的行为方式。\n 配置的例子 确保 Envoy 规范化请求路径以符合你的后端服务的期望，对你的系统安全至关重要。下面的例子可以作为你配置系统的参考。规范化的 URL 路径，如果选择了 NONE，则是原始的 URL 路径将：\n 用来对照授权策略进行检查 转发到后端应用程序     你的应用程序 选择     依靠代理进行规范化处理 BASE, MERGE_SLASHES or DECODE_AND_MERGE_SLASHES   根据 RFC 3986 规范化请求路径，不合并斜线 BASE   根据 RFC 3986 规范化请求路径，合并斜线，但不对百分比编码的斜线进行解码 MERGE_SLASHES   根据 RFC 3986 规范化请求路径，合并斜线，并对百分比编码的斜线进行解码 DECODE_AND_MERGE_SLASHES   处理请求路径的方式与 RFC 3986 不兼容 NONE    如何配置 你可以使用 istioctl 来更新 mesh 配置：\n$ istioctl upgrade --set meshConfig.pathNormalization.normalization=DECODE_AND_MERGE_SLASHES 或通过改变你的 Operator 重写文件\n$ cat \u0026lt;\u0026lt;EOF \u0026gt; iop.yamlapiVersion:install.istio.io/v1alpha1kind:IstioOperatorspec:meshConfig:pathNormalization:normalization:DECODE_AND_MERGE_SLASHESEOF$ istioctl install -f iop.yaml另外，如果你想直接编辑 Mesh 配置，你可以将 pathNormalization添加到 mesh 配置中，该配置是 istio-\u0026lt;REVISION_ID\u0026gt; 的 CongfigMap，在 istio-system 命名空间。例如，如果你选择 DECODE_AND_MERGE_SLASHES 选项，你修改 mesh 配置如下：\napiVersion:v1data:mesh:|-... pathNormalization: normalization: DECODE_AND_MERGE_SLASHES ... 不太常见的规范化配置 大小写规范化 在某些环境中，以不区分大小写的方式比较授权策略中的路径可能是有用的。例如，将 https://myurl/get 和 https://myurl/GeT 等同对待。在这些情况下，可以使用下面的 EnvoyFilter。这个过滤器将改变用于比较的路径和呈现给应用程序的路径。\napiVersion:networking.istio.io/v1alpha3kind:EnvoyFiltermetadata:name:ingress-case-insensitivenamespace:istio-systemspec:configPatches:- applyTo:HTTP_FILTERmatch:context:GATEWAYlistener:filterChain:filter:name:\u0026#34;envoy.filters.network.http_connection_manager\u0026#34;subFilter:name:\u0026#34;envoy.filters.http.router\u0026#34;patch:operation:INSERT_BEFOREvalue:name:envoy.luatyped_config:\u0026#34;@type\u0026#34;: \u0026#34;type.googleapis.com/envoy.extensions.filters.http.lua.v3.Lua\u0026#34;inlineCode:|function envoy_on_request(request_handle) local path = request_handle:headers():get(\u0026#34;:path\u0026#34;) request_handle:headers():replace(\u0026#34;:path\u0026#34;, string.lower(path)) end 了解流量采集的限制 Istio sidecar 的工作原理是捕获入站流量和出站流量，并通过 sidecar 代理引导它们。\n然而，并不是所有的流量都被捕获：\n 重定向只处理基于 TCP 的流量。任何 UDP 或 ICMP 数据包都不会被捕获或修改。 Sidecar 使用的许多端口以及 22 号端口的入站捕获被禁用。这个列表可以通过 traffic.sidecar.istio.io/excludeInboundPorts 等选项来扩展。 出站捕获同样可以通过 traffic.sidecar.istio.io/excludeOutboundPorts 等设置或其他方式减少。  一般来说，应用程序和其 sidecar 代理之间的安全边界最小。对 sidecar 的配置是以每个模块为基础的，并且两者都在同一个网络 / 进程命名空间中运行。因此，应用程序可能有能力删除重定向规则，并删除、改变、终止或替换 sidecar 代理。这允许一个 pod 故意绕过它的 sidecar 的出站流量或故意让入站流量绕过它的 sidecar。\n因此，依靠 Istio 无条件地捕获所有流量是不安全的。相反，安全边界是客户端不能绕过另一个 pod 的 sidecar。\n例如，如果我在 9080 端口运行 review 应用程序，我可以假设来自 productpage 应用程序的所有流量将被 sidecar 代理捕获，其中 Istio 认证和授权策略可能适用。\n利用 NetworkPolicy 进行深度防御 为了进一步确保流量安全，Istio 策略可以与 Kubernetes 网络策略分层。这实现了一个强大的深度防御策略，可以用来进一步加强你的网格的安全性。\n例如，你可以选择只允许流量到我们 review 应用程序的 9080 端口。如果集群中的 Pod 被破坏或存在安全漏洞，这可能会限制或阻止攻击者的进展。\n确保出口流量的安全 一个常见的误解是，像 outboundTrafficPolicy: REGISTRY_ONLY 作为一个安全策略，防止所有对未申报服务的访问。然而，如上所述，这并不是一个强大的安全边界，应该被认为是尽力而为。\n虽然这对防止意外的依赖性很有用，但如果你想保证出口流量的安全，并强制要求所有出站流量通过代理，你应该依靠 Egress Gateway。当与网络策略相结合时，你可以强制所有的流量，或一些子集，通过出口网关。这确保了即使客户意外地或恶意地绕过他们的 sidecar，该请求也会被阻止。\n当使用 TLS 发起时，在目的地规则中配置 TLS 验证 Istio 提供了从一个 sidecar 代理或网关发起 TLS 的能力。这使得发送纯文本 HTTP 流量的应用程序能够透明地 “升级 “到 HTTPS。\n在配置 DestinationRule 的 tls 设置时，必须注意指定 caCertificates 字段。如果没有设置，服务器的证书将不会被验证。\n例如：\napiVersion:networking.istio.io/v1beta1kind:DestinationRulemetadata:name:google-tlsspec:host:google.comtrafficPolicy:tls:mode:SIMPLEcaCertificates:/etc/ssl/certs/ca-certificates.crt网关 在运行 Istio 网关时，涉及一些资源：\n Gateways，它控制网关的端口和 TLS 设置。 VirtualServices，控制路由逻辑。这些都是通过在网关字段中的直接引用和在网关和 VirtualService 的 hosts …","date":1627880742,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"666ddb41eaf0e7a5f7b0eb9c1cc88cdc","permalink":"https://lib.jimmysong.io/blog/istio-security-best-practices/","publishdate":"2021-08-02T13:05:42+08:00","relpermalink":"/blog/istio-security-best-practices/","section":"blog","summary":"本文列举了 Istio 安全的最佳实践。","tags":["Istio","安全"],"title":"Istio 安全最佳实践","type":"post"},{"authors":["粟伟","梁举","王天宜","石建伟"],"categories":null,"content":"话题  开场演讲：欢迎来到云原生社区成都站，宋净超，云原生社区创始人、Tetrate 布道师 Amazon EKS Distro 开源项目解析 \u0026amp; 演示，粟伟，亚马逊云科技资深解决方案架构师 面向量化投资的 AI 平台 ——AI 赋能投资：打造以大数据 + AI 为核心的下一代投资平台，梁举，宽邦科技 CEO 基于 TiDB 的云原生数据库实践，王天宜，TiDB 社区部门架构师 Layotto: 开启服务网格 + 应用运行时新篇章，石建伟（卓与），蚂蚁集团高级技术专家  ","date":1625288400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"7984aaeb6bd7f2e4bd98deb176fe136e","permalink":"https://lib.jimmysong.io/event/cloud-native-meetup-chengdu-05/","publishdate":"2021-07-03T18:00:00+08:00","relpermalink":"/event/cloud-native-meetup-chengdu-05/","section":"event","summary":"本次活动关注 EKS Distro、TiDB、AI 和服务网格。","tags":["Layotto","TiDB"],"title":"云原生社区 meetup 第五期成都站","type":"event"},{"authors":["王院生","黄国锋","吴凌峰","敖小剑"],"categories":null,"content":"开场致辞 讲师：宋净超（Tetrate 布道师、云原生社区创始人）\n讲师介绍：Tetrate 云原生布道师，云原生社区创始人，CNCF Ambassador。\n有了 Nginx 和 Kong，为什么还需要 Apache APISIX？ 讲师：王院生\n个人介绍：支流科技联合创始人 CTO\n演讲概要\n在云原生时代，k8s 和微服务已经成为主流，在带来巨大生产力提升的同时，也增加了系统的复杂度。如何发布、管理和可视化服务，成为了一个重要的问题。每次修改配置都要 reload 的 Nginx、依赖 postgres 才能工作的 Kong，都不是云原生时代的理想之选。这正是我们创造 Apache APISIX 的原因：没有 reload、毫秒内全集群生效、不依赖数据库、极致性能、支持 Java 和 Go 开发插件。\n听众收益\n更好的理解 API 网关、服务网格，以及各个开源项目的优劣势\n云原生时代的研发效能 讲师：黄国峰\n个人介绍：腾讯 PCG 工程效能专家。10 多年的软件和互联网从业经验；现任腾讯工程效能部，负责持续集成、研发流程和构建系统等平台；曾任职唯品会高级经理，负责架构团队。在云原生平台下的研发效能方向有丰富的理论知识和实践经验。\n演讲概要\n云原生时代，软件研发的逻辑彻底改变了。传统的软件开发在本机编码 / 调试、部署到测试环境测试、再发布到生产环境；而云原生时代的开发，基于不可变设施，研发流程从编码、构建、持续测试、持续集成到持续部署，整个过程几乎完全代码化。\n听众收益\n 了解云原生开发的新挑战和难点 了解腾讯云原生开发实践的流程和思路 了解腾讯云原生开发中的遇到的坑和解决思路  37 手游 Go 微服务架构演进和云原生实践 讲师：吴凌峰\n个人介绍：任职于三七互娱集团 37 手游技术部基础架构组，负责平台 golang 基础框架以及 DevOps、CI/CD 生态建设，从业以来一直专注于云原生、DevOps 和容器化等技术应用和推广，在 golang 工程化领域有一定的心得。\n演讲概要\nGolang 微服务应用和云原生的概念近年越来越火热，传统技术栈公司随着业务规模增长，在云原生技术应用落地探索和转型的过程中一定会遇到很多共通的问题以及有各自不同的思考，包括如何更好地提升我们的开发效率、提升服务稳定性、降低运维成本？面对不断增长的服务数量和不断变长变复杂的调用关系网，怎样才能更好地观测、管理和保证核心服务高可用，本次演讲分享将会围绕 37 手游转型为 Go 微服务架构以及建设云原生 DevOps 体系的历程、过程中的领悟和思考展开。\n听众收益\n 了解 Golang 云原生微服务框架的关键技术和优化实践经验 了解云原生观测体系如链路追踪、监控等 Golang 微服务落地实践经验 了解混合云混合部署 DevOps 和 CI/CD 体系的企业实践经验  死生之地不可不察：论 API 标准化对 Dapr 的重要性 讲师：敖小剑\n个人介绍：资深码农，十九年软件开发经验，微服务专家，Service Mesh 布道师，Servicemesher 社区联合创始人，Dapr Maintainer。专注于基础架构，Cloud Native 拥护者，敏捷实践者，坚守开发一线打磨匠艺的架构师。曾在亚信、爱立信、唯品会、蚂蚁金服等任职，对基础架构和微服务有过深入研究和实践。目前就职阿里云，在云原生应用平台全职从事 Dapr 开发。\n演讲概要\nDapr 作为新兴的云原生项目，以 “应用运行时” 之名致力于围绕云原生应用的各种分布式需求打造一个通用而可移植的抽象能力层。这个愿景有着令人兴奋而向往的美好前景：一个受到普通认可和遵循的云原生业界标准，基于此开发的云原生应用可以在不同的厂家的云上自由的部署和迁移，恍惚间一派云原生下世界大同的美景。然而事情往往没这么简单，API 的标准化之路异常的艰辛而痛苦，Dapr 的分布式能力抽象在实践中会遇到各种挑战和困扰。\n听众收益\n 了解 Dapr 的愿景和分布式能力抽象层的重要 了解 Dapr API 在抽象和实现时遇到的实际问题，尤其是取舍之间的艰难 了解目前 Dapr 在 API 抽象上正在进行的努力和新近准备增加的 API  ","date":1621659600,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"c58a8a96d593ceb233458c9ee06a93b0","permalink":"https://lib.jimmysong.io/event/cloud-native-meetup-guangzhou-04/","publishdate":"2021-05-22T20:00:00+08:00","relpermalink":"/event/cloud-native-meetup-guangzhou-04/","section":"event","summary":"本次活动关注于 Dapr 和 APISIX 等。","tags":["Dapr","APISIX"],"title":"云原生社区 meetup 第四期广州站","type":"event"},{"authors":["周强","王佰平","孙健波","张义飞"],"categories":null,"content":"开场致辞 讲师：宋净超（Tetrate 布道师、云原生社区创始人）\n讲师介绍：Tetrate 云原生布道师，云原生社区创始人，CNCF Ambassador。\n使用 Chaos Mesh 来保障云原生系统的健壮性 讲师：周强\n公司：PingCAP\n讲师介绍：周强，PingCAP 工程效率负责人，Chaos Mesh 负责人，专注稳定性和性能测试平台。在混沌工程领域有 4 年的从业经验，领导开发云原生混沌测试平台 Chaos Mesh。\n演讲概要:\n在云原生的世界中，错误无处不在，混沌工程在提高系统稳定性方面起着至关重要的作用。通过执行混沌工程实验，我们可以了解系统的弱点并主动解决。我们开发了云原生混沌工程平台 Chaos Mesh，并在内部使用 Chaos Mesh 来提升云原生分布式数据库 TiDB 的健壮性。目前 Chaos Mesh 已加入 CNCF Sandbox 项目，该平台依托于 k8s 基础设施，通过对 pod/container 进行诸如杀节点、IO 错误和延时注入、时间回退、内核分配内存失败等等来进行混沌测试。主题大纲:\n 在分布式领域会遇到的质量和稳定性问题 混沌工程在提升系统稳定性方面的作用和意义 Chaos Mesh 项目简介 混沌工程主要的测试方式和使用案例 混沌工程平台的构建实践  听众收益：\n 了解在构建分布式系统可能出现的问题和风险 了解混沌工程的使用经验和踩过的坑，观众后续可以通过混沌工程来进行相关实践，提升产品质量 通过 case study 可以帮助大家构建分布式混沌测试平台  Envoy 在轻舟微服务中落地实践 讲师：王佰平\n公司：网易\n讲师介绍：网易数帆资深工程师，负责轻舟 Envoy 网关与轻舟 Service Mesh 数据面开发、功能增强、性能优化等工作。对于 Envoy 数据面开发、增强、落地具有较为丰富的经验。\n演讲概要：\nEnvoy 是由 Lyft 开源的高性能数据和服务代理，以其可观测性和高扩展性著称。如何充分利用 Envoy 的特性，为业务构建灵活易扩展、稳定高性能的基础设施（服务网格、API 网关）是 Envoy 落地生产实践必须考虑的问题。本次分享主要介绍 Envoy 在轻舟微服务网关与轻舟微服务网格中落地实践经验和构建此类基础设施时轻舟关注的核心问题。希望能够给大家带来一些帮助。\n听众收益：\n 了解 Envoy 本身架构与关键特性； 在生产实践当中微服务网关与服务网格关注的核心问题以及轻舟 Envoy 的解决之道； 为期望实现集群流量全方位治理和观察的听众提供些许借鉴。  KubeVela：阿里巴巴新一代应用交付管理系统实践 讲师：孙健波\n公司：阿里巴巴\n讲师介绍：孙健波 (花名：天元) 阿里云技术专家，云原生应用模型 OAM (Open Application Model) 核心成员和主要制定者，KubeVela 项目作者，致力于推动云原生应用标准化，负责大规模云原生应用交付与应用管理相关工作。曾参与编写《Docker 容器与容器云》技术书籍。\n演讲概要：\n 云原生应用交付面临的问题与挑战 社区中常见的应用交付解决方案对比 基于 KubeVela 的标准化应用交付管理核心原理 阿里巴巴基于 KubeVela 的应用交付实践  听众收益：\n随着 “云原生” 的普及，基础设施逐渐成熟的今天，越来越多的应用开发者们开始追求快速的构建交付应用。然而使用场景的不同往往意味着应用交付的环境会有巨大的差异。就比如，K8s 中不同的工作负载类型需要对接不同的灰度发布、流量管理等实现方案，不同的部署环境（公有云、私有化部署等）也常常需要对接不同的日志监控体系。如何才能将应用管理和交付变得标准化，使得应用研发不再需要花大量精力对接不同的交付平台？这已经逐渐成为云原生应用管理领域的一大痛点。本次分享将针对这些问题为大家介绍如何基于 KubeVela 构建标准化的应用交付管理平台，介绍阿里巴巴在此基础上的实践经验。\nEnvoy 在阿里巴巴内部的落地实践 讲师：张义飞\n公司：阿里巴巴\n讲师介绍：阿里巴巴云原生部门高级工程师，主要负责阿里巴巴内部 ServiceMesh 数据面的落地。Envoy/Istio 社区 Member，给 envoy 社区贡献了 Dubbo Proxy filter，metadata 优化等。\n演讲概要：\n  介绍 Envoy 在大规模场景下存在的问题以及如何优化\n  介绍 Envoy 实现自定义协议的最佳实践\n   扩展自定义协议 扩展连接池 扩展 Cluster    介绍 Envoy 在阿里巴巴内部落地遇到的一些困难\n  听众收益：\n Envoy 在大规模场景下存在的一些问题 机器数量过多导致的内存问题 机器全量下发导致的 CPU 问题  ","date":1618635600,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"c40acfee850749d333afb24c0092c194","permalink":"https://lib.jimmysong.io/event/cloud-native-meetup-hangzhou-03/","publishdate":"2021-04-17T20:00:00+08:00","relpermalink":"/event/cloud-native-meetup-hangzhou-03/","section":"event","summary":"本次活动关注于 ChaosMesh、Envoy 和 KubeVela。","tags":["ChaosMesh","Envoy","KubeVela"],"title":"云原生社区 meetup 第三期杭州站","type":"event"},{"authors":null,"categories":null,"content":"《Quarkus 实战——专为 Kubernetes 而优化的 Java 解决方案》，Alex Soto Bueno、Jason Porter 著，张晓宇、刘岩、宋净超译，机械工业出版社出版，2021 年 3 月。\n   译者序 Quarkus 是一款有别于传统 Java 架构的新技术框架，它是建立在我们熟知的技术栈上，使用了诸多成熟的技术，如 JPA，JAX-RS、Eclipse Vert.x、Eclipse MicroProfile 和 CDI 等，并将之和 Kubernetes 紧密融合在一起。用户可以借助 Kubernetes 的高效的调度运维能力，最大限度地节约资源。\n云原生的星星之火，自社区 Kubernetes 爆红之后，变成燎原之势。云原生相关的技术如雨后春笋般涌出。刘岩，宋净超和我都是云原生社区的成员，也钟爱布道各种相关技术，是这一领域的狂热爱好者。我们共同的爱好之一，就是时刻关注有好的国外技术或者成熟技术的优秀书籍发布。\n在这一过程中，我们机缘巧合地发现了这本书，恰好这本书在国内还没有进行翻译，满怀热情的我们就此踏上了研究 Quarkus 之旅。\n这本采用十分简单的抛出问题，提出解决方案，和引发讨论的方式，将 Quarkus 的技术点描绘的细致透彻。通过本书，用户可以自学相关内容，借助 Quarkus，提高 Java 相关研发的工作效率，让你在快节奏的微服务构建和基于云的应用程序开发领域立于不败之地。\n在整个翻译过程中，我们得到华章出版社和李忠明编辑的全力帮助，在此表示衷心感谢。\n最后，感谢大家有缘阅读到此书，希望我们三人的绵薄之力可以帮助到崇尚云原生技术的你，在 Quarkus 的技术道路上，能够享受到和我们一样的欣喜。\n","date":1617148800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"c8dd8e154fb2f6ff5a1da83d9217e955","permalink":"https://lib.jimmysong.io/book/quarkus-cookbook/","publishdate":"2021-03-31T00:00:00Z","relpermalink":"/book/quarkus-cookbook/","section":"book","summary":"专为 Kubernetes 而优化的 Java 解决方案","tags":["出版物"],"title":"Quarkus 实战","type":"publication"},{"authors":["宋净超","孙召昌","赵新","张城","刘硕然"],"categories":null,"content":"Istio 1.8——还是从前那个少年 讲师：宋净超（Tetrate 布道师、云原生社区创始人）\n个人介绍：Tetrate 布道师、CNCF Ambassador、云原生社区 创始人、电子工业出版社优秀译者、出品人。Kubernetes、Istio 等技术的早期使用及推广者。曾就职于科大讯飞、TalkingData 和蚂蚁集团。\n议题简介：带你回顾 Istio 的发展历程，看他是否还是从前那个少年，“没有一丝丝改变”，能够经历时间的考验。带你一起来了解 Istio 1.8 的新特性，看它是如何作为传统和现代应用的桥接器，成为云原生应用中的中流砥柱。同时也会为你分享云原生社区的规划，为了推行云原生，我们在行动。\n百度服务网格在金融行业的大规模落地实践 讲师：孙召昌（百度高级研发工程师）\n个人介绍：百度高级研发工程师，现就职于百度基础架构部云原生团队，参与了服务网格产品的研发工作和大规模落地实践，对云原生、微服务、Service Mesh等方向有深入的研究和实践经验。\n议题简介：百度服务网格技术在金融行业大规模落地过程的实践经验和思考，主要包括：\n 支持传统微服务应用的平滑迁移，兼容SpringCloud和Dubbo应用； 灵活对接多种注册中心，支持百万级别的服务注册和发现； 提供丰富的流量治理策略，包括自定义路由、全链路灰度等； 实现业务无侵入的指标统计和调用链展示，满足用户的可观测性需求。  Apache/Dubbo-go 在云原生时代的实践与探索 讲师：赵新（于雨）\n个人介绍：于雨（GitHub ID AlexStocks），dubbogo 社区负责人，一个有十多年服务端基础架构研发经验的一线程序员，陆续改进过 Redis/Muduo/Pika/Dubbo/Dubbo-go/Sentinel-go 等知名项目，目前在蚂蚁集团可信原生部从事容器编排和 Service Mesh 工作。\n议题简介：\n 基于Kubernetes 的微服务通信能力 基于 MOSN 的云原生 Service Mesh 能力 基于应用级注册的服务自省能力 dubbo-go 3.0 规划  合影、中场休息、签售 中场休息时会有《云原生操作系统 Kubernetes》作者之一张城为大家现场签售。\n云原生下的可观测性 讲师：张城（元乙）\n个人介绍：阿里云技术专家，负责阿里巴巴集团、蚂蚁金服、阿里云等日志采集基础设施，服务数万内外部客户，日流量数十PB。同时负责云原生相关的日志/监控解决方案，包括系统组件，负载均衡，审计，安全，Service Mesh，事件，应用等监控方案。目前主要关注可观测性、AIOps、大规模分析引擎等方向。\n议题简介：近年来随着云原生技术的普及，PaaS和SaaS化的程度越来越高，传统的监控系统正在朝可观测性系统的方向演进。在这背景下OpenTelemetry诞生，OpenTelemetry为我们带来了Metric、Tracing、Logging的统一标准，便于我们构建一个统一的可观测性平台。\n云原生分布式存储解决方案实践 讲师：刘硕然（OPPO）\n个人介绍：OPPO互联网云平台分布式文件存储技术负责人，ChubaoFS初创成员及项目维护者。\n议题简介：ChubaoFS是云原生的分布式存储系统，目前已经在多家公司生产环境为大规模容器平台的云原生应用提供分布式存储解决方案。主要特点包括高可用，高可扩展，多租户，文件及对象双接口等。与云原生社区的生态也有非常紧密的结合，目前监控使用Prometheus，部署支持Helm，使用支持CSI driver。\n","date":1608440400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"1945b774f78bb1667f9376e845ed281d","permalink":"https://lib.jimmysong.io/event/cloud-native-meetup-beijing-02/","publishdate":"2020-12-20T18:00:00+08:00","relpermalink":"/event/cloud-native-meetup-beijing-02/","section":"event","summary":"本次活动关注 Istio、云原生存储、可观测性、DubboGo 等。","tags":["Istio","DubboGo","SpringCloud","ChubaoFS","OpenTelemetry"],"title":"云原生社区 meetup 第二期北京站","type":"event"},{"authors":["宋净超","高洪涛","杨可奥","侯诗军","程亮"],"categories":null,"content":"\n  演讲幻灯片及视频回放请点击页面上方的按钮。   开场演讲 讲师：宋净超\n公司：Tetrate\n讲师介绍：Tetrate 布道师，云原生社区创始人，CNCF Ambassador。\nKubernetes 在 UCloud 内部的应用 讲师：高鹏\n公司：UCloud\n讲师介绍：高鹏 UCloud 后台研发工程师，负责内部云原生平台的建设。\n演讲概要：在 Kubernetes 的实际应用中，我们会碰到各种各样的问题，比如复杂的网络结构、持久化存储的实现、多租户的权限模型、集群的升级、Istio 的使用、镜像仓库的高可用、CI/CD、监控告警、日志、Operator 等等等等。这次分享将介绍 UCloud 内部对 Kubernetes 以及云原生生态的应用实践，和大家分析每个选择背后的原因，碰到的问题以及解决方案。\n使用 Apache SkyWalking Adapter 实现 K8s HPA 讲师：高洪涛\n公司：Tetrate\n讲师介绍：高洪涛 美国 servicemesh 服务商 Tetrate 创始工程师。原华为软件开发云技术专家，对云 APM 有深入的理解，并有丰富的 APM 产品设计，研发与实施经验。对分布式数据库，容器调度，微服务，ServicMesh 等技术有深入的了解。目前为 Apache SkyWalking 核心贡献者，参与该开源项目在软件开发云的商业化进程。前当当网系统架构师，开源达人，曾参与 Apache ShardingSphere，Elastic-Job 等知名开源项目。对开源项目的管理，推广和社区运营有丰富的经验。积极参与技术分享，曾在多个技术大会中做过分享，包括 ArchSummit， Top100，Oracle 嘉年华等。在多个媒体发表过文章，如 InfoQ。\n演讲概要：Apache SkyWalking 作为云原生可观测性工具在 Kubernetes 领域内有诸多应用，包括监控微服务的性能，观测基于 ISTIO 的 Service Mesh 服务等。本次分享将带来使用 SkyWalking 的 Adapter 来实现 Kubernetes 的 HPA 功能。应用该能力后，应用或服务将会根据 SkyWalking 分析的指标进行横向扩展。\nChaos Mesh - 让应用与混沌在 Kubernetes 上共舞 讲师：杨可奥\n公司：PingCAP\n讲师介绍：杨可奥，是 Chaos Mesh 的核心开发者之一；也是当前 Chaos Mesh 的 maintainer。在混沌工程的实践和实现上拥有一定经验和见解。除了 Chaos Mesh 之外还维护有多个受欢迎的开源项目，如 pprof-rs。他也曾多次主持或参与社区活动，对云环境下的应用有自己的见解和热爱。\n演讲概要：在生产环境中，各种各样的故障随时会发生，一个稳健的应用应当时刻处于能够应对故障的状态。对于云环境下的应用来说，这一点尤为重要。近些年来，混沌工程逐渐成为了一个稳定性保障和测试的重要话题。而 Chaos Mesh 以 Kubernetes 为平台，提供了云环境下的混沌工程实践方案。在这次分享中将介绍混沌工程和 Chaos Mesh 这一实践方案，并对其动机、Operator 模式、部分实现进行讲解。\n云原生技术在风控 SaaS 领域的踩坑与最佳实践 讲师：侯诗军\n公司：同盾科技\n讲师介绍：侯诗军 – 同盾科技云原生计算部门负责人，香港理工大学与华中科技大学双硕士，十几年来一直专注于云计算技术与管理领域，是 kube-router 等 kubernetes 组件的源代码贡献者，拥有多项云技术专利。16 年加入同盾科技，带领云原生团队从 0 到 N 进行了集团层面的自动化运维、K8S 云平台、DevOps 研发体系变革，将公司在线业务 100% 容器化。\n演讲概要：同盾是国内智能风控领域的头部领军企业，全国独角兽 top50。在中国、印尼雅加达、北美、新加坡拥有 8 个混合云数据中心，共计服务器数量 5000 + 台，在线运转的容器数量 2 万 + 个。截至目前已有超过 1 万家客户选择了同盾的产品及服务，API 单日调用量最低 1 亿次以上，平均响应时间最高 200MS 以内。截止 2019 年，同盾科技全部 (1000 个 app) 在线业务已 100% 容器化。本次议题分别从自动化运维、CI/CD、镜像、云原生网络、域名切换、监控与日志、机器学习、容器安全、混合云弹性、多云管理等方面来讲述公司的踩坑与最佳实践。\n云原生监控体系建设 讲师：程亮\n公司：VIPKID\n讲师介绍： VIPKID 资深架构师，曾任百度高级研发工程师，阿里巴巴技术专家。目前负责大班课后端总架构，VIPKID 监控系统。\n演讲概要：\nvipkid 的传统监控体系介绍：传统的机器监控，zabbix，falcon；日志监控，钉钉，邮件直接上报；业务监控\n基于 k8s 发布之后的监控体系：从 19 开始，vipkid 开始基于 k8s 的发布流程改造，响应的基于 thanos 的监控体系升级。基于公司内部的 CMDBCMD 系统，开发 k8s 的 opertor，自动化适配 vm（虚拟机）发布项目及监控。\n基于日志链路监控：基于流量 CDN-LB-WAF-NG 等链路信息的日志监控\n业务监控的全新规划: 对于研发 RD 和测试 QA 同学来说，更加关注线上业务的正确性。构建业务监控平台，支持线上业务指标追踪。\n","date":1606539600,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"8600100d5ffa5c951169c42f8f325783","permalink":"https://lib.jimmysong.io/event/cloud-native-meetup-shanghai-01/","publishdate":"2020-11-28T20:00:00+08:00","relpermalink":"/event/cloud-native-meetup-shanghai-01/","section":"event","summary":"云原生社区举办的第一届城市站 meetup。","tags":["Istio","SkyWalking","DubboGo","ChaosMesh"],"title":"云原生社区 meetup 第一期上海站","type":"event"},{"authors":null,"categories":null,"content":"《云原生模式》，Cornelia Davis 著，张若飞、宋净超译，电子工业出版社出版，2020 年 8 月。\n  《云原生模式》 图书封面  当我们在讨论云原生时究竟在讨论什么？这些年来我一直在思索这个问题，大家的观点可能不尽相同。三年前从我翻译了第一本云原生领域书籍开始，陆续参与翻译和创作了一系列云原生作品，同时通过对云原生领域的开源项目、社区、基金会、应用云化过程的参与和观察，我得出了下面的结论：云原生是一种行为方式和设计理念，究其本质，凡是能够提高云上资源利用率和应用交付效率的行为或方式都是云原生的。云计算的发展史就是一部云原生化的历史。云原生是云计算适应社会分工的必然结果，将系统资源、底层基础设施和应用编排交由云平台管理，让开发者专注于业务逻辑，这不正是云计算长久以来孜孜以求的吗？云原生应用追求的是快速构建高容错性、弹性的分布式应用，追求的极致的研发效率和友好的上线与运维体验，随云云原生的理念应运而生，它们天生适合部署在云上，可以最大限度利用云计算带来的红利。\n在此之前我曾翻译过几本云原生主题的图书，其中《Cloud Native Go》的作者 Kevin Hoffman，《云原生 Java》的作者 Josh Long，他们都是来自 Pivotal 或曾在 Pivotal 工作多年，当看到此书时，我惊奇的发现，作者 Cornelia Davis 同样来自这家公司，Pivotal 真可谓是云原生的黄埔军校，此书的内容跟以往的云原生书籍有所不同，对于模式的梳理标新立异，因此我立马联系了电子工业出版社的张春雨编辑，经他了解到张若飞正在翻译此书，此前我已与他合作翻译了《云原生 Java》，本书算是我跟他的第二次合作，他翻译图书时的精准和高效着实让我佩服，我们各自翻译了本书一半的内容。 人人都在讨论云原生，但是究竟如何实现却莫衷一是。本书列举了构建云原生应用的 12 种模式，主要关注的是云原生应用的数据、服务与交互，即应用层面的设计模式，这些模式穿插于本书的第二部分各个章节中，基本覆盖了云原生应用的各个方面，并将理论结合实践，带领读者使用 Java 来实现一个云原生应用。\n同时还要感谢云原生社区的成员及志愿者们，对于云原生在中国的发展做出的贡献，你们的鼓励和支持是在云原生领域不断努力和探索的动力。本书在翻译过程中难免有一些纰漏，还望读者指正。\n","date":1596931200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"89c91f15f5e3f5c3fb85d7856f0fa83d","permalink":"https://lib.jimmysong.io/book/cloud-native-patterns/","publishdate":"2020-08-09T00:00:00Z","relpermalink":"/book/cloud-native-patterns/","section":"book","summary":"张若飞、宋净超 译","tags":["出版物"],"title":"云原生模式","type":"publication"},{"authors":["靳文祥","付铖","黄家琦","王国云"],"categories":null,"content":"讲师与演讲话题 蚂蚁集团 API Gateway Mesh 的思考与实践 主讲人: 贾岛\n在 Service Mesh 微服务架构中，我们常常会听到东西流量和南北流量两个术语。蚂蚁集团开源的Service Mesh Sidecar MOSN 已经多次与大家见面交流了，以往的议题重点在东西流量的服务发现与路由，那么蚂蚁集团在南北流量上的思考是怎样的？本次分享，将从蚂蚁集团 API 网关发展历程来看，Mesh 化的网关架构是怎样的，解决了什么问题，双十一的实践表现，以及我们对未来的思考。\n酷家乐的 Istio 与 Knative 踩坑实录 主讲人: 付铖\n酷家乐在部分业务模块，自2018年使用了 Istio 进行服务治理，自2019年使用了 Knative 作为 FaaS 基础设施，在实践过程中解决了大量问题，也积累了不少第一手经验。本次分享，将重点讨论服务网格的性能损耗，存量业务迁移难题，函数计算的冷启动时间问题以及解决方案等。\n云原生开放智能网络代理 MOSN 金融级云原生架构助推器-蚂蚁集团 主讲人：肖涵（涵畅）\n圆桌环节：Service Mesh 落地的务实与创新 主讲人: 鲁直 、涵畅 、张超盟 、付铖 、王国云\n蚂蚁集团 Service Mesh 技术风险思考和实践 主讲人: 嘉祁\nServish Mesh 是微服务架构与云原生碰撞出的火花，对于传统的中间件体系与运维支撑能力是极大的挑战。本次分享的主题主要关注于在蚂蚁集团内部如何应对这些挑战，并建设相应的技术风险能力来保障其稳定。\n网易严选的 Service Mesh 实践 主讲人: 王国云\n网易严选在2016年选择了 Service Mesh 作为未来微服务改造的基础架构，并在过去几年支持了业务的持续快速增长。本次分享主要介绍 Service Mesh 在严选的落地和演进情况，讨论 Service Mesh 在混合云架构下落地遇到的挑战和我们的解决方案，同时也希望和大家交流一下在架构方面的一些思考。\n","date":1577163600,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"adfcbdf724f50e2689e5ae03c2c517f3","permalink":"https://lib.jimmysong.io/event/service-mesh-meetup-09/","publishdate":"2019-12-24T13:00:00+08:00","relpermalink":"/event/service-mesh-meetup-09/","section":"event","summary":"这是第七届 Service Mesh Meetup。","tags":["Service Mesh"],"title":"Service Mesh Meetup #9 杭州站","type":"event"},{"authors":["曾凡松","黄挺","雷志远","周涛","曹寅","宋顺","李云","王成昌","吴天龙"],"categories":null,"content":"本期为 Service Mesh Meetup#8 特别场，联合 CNCF、阿里巴巴及蚂蚁集团共同举办。\n不是任何一朵云都撑得住双 11。\n成交 2684 亿，阿里巴巴核心系统 100% 上云。\n蚂蚁集团的核心交易链路大规模上线 Service Mesh。\n这次，让双 11 狂欢继续，让云原生经得起双 11 大考，也让云原生走到开发者身边。\n你将收获 3 大经验加持：\n 双 11 洗礼下的阿里巴巴 K8s 超大规模实践经验 蚂蚁集团首次 Service Mesh 大规模落地经验 阿里巴巴超大规模神龙裸金属 K8s 集群运维实践经验  讲师与演讲话题 释放云原生价值，双 11 洗礼下的阿里巴巴 K8s 超大规模实践 主讲人：曾凡松（逐灵） 、汪萌海（木苏）\n2019 双 11 点燃了全球人民的购物热情，而阿里经济体核心系统全面上云则刷爆了国内的技术圈子，引起了众多热爱云计算、云原生技术专家的热议。阿里巴巴是首个在超大规模体量公司内大规模使用 K8s 的公司，借此机会将为大家带来阿里巴巴在生产场景中大规模应用 K8s 的实践经验，包括在大规模应用管理上的经验教训；当前如何通过云原生方式高效管理应用；以及对未来应用管理发展趋势的基本看法。\n蚂蚁集团双 11 Service Mesh 超大规模落地实践 主讲人: 黄挺（鲁直） 、雷志远（碧远）\nService Mesh 在过去几年中取得了巨大的关注，但是业界大规模的落地却比较少，目前蚂蚁集团已经在双十一的核心交易链路中大规模上线 Service Mesh，在本次分享中，我们将详细分享我们如何做大规模的落地，在大规模落地 Service Mesh 的时候遇到了什么样的问题，对应的解法又是什么，分享我们在大规模落地 Service Mesh 之后取得的收益，希望能够给有志于尝试 Service Mesh 的公司带来更多的参考。\n阿里巴巴超大规模神龙裸金属 K8s 集群运维实践 主讲人: 周涛 (广侯）\n2019 年是云原生大规模落地的元年，阿里巴巴集团底层基础设施全部搬迁到公有云上，并基于阿里云最新一代的神龙裸金属服务器共同形成了云原生的最佳组合。本次分享将介绍大规模的神龙裸金属云原生集群如何管理和运维的实践。\n深入Kubernetes的“无人区” — 蚂蚁集团双十一的调度系统 主讲人: 曹寅\n蚂蚁集团今年已全面落地 Kubernetes, 支撑双 11 大促。这次分享主要介绍我们在落地过程中面对的各项新技术挑战，及应对这些问题形成的最佳实践，话题包含了规模化 Kubernetes 实践、计算型业务的统一调度与资源混部、蚂蚁双大促分时调度以及 Service Mesh 落地等。\n服务网格在“路口”的产品思考与实践 主讲人: 宋顺（齐天）\n这次分享主要介绍在云原生概念如火如荼而金融行业还处在数字化转型初期的当下，蚂蚁集团服务网格从技术到产品化过程中的思考与实践。内容包括：规模化场景下如何平滑过渡、如何兼顾性能与稳定性、如何支持多语言多协议、无侵入、异构服务统一治理等方面的思考和实践。\n阿里集团核心应用落地 Service Mesh 的挑战与机遇 主讲人: 李云（至简）\n将分享阿里巴巴集团在核心应用落地 Service Mesh 时如何做到“在飞行的飞机上换引擎”，以及在落地的过程中面临的挑战。那些挑战虽然在现阶段的落地过程中并没有全面解决，但给后面的发展和技术突破重点指引了方向。 此外，Service Mesh 作为云原生技术的关键内容，还将分享在这个技术趋势下的发展思考，与大家交流什么是“三位一体”，以及未来 Service Mesh 应当“长成什么模样”。\n蚂蚁集团云原生 PaaS 实践之路 主讲人: 王成昌（晙曦）\n已实现异地多活单元化架构的蚂蚁集团、网商银行基础设施正全面拥抱云原生。本次分享将首次披露蚂蚁集团 PaaS 产品层的建设思路：SOFAStack-CAFE - 承担着海量应用管理、变更保障、容灾多活的应用 PaaS，如何结合实际金融级技术风险保障诉求，基于 Kubernetes 构建云原生运维体系。内容将包括多集群联邦、核心运维能力下沉、发布变更体系、运行时安全、应用交付模式、SOFAMesh 运维体系支撑等方面的实践探索。\n函数计算在双十一小程序场景的应用 主讲人: 吴天龙 （木吴）\n小程序是轻量级的快速迭代的移动应用，对小程序开发者的开发效率有很高的要求。使用函数计算，开发者无需关心后端服务的搭建运维，只需要编写函数就能够提供稳定可靠并且弹性伸缩的服务。双 11 中很多推广活动都是以小程序的方式提供给用户的，在活动时间会有集中的访问，这对后端服务的稳定和弹性是很大的考验。使用函数计算提供的预留实例，可提前为活动高峰预留一部分资源，结合极速的弹性伸缩，轻松应对活动高峰。\n","date":1574571600,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"ca3bd72ddaabcc37ae049c1d121828d2","permalink":"https://lib.jimmysong.io/event/service-mesh-meetup-08/","publishdate":"2019-11-24T13:00:00+08:00","relpermalink":"/event/service-mesh-meetup-08/","section":"event","summary":"这是第八届 Service Mesh Meetup。","tags":["Service Mesh"],"title":"Service Mesh Meetup #8 北京站","type":"event"},{"authors":["赵化冰","肖涵","杨川胡","杨彪"],"categories":null,"content":"本期 Meetup 邀请社区大咖，从服务网格下微服务架构设计、在 5G 时代的应用、如何使用开源的 Traefik 构建云原生边缘路由及蚂蚁集团的服务网格代理演进角度给大家带来精彩分享。\n讲师与演讲话题 服务网格技术在5G网络管理平台中的落地实践 赵化冰（中兴通讯网管软件资深专家）\n在通信网络向5G演进的过程中，电信行业借鉴了IT行业的微服务架构和云原生相关技术对5G网络功能进行重构，以提供敏捷、灵活、易于扩展的业务能力。 本演讲主题将介绍在5G网络管理平台的微服务架构中落地微服务网格的产品实践，包括多网络平面支持、API网关和网格Ingress的定位、Consul Registry的性能增强等等。\n蚂蚁集团网络代理的演进之路 肖涵（蚂蚁集团高级技术专家）\n从网络硬件设备到自研平台，从传统服务治理到 Service Mesh，本次分享将介绍蚂蚁集团网络代理在接入层以及 Service Mesh 化道路上是如何一步步支撑秒级百万支付，千万红包请求的。\n进击的Traefik——云原生边缘路由探秘 杨川胡（ 知群后台负责人）\nTraefik 是一个云原生的边缘路由器，开源的反向代理和负载均衡器，寄予厚望的 2.0 版本历时一年的开发终于发布了，此处大版本的更新新增了许多新的特性，特别是大家期望的对 TCP 的支持，在当前 topic 中我们将来探索 Traefik 2.0 有哪些值得我们关注的新特性。\nService Mesh下微服务的架构设计 杨彪（美团高级技术专家）\n当下Service Mesh技术可以说是炙手可热，它通过容器编排、持续交付DevOps、以及微服务等理论和方法来构建和运行云原生应用。然而Service Mesh毕竟发展才短短的2年，对于我们刚刚熟悉和稳定的微服务架构又将带来哪些挑战，业务系统架构是否有必要升级到Service Mesh，我们的微服务架构设计将发生什么改变，这些问题我将在本次大会交流讨论，敬请期待。\n","date":1572066000,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"73d940fabe0ee00971625962ad5ac976","permalink":"https://lib.jimmysong.io/event/service-mesh-meetup-07/","publishdate":"2019-10-26T13:00:00+08:00","relpermalink":"/event/service-mesh-meetup-07/","section":"event","summary":"这是第七届 Service Mesh Meetup。","tags":["Service Mesh"],"title":"Service Mesh Meetup #7 成都站","type":"event"},{"authors":null,"categories":null,"content":"本书为 Cloud Native Infrastructure 中文版，作者 Justin Garrison 和 Kris Nova ，英文版发行于 2017 年 11 月，已可以在网上免费获得，本书是关于创建和管理基础架构，以适用于云原生应用全生命周期管理的模式和实践。\n阅读完这本书后，您将会有如下收获：\n 理解为什么说云原生基础架构是高效运行云原生应用所必须的 根据准则来决定您的业务何时以及是否应该采用云原生 了解部署和管理基础架构和应用程序的模式 设计测试以证明您的基础架构可以按预期工作，即使在各种边缘情况下也是如此 了解如何以策略即代码的方式保护基础架构  许可证 本书英文版版权属于 O’Reilly，中文版版权归属于机械工业出版，基于署名-非商业性使用-相同方式共享 4.0（CC BY-NC-SA 4.0）分享，本书为本人自行翻译，目的在于学习和巩固云原生知识，如有需要请购买纸质书。\n","date":1571616000,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"ab6cb03678341ec789ecf55430a12ff9","permalink":"https://lib.jimmysong.io/book/cloud-native-infrastructure/","publishdate":"2019-10-21T00:00:00Z","relpermalink":"/book/cloud-native-infrastructure/","section":"book","summary":"第一本云原生书籍","tags":["翻译电子书"],"title":"云原生基础架构","type":"publication"},{"authors":null,"categories":null,"content":"Google 有许多通用工程实践，几乎涵盖所有语言和项目。此文档为长期积累的最佳实践，是集体经验的结晶。我们尽可能地将其公之于众，您的组织和开源项目也会从中受益。\n当前包含以下文档：\nGoogle 代码审查指南，实则两套指南：\n 代码审查者指南 代码开发者指南  译者序 此仓库翻译自 google/eng-practices，目前为止的主要内容为 Google 总结的如何进行 Code Review（代码审查） 指南，根据原 Github 仓库的标题判断以后会追加更多 Google 工程实践的内容。\n 本文档的 Github 地址，点击页面底部的 Edit this page 链接可以编辑页面。 其他语言的版本。 感谢 Ta-Ching Chen 的审阅。 译者：Jimmy Song 本网站使用 Hugo 构建。 本网站使用 hugo-book 主题。  许可证 本项目中的文档适用于 CC-By 3.0 许可证，该许可证鼓励您共享这些文档。\n","date":1569801600,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"415ed3757cbd85008bb45b82a1580dd7","permalink":"https://lib.jimmysong.io/book/google-engineering-practices/","publishdate":"2019-09-30T00:00:00Z","relpermalink":"/book/google-engineering-practices/","section":"book","summary":"谷歌代码审查实践指南","tags":["翻译电子书"],"title":"谷歌工程实践","type":"publication"},{"authors":["张波","彭泽文","涂小刚","敖小剑"],"categories":null,"content":"讲师与演讲话题 虎牙直播在微服务改造方面的实践 张波 虎牙基础保障部中间件团队负责人\n本次主要分享虎牙注册中心、名字服务、DNS 的改造实践，以及如何通过 Nacos 实现与 istio 打通实现，使微服务平滑过渡到 service mesh。\nService Mesh 在蚂蚁集团的生产级安全实践 彭泽文 蚂蚁集团高级开发工程师\n介绍通过 Envoy SDS（Secret Discovery Service）实现 Sidecar 证书管理的落地方案；分享如何为可信身份服务构建敏感信息数据下发通道，以及 Service Mesh Sidecar 的 TLS 生产级落地实践。\n基于 Kubernetes 的微服务实践 涂小刚 慧择网运维经理\n介绍如何跟据现有业务环境情况制定容器化整体解决方案，导入业务进入 K8S 平台，容器和原有业务环境互通。制订接入规范、配置中心对接 K8S 服务、网络互通方案、DNS 互通方案、jenkins-pipeline 流水线构建方案、日志采集方案、监控方案等。\nService Mesh 发展趋势（续）：棋到中盘路往何方 敖小剑 蚂蚁集团高级技术专家\n继续探讨 Service Mesh 发展趋势：深度分析 Istio 的重大革新 Mixer v2，Envoy 支持 Web Assembly 的意义所在，以及在 Mixer v2 出来之前的权宜之计; 深入介绍 Google Traffic Director 对虚拟机模式的创新支持方式，以及最近围绕 SMI 发生的故事。\n","date":1565499600,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"e43a0b86d60910a000c4731c733e3067","permalink":"https://lib.jimmysong.io/event/service-mesh-meetup-06/","publishdate":"2019-08-11T13:00:00+08:00","relpermalink":"/event/service-mesh-meetup-06/","section":"event","summary":"这是第六届 Service Mesh Meetup。","tags":["Service Mesh"],"title":"Service Mesh Meetup #6 广州站","type":"event"},{"authors":null,"categories":null,"content":"《未来架构 —— 从服务化到云原生》，张亮、吴晟、敖小剑、宋净超著，电子工业出版社出版，2019 年 4 月。\n  《未来架构》图书封面  这本书的第一作者是张亮，现就职于京东金融，为了丰富全书的内容，张亮要求了圈内的好友吴晟、敖小剑和我，共同创作了这本宏大命题的《未来架构》，下面引述的他介绍的成书原由。\n成书缘由 身处互联网行业的我们一向处在变革的最前端，受到世界浪潮的洗礼，不停歇地追赶着这一波又一波的技术潮流，才不会落在时代脚步之后。特别是近几年来，互联网架构不断演化，经历了从集中式架构到分布式架构，再到云原生架构的过程。云原生因能解决传统应用升级缓慢、架构臃肿、不能快速迭代等问题而逐渐成为这个时代舞台的主角。\n身处在这个变化浪潮中，我看着它改变着互联网架构的航行方向，并给越来越多的公司和个人带来新的思想和发展，也用我这些年走过的路、积累的经验、沉淀的眼界去学习它、读懂它，并让它融入我的知识体系网，来更新大脑里那张探索不断、充满指南针意义的架构地图。\n2017、2018 年，我与这些变化同进同退，让 Elastic-Job、Sharding-Sphere 成为业界里大家认可的项目、让所负责的开源项目开始走向国际化、也认识了更多的良师益友…… 这种种的经历和发展，触动我开始将所闻、所见、所知、所感的珠玑落到了笔尖，串联成了这本书：《未来架构 —— 从服务化到云原生》。\n这本书里有你想认识的分布式、服务化、服务网格、容器、编排治理、云原生、云数据库……\n这本书里既有我多年深思熟虑的见解和沉淀良久的经验，也有我弃笔又拾笔的挣扎，因为我需要让书的内容对读者负责……\n这本书里更有这些资深大咖的精彩章节叙述：Apache 孵化器项目 SkyWalking 创始人 \u0026amp; APM 专家吴晟、CNCF Ambassador \u0026amp; 云原生布道师 \u0026amp; 云原生社区创始人宋净超、Service Mesh 布道师敖小剑。\n目录  第 1 章 云原生 第 2 章 远程通信 第 3 章 配置 第 4 章 服务治理 第 5 章 观察分布式服务 第 6 章 侵入式服务治理方案 第 7 章 云原生生态的基石 Kubernetes 第 8 章 跨语言服务治理方案 Service Mesh 第 9 章 云原生数据架构 第 10 章 分布式数据库中间件生态圈 ShardingSphere  寄托期翼 书的封页是张亮老师选择的老特拉福德球场前矗立的曼联 Holy Trinity 雕像作为背景图。\n1958 年 2 月 6 日，曼联队在南斯拉夫参加欧冠杯获得半决赛权后，回程途中遭遇慕尼黑空难，曼联战队瞬间消失在夜空。为了曼联的复兴，幸存下来的曼联队的主帅马特・巴斯比强忍悲痛，用血泪和汗水重建曼联。1968 年 5 月 29 日，在慕尼黑空难整整 10 年后，巴斯比带领他的新战队终于捧起了欧洲冠军杯，告慰了那些故去的亡魂！这座 Holy Trinity 雕像变成了永恒的纪念！\n信仰、永不言弃、坚持不懈、创造奇迹、浴火重生…，是我从这座雕塑中感受到的力量。每个人的一生一定都会经历高峰和低谷，见过山川和沙漠，也希望这本书不仅仅能为大家带来互联网架构的干货知识，也能寄托我对大家的祝福：希望在这十万长征路上正在不懈拼搏的你，能够拥有自己的信仰和希望，即使要途径无数沙漠和海洋，也能在经历千帆后柳暗花明！\n","date":1554076800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"3fcfce4a4eaa9356b969ffe6d6a97952","permalink":"https://lib.jimmysong.io/book/future-architecture/","publishdate":"2019-04-01T00:00:00Z","relpermalink":"/book/future-architecture/","section":"book","summary":"张亮、吴晟、敖小剑、宋净超 著","tags":["出版物"],"title":"未来架构——从服务化到云原生","type":"publication"},{"authors":["郑德惠","陈逸凡","崔秀龙","宋净超","敖小剑"],"categories":null,"content":"讲师与演讲话题 唯品会 Service Mesh 的实践分享 郑德惠 唯品会Java资深开发工程师，内部Service Mesh框架负责人，唯品会开源项目vjtools重要开发者，10年电信与互联网后台开发经验。\nSOFAMosn 持续演进路径及实践案例 陈逸凡 花名无钩，蚂蚁集团资深开发工程师。专注于网络接入层，高性能服务器研发，SOFAMosn团队核心成员\n在网格的边缘试探——企业 Istio 试水指南 崔秀龙 HPE 软件分析师，Kubernetes 权威指南作者之一，Kubernetes、Istio 项目成员\nRoundtable：回顾2018，Service Mesh 蓄势待发 主持人：宋净超，ServiceMesher 社区联合创始人\n","date":1546750800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"71aa318f87a07770754a4650d2134da8","permalink":"https://lib.jimmysong.io/event/service-mesh-meetup-05/","publishdate":"2019-01-06T13:00:00+08:00","relpermalink":"/event/service-mesh-meetup-05/","section":"event","summary":"这是第五届 Service Mesh Meetup。","tags":["Service Mesh"],"title":"Service Mesh Meetup #5 广州站","type":"event"},{"authors":["吴晟","敖小剑","冯玮","徐运元"],"categories":null,"content":"讲师与演讲话题 Observability and Istio telemetry 吴晟 Apache SkyWalking创始人、Apache Sharding-Sphere原型作者、比特大陆资深技术专家、CNCF OpenTracing标准化委员会成员\n蚂蚁集团 Service Mesh 渐进式迁移方案 敖小剑 蚂蚁集团高级技术专家，十六年软件开发经验，微服务专家，Service Mesh布道师，Servicemesher社区联合创始人\n张瑜标 阿里巴巴技术专家、前京东Hadoop负责人、Hadoop代码贡献者、现负责UC 基于Kubernetes自研的PaaS平台整体的稳定性\n探讨和实践基于Isito的微服务治理事件监控 徐运元 谐云科技云平台架构师，致力于容器 PaaS 平台、企业级容器云平台的方案设计和技术落地\nEnvoy、Contour与Kubernetes实践 冯玮 七牛容器云平台产品架构师，曾在百度和华为从事公有云领域高性能分布式计算和存储平台的架构设计和产品研发\n","date":1543122000,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"15d6465073096faa5872f9ca72446fd0","permalink":"https://lib.jimmysong.io/event/service-mesh-meetup-04/","publishdate":"2018-11-25T18:00:00+08:00","relpermalink":"/event/service-mesh-meetup-04/","section":"event","summary":"这是第四届 Service Mesh Meetup。","tags":["Service Mesh"],"title":"Service Mesh Meetup #4 上海站","type":"event"},{"authors":["张超盟","朱经惠","邵俊雄","杨文"],"categories":null,"content":"讲师与演讲话题 张超盟（华为）——Kubernetes容器应用基于Istio的灰度发布实践\nTopic摘要：随着1.0版本在上月底的发布，标志着Istio作为最火热的ServcieMesh框架已经逐渐成熟。本次议题中将以典型的灰度发布为例，分享华为云容器服务在Istio的实践，以及Istio和Kubernetes的完美结合释放云原生应用的核心优势，加速企业微服务技术转型。\n讲师简介：华为云微服务平台架构师，现负责华为云容器服务Istio产品化工作。参与华为PaaS平台产品设计研发，在Kubernetes容器服务、微服务架构、云服务目录、大数据、APM、DevOpS工具等多个领域有深入研究与实践。曾供职于趋势科技。\n朱经惠 （联邦车网）——Istio控制平面组件原理解析\nTopic摘要：网上有很多关于Istio的介绍，但主要的关注是数据平面。所以这次独辟蹊径，给大家解密Istio里强大的控制平面：管理生命周期的Pilot-Agent，配置中心Pilot-Discovery， 生成遥测报告的Mixer以及安全证书管理的Istio_Ca。通过本次分享您将了解其工作原理和现存的问题。\n讲师简介：朱经惠，ETC车宝平台工程师。喜欢开源，个人开源项目《Jaeger PHP Client》；喜欢研究源码，对NSQ，Jaeger，Istio（控制平面）等go语言开源项目进行过研究。除了代码还喜欢爬山和第二天睡醒后全身酸疼的感觉。\n邵俊雄（蚂蚁集团）——SOFAMesh 的通用协议扩展\nTopic摘要：介绍蚂蚁集团在SOFAMesh上开发对SOFA RPC与HSF这两个RPC框架的支持过程中总结出来的一个通用协议扩展方案。\n讲师介绍：蚂蚁集团中间件团队高级技术专家，目前主要负责 SOFAMesh 的开发工作。\n杨文（JEX）——Kubernetes、Service Mesh、CI/CD 实践\nTopic摘要：本次主题我将跟大家分享我们在提升研发团队工程效率上的一些思考和实践，包括如何构建自动化 CI/CD 平台，如何提升持续交付能力，以及我们在这一系列演化过程中所踩过一些坑。\n讲师介绍：JEX 技术VP，前小恩爱技术总监，开源爱好者，TiDB、logkit 等多个开源项目的 Contributor，Go 夜读发起人。\n","date":1535173200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"a626ff20443cad1d9fe4d02a4f4607ec","permalink":"https://lib.jimmysong.io/event/service-mesh-meetup-03/","publishdate":"2018-08-25T18:00:00+08:00","relpermalink":"/event/service-mesh-meetup-03/","section":"event","summary":"这是第三届 Service Mesh Meetup。","tags":["Service Mesh"],"title":"Service Mesh Meetup #3 深圳站","type":"event"},{"authors":["张亮","吴晟","朵晓东","丁振凯"],"categories":null,"content":"讲师与演讲话题 张亮（京东金融数据研发负责人）：Service Mesh的延伸 —— 论道Database Mesh\n个人简介：张亮，京东金融数据研发负责人。热爱开源，目前主导两个开源项目Elastic-Job和Sharding-Sphere(Sharding-JDBC)。擅长以java为主分布式架构以及以Kubernetes和Mesos为主的云平台方向，推崇优雅代码，对如何写出具有展现力的代码有较多研究。2018年初加入京东金融，现担任数据研发负责人。目前主要精力投入在将Sharding-Sphere打造为业界一流的金融级数据解决方案之上。\n随着Service Mesh概念的推广与普及，云原生、低接入成本以及分布式组件下移等理念，已逐渐被认可。在Service Mesh依旧处于高速迭代的发展期的同时，以它的理念为参考，其他的Mesh思想也在崭露萌芽。 Database Mesh即是Service Mesh的其中一种延伸，虽然理念与Service Mesh相近，但数据库与无状态的服务却有着巨大的差别。Database Mesh与分布式数据库（如NoSQL和NewSQL）的功能范畴并非重叠而是互补，它更加关注数据库之上的中间啮合层。本次将与您一起交流Database Mesh的一些思考，以及探讨如何与现有产品相结合，实现更加强大与优雅的云原生数据库解决方案。\n 吴晟（Apache SkyWalking创始人）：Observability on Service Mesh —— Apache SkyWalking 6.0\n个人简介：Apache SkyWalking 创始人，PPMC和Committer，比特大陆资深技术专家，Tetrate.io Founding Engineer，专注APM和自动化运维相关领域。Microsoft MVP。CNCF OpenTracing标准化委员会成员。Sharding-Sphere PMC 成员。\nAPM在传统意义上，都是通过语言探针，对应用性能进行整体分析。但随着Cloud Native, K8s容器化之后，以Istio为代表的Service Mesh的出现，为可观测性和APM提供了一种新的选择。SkyWalking作为传统上提供多语言自动探针的Apache开源项目，在service mesh的大背景下，也开始从新的角度提供可观测性支持。\nSkyWalking和Tetrate Inc. Istio核心团队合作，从Mixer接口提取遥感数据，提供SkyWalking语言探针一样的功能，展现service mesh风格探针的强大力量。之后，也会和更多的mesh实现进行合作，深入在此领域的运用。\n 朵晓东（蚂蚁集团，高级技术专家）：蚂蚁集团开源的Service Mesh数据平面SOFA MOSN深层揭秘\n个人简介：蚂蚁集团高级技术专家，专注云计算技术及产品。Apache Kylin创始团队核心成员；蚂蚁金融云PaaS创始团队核心成员，Antstack网络产品负责人；SOFAMesh创始团队核心成员。\nService Mesh技术体系在蚂蚁落地过程中，我们意识到Mesh结合云原生在多语言，流量调度等各方面的优势，同时面对蚂蚁内部语言体系与运维构架深度融合，7层流量调度规则方式复杂多样，金融级安全要求等诸多特征带来的问题和挑战，最终选择结合蚂蚁自身情况自研Golang版本数据平面MOSN，同时拥抱开源社区，支持作为Envoy替代方案与Istio集成工作。本次session将从功能、构架、跨语言、安全、性能、开源等多方面分享Service Mesh在蚂蚁落地过程中在数据平面的思考和阶段成果。\n 丁振凯（新浪微博，微博搜索架构师）：微博Service Mesh实践 - WeiboMesh\n个人简介：微博搜索架构师，主要负责搜索泛前端架构工作。主导搜索结果和热搜榜峰值应对及稳定性解决方案，以及微服务化方案落地。在Web系统架构方面拥有比较丰富的实践和积累。喜欢思考，深究技术本质。去年十一鹿晗关晓彤事件中一不小心成为网红工程师，并成功登上自家热搜榜。\nWeiboMesh源自于微博内部对异构体系服务化的强烈需求以及对历史沉淀的取舍权衡，它没有把历史作为包袱，而是巧妙的结合自身实际情况完成了对Service Mesh规范的实现。目前WeiboMesh在公司内部已经大规模落地，并且已经开源，WeiboMesh是非常接地气的Service Mesh实现。本次分享主要介绍微博在跨语言服务化面临的问题及WeiboMesh方案介绍，并结合业务实例分析WeiboMesh的独到之处。\n","date":1532840400,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"57e36a41d7270d13e1609109d6ec7d26","permalink":"https://lib.jimmysong.io/event/service-mesh-meetup-02/","publishdate":"2018-07-29T13:00:00+08:00","relpermalink":"/event/service-mesh-meetup-02/","section":"event","summary":"这是第二届 Service Mesh Meetup。","tags":["Service Mesh"],"title":"Service Mesh Meetup #2 北京站","type":"event"},{"authors":null,"categories":null,"content":"《Python 云原生》，Marish Sethi 著，宋净超译，电子工业出版社出版，2018 年 7 月。\n  《Python云原生》图书封面  Cloud Native Python 介绍 随着当今商业的迅速发展，企业为了支撑自身的迅速扩张，仅仅通过自有的基础设施是远远不够的。因此，他们一直在追求利用云的弹性来构建支持高度可扩展应用程序的平台。\n这本书能够帮助您一站式的了解使用Python构建云原生应用架构的所有信息。本书中我们首先向您介绍云原生应用架构和他们能够帮助您解决哪些问题。然后您将了解到如何使用REST API和Python构建微服务，通过事件驱动的方式构建Web层。接下来，您将了解到如何与数据服务进行交互，并使用React构建Web视图，之后我们将详细介绍应用程序的安全性和性能。然后，您还将了解到如何Docker容器化您的服务。最后，您将学习如何在AWS和Azure平台上部署您的应用程序。在您部署了应用程序后，我们将围绕关于应用程序故障排查的一系列概念和技术来结束这本书。\n本书中涵盖哪些内容  第1章 介绍云原生应用架构和微服务，讨论云原生架构的基本概念和构建应用程序开发环境。 第2章 使用Python构建微服务，构建自己的微服务知识体系并根据您的用例进行扩展。 第3章 使用Python构建Web应用程序，构建一个初始的Web应用程序并与微服务集成。 第4章 与数据服务交互，教您如何将应用程序迁移到不同的数据库服务。 第5章 使用React构建Web视图。 第6章 使用Flux创建可扩展UI，帮助您理解如何使用Flux创建可扩展的应用程序。 第7章 事件溯源和CQRS，讨论如何以事件形式存储合约（transaction）。 第8章 保护Web应用程序，让您的应用程序免于受到外部威胁。 第9章 持续交付，应用程序频繁发布的相关知识。 第10章 Docker容器化您的服务，讨论容器服务和在Docker中运行应用程序。 第11章 将应用程序部署到AWS平台上，教您如何在AWS上构建基础设施并建立应用程序的生产环境。 第12章 将应用程序部署到Azure平台上，讨论如何在Azure上构建基础设施并建立应用程序的生产环境。 第13章 监控云应用，了解不同的基础设施和应用的监控工具。  使用本书您需要哪些工具和环境 您需要在系统上安装Python。一个文本编辑器，最好是Vim、Sublime或者Notepad++。在有一个章节中您需要下载POSTMAN，这是一个功能强大的API测试套件，可以作为作为Chrome扩展插件来安装。您可以从这里下载。\n除此之外，如果您还有如下网站的账号那就更好了：\n Jenkins Docker Amazon Web Services Terraform  目标读者 本书适用于具有Python基础知识、熟悉命令行和基于HTTP的应用程序基本原理的开发人员。对于那些想要了解如何构建、测试和扩展Python开发的应用程序的人员来说本书是个理想选择。不需要有使用Python构建微服务的经验。\n","date":1530403200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"ee1ffade370d7cff3745bdfe71a1a446","permalink":"https://lib.jimmysong.io/book/cloud-native-python/","publishdate":"2018-07-01T00:00:00Z","relpermalink":"/book/cloud-native-python/","section":"book","summary":"构建应对海量用户数据的高可扩展Web应用","tags":["出版物"],"title":"Python 云原生","type":"publication"},{"authors":null,"categories":null,"content":"《云原生 Java》，Josh Long 著，张若飞、宋净超译，电子工业出版社出版，2017 年 7 月。\n  云原生 Java 图书封面     照片拍摄于 2018 年 11 月 3 日，北京\n本书介绍 传统企业与如亚马逊、Netflix和Etsy这类企业之间的区别是什么？这些公司有完善云原生开发方法，这些方法使得他们能够保持优势并领先于竞争对手。本实践指南向Java/JVM开发人员展示如何使用Spring Boot、Spring Cloud和Cloud Foundry更快更好得构建软件。\n很多组织都已踏足云计算、测试驱动开发、微服务与持续集成和交付领域。本书作者Josh Long和Kenny Bastani将带您深入研究这些工具和方法，并帮助您将传统应用程序转变为真正的云原生应用程序。\n本书中包含以下四大部分：\n 基础知识：了解云原生思维背后的动机；配置和测试Spring Boot应用程序；将您的传统应用程序迁移至云端 微服务：使用Spring构建HTTP和RESTful服务；在分布式系统中路由请求；建立更接近数据的边缘服务 数据整合：使用Spring Data管理数据，并将分布式服务与Spring支持的事件驱动的，以消息传递为中心的架构集成 生产：让您的系统可观测；使用服务代理来连接有状态的服务；了解持续交付背后的重要思想  如果您正在构建云原生应用程序，这本书将是使用Java生态系统的基本指南。本书中包含了所有内容——构建弹性服务、管理数据流（通过REST和异步事件）、测试、部署和可观测性的关键任务。\n——Daniel Bryant，SpectoLabs的软件开发者和CTO\n我预测无论是刚开始云原生之旅还是已经接近云原生的目标，所有参与其中的人都将从这本云原生Java的洞察和经验中受益。\n——Dava Syer博士，Spring框架的贡献者，Spring Boot和Spring Cloud的贡献者和联合创始人\n作者信息 Josh Long是一名Spring布道师，同时也是InfoQ.com的Java queue编辑，以及包括Spring Recipes第二版（Apress出版社出版）在内的多本书籍的主要作者。Josh在许多国际行业会议上发表过演讲，包括TheServiceSide Java Symposium、SpringOne、OSCON、JavaZone、Devoxx、Java2Days等。当他没在编写SpringSource的代码的时候，不是泡在Java用户组就是在咖啡店里喝咖啡。Josh喜欢能够推动技术发展的解决方案。他的兴趣包括可扩展性、BPM、网格计算、移动计算和所谓的“智能”系统等。您可以在http://blog.springsource.org或http://joshlong.com上浏览他的博客。\nKenny Bastani是Pivotal的Spring布道师。作为一名开源贡献者和博客作者，Kenny关注图数据库、微服务等，并喜欢吸引一群充满热情的软件开发人员。Kenny还是OSCON、SpringOne Platform和GOTO等行业会议的常客。他维护了一个关于软件架构的个人博客，并提供用于构建事件驱动的微服务和无服务器架构的教程和开源参考示例。\n目录 序言（James Watters） xvii\n序言（Rod Johnson） xix\n前言 xxi\n第Ⅰ部分　基础知识\n 第1 章　云原生应用程序 3 第2 章　训练营：Spring Boot 和Cloud Foundry 21 第3 章　符合十二要素程序风格的配置 67 第４章　测试 85 第5 章　迁移遗留的应用程序 115  第Ⅱ部分　Web 服务\n 第6 章　REST API 137 第7 章　路由 179 第8 章　边缘服务 197  第Ⅲ部分　数据整合\n 第9 章　数据管理 251 第10 章　消息系统 303 第11 章　批处理和任务 325 第12 章　数据集成 363  第IV 部分　生产\n 第13 章　可观测的系统 411 第14 章　服务代理 469 第15 章　持续交付 497  第V 部分 附录\n附录A 在Java EE 中使用Spring Boot 527\n","date":1530403200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"5e4ce4787f40566e2879959ee0cdac40","permalink":"https://lib.jimmysong.io/book/cloud-native-java/","publishdate":"2018-07-01T00:00:00Z","relpermalink":"/book/cloud-native-java/","section":"book","summary":"张若飞、宋净超 译","tags":["出版物"],"title":"云原生 Java","type":"publication"},{"authors":["敖小剑","刘超","唐鹏程","徐运元"],"categories":null,"content":"讲师分享  云原生社区 meetup 第七期深圳站开场致辞 - 宋净超 使用 IAST 构建高效的 DevSecOps 流程 - 董志勇 云原生场景下的开发和调试-汪晟杰，黄金浩 Envoy 在腾讯游戏云原生平台应用 - 田甜 使用 KubeVela 构建混合云应用管理平台 - 邓洪超  ","date":1530334800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"83056fece6b9c5f9d8c261c13b6a6a6e","permalink":"https://lib.jimmysong.io/event/service-mesh-meetup-01/","publishdate":"2018-06-30T18:00:00+08:00","relpermalink":"/event/service-mesh-meetup-01/","section":"event","summary":"这是第一届 Service Mesh Meetup。","tags":["Service Mesh"],"title":"Service Mesh Meetup #1 杭州站","type":"event"},{"authors":["敖小剑","章耿","董一韬","宋国恒","曹杰","俞仁杰","陈龙"],"categories":null,"content":"SOFAStack（Scalable Open Financial Architecture Stack）是蚂蚁集团自主研发并开源的金融级分布式架构，包含了构建金融级云原生架构所需的各个组件，是在金融场景里锤炼出来的最佳实践。SOFAStack 官方网站：https://www.sofastack.tech/\n参加此次 Meetup 您将获得：\n 基于 SOFAStack 快速构建微服务 金融场景下的分布式事务最佳实践 基于 Kubernetes 的云原生部署体验 云上的 Service Mesh 基本使用场景体验 基于 Serverless 轻松构建云上应用  如何注册：此活动须提前注册。请将 SOFAStack Cloud Native Workshop 添加到您 KubeCon + CloudNativeCon + Open Source Summit 的注册表里。您可以使用 KCCN19COMATF 折扣码获取 KubeCon 半价门票！\n如果对此活动有任何疑问，请发送邮件至 jingchao.sjc@antfin.com。\n活动详情 9:00 - 9:20 开场演讲 SOFAStack 云原生开源体系介绍 by 余淮\n9:20 - 10:10 使用 SOFAStack 快速构建微服务 by 玄北\n基于 SOFA 技术栈构建微服务应用。通过本 workshop ，您可以了解在 SOFA 体系中如何上报应用监控数据、服务链路数据以及发布及订阅服务。\n10:15 - 11:05 SOFABoot 动态模块实践 by 卫恒\n在本 workshop 中，您可以基于 SOFADashboard 的 ARK 管控能力来实现 SOFAArk 提供的合并部署和动态模块推送的功能。\n11:10 - 12:00 使用 Seata 保障支付一致性 by 屹远\n微服务架构下，分布式事务问题是一个业界难题。通过本workshop，您可以了解到分布式架构下，分布式事务问题产生的背景，以及常见的分布式事务解决方案；并亲身体验到如何使用开源分布式事务框架Seata的AT模式、TCC模式解决业务数据的最终一致性问题。\n12:00 - 13:00 午餐时间\n13:00 - 13:30 蚂蚁集团的云原生探索与实践 by 首仁\n13:30 - 14:40 通过 Serverless 快速上云 by 隐秀\n作为云原生技术前进方向之一，Serverless 架构让您进一步提高资源利用率，更专注于业务研发。通过我们的 workshop，您可以体验到快速创建 Serveless 应用、根据业务请求秒级 0-1-N 自动伸缩、通过日志查看器快速排错、按时间触发应用等产品新功能。\n14:50 - 16:00 使用 CloudMesh 轻松实践 Service Mesh by 敖小剑\nService Mesh 将服务间通信能力下沉到基础设施，让应用解耦并轻量化。但 Service Mesh 本身的复杂度依然存在，CloudMesh 通过将 Service Mesh 托管在云上，使得您可以轻松的实践 Service Mesh 技术。通过我们的 workshop，您可以快速部署应用到 CloudMesh ，对服务进行访问，通过监控查看流量，体验服务治理、Sidecar管理和对服务的新版本进行灰度发布等实用功能。\n","date":1529805600,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"95540d21f15586a7e48215064ab94210","permalink":"https://lib.jimmysong.io/event/sofastack-cloud-native-workshop/","publishdate":"2018-06-24T20:00:00+08:00","relpermalink":"/event/sofastack-cloud-native-workshop/","section":"event","summary":"这是 KubeCon 第一次在中国举办，蚂蚁集团参与了同场活动，进行了 SOFAStack 云原生动手实验。","tags":["SOFAStack","KubeCon"],"title":"SOFAStack Cloud Native Workshop","type":"event"},{"authors":null,"categories":null,"content":"《Cloud Native Go》，Kevin Hoffman 著，宋净超、吴迎松、徐蓓、马超译，电子工业出版社出版，2017 年 8 月。\n  Cloud Native Go 图书封面   本书作者：Kevin Hoffman \u0026amp; Dan Nemeth 译者：宋净超、吴迎松、徐蓓、马超译 出版社：电子工业出版社 全名：Cloud Native Go - 基于Go和React的web云原生应用构建指南  本书已由电子工业出版社出版，可以在京东上购买。\n   照片拍摄于 2017 年 9 月 12 日，北京\n简介 Cloud Native Go向开发人员展示如何构建大规模云应用程序，在满足当今客户的强大需求的同时还可以动态扩展来处理几乎任何规模的数据量、流量或用户。\nKevin Hoffman和Dan Nemeth详细描述了现代云原生应用程序，阐明了与快速、可靠的云原生开发相关的因素、规则和习惯。他们还介绍了Go这种“简单优雅”的高性能语言，它特别适合于云开发。\n在本书中你将使用Go语言创建微服务，使用ReactJS和Flux添加前端Web组件，并掌握基于Go的高级云原生技术。Hoffman和Nemeth展示了如何使用Wercker、Docker和Dockerhub等工具构建持续交付管道; 自动推送应用程序到平台上; 并系统地监控生产中的应用程序性能。\n 学习“云之道”：为什么开发好的云软件基本上是关于心态和规则 了解为什么使用Go语言是云本地微服务开发的理想选择 规划支持持续交付和部署的云应用程序 设计服务生态系统，然后以test-first的方式构建它们 将正在进行的工作推送到云 使用事件源和CQRS模式来响应大规模和高吞吐量 安全的基于云的Web应用程序：做与不做的选择 使用第三方消息传递供应商创建响应式云应用程序 使用React和Flux构建大规模，云友好的GUI 监控云中的动态扩展，故障转移和容错  章节简介如下图。\n   关于作者 Kevin Hoffman通过现代化和以多种不同语言构建云原生服务的方式帮助企业将其应用程序引入云端。他10岁时开始编程，在重新组装的CommodoreVIC-20上自习BASIC。从那时起，他已经沉迷于构建软件，并花了很多时间学习语言、框架和模式。他已经构建了从遥控摄影无人机、仿生性安全系统、超低延迟金融应用程序到移动应用程序等一系列软件。他在构建需要与Pivotal Cloud Foundry配合使用的自定义组件时爱上了Go语言。\nKevin 是流行的幻想书系列（The Sigilord Chronicles ）的作者，他热切地期待着最终能够将自己对构建软件的热爱与对构建幻想世界的热爱结合起来。\nDan Nemeth目前在Pivotal担任咨询解决方案架构师，负责支持Pivotal Cloud Foundry。他从Commodore 64开始就一直在开发软件，从1995年起开始专业编码，使用ANSIC编写了用于本地ISP的CGI脚本。从那时起，他职业生涯的大部分时间里是作为独立顾问为从金融到制药行业提供解决方案，并使用当时流行的各种语言和框架。Dan最近接受了Go作为自己的归宿，并热情地将它用于所有的项目。\n如果你发现Dan没在电脑前，他很可能就是在靠近安纳波利斯的水域玩帆船或飞钓。\n目录  第1章 云之道 第2章 开始 第3章 Go入门 第4章 持续交付 第5章 在Go中构建微服务 第6章 运用后端服务 第7章 构建数据服务 第8章 事件溯源和CQRS 第9章 使用Go构建web应用程序 第10章 云安全 第11章 使用WebSockets 第12章 使用React构建Web视图 第13章 使用Flux构建可扩展的UI 第14章 创建完整应用World of FluxCraft 第15章 结论 附录A 云应用的故障排查 索引  ","date":1501545600,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"0e73bfe8c6dd89864805484bce53c620","permalink":"https://lib.jimmysong.io/book/cloud-native-go/","publishdate":"2017-08-01T00:00:00Z","relpermalink":"/book/cloud-native-go/","section":"book","summary":"宋净超、吴迎松、徐蓓、马超译","tags":["出版物"],"title":"Cloud Native Go","type":"publication"},{"authors":null,"categories":null,"content":"本书是 Migrating to Cloud Native Application Architectures 的中文版，本书英文版发布于 2015 年 2 月，中文版由 Jimmy Song 翻译，发布于 2017 年 7 月。\n译者序 云时代的云原生应用大势已来，将传统的单体架构应用迁移到云原生架构，你准备好了吗？\n俗话说“意识决定行动”，在迁移到云原生应用之前，我们大家需要先对 Cloud Native（云原生）的概念、组织形式并对实现它的技术有一个大概的了解，这样才能指导我们的云原生架构实践。\n原书作于2015年，其中的示例主要针对 Java 应用，实际上也适用于任何应用类型，云原生应用架构适用于异构语言的程序开发，不仅仅是针对 Java 语言的程序开发。截止到本人翻译本书时，云原生应用生态系统已经初具规模，CNCF 成员不断发展壮大，基于 Cloud Native 的创业公司不断涌现，kubernetes 引领容器编排潮流，和 Service Mesh 技术（如 Linkerd 和 Istio） 的出现，Go 语言的兴起（参考另一本书 Cloud Native Go）等为我们将应用迁移到云原生架构的提供了更多的方案选择。\n简介 当前很多企业正在采用云原生应用架构，这可以帮助其IT转型，成为市场竞争中真正敏捷的力量。 O’Reilly 的报告中定义了云原生应用架构的特性，如微服务和十二因素应用程序。\n本书中作者Matt Stine还探究了将传统的单体应用和面向服务架构（SOA）应用迁移到云原生架构所需的文化、组织和技术变革。本书中还有一个迁移手册，其中包含将单体应用程序分解为微服务，实施容错模式和执行云原生服务的自动测试的方法。\n本书中讨论的应用架构包括：\n 十二因素应用程序：云原生应用架构模式的集合 微服务：独立部署的服务，每个服务只做一件事情 自助服务的敏捷基础设施：快速，可重复和一致地提供应用环境和后台服务的平台 基于API的协作：发布和版本化的API，允许在云原生应用架构中的服务之间进行交互 抗压性：根据压力变强的系统  关于作者 Matt Stine，Pivotal的技术产品经理，拥有15年企业IT和众多业务领域的经验。Matt 强调精益/敏捷方法、DevOps、架构模式和编程范例，他正在探究使用技术组合帮助企业IT部门能够像初创公司一样工作。\n","date":1499731200,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"486df942c684c0eed9030caa9246ec19","permalink":"https://lib.jimmysong.io/book/migrating-to-cloud-native-application-architectures/","publishdate":"2017-07-11T00:00:00Z","relpermalink":"/book/migrating-to-cloud-native-application-architectures/","section":"book","summary":"第一本介绍云原生的电子书","tags":["翻译电子书"],"title":"迁移到云原生应用架构","type":"publication"},{"authors":null,"categories":null,"content":"本书起始于 2017 年 3 月，记录了本人从零开始学习和使用 Kubernetes 的心路历程，着重于经验分享和总结，同时也会有相关的概念解析，希望能够帮助大家少踩坑，少走弯路，还会指引大家关注 Kubernetes 生态周边，如微服务构建、DevOps、大数据应用、服务网格（Service Mesh）、云原生等领域。\n本书主题 本书的主题不局限于 Kubernetes，还包括以下几大主题：\n 云原生开源组件 云原生应用与微服务架构 基于 Kubernetes 的 Service Mesh 架构 Kubernetes 与微服务结合实践  ","date":1489104000,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"28cfdf5e03d31fe45dbec103a514ab86","permalink":"https://lib.jimmysong.io/book/kubernetes-handbook/","publishdate":"2017-03-10T00:00:00Z","relpermalink":"/book/kubernetes-handbook/","section":"book","summary":"云原生应用架构实战手册","tags":["Handbook系列"],"title":"Kubernetes 基础教程","type":"publication"},{"authors":null,"categories":["Envoy"],"content":"速率限制的统计 无论是使用全局还是局部的速率限制，Envoy 都会发出下表中描述的指标。我们可以在配置过滤器时使用 stat_prefix 字段来设置统计信息的前缀。\n当使用局部速率限制器时，每个度量名称的前缀是 \u0026lt;stat_prefix\u0026gt;.http_local_rate_limit.\u0026lt;metric_name\u0026gt;，当使用全局速率限制器时，前缀是 cluster.\u0026lt;route_target_cluster\u0026gt;.ratelimit.\u0026lt;metric_name\u0026gt;。\n   速率限制器 指标名称 描述     局部 enabled 速率限制器被调用的请求总数   局部 / 全局 ok 来自令牌桶的低于限制的响应总数   局部 rate_limited 没有可用令牌的答复总数（但不一定强制执行）   局部 enforced 有速率限制的请求总数（例如，返回 HTTP 429）。   全局 over_limit 速率限制服务的超限答复总数   全局 error 与速率限制服务联系的错误总数   全局 failure_mode_allowed 属于错误但由于 failure_mode_deny 设置而被允许的请求总数    ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"ee5f05a328dbe72eb78961199616dc1a","permalink":"https://lib.jimmysong.io/envoy-handbook/hcm/rate-limiting-stats/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/envoy-handbook/hcm/rate-limiting-stats/","section":"envoy-handbook","summary":"速率限制的统计 无论是使用全局还是局部的速率限制，Envoy 都会发出下表中描述的指标。我们可以在配置过滤器时使用 stat_prefix 字段来设置统计信息的前缀。 当使用局部速率限制器时，每个度量名称的前缀是 \u003cstat_","tags":["Envoy"],"title":"","type":"envoy-handbook"},{"authors":null,"categories":["安全"],"content":"Kubernetes Hardening Guidance National Security Agency Cybersecurity and Infrastructure Security Agency\nCybersecurity Technical Report\nNotices and history Document change history\nAugust 2021 1.0 Initial release\nDisclaimer of warranties and endorsement\nThe information and opinions contained in this document are provided “as is” and without any warranties or guarantees. Reference herein to any specific commercial products, process, or service by trade name, trademark, manufacturer, or otherwise, does not necessarily constitute or imply its endorsement, recommendation, or favoring by the United States Government, and this guidance shall not be used for advertising or product endorsement purposes.\nTrademark recognition\nKubernetes is a registered trademark of The Linux Foundation. ▪ SELinux is a registered trademark of the National Security Agency. ▪ AppArmor is a registered trademark of SUSE LLC. ▪ Windows and Hyper-V are registered trademarks of Microsoft Corporation. ▪ ETCD is a registered trademark of CoreOS, Inc. ▪ Syslog-ng is a registered trademark of One Identity Software International Designated Activity Company. ▪ Prometheus is a registered trademark of The Linux Foundation. ▪ Grafana is a registered trademark of Raintank, Inc. dba Grafana Labs ▪ Elasticsearch and ELK Stack are registered trademarks of Elasticsearch B.V.\nCopyright recognition\nInformation, examples, and figures in this document are based on Kubernetes Documentation by The Kubernetes Authors, published under a Creative Commons Attribution 4.0 license.\nPublication information Author(s)\nNational Security Agency (NSA) Cybersecurity Directorate Endpoint Security\nCybersecurity and Infrastructure Security Agency (CISA)\nContact information\nClient Requirements / General Cybersecurity Inquiries: Cybersecurity Requirements Center, 410-854-4200, Cybersecurity_Requests@nsa.gov\nMedia inquiries / Press Desk: Media Relations, 443-634-0721, MediaRelations@nsa.gov\nFor incident response resources, contact CISA at CISAServiceDesk@cisa.dhs.gov.\nPurpose\nNSA and CISA developed this document in furtherance of their respective cybersecurity missions, including their responsibilities to develop and issue cybersecurity specifications and mitigations. This information may be shared broadly to reach all appropriate stakeholders.\nExecutive summary Kubernetes® is an open-source system that automates the deployment, scaling, and management of applications run in containers, and is often hosted in a cloud environment. Using this type of virtualized infrastructure can provide several flexibility and security benefits compared to traditional, monolithic software platforms. However, securely managing everything from microservices to the underlying infrastructure introduces other complexities. The hardening guidance detailed in this report is designed to help organizations handle associated risks and enjoy the benefits of using this technology.\nThree common sources of compromise in Kubernetes are supply chain risks, malicious threat actors, and insider threats.\nSupply chain risks are often challenging to mitigate and can arise in the container build cycle or infrastructure acquisition. Malicious threat actors can exploit vulnerabilities and misconfigurations in components of the Kubernetes architecture, such as the control plane, worker nodes, or containerized applications. Insider threats can be administrators, users, or cloud service providers. Insiders with special access to an organization’s Kubernetes infrastructure may be able to abuse these privileges.\nThis guidance describes the security challenges associated with setting up and securing a Kubernetes cluster. It includes hardening strategies to avoid common misconfigurations and guide system administrators and developers of National Security Systems on how to deploy Kubernetes with example configurations for the recommended hardening measures and mitigations. This guidance details the following mitigations:\n Scan containers and Pods for vulnerabilities or misconfigurations. Sun containers and Pods with the least privileges possible. Use network separation to control the amount of damage a compromise can cause. Use firewalls to limit unneeded network connectivity and encryption to protect confidentiality. Use strong authentication and authorization to limit user and administrator access as well as to limit the attack surface. Use log auditing so that administrators can monitor activity and be alerted to potential malicious activity. Periodically review all Kubernetes settings and use vulnerability scans to help ensure risks are appropriately accounted for and security patches are applied.  For additional security hardening guidance, see the Center for Internet Security Kubernetes benchmarks, the Docker and Kubernetes Security Technical Implementation Guides, the Cybersecurity and Infrastructure Security Agency (CISA) analysis report, and Kubernetes documentation [1], [2], [3], [6].\nIntroduction Kubernetes, …","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"0eff61ecdb5db16ac492ae8211e11b40","permalink":"https://lib.jimmysong.io/kubernetes-hardening-guidance/kubernetes-hardening-guidance-english/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/kubernetes-hardening-guidance/kubernetes-hardening-guidance-english/","section":"kubernetes-hardening-guidance","summary":"Kubernetes Hardening Guidance National Security Agency Cybersecurity and Infrastructure Security Agency\nCybersecurity Technical Report\nNotices and history Document change history\nAugust 2021 1.0 Initial release\nDisclaimer of warranties and endorsement\nThe information and opinions contained in this document are provided “as is” and without any warranties or guarantees. Reference herein to any specific commercial products, process, or service by trade name, trademark, manufacturer, or otherwise, does not necessarily constitute or imply its endorsement, recommendation, or favoring by the United States Government, and this guidance shall not be used for advertising or product endorsement purposes.\nTrademark recognition\nKubernetes is a registered trademark of The Linux Foundation.","tags":["Kubernetes","安全"],"title":"","type":"kubernetes-hardening-guidance"},{"authors":null,"categories":["安全"],"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1657070661,"objectID":"fa83ce21d33f41d95c5f04fe6fd177ae","permalink":"https://lib.jimmysong.io/service-mesh-devsecops/intro/reference-platform-for-the-implementation-of-devsecops-primitives/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/service-mesh-devsecops/intro/reference-platform-for-the-implementation-of-devsecops-primitives/","section":"service-mesh-devsecops","summary":"","tags":["Service Mesh","DevSecOps"],"title":"","type":"service-mesh-devsecops"}]